From |ove|y||tt|ed@|@|e@ @end|ng |rom gm@||@com  Mon Jul  1 19:09:22 2019
From: |ove|y||tt|ed@|@|e@ @end|ng |rom gm@||@com (Jiawen Ng)
Date: Mon, 1 Jul 2019 18:09:22 +0100
Subject: [R-sig-Geo] Running huge dataset with dnearneigh
In-Reply-To: <alpine.LFD.2.21.1906301721550.17653@reclus.nhh.no>
References: <CAHz1c-V8DcT4UNqiw38pLvJrDJgFjGu+rq=a1PLhznxsTLdAGw@mail.gmail.com>
 <alpine.LFD.2.21.1906301721550.17653@reclus.nhh.no>
Message-ID: <CAHz1c-WQCzcC6zZx8mcgLBw=Vn26jqg684K-tCYHUBfUjbYanw@mail.gmail.com>

Dear Roger,

Thank you so much for your detailed response and pointing out potential
pitfalls! It has prompted me to re-evalutate my approach.

Here is the context: I have some stores' sales data (this is my training
set of 214 points), I would like to find out where best to set up new
stores in UK. I am using a geodemographics approach to do this: Perform a
regression of sales against census data, then predict sales on UK output
areas (by centroids) and finally identify new areas with
location-allocation models. As the stores are points, this has led me to
define UK output areas by its population-weighted centroids, thus resulting
in the prediction by points rather than by areas. Tests (like moran's I and
lagrange multiplier) for spatial relationships among the points in my
training set were significant hence this has led me to implement some
spatial models (specifically spatial lag, error and durbin models) to
account for the spatial relationships in the data.

I am quite unsettled and unclear as to which neighbourhood definition to go
for actually. I thought of IDW at first as I thought this would summarise
each point's relationship with their neighbours very precisely thus making
the predictions more accurate. Upon your advice (don't use IDW or other
general weights for predictions), I decided not to use IDW, and changed it
to dnearneigh instead (although now I am questioning myself on the
definition of what is meant by general weights. Perhaps I am understanding
the definition of general weights wrong, if dnearneigh is still considered
to be a 'general weights' method) Why is the use of IDW not advisable
however? Is it due to computational reasons? Also, why would having
thousands of neighbours be making no sense? Apologies for asking so many
questions, I'd just like to really understand the concepts!

I believe that both the train and test set has varying intensities. I was
weighing the different neighbourhood methods: dnearneigh, knearneigh, using
IDW etc. and I felt like each method would have its disadvantages -- its
difficult to pinpoint which neighbourhood definition would be best. If one
were to go for knearneigh for example, results may not be fair due to the
inhomogeneity of the points -- for instance, point A's nearest neighbours
may be within a few hundreds of kilometres while point B's nearest
neighbours may be in the thousands. I feel like the choice of any
neighbourhood definition can be highly debateable... What do you think?

After analysing my problem again, I think that predicting by output areas
(points) would be best for my case as I would have to make use of the
population data after building the model. Interpolating census data of the
output area (points) would cause me to lose that information.

Thank you for the comments and the advice so far,  I would greatly welcome
and appreciate additional feedback!

Thank you so much once again!

Jiawen








On Sun, 30 Jun 2019 at 16:57, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Sat, 29 Jun 2019, Jiawen Ng wrote:
>
> > Dear Roger,
>
> Postings go to the whole list ...
>
> >
> > How can we deal with a huge dataset when using dnearneigh?
> >
>
> First, why distance neighbours? What is the support of the data, point or
> polygon? If polygon, contiguity neighbours are preferred. If not, and the
> intensity of observations is similar across the whole area, distance may
> be justified, but if the intensity varies, some observations will have
> very many neighbours. In that case, unless you have a clear ecological or
> environmental reason for knowing that a known distance threshold binds, it
> is not a good choice.
>
> > Here is my code:
> >
> > d <- dnearneigh(spdf,0, 22000)
> > all_listw <- nb2listw(d, style = "W")
> >
> > where the spdf object is in the british national grid CRS:
> > +init=epsg:27700, with 227,973 observations/points. The distance of
> 22,000
> > was decided by a training set that had 214 observations and the spdf
> object
> > contains both the training set and the testing set.
> >
>
> This is questionable. You train on 214 observations - do their areal
> intensity match those of the whole data set? If chosen at random, you run
> into the spatial sampling problems discussed in:
>
>
> https://www.sciencedirect.com/science/article/pii/S0304380019302145?dgcid=author
>
> Are 214 observations for training representative of 227,973 prediction
> sites? Do you only have observations on the response for 214, and an
> unobserved response otherwise? What are the data, what are you trying to
> do and why? This is not a sensible setting for models using weights
> matrices for prediction (I think), because we do not have estimates of the
> prediction error in general.
>
> > I am using a Mac, with a processor of 2.3 GHz Intel Core i5 and 8 GB
> > memory. My laptop showed that when dnearneigh command was run on all
> > observations, around 6.9 out of 8GB was used by the rsession and that the
> > %CPU used by the rsession was stated to be around 98%, although another
> > indicator showed that my computer was around 60% idle. After running the
> > command for a day, rstudio alerted me that the connection to the rsession
> > could not be established, so I aborted the entire process altogether. I
> > think the problem here may be the size of the dataset and perhaps the
> > limitations of my laptop specs.
> >
>
> On planar data, there is no good reason for this, as each observation is
> treated separately, finding and sorting distances, and choosing those
> under the threshold. It will undoubtedly slow if there are more than a few
> neighbours within the threshold, but I already covered the inadvisability
> of defining neighbours in that way.
>
> Using an rtree might help, but you get hit badly if there are many
> neighbours within the threshold you have chosen anyway.
>
> On most 8GB hardware and modern OS, you do not have more than 3-4GB for
> work. So something was swapping on your laptop.
>
> > Do you have any advice on how I can go about making a neighbours list
> with
> > dnearneigh for 227,973 observations in a successful and efficient way?
> > Also, would you foresee any problems in the next steps, especially when I
> > will be using the neighbourhood listw object as an input in fitting and
> > predicting using the spatial lag/error models? (see code below)
> >
> > model <-  spatialreg::lagsarlm(rest_formula, data=train, train_listw)
> > model_pred <- spatialreg::predict.sarlm(model, test, all_listw)
> >
>
> Why would using a spatial lag model make sense? Why are you suggesting
> this model, do you have a behavioural for why only the spatially lagged
> response should be included?
>
> Why do you think that this is sensible? You are predicting 1000 times for
> each observation - this is not what the prediction methods are written
> for. Most involve inverting an nxn inverse matrix - did you refer to
> Goulard et al. (2017) to get a good understanding of the underlying
> methods?
>
> > I think the predicting part may take some time, since my test set
> consists
> > of 227,973 - 214 observations = 227,759 observations.
> >
> > Here are some solutions that I have thought of:
> >
> > 1. Interpolate the test set point data of 227,759 observations over a
> more
> > manageable spatial pixel dataframe with cell size of perhaps 10,000m by
> > 10,000m which would give me around 4900 points. So instead of 227,759
> > observations, I can make the listw object based on just 4900 + 214
> training
> > points and predict just on 4900 observations.
>
> But what are you trying to do? Are the observations output areas? House
> sales? If you are not filling in missing areal units (the Goulard et al.
> case), couldn't you simply use geostatistical methods which seem to match
> your support better, and can be fitted and can predict using a local
> neighbourhood? While you are doing that, you could switch to INLA with
> SPDE, which interposes a mesh like the one you suggest. But in that case,
> beware of the mesh choice issue in:
>
> https://doi.org/10.1080/03610926.2018.1536209
>
> >
> > 2. Get hold of better performance machines through cloud computing such
> as
> > AWS EC2 services and try running the commands and models there.
> >
>
> What you need are methods, not wasted money on hardware as a service.
>
> > 3. Parallel computing using the parallel package from r (although I am
> not
> > sure whether dnearneigh can be parallelised).
> >
>
> This could easily be implemented if it was really needed, which I don't
> think it is; better methods understanding lets one do more with less.
>
> > I believe option 1 would be the most manageable but I am not sure how and
> > by how much this would affect the accuracy of the predictions as
> > interpolating the dataset would be akin to introducing more estimations
> in
> > the prediction. However, I am also grappling with the trade-off between
> > accuracy and computation time. Hence, if options 2 and 3 can offer a
> > reasonable computation time (1-2 hours) then I would forgo option 1.
> >
> > What do you think? Is it possible to make a neighbourhood listw object
> out
> > of 227,973 observations efficiently?
>
> Yes, but only if the numbers of neighbours are very small. Look in Bivand
> et al. (2013) to see the use of some fairly large n, but only with few
> neighbours for each observation. You seem to be getting average neighbour
> counts in the thousands, which makes no sense.
>
> >
> > Thank you for reading to the end! Apologies for writing a lengthy one,
> just
> > wanted to fully describe what I am facing, I hope I didn't miss out
> > anything crucial.
> >
>
> Long is OK, but there is no motivation here for why you want to make 200K
> predictions from 200 observations with point support (?) using weights
> matrices.
>
> Hope this clarifies,
>
> Roger
>
> > Thank you so much once again!
> >
> > jiawen
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Jul  1 20:12:03 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 1 Jul 2019 20:12:03 +0200
Subject: [R-sig-Geo] Running huge dataset with dnearneigh
In-Reply-To: <CAHz1c-WQCzcC6zZx8mcgLBw=Vn26jqg684K-tCYHUBfUjbYanw@mail.gmail.com>
References: <CAHz1c-V8DcT4UNqiw38pLvJrDJgFjGu+rq=a1PLhznxsTLdAGw@mail.gmail.com>
 <alpine.LFD.2.21.1906301721550.17653@reclus.nhh.no>
 <CAHz1c-WQCzcC6zZx8mcgLBw=Vn26jqg684K-tCYHUBfUjbYanw@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1907011939590.8431@reclus.nhh.no>

On Mon, 1 Jul 2019, Jiawen Ng wrote:

> Dear Roger,
>
> Thank you so much for your detailed response and pointing out potential
> pitfalls! It has prompted me to re-evalutate my approach.
>
> Here is the context: I have some stores' sales data (this is my training
> set of 214 points), I would like to find out where best to set up new
> stores in UK. I am using a geodemographics approach to do this: Perform a
> regression of sales against census data, then predict sales on UK output
> areas (by centroids) and finally identify new areas with
> location-allocation models. As the stores are points, this has led me to
> define UK output areas by its population-weighted centroids, thus resulting
> in the prediction by points rather than by areas. Tests (like moran's I and
> lagrange multiplier) for spatial relationships among the points in my
> training set were significant hence this has led me to implement some
> spatial models (specifically spatial lag, error and durbin models) to
> account for the spatial relationships in the data.

I'm afraid that my retail geography is not very up to date, but also that 
your approach is most unlikely to yield constructive results.

Most retail stores are organised in large chains, so optimise costs 
between wholesale and retail. Independent retail stores depend crucially 
on access to wholesale stores, so anyway cannot locate without regard to 
supply costs. Some service activities without wholesale dependencies are 
less tied.

Most chains certainly behave strategically with regard to each other, 
sometimes locating toe-to-toe to challenge a competing chain 
(Carrefour/Tesco or their local shop variants), sometimes avoiding nearby 
competing chain locations to establish a local monopoly (think Hotelling).

Population density doesn't express demand, especially unmet demand well at 
all. Think food deserts - maybe plenty of people but little disposable 
income. Look at the food desert literature, or the US food stamp 
literature.

Finally (all bad news) retail is not only challenged by location shifting 
from high streets to malls, but critically by online shopping, which 
shifts the cost structures one the buyer is engaged at a proposed price to 
logistics, to complete the order at the highest margin including returns. 
That only marginally relates to population density.

So you'd need more data than you have, a model that explicitly handles 
competition between chains as well as market gaps, and some way of 
handling online leakage to move forward.

If population density was a proxy for accessibility (most often it isn't), 
it might look like the beginnings of a model, but most often we don't know 
what bid-rent surfaces look like, and then, most often different 
activities sort differently across those surfaces.

>
> I am quite unsettled and unclear as to which neighbourhood definition to go
> for actually. I thought of IDW at first as I thought this would summarise
> each point's relationship with their neighbours very precisely thus making
> the predictions more accurate. Upon your advice (don't use IDW or other
> general weights for predictions), I decided not to use IDW, and changed it
> to dnearneigh instead (although now I am questioning myself on the
> definition of what is meant by general weights. Perhaps I am understanding
> the definition of general weights wrong, if dnearneigh is still considered
> to be a 'general weights' method) Why is the use of IDW not advisable
> however? Is it due to computational reasons? Also, why would having
> thousands of neighbours be making no sense? Apologies for asking so many
> questions, I'd just like to really understand the concepts!
>

The model underlying spatial regressions using neighbours tapers 
dependency as the pairwise elements of (I - \rho W)^{-1} (conditional) and 
[(I - \rho W) (I - \rho W')]^{-1} (see Wall 2004). These are NxN dense 
matrices. (I - \rho W) is typically sparse, and under certain conditions 
leads to (I - \rho W)^{-1} = \sum_{i=0}^{\inf} \rho^i W^i, the sum of a 
power series in \rho and W. \rho is typically upward bounded < 1, so 
\rho^i declines as i increases. This dampens \rho^i W^i, so that i 
influences j less and less with increasing i. So in the general case IDW 
is simply replicating what simple contiguity gives you anyway. So the 
sparser W is (within reason), the better. Unless you really know that the 
physics, chemistry or biology of your system give you a known systematic 
relationship like IDW, you may as well stay with contiguity.

However, this isn't any use in solving a retail location problem at all.

> I believe that both the train and test set has varying intensities. I was
> weighing the different neighbourhood methods: dnearneigh, knearneigh, using
> IDW etc. and I felt like each method would have its disadvantages -- its
> difficult to pinpoint which neighbourhood definition would be best. If one
> were to go for knearneigh for example, results may not be fair due to the
> inhomogeneity of the points -- for instance, point A's nearest neighbours
> may be within a few hundreds of kilometres while point B's nearest
> neighbours may be in the thousands. I feel like the choice of any
> neighbourhood definition can be highly debateable... What do you think?
>

When in doubt use contiguity for polygons and similar graph based methods 
for points. Try to keep the graphs planar (as few intersecting edges as 
possible - rule of thumb).


> After analysing my problem again, I think that predicting by output areas
> (points) would be best for my case as I would have to make use of the
> population data after building the model. Interpolating census data of the
> output area (points) would cause me to lose that information.
>

Baseline, this is not going anywhere constructive, and simply approaching 
retail location in this way is unhelpful - there is far too little 
information in your model.

If you really must, first find a fully configured retail model with the 
complete data set needed to replicate the results achieved, and use that 
to benchmark how far your approach succeeds in reaching a similar result 
for that restricted area. I think that you'll find that the retail model 
is much more successful, but if not, there is less structure in 
contemporary retail than I though.

Best wishes,

Roger

> Thank you for the comments and the advice so far,  I would greatly welcome
> and appreciate additional feedback!
>
> Thank you so much once again!
>
> Jiawen
>
>
>
>
>
>
>
>
> On Sun, 30 Jun 2019 at 16:57, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Sat, 29 Jun 2019, Jiawen Ng wrote:
>>
>>> Dear Roger,
>>
>> Postings go to the whole list ...
>>
>>>
>>> How can we deal with a huge dataset when using dnearneigh?
>>>
>>
>> First, why distance neighbours? What is the support of the data, point or
>> polygon? If polygon, contiguity neighbours are preferred. If not, and the
>> intensity of observations is similar across the whole area, distance may
>> be justified, but if the intensity varies, some observations will have
>> very many neighbours. In that case, unless you have a clear ecological or
>> environmental reason for knowing that a known distance threshold binds, it
>> is not a good choice.
>>
>>> Here is my code:
>>>
>>> d <- dnearneigh(spdf,0, 22000)
>>> all_listw <- nb2listw(d, style = "W")
>>>
>>> where the spdf object is in the british national grid CRS:
>>> +init=epsg:27700, with 227,973 observations/points. The distance of
>> 22,000
>>> was decided by a training set that had 214 observations and the spdf
>> object
>>> contains both the training set and the testing set.
>>>
>>
>> This is questionable. You train on 214 observations - do their areal
>> intensity match those of the whole data set? If chosen at random, you run
>> into the spatial sampling problems discussed in:
>>
>>
>> https://www.sciencedirect.com/science/article/pii/S0304380019302145?dgcid=author
>>
>> Are 214 observations for training representative of 227,973 prediction
>> sites? Do you only have observations on the response for 214, and an
>> unobserved response otherwise? What are the data, what are you trying to
>> do and why? This is not a sensible setting for models using weights
>> matrices for prediction (I think), because we do not have estimates of the
>> prediction error in general.
>>
>>> I am using a Mac, with a processor of 2.3 GHz Intel Core i5 and 8 GB
>>> memory. My laptop showed that when dnearneigh command was run on all
>>> observations, around 6.9 out of 8GB was used by the rsession and that the
>>> %CPU used by the rsession was stated to be around 98%, although another
>>> indicator showed that my computer was around 60% idle. After running the
>>> command for a day, rstudio alerted me that the connection to the rsession
>>> could not be established, so I aborted the entire process altogether. I
>>> think the problem here may be the size of the dataset and perhaps the
>>> limitations of my laptop specs.
>>>
>>
>> On planar data, there is no good reason for this, as each observation is
>> treated separately, finding and sorting distances, and choosing those
>> under the threshold. It will undoubtedly slow if there are more than a few
>> neighbours within the threshold, but I already covered the inadvisability
>> of defining neighbours in that way.
>>
>> Using an rtree might help, but you get hit badly if there are many
>> neighbours within the threshold you have chosen anyway.
>>
>> On most 8GB hardware and modern OS, you do not have more than 3-4GB for
>> work. So something was swapping on your laptop.
>>
>>> Do you have any advice on how I can go about making a neighbours list
>> with
>>> dnearneigh for 227,973 observations in a successful and efficient way?
>>> Also, would you foresee any problems in the next steps, especially when I
>>> will be using the neighbourhood listw object as an input in fitting and
>>> predicting using the spatial lag/error models? (see code below)
>>>
>>> model <-  spatialreg::lagsarlm(rest_formula, data=train, train_listw)
>>> model_pred <- spatialreg::predict.sarlm(model, test, all_listw)
>>>
>>
>> Why would using a spatial lag model make sense? Why are you suggesting
>> this model, do you have a behavioural for why only the spatially lagged
>> response should be included?
>>
>> Why do you think that this is sensible? You are predicting 1000 times for
>> each observation - this is not what the prediction methods are written
>> for. Most involve inverting an nxn inverse matrix - did you refer to
>> Goulard et al. (2017) to get a good understanding of the underlying
>> methods?
>>
>>> I think the predicting part may take some time, since my test set
>> consists
>>> of 227,973 - 214 observations = 227,759 observations.
>>>
>>> Here are some solutions that I have thought of:
>>>
>>> 1. Interpolate the test set point data of 227,759 observations over a
>> more
>>> manageable spatial pixel dataframe with cell size of perhaps 10,000m by
>>> 10,000m which would give me around 4900 points. So instead of 227,759
>>> observations, I can make the listw object based on just 4900 + 214
>> training
>>> points and predict just on 4900 observations.
>>
>> But what are you trying to do? Are the observations output areas? House
>> sales? If you are not filling in missing areal units (the Goulard et al.
>> case), couldn't you simply use geostatistical methods which seem to match
>> your support better, and can be fitted and can predict using a local
>> neighbourhood? While you are doing that, you could switch to INLA with
>> SPDE, which interposes a mesh like the one you suggest. But in that case,
>> beware of the mesh choice issue in:
>>
>> https://doi.org/10.1080/03610926.2018.1536209
>>
>>>
>>> 2. Get hold of better performance machines through cloud computing such
>> as
>>> AWS EC2 services and try running the commands and models there.
>>>
>>
>> What you need are methods, not wasted money on hardware as a service.
>>
>>> 3. Parallel computing using the parallel package from r (although I am
>> not
>>> sure whether dnearneigh can be parallelised).
>>>
>>
>> This could easily be implemented if it was really needed, which I don't
>> think it is; better methods understanding lets one do more with less.
>>
>>> I believe option 1 would be the most manageable but I am not sure how and
>>> by how much this would affect the accuracy of the predictions as
>>> interpolating the dataset would be akin to introducing more estimations
>> in
>>> the prediction. However, I am also grappling with the trade-off between
>>> accuracy and computation time. Hence, if options 2 and 3 can offer a
>>> reasonable computation time (1-2 hours) then I would forgo option 1.
>>>
>>> What do you think? Is it possible to make a neighbourhood listw object
>> out
>>> of 227,973 observations efficiently?
>>
>> Yes, but only if the numbers of neighbours are very small. Look in Bivand
>> et al. (2013) to see the use of some fairly large n, but only with few
>> neighbours for each observation. You seem to be getting average neighbour
>> counts in the thousands, which makes no sense.
>>
>>>
>>> Thank you for reading to the end! Apologies for writing a lengthy one,
>> just
>>> wanted to fully describe what I am facing, I hope I didn't miss out
>>> anything crucial.
>>>
>>
>> Long is OK, but there is no motivation here for why you want to make 200K
>> predictions from 200 observations with point support (?) using weights
>> matrices.
>>
>> Hope this clarifies,
>>
>> Roger
>>
>>> Thank you so much once again!
>>>
>>> jiawen
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |ove|y||tt|ed@|@|e@ @end|ng |rom gm@||@com  Tue Jul  2 01:41:08 2019
From: |ove|y||tt|ed@|@|e@ @end|ng |rom gm@||@com (Jiawen Ng)
Date: Tue, 2 Jul 2019 00:41:08 +0100
Subject: [R-sig-Geo] Running huge dataset with dnearneigh
In-Reply-To: <alpine.LFD.2.21.1907011939590.8431@reclus.nhh.no>
References: <CAHz1c-V8DcT4UNqiw38pLvJrDJgFjGu+rq=a1PLhznxsTLdAGw@mail.gmail.com>
 <alpine.LFD.2.21.1906301721550.17653@reclus.nhh.no>
 <CAHz1c-WQCzcC6zZx8mcgLBw=Vn26jqg684K-tCYHUBfUjbYanw@mail.gmail.com>
 <alpine.LFD.2.21.1907011939590.8431@reclus.nhh.no>
Message-ID: <CAHz1c-X1TSEO31dqScCBOKxR6qNWJL9opDfX1fdg1_bHeZ5avw@mail.gmail.com>

Dear Roger,

Thanks for your reply and explanation!

I am just exploring the aspect of geodemographics in store locations. There
are many factors that can be considered, as you have highlighted!

Thank you so much for taking the time to write back to me! I will study and
consider your advice! Thank you!

Jiawen

On Mon, 1 Jul 2019 at 19:12, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Mon, 1 Jul 2019, Jiawen Ng wrote:
>
> > Dear Roger,
> >
> > Thank you so much for your detailed response and pointing out potential
> > pitfalls! It has prompted me to re-evalutate my approach.
> >
> > Here is the context: I have some stores' sales data (this is my training
> > set of 214 points), I would like to find out where best to set up new
> > stores in UK. I am using a geodemographics approach to do this: Perform a
> > regression of sales against census data, then predict sales on UK output
> > areas (by centroids) and finally identify new areas with
> > location-allocation models. As the stores are points, this has led me to
> > define UK output areas by its population-weighted centroids, thus
> resulting
> > in the prediction by points rather than by areas. Tests (like moran's I
> and
> > lagrange multiplier) for spatial relationships among the points in my
> > training set were significant hence this has led me to implement some
> > spatial models (specifically spatial lag, error and durbin models) to
> > account for the spatial relationships in the data.
>
> I'm afraid that my retail geography is not very up to date, but also that
> your approach is most unlikely to yield constructive results.
>
> Most retail stores are organised in large chains, so optimise costs
> between wholesale and retail. Independent retail stores depend crucially
> on access to wholesale stores, so anyway cannot locate without regard to
> supply costs. Some service activities without wholesale dependencies are
> less tied.
>
> Most chains certainly behave strategically with regard to each other,
> sometimes locating toe-to-toe to challenge a competing chain
> (Carrefour/Tesco or their local shop variants), sometimes avoiding nearby
> competing chain locations to establish a local monopoly (think Hotelling).
>
> Population density doesn't express demand, especially unmet demand well at
> all. Think food deserts - maybe plenty of people but little disposable
> income. Look at the food desert literature, or the US food stamp
> literature.
>
> Finally (all bad news) retail is not only challenged by location shifting
> from high streets to malls, but critically by online shopping, which
> shifts the cost structures one the buyer is engaged at a proposed price to
> logistics, to complete the order at the highest margin including returns.
> That only marginally relates to population density.
>
> So you'd need more data than you have, a model that explicitly handles
> competition between chains as well as market gaps, and some way of
> handling online leakage to move forward.
>
> If population density was a proxy for accessibility (most often it isn't),
> it might look like the beginnings of a model, but most often we don't know
> what bid-rent surfaces look like, and then, most often different
> activities sort differently across those surfaces.
>
> >
> > I am quite unsettled and unclear as to which neighbourhood definition to
> go
> > for actually. I thought of IDW at first as I thought this would summarise
> > each point's relationship with their neighbours very precisely thus
> making
> > the predictions more accurate. Upon your advice (don't use IDW or other
> > general weights for predictions), I decided not to use IDW, and changed
> it
> > to dnearneigh instead (although now I am questioning myself on the
> > definition of what is meant by general weights. Perhaps I am
> understanding
> > the definition of general weights wrong, if dnearneigh is still
> considered
> > to be a 'general weights' method) Why is the use of IDW not advisable
> > however? Is it due to computational reasons? Also, why would having
> > thousands of neighbours be making no sense? Apologies for asking so many
> > questions, I'd just like to really understand the concepts!
> >
>
> The model underlying spatial regressions using neighbours tapers
> dependency as the pairwise elements of (I - \rho W)^{-1} (conditional) and
> [(I - \rho W) (I - \rho W')]^{-1} (see Wall 2004). These are NxN dense
> matrices. (I - \rho W) is typically sparse, and under certain conditions
> leads to (I - \rho W)^{-1} = \sum_{i=0}^{\inf} \rho^i W^i, the sum of a
> power series in \rho and W. \rho is typically upward bounded < 1, so
> \rho^i declines as i increases. This dampens \rho^i W^i, so that i
> influences j less and less with increasing i. So in the general case IDW
> is simply replicating what simple contiguity gives you anyway. So the
> sparser W is (within reason), the better. Unless you really know that the
> physics, chemistry or biology of your system give you a known systematic
> relationship like IDW, you may as well stay with contiguity.
>
> However, this isn't any use in solving a retail location problem at all.
>
> > I believe that both the train and test set has varying intensities. I was
> > weighing the different neighbourhood methods: dnearneigh, knearneigh,
> using
> > IDW etc. and I felt like each method would have its disadvantages -- its
> > difficult to pinpoint which neighbourhood definition would be best. If
> one
> > were to go for knearneigh for example, results may not be fair due to the
> > inhomogeneity of the points -- for instance, point A's nearest neighbours
> > may be within a few hundreds of kilometres while point B's nearest
> > neighbours may be in the thousands. I feel like the choice of any
> > neighbourhood definition can be highly debateable... What do you think?
> >
>
> When in doubt use contiguity for polygons and similar graph based methods
> for points. Try to keep the graphs planar (as few intersecting edges as
> possible - rule of thumb).
>
>
> > After analysing my problem again, I think that predicting by output areas
> > (points) would be best for my case as I would have to make use of the
> > population data after building the model. Interpolating census data of
> the
> > output area (points) would cause me to lose that information.
> >
>
> Baseline, this is not going anywhere constructive, and simply approaching
> retail location in this way is unhelpful - there is far too little
> information in your model.
>
> If you really must, first find a fully configured retail model with the
> complete data set needed to replicate the results achieved, and use that
> to benchmark how far your approach succeeds in reaching a similar result
> for that restricted area. I think that you'll find that the retail model
> is much more successful, but if not, there is less structure in
> contemporary retail than I though.
>
> Best wishes,
>
> Roger
>
> > Thank you for the comments and the advice so far,  I would greatly
> welcome
> > and appreciate additional feedback!
> >
> > Thank you so much once again!
> >
> > Jiawen
> >
> >
> >
> >
> >
> >
> >
> >
> > On Sun, 30 Jun 2019 at 16:57, Roger Bivand <Roger.Bivand at nhh.no> wrote:
> >
> >> On Sat, 29 Jun 2019, Jiawen Ng wrote:
> >>
> >>> Dear Roger,
> >>
> >> Postings go to the whole list ...
> >>
> >>>
> >>> How can we deal with a huge dataset when using dnearneigh?
> >>>
> >>
> >> First, why distance neighbours? What is the support of the data, point
> or
> >> polygon? If polygon, contiguity neighbours are preferred. If not, and
> the
> >> intensity of observations is similar across the whole area, distance may
> >> be justified, but if the intensity varies, some observations will have
> >> very many neighbours. In that case, unless you have a clear ecological
> or
> >> environmental reason for knowing that a known distance threshold binds,
> it
> >> is not a good choice.
> >>
> >>> Here is my code:
> >>>
> >>> d <- dnearneigh(spdf,0, 22000)
> >>> all_listw <- nb2listw(d, style = "W")
> >>>
> >>> where the spdf object is in the british national grid CRS:
> >>> +init=epsg:27700, with 227,973 observations/points. The distance of
> >> 22,000
> >>> was decided by a training set that had 214 observations and the spdf
> >> object
> >>> contains both the training set and the testing set.
> >>>
> >>
> >> This is questionable. You train on 214 observations - do their areal
> >> intensity match those of the whole data set? If chosen at random, you
> run
> >> into the spatial sampling problems discussed in:
> >>
> >>
> >>
> https://www.sciencedirect.com/science/article/pii/S0304380019302145?dgcid=author
> >>
> >> Are 214 observations for training representative of 227,973 prediction
> >> sites? Do you only have observations on the response for 214, and an
> >> unobserved response otherwise? What are the data, what are you trying to
> >> do and why? This is not a sensible setting for models using weights
> >> matrices for prediction (I think), because we do not have estimates of
> the
> >> prediction error in general.
> >>
> >>> I am using a Mac, with a processor of 2.3 GHz Intel Core i5 and 8 GB
> >>> memory. My laptop showed that when dnearneigh command was run on all
> >>> observations, around 6.9 out of 8GB was used by the rsession and that
> the
> >>> %CPU used by the rsession was stated to be around 98%, although another
> >>> indicator showed that my computer was around 60% idle. After running
> the
> >>> command for a day, rstudio alerted me that the connection to the
> rsession
> >>> could not be established, so I aborted the entire process altogether. I
> >>> think the problem here may be the size of the dataset and perhaps the
> >>> limitations of my laptop specs.
> >>>
> >>
> >> On planar data, there is no good reason for this, as each observation is
> >> treated separately, finding and sorting distances, and choosing those
> >> under the threshold. It will undoubtedly slow if there are more than a
> few
> >> neighbours within the threshold, but I already covered the
> inadvisability
> >> of defining neighbours in that way.
> >>
> >> Using an rtree might help, but you get hit badly if there are many
> >> neighbours within the threshold you have chosen anyway.
> >>
> >> On most 8GB hardware and modern OS, you do not have more than 3-4GB for
> >> work. So something was swapping on your laptop.
> >>
> >>> Do you have any advice on how I can go about making a neighbours list
> >> with
> >>> dnearneigh for 227,973 observations in a successful and efficient way?
> >>> Also, would you foresee any problems in the next steps, especially
> when I
> >>> will be using the neighbourhood listw object as an input in fitting and
> >>> predicting using the spatial lag/error models? (see code below)
> >>>
> >>> model <-  spatialreg::lagsarlm(rest_formula, data=train, train_listw)
> >>> model_pred <- spatialreg::predict.sarlm(model, test, all_listw)
> >>>
> >>
> >> Why would using a spatial lag model make sense? Why are you suggesting
> >> this model, do you have a behavioural for why only the spatially lagged
> >> response should be included?
> >>
> >> Why do you think that this is sensible? You are predicting 1000 times
> for
> >> each observation - this is not what the prediction methods are written
> >> for. Most involve inverting an nxn inverse matrix - did you refer to
> >> Goulard et al. (2017) to get a good understanding of the underlying
> >> methods?
> >>
> >>> I think the predicting part may take some time, since my test set
> >> consists
> >>> of 227,973 - 214 observations = 227,759 observations.
> >>>
> >>> Here are some solutions that I have thought of:
> >>>
> >>> 1. Interpolate the test set point data of 227,759 observations over a
> >> more
> >>> manageable spatial pixel dataframe with cell size of perhaps 10,000m by
> >>> 10,000m which would give me around 4900 points. So instead of 227,759
> >>> observations, I can make the listw object based on just 4900 + 214
> >> training
> >>> points and predict just on 4900 observations.
> >>
> >> But what are you trying to do? Are the observations output areas? House
> >> sales? If you are not filling in missing areal units (the Goulard et al.
> >> case), couldn't you simply use geostatistical methods which seem to
> match
> >> your support better, and can be fitted and can predict using a local
> >> neighbourhood? While you are doing that, you could switch to INLA with
> >> SPDE, which interposes a mesh like the one you suggest. But in that
> case,
> >> beware of the mesh choice issue in:
> >>
> >> https://doi.org/10.1080/03610926.2018.1536209
> >>
> >>>
> >>> 2. Get hold of better performance machines through cloud computing such
> >> as
> >>> AWS EC2 services and try running the commands and models there.
> >>>
> >>
> >> What you need are methods, not wasted money on hardware as a service.
> >>
> >>> 3. Parallel computing using the parallel package from r (although I am
> >> not
> >>> sure whether dnearneigh can be parallelised).
> >>>
> >>
> >> This could easily be implemented if it was really needed, which I don't
> >> think it is; better methods understanding lets one do more with less.
> >>
> >>> I believe option 1 would be the most manageable but I am not sure how
> and
> >>> by how much this would affect the accuracy of the predictions as
> >>> interpolating the dataset would be akin to introducing more estimations
> >> in
> >>> the prediction. However, I am also grappling with the trade-off between
> >>> accuracy and computation time. Hence, if options 2 and 3 can offer a
> >>> reasonable computation time (1-2 hours) then I would forgo option 1.
> >>>
> >>> What do you think? Is it possible to make a neighbourhood listw object
> >> out
> >>> of 227,973 observations efficiently?
> >>
> >> Yes, but only if the numbers of neighbours are very small. Look in
> Bivand
> >> et al. (2013) to see the use of some fairly large n, but only with few
> >> neighbours for each observation. You seem to be getting average
> neighbour
> >> counts in the thousands, which makes no sense.
> >>
> >>>
> >>> Thank you for reading to the end! Apologies for writing a lengthy one,
> >> just
> >>> wanted to fully describe what I am facing, I hope I didn't miss out
> >>> anything crucial.
> >>>
> >>
> >> Long is OK, but there is no motivation here for why you want to make
> 200K
> >> predictions from 200 observations with point support (?) using weights
> >> matrices.
> >>
> >> Hope this clarifies,
> >>
> >> Roger
> >>
> >>> Thank you so much once again!
> >>>
> >>> jiawen
> >>>
> >>>       [[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-Geo mailing list
> >>> R-sig-Geo at r-project.org
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>>
> >>
> >> --
> >> Roger Bivand
> >> Department of Economics, Norwegian School of Economics,
> >> Helleveien 30, N-5045 Bergen, Norway.
> >> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> >> https://orcid.org/0000-0003-2392-6140
> >> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Jul  2 09:48:45 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 2 Jul 2019 09:48:45 +0200
Subject: [R-sig-Geo] Running huge dataset with dnearneigh
In-Reply-To: <CAHz1c-X1TSEO31dqScCBOKxR6qNWJL9opDfX1fdg1_bHeZ5avw@mail.gmail.com>
References: <CAHz1c-V8DcT4UNqiw38pLvJrDJgFjGu+rq=a1PLhznxsTLdAGw@mail.gmail.com>
 <alpine.LFD.2.21.1906301721550.17653@reclus.nhh.no>
 <CAHz1c-WQCzcC6zZx8mcgLBw=Vn26jqg684K-tCYHUBfUjbYanw@mail.gmail.com>
 <alpine.LFD.2.21.1907011939590.8431@reclus.nhh.no>
 <CAHz1c-X1TSEO31dqScCBOKxR6qNWJL9opDfX1fdg1_bHeZ5avw@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1907020936080.16081@reclus.nhh.no>

On Tue, 2 Jul 2019, Jiawen Ng wrote:

> Dear Roger,
>
> Thanks for your reply and explanation!
>
> I am just exploring the aspect of geodemographics in store locations. There
> are many factors that can be considered, as you have highlighted!

OK, so I suggest choosing a modest sized case until a selection of working 
models emerges. Once you reach that stage, you can return to scaling up. I 
think you need much more data on the customer behaviour around the stores 
you use to train your models, particularly customer flows associated with 
actual purchases. Firms used to do this through loyalty programmes and 
cards, but this data is not open, so you'd need proxies which say city 
bikes will not give you.

Geodemographics (used for direct mailing as a marketing tool) have largely 
been eclipsed by profiling in social media with the exception of segments 
without social media profiles. This is because postcode or OA profiling is 
often too noisy and so is expensive because there are many false hits. 
Retail is interesting but very multi-faceted, but some personal services 
are more closely related to population as they are hard to digitise.

Hope this helps,

Roger

>
> Thank you so much for taking the time to write back to me! I will study and
> consider your advice! Thank you!
>
> Jiawen
>
> On Mon, 1 Jul 2019 at 19:12, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Mon, 1 Jul 2019, Jiawen Ng wrote:
>>
>>> Dear Roger,
>>>
>>> Thank you so much for your detailed response and pointing out potential
>>> pitfalls! It has prompted me to re-evalutate my approach.
>>>
>>> Here is the context: I have some stores' sales data (this is my training
>>> set of 214 points), I would like to find out where best to set up new
>>> stores in UK. I am using a geodemographics approach to do this: Perform a
>>> regression of sales against census data, then predict sales on UK output
>>> areas (by centroids) and finally identify new areas with
>>> location-allocation models. As the stores are points, this has led me to
>>> define UK output areas by its population-weighted centroids, thus
>> resulting
>>> in the prediction by points rather than by areas. Tests (like moran's I
>> and
>>> lagrange multiplier) for spatial relationships among the points in my
>>> training set were significant hence this has led me to implement some
>>> spatial models (specifically spatial lag, error and durbin models) to
>>> account for the spatial relationships in the data.
>>
>> I'm afraid that my retail geography is not very up to date, but also that
>> your approach is most unlikely to yield constructive results.
>>
>> Most retail stores are organised in large chains, so optimise costs
>> between wholesale and retail. Independent retail stores depend crucially
>> on access to wholesale stores, so anyway cannot locate without regard to
>> supply costs. Some service activities without wholesale dependencies are
>> less tied.
>>
>> Most chains certainly behave strategically with regard to each other,
>> sometimes locating toe-to-toe to challenge a competing chain
>> (Carrefour/Tesco or their local shop variants), sometimes avoiding nearby
>> competing chain locations to establish a local monopoly (think Hotelling).
>>
>> Population density doesn't express demand, especially unmet demand well at
>> all. Think food deserts - maybe plenty of people but little disposable
>> income. Look at the food desert literature, or the US food stamp
>> literature.
>>
>> Finally (all bad news) retail is not only challenged by location shifting
>> from high streets to malls, but critically by online shopping, which
>> shifts the cost structures one the buyer is engaged at a proposed price to
>> logistics, to complete the order at the highest margin including returns.
>> That only marginally relates to population density.
>>
>> So you'd need more data than you have, a model that explicitly handles
>> competition between chains as well as market gaps, and some way of
>> handling online leakage to move forward.
>>
>> If population density was a proxy for accessibility (most often it isn't),
>> it might look like the beginnings of a model, but most often we don't know
>> what bid-rent surfaces look like, and then, most often different
>> activities sort differently across those surfaces.
>>
>>>
>>> I am quite unsettled and unclear as to which neighbourhood definition to
>> go
>>> for actually. I thought of IDW at first as I thought this would summarise
>>> each point's relationship with their neighbours very precisely thus
>> making
>>> the predictions more accurate. Upon your advice (don't use IDW or other
>>> general weights for predictions), I decided not to use IDW, and changed
>> it
>>> to dnearneigh instead (although now I am questioning myself on the
>>> definition of what is meant by general weights. Perhaps I am
>> understanding
>>> the definition of general weights wrong, if dnearneigh is still
>> considered
>>> to be a 'general weights' method) Why is the use of IDW not advisable
>>> however? Is it due to computational reasons? Also, why would having
>>> thousands of neighbours be making no sense? Apologies for asking so many
>>> questions, I'd just like to really understand the concepts!
>>>
>>
>> The model underlying spatial regressions using neighbours tapers
>> dependency as the pairwise elements of (I - \rho W)^{-1} (conditional) and
>> [(I - \rho W) (I - \rho W')]^{-1} (see Wall 2004). These are NxN dense
>> matrices. (I - \rho W) is typically sparse, and under certain conditions
>> leads to (I - \rho W)^{-1} = \sum_{i=0}^{\inf} \rho^i W^i, the sum of a
>> power series in \rho and W. \rho is typically upward bounded < 1, so
>> \rho^i declines as i increases. This dampens \rho^i W^i, so that i
>> influences j less and less with increasing i. So in the general case IDW
>> is simply replicating what simple contiguity gives you anyway. So the
>> sparser W is (within reason), the better. Unless you really know that the
>> physics, chemistry or biology of your system give you a known systematic
>> relationship like IDW, you may as well stay with contiguity.
>>
>> However, this isn't any use in solving a retail location problem at all.
>>
>>> I believe that both the train and test set has varying intensities. I was
>>> weighing the different neighbourhood methods: dnearneigh, knearneigh,
>> using
>>> IDW etc. and I felt like each method would have its disadvantages -- its
>>> difficult to pinpoint which neighbourhood definition would be best. If
>> one
>>> were to go for knearneigh for example, results may not be fair due to the
>>> inhomogeneity of the points -- for instance, point A's nearest neighbours
>>> may be within a few hundreds of kilometres while point B's nearest
>>> neighbours may be in the thousands. I feel like the choice of any
>>> neighbourhood definition can be highly debateable... What do you think?
>>>
>>
>> When in doubt use contiguity for polygons and similar graph based methods
>> for points. Try to keep the graphs planar (as few intersecting edges as
>> possible - rule of thumb).
>>
>>
>>> After analysing my problem again, I think that predicting by output areas
>>> (points) would be best for my case as I would have to make use of the
>>> population data after building the model. Interpolating census data of
>> the
>>> output area (points) would cause me to lose that information.
>>>
>>
>> Baseline, this is not going anywhere constructive, and simply approaching
>> retail location in this way is unhelpful - there is far too little
>> information in your model.
>>
>> If you really must, first find a fully configured retail model with the
>> complete data set needed to replicate the results achieved, and use that
>> to benchmark how far your approach succeeds in reaching a similar result
>> for that restricted area. I think that you'll find that the retail model
>> is much more successful, but if not, there is less structure in
>> contemporary retail than I though.
>>
>> Best wishes,
>>
>> Roger
>>
>>> Thank you for the comments and the advice so far,  I would greatly
>> welcome
>>> and appreciate additional feedback!
>>>
>>> Thank you so much once again!
>>>
>>> Jiawen
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> On Sun, 30 Jun 2019 at 16:57, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>>
>>>> On Sat, 29 Jun 2019, Jiawen Ng wrote:
>>>>
>>>>> Dear Roger,
>>>>
>>>> Postings go to the whole list ...
>>>>
>>>>>
>>>>> How can we deal with a huge dataset when using dnearneigh?
>>>>>
>>>>
>>>> First, why distance neighbours? What is the support of the data, point
>> or
>>>> polygon? If polygon, contiguity neighbours are preferred. If not, and
>> the
>>>> intensity of observations is similar across the whole area, distance may
>>>> be justified, but if the intensity varies, some observations will have
>>>> very many neighbours. In that case, unless you have a clear ecological
>> or
>>>> environmental reason for knowing that a known distance threshold binds,
>> it
>>>> is not a good choice.
>>>>
>>>>> Here is my code:
>>>>>
>>>>> d <- dnearneigh(spdf,0, 22000)
>>>>> all_listw <- nb2listw(d, style = "W")
>>>>>
>>>>> where the spdf object is in the british national grid CRS:
>>>>> +init=epsg:27700, with 227,973 observations/points. The distance of
>>>> 22,000
>>>>> was decided by a training set that had 214 observations and the spdf
>>>> object
>>>>> contains both the training set and the testing set.
>>>>>
>>>>
>>>> This is questionable. You train on 214 observations - do their areal
>>>> intensity match those of the whole data set? If chosen at random, you
>> run
>>>> into the spatial sampling problems discussed in:
>>>>
>>>>
>>>>
>> https://www.sciencedirect.com/science/article/pii/S0304380019302145?dgcid=author
>>>>
>>>> Are 214 observations for training representative of 227,973 prediction
>>>> sites? Do you only have observations on the response for 214, and an
>>>> unobserved response otherwise? What are the data, what are you trying to
>>>> do and why? This is not a sensible setting for models using weights
>>>> matrices for prediction (I think), because we do not have estimates of
>> the
>>>> prediction error in general.
>>>>
>>>>> I am using a Mac, with a processor of 2.3 GHz Intel Core i5 and 8 GB
>>>>> memory. My laptop showed that when dnearneigh command was run on all
>>>>> observations, around 6.9 out of 8GB was used by the rsession and that
>> the
>>>>> %CPU used by the rsession was stated to be around 98%, although another
>>>>> indicator showed that my computer was around 60% idle. After running
>> the
>>>>> command for a day, rstudio alerted me that the connection to the
>> rsession
>>>>> could not be established, so I aborted the entire process altogether. I
>>>>> think the problem here may be the size of the dataset and perhaps the
>>>>> limitations of my laptop specs.
>>>>>
>>>>
>>>> On planar data, there is no good reason for this, as each observation is
>>>> treated separately, finding and sorting distances, and choosing those
>>>> under the threshold. It will undoubtedly slow if there are more than a
>> few
>>>> neighbours within the threshold, but I already covered the
>> inadvisability
>>>> of defining neighbours in that way.
>>>>
>>>> Using an rtree might help, but you get hit badly if there are many
>>>> neighbours within the threshold you have chosen anyway.
>>>>
>>>> On most 8GB hardware and modern OS, you do not have more than 3-4GB for
>>>> work. So something was swapping on your laptop.
>>>>
>>>>> Do you have any advice on how I can go about making a neighbours list
>>>> with
>>>>> dnearneigh for 227,973 observations in a successful and efficient way?
>>>>> Also, would you foresee any problems in the next steps, especially
>> when I
>>>>> will be using the neighbourhood listw object as an input in fitting and
>>>>> predicting using the spatial lag/error models? (see code below)
>>>>>
>>>>> model <-  spatialreg::lagsarlm(rest_formula, data=train, train_listw)
>>>>> model_pred <- spatialreg::predict.sarlm(model, test, all_listw)
>>>>>
>>>>
>>>> Why would using a spatial lag model make sense? Why are you suggesting
>>>> this model, do you have a behavioural for why only the spatially lagged
>>>> response should be included?
>>>>
>>>> Why do you think that this is sensible? You are predicting 1000 times
>> for
>>>> each observation - this is not what the prediction methods are written
>>>> for. Most involve inverting an nxn inverse matrix - did you refer to
>>>> Goulard et al. (2017) to get a good understanding of the underlying
>>>> methods?
>>>>
>>>>> I think the predicting part may take some time, since my test set
>>>> consists
>>>>> of 227,973 - 214 observations = 227,759 observations.
>>>>>
>>>>> Here are some solutions that I have thought of:
>>>>>
>>>>> 1. Interpolate the test set point data of 227,759 observations over a
>>>> more
>>>>> manageable spatial pixel dataframe with cell size of perhaps 10,000m by
>>>>> 10,000m which would give me around 4900 points. So instead of 227,759
>>>>> observations, I can make the listw object based on just 4900 + 214
>>>> training
>>>>> points and predict just on 4900 observations.
>>>>
>>>> But what are you trying to do? Are the observations output areas? House
>>>> sales? If you are not filling in missing areal units (the Goulard et al.
>>>> case), couldn't you simply use geostatistical methods which seem to
>> match
>>>> your support better, and can be fitted and can predict using a local
>>>> neighbourhood? While you are doing that, you could switch to INLA with
>>>> SPDE, which interposes a mesh like the one you suggest. But in that
>> case,
>>>> beware of the mesh choice issue in:
>>>>
>>>> https://doi.org/10.1080/03610926.2018.1536209
>>>>
>>>>>
>>>>> 2. Get hold of better performance machines through cloud computing such
>>>> as
>>>>> AWS EC2 services and try running the commands and models there.
>>>>>
>>>>
>>>> What you need are methods, not wasted money on hardware as a service.
>>>>
>>>>> 3. Parallel computing using the parallel package from r (although I am
>>>> not
>>>>> sure whether dnearneigh can be parallelised).
>>>>>
>>>>
>>>> This could easily be implemented if it was really needed, which I don't
>>>> think it is; better methods understanding lets one do more with less.
>>>>
>>>>> I believe option 1 would be the most manageable but I am not sure how
>> and
>>>>> by how much this would affect the accuracy of the predictions as
>>>>> interpolating the dataset would be akin to introducing more estimations
>>>> in
>>>>> the prediction. However, I am also grappling with the trade-off between
>>>>> accuracy and computation time. Hence, if options 2 and 3 can offer a
>>>>> reasonable computation time (1-2 hours) then I would forgo option 1.
>>>>>
>>>>> What do you think? Is it possible to make a neighbourhood listw object
>>>> out
>>>>> of 227,973 observations efficiently?
>>>>
>>>> Yes, but only if the numbers of neighbours are very small. Look in
>> Bivand
>>>> et al. (2013) to see the use of some fairly large n, but only with few
>>>> neighbours for each observation. You seem to be getting average
>> neighbour
>>>> counts in the thousands, which makes no sense.
>>>>
>>>>>
>>>>> Thank you for reading to the end! Apologies for writing a lengthy one,
>>>> just
>>>>> wanted to fully describe what I am facing, I hope I didn't miss out
>>>>> anything crucial.
>>>>>
>>>>
>>>> Long is OK, but there is no motivation here for why you want to make
>> 200K
>>>> predictions from 200 observations with point support (?) using weights
>>>> matrices.
>>>>
>>>> Hope this clarifies,
>>>>
>>>> Roger
>>>>
>>>>> Thank you so much once again!
>>>>>
>>>>> jiawen
>>>>>
>>>>>       [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Jul  2 11:20:20 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 2 Jul 2019 11:20:20 +0200
Subject: [R-sig-Geo] Running huge dataset with dnearneigh
In-Reply-To: <alpine.LFD.2.21.1907020936080.16081@reclus.nhh.no>
References: <CAHz1c-V8DcT4UNqiw38pLvJrDJgFjGu+rq=a1PLhznxsTLdAGw@mail.gmail.com>
 <alpine.LFD.2.21.1906301721550.17653@reclus.nhh.no>
 <CAHz1c-WQCzcC6zZx8mcgLBw=Vn26jqg684K-tCYHUBfUjbYanw@mail.gmail.com>
 <alpine.LFD.2.21.1907011939590.8431@reclus.nhh.no>
 <CAHz1c-X1TSEO31dqScCBOKxR6qNWJL9opDfX1fdg1_bHeZ5avw@mail.gmail.com>
 <alpine.LFD.2.21.1907020936080.16081@reclus.nhh.no>
Message-ID: <alpine.LFD.2.21.1907021119120.16081@reclus.nhh.no>

Follow-up: maybe read: https://geocompr.robinlovelace.net/location.html 
for a geomarketing case.

Roger

On Tue, 2 Jul 2019, Roger Bivand wrote:

> On Tue, 2 Jul 2019, Jiawen Ng wrote:
>
>>  Dear Roger,
>>
>>  Thanks for your reply and explanation!
>>
>>  I am just exploring the aspect of geodemographics in store locations.
>>  There
>>  are many factors that can be considered, as you have highlighted!
>
> OK, so I suggest choosing a modest sized case until a selection of working 
> models emerges. Once you reach that stage, you can return to scaling up. I 
> think you need much more data on the customer behaviour around the stores you 
> use to train your models, particularly customer flows associated with actual 
> purchases. Firms used to do this through loyalty programmes and cards, but 
> this data is not open, so you'd need proxies which say city bikes will not 
> give you.
>
> Geodemographics (used for direct mailing as a marketing tool) have largely 
> been eclipsed by profiling in social media with the exception of segments 
> without social media profiles. This is because postcode or OA profiling is 
> often too noisy and so is expensive because there are many false hits. Retail 
> is interesting but very multi-faceted, but some personal services are more 
> closely related to population as they are hard to digitise.
>
> Hope this helps,
>
> Roger
>
>>
>>  Thank you so much for taking the time to write back to me! I will study
>>  and
>>  consider your advice! Thank you!
>>
>>  Jiawen
>>
>>  On Mon, 1 Jul 2019 at 19:12, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>
>>>  On Mon, 1 Jul 2019, Jiawen Ng wrote:
>>>
>>>>  Dear Roger,
>>>>
>>>>  Thank you so much for your detailed response and pointing out potential
>>>>  pitfalls! It has prompted me to re-evalutate my approach.
>>>>
>>>>  Here is the context: I have some stores' sales data (this is my training
>>>>  set of 214 points), I would like to find out where best to set up new
>>>>  stores in UK. I am using a geodemographics approach to do this: Perform
>>>>  a
>>>>  regression of sales against census data, then predict sales on UK output
>>>>  areas (by centroids) and finally identify new areas with
>>>>  location-allocation models. As the stores are points, this has led me to
>>>>  define UK output areas by its population-weighted centroids, thus
>>>  resulting
>>>>  in the prediction by points rather than by areas. Tests (like moran's I
>>>  and
>>>>  lagrange multiplier) for spatial relationships among the points in my
>>>>  training set were significant hence this has led me to implement some
>>>>  spatial models (specifically spatial lag, error and durbin models) to
>>>>  account for the spatial relationships in the data.
>>>
>>>  I'm afraid that my retail geography is not very up to date, but also that
>>>  your approach is most unlikely to yield constructive results.
>>>
>>>  Most retail stores are organised in large chains, so optimise costs
>>>  between wholesale and retail. Independent retail stores depend crucially
>>>  on access to wholesale stores, so anyway cannot locate without regard to
>>>  supply costs. Some service activities without wholesale dependencies are
>>>  less tied.
>>>
>>>  Most chains certainly behave strategically with regard to each other,
>>>  sometimes locating toe-to-toe to challenge a competing chain
>>>  (Carrefour/Tesco or their local shop variants), sometimes avoiding nearby
>>>  competing chain locations to establish a local monopoly (think
>>>  Hotelling).
>>>
>>>  Population density doesn't express demand, especially unmet demand well
>>>  at
>>>  all. Think food deserts - maybe plenty of people but little disposable
>>>  income. Look at the food desert literature, or the US food stamp
>>>  literature.
>>>
>>>  Finally (all bad news) retail is not only challenged by location shifting
>>>  from high streets to malls, but critically by online shopping, which
>>>  shifts the cost structures one the buyer is engaged at a proposed price
>>>  to
>>>  logistics, to complete the order at the highest margin including returns.
>>>  That only marginally relates to population density.
>>>
>>>  So you'd need more data than you have, a model that explicitly handles
>>>  competition between chains as well as market gaps, and some way of
>>>  handling online leakage to move forward.
>>>
>>>  If population density was a proxy for accessibility (most often it
>>>  isn't),
>>>  it might look like the beginnings of a model, but most often we don't
>>>  know
>>>  what bid-rent surfaces look like, and then, most often different
>>>  activities sort differently across those surfaces.
>>> 
>>>>
>>>>  I am quite unsettled and unclear as to which neighbourhood definition to
>>>  go
>>>>  for actually. I thought of IDW at first as I thought this would
>>>>  summarise
>>>>  each point's relationship with their neighbours very precisely thus
>>>  making
>>>>  the predictions more accurate. Upon your advice (don't use IDW or other
>>>>  general weights for predictions), I decided not to use IDW, and changed
>>>  it
>>>>  to dnearneigh instead (although now I am questioning myself on the
>>>>  definition of what is meant by general weights. Perhaps I am
>>>  understanding
>>>>  the definition of general weights wrong, if dnearneigh is still
>>>  considered
>>>>  to be a 'general weights' method) Why is the use of IDW not advisable
>>>>  however? Is it due to computational reasons? Also, why would having
>>>>  thousands of neighbours be making no sense? Apologies for asking so many
>>>>  questions, I'd just like to really understand the concepts!
>>>> 
>>>
>>>  The model underlying spatial regressions using neighbours tapers
>>>  dependency as the pairwise elements of (I - \rho W)^{-1} (conditional)
>>>  and
>>>  [(I - \rho W) (I - \rho W')]^{-1} (see Wall 2004). These are NxN dense
>>>  matrices. (I - \rho W) is typically sparse, and under certain conditions
>>>  leads to (I - \rho W)^{-1} = \sum_{i=0}^{\inf} \rho^i W^i, the sum of a
>>>  power series in \rho and W. \rho is typically upward bounded < 1, so
>>>  \rho^i declines as i increases. This dampens \rho^i W^i, so that i
>>>  influences j less and less with increasing i. So in the general case IDW
>>>  is simply replicating what simple contiguity gives you anyway. So the
>>>  sparser W is (within reason), the better. Unless you really know that the
>>>  physics, chemistry or biology of your system give you a known systematic
>>>  relationship like IDW, you may as well stay with contiguity.
>>>
>>>  However, this isn't any use in solving a retail location problem at all.
>>>
>>>>  I believe that both the train and test set has varying intensities. I
>>>>  was
>>>>  weighing the different neighbourhood methods: dnearneigh, knearneigh,
>>>  using
>>>>  IDW etc. and I felt like each method would have its disadvantages -- its
>>>>  difficult to pinpoint which neighbourhood definition would be best. If
>>>  one
>>>>  were to go for knearneigh for example, results may not be fair due to
>>>>  the
>>>>  inhomogeneity of the points -- for instance, point A's nearest
>>>>  neighbours
>>>>  may be within a few hundreds of kilometres while point B's nearest
>>>>  neighbours may be in the thousands. I feel like the choice of any
>>>>  neighbourhood definition can be highly debateable... What do you think?
>>>> 
>>>
>>>  When in doubt use contiguity for polygons and similar graph based methods
>>>  for points. Try to keep the graphs planar (as few intersecting edges as
>>>  possible - rule of thumb).
>>> 
>>>
>>>>  After analysing my problem again, I think that predicting by output
>>>>  areas
>>>>  (points) would be best for my case as I would have to make use of the
>>>>  population data after building the model. Interpolating census data of
>>>  the
>>>>  output area (points) would cause me to lose that information.
>>>> 
>>>
>>>  Baseline, this is not going anywhere constructive, and simply approaching
>>>  retail location in this way is unhelpful - there is far too little
>>>  information in your model.
>>>
>>>  If you really must, first find a fully configured retail model with the
>>>  complete data set needed to replicate the results achieved, and use that
>>>  to benchmark how far your approach succeeds in reaching a similar result
>>>  for that restricted area. I think that you'll find that the retail model
>>>  is much more successful, but if not, there is less structure in
>>>  contemporary retail than I though.
>>>
>>>  Best wishes,
>>>
>>>  Roger
>>>
>>>>  Thank you for the comments and the advice so far,  I would greatly
>>>  welcome
>>>>  and appreciate additional feedback!
>>>>
>>>>  Thank you so much once again!
>>>>
>>>>  Jiawen
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>>
>>>>  On Sun, 30 Jun 2019 at 16:57, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>>>
>>>>>  On Sat, 29 Jun 2019, Jiawen Ng wrote:
>>>>>
>>>>>>  Dear Roger,
>>>>>
>>>>>  Postings go to the whole list ...
>>>>> 
>>>>>>
>>>>>>  How can we deal with a huge dataset when using dnearneigh?
>>>>>> 
>>>>>
>>>>>  First, why distance neighbours? What is the support of the data, point
>>>  or
>>>>>  polygon? If polygon, contiguity neighbours are preferred. If not, and
>>>  the
>>>>>  intensity of observations is similar across the whole area, distance
>>>>>  may
>>>>>  be justified, but if the intensity varies, some observations will have
>>>>>  very many neighbours. In that case, unless you have a clear ecological
>>>  or
>>>>>  environmental reason for knowing that a known distance threshold binds,
>>>  it
>>>>>  is not a good choice.
>>>>>
>>>>>>  Here is my code:
>>>>>>
>>>>>>  d <- dnearneigh(spdf,0, 22000)
>>>>>>  all_listw <- nb2listw(d, style = "W")
>>>>>>
>>>>>>  where the spdf object is in the british national grid CRS:
>>>>>>  +init=epsg:27700, with 227,973 observations/points. The distance of
>>>>>  22,000
>>>>>>  was decided by a training set that had 214 observations and the spdf
>>>>>  object
>>>>>>  contains both the training set and the testing set.
>>>>>> 
>>>>>
>>>>>  This is questionable. You train on 214 observations - do their areal
>>>>>  intensity match those of the whole data set? If chosen at random, you
>>>  run
>>>>>  into the spatial sampling problems discussed in:
>>>>> 
>>>>> 
>>>>>
>>>  https://www.sciencedirect.com/science/article/pii/S0304380019302145?dgcid=author
>>>>>
>>>>>  Are 214 observations for training representative of 227,973 prediction
>>>>>  sites? Do you only have observations on the response for 214, and an
>>>>>  unobserved response otherwise? What are the data, what are you trying
>>>>>  to
>>>>>  do and why? This is not a sensible setting for models using weights
>>>>>  matrices for prediction (I think), because we do not have estimates of
>>>  the
>>>>>  prediction error in general.
>>>>>
>>>>>>  I am using a Mac, with a processor of 2.3 GHz Intel Core i5 and 8 GB
>>>>>>  memory. My laptop showed that when dnearneigh command was run on all
>>>>>>  observations, around 6.9 out of 8GB was used by the rsession and that
>>>  the
>>>>>>  %CPU used by the rsession was stated to be around 98%, although
>>>>>>  another
>>>>>>  indicator showed that my computer was around 60% idle. After running
>>>  the
>>>>>>  command for a day, rstudio alerted me that the connection to the
>>>  rsession
>>>>>>  could not be established, so I aborted the entire process altogether.
>>>>>>  I
>>>>>>  think the problem here may be the size of the dataset and perhaps the
>>>>>>  limitations of my laptop specs.
>>>>>> 
>>>>>
>>>>>  On planar data, there is no good reason for this, as each observation
>>>>>  is
>>>>>  treated separately, finding and sorting distances, and choosing those
>>>>>  under the threshold. It will undoubtedly slow if there are more than a
>>>  few
>>>>>  neighbours within the threshold, but I already covered the
>>>  inadvisability
>>>>>  of defining neighbours in that way.
>>>>>
>>>>>  Using an rtree might help, but you get hit badly if there are many
>>>>>  neighbours within the threshold you have chosen anyway.
>>>>>
>>>>>  On most 8GB hardware and modern OS, you do not have more than 3-4GB for
>>>>>  work. So something was swapping on your laptop.
>>>>>
>>>>>>  Do you have any advice on how I can go about making a neighbours list
>>>>>  with
>>>>>>  dnearneigh for 227,973 observations in a successful and efficient way?
>>>>>>  Also, would you foresee any problems in the next steps, especially
>>>  when I
>>>>>>  will be using the neighbourhood listw object as an input in fitting
>>>>>>  and
>>>>>>  predicting using the spatial lag/error models? (see code below)
>>>>>>
>>>>>>  model <-  spatialreg::lagsarlm(rest_formula, data=train, train_listw)
>>>>>>  model_pred <- spatialreg::predict.sarlm(model, test, all_listw)
>>>>>> 
>>>>>
>>>>>  Why would using a spatial lag model make sense? Why are you suggesting
>>>>>  this model, do you have a behavioural for why only the spatially lagged
>>>>>  response should be included?
>>>>>
>>>>>  Why do you think that this is sensible? You are predicting 1000 times
>>>  for
>>>>>  each observation - this is not what the prediction methods are written
>>>>>  for. Most involve inverting an nxn inverse matrix - did you refer to
>>>>>  Goulard et al. (2017) to get a good understanding of the underlying
>>>>>  methods?
>>>>>
>>>>>>  I think the predicting part may take some time, since my test set
>>>>>  consists
>>>>>>  of 227,973 - 214 observations = 227,759 observations.
>>>>>>
>>>>>>  Here are some solutions that I have thought of:
>>>>>>
>>>>>>  1. Interpolate the test set point data of 227,759 observations over a
>>>>>  more
>>>>>>  manageable spatial pixel dataframe with cell size of perhaps 10,000m
>>>>>>  by
>>>>>>  10,000m which would give me around 4900 points. So instead of 227,759
>>>>>>  observations, I can make the listw object based on just 4900 + 214
>>>>>  training
>>>>>>  points and predict just on 4900 observations.
>>>>>
>>>>>  But what are you trying to do? Are the observations output areas? House
>>>>>  sales? If you are not filling in missing areal units (the Goulard et
>>>>>  al.
>>>>>  case), couldn't you simply use geostatistical methods which seem to
>>>  match
>>>>>  your support better, and can be fitted and can predict using a local
>>>>>  neighbourhood? While you are doing that, you could switch to INLA with
>>>>>  SPDE, which interposes a mesh like the one you suggest. But in that
>>>  case,
>>>>>  beware of the mesh choice issue in:
>>>>>
>>>>>  https://doi.org/10.1080/03610926.2018.1536209
>>>>> 
>>>>>>
>>>>>>  2. Get hold of better performance machines through cloud computing
>>>>>>  such
>>>>>  as
>>>>>>  AWS EC2 services and try running the commands and models there.
>>>>>> 
>>>>>
>>>>>  What you need are methods, not wasted money on hardware as a service.
>>>>>
>>>>>>  3. Parallel computing using the parallel package from r (although I am
>>>>>  not
>>>>>>  sure whether dnearneigh can be parallelised).
>>>>>> 
>>>>>
>>>>>  This could easily be implemented if it was really needed, which I don't
>>>>>  think it is; better methods understanding lets one do more with less.
>>>>>
>>>>>>  I believe option 1 would be the most manageable but I am not sure how
>>>  and
>>>>>>  by how much this would affect the accuracy of the predictions as
>>>>>>  interpolating the dataset would be akin to introducing more
>>>>>>  estimations
>>>>>  in
>>>>>>  the prediction. However, I am also grappling with the trade-off
>>>>>>  between
>>>>>>  accuracy and computation time. Hence, if options 2 and 3 can offer a
>>>>>>  reasonable computation time (1-2 hours) then I would forgo option 1.
>>>>>>
>>>>>>  What do you think? Is it possible to make a neighbourhood listw object
>>>>>  out
>>>>>>  of 227,973 observations efficiently?
>>>>>
>>>>>  Yes, but only if the numbers of neighbours are very small. Look in
>>>  Bivand
>>>>>  et al. (2013) to see the use of some fairly large n, but only with few
>>>>>  neighbours for each observation. You seem to be getting average
>>>  neighbour
>>>>>  counts in the thousands, which makes no sense.
>>>>> 
>>>>>>
>>>>>>  Thank you for reading to the end! Apologies for writing a lengthy one,
>>>>>  just
>>>>>>  wanted to fully describe what I am facing, I hope I didn't miss out
>>>>>>  anything crucial.
>>>>>> 
>>>>>
>>>>>  Long is OK, but there is no motivation here for why you want to make
>>>  200K
>>>>>  predictions from 200 observations with point support (?) using weights
>>>>>  matrices.
>>>>>
>>>>>  Hope this clarifies,
>>>>>
>>>>>  Roger
>>>>>
>>>>>>  Thank you so much once again!
>>>>>>
>>>>>>  jiawen
>>>>>>
>>>>>>        [[alternative HTML version deleted]]
>>>>>>
>>>>>>  _______________________________________________
>>>>>>  R-sig-Geo mailing list
>>>>>>  R-sig-Geo at r-project.org
>>>>>>  https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>> 
>>>>>
>>>>>  --
>>>>>  Roger Bivand
>>>>>  Department of Economics, Norwegian School of Economics,
>>>>>  Helleveien 30, N-5045 Bergen, Norway.
>>>>>  voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>>  https://orcid.org/0000-0003-2392-6140
>>>>>  https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>> 
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>>  _______________________________________________
>>>>  R-sig-Geo mailing list
>>>>  R-sig-Geo at r-project.org
>>>>  https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>> 
>>>
>>>  --
>>>  Roger Bivand
>>>  Department of Economics, Norwegian School of Economics,
>>>  Helleveien 30, N-5045 Bergen, Norway.
>>>  voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>  https://orcid.org/0000-0003-2392-6140
>>>  https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>> 
>>
>>   [[alternative HTML version deleted]]
>>
>>  _______________________________________________
>>  R-sig-Geo mailing list
>>  R-sig-Geo at r-project.org
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> 
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From @@g||@rd|5 @end|ng |rom c@mpu@@un|m|b@|t  Wed Jul  3 10:58:48 2019
From: @@g||@rd|5 @end|ng |rom c@mpu@@un|m|b@|t (Andrea Gilardi)
Date: Wed, 3 Jul 2019 10:58:48 +0200
Subject: [R-sig-Geo] How to reshape a LINESTRING sfc into equal length
 segments
Message-ID: <CA+Q+afHrf4g+0OsGYoVYpFVB8F50WA5=9dD3mF-PepHK6jZ8xQ-1125@mail.gmail.com>

Hi everyone. This is my first mail to this mailing list so excuse me if I'm
doing something wrong. I was wondering if it's possible to reshape a
LINESTRING sfc of all connected segments into equal length segments.

I tried to explain a little bit on why I want to do that here:
https://gis.stackexchange.com/questions/327376/how-to-divide-linestring-spatial-network-into-equal-length-segments-using-r-sf

I'm pretty sure that I'm overcomplicating something simple and, in that
case, could you point me in the right direction?

Thank you in advance
Andrea

	[[alternative HTML version deleted]]


From edzer@pebe@m@ @end|ng |rom un|-muen@ter@de  Thu Jul  4 11:56:31 2019
From: edzer@pebe@m@ @end|ng |rom un|-muen@ter@de (Edzer Pebesma)
Date: Thu, 4 Jul 2019 11:56:31 +0200
Subject: [R-sig-Geo] How to reshape a LINESTRING sfc into equal length
 segments
In-Reply-To: <CA+Q+afHrf4g+0OsGYoVYpFVB8F50WA5=9dD3mF-PepHK6jZ8xQ-1125@mail.gmail.com>
References: <CA+Q+afHrf4g+0OsGYoVYpFVB8F50WA5=9dD3mF-PepHK6jZ8xQ-1125@mail.gmail.com>
Message-ID: <937d4b47-2de7-6158-41ba-d6689f742343@uni-muenster.de>

You may want to look into sf::st_segmentize.

On 7/3/19 10:58 AM, Andrea Gilardi wrote:
> Hi everyone. This is my first mail to this mailing list so excuse me if I'm
> doing something wrong. I was wondering if it's possible to reshape a
> LINESTRING sfc of all connected segments into equal length segments.
> 
> I tried to explain a little bit on why I want to do that here:
> https://gis.stackexchange.com/questions/327376/how-to-divide-linestring-spatial-network-into-equal-length-segments-using-r-sf
> 
> I'm pretty sure that I'm overcomplicating something simple and, in that
> case, could you point me in the right direction?
> 
> Thank you in advance
> Andrea
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics
Heisenbergstrasse 2, 48151 Muenster, Germany
Phone: +49 251 8333081

-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 2472 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190704/f569cddb/attachment.bin>

From @@g||@rd|5 @end|ng |rom c@mpu@@un|m|b@|t  Thu Jul  4 18:33:12 2019
From: @@g||@rd|5 @end|ng |rom c@mpu@@un|m|b@|t (Andrea Gilardi)
Date: Thu, 4 Jul 2019 18:33:12 +0200
Subject: [R-sig-Geo] How to reshape a LINESTRING sfc into equal length
 segments
In-Reply-To: <937d4b47-2de7-6158-41ba-d6689f742343@uni-muenster.de>
References: <CA+Q+afHrf4g+0OsGYoVYpFVB8F50WA5=9dD3mF-PepHK6jZ8xQ-1125@mail.gmail.com>
 <937d4b47-2de7-6158-41ba-d6689f742343@uni-muenster.de>
Message-ID: <CA+Q+afEuoe8dXWUmXLFW5iA5Uo+W50tOkRE_7SUfq6v=REqzvw@mail.gmail.com>

Thank you very much for your answer. I tried to look into st_segmentize but
I don't think I can use it in this case (or I'm not able to). This is a
little example to show what I mean:

library(sf)
my_sf <- st_sf(a = c("1", "2"), geom = st_sfc(st_linestring(rbind(c(0, 0),
c(0, 1))), st_linestring(rbind(c(0, 1), c(1, 1)))), crs = 3003)

The sf object is formed by two touching linestring geometries of length 1m.
I would like to refactor the sf object into segments whose length is
approximately equal to 0.4m (I choose this value just for this example).
Since the two segments are touching each other, I'd like that the result
will be a new sf object with 5 geometries: 1) a linestring from (0, 0) to
(0, 0.4); 2) a linestring from (0, 0.4) to (0, 0.8); 3) a linestring from
(0, 0.8) to (0, 1) and from (0, 1) to (0.2, 1); 4) a linestring from (0.2,
1) to (0.6, 1) and 5) a linestring from (0.6, 1) to (1, 1).

Even if I use st_segmentize, the split of each segment does not consider
the neighbouring segments.

I'm sure that in a real dataset this is much more difficult and complicated
than this example since
1) it could happend that for one boundary point there are more than one
touching segments and
2) the total length of the network will never be an exact multiple of the
segment distance which implies that there will be some residuals
so, maybe, I'm just asking something unreasonable. I think that any
reasonable refactoring of the sfc of linestrings is fine for my
application.


Il giorno gio 4 lug 2019 alle ore 11:56 Edzer Pebesma <
edzer.pebesma at uni-muenster.de> ha scritto:

> You may want to look into sf::st_segmentize.
>
> On 7/3/19 10:58 AM, Andrea Gilardi wrote:
> > Hi everyone. This is my first mail to this mailing list so excuse me if
> I'm
> > doing something wrong. I was wondering if it's possible to reshape a
> > LINESTRING sfc of all connected segments into equal length segments.
> >
> > I tried to explain a little bit on why I want to do that here:
> >
> https://gis.stackexchange.com/questions/327376/how-to-divide-linestring-spatial-network-into-equal-length-segments-using-r-sf
> >
> > I'm pretty sure that I'm overcomplicating something simple and, in that
> > case, could you point me in the right direction?
> >
> > Thank you in advance
> > Andrea
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Edzer Pebesma
> Institute for Geoinformatics
> Heisenbergstrasse 2, 48151 Muenster, Germany
> Phone: +49 251 8333081
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Fri Jul  5 02:49:40 2019
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Thu, 4 Jul 2019 20:49:40 -0400
Subject: [R-sig-Geo] adehabitat: Working with enfa() function using *asc
 files
Message-ID: <a59283f3-1c83-4668-e185-23da7a7ce59a@yahoo.com.br>

Dear R-Sig-Geo Members,

 ?????? Many years ago I used enfa() function of adehabitat package with my 
*asc files with environmental variables and my script works very well, 
but now adehabitat is deprecated and there are many versions of 
adehabitat. I install the adehabitatHS but for doesn't work with my *asc 
files.

 ?????? My objective was create a data2enfa object and for this I used 
manually all the functions in an old adehabitat package (kasc2df, 
import.asc, as.kasc and data2enfa) but doesn't work too. In my script I 
make:

#Create enfa analysis object
pinkdata <- data2enfa(as.kasc(list(tmax=import.asc("tmax.asc") , 
tmin=import.asc("tmin.asc"))), pink.sub.pnt at coords)
#

 ??Error in count.points(pts, kasc) :
 ?? w should inherit the class SpatialPixels

 ?????? I need to create a data2enfa object using *asc files as tab and 
organism position pink.sub.pnt at coords as pr in enfa1 <- 
enfa(dudi.pca(tab), pr,?? scannf = FALSE).

 ?????? Please, any member that work with *asc files and ENFA help me?

Thanks in advanced,

Alexandre

-- 
======================================================================
Alexandre dos Santos
Prote????o Florestal
IFMT - Instituto Federal de Educa????o, Ci??ncia e Tecnologia de Mato Grosso
Campus C??ceres
Caixa Postal 244
Avenida dos Ramires, s/n
Bairro: Distrito Industrial
C??ceres - MT                      CEP: 78.200-000
Fone: (+55) 65 99686-6970 (VIVO) (+55) 65 3221-2674 (FIXO)

         alexandre.santos at cas.ifmt.edu.br
Lattes: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
Researchgate: www.researchgate.net/profile/Alexandre_Santos10
LinkedIn: br.linkedin.com/in/alexandre-dos-santos-87961635
Mendeley:www.mendeley.com/profiles/alexandre-dos-santos6/


From |ove|y||tt|ed@|@|e@ @end|ng |rom gm@||@com  Sun Jul  7 01:52:53 2019
From: |ove|y||tt|ed@|@|e@ @end|ng |rom gm@||@com (Jiawen Ng)
Date: Sun, 7 Jul 2019 00:52:53 +0100
Subject: [R-sig-Geo] Error in spatialreg::predict.sarlm: unknown mismatch.
 please report this bug
Message-ID: <CAHz1c-WHjZfOx4mnGhBU7V2SAg0QbeVVr79C7bn17h2ba-A=8w@mail.gmail.com>

Dear All,

I am trying to do a out-of-sample prediction using predict.sarlm but I am
getting the 'unknown mismatch' error from predict.sarlm.

I have a training and a testing spatial points dataframe (train_spdf and
test_spdf) and I know that I would require a listw object that contains the
neighbours of the training set for fitting the model and a listw object
that contains the neighbours of training + testing set for predicting.

Here is how I have created the listw object for predicting:
all_coords <- rbind(train_spdf at coords, test_spdf at coords)
col.rel.nb <- graph2nb(relativeneigh(all_coords), sym=TRUE)
test.listw <- nb2listw(col.rel.nb)

It seems like there are at least 2 ways one can create the listw object for
model fitting.

First Way:
nb <- graph2nb(relativeneigh(train_spdf at coords), sym=TRUE)
train.listw <- nb2listw(nb)

Second Way:
train_index <- rep(1:214) # the first 214 observations should be my
training set since that was how I rbind-ed it in all_coords object
train.listw <- subset(test.listw, 1:nrow(all_coords) %in% train_index)

Eventually, the train.listw and test.listw object will be used as follows:

model <- spatialreg::lagsarlm(myformula, data=train_spdf at data,
                        train.listw, type="mixed",zero.policy = T)

pred <- spatialreg::predict.sarlm(model, test_spdf at data,
test.listw,zero.policy = T)

When I did it the first way, I obtained an error and warning messages from
the predict.sarlm function:
Error in spatialreg::predict.sarlm(model, final_test_spdf at data, test.listw,
 :
  unknown mismatch. please report this bug
In addition: Warning messages:
1: In spatialreg::predict.sarlm(model, final_test_spdf at data, test.listw,  :
  some region.id are both in data and newdata
2: In colnames(Xs.not.lagged) != colnames(Xo) :
  longer object length is not a multiple of shorter object length

When I did it the second way,  I obtained warning messages from the
lagsarlm function:
Warning message:
In spatialreg::lagsarlm(as.formula(rest_formula), data =
final_train_spdf at data,  :
  Aliased variables found:
A lag.B lag.C lag.D lag. E lag.F lag.G lag.H lag.I lag.J lag.K lag.L lag.M
lag.N lag.O lag.P  [... truncated]

And then the same error and warning messages as the first way when using
the predict.sarlm function:
Error in spatialreg::predict.sarlm(model2, final_test_spdf at data,
test.listw,  :
  unknown mismatch. please report this bug
In addition: Warning messages:
1: In spatialreg::predict.sarlm(model2, final_test_spdf at data, test.listw,  :
  some region.id are both in data and newdata
2: In colnames(Xs.not.lagged) != colnames(Xo) :
  longer object length is not a multiple of shorter object length

As you can see, either method would have me run into the 'unknown mismatch'
error in predict.sarlm.

Here I have 2 questions:
1. Which way should be the right way of creating the listw object for
fitting the model?
2. How can I solve the 'unknown mismatch' error?

Would appreciate any advice! Thank you!

	[[alternative HTML version deleted]]


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Mon Jul  8 00:21:10 2019
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Sun, 7 Jul 2019 18:21:10 -0400
Subject: [R-sig-Geo] Split RGB raster in multiples pieces and save file in
 GeoTiff with conditions
Message-ID: <b60c33f2-8405-5719-991e-3cc43a116d3c@yahoo.com.br>

Dear R-Sig-Geo Members,

 ?????? I've like to split a RGB raster in multiple pieces and save each 
image in GeoTiff format, but in the file name I need the count of points 
(pts) inside each piece. My code runs perfectly when I used a single 
raster image, but with stack or brick object doesn't work. I've like as 
final output in my directory nine GeoTiff files (I divided the original 
raster in 9 parts) with files names: SplitRas1points0.tif ... 
SplitRas9points0.tif

 ?????? My code is:

#Packages
library(raster)
library(sp)
library(rgdal)

## Create a simulated RBG rasters
r <- raster(nc=30, nr=30)
r <- setValues(r, round(runif(ncell(r))* 255))
g <- raster(nc=30, nr=30)
g <- setValues(r, round(runif(ncell(r))* 255))
b <- raster(nc=30, nr=30)
b <- setValues(r, round(runif(ncell(r))* 255))
rgb<-stack(r,g,b)
plotRGB(rgb,
 ?????????????? r = 1, g = 2, b = 3)

##Given interesting points coordinates
xd???????? <- c(-24.99270,45.12069,99.40321,73.64419)
yd?? <- c(-45.435267,-88.369745,-7.086949,44.174530)
pts <- data.frame(xd,yd)
pts_s<- SpatialPoints(pts)
points(pts_s, col="black", pch=16)


# This function spatially aggregates the original raster
# it turns each aggregated cell into a polygon
# then the extent of each polygon is used to crop
# the original raster.
# The function returns a list with all the pieces
# it saves and plots each piece
# The arguments are:
# raster = raster to be chopped?????????????????????? (raster object)
# ppside = pieces per side???????????????????????????????? (integer)
# save???? = write raster?????????????????????????????????????? (TRUE or FALSE)
# plot???? = do you want to plot the output? (TRUE or FALSE)
SplitRas <- function(raster,ppside,save,plot){
 ?? h?????????????? <- ceiling(ncol(raster)/ppside)
 ?? v?????????????? <- ceiling(nrow(raster)/ppside)
 ?? agg?????????? <- aggregate(raster,fact=c(h,v))
 ?? agg[]?????? <- 1:ncell(agg)
 ?? agg_poly <- rasterToPolygons(agg)
 ?? names(agg_poly) <- "polis"
 ?? r_list <- list()
 ?? for(i in 1:ncell(agg)){
 ?????? e1?????????????????? <- extent(agg_poly[agg_poly$polis==i,])
 ?????? r_list[[i]] <- crop(raster,e1)
 ?? }
 ?? if(save==T){
 ?????? for(i in 1:length(r_list)){
 ?????????? writeRaster(r_list[[i]],filename=paste("SplitRas",i,sep=""),
 ?????????????????????????????????? format="GTiff",datatype="FLT4S",overwrite=TRUE)
 ?????? }
 ?? }
 ?? if(plot==T){
 ?????? par(mfrow=c(ppside,ppside))
 ?????? for(i in 1:length(r_list)){
 ?????????? plot(r_list[[i]],axes=F,legend=F,bty="n",box=FALSE)
 ?????? }
 ?? }
 ?? return(r_list)
}


#Slip RGB raster in 3 parts
splitRBG<-SplitRas(raster=rgb,ppside=3,save=FALSE,plot=FALSE)

#Count number of points inside each piece of raster
res<-NULL
for(i in 1:3){
 ?? pointcount = function(r, pts){
 ?? # make a raster of zeroes like the input
 ?? r2 = r
 ?? r2[] = 0
 ?? # get the cell index for each point and make a table:
 ?? counts = table(cellFromXY(r,pts))
 ?? # fill in the raster with the counts from the cell index:
 ?? r2[as.numeric(names(counts))] = counts
 ?? return(r2)
 ??}
r2 = pointcount(splitRBG[i], pts_s)
res<-rbind(res,c(r2))
}
#


#Run a code on each piece with the number of points inside each piece in 
the file name (res[i])
list2 <- list()
for(i in 1:3){ # 3 is the number of pieces
 ?? rx <- raster(paste("splitRBG",i,".tif",sep=""))
writeRaster(splitRBG,filename=paste("SplitRas",i,"points",res[i],sep=""),
 ?????????????????????????? format="GTiff",datatype="FLT4S",overwrite=TRUE)
}
#


Any ideas please?

Thanks in advanced,

Alexandre

-- 
======================================================================
Alexandre dos Santos
Prote????o Florestal
IFMT - Instituto Federal de Educa????o, Ci??ncia e Tecnologia de Mato Grosso
Campus C??ceres
Caixa Postal 244
Avenida dos Ramires, s/n
Bairro: Distrito Industrial
C??ceres - MT                      CEP: 78.200-000
Fone: (+55) 65 99686-6970 (VIVO) (+55) 65 3221-2674 (FIXO)

         alexandre.santos at cas.ifmt.edu.br
Lattes: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
Researchgate: www.researchgate.net/profile/Alexandre_Santos10
LinkedIn: br.linkedin.com/in/alexandre-dos-santos-87961635
Mendeley:www.mendeley.com/profiles/alexandre-dos-santos6/


From |ove|y||tt|ed@|@|e@ @end|ng |rom gm@||@com  Mon Jul  8 02:02:06 2019
From: |ove|y||tt|ed@|@|e@ @end|ng |rom gm@||@com (Jiawen Ng)
Date: Mon, 8 Jul 2019 01:02:06 +0100
Subject: [R-sig-Geo] Error in predict.sarlm: non-unique row.names given
Message-ID: <CAHz1c-V7RG0ynjhBZKHYjUDw08tMO0WYajQSaUUBTkgX3JjxLw@mail.gmail.com>

Another question on predict.sarlm!

Here is the line of code that is producing the error:
pred <- spatialreg::predict.sarlm(model, df, test.listw,zero.policy = T)

Here is the error:

Error in mat2listw(W, row.names = region.id.mixed, style = style) :
  non-unique row.names given
In addition: Warning messages:
1: In spatialreg::predict.sarlm(model, df, test.listw,  :
  some region.id are both in data and newdata
2: In subset(attr(listw.mixed, "region.id"), attr(listw.mixed, "region.id")
%in%  :
  longer object length is not a multiple of shorter object length

Any idea how I can solve the non-unique row.names error?

Thank you!

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Jul  8 05:32:01 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 8 Jul 2019 05:32:01 +0200
Subject: [R-sig-Geo] Error in predict.sarlm: non-unique row.names given
In-Reply-To: <CAHz1c-V7RG0ynjhBZKHYjUDw08tMO0WYajQSaUUBTkgX3JjxLw@mail.gmail.com>
References: <CAHz1c-V7RG0ynjhBZKHYjUDw08tMO0WYajQSaUUBTkgX3JjxLw@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1907080510470.26680@reclus.nhh.no>

Do provide a complete reproducible example. I really appeal to all posting 
questions to give potential helpers something to work on. Asking for 
reproducible examples is the absolutely dominant response to postings that 
lack them, if they get any response at all.

Start with this and work backwards until you can reproduce your 
misunderstanding:

col <- st_read(system.file("shapes/columbus.shp", package="spData"))
train <- col[col$EW == 1,]
test <- col[col$EW == 0,]
col.nb <- spdep::poly2nb(col)
train.nb <- spdep::poly2nb(train)
test.nb <- spdep::poly2nb(test)
attr(col.nb, "region.id")
attr(train.nb, "region.id")
attr(test.nb, "region.id")
train.mod <- lagsarlm(CRIME ~ INC + HOVAL, data=train,
   listw=spdep::nb2listw(train.nb))
try(preds <- predict(train.mod, newdata=test,
   listw=spdep::nb2listw(test.nb)))
preds[2]
try(preds1 <- predict(train.mod, newdata=col,
   listw=spdep::nb2listw(col.nb)))
# warning


preds1[4]
try(preds2 <- predict(train.mod, newdata=test,
   listw=spdep::nb2listw(col.nb)))
preds2[2]

Using the complete set of weights permits the spatial process to flow 
between neighbouring members of train/test sets.

Your problem is probably that your two data objects do not use row.names 
as expected:

attr(test.nb, "region.id") <- as.character(1:length(test.nb))
attr(train.nb, "region.id") <- as.character(1:length(train.nb))
train.mod1 <- lagsarlm(CRIME ~ INC + HOVAL, data=train,
   listw=spdep::nb2listw(train.nb))
try(preds3 <- predict(train.mod, newdata=test,
   listw=spdep::nb2listw(test.nb)))
# Error in predict.sarlm(train.mod, newdata = test, listw = 
# spdep::nb2listw(test.nb)) :
#   mismatch between newdata and spatial weights. newdata should have 
# region.id as row.names

as is obvious. So when the predict method is trying to assign the newdata 
neighbours (it needs to identify the correct rows in newdata based on the 
"region.id" attribute of the provided weights), it fails as described.

Use the whole data weights when predicting for the test set newdata=, or 
if the two graphs do not neighbour each other, that is train.nb is 
separate from test.nb (think two islands), make sure that the region.ids 
and row.names do not overlap between test and train sets.

Please use the example to explore the problem in your workflow, (re-)read 
Goulard et al. (2017), and the help page, and report back. Remember that 
you can only predict for a test set of reasonable size (because as you see 
from the underlying article, you probably need an inverted nxn matrix in 
the spatial lag model case).

Hope this clarifies

Roger




On Mon, 8 Jul 2019, Jiawen Ng wrote:

> Another question on predict.sarlm!
>
> Here is the line of code that is producing the error:
> pred <- spatialreg::predict.sarlm(model, df, test.listw,zero.policy = T)
>
> Here is the error:
>
> Error in mat2listw(W, row.names = region.id.mixed, style = style) :
>  non-unique row.names given
> In addition: Warning messages:
> 1: In spatialreg::predict.sarlm(model, df, test.listw,  :
>  some region.id are both in data and newdata
> 2: In subset(attr(listw.mixed, "region.id"), attr(listw.mixed, "region.id")
> %in%  :
>  longer object length is not a multiple of shorter object length
>
> Any idea how I can solve the non-unique row.names error?
>
> Thank you!
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From benj@m|n@p@uget @end|ng |rom te@or@@|r  Tue Jul  9 13:48:38 2019
From: benj@m|n@p@uget @end|ng |rom te@or@@|r (Benjamin Pauget)
Date: Tue, 9 Jul 2019 11:48:38 +0000
Subject: [R-sig-Geo] geoJSON and leaflet
Message-ID: <d86e6b53edbd45709c7937af7c0db494@tesora.fr>

Hi,
I?m writting because I have some trouble with a geoJSON file and a leaflet.
I?m trying to display polygon on a leaflet map, but nothing append ?
I have no error message.
Do you have some advice/ideas?

Best regards


Here is my code :
library(leaflet)
library(jsonlite)
url <- "http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10"
geojson <- jsonlite::fromJSON(url)
leaflet() %>%
  addTiles()%>%
  setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
  addMarkers(lng = 4.872536, lat = 45.758321)%>%
  addGeoJSON(geojson) # doesn?t work by using geojson$data or geojson$data$geom




Benjamin PAUGET
Responsable R&D

[cid:image005.png at 01D4F3BB.3531D840]


+33 (0)1 81 94 13 70

+33 (0)6 47 01 85 92

Le Visium

22 Av Aristide Briand

94110 ARCUEIL

https://tesora.fr/
Linkedin<https://www.linkedin.com/company/tesora-france>




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190709/a4a0b819/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 5152 bytes
Desc: image001.png
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190709/a4a0b819/attachment.png>

From t|m@@ppe|h@n@ @end|ng |rom gm@||@com  Tue Jul  9 15:13:30 2019
From: t|m@@ppe|h@n@ @end|ng |rom gm@||@com (Tim Salabim)
Date: Tue, 9 Jul 2019 15:13:30 +0200
Subject: [R-sig-Geo] geoJSON and leaflet
In-Reply-To: <d86e6b53edbd45709c7937af7c0db494@tesora.fr>
References: <d86e6b53edbd45709c7937af7c0db494@tesora.fr>
Message-ID: <CAMbH+HiRhyDuVtmgMEXKmphKqoPkGyha4mTy23JV79A52Lgrkg@mail.gmail.com>

Hi Benjamin,
What you get back from the jsonlite::fromJSON() call is a simplified list
with the data (including the geometry information) as a data frame in one
of the list slots. GeoJson is usually a character string. Therefore, if you
open your map in the browser and open the console (Ctrl + i) you will see
the error message: "Invalid GeoJson object".

I am no expert on geojson structure, but it seems that the data that you
request is not in standard format. I tried a few different ways of parsing
the data to a valid GeoJson string but did not have success. Maybe someone
else with more insight has some helpful ideas how to achieve this.

However, I found a workaround to get the polygons shown on the map using
library(sf) - see code below. Note, this only visualises the geometry
information only, so direct popup queries via "~" are not possible.

library(leaflet)
library(jsonlite)
library(sf)

url <- "
http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10
"

geojson <- jsonlite::fromJSON(url)

# convert coordinate arrays to matrices
geom = lapply(geojson$data$geom$coordinates, matrix, ncol = 2)
# create multipolygon from coordinate matrices
geom = st_cast(st_polygon(geom), "MULTIPOLYGON")

leaflet() %>%
  addTiles()%>%
  setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
  addMarkers(lng = 4.872536, lat = 45.758321)%>%
  addPolygons(data = geom)

HTH,
Tim



On Tue, Jul 9, 2019 at 1:49 PM Benjamin Pauget <benjamin.pauget at tesora.fr>
wrote:

> Hi,
>
> I?m writting because I have some trouble with a geoJSON file and a leaflet.
>
> I?m trying to display polygon on a leaflet map, but nothing append ?
>
> I have no error message.
>
> Do you have some advice/ideas?
>
>
>
> Best regards
>
>
>
>
>
> Here is my code :
>
> library(leaflet)
>
> library(jsonlite)
>
> url <- "
> http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10
> "
>
> geojson <- jsonlite::fromJSON(url)
>
> leaflet() %>%
>
>   addTiles()%>%
>
>   setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
>
>   addMarkers(lng = 4.872536, lat = 45.758321)%>%
>
>   addGeoJSON(geojson) # doesn?t work by using geojson$data or
> geojson$data$geom
>
>
>
>
>
>
>
>
>
> *Benjamin PAUGET*
>
> Responsable R&D
>
> [image: cid:image005.png at 01D4F3BB.3531D840]
>
>
>
> +33 (0)1 81 94 13 70
>
> +33 (0)6 47 01 85 92
>
> Le Visium
>
> 22 Av Aristide Briand
>
> 94110 ARCUEIL
>
> https://tesora.fr/
>
> Linkedin <https://www.linkedin.com/company/tesora-france>
>
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190709/60fb117b/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 5152 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190709/60fb117b/attachment.png>

From t|m@@ppe|h@n@ @end|ng |rom gm@||@com  Tue Jul  9 16:24:58 2019
From: t|m@@ppe|h@n@ @end|ng |rom gm@||@com (Tim Salabim)
Date: Tue, 9 Jul 2019 16:24:58 +0200
Subject: [R-sig-Geo] geoJSON and leaflet
In-Reply-To: <27d3bbc072ae4f90b32ad7a53a55457d@tesora.fr>
References: <d86e6b53edbd45709c7937af7c0db494@tesora.fr>
 <CAMbH+HiRhyDuVtmgMEXKmphKqoPkGyha4mTy23JV79A52Lgrkg@mail.gmail.com>
 <27d3bbc072ae4f90b32ad7a53a55457d@tesora.fr>
Message-ID: <CAMbH+HjK2L2mwtORZFWQsEnHP1YbcY8ghv7qfCv+L9Pcz+ptfQ@mail.gmail.com>

Hi Benjamin,
please respond to the list as well. Other people may have similar issues
and this way we can find the solutions online.

If you want to extract the data along with the geometries, here's how you
could do it:

library(leaflet)
library(jsonlite)
library(sf)

url <- "
http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10
"

geojson <- jsonlite::fromJSON(url)

# convert coordinate arrays to matrices
geom = lapply(geojson$data$geom$coordinates, matrix, ncol = 2)
# create multipolygon from coordinate matrices
geom = lapply(geom, function(i) st_polygon(list(i)))
# overwrite the geom column of data in geojson
geojson$data$geom = st_sfc(geom, crs = 4326)
# exract data from geojson and turn into an sf object
dat = st_as_sf(geojson$data)

leaflet() %>%
  addTiles()%>%
  setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
  addMarkers(lng = 4.872536, lat = 45.758321)%>%
  addPolygons(data = dat, popup = ~nom)

This way, as you can see, you can use the tilde (~) notation to include
popups referring to a column of the data.

I don't think that what you get using RSONIO is any closer to a valid
geojson.

Best
Tim

On Tue, Jul 9, 2019 at 3:28 PM Benjamin Pauget <benjamin.pauget at tesora.fr>
wrote:

> Dear Tim,
>
> Tanks you soooo much for your response !
>
> I am a beginner and working with geospatial is still unclear.
>
>
>
> I have tried to open my json file with ?from_JSON? and I have another
> format of my data :
>
> url <- "
> http://www.georisques.gouv.fr/api/v1/sis?rayon=5000&latlon=4.854899%2C%2045.763079&page=1&page_size=10
> "
>
> geojson4 <- RJSONIO::fromJSON(url)
>
>
>
> Do you think this format is more relevant and will allow to display the
> pop?up info ?
>
>
>
> Thank you again for your code (I just pass the two last day on trying to
> display the polygons) !
>
>
>
> Best regards,
>
>
>
>
>
>
>
> *Benjamin PAUGET*
>
> Responsable R&D
>
> [image: cid:image005.png at 01D4F3BB.3531D840]
>
>
>
> +33 (0)1 81 94 13 70
>
> +33 (0)6 47 01 85 92
>
> Le Visium
>
> 22 Av Aristide Briand
>
> 94110 ARCUEIL
>
> https://tesora.fr/
>
> Linkedin <https://www.linkedin.com/company/tesora-france>
>
>
>
>
>
> *De :* Tim Salabim <tim.appelhans at gmail.com>
> *Envoy? :* mardi 9 juillet 2019 15:14
> *? :* Benjamin Pauget <benjamin.pauget at tesora.fr>
> *Cc :* r-sig-geo at r-project.org
> *Objet :* Re: [R-sig-Geo] geoJSON and leaflet
>
>
>
> Hi Benjamin,
>
> What you get back from the jsonlite::fromJSON() call is a simplified list
> with the data (including the geometry information) as a data frame in one
> of the list slots. GeoJson is usually a character string. Therefore, if you
> open your map in the browser and open the console (Ctrl + i) you will see
> the error message: "Invalid GeoJson object".
>
>
>
> I am no expert on geojson structure, but it seems that the data that you
> request is not in standard format. I tried a few different ways of parsing
> the data to a valid GeoJson string but did not have success. Maybe someone
> else with more insight has some helpful ideas how to achieve this.
>
>
>
> However, I found a workaround to get the polygons shown on the map using
> library(sf) - see code below. Note, this only visualises the geometry
> information only, so direct popup queries via "~" are not possible.
>
>
>
> library(leaflet)
> library(jsonlite)
> library(sf)
>
> url <- "
> http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10
> "
>
> geojson <- jsonlite::fromJSON(url)
>
> # convert coordinate arrays to matrices
> geom = lapply(geojson$data$geom$coordinates, matrix, ncol = 2)
> # create multipolygon from coordinate matrices
> geom = st_cast(st_polygon(geom), "MULTIPOLYGON")
>
> leaflet() %>%
>   addTiles()%>%
>   setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
>   addMarkers(lng = 4.872536, lat = 45.758321)%>%
>   addPolygons(data = geom)
>
>
>
> HTH,
>
> Tim
>
>
>
>
>
>
>
> On Tue, Jul 9, 2019 at 1:49 PM Benjamin Pauget <benjamin.pauget at tesora.fr>
> wrote:
>
> Hi,
>
> I?m writting because I have some trouble with a geoJSON file and a leaflet.
>
> I?m trying to display polygon on a leaflet map, but nothing append ?
>
> I have no error message.
>
> Do you have some advice/ideas?
>
>
>
> Best regards
>
>
>
>
>
> Here is my code :
>
> library(leaflet)
>
> library(jsonlite)
>
> url <- "
> http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10
> "
>
> geojson <- jsonlite::fromJSON(url)
>
> leaflet() %>%
>
>   addTiles()%>%
>
>   setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
>
>   addMarkers(lng = 4.872536, lat = 45.758321)%>%
>
>   addGeoJSON(geojson) # doesn?t work by using geojson$data or
> geojson$data$geom
>
>
>
>
>
>
>
>
>
> *Benjamin PAUGET*
>
> Responsable R&D
>
> [image: cid:image005.png at 01D4F3BB.3531D840]
>
>
>
> +33 (0)1 81 94 13 70
>
> +33 (0)6 47 01 85 92
>
> Le Visium
>
> 22 Av Aristide Briand
>
> 94110 ARCUEIL
>
> https://tesora.fr/
>
> Linkedin <https://www.linkedin.com/company/tesora-france>
>
>
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190709/8810dbe8/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 5152 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190709/8810dbe8/attachment.png>

From benj@m|n@p@uget @end|ng |rom te@or@@|r  Tue Jul  9 16:47:39 2019
From: benj@m|n@p@uget @end|ng |rom te@or@@|r (Benjamin Pauget)
Date: Tue, 9 Jul 2019 14:47:39 +0000
Subject: [R-sig-Geo] geoJSON and leaflet
In-Reply-To: <CAMbH+HjK2L2mwtORZFWQsEnHP1YbcY8ghv7qfCv+L9Pcz+ptfQ@mail.gmail.com>
References: <d86e6b53edbd45709c7937af7c0db494@tesora.fr>
 <CAMbH+HiRhyDuVtmgMEXKmphKqoPkGyha4mTy23JV79A52Lgrkg@mail.gmail.com>
 <27d3bbc072ae4f90b32ad7a53a55457d@tesora.fr>
 <CAMbH+HjK2L2mwtORZFWQsEnHP1YbcY8ghv7qfCv+L9Pcz+ptfQ@mail.gmail.com>
Message-ID: <02c06c495cbb411c8cd9d72ce6dcf174@tesora.fr>

Hi Tim,
Thanks again for your response !
It fit perfectly !
Best

Ben




De : Tim Salabim <tim.appelhans at gmail.com>
Envoy? : mardi 9 juillet 2019 16:25
? : Benjamin Pauget <benjamin.pauget at tesora.fr>; r-sig-geo <r-sig-geo at r-project.org>
Objet : Re: [R-sig-Geo] geoJSON and leaflet

Hi Benjamin,
please respond to the list as well. Other people may have similar issues and this way we can find the solutions online.

If you want to extract the data along with the geometries, here's how you could do it:

library(leaflet)
library(jsonlite)
library(sf)

url <- "http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10"

geojson <- jsonlite::fromJSON(url)

# convert coordinate arrays to matrices
geom = lapply(geojson$data$geom$coordinates, matrix, ncol = 2)
# create multipolygon from coordinate matrices
geom = lapply(geom, function(i) st_polygon(list(i)))
# overwrite the geom column of data in geojson
geojson$data$geom = st_sfc(geom, crs = 4326)
# exract data from geojson and turn into an sf object
dat = st_as_sf(geojson$data)

leaflet() %>%
  addTiles()%>%
  setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
  addMarkers(lng = 4.872536, lat = 45.758321)%>%
  addPolygons(data = dat, popup = ~nom)

This way, as you can see, you can use the tilde (~) notation to include popups referring to a column of the data.

I don't think that what you get using RSONIO is any closer to a valid geojson.

Best
Tim

On Tue, Jul 9, 2019 at 3:28 PM Benjamin Pauget <benjamin.pauget at tesora.fr<mailto:benjamin.pauget at tesora.fr>> wrote:
Dear Tim,
Tanks you soooo much for your response !
I am a beginner and working with geospatial is still unclear.

I have tried to open my json file with ?from_JSON? and I have another format of my data :
url <- "http://www.georisques.gouv.fr/api/v1/sis?rayon=5000&latlon=4.854899%2C%2045.763079&page=1&page_size=10"
geojson4 <- RJSONIO::fromJSON(url)

Do you think this format is more relevant and will allow to display the pop?up info ?

Thank you again for your code (I just pass the two last day on trying to display the polygons) !

Best regards,
 Ben




De : Tim Salabim <tim.appelhans at gmail.com<mailto:tim.appelhans at gmail.com>>
Envoy? : mardi 9 juillet 2019 15:14
? : Benjamin Pauget <benjamin.pauget at tesora.fr<mailto:benjamin.pauget at tesora.fr>>
Cc : r-sig-geo at r-project.org<mailto:r-sig-geo at r-project.org>
Objet : Re: [R-sig-Geo] geoJSON and leaflet

Hi Benjamin,
What you get back from the jsonlite::fromJSON() call is a simplified list with the data (including the geometry information) as a data frame in one of the list slots. GeoJson is usually a character string. Therefore, if you open your map in the browser and open the console (Ctrl + i) you will see the error message: "Invalid GeoJson object".

I am no expert on geojson structure, but it seems that the data that you request is not in standard format. I tried a few different ways of parsing the data to a valid GeoJson string but did not have success. Maybe someone else with more insight has some helpful ideas how to achieve this.

However, I found a workaround to get the polygons shown on the map using library(sf) - see code below. Note, this only visualises the geometry information only, so direct popup queries via "~" are not possible.

library(leaflet)
library(jsonlite)
library(sf)

url <- "http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10"

geojson <- jsonlite::fromJSON(url)

# convert coordinate arrays to matrices
geom = lapply(geojson$data$geom$coordinates, matrix, ncol = 2)
# create multipolygon from coordinate matrices
geom = st_cast(st_polygon(geom), "MULTIPOLYGON")

leaflet() %>%
  addTiles()%>%
  setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
  addMarkers(lng = 4.872536, lat = 45.758321)%>%
  addPolygons(data = geom)

HTH,
Tim



On Tue, Jul 9, 2019 at 1:49 PM Benjamin Pauget <benjamin.pauget at tesora.fr<mailto:benjamin.pauget at tesora.fr>> wrote:
Hi,
I?m writting because I have some trouble with a geoJSON file and a leaflet.
I?m trying to display polygon on a leaflet map, but nothing append ?
I have no error message.
Do you have some advice/ideas?

Best regards


Here is my code :
library(leaflet)
library(jsonlite)
url <- "http://www.georisques.gouv.fr/api/v1/sis?rayon=1000&latlon=4.854899%2C%2045.763079&page=1&page_size=10"
geojson <- jsonlite::fromJSON(url)
leaflet() %>%
  addTiles()%>%
  setView(lng = 4.854899, lat = 45.763079, zoom = 14) %>%
  addMarkers(lng = 4.872536, lat = 45.758321)%>%
  addGeoJSON(geojson) # doesn?t work by using geojson$data or geojson$data$geom




Benjamin PAUGET
Responsable R&D

[cid:image005.png at 01D4F3BB.3531D840]


+33 (0)1 81 94 13 70

+33 (0)6 47 01 85 92

Le Visium

22 Av Aristide Briand

94110 ARCUEIL

https://tesora.fr/
Linkedin<https://www.linkedin.com/company/tesora-france>



_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org<mailto:R-sig-Geo at r-project.org>
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190709/8037ee6a/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 5152 bytes
Desc: image001.png
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190709/8037ee6a/attachment.png>

From pur@n|k@@m|th@ @end|ng |rom gm@||@com  Thu Jul 11 05:15:39 2019
From: pur@n|k@@m|th@ @end|ng |rom gm@||@com (Amitha Puranik)
Date: Thu, 11 Jul 2019 08:45:39 +0530
Subject: [R-sig-Geo] LR1.sarlm specifications
Message-ID: <CANcbSEgDVsFjHR8TkzD2FCVab25WX3bTPL8F6YWpeLWzW2z2Zw@mail.gmail.com>

Hi Prof. Roger,

I am using the approach proposed by Prof Paul Erhorst for choosing a
spatial model in his paper '*Applied spatial econometrics: raising the bar*'
.
As per the strategy, one has to check the likelihood ratio test for theta
(spatial autocorrelation in exogenous (independent) variables) and also in
theta+rho*beta (spatial autocorrelation in residuals).
Suppose I fit a spatial durbin model and use the code LR1.sarlm(sp.dm), how
would I know whether the likelihood ratio test checks for autocorrelation
in dependent variable or autocorrelation in the independent variable?

Thanks in advance.
Amitha Puranik.

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Jul 11 09:27:50 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 11 Jul 2019 09:27:50 +0200
Subject: [R-sig-Geo] LR1.sarlm specifications
In-Reply-To: <CANcbSEgDVsFjHR8TkzD2FCVab25WX3bTPL8F6YWpeLWzW2z2Zw@mail.gmail.com>
References: <CANcbSEgDVsFjHR8TkzD2FCVab25WX3bTPL8F6YWpeLWzW2z2Zw@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1907110917120.9807@reclus.nhh.no>

On Thu, 11 Jul 2019, Amitha Puranik wrote:

> Hi Prof. Roger,
>
> I am using the approach proposed by Prof Paul Erhorst for choosing a 
> spatial model in his paper '*Applied spatial econometrics: raising the 
> bar*' . As per the strategy, one has to check the likelihood ratio test 
> for theta (spatial autocorrelation in exogenous (independent) variables) 
> and also in theta+rho*beta (spatial autocorrelation in residuals). 
> Suppose I fit a spatial durbin model and use the code LR1.sarlm(sp.dm), 
> how would I know whether the likelihood ratio test checks for 
> autocorrelation in dependent variable or autocorrelation in the 
> independent variable?

The spatialreg::LR1.sarlm() test simply between the fitted model and the 
same model assuming the spatial coefficients are zero, so it only tests 
the possible benefit of including (a) spatial process(es). 
spatialreg::LR.sarlm() lets you test between nested models, and works like 
lmtest::lrtest(). The models need to be nested, so you can test SDM/SLM, 
SDM/SEM (equivalent to a Common Factor test), and so on, but only if the 
models nest (not SEM/SLM, because they do not nest).

Hope this helps,

Roger

>
> Thanks in advance.
> Amitha Puranik.
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Jul 11 09:32:55 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 11 Jul 2019 09:32:55 +0200
Subject: [R-sig-Geo] PhD course ECS530, Bergen, 2-7 December
Message-ID: <alpine.LFD.2.21.1906071421460.3732@reclus.nhh.no>

A PhD-level course in spatial data analysis (ECS530, Auditorium C) will be 
held 2-7 December 2019 in Bergen:

https://www.nhh.no/en/courses/analysing-spatial-data/

External participants should apply using this form:

https://www.nhh.no/en/study-programmes/phd-programme-at-nhh/phd-courses/application-to-follow-phd-courses-at-nhh/

For further details see the course page. Participants must cover their 
own travel and living costs; no support is offered, I'm afraid, apart from 
free tuition.

Roger

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From pur@n|k@@m|th@ @end|ng |rom gm@||@com  Thu Jul 11 16:33:49 2019
From: pur@n|k@@m|th@ @end|ng |rom gm@||@com (Amitha Puranik)
Date: Thu, 11 Jul 2019 20:03:49 +0530
Subject: [R-sig-Geo] LR1.sarlm specifications
In-Reply-To: <alpine.LFD.2.21.1907110917120.9807@reclus.nhh.no>
References: <CANcbSEgDVsFjHR8TkzD2FCVab25WX3bTPL8F6YWpeLWzW2z2Zw@mail.gmail.com>
 <alpine.LFD.2.21.1907110917120.9807@reclus.nhh.no>
Message-ID: <CANcbSEgw9A3Lmtta9uWqQiPCZDvLctJaOE2SrpuV5N5XL8x=0w@mail.gmail.com>

Dear Prof. Roger Bivand,

Thank you very much for clarifying my doubt. This helped me a great deal!

















On Thu, Jul 11, 2019 at 12:58 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Thu, 11 Jul 2019, Amitha Puranik wrote:
>
> > Hi Prof. Roger,
> >
> > I am using the approach proposed by Prof Paul Erhorst for choosing a
> > spatial model in his paper '*Applied spatial econometrics: raising the
> > bar*' . As per the strategy, one has to check the likelihood ratio test
> > for theta (spatial autocorrelation in exogenous (independent) variables)
> > and also in theta+rho*beta (spatial autocorrelation in residuals).
> > Suppose I fit a spatial durbin model and use the code LR1.sarlm(sp.dm),
> > how would I know whether the likelihood ratio test checks for
> > autocorrelation in dependent variable or autocorrelation in the
> > independent variable?
>
> The spatialreg::LR1.sarlm() test simply between the fitted model and the
> same model assuming the spatial coefficients are zero, so it only tests
> the possible benefit of including (a) spatial process(es).
> spatialreg::LR.sarlm() lets you test between nested models, and works like
> lmtest::lrtest(). The models need to be nested, so you can test SDM/SLM,
> SDM/SEM (equivalent to a Common Factor test), and so on, but only if the
> models nest (not SEM/SLM, because they do not nest).
>
> Hope this helps,
>
> Roger
>
> >
> > Thanks in advance.
> > Amitha Puranik.
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From em@nue|e@b@rc@ @end|ng |rom b@@|r@@@cnr@|t  Sat Jul 13 07:32:22 2019
From: em@nue|e@b@rc@ @end|ng |rom b@@|r@@@cnr@|t (Emanuele Barca)
Date: Sat, 13 Jul 2019 07:32:22 +0200
Subject: [R-sig-Geo] problems in anisotropy detection
Message-ID: <8c68bf3be61c018804884c8decefab2f@ba.irsa.cnr.it>

Dear friends,

I have a dataset of hydraulic heads, mydata = <X, Y, HH, Z> and i would 
like to check for anisotropy.

In R I found 3 functions to carry out such task:

1. likfit

x_geodata <- as.geodata(mydata, coord.cols = 1:2, data.col = 3,
                         covar.col = 4)
fit_mle  <- likfit(x_geodata,
                    fix.nugget = FALSE,
                    cov.model = "exponential", psiA = 0, psiR = 1,
                    ini = c(var(Y), 1), fix.psiA = FALSE, fix.psiR = 
FALSE,
                    hessian = TRUE)

that detects no anisotropy.

2. estimateAnisotropy

mydata.sp <- mydata
coordinates(mydata.sp) = ~ X + Y
estimateAnisotropy(mydata.sp, depVar = "LivStat", "LivStat ~ Z")

that returns the following

[generalized least squares trend estimation]
$`ratio`
[1] 1.340775

$direction
[1] -35.29765

$Q
               Q11          Q22          Q12
[1,] 1.926136e-05 2.329241e-05 5.721893e-06

$doRotation
[1] TRUE

finally,

3. estiStAni

vmod1 <- fit.variogram(vgm1, vgm(18, "Ste", 1300, 0.78, kappa = 1.7))
estiStAni(vgm1, c(10, 150), "vgm", vmod1)

that returns an error:

Error in `$<-.data.frame`(`*tmp*`, "dir.hor", value = 0) :
   replacement has 1 row, data has 0.

I am very puzzled, can anyone help me understanding if there is 
anisotropy in my dataset?

thanks

emanuele


From @trev|@@n| @end|ng |rom |u@v@|t  Sat Jul 13 09:43:26 2019
From: @trev|@@n| @end|ng |rom |u@v@|t (Sebastiano Trevisani)
Date: Sat, 13 Jul 2019 09:43:26 +0200
Subject: [R-sig-Geo] problems in anisotropy detection
In-Reply-To: <8c68bf3be61c018804884c8decefab2f@ba.irsa.cnr.it>
References: <8c68bf3be61c018804884c8decefab2f@ba.irsa.cnr.it>
Message-ID: <CAC8_bdEzKdtRPQwRt2nfuCnFei03b5xV3bmgA0dJgQ9OvAAe0Q@mail.gmail.com>

Dear Emanuele,

Did you first checked manually the presence of anisotropy with gstat
variogram function? Then consider also that automatic fitting procedure
generally are not capable to fit a zonal anisotropy but only a geometric
one.

Sebastiano


Il sab 13 lug 2019, 07:32 Emanuele Barca <emanuele.barca at ba.irsa.cnr.it> ha
scritto:

> Dear friends,
>
> I have a dataset of hydraulic heads, mydata = <X, Y, HH, Z> and i would
> like to check for anisotropy.
>
> In R I found 3 functions to carry out such task:
>
> 1. likfit
>
> x_geodata <- as.geodata(mydata, coord.cols = 1:2, data.col = 3,
>                          covar.col = 4)
> fit_mle  <- likfit(x_geodata,
>                     fix.nugget = FALSE,
>                     cov.model = "exponential", psiA = 0, psiR = 1,
>                     ini = c(var(Y), 1), fix.psiA = FALSE, fix.psiR =
> FALSE,
>                     hessian = TRUE)
>
> that detects no anisotropy.
>
> 2. estimateAnisotropy
>
> mydata.sp <- mydata
> coordinates(mydata.sp) = ~ X + Y
> estimateAnisotropy(mydata.sp, depVar = "LivStat", "LivStat ~ Z")
>
> that returns the following
>
> [generalized least squares trend estimation]
> $`ratio`
> [1] 1.340775
>
> $direction
> [1] -35.29765
>
> $Q
>                Q11          Q22          Q12
> [1,] 1.926136e-05 2.329241e-05 5.721893e-06
>
> $doRotation
> [1] TRUE
>
> finally,
>
> 3. estiStAni
>
> vmod1 <- fit.variogram(vgm1, vgm(18, "Ste", 1300, 0.78, kappa = 1.7))
> estiStAni(vgm1, c(10, 150), "vgm", vmod1)
>
> that returns an error:
>
> Error in `$<-.data.frame`(`*tmp*`, "dir.hor", value = 0) :
>    replacement has 1 row, data has 0.
>
> I am very puzzled, can anyone help me understanding if there is
> anisotropy in my dataset?
>
> thanks
>
> emanuele
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From @tu@rt@reece @end|ng |rom b|gpond@com  Mon Jul 15 23:32:34 2019
From: @tu@rt@reece @end|ng |rom b|gpond@com (Stuart Reece)
Date: Tue, 16 Jul 2019 07:32:34 +1000
Subject: [R-sig-Geo] Including Alaska and Hawaii in listw Specifications of
 US States and Substate Regions
Message-ID: <!&!AAAAAAAAAAAuAAAAAAAAACvIFxYXnARFpK90t5CVwycBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAA+yW//9ECLR42Tq2VQLkghAQAAAAA=@bigpond.com>

Hi,

 

I was wanting to include a listw list of neighbours for USA states which
include Alaska and Hawaii with Alaska a neighbour for Washington and Oregon
states and Hawaii a neighbour for California and the Western continental US
states.

 

Naturally queen relationships fail.

 

The Albers shapefiles at this URL are really useful for general mapping
purposes but result in erroneous relationships with k-nearest neighbours
when this is performed in GeoDa.

These maps have Hawaii and Alaska elided to south of Texas.

 

https://github.com/hrbrmstr/albersusa

 

When one uses the native map - with unelided states - available from the URL
below - then k-nearest neighbours also does not work properly in GeoDa.

 

 

I also want to extend this to the SAMHSA substate shapefile found at this
URL for the 395 substate areas used by the USA National Survey of Drug Use
and Health within SAMHSA.

 

https://www.samhsa.gov/data/report/2014-2016-nsduh-substate-region-shapefile

 

I did see that someone was able to create relationships across a straight
for Italy although I am not quite sure how this was accomplished
technically.

 

I would be grateful for any help you could provide.

 

Thankyou so much,

 

Stuart Reece.


	[[alternative HTML version deleted]]


From D@v|d@H|ne @end|ng |rom |@nd@ndw@ter@com@@u  Wed Jul 17 07:24:56 2019
From: D@v|d@H|ne @end|ng |rom |@nd@ndw@ter@com@@u (David Hine)
Date: Wed, 17 Jul 2019 15:24:56 +1000
Subject: [R-sig-Geo] individual plots for farm fields in mapview
In-Reply-To: <02c06c495cbb411c8cd9d72ce6dcf174@tesora.fr>
References: <d86e6b53edbd45709c7937af7c0db494@tesora.fr>
 <CAMbH+HiRhyDuVtmgMEXKmphKqoPkGyha4mTy23JV79A52Lgrkg@mail.gmail.com>
 <27d3bbc072ae4f90b32ad7a53a55457d@tesora.fr>
 <CAMbH+HjK2L2mwtORZFWQsEnHP1YbcY8ghv7qfCv+L9Pcz+ptfQ@mail.gmail.com>
 <02c06c495cbb411c8cd9d72ce6dcf174@tesora.fr>
Message-ID: <3e3ba8b9-ebbb-eee0-d2ac-15c9a7a754fc@landandwater.com.au>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190717/517f1298/attachment.html>

From @tu@rt@reece @end|ng |rom b|gpond@com  Wed Jul 17 11:16:35 2019
From: @tu@rt@reece @end|ng |rom b|gpond@com (Stuart Reece)
Date: Wed, 17 Jul 2019 19:16:35 +1000
Subject: [R-sig-Geo] FW: Including Alaska and Hawaii in listw Specifications
 of US States and Substate Regions
References: <!&!AAAAAAAAAAAuAAAAAAAAACvIFxYXnARFpK90t5CVwycBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAA+yW//9ECLR42Tq2VQLkghAQAAAAA=@bigpond.com>
Message-ID: <007e01d53c80$558ef3d0$00acdb70$@bigpond.com>

Hi,

 

 

I was wanting to include a listw list of neighbours for USA states which
include Alaska and Hawaii with Alaska a neighbour for Washington and Oregon
states and Hawaii a neighbour for California and the Western continental US
states.

 

Naturally queen relationships fail.

 

The Albers shapefiles at this URL are really useful for general mapping
purposes but result in erroneous relationships with k-nearest neighbours
when this is performed in GeoDa.

 

These maps have Hawaii and Alaska elided to south of Texas.

 

 <https://github.com/hrbrmstr/albersusa>
https://github.com/hrbrmstr/albersusa 

 

When one uses the native map - with unelided states - available from the URL
below - then k-nearest neighbours also does not work properly in GeoDa.

 

I also want to extend this to the SAMHSA substate shapefile found at this
URL for the 395 substate areas used by the USA National Survey of Drug Use
and Health within SAMHSA.

 

 
<https://www.samhsa.gov/data/report/2014-2016-nsduh-substate-region-shapefil
e>
https://www.samhsa.gov/data/report/2014-2016-nsduh-substate-region-shapefile


 

I did see that someone was able to create relationships across a straight
for Italy although I am not quite sure how this was accomplished
technically. 

 

I would be grateful for any help you could provide.

 

Thankyou so much,

 

Stuart Reece.

 

 

                [[alternative HTML version deleted]]

 

_______________________________________________

R-sig-Geo mailing list

 <mailto:R-sig-Geo at r-project.org> R-sig-Geo at r-project.org

 <https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Jul 17 12:02:47 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 17 Jul 2019 12:02:47 +0200
Subject: [R-sig-Geo] 
 FW: Including Alaska and Hawaii in listw Specifications
 of US States and Substate Regions
In-Reply-To: <007e01d53c80$558ef3d0$00acdb70$@bigpond.com>
References: <!&!AAAAAAAAAAAuAAAAAAAAACvIFxYXnARFpK90t5CVwycBAMO2jhD3dRHOtM0AqgC7tuYAAAAAAA4AABAAAAA+yW//9ECLR42Tq2VQLkghAQAAAAA=@bigpond.com>
 <007e01d53c80$558ef3d0$00acdb70$@bigpond.com>
Message-ID: <alpine.LFD.2.21.1907171150200.13771@reclus.nhh.no>

On Wed, 17 Jul 2019, Stuart Reece wrote:

> Hi,
>
> I was wanting to include a listw list of neighbours for USA states which
> include Alaska and Hawaii with Alaska a neighbour for Washington and Oregon
> states and Hawaii a neighbour for California and the Western continental US
> states.
>
> Naturally queen relationships fail.
>
> The Albers shapefiles at this URL are really useful for general mapping
> purposes but result in erroneous relationships with k-nearest neighbours
> when this is performed in GeoDa.
>
>
>
> These maps have Hawaii and Alaska elided to south of Texas.
>
>
>
> <https://github.com/hrbrmstr/albersusa>
> https://github.com/hrbrmstr/albersusa
>
>

They are troublesome to install, accessing just the file is impossible.

Did you look at the documentation of spdep::edit.nb()? Reading the list of 
functions and methods in a package tends to be helpful. Use 
spdep::make.sym.nb() after storing the output of edit.nb(). Because 
edit.nb() is interactive, I can't provide a script. The next release of 
spdep will support displaying sf polygons, for now you'd need to use:

usa <- usa_sf("aeqd")
nb <- spdep::poly2nb(usa)
crds <- st_coordinates(st_centroid(usa, of_largest_polygon=TRUE))
nb1 <- spdep::edit.nb(nb, crds, as(usa, "Spatial"))
nb2 <- spdep::make.sym.nb(nb1)

or similar.

Hope this clarifies,

Roger

>
> When one uses the native map - with unelided states - available from the URL
> below - then k-nearest neighbours also does not work properly in GeoDa.
>
>
>
> I also want to extend this to the SAMHSA substate shapefile found at this
> URL for the 395 substate areas used by the USA National Survey of Drug Use
> and Health within SAMHSA.
>
>
>
>
> <https://www.samhsa.gov/data/report/2014-2016-nsduh-substate-region-shapefil
> e>
> https://www.samhsa.gov/data/report/2014-2016-nsduh-substate-region-shapefile
>
>
>
>
> I did see that someone was able to create relationships across a straight
> for Italy although I am not quite sure how this was accomplished
> technically.
>
>
>
> I would be grateful for any help you could provide.
>
>
>
> Thankyou so much,
>
>
>
> Stuart Reece.
>
>
>
>
>
>                [[alternative HTML version deleted]]
>
>
>
> _______________________________________________
>
> R-sig-Geo mailing list
>
> <mailto:R-sig-Geo at r-project.org> R-sig-Geo at r-project.org
>
> <https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From hugo@gco@t@ @end|ng |rom gm@||@com  Mon Jul 22 12:15:08 2019
From: hugo@gco@t@ @end|ng |rom gm@||@com (Hugo Costa)
Date: Mon, 22 Jul 2019 11:15:08 +0100
Subject: [R-sig-Geo] st_simplify outputs the input geometry with no
 simplification
Message-ID: <CADvFi5oPaC6cjbbnS_u9qUabzb-rHzasLhm3KMZQp=GP+x-yQw@mail.gmail.com>

Dear list,

function st_simplify outputs exactly the same geometry as the input. This
is an example:

library(sf)

nc = st_read(system.file("shape/nc.shp", package="sf"))
nc <- nc %>% st_transform(3857)
plot(st_geometry(nc))

nc_simplfy<-st_simplify(nc)
plot(st_geometry(nc_simplfy))

However, I'm quite sure some days ago st_simpliffy was working as expected.
Could it be something wrong in my machine? What am I missing?

> sessionInfo()
R version 3.5.3 (2019-03-11)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows >= 8 x64 (build 9200)

Matrix products: default

locale:
[1] LC_COLLATE=Portuguese_Portugal.1252  LC_CTYPE=Portuguese_Portugal.1252
   LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C
[5] LC_TIME=Portuguese_Portugal.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] sf_0.7-3

loaded via a namespace (and not attached):
 [1] compiler_3.5.3 magrittr_1.5   class_7.3-15   DBI_1.0.0
 tools_3.5.3    units_0.6-2    Rcpp_1.0.1     grid_3.5.3     e1071_1.7-0.1
[10] classInt_0.3-1

	[[alternative HTML version deleted]]


From md@umner @end|ng |rom gm@||@com  Mon Jul 22 16:12:22 2019
From: md@umner @end|ng |rom gm@||@com (Michael Sumner)
Date: Tue, 23 Jul 2019 00:12:22 +1000
Subject: [R-sig-Geo] st_simplify outputs the input geometry with no
 simplification
In-Reply-To: <CADvFi5oPaC6cjbbnS_u9qUabzb-rHzasLhm3KMZQp=GP+x-yQw@mail.gmail.com>
References: <CADvFi5oPaC6cjbbnS_u9qUabzb-rHzasLhm3KMZQp=GP+x-yQw@mail.gmail.com>
Message-ID: <CAAcGz986Dj2kfz9X9vP3bxk0DS=YBupf4zP7JzixkN0EWnMe9Q@mail.gmail.com>

It has changed it, there's now a mix of POLYGON and MULTIPOLYGON
geometries, but no change in the underlying coordinates. You have to use
the dTolerance argument:

nrow(st_coordinates(nc))
[1] 2529
nrow(st_coordinates(st_cast(nc_simplfy)))
[1] 2529
nrow(st_coordinates(st_cast(st_simplify(nc, dTolerance = 1000))))
[1] 1941

It still might change the geometry with dTolerance = 0 (the default), in
that case it will remove vertices that are collinear (unnecessarily dense
straight lines, reduce to two-vertex edges).  I only learnt that recently.

HTH



On Mon, Jul 22, 2019 at 8:15 PM Hugo Costa <hugoagcosta at gmail.com> wrote:

> Dear list,
>
> function st_simplify outputs exactly the same geometry as the input. This
> is an example:
>
> library(sf)
>
> nc = st_read(system.file("shape/nc.shp", package="sf"))
> nc <- nc %>% st_transform(3857)
> plot(st_geometry(nc))
>
> nc_simplfy<-st_simplify(nc)
> plot(st_geometry(nc_simplfy))
>
> However, I'm quite sure some days ago st_simpliffy was working as expected.
> Could it be something wrong in my machine? What am I missing?
>
> > sessionInfo()
> R version 3.5.3 (2019-03-11)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows >= 8 x64 (build 9200)
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=Portuguese_Portugal.1252  LC_CTYPE=Portuguese_Portugal.1252
>    LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C
> [5] LC_TIME=Portuguese_Portugal.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] sf_0.7-3
>
> loaded via a namespace (and not attached):
>  [1] compiler_3.5.3 magrittr_1.5   class_7.3-15   DBI_1.0.0
>  tools_3.5.3    units_0.6-2    Rcpp_1.0.1     grid_3.5.3     e1071_1.7-0.1
> [10] classInt_0.3-1
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


-- 
Michael Sumner
Software and Database Engineer
Australian Antarctic Division
Hobart, Australia
e-mail: mdsumner at gmail.com

	[[alternative HTML version deleted]]


From hugo@gco@t@ @end|ng |rom gm@||@com  Mon Jul 22 16:53:27 2019
From: hugo@gco@t@ @end|ng |rom gm@||@com (Hugo Costa)
Date: Mon, 22 Jul 2019 15:53:27 +0100
Subject: [R-sig-Geo] st_simplify outputs the input geometry with no
 simplification
In-Reply-To: <CAAcGz986Dj2kfz9X9vP3bxk0DS=YBupf4zP7JzixkN0EWnMe9Q@mail.gmail.com>
References: <CADvFi5oPaC6cjbbnS_u9qUabzb-rHzasLhm3KMZQp=GP+x-yQw@mail.gmail.com>
 <CAAcGz986Dj2kfz9X9vP3bxk0DS=YBupf4zP7JzixkN0EWnMe9Q@mail.gmail.com>
Message-ID: <CADvFi5rLVeXSabVA8e2Ei2saxXr4_Cx542DAQBsXHvyuToWRow@mail.gmail.com>

I missed it completely.
Thank you very much!
Hugo

Michael Sumner <mdsumner at gmail.com> escreveu no dia segunda, 22/07/2019
?(s) 15:12:

> It has changed it, there's now a mix of POLYGON and MULTIPOLYGON
> geometries, but no change in the underlying coordinates. You have to use
> the dTolerance argument:
>
> nrow(st_coordinates(nc))
> [1] 2529
> nrow(st_coordinates(st_cast(nc_simplfy)))
> [1] 2529
> nrow(st_coordinates(st_cast(st_simplify(nc, dTolerance = 1000))))
> [1] 1941
>
> It still might change the geometry with dTolerance = 0 (the default), in
> that case it will remove vertices that are collinear (unnecessarily dense
> straight lines, reduce to two-vertex edges).  I only learnt that recently.
>
> HTH
>
>
>
> On Mon, Jul 22, 2019 at 8:15 PM Hugo Costa <hugoagcosta at gmail.com> wrote:
>
>> Dear list,
>>
>> function st_simplify outputs exactly the same geometry as the input. This
>> is an example:
>>
>> library(sf)
>>
>> nc = st_read(system.file("shape/nc.shp", package="sf"))
>> nc <- nc %>% st_transform(3857)
>> plot(st_geometry(nc))
>>
>> nc_simplfy<-st_simplify(nc)
>> plot(st_geometry(nc_simplfy))
>>
>> However, I'm quite sure some days ago st_simpliffy was working as
>> expected.
>> Could it be something wrong in my machine? What am I missing?
>>
>> > sessionInfo()
>> R version 3.5.3 (2019-03-11)
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> Running under: Windows >= 8 x64 (build 9200)
>>
>> Matrix products: default
>>
>> locale:
>> [1] LC_COLLATE=Portuguese_Portugal.1252  LC_CTYPE=Portuguese_Portugal.1252
>>    LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C
>> [5] LC_TIME=Portuguese_Portugal.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] sf_0.7-3
>>
>> loaded via a namespace (and not attached):
>>  [1] compiler_3.5.3 magrittr_1.5   class_7.3-15   DBI_1.0.0
>>  tools_3.5.3    units_0.6-2    Rcpp_1.0.1     grid_3.5.3     e1071_1.7-0.1
>> [10] classInt_0.3-1
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
> --
> Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> Hobart, Australia
> e-mail: mdsumner at gmail.com
>

	[[alternative HTML version deleted]]


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Mon Jul 22 23:36:32 2019
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (Alexandre Santos)
Date: Mon, 22 Jul 2019 21:36:32 +0000 (UTC)
Subject: [R-sig-Geo] Simple Ripley's CRS test for market point patters
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
Message-ID: <1075794206.5056673.1563831392519@mail.yahoo.com>

Dear R-Sig-Geo Members,?? ? I"ve like to find any simple way for apply?CRS test for market point patters, for this I try to create a script below:
#Packages?require(spatstat)require(sp)

# Create some points that represents ant nests in UTMxp<-c(371278.588,371250.722,371272.618,371328.421,371349.974,371311.95,371296.265,371406.46,371411.551,371329.041,371338.081,371334.182,371333.756,371299.818,371254.374,371193.673,371172.836,371173.803,371153.73,371165.051,371140.417,371168.279,371166.367,371180.575,371132.664,371129.791,371232.919,371208.502,371289.462,371207.595,371219.008,371139.921,371133.215,371061.467,371053.69,371099.897,371108.782,371112.52,371114.241,371176.236,371159.185,371159.291,371158.552,370978.252,371120.03,371116.993)
yp<-c(8246507.94,8246493.176,8246465.974,8246464.413,8246403.465,8246386.098,8246432.144,8246394.827,8246366.201,8246337.626,8246311.125,8246300.039,8246299.594,8246298.072,8246379.351,8246431.998,8246423.913,8246423.476,8246431.658,8246418.226,8246400.161,8246396.891,8246394.225,8246400.391,8246370.244,8246367.019,8246311.075,8246255.174,8246255.085,8246226.514,8246215.847,8246337.316,8246330.197,8246311.197,8246304.183,8246239.282,8246239.887,8246241.678,8246240.361,8246167.364,8246171.581,8246171.803,8246169.807,8246293.57,8246183.194,8246189.926)
# Then I create the size of each nest - my covariate used as marked processarea<-c(117,30,4,341,15,160,35,280,108,168,63,143,2,48,182,42,88,56,27,156,288,45,49,234,72,270,91,40,304,56,35,4,56.7,9,4.6,105,133,135,23.92,190,12.9,15.2,192.78,104,255,24)

# Make a countour - only as exerciseW <- convexhull.xy(xp,yp)
#Create a ppp objectp_xy<-cbind(xp,yp)syn.ppp<-ppp(x=coordinates(p_xy)[,1],y=coordinates(p_xy)[,2],window=W, marks=area)syn.ppp <- as.ppp(syn.ppp)plot(syn.ppp, main=" ")
First, I've like to study CSR of market point process (my hypothesis is that different size have a same spatial distribution) when area >= 0, area < 25 and area >=25, area < 55, for this I make:
# Area 0-25env.formi1<-envelope(syn.ppp,nsim=99,fun=Kest, area >= 0, area < 25)plot(env.formi1,lwd=list(3,1,1,1), main="")?
# Area 25-55env.formi2<-envelope(syn.ppp,nsim=99,fun=Kest, area >=25, area < 55)plot(env.formi2,lwd=list(3,1,1,1), main="")?
My first problem is that I have the same plot in both conditions and I don't know why.
Second, if I try to estimate the market intensity observed pattern
est.int <- density(syn.ppp)est_xy <-? rmpoispp(est.int)plot(est_xy, main=" ")
My output is only my points position without marked area in my ppp object created.
My question is what is the problem with this Ripley's reduced second moment function approach? There are any way for study my point process when the area is a covariate of my point process?
Thanks in advanced
Alexandre


-- ======================================================================Alexandre dos SantosProte??o Florestal IFMT - Instituto Federal de Educa??o, Ci?ncia e Tecnologia de Mato GrossoCampus C?ceresCaixa Postal 244Avenida dos Ramires, s/nBairro: Distrito Industrial C?ceres - MT                      CEP: 78.200-000Fone: (+55) 65 99686-6970 (VIVO) (+55) 65 3221-2674 (FIXO)e-mails:alexandresantosbr at yahoo.com.br         alexandre.santos at cas.ifmt.edu.br Lattes: http://lattes.cnpq.br/1360403201088680 OrcID: orcid.org/0000-0001-8232-6722   -   ResearcherID: A-5790-2016Researchgate: www.researchgate.net/profile/Alexandre_Santos10                       LinkedIn: br.linkedin.com/in/alexandre-dos-santos-87961635Mendeley:www.mendeley.com/profiles/alexandre-dos-santos6/
======================================================================



	[[alternative HTML version deleted]]


From m@rce||no@de|@cruz @end|ng |rom urjc@e@  Tue Jul 23 13:26:21 2019
From: m@rce||no@de|@cruz @end|ng |rom urjc@e@ (Marcelino De La Cruz Rot)
Date: Tue, 23 Jul 2019 11:26:21 +0000
Subject: [R-sig-Geo] Simple Ripley's CRS test for market point patters
In-Reply-To: <1075794206.5056673.1563831392519@mail.yahoo.com>
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
 <1075794206.5056673.1563831392519@mail.yahoo.com>
Message-ID: <26d45ed7-703d-2774-6aa8-fe4bdc12598c@urjc.es>

Dear Alexandre,

I think that the solution to your first problem would be something like 
this:

#Area 0-25
env.formi1<-envelope(syn.ppp[syn.ppp$marks>=0 & syn.ppp$marks<25], 
nsim=99,fun=Kest)
plot(env.formi1,lwd=list(3,1,1,1), main="")

Then, for your second problem, you should clarify what do you understand 
by "market intensity observed pattern". In fact, rmpoispp()? simulates 
random? multitype ppp's, i.e., it does not estimate parameters of 
quantitatively marked point processes.

Maybe be you would consider interesting

Smooth.ppp(syn.ppp)

and

envelope(syn.ppp, nsim=99,fun=markcorr, 
simulate=expression(rlabel(syn.ppp)))


Cheers,


Marcelino







El 22/07/2019 a las 23:36, Alexandre Santos via R-sig-Geo escribi?:
> Dear R-Sig-Geo Members,?? ? I"ve like to find any simple way for apply?CRS test for market point patters, for this I try to create a script below:
> #Packages?require(spatstat)require(sp)
>
> # Create some points that represents ant nests in UTMxp<-c(371278.588,371250.722,371272.618,371328.421,371349.974,371311.95,371296.265,371406.46,371411.551,371329.041,371338.081,371334.182,371333.756,371299.818,371254.374,371193.673,371172.836,371173.803,371153.73,371165.051,371140.417,371168.279,371166.367,371180.575,371132.664,371129.791,371232.919,371208.502,371289.462,371207.595,371219.008,371139.921,371133.215,371061.467,371053.69,371099.897,371108.782,371112.52,371114.241,371176.236,371159.185,371159.291,371158.552,370978.252,371120.03,371116.993)
> yp<-c(8246507.94,8246493.176,8246465.974,8246464.413,8246403.465,8246386.098,8246432.144,8246394.827,8246366.201,8246337.626,8246311.125,8246300.039,8246299.594,8246298.072,8246379.351,8246431.998,8246423.913,8246423.476,8246431.658,8246418.226,8246400.161,8246396.891,8246394.225,8246400.391,8246370.244,8246367.019,8246311.075,8246255.174,8246255.085,8246226.514,8246215.847,8246337.316,8246330.197,8246311.197,8246304.183,8246239.282,8246239.887,8246241.678,8246240.361,8246167.364,8246171.581,8246171.803,8246169.807,8246293.57,8246183.194,8246189.926)
> # Then I create the size of each nest - my covariate used as marked processarea<-c(117,30,4,341,15,160,35,280,108,168,63,143,2,48,182,42,88,56,27,156,288,45,49,234,72,270,91,40,304,56,35,4,56.7,9,4.6,105,133,135,23.92,190,12.9,15.2,192.78,104,255,24)
>
> # Make a countour - only as exerciseW <- convexhull.xy(xp,yp)
> #Create a ppp objectp_xy<-cbind(xp,yp)syn.ppp<-ppp(x=coordinates(p_xy)[,1],y=coordinates(p_xy)[,2],window=W, marks=area)syn.ppp <- as.ppp(syn.ppp)plot(syn.ppp, main=" ")
> First, I've like to study CSR of market point process (my hypothesis is that different size have a same spatial distribution) when area >= 0, area < 25 and area >=25, area < 55, for this I make:
> # Area 0-25env.formi1<-envelope(syn.ppp,nsim=99,fun=Kest, area >= 0, area < 25)plot(env.formi1,lwd=list(3,1,1,1), main="")
> # Area 25-55env.formi2<-envelope(syn.ppp,nsim=99,fun=Kest, area >=25, area < 55)plot(env.formi2,lwd=list(3,1,1,1), main="")
> My first problem is that I have the same plot in both conditions and I don't know why.
> Second, if I try to estimate the market intensity observed pattern
> est.int <- density(syn.ppp)est_xy <-? rmpoispp(est.int)plot(est_xy, main=" ")
> My output is only my points position without marked area in my ppp object created.
> My question is what is the problem with this Ripley's reduced second moment function approach? There are any way for study my point process when the area is a covariate of my point process?
> Thanks in advanced
> Alexandre
>
>
> -- ======================================================================Alexandre dos SantosProte??o Florestal IFMT - Instituto Federal de Educa??o, Ci?ncia e Tecnologia de Mato GrossoCampus C?ceresCaixa Postal 244Avenida dos Ramires, s/nBairro: Distrito Industrial C?ceres - MT                      CEP: 78.200-000Fone: (+55) 65 99686-6970 (VIVO) (+55) 65 3221-2674 (FIXO)e-mails:alexandresantosbr at yahoo.com.br         alexandre.santos at cas.ifmt.edu.br Lattes: http://lattes.cnpq.br/1360403201088680 OrcID: orcid.org/0000-0001-8232-6722   -   ResearcherID: A-5790-2016Researchgate: www.researchgate.net/profile/Alexandre_Santos10                       LinkedIn: br.linkedin.com/in/alexandre-dos-santos-87961635Mendeley:www.mendeley.com/profiles/alexandre-dos-santos6/
> ======================================================================
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> .
>

-- 
Marcelino de la Cruz Rot
Depto. de Biolog?a y Geolog?a
F?sica y Qu?mica Inorg?nica
Universidad Rey Juan Carlos
M?stoles Espa?a


From Freder|c@Pon@ @end|ng |rom cerem@@|r  Tue Jul 23 15:47:48 2019
From: Freder|c@Pon@ @end|ng |rom cerem@@|r (PONS Frederic - CEREMA/DTerMed/DREC/SRILH)
Date: Tue, 23 Jul 2019 15:47:48 +0200
Subject: [R-sig-Geo] DELDIR
In-Reply-To: <22a5a38a-1c39-b9f8-de6e-c5d850cc33bb@cerema.fr>
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
 <1075794206.5056673.1563831392519@mail.yahoo.com>
 <26d45ed7-703d-2774-6aa8-fe4bdc12598c@urjc.es>
 <22a5a38a-1c39-b9f8-de6e-c5d850cc33bb@cerema.fr>
Message-ID: <af2400ab-cfec-153c-8ac4-396435b8ef11@cerema.fr>

Dear all

Sometines we are using the deldir function for squeletisation.

Since the last review, deldir stops in some shapefile because polygons 
are too narrow. In the version berfore, there was no problems.

We have readed the " Notes on error messages" and the problem of 
anticlockwise order of triangle is listed.

In the trifnd R function , the code is
# Check that the vertices of the triangle listed in tau are
# in anticlockwise order.  (If they aren't then alles upgefucken
# ist; throw an error.)
call acchk(tau(1),tau(2),tau(3),anticl,x,y,ntot,eps)
if(!anticl) {
     call acchk(tau(3),tau(2),tau(1),anticl,x,y,ntot,eps)
     if(!anticl) {
         call fexit("Both vertex orderings are clockwise. See help for 
deldir.")
     } else {
         ivtmp  = tau(3)
         tau(3) = tau(1)
         tau(1) = ivtmp
     }
}

We don't understand why do not order the bad triangles into the good 
order. Perhaps, if this problem appears, the beginning of the deldir 
function is not good.

If someone can explain us.

Best regards

*Fr?d?ric Pons
*

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Wed Jul 24 01:39:12 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 24 Jul 2019 11:39:12 +1200
Subject: [R-sig-Geo] DELDIR
In-Reply-To: <af2400ab-cfec-153c-8ac4-396435b8ef11@cerema.fr>
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
 <1075794206.5056673.1563831392519@mail.yahoo.com>
 <26d45ed7-703d-2774-6aa8-fe4bdc12598c@urjc.es>
 <22a5a38a-1c39-b9f8-de6e-c5d850cc33bb@cerema.fr>
 <af2400ab-cfec-153c-8ac4-396435b8ef11@cerema.fr>
Message-ID: <ea4a3dbd-80f3-c685-3110-f10f64a2b21f@auckland.ac.nz>


This question should have been, in first instance, directed to me (the 
maintainer of the deldir package), rather than to the r-sig-geo list.

On 24/07/19 1:47 AM, PONS Frederic - CEREMA/DTerMed/DREC/SRILH wrote:

> Dear all
> 
> Sometines we are using the deldir function for squeletisation.

Had to Google this word. According to what I see it should be spelled
"squelettisation" (with a double "t").  The English word is 
"skeletonisation", and this list is (for better or for worse) an English 
language list.

> Since the last review, deldir stops in some shapefile because polygons
> are too narrow. In the version berfore, there was no problems.
> 
> We have readed the " Notes on error messages" and the problem of
> anticlockwise order of triangle is listed.
> 
> In the trifnd R function , the code is
> # Check that the vertices of the triangle listed in tau are
> # in anticlockwise order.  (If they aren't then alles upgefucken
> # ist; throw an error.)
> call acchk(tau(1),tau(2),tau(3),anticl,x,y,ntot,eps)
> if(!anticl) {
>       call acchk(tau(3),tau(2),tau(1),anticl,x,y,ntot,eps)
>       if(!anticl) {
>           call fexit("Both vertex orderings are clockwise. See help for
> deldir.")
>       } else {
>           ivtmp  = tau(3)
>           tau(3) = tau(1)
>           tau(1) = ivtmp
>       }
> }
> 
> We don't understand why do not order the bad triangles into the good
> order. Perhaps, if this problem appears, the beginning of the deldir
> function is not good.
> 
> If someone can explain us.

The error message you quote indicates that *both* orderings are bad so 
something is toadally out of whack.  It would be rash (and arrogant) for 
the software to fool around with the structure.  In such situations the 
user should diagnose what is going wrong.

If you cannot diagnose the problem, please send me a reprex and I will 
look into it.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@turner @end|ng |rom @uck|@nd@@c@nz  Wed Jul 24 04:00:49 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 24 Jul 2019 14:00:49 +1200
Subject: [R-sig-Geo] [FORGED] Simple Ripley's CRS test for market point
 patters
In-Reply-To: <1075794206.5056673.1563831392519@mail.yahoo.com>
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
 <1075794206.5056673.1563831392519@mail.yahoo.com>
Message-ID: <8a7d0858-37ea-de97-b988-5f29dfcc38c8@auckland.ac.nz>


Thanks to Marcelino for chiming in on this.  I should have responded 
earlier, but was "busy on a personal matter".

To add to what Marcelino said:

(1) Your post in HTML was horribly garbled and a struggle to read. 
*PLEASE* set your email client so that it does *NOT* post in HTML when 
posting to this list.

(2) A very minor point:  Your construction of "syn.ppp" was 
unnecessarily complicated and convoluted.  You could simply do:

     syn.ppp  <- ppp(x=xp,y=yp,window=W,marks=area)

(3) You appear to be confused as to the distinction between "marks" and 
"covariates".   These are superficially similar but conceptually 
completely different.  What you are dealing with is *marks*.  There are
no covariates involved.  See [1], p. 147.

(4) Your calls to envelope() are mal-formed; the expression "area >= 0" 
and "area < 25" will be taken as the values of "nrank" and ... who knows 
what?  Did you not notice the warning messages you got about there being 
something wrong with "nrank"?

You are being hopelessly na?ve in expecting envelope() to interpret 
"area >= 0" and "area < 25" in the way you want it to interpret them.
The spatstat package does not read minds.

Marcelino has shown you how to make a proper call.

(5) Since you are interested in categorising "area" into groups, rather 
than being interested in the *numerical* value of area, you should do 
the categorisation explicitly:

acat <- cut(area,breaks=c(-Inf,25,55,Inf),right=FALSE,
             labels=c("s","m","l")
# right=FALSE since you want "area < 25" rather than
# "area <= 25" --- although this makes no difference for the
# area values actually used.

syn.ppp <- ppp(x=xp,y=yp,marks=acat)

(6) It is not clear to me what your "research question" is.  Do you want 
to test whether the intensities differ between the area categories? 
Unless my thinking is hopelessly confused, this has nothing to do with 
(or at least does not require the use of) the envelope() function:

f1 <- ppm(syn.ppp) # Same (constant) intensity for each area category.
f2 <- ppm(syn.ppp ~ marks) # Allows different (constant) intensity
                            # for each area category.
anova(f1,f2,test="Chi")

This test (not surprisingly) rejects the hypothesis that the intensities 
are the same; p-value = 0.0020.

If this is not the hypothesis that you are interested in, please clarify 
your thinking and your question, and get back to us.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

[1] Spatial Point Patterns: Methodology and Applications with R
1st Edition, Adrian Baddeley, Ege Rubak, Rolf Turner
Chapman and Hall/CRC, 2015

P. S.  It is of course (!!!) highly recommended that you spend some time 
reading the book cited above, if you are going to work in this area.

R. T.

On 23/07/19 9:36 AM, Alexandre Santos via R-sig-Geo wrote:
> Dear R-Sig-Geo Members,?? ? I"ve like to find any simple way for apply?CRS test for market point patters, for this I try to create a script below:
> #Packages?require(spatstat)require(sp)
> 
> # Create some points that represents ant nests in UTMxp<-c(371278.588,371250.722,371272.618,371328.421,371349.974,371311.95,371296.265,371406.46,371411.551,371329.041,371338.081,371334.182,371333.756,371299.818,371254.374,371193.673,371172.836,371173.803,371153.73,371165.051,371140.417,371168.279,371166.367,371180.575,371132.664,371129.791,371232.919,371208.502,371289.462,371207.595,371219.008,371139.921,371133.215,371061.467,371053.69,371099.897,371108.782,371112.52,371114.241,371176.236,371159.185,371159.291,371158.552,370978.252,371120.03,371116.993)
> yp<-c(8246507.94,8246493.176,8246465.974,8246464.413,8246403.465,8246386.098,8246432.144,8246394.827,8246366.201,8246337.626,8246311.125,8246300.039,8246299.594,8246298.072,8246379.351,8246431.998,8246423.913,8246423.476,8246431.658,8246418.226,8246400.161,8246396.891,8246394.225,8246400.391,8246370.244,8246367.019,8246311.075,8246255.174,8246255.085,8246226.514,8246215.847,8246337.316,8246330.197,8246311.197,8246304.183,8246239.282,8246239.887,8246241.678,8246240.361,8246167.364,8246171.581,8246171.803,8246169.807,8246293.57,8246183.194,8246189.926)
> # Then I create the size of each nest - my covariate used as marked processarea<-c(117,30,4,341,15,160,35,280,108,168,63,143,2,48,182,42,88,56,27,156,288,45,49,234,72,270,91,40,304,56,35,4,56.7,9,4.6,105,133,135,23.92,190,12.9,15.2,192.78,104,255,24)
> 
> # Make a countour - only as exerciseW <- convexhull.xy(xp,yp)
> #Create a ppp objectp_xy<-cbind(xp,yp)syn.ppp<-ppp(x=coordinates(p_xy)[,1],y=coordinates(p_xy)[,2],window=W, marks=area)syn.ppp <- as.ppp(syn.ppp)plot(syn.ppp, main=" ")
> First, I've like to study CSR of market point process (my hypothesis is that different size have a same spatial distribution) when area >= 0, area < 25 and area >=25, area < 55, for this I make:
> # Area 0-25env.formi1<-envelope(syn.ppp,nsim=99,fun=Kest, area >= 0, area < 25)plot(env.formi1,lwd=list(3,1,1,1), main="")
> # Area 25-55env.formi2<-envelope(syn.ppp,nsim=99,fun=Kest, area >=25, area < 55)plot(env.formi2,lwd=list(3,1,1,1), main="")
> My first problem is that I have the same plot in both conditions and I don't know why.
> Second, if I try to estimate the market intensity observed pattern
> est.int <- density(syn.ppp)est_xy <-? rmpoispp(est.int)plot(est_xy, main=" ")
> My output is only my points position without marked area in my ppp object created.
> My question is what is the problem with this Ripley's reduced second moment function approach? There are any way for study my point process when the area is a covariate of my point process?
> Thanks in advanced
> Alexandre


From geo|@|rz @end|ng |rom nu@@edu@@g  Wed Jul 24 10:03:03 2019
From: geo|@|rz @end|ng |rom nu@@edu@@g (Letisha Sarah Fong Rui Zhen)
Date: Wed, 24 Jul 2019 08:03:03 +0000
Subject: [R-sig-Geo] Calculating impact measures for spatial panel models
 and trouble specifying SDEM model using spml
Message-ID: <SG2PR06MB387093D5FF97FD899829FB92EEC60@SG2PR06MB3870.apcprd06.prod.outlook.com>

Dear all,

I?m working with panel data of 9 countries and 18 years and I?m running fixed effects SDEM, SLX and SEM with the splm package.

I have three questions:

1. I can?t seem to get the SDEM model to run. My code for each of the 3 models is:

model.SDEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE + slag(ln(GDP), listw = my.listw) +  slag((ln(GDP))^2, listw = my.listw), data = my.data, listw = my.listw, model = ?within?, effect = ?individual?, lag = F, spatial.error = ?b?)

model.SLX <- plm(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE + slag(ln(GDP), listw = my.listw) +  slag((ln(GDP))^2, listw = my.listw), data = my.data, model = ?within?, effect = ?individual?)

model.SEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE, data = my.data, listw = my.listw, model = ?within?, effect = ?individual?, lag = F, spatial.error = ?b?)

I am able to run both SLX and SEM models without problem, but when I try to run the SDEM model, the error message is:

Error in UseMethod("slag") :
  no applicable method for 'slag' applied to an object of class "c('double', 'numeric')"

I don?t understand what is wrong here, as I have no problems with the slag() function in the SLX model. My data is a pdataframe and each variable is a numeric pseries.


2. How can I calculate impact measures (direct, indirect and total) for spatial panel models?

The impacts() function in spdep doesn?t work anymore and the impacts() function from the spatialreg package seems to work only for cross-sectional data and not panel data.

For example, I ran:

spatialreg::impacts(model.SLX)

And the error message is:

Error in UseMethod("impacts", obj) :
  no applicable method for 'impacts' applied to an object of class "c('plm', 'panelmodel')"

I have tried methods(impacts) but none of the functions seem to work for my SLX model created with the splm package.

I also looked at some previous examples in the splm documentation and more specifically the spml() function and the example provided (specifically the impact measures) doesn?t work anymore:

data(Produc, package = "plm")
data(usaww)
fm <- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
## random effects panel with spatial lag
respatlag <- spml(fm, data = Produc, listw = mat2listw(usaww),
                  model="random", spatial.error="none", lag=TRUE)
summary(respatlag)
## calculate impact measures
impac1 <- impacts(respatlag, listw = mat2listw(usaww, style = "W"), time = 17)
summary(impac1, zstats=TRUE, short=TRUE)

The error message when I run the impacts() function is:

Error in UseMethod("impacts", obj) :
  no applicable method for 'impacts' applied to an object of class "splm"

My question is therefore, how do I go about calculating direct, indirect and total impact measures for spatial panel data?


3. How can I test if the SDEM model can be simplified to the SLX model (since I estimate the SDEM by maximum likelihood (spml function) and the SLX by ordinary linear regression (plm function))? From my understanding the plm() function does not compute a loglikelihood or AIC so I probably can?t do a likelihood ratio test to choose between models (I haven?t tried this out because I?m stuck at running the SDEM model).

Any help or advice would be greatly appreciated. Thank you.

Best wishes,
Sarah



________________________________

Important: This email is confidential and may be privileged. If you are not the intended recipient, please delete it and notify us immediately; you should not copy or use it for any purpose, nor disclose its contents to any other person. Thank you.

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Jul 24 15:50:13 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 24 Jul 2019 15:50:13 +0200
Subject: [R-sig-Geo] 
 Calculating impact measures for spatial panel models
 and trouble specifying SDEM model using spml
In-Reply-To: <SG2PR06MB387093D5FF97FD899829FB92EEC60@SG2PR06MB3870.apcprd06.prod.outlook.com>
References: <SG2PR06MB387093D5FF97FD899829FB92EEC60@SG2PR06MB3870.apcprd06.prod.outlook.com>
Message-ID: <alpine.LFD.2.21.1907241534380.16556@reclus.nhh.no>

On Wed, 24 Jul 2019, Letisha Sarah Fong Rui Zhen wrote:

> Dear all,

Please do not post HTML-formated messages.

>
> I???m working with panel data of 9 countries and 18 years and I???m 
> running fixed effects SDEM, SLX and SEM with the splm package.
>
> I have three questions:
>
> 1. I can???t seem to get the SDEM model to run. My code for each of the
>    3 models is:
>
> model.SDEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE + 
> slag(ln(GDP), listw = my.listw) + slag((ln(GDP))^2, listw = my.listw), 
> data = my.data, listw = my.listw, model = ???within???, effect = 
> ???individual???, lag = F, spatial.error = ???b???)
>
> model.SLX <- plm(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE + slag(ln(GDP), 
> listw = my.listw) + slag((ln(GDP))^2, listw = my.listw), data = my.data, 
> model = ???within???, effect = ???individual???)
>
> model.SEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE, data = 
> my.data, listw = my.listw, model = ???within???, effect = 
> ???individual???, lag = F, spatial.error = ???b???)
>
> I am able to run both SLX and SEM models without problem, but when I try 
> to run the SDEM model, the error message is:
>
> Error in UseMethod("slag") :
>  no applicable method for 'slag' applied to an object of class
>  "c('double', 'numeric')"
>
> I don???t understand what is wrong here, as I have no problems with the 
> slag() function in the SLX model. My data is a pdataframe and each 
> variable is a numeric pseries.

My guess would be that you should protect your square with I() in general, 
but have no idea - this is not a reproducible example.

>
>
> 2. How can I calculate impact measures (direct, indirect and total) for
>    spatial panel models?
>
> The impacts() function in spdep doesn???t work anymore and the impacts() 
> function from the spatialreg package seems to work only for 
> cross-sectional data and not panel data.
>
> For example, I ran:
>
> spatialreg::impacts(model.SLX)
>
> And the error message is:
>
> Error in UseMethod("impacts", obj) :
>  no applicable method for 'impacts' applied to an object of class
>  "c('plm', 'panelmodel')"
>
> I have tried methods(impacts) but none of the functions seem to work for 
> my SLX model created with the splm package.

But your SLX model is created with the plm package, isn't it? The only use 
of splm is for the manual lags with slag()?

>
> I also looked at some previous examples in the splm documentation and 
> more specifically the spml() function and the example provided 
> (specifically the impact measures) doesn???t work anymore:
>
> data(Produc, package = "plm")
> data(usaww)
> fm <- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
> ## random effects panel with spatial lag
> respatlag <- spml(fm, data = Produc, listw = mat2listw(usaww),
>                  model="random", spatial.error="none", lag=TRUE) 
> summary(respatlag) ## calculate impact measures impac1 <- 
> impacts(respatlag, listw = mat2listw(usaww, style = "W"), time = 17) 
> summary(impac1, zstats=TRUE, short=TRUE)
>

The implemented impacts methods in splm apply to the case where the lagged 
response is included. For SDEM and SLX, you can get the impacts by 
addition for the total impactss, and by linear combination for their 
standard errors. This is not implemented in a method. Further, slag() does 
not give any impacts method any information on which variables have been 
lagged - in your illustration above EI and RE are not lagged.

> The error message when I run the impacts() function is:
>
> Error in UseMethod("impacts", obj) :
>  no applicable method for 'impacts' applied to an object of class "splm"
>
> My question is therefore, how do I go about calculating direct, indirect 
> and total impact measures for spatial panel data?
>
>
> 3. How can I test if the SDEM model can be simplified to the SLX model
>    (since I estimate the SDEM by maximum likelihood (spml function) and
>    the SLX by ordinary linear regression (plm function))? From my
>    understanding the plm() function does not compute a loglikelihood or
>    AIC so I probably can???t do a likelihood ratio test to choose
>    between models (I haven???t tried this out because I???m stuck at
>    running the SDEM model).

Do you know definitely that plm does not provide a log likelihood? I 
realise that it isn't OLS unless pooled. Have you reviewed the JSS plm and 
splm articles?

Roger

>
> Any help or advice would be greatly appreciated. Thank you.
>
> Best wishes,
> Sarah
>
>
>
> ________________________________
>
> Important: This email is confidential and may be privileged. If you are 
> not the intended recipient, please delete it and notify us immediately; 
> you should not copy or use it for any purpose, nor disclose its contents 
> to any other person. Thank you.
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Thu Jul 25 02:50:42 2019
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Wed, 24 Jul 2019 20:50:42 -0400
Subject: [R-sig-Geo] Simple Ripley's CRS test for market point patters
In-Reply-To: <8a7d0858-37ea-de97-b988-5f29dfcc38c8@auckland.ac.nz>
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
 <1075794206.5056673.1563831392519@mail.yahoo.com>
 <8a7d0858-37ea-de97-b988-5f29dfcc38c8@auckland.ac.nz>
Message-ID: <dd69fad4-192c-ae1c-5096-65b9c2e7f29f@yahoo.com.br>

Thanks for your help Marcelino e for the careful explanation Rolf Turner 
and so sorry about my post script configuration, I expected that I 
solved that in my new post.

First my variable area is a marked point (attribute or auxiliary 
information about my point process - page 7 and not a spatial covariate, 
effect in the outcome of my experimental area - page 50). Based in this 
information, my hypophyses is that the *size of ant nests* a cause of 
ecological intraspecific competition for resources (such as food and 
territory) *have different patterns of spatial distribution*, for this:

#Packages
require(spatstat)
require(sp)

# Create some points that represents ant nests

xp<-c(371278.588,371250.722,371272.618,371328.421,371349.974,
371311.95,371296.265,371406.46,371411.551,371329.041,371338.081,
371334.182,371333.756,371299.818,371254.374,371193.673,371172.836,
371173.803,371153.73,371165.051,371140.417,371168.279,371166.367,
371180.575,371132.664,371129.791,371232.919,371208.502,371289.462,
371207.595,371219.008,371139.921,371133.215,371061.467,371053.69,
371099.897,371108.782,371112.52,371114.241,371176.236,371159.185,
371159.291,371158.552,370978.252,371120.03,371116.993)

yp<-c(8246507.94,8246493.176,8246465.974,8246464.413,8246403.465,
8246386.098,8246432.144,8246394.827,8246366.201,8246337.626,
8246311.125,8246300.039,8246299.594,8246298.072,8246379.351,
8246431.998,8246423.913,8246423.476,8246431.658,8246418.226,
8246400.161,8246396.891,8246394.225,8246400.391,8246370.244,
8246367.019,8246311.075,8246255.174,8246255.085,8246226.514,
8246215.847,8246337.316,8246330.197,8246311.197,8246304.183,
8246239.282,8246239.887,8246241.678,8246240.361,8246167.364,
8246171.581,8246171.803,8246169.807,8246293.57,8246183.194,8246189.926)

#Now I have the size of each nest (marked process)

area<-c(117,30,4,341,15,160,35,280,108,168,63,143,2,48,182,42,
88,56,27,156,288,45,49,234,72,270,91,40,304,56,35,4,56.7,9,4.6,
105,133,135,23.92,190,12.9,15.2,192.78,104,255,24)

# Make a countour for the window creation
W <- convexhull.xy(xp,yp)


# Class of nests size - large, medium and small
acat <- cut(area,breaks=c(-Inf,25,55,Inf),right=FALSE, 
labels=c("s","m","l"))

#Create a ppp object

syn.ppp <- ppp(x=xp,y=yp,window=W, marks=acat)

# Test intensity hypothesis

f1 <- ppm(syn.ppp) # Same (constant) intensity for each area category.
f2 <- ppm(syn.ppp ~ marks) # Allows different (constant) intensity for 
each area category.
anova(f1,f2,test="Chi")

#0.002015 ** OK, the hypothesis that the intensities are the same was 
reject, but intensities is not the question.

Based in my initial hypothesis, I've like to show envelopes and observed 
values of the use of some function for the three point patters (large, 
medium and small ant nest size)under CSR. And for this I try to:

kS<-envelope(syn.ppp[syn.ppp$acat=="s"], nsim=99,fun=Kest)

plot(kS,lwd=list(3,1,1,1), main="")

kM<-envelope(syn.ppp[syn.ppp$acat=="m"], nsim=99,fun=Kest)

plot(kM,lwd=list(3,1,1,1), main="")

kL<-envelope(syn.ppp[syn.ppp$acat=="l"], nsim=99,fun=Kest)

plot(kL,lwd=list(3,1,1,1), main="")

But doesn't work yet. My approach now sounds correct?

Thanks in advanced,

Alexandre


-- 
======================================================================
Alexandre dos Santos
Prote????o Florestal
IFMT - Instituto Federal de Educa????o, Ci??ncia e Tecnologia de Mato Grosso
Campus C??ceres
Caixa Postal 244
Avenida dos Ramires, s/n
Bairro: Distrito Industrial
C??ceres - MT                      CEP: 78.200-000
Fone: (+55) 65 99686-6970 (VIVO) (+55) 65 3221-2674 (FIXO)

         alexandre.santos at cas.ifmt.edu.br
Lattes: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
Researchgate: www.researchgate.net/profile/Alexandre_Santos10
LinkedIn: br.linkedin.com/in/alexandre-dos-santos-87961635
Mendeley:www.mendeley.com/profiles/alexandre-dos-santos6/
======================================================================

Em 23/07/2019 22:00, Rolf Turner escreveu:
>
> Thanks to Marcelino for chiming in on this.?? I should have responded 
> earlier, but was "busy on a personal matter".
>
> To add to what Marcelino said:
>
> (1) Your post in HTML was horribly garbled and a struggle to read. 
> *PLEASE* set your email client so that it does *NOT* post in HTML when 
> posting to this list.
>
> (2) A very minor point:?? Your construction of "syn.ppp" was 
> unnecessarily complicated and convoluted.?? You could simply do:
>
> ?????? syn.ppp?? <- ppp(x=xp,y=yp,window=W,marks=area)
>
> (3) You appear to be confused as to the distinction between "marks" 
> and "covariates".???? These are superficially similar but conceptually 
> completely different.?? What you are dealing with is *marks*.?? There are
> no covariates involved.?? See [1], p. 147.
>
> (4) Your calls to envelope() are mal-formed; the expression "area >= 
> 0" and "area < 25" will be taken as the values of "nrank" and ... who 
> knows what??? Did you not notice the warning messages you got about 
> there being something wrong with "nrank"?
>
> You are being hopelessly na??ve in expecting envelope() to interpret 
> "area >= 0" and "area < 25" in the way you want it to interpret them.
> The spatstat package does not read minds.
>
> Marcelino has shown you how to make a proper call.
>
> (5) Since you are interested in categorising "area" into groups, 
> rather than being interested in the *numerical* value of area, you 
> should do the categorisation explicitly:
>
> acat <- cut(area,breaks=c(-Inf,25,55,Inf),right=FALSE,
> ?????????????????????? labels=c("s","m","l")
> # right=FALSE since you want "area < 25" rather than
> # "area <= 25" --- although this makes no difference for the
> # area values actually used.
>
> syn.ppp <- ppp(x=xp,y=yp,marks=acat)
>
> (6) It is not clear to me what your "research question" is.?? Do you 
> want to test whether the intensities differ between the area 
> categories? Unless my thinking is hopelessly confused, this has 
> nothing to do with (or at least does not require the use of) the 
> envelope() function:
>
> f1 <- ppm(syn.ppp) # Same (constant) intensity for each area category.
> f2 <- ppm(syn.ppp ~ marks) # Allows different (constant) intensity
> ???????????????????????????????????????????????????? # for each area category.
> anova(f1,f2,test="Chi")
>
> This test (not surprisingly) rejects the hypothesis that the 
> intensities are the same; p-value = 0.0020.
>
> If this is not the hypothesis that you are interested in, please 
> clarify your thinking and your question, and get back to us.
>
> cheers,
>
> Rolf Turner
>

	[[alternative HTML version deleted]]


From geo|@|rz @end|ng |rom nu@@edu@@g  Thu Jul 25 11:16:43 2019
From: geo|@|rz @end|ng |rom nu@@edu@@g (Letisha Sarah Fong Rui Zhen)
Date: Thu, 25 Jul 2019 09:16:43 +0000
Subject: [R-sig-Geo] 
 Calculating impact measures for spatial panel models
 and trouble specifying SDEM model using spml
In-Reply-To: <alpine.LFD.2.21.1907241534380.16556@reclus.nhh.no>
References: <SG2PR06MB387093D5FF97FD899829FB92EEC60@SG2PR06MB3870.apcprd06.prod.outlook.com>,
 <alpine.LFD.2.21.1907241534380.16556@reclus.nhh.no>
Message-ID: <SG2PR06MB3870C9253EB325F2D420C36BEEC10@SG2PR06MB3870.apcprd06.prod.outlook.com>

Dear Roger,

Thank you for your quick response.

I have uploaded the spatial weights matrix and sample dataset I'm working with here: https://drive.google.com/drive/folders/1NjCODKEix-_nA5CfiIos6uiKAUbGp_BZ?usp=sharing

Reading the data in and transforming them into a pdataframe and listw, respectively:
spatialweight <- read.csv("spatialweight.csv", header = T)
row.names(spatialweight) <- spatialweight$X
spatialweight <- spatialweight[, -1]
spatialweight.mat <- as.matrix(spatialweight)
mylistw <- mat2listw(spatialweight.mat, style = "M")
mydata <- read.csv("sampledata.csv", header = T)
mydata <- pdata.frame(mydata, index = c("Country", "Year"))

I first ran a non-spatial model to determine the best specification for fixed effects:

nonspatial.pooledOLS <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "pooling")
nonspatial.individualFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "individual")
nonspatial.timeFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "time")
nonspatial.twowayFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "twoways")

I would like to compare these models based on log likelihood and AIC, but the plm() function does not appear to provide a log likelihood or AIC. I have read through the JSS plm article and it states that models made with the plm() function are "estimated using the lm function to the transformed data". I'm aware that we can use logLik() and AIC() for a model estimated with the lm() function. However it doesn't seem to work with the plm() function.

For example, I did logLik(nonspatial.twowayFE) and AIC(nonspatial.twowayFE) but the error message for both is:

Error in UseMethod("logLik") :
  no applicable method for 'logLik' applied to an object of class "c('plm', 'panelmodel')"

Please let me know if I'm calling the wrong function(s) and/or if you're aware of a way to compare these models based on log likelihood and/or AIC.

For the spatial models, here is my code:

spatial.SDEM <- spml(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI + slag(lnGDP, listw = mylistw) + slag(lnGDP2, listw = mylistw) + slag(lnEI, listw = mylistw), data = mydata, listw = mylistw, model = "within", effect = "twoways", lag = F, spatial.error = "b")
spatial.SEM <- spml(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, listw = mylistw, model = "within", effect = "individual", lag = F, spatial.error = "b")
spatial.SLX <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI + slag(lnGDP, listw = mylistw) + slag(lnGDP2, listw = mylistw) + slag(lnEI, listw = mylistw), data = mydata, model = "within", effect = "individual")

As in my original post, the SLX and SEM models ran OK but the error when I try to run the SDEM model is:

Error in UseMethod("slag") :
  no applicable method for 'slag' applied to an object of class "c('double', 'numeric')"

The variables that I use the slag() function on are all numeric, so I don't know what's wrong. I seem to be able to use slag() with plm() but not with spml(), but I don't know why this is so.

I need to compare the models to see if SDEM can be reduced to one of its nested form. As was the case of the non-spatial models, I can't get the log likelihood for models created with the plm() function, so any suggestions are welcome. I've already read through the JSS articles for splm and plm as well as both documentations and there's no information on this (except that models built with the plm() function are estimated using the lm function to the transformed data).

Thanks for clarifying the impact measures for SDEM and SLX. Just to check - when you say linear combination for standard errors do you mean e.g. beta1*se + theta1*se = totalse (where beta1 is the coefficient of the direct impact and theta1 is the coefficient of the indirect impact)?

Thank you for your help!

Best wishes,
Sarah

________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Wednesday, July 24, 2019 9:50:13 PM
To: Letisha Sarah Fong Rui Zhen <geolsfrz at nus.edu.sg>
Cc: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
Subject: Re: [R-sig-Geo] Calculating impact measures for spatial panel models and trouble specifying SDEM model using spml

On Wed, 24 Jul 2019, Letisha Sarah Fong Rui Zhen wrote:

> Dear all,

Please do not post HTML-formated messages.

>
> I???m working with panel data of 9 countries and 18 years and I???m
> running fixed effects SDEM, SLX and SEM with the splm package.
>
> I have three questions:
>
> 1. I can???t seem to get the SDEM model to run. My code for each of the
>    3 models is:
>
> model.SDEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE +
> slag(ln(GDP), listw = my.listw) + slag((ln(GDP))^2, listw = my.listw),
> data = my.data, listw = my.listw, model = ???within???, effect =
> ???individual???, lag = F, spatial.error = ???b???)
>
> model.SLX <- plm(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE + slag(ln(GDP),
> listw = my.listw) + slag((ln(GDP))^2, listw = my.listw), data = my.data,
> model = ???within???, effect = ???individual???)
>
> model.SEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE, data =
> my.data, listw = my.listw, model = ???within???, effect =
> ???individual???, lag = F, spatial.error = ???b???)
>
> I am able to run both SLX and SEM models without problem, but when I try
> to run the SDEM model, the error message is:
>
> Error in UseMethod("slag") :
>  no applicable method for 'slag' applied to an object of class
>  "c('double', 'numeric')"
>
> I don???t understand what is wrong here, as I have no problems with the
> slag() function in the SLX model. My data is a pdataframe and each
> variable is a numeric pseries.

My guess would be that you should protect your square with I() in general,
but have no idea - this is not a reproducible example.

>
>
> 2. How can I calculate impact measures (direct, indirect and total) for
>    spatial panel models?
>
> The impacts() function in spdep doesn???t work anymore and the impacts()
> function from the spatialreg package seems to work only for
> cross-sectional data and not panel data.
>
> For example, I ran:
>
> spatialreg::impacts(model.SLX)
>
> And the error message is:
>
> Error in UseMethod("impacts", obj) :
>  no applicable method for 'impacts' applied to an object of class
>  "c('plm', 'panelmodel')"
>
> I have tried methods(impacts) but none of the functions seem to work for
> my SLX model created with the splm package.

But your SLX model is created with the plm package, isn't it? The only use
of splm is for the manual lags with slag()?

>
> I also looked at some previous examples in the splm documentation and
> more specifically the spml() function and the example provided
> (specifically the impact measures) doesn???t work anymore:
>
> data(Produc, package = "plm")
> data(usaww)
> fm <- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
> ## random effects panel with spatial lag
> respatlag <- spml(fm, data = Produc, listw = mat2listw(usaww),
>                  model="random", spatial.error="none", lag=TRUE)
> summary(respatlag) ## calculate impact measures impac1 <-
> impacts(respatlag, listw = mat2listw(usaww, style = "W"), time = 17)
> summary(impac1, zstats=TRUE, short=TRUE)
>

The implemented impacts methods in splm apply to the case where the lagged
response is included. For SDEM and SLX, you can get the impacts by
addition for the total impactss, and by linear combination for their
standard errors. This is not implemented in a method. Further, slag() does
not give any impacts method any information on which variables have been
lagged - in your illustration above EI and RE are not lagged.

> The error message when I run the impacts() function is:
>
> Error in UseMethod("impacts", obj) :
>  no applicable method for 'impacts' applied to an object of class "splm"
>
> My question is therefore, how do I go about calculating direct, indirect
> and total impact measures for spatial panel data?
>
>
> 3. How can I test if the SDEM model can be simplified to the SLX model
>    (since I estimate the SDEM by maximum likelihood (spml function) and
>    the SLX by ordinary linear regression (plm function))? From my
>    understanding the plm() function does not compute a loglikelihood or
>    AIC so I probably can???t do a likelihood ratio test to choose
>    between models (I haven???t tried this out because I???m stuck at
>    running the SDEM model).

Do you know definitely that plm does not provide a log likelihood? I
realise that it isn't OLS unless pooled. Have you reviewed the JSS plm and
splm articles?

Roger

>
> Any help or advice would be greatly appreciated. Thank you.
>
> Best wishes,
> Sarah
>
>
>
> ________________________________
>
> Important: This email is confidential and may be privileged. If you are
> not the intended recipient, please delete it and notify us immediately;
> you should not copy or use it for any purpose, nor disclose its contents
> to any other person. Thank you.
>
>        [[alternative HTML version deleted]]
>
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

________________________________

Important: This email is confidential and may be privileged. If you are not the intended recipient, please delete it and notify us immediately; you should not copy or use it for any purpose, nor disclose its contents to any other person. Thank you.

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Jul 26 02:37:04 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 26 Jul 2019 12:37:04 +1200
Subject: [R-sig-Geo] Simple Ripley's CRS test for market point patters
In-Reply-To: <dd69fad4-192c-ae1c-5096-65b9c2e7f29f@yahoo.com.br>
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
 <1075794206.5056673.1563831392519@mail.yahoo.com>
 <8a7d0858-37ea-de97-b988-5f29dfcc38c8@auckland.ac.nz>
 <dd69fad4-192c-ae1c-5096-65b9c2e7f29f@yahoo.com.br>
Message-ID: <3d447698-95dc-24e9-9e60-333042d1c3e9@auckland.ac.nz>


On 25/07/19 12:50 PM, ASANTOS wrote:

> Thanks for your help Marcelino e for the careful explanation Rolf Turner 
> and so sorry about my post script configuration, I expected that I 
> solved that in my new post.
> 
> First my variable area is a marked point (attribute or auxiliary 
> information about my point process - page 7 and not a spatial covariate, 
> effect in the outcome of my experimental area - page 50). Based in this 
> information, my hypophyses is that the *size of ant nests* a cause of 
> ecological intraspecific competition for resources (such as food and 
> territory) *have different patterns of spatial distribution*, for this:
> 
> #Packages
> require(spatstat)
> require(sp)
> 
> # Create some points that represents ant nests
> 
> xp<-c(371278.588,371250.722,371272.618,371328.421,371349.974,
> 371311.95,371296.265,371406.46,371411.551,371329.041,371338.081,
> 371334.182,371333.756,371299.818,371254.374,371193.673,371172.836,
> 371173.803,371153.73,371165.051,371140.417,371168.279,371166.367,
> 371180.575,371132.664,371129.791,371232.919,371208.502,371289.462,
> 371207.595,371219.008,371139.921,371133.215,371061.467,371053.69,
> 371099.897,371108.782,371112.52,371114.241,371176.236,371159.185,
> 371159.291,371158.552,370978.252,371120.03,371116.993)
> 
> yp<-c(8246507.94,8246493.176,8246465.974,8246464.413,8246403.465,
> 8246386.098,8246432.144,8246394.827,8246366.201,8246337.626,
> 8246311.125,8246300.039,8246299.594,8246298.072,8246379.351,
> 8246431.998,8246423.913,8246423.476,8246431.658,8246418.226,
> 8246400.161,8246396.891,8246394.225,8246400.391,8246370.244,
> 8246367.019,8246311.075,8246255.174,8246255.085,8246226.514,
> 8246215.847,8246337.316,8246330.197,8246311.197,8246304.183,
> 8246239.282,8246239.887,8246241.678,8246240.361,8246167.364,
> 8246171.581,8246171.803,8246169.807,8246293.57,8246183.194,8246189.926)
> 
> #Now I have the size of each nest (marked process)
> 
> area<-c(117,30,4,341,15,160,35,280,108,168,63,143,2,48,182,42,
> 88,56,27,156,288,45,49,234,72,270,91,40,304,56,35,4,56.7,9,4.6,
> 105,133,135,23.92,190,12.9,15.2,192.78,104,255,24)
> 
> # Make a countour for the window creation
> W <- convexhull.xy(xp,yp)
> 
> 
> # Class of nests size - large, medium and small
> acat <- cut(area,breaks=c(-Inf,25,55,Inf),right=FALSE, 
> labels=c("s","m","l"))
> 
> #Create a ppp object
> 
> syn.ppp <- ppp(x=xp,y=yp,window=W, marks=acat)
> 
> # Test intensity hypothesis
> 
> f1 <- ppm(syn.ppp) # Same (constant) intensity for each area category.
> f2 <- ppm(syn.ppp ~ marks) # Allows different (constant) intensity for 
> each area category.
> anova(f1,f2,test="Chi")
> 
> #0.002015 ** OK, the hypothesis that the intensities are the same was 
> reject, but intensities is not the question.
> 
> Based in my initial hypothesis, I've like to show envelopes and observed 
> values of the use of some function for the three point patters (large, 
> medium and small ant nest size)under CSR. And for this I try to:
> 
> kS<-envelope(syn.ppp[syn.ppp$acat=="s"], nsim=99,fun=Kest)
> 
> plot(kS,lwd=list(3,1,1,1), main="")
> 
> kM<-envelope(syn.ppp[syn.ppp$acat=="m"], nsim=99,fun=Kest)
> 
> plot(kM,lwd=list(3,1,1,1), main="")
> 
> kL<-envelope(syn.ppp[syn.ppp$acat=="l"], nsim=99,fun=Kest)
> 
> plot(kL,lwd=list(3,1,1,1), main="")
> 
> But doesn't work yet. My approach now sounds correct?
> 
> Thanks in advanced,

The formatting of your latest email was a great improvement and is now 
admirably legible.

I am still not completely clear what your research question is, or what 
hypotheses you wish to test.

Let me preface my remarks by saying that I have so far gone along with 
your inclination to convert the numeric marks (area) of your pattern to 
categorical (small, medium, large) marks.  I believe that this may be a
suboptimal approach.  (As someone said in an old post to R-help, on an 
entirely  different topic, 'There is a reason that the speedometer in 
your car doesn't just read "slow" and "fast".' :-) )

On the other hand there is not a lot on offer in the way of techniques 
for handling numeric marks, so the "discretisation" approach may be the 
best that one can do.

Now to proceed with trying to answer your question(s).  You say:

> my hypophyses is that the size of ant nests a cause of ecological
> intraspecific competition for resources (such as food and territory)
> have different patterns of spatial distribution

That's a (rather vague) "*alternative*" hypothesis.  The corresponding 
null  hypothesis is that the three categories have the *same* spatial 
distribution.   We reject that null hypothesis, since we reject the 
hypothesis that they have the same intensities.

Now what?

You say:

> Based in my initial hypothesis, I've like to show envelopes and observed
> values of the use of some function for the three point patters (large,
> medium and small ant nest size) under CSR.

This is a bit incoherent and a demonstrates somewhat confused thinking. 
  As the signature file of a frequent poster to R-help says "Tell me 
what you want to do, not how you want to do it."

Perhaps the following will help to clarify.  For a categorical marked 
point pattern (which is what we are currently dealing with) there are 
infinitely many possibilities for the joint distribution of the patterns 
corresponding to the marks.

The simplest of these is CSR (which means that the marks are essentially 
irrelevant).  We have already rejected this hypothesis.

The next simplest is what is designated in [1; p. 584] as "CSRI" 
(complete spatial randomness with independence).   This means that each 
of the patterns is "individually CSR" (Poisson process, constant 
intensity) and that the patterns are independent.

After that, the possibilities explode astronomically --- different 
(non-Poisson) structures for each of the patterns, different dependence 
structures, ....  The sky's the limit.

One "reasonably simple" possibility amongst this plethora of 
possibilities is the multitype Strauss point process model, which could 
be fitted (with some effort) using ppm() and profilepl().

However before we open that can of worms, we can test "CSRI":

fit <- ppm(syn.ppp ~ marks)
set.seed(42)
E <- envelope(fit,savefuns=TRUE)
dclf.test(E)
mad.test(E)

The dclf test gives a p-value of 0.14; the mad test gives a p-value of 
0.18.  Thus there appears to be no evidence against the null hypothesis 
of CSRI.

Therefore "CSRI" would appear to be an adequate/plausible model for 
these data.

There may be more that needs to be done, but this is a start.

I would appreciate comments from Marcelino (and of course from Adrian 
and Ege) especially if any of them notice something incorrect in my advice.

cheers,

Rolf

[1] Spatial Point Patterns: Methodology and Applications with R
1st Edition, Adrian Baddeley, Ege Rubak, Rolf Turner
Chapman and Hall/CRC, 2015

P. S.  You said of your efforts to continue your analysis: "But doesn't 
work yet."  This is a very vague and unhelpful assertion!  One thing 
that is wrong with your efforts is that in,  e.g.:

     kS<-envelope(syn.ppp[syn.ppp$acat=="s"], nsim=99,fun=Kest)

the object syn.ppp has no component named "acat"!!!  (You really need to 
gain some understanding of how R works!)

You *could* say

    kS <- envelope(syn.ppp[syn.ppp$marks=="s"])

or (better)

     kS <- envelope(syn.ppp[marks(syn.ppp)=="s"])

to get the sort of result that you had hoped for.  Note that specifying 
the arguments nsim=99 and fun=Kest, while harmless, is redundant (and 
therefore tends to obfuscate).  These are the *default* values!!!  Read 
the help, and learn to use R efficiently!

R.

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From geo|@|rz @end|ng |rom nu@@edu@@g  Fri Jul 26 10:57:28 2019
From: geo|@|rz @end|ng |rom nu@@edu@@g (Letisha Sarah Fong Rui Zhen)
Date: Fri, 26 Jul 2019 08:57:28 +0000
Subject: [R-sig-Geo] 
 Calculating impact measures for spatial panel models
 and trouble specifying SDEM model using spml
In-Reply-To: <SG2PR06MB3870C9253EB325F2D420C36BEEC10@SG2PR06MB3870.apcprd06.prod.outlook.com>
References: <SG2PR06MB387093D5FF97FD899829FB92EEC60@SG2PR06MB3870.apcprd06.prod.outlook.com>,
 <alpine.LFD.2.21.1907241534380.16556@reclus.nhh.no>,
 <SG2PR06MB3870C9253EB325F2D420C36BEEC10@SG2PR06MB3870.apcprd06.prod.outlook.com>
Message-ID: <KL1PR0601MB3863CCFF4397920B3B68F36EEEC00@KL1PR0601MB3863.apcprd06.prod.outlook.com>

Dear Roger and all,

In the end I used lm() to estimate my non-spatial FE models in order to extract the log likelihood values (the results are exactly the same when using the plm() function.

I also used this method to estimate the SLX model. I just used the slag() function on each explanatory variable I wanted to have a spatial lag for, added it to my dataframe, and then estimated using lm() so that I can extract the log likelihood. I also managed to run my SDEM model with the spml() function without using slag() within the model.

I just have one question: in the splm package documentation, there should be an extractable logLik value when I build a model using the spml() function

However, when I do summary(spatial.SEM)$logLik, the result is NULL. When I do logLik(spatial.SEM), the error message is:

Error in UseMethod("logLik") :
no applicable method for 'logLik' applied to an object of class "splm"

Since spml() uses maximum likelihood estimation so why can't I extract the log likelihood value?

Any help would be greatly appreciated!

Best wishes,
Letisha
________________________________
From: Letisha Sarah Fong Rui Zhen <geolsfrz at nus.edu.sg>
Sent: Thursday, July 25, 2019 5:16:43 PM
To: Roger.Bivand at nhh.no <Roger.Bivand at nhh.no>
Cc: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
Subject: Re: [R-sig-Geo] Calculating impact measures for spatial panel models and trouble specifying SDEM model using spml

Dear Roger,

Thank you for your quick response.

I have uploaded the spatial weights matrix and sample dataset I'm working with here: https://drive.google.com/drive/folders/1NjCODKEix-_nA5CfiIos6uiKAUbGp_BZ?usp=sharing

Reading the data in and transforming them into a pdataframe and listw, respectively:
spatialweight <- read.csv("spatialweight.csv", header = T)
row.names(spatialweight) <- spatialweight$X
spatialweight <- spatialweight[, -1]
spatialweight.mat <- as.matrix(spatialweight)
mylistw <- mat2listw(spatialweight.mat, style = "M")
mydata <- read.csv("sampledata.csv", header = T)
mydata <- pdata.frame(mydata, index = c("Country", "Year"))

I first ran a non-spatial model to determine the best specification for fixed effects:

nonspatial.pooledOLS <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "pooling")
nonspatial.individualFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "individual")
nonspatial.timeFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "time")
nonspatial.twowayFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "twoways")

I would like to compare these models based on log likelihood and AIC, but the plm() function does not appear to provide a log likelihood or AIC. I have read through the JSS plm article and it states that models made with the plm() function are "estimated using the lm function to the transformed data". I'm aware that we can use logLik() and AIC() for a model estimated with the lm() function. However it doesn't seem to work with the plm() function.

For example, I did logLik(nonspatial.twowayFE) and AIC(nonspatial.twowayFE) but the error message for both is:

Error in UseMethod("logLik") :
  no applicable method for 'logLik' applied to an object of class "c('plm', 'panelmodel')"

Please let me know if I'm calling the wrong function(s) and/or if you're aware of a way to compare these models based on log likelihood and/or AIC.

For the spatial models, here is my code:

spatial.SDEM <- spml(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI + slag(lnGDP, listw = mylistw) + slag(lnGDP2, listw = mylistw) + slag(lnEI, listw = mylistw), data = mydata, listw = mylistw, model = "within", effect = "twoways", lag = F, spatial.error = "b")
spatial.SEM <- spml(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, listw = mylistw, model = "within", effect = "individual", lag = F, spatial.error = "b")
spatial.SLX <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI + slag(lnGDP, listw = mylistw) + slag(lnGDP2, listw = mylistw) + slag(lnEI, listw = mylistw), data = mydata, model = "within", effect = "individual")

As in my original post, the SLX and SEM models ran OK but the error when I try to run the SDEM model is:

Error in UseMethod("slag") :
  no applicable method for 'slag' applied to an object of class "c('double', 'numeric')"

The variables that I use the slag() function on are all numeric, so I don't know what's wrong. I seem to be able to use slag() with plm() but not with spml(), but I don't know why this is so.

I need to compare the models to see if SDEM can be reduced to one of its nested form. As was the case of the non-spatial models, I can't get the log likelihood for models created with the plm() function, so any suggestions are welcome. I've already read through the JSS articles for splm and plm as well as both documentations and there's no information on this (except that models built with the plm() function are estimated using the lm function to the transformed data).

Thanks for clarifying the impact measures for SDEM and SLX. Just to check - when you say linear combination for standard errors do you mean e.g. beta1*se + theta1*se = totalse (where beta1 is the coefficient of the direct impact and theta1 is the coefficient of the indirect impact)?

Thank you for your help!

Best wishes,
Sarah

________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Wednesday, July 24, 2019 9:50:13 PM
To: Letisha Sarah Fong Rui Zhen <geolsfrz at nus.edu.sg>
Cc: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
Subject: Re: [R-sig-Geo] Calculating impact measures for spatial panel models and trouble specifying SDEM model using spml

On Wed, 24 Jul 2019, Letisha Sarah Fong Rui Zhen wrote:

> Dear all,

Please do not post HTML-formated messages.

>
> I???m working with panel data of 9 countries and 18 years and I???m
> running fixed effects SDEM, SLX and SEM with the splm package.
>
> I have three questions:
>
> 1. I can???t seem to get the SDEM model to run. My code for each of the
>    3 models is:
>
> model.SDEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE +
> slag(ln(GDP), listw = my.listw) + slag((ln(GDP))^2, listw = my.listw),
> data = my.data, listw = my.listw, model = ???within???, effect =
> ???individual???, lag = F, spatial.error = ???b???)
>
> model.SLX <- plm(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE + slag(ln(GDP),
> listw = my.listw) + slag((ln(GDP))^2, listw = my.listw), data = my.data,
> model = ???within???, effect = ???individual???)
>
> model.SEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE, data =
> my.data, listw = my.listw, model = ???within???, effect =
> ???individual???, lag = F, spatial.error = ???b???)
>
> I am able to run both SLX and SEM models without problem, but when I try
> to run the SDEM model, the error message is:
>
> Error in UseMethod("slag") :
>  no applicable method for 'slag' applied to an object of class
>  "c('double', 'numeric')"
>
> I don???t understand what is wrong here, as I have no problems with the
> slag() function in the SLX model. My data is a pdataframe and each
> variable is a numeric pseries.

My guess would be that you should protect your square with I() in general,
but have no idea - this is not a reproducible example.

>
>
> 2. How can I calculate impact measures (direct, indirect and total) for
>    spatial panel models?
>
> The impacts() function in spdep doesn???t work anymore and the impacts()
> function from the spatialreg package seems to work only for
> cross-sectional data and not panel data.
>
> For example, I ran:
>
> spatialreg::impacts(model.SLX)
>
> And the error message is:
>
> Error in UseMethod("impacts", obj) :
>  no applicable method for 'impacts' applied to an object of class
>  "c('plm', 'panelmodel')"
>
> I have tried methods(impacts) but none of the functions seem to work for
> my SLX model created with the splm package.

But your SLX model is created with the plm package, isn't it? The only use
of splm is for the manual lags with slag()?

>
> I also looked at some previous examples in the splm documentation and
> more specifically the spml() function and the example provided
> (specifically the impact measures) doesn???t work anymore:
>
> data(Produc, package = "plm")
> data(usaww)
> fm <- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
> ## random effects panel with spatial lag
> respatlag <- spml(fm, data = Produc, listw = mat2listw(usaww),
>                  model="random", spatial.error="none", lag=TRUE)
> summary(respatlag) ## calculate impact measures impac1 <-
> impacts(respatlag, listw = mat2listw(usaww, style = "W"), time = 17)
> summary(impac1, zstats=TRUE, short=TRUE)
>

The implemented impacts methods in splm apply to the case where the lagged
response is included. For SDEM and SLX, you can get the impacts by
addition for the total impactss, and by linear combination for their
standard errors. This is not implemented in a method. Further, slag() does
not give any impacts method any information on which variables have been
lagged - in your illustration above EI and RE are not lagged.

> The error message when I run the impacts() function is:
>
> Error in UseMethod("impacts", obj) :
>  no applicable method for 'impacts' applied to an object of class "splm"
>
> My question is therefore, how do I go about calculating direct, indirect
> and total impact measures for spatial panel data?
>
>
> 3. How can I test if the SDEM model can be simplified to the SLX model
>    (since I estimate the SDEM by maximum likelihood (spml function) and
>    the SLX by ordinary linear regression (plm function))? From my
>    understanding the plm() function does not compute a loglikelihood or
>    AIC so I probably can???t do a likelihood ratio test to choose
>    between models (I haven???t tried this out because I???m stuck at
>    running the SDEM model).

Do you know definitely that plm does not provide a log likelihood? I
realise that it isn't OLS unless pooled. Have you reviewed the JSS plm and
splm articles?

Roger

>
> Any help or advice would be greatly appreciated. Thank you.
>
> Best wishes,
> Sarah
>
>
>
> ________________________________
>
> Important: This email is confidential and may be privileged. If you are
> not the intended recipient, please delete it and notify us immediately;
> you should not copy or use it for any purpose, nor disclose its contents
> to any other person. Thank you.
>
>        [[alternative HTML version deleted]]
>
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

________________________________

Important: This email is confidential and may be privileged. If you are not the intended recipient, please delete it and notify us immediately; you should not copy or use it for any purpose, nor disclose its contents to any other person. Thank you.

	[[alternative HTML version deleted]]


From b@row||ng@on @end|ng |rom gm@||@com  Fri Jul 26 16:05:34 2019
From: b@row||ng@on @end|ng |rom gm@||@com (Barry Rowlingson)
Date: Fri, 26 Jul 2019 15:05:34 +0100
Subject: [R-sig-Geo] FOSS4G UK Conference
Message-ID: <CANVKczMjn8p2dy_TkA3+cEfjiGNU9ptkiRxz2mMDO9T4oOENvw@mail.gmail.com>

The UK edition of the leading open source geospatial software conference is
happening in Edinburgh in September:

 https://uk.osgeo.org/foss4guk2019/

there's a few talks with R, but also plenty of opportunity to expand your
geospatial knowledge beyond R.

Generous sponsorship has meant we have kept the price low - a two-day
conference (incl lunch and refreshments) for ?80.

I've been organising the workshops sessions and am also giving a talk on
integration of R and QGIS.

Hope to see some of you there.

Barry

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Jul 26 23:29:12 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 27 Jul 2019 09:29:12 +1200
Subject: [R-sig-Geo] 
 Simple Ripley's CRS test for market point patters --- addendum.
In-Reply-To: <3d447698-95dc-24e9-9e60-333042d1c3e9@auckland.ac.nz>
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
 <1075794206.5056673.1563831392519@mail.yahoo.com>
 <8a7d0858-37ea-de97-b988-5f29dfcc38c8@auckland.ac.nz>
 <dd69fad4-192c-ae1c-5096-65b9c2e7f29f@yahoo.com.br>
 <3d447698-95dc-24e9-9e60-333042d1c3e9@auckland.ac.nz>
Message-ID: <449bd5c0-a6b7-ecf7-4553-ea4d33c13c82@auckland.ac.nz>


I have realised that there were a couple of oversights in my previous 
posting on this issue.  One is a bit subtle; the other is a bit of a 
blunder on my part.

First the "subtle" one. The test that I proposed for CSRI is a test done 
using the estimated parameters of the proposed model to generate 
realisations of data sets under the null hypothesis.  Such tests tend to 
be conservative.  (See section 10.6.3, p. 388 ff., in [1].)

In the current instance (testing for CSRI) the conservatism can be 
overcome by simulating data conditional on the numbers of points of each 
type in the "real" data.  This can be done here via:

foo <- function(W){
s <- runifpoint(10,win=W)
m <- runifpoint(9,win=W)
l <- runifpoint(27,win=W)
superimpose(s=s,m=m,l=l)
}
simex <- expression(foo(W))

and then

set.seed(42)
E <- envelope(syn.ppp,simulate=simex,savefuns=TRUE)
dtst <- dclf.test(E)
mtst <- mad.test(E)

This gives p-values of 0.06 from the dclf test and 0.09 from the mad 
test.  Thus there appears to be some slight evidence against the null 
hypothesis.  (Evidence at the 0.10 significance level.)

That this should be so is *OBVIOUS* (!!!) if we turn to the unsubtle 
point that I overlooked.  It is clear that the pattern of ants' nests 
cannot be truly a realisation of a Poisson process since there must be a 
bit of a "hard core" effect.  Two ants' nests cannot overlap.  Thus if 
we approximate the shape of each nest by a disc, points i and j must be 
a distance of at least r_i + r_j from each other, where r_i = 
sqrt(area_i/pi), and similarly for r_j.

However I note that the data provided seem to violate this principle in 
several instances.  E.g. points 41 and 42 are a distance of only 0.2460 
metres apart but areas 41 and 42 are 12.9 and 15.2 square metres, 
yielding putative radii of 3.5917 and 3.8987 metres, whence the closest
these points could possibly be (under the "disc-shaped assumption") is
7.4904 metres, far larger than 0.2460.   So something is a bit out of 
whack here.  Perhaps these are made-up ("synthetic") data and the 
process of making up the data did not take account of the minimum 
distance constraint.

How to incorporate the "hard core" aspect of your (real?) data into the 
modelling exercise, and what the impact of it is upon your research 
question(s), is unclear to me and is likely to be complicated.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

[1] Spatial Point Patterns: Methodology and Applications with R
1st Edition, Adrian Baddeley, Ege Rubak, Rolf Turner
Chapman and Hall/CRC, 2015


From p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r  Sat Jul 27 17:10:31 2019
From: p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r (Patrick Giraudoux)
Date: Sat, 27 Jul 2019 17:10:31 +0200
Subject: [R-sig-Geo] how to specify the size of device so that it fits a
 SpatialPolygonsDataFrame plot exactly
Message-ID: <04cc3d06-a85d-427a-17a2-40988643f03e@univ-fcomte.fr>

Dear listers,

I would like to write jpeg (or png, etc.) maps using the function 
graphics:jpeg (png, etc.), but with the device window exactly fitting 
the map (= no white strip above, below or on the sides of the map). The 
map is derived from a SpatialPolygonsDataFrame (WGS84)

However, even if I set

par(mar=c(0,0,0,0) and pass xaxs='i', yaxs='i' in the plot, there are 
still white strips

I tried several tricks but none are fully satisfying.

Trial #1: I computed the ratio of the bounding box and using this ratio 
in the function jpeg with the height and width arguments. I did it both 
in WGS84 and in UTM47:

ratio<-(max(bbox(ChinaUTM)[1,])-min(bbox(ChinaUTM)[1,]))/(max(bbox(ChinaUTM)[2,])-min(bbox(ChinaUTM)[2,])) 
# ratio width/height

then

jpeg("./MapChina.jpeg",height=8,width=8*ratio,res=300,units="in")

par(mar=c(0,0,0,0)) # no margin
plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')

dev.off()

The best is obtained with UTM47 (planar projection), however I get 
(almost) rid of any white strip only if I add an additionnal 1.04 
coefficient (obtained by trial-error)

Hence:

jpeg("./MapChina.jpeg",height=8,width=8*ratio*1.04,res=300,units="in")

par(mar=c(0,0,0,0)) # no margin
plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')

dev.off()

Trial #2: The other way has been to pick up the parameter values like that:

par(mar=c(0,0,0,0)) # no margin
plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')
pt<-par()$pin

ratio<-pt[2]/pt[1]

jpeg("./MapChina.jpeg",height=8,width=8*ratio,res=300,units="in")
par(mar=c(0,0,0,0)) # no margin
plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i') #

dev.off()

Does not work either

Any idea about how to deal with this ? In short how to get the exact 
size of a SpatialPolygonDataFrame plot to make the device window exactly 
this? size?

Best,

Patrick


From Roger@B|v@nd @end|ng |rom nhh@no  Sun Jul 28 16:21:58 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sun, 28 Jul 2019 16:21:58 +0200
Subject: [R-sig-Geo] 
 Calculating impact measures for spatial panel models
 and trouble specifying SDEM model using spml
In-Reply-To: <SG2PR06MB3870C9253EB325F2D420C36BEEC10@SG2PR06MB3870.apcprd06.prod.outlook.com>
References: <SG2PR06MB387093D5FF97FD899829FB92EEC60@SG2PR06MB3870.apcprd06.prod.outlook.com>,
 <alpine.LFD.2.21.1907241534380.16556@reclus.nhh.no>
 <SG2PR06MB3870C9253EB325F2D420C36BEEC10@SG2PR06MB3870.apcprd06.prod.outlook.com>
Message-ID: <alpine.LFD.2.21.1907281542390.1553@reclus.nhh.no>

On Thu, 25 Jul 2019, Letisha Sarah Fong Rui Zhen wrote:

> Dear Roger,
>
> Thank you for your quick response.
>
> I have uploaded the spatial weights matrix and sample dataset I'm 
> working with here: 
> https://drive.google.com/drive/folders/1NjCODKEix-_nA5CfiIos6uiKAUbGp_BZ?usp=sharing

Please only use built-in data - loading user-offered data is inherently 
unfortunate for many reasons.

>
> Reading the data in and transforming them into a pdataframe and listw, respectively:
> spatialweight <- read.csv("spatialweight.csv", header = T)
> row.names(spatialweight) <- spatialweight$X
> spatialweight <- spatialweight[, -1]
> spatialweight.mat <- as.matrix(spatialweight)
> mylistw <- mat2listw(spatialweight.mat, style = "M")
> mydata <- read.csv("sampledata.csv", header = T)
> mydata <- pdata.frame(mydata, index = c("Country", "Year"))
>

Hand crafting weights is typically error prone, also because the 
provenance of the weights (how they were constructed) is not known.

> I first ran a non-spatial model to determine the best specification for 
> fixed effects:
>
> nonspatial.pooledOLS <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "pooling")
> nonspatial.individualFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "individual")
> nonspatial.timeFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "time")
> nonspatial.twowayFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "twoways")
>
> I would like to compare these models based on log likelihood and AIC, 
> but the plm() function does not appear to provide a log likelihood or 
> AIC. I have read through the JSS plm article and it states that models 
> made with the plm() function are "estimated using the lm function to the 
> transformed data". I'm aware that we can use logLik() and AIC() for a 
> model estimated with the lm() function. However it doesn't seem to work 
> with the plm() function.

So either debug the function itself and find out where the object needed 
gets lost, or write to the package maintainer with a patch to return the 
value, see:

https://r-forge.r-project.org/scm/viewvc.php/pkg/R/est_plm.R?view=markup&root=plm

and its use of the internal function plm:::mylm(). It runs lm(), but only 
returns some of the components in the lm() object.

>
> For example, I did logLik(nonspatial.twowayFE) and 
> AIC(nonspatial.twowayFE) but the error message for both is:
>
> Error in UseMethod("logLik") :
>  no applicable method for 'logLik' applied to an object of class
>  "c('plm', 'panelmodel')"
>
> Please let me know if I'm calling the wrong function(s) and/or if you're 
> aware of a way to compare these models based on log likelihood and/or 
> AIC.
>
> For the spatial models, here is my code:
>
> spatial.SDEM <- spml(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI + slag(lnGDP, listw = mylistw) + slag(lnGDP2, listw = mylistw) + slag(lnEI, listw = mylistw), data = mydata, listw = mylistw, model = "within", effect = "twoways", lag = F, spatial.error = "b")
> spatial.SEM <- spml(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, listw = mylistw, model = "within", effect = "individual", lag = F, spatial.error = "b")
> spatial.SLX <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI + slag(lnGDP, listw = mylistw) + slag(lnGDP2, listw = mylistw) + slag(lnEI, listw = mylistw), data = mydata, model = "within", effect = "individual")
>
> As in my original post, the SLX and SEM models ran OK but the error when 
> I try to run the SDEM model is:
>
> Error in UseMethod("slag") :
>  no applicable method for 'slag' applied to an object of class
>  "c('double', 'numeric')"

Please replicate with built-in data. If you cannot replicate, it is 
something in your data that you need to address.

Roger

>
> The variables that I use the slag() function on are all numeric, so I 
> don't know what's wrong. I seem to be able to use slag() with plm() but 
> not with spml(), but I don't know why this is so.
>
> I need to compare the models to see if SDEM can be reduced to one of its 
> nested form. As was the case of the non-spatial models, I can't get the 
> log likelihood for models created with the plm() function, so any 
> suggestions are welcome. I've already read through the JSS articles for 
> splm and plm as well as both documentations and there's no information 
> on this (except that models built with the plm() function are estimated 
> using the lm function to the transformed data).
>
> Thanks for clarifying the impact measures for SDEM and SLX. Just to 
> check - when you say linear combination for standard errors do you mean 
> e.g. beta1*se + theta1*se = totalse (where beta1 is the coefficient of 
> the direct impact and theta1 is the coefficient of the indirect impact)?
>
> Thank you for your help!
>
> Best wishes,
> Sarah
>
> ________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Wednesday, July 24, 2019 9:50:13 PM
> To: Letisha Sarah Fong Rui Zhen <geolsfrz at nus.edu.sg>
> Cc: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
> Subject: Re: [R-sig-Geo] Calculating impact measures for spatial panel models and trouble specifying SDEM model using spml
>
> On Wed, 24 Jul 2019, Letisha Sarah Fong Rui Zhen wrote:
>
>> Dear all,
>
> Please do not post HTML-formated messages.
>
>>
>> I???m working with panel data of 9 countries and 18 years and I???m
>> running fixed effects SDEM, SLX and SEM with the splm package.
>>
>> I have three questions:
>>
>> 1. I can???t seem to get the SDEM model to run. My code for each of the
>>    3 models is:
>>
>> model.SDEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE +
>> slag(ln(GDP), listw = my.listw) + slag((ln(GDP))^2, listw = my.listw),
>> data = my.data, listw = my.listw, model = ???within???, effect =
>> ???individual???, lag = F, spatial.error = ???b???)
>>
>> model.SLX <- plm(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE + slag(ln(GDP),
>> listw = my.listw) + slag((ln(GDP))^2, listw = my.listw), data = my.data,
>> model = ???within???, effect = ???individual???)
>>
>> model.SEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE, data =
>> my.data, listw = my.listw, model = ???within???, effect =
>> ???individual???, lag = F, spatial.error = ???b???)
>>
>> I am able to run both SLX and SEM models without problem, but when I try
>> to run the SDEM model, the error message is:
>>
>> Error in UseMethod("slag") :
>>  no applicable method for 'slag' applied to an object of class
>>  "c('double', 'numeric')"
>>
>> I don???t understand what is wrong here, as I have no problems with the
>> slag() function in the SLX model. My data is a pdataframe and each
>> variable is a numeric pseries.
>
> My guess would be that you should protect your square with I() in general,
> but have no idea - this is not a reproducible example.
>
>>
>>
>> 2. How can I calculate impact measures (direct, indirect and total) for
>>    spatial panel models?
>>
>> The impacts() function in spdep doesn???t work anymore and the impacts()
>> function from the spatialreg package seems to work only for
>> cross-sectional data and not panel data.
>>
>> For example, I ran:
>>
>> spatialreg::impacts(model.SLX)
>>
>> And the error message is:
>>
>> Error in UseMethod("impacts", obj) :
>>  no applicable method for 'impacts' applied to an object of class
>>  "c('plm', 'panelmodel')"
>>
>> I have tried methods(impacts) but none of the functions seem to work for
>> my SLX model created with the splm package.
>
> But your SLX model is created with the plm package, isn't it? The only use
> of splm is for the manual lags with slag()?
>
>>
>> I also looked at some previous examples in the splm documentation and
>> more specifically the spml() function and the example provided
>> (specifically the impact measures) doesn???t work anymore:
>>
>> data(Produc, package = "plm")
>> data(usaww)
>> fm <- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
>> ## random effects panel with spatial lag
>> respatlag <- spml(fm, data = Produc, listw = mat2listw(usaww),
>>                  model="random", spatial.error="none", lag=TRUE)
>> summary(respatlag) ## calculate impact measures impac1 <-
>> impacts(respatlag, listw = mat2listw(usaww, style = "W"), time = 17)
>> summary(impac1, zstats=TRUE, short=TRUE)
>>
>
> The implemented impacts methods in splm apply to the case where the lagged
> response is included. For SDEM and SLX, you can get the impacts by
> addition for the total impactss, and by linear combination for their
> standard errors. This is not implemented in a method. Further, slag() does
> not give any impacts method any information on which variables have been
> lagged - in your illustration above EI and RE are not lagged.
>
>> The error message when I run the impacts() function is:
>>
>> Error in UseMethod("impacts", obj) :
>>  no applicable method for 'impacts' applied to an object of class "splm"
>>
>> My question is therefore, how do I go about calculating direct, indirect
>> and total impact measures for spatial panel data?
>>
>>
>> 3. How can I test if the SDEM model can be simplified to the SLX model
>>    (since I estimate the SDEM by maximum likelihood (spml function) and
>>    the SLX by ordinary linear regression (plm function))? From my
>>    understanding the plm() function does not compute a loglikelihood or
>>    AIC so I probably can???t do a likelihood ratio test to choose
>>    between models (I haven???t tried this out because I???m stuck at
>>    running the SDEM model).
>
> Do you know definitely that plm does not provide a log likelihood? I
> realise that it isn't OLS unless pooled. Have you reviewed the JSS plm and
> splm articles?
>
> Roger
>
>>
>> Any help or advice would be greatly appreciated. Thank you.
>>
>> Best wishes,
>> Sarah
>>
>>
>>
>> ________________________________
>>
>> Important: This email is confidential and may be privileged. If you are
>> not the intended recipient, please delete it and notify us immediately;
>> you should not copy or use it for any purpose, nor disclose its contents
>> to any other person. Thank you.
>>
>>        [[alternative HTML version deleted]]
>>
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
> ________________________________
>
> Important: This email is confidential and may be privileged. If you are not the intended recipient, please delete it and notify us immediately; you should not copy or use it for any purpose, nor disclose its contents to any other person. Thank you.
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Sun Jul 28 16:39:51 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sun, 28 Jul 2019 16:39:51 +0200
Subject: [R-sig-Geo] 
 Calculating impact measures for spatial panel models
 and trouble specifying SDEM model using spml
In-Reply-To: <KL1PR0601MB3863CCFF4397920B3B68F36EEEC00@KL1PR0601MB3863.apcprd06.prod.outlook.com>
References: <SG2PR06MB387093D5FF97FD899829FB92EEC60@SG2PR06MB3870.apcprd06.prod.outlook.com>,
 <alpine.LFD.2.21.1907241534380.16556@reclus.nhh.no>,
 <SG2PR06MB3870C9253EB325F2D420C36BEEC10@SG2PR06MB3870.apcprd06.prod.outlook.com>
 <KL1PR0601MB3863CCFF4397920B3B68F36EEEC00@KL1PR0601MB3863.apcprd06.prod.outlook.com>
Message-ID: <alpine.LFD.2.21.1907281632350.1553@reclus.nhh.no>

On Fri, 26 Jul 2019, Letisha Sarah Fong Rui Zhen wrote:

> Dear Roger and all,
>
> In the end I used lm() to estimate my non-spatial FE models in order to extract the log likelihood values (the results are exactly the same when using the plm() function.
>
> I also used this method to estimate the SLX model. I just used the slag() function on each explanatory variable I wanted to have a spatial lag for, added it to my dataframe, and then estimated using lm() so that I can extract the log likelihood. I also managed to run my SDEM model with the spml() function without using slag() within the model.
>
> I just have one question: in the splm package documentation, there should be an extractable logLik value when I build a model using the spml() function
>
> However, when I do summary(spatial.SEM)$logLik, the result is NULL. When I do logLik(spatial.SEM), the error message is:
>
> Error in UseMethod("logLik") :
> no applicable method for 'logLik' applied to an object of class "splm"
>
> Since spml() uses maximum likelihood estimation so why can't I extract the log likelihood value?

Suggestion: find where in the code 
(https://r-forge.r-project.org/scm/viewvc.php/pkg/R/?root=splm) the 
optimization takes place, and suggest the relevant patch to the package 
maintainers. I can see some calls in R/likelihoodsFE.R, returning an opt 
object, later assigned to optres. I can't see optres returned (LL is even 
calculated but not returned either). Other files may cover other cases.

Roger

>
> Any help would be greatly appreciated!
>
> Best wishes,
> Letisha
> ________________________________
> From: Letisha Sarah Fong Rui Zhen <geolsfrz at nus.edu.sg>
> Sent: Thursday, July 25, 2019 5:16:43 PM
> To: Roger.Bivand at nhh.no <Roger.Bivand at nhh.no>
> Cc: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
> Subject: Re: [R-sig-Geo] Calculating impact measures for spatial panel models and trouble specifying SDEM model using spml
>
> Dear Roger,
>
> Thank you for your quick response.
>
> I have uploaded the spatial weights matrix and sample dataset I'm working with here: https://drive.google.com/drive/folders/1NjCODKEix-_nA5CfiIos6uiKAUbGp_BZ?usp=sharing
>
> Reading the data in and transforming them into a pdataframe and listw, respectively:
> spatialweight <- read.csv("spatialweight.csv", header = T)
> row.names(spatialweight) <- spatialweight$X
> spatialweight <- spatialweight[, -1]
> spatialweight.mat <- as.matrix(spatialweight)
> mylistw <- mat2listw(spatialweight.mat, style = "M")
> mydata <- read.csv("sampledata.csv", header = T)
> mydata <- pdata.frame(mydata, index = c("Country", "Year"))
>
> I first ran a non-spatial model to determine the best specification for fixed effects:
>
> nonspatial.pooledOLS <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "pooling")
> nonspatial.individualFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "individual")
> nonspatial.timeFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "time")
> nonspatial.twowayFE <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, model = "within", effect = "twoways")
>
> I would like to compare these models based on log likelihood and AIC, but the plm() function does not appear to provide a log likelihood or AIC. I have read through the JSS plm article and it states that models made with the plm() function are "estimated using the lm function to the transformed data". I'm aware that we can use logLik() and AIC() for a model estimated with the lm() function. However it doesn't seem to work with the plm() function.
>
> For example, I did logLik(nonspatial.twowayFE) and AIC(nonspatial.twowayFE) but the error message for both is:
>
> Error in UseMethod("logLik") :
>  no applicable method for 'logLik' applied to an object of class "c('plm', 'panelmodel')"
>
> Please let me know if I'm calling the wrong function(s) and/or if you're aware of a way to compare these models based on log likelihood and/or AIC.
>
> For the spatial models, here is my code:
>
> spatial.SDEM <- spml(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI + slag(lnGDP, listw = mylistw) + slag(lnGDP2, listw = mylistw) + slag(lnEI, listw = mylistw), data = mydata, listw = mylistw, model = "within", effect = "twoways", lag = F, spatial.error = "b")
> spatial.SEM <- spml(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI, data = mydata, listw = mylistw, model = "within", effect = "individual", lag = F, spatial.error = "b")
> spatial.SLX <- plm(lnY ~ lnGDP + lnGDP2 + U + RE + IS + lnEI + slag(lnGDP, listw = mylistw) + slag(lnGDP2, listw = mylistw) + slag(lnEI, listw = mylistw), data = mydata, model = "within", effect = "individual")
>
> As in my original post, the SLX and SEM models ran OK but the error when I try to run the SDEM model is:
>
> Error in UseMethod("slag") :
>  no applicable method for 'slag' applied to an object of class "c('double', 'numeric')"
>
> The variables that I use the slag() function on are all numeric, so I don't know what's wrong. I seem to be able to use slag() with plm() but not with spml(), but I don't know why this is so.
>
> I need to compare the models to see if SDEM can be reduced to one of its nested form. As was the case of the non-spatial models, I can't get the log likelihood for models created with the plm() function, so any suggestions are welcome. I've already read through the JSS articles for splm and plm as well as both documentations and there's no information on this (except that models built with the plm() function are estimated using the lm function to the transformed data).
>
> Thanks for clarifying the impact measures for SDEM and SLX. Just to check - when you say linear combination for standard errors do you mean e.g. beta1*se + theta1*se = totalse (where beta1 is the coefficient of the direct impact and theta1 is the coefficient of the indirect impact)?
>
> Thank you for your help!
>
> Best wishes,
> Sarah
>
> ________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Wednesday, July 24, 2019 9:50:13 PM
> To: Letisha Sarah Fong Rui Zhen <geolsfrz at nus.edu.sg>
> Cc: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
> Subject: Re: [R-sig-Geo] Calculating impact measures for spatial panel models and trouble specifying SDEM model using spml
>
> On Wed, 24 Jul 2019, Letisha Sarah Fong Rui Zhen wrote:
>
>> Dear all,
>
> Please do not post HTML-formated messages.
>
>>
>> I???m working with panel data of 9 countries and 18 years and I???m
>> running fixed effects SDEM, SLX and SEM with the splm package.
>>
>> I have three questions:
>>
>> 1. I can???t seem to get the SDEM model to run. My code for each of the
>>    3 models is:
>>
>> model.SDEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE +
>> slag(ln(GDP), listw = my.listw) + slag((ln(GDP))^2, listw = my.listw),
>> data = my.data, listw = my.listw, model = ???within???, effect =
>> ???individual???, lag = F, spatial.error = ???b???)
>>
>> model.SLX <- plm(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE + slag(ln(GDP),
>> listw = my.listw) + slag((ln(GDP))^2, listw = my.listw), data = my.data,
>> model = ???within???, effect = ???individual???)
>>
>> model.SEM <- spml(ln(Y) ~ ln(GDP) + (ln(GDP))^2 + EI + RE, data =
>> my.data, listw = my.listw, model = ???within???, effect =
>> ???individual???, lag = F, spatial.error = ???b???)
>>
>> I am able to run both SLX and SEM models without problem, but when I try
>> to run the SDEM model, the error message is:
>>
>> Error in UseMethod("slag") :
>>  no applicable method for 'slag' applied to an object of class
>>  "c('double', 'numeric')"
>>
>> I don???t understand what is wrong here, as I have no problems with the
>> slag() function in the SLX model. My data is a pdataframe and each
>> variable is a numeric pseries.
>
> My guess would be that you should protect your square with I() in general,
> but have no idea - this is not a reproducible example.
>
>>
>>
>> 2. How can I calculate impact measures (direct, indirect and total) for
>>    spatial panel models?
>>
>> The impacts() function in spdep doesn???t work anymore and the impacts()
>> function from the spatialreg package seems to work only for
>> cross-sectional data and not panel data.
>>
>> For example, I ran:
>>
>> spatialreg::impacts(model.SLX)
>>
>> And the error message is:
>>
>> Error in UseMethod("impacts", obj) :
>>  no applicable method for 'impacts' applied to an object of class
>>  "c('plm', 'panelmodel')"
>>
>> I have tried methods(impacts) but none of the functions seem to work for
>> my SLX model created with the splm package.
>
> But your SLX model is created with the plm package, isn't it? The only use
> of splm is for the manual lags with slag()?
>
>>
>> I also looked at some previous examples in the splm documentation and
>> more specifically the spml() function and the example provided
>> (specifically the impact measures) doesn???t work anymore:
>>
>> data(Produc, package = "plm")
>> data(usaww)
>> fm <- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
>> ## random effects panel with spatial lag
>> respatlag <- spml(fm, data = Produc, listw = mat2listw(usaww),
>>                  model="random", spatial.error="none", lag=TRUE)
>> summary(respatlag) ## calculate impact measures impac1 <-
>> impacts(respatlag, listw = mat2listw(usaww, style = "W"), time = 17)
>> summary(impac1, zstats=TRUE, short=TRUE)
>>
>
> The implemented impacts methods in splm apply to the case where the lagged
> response is included. For SDEM and SLX, you can get the impacts by
> addition for the total impactss, and by linear combination for their
> standard errors. This is not implemented in a method. Further, slag() does
> not give any impacts method any information on which variables have been
> lagged - in your illustration above EI and RE are not lagged.
>
>> The error message when I run the impacts() function is:
>>
>> Error in UseMethod("impacts", obj) :
>>  no applicable method for 'impacts' applied to an object of class "splm"
>>
>> My question is therefore, how do I go about calculating direct, indirect
>> and total impact measures for spatial panel data?
>>
>>
>> 3. How can I test if the SDEM model can be simplified to the SLX model
>>    (since I estimate the SDEM by maximum likelihood (spml function) and
>>    the SLX by ordinary linear regression (plm function))? From my
>>    understanding the plm() function does not compute a loglikelihood or
>>    AIC so I probably can???t do a likelihood ratio test to choose
>>    between models (I haven???t tried this out because I???m stuck at
>>    running the SDEM model).
>
> Do you know definitely that plm does not provide a log likelihood? I
> realise that it isn't OLS unless pooled. Have you reviewed the JSS plm and
> splm articles?
>
> Roger
>
>>
>> Any help or advice would be greatly appreciated. Thank you.
>>
>> Best wishes,
>> Sarah
>>
>>
>>
>> ________________________________
>>
>> Important: This email is confidential and may be privileged. If you are
>> not the intended recipient, please delete it and notify us immediately;
>> you should not copy or use it for any purpose, nor disclose its contents
>> to any other person. Thank you.
>>
>>        [[alternative HTML version deleted]]
>>
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
> ________________________________
>
> Important: This email is confidential and may be privileged. If you are not the intended recipient, please delete it and notify us immediately; you should not copy or use it for any purpose, nor disclose its contents to any other person. Thank you.
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Sun Jul 28 17:08:23 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sun, 28 Jul 2019 17:08:23 +0200
Subject: [R-sig-Geo] how to specify the size of device so that it fits a
 SpatialPolygonsDataFrame plot exactly
In-Reply-To: <04cc3d06-a85d-427a-17a2-40988643f03e@univ-fcomte.fr>
References: <04cc3d06-a85d-427a-17a2-40988643f03e@univ-fcomte.fr>
Message-ID: <alpine.LFD.2.21.1907281705240.1553@reclus.nhh.no>

On Sat, 27 Jul 2019, Patrick Giraudoux wrote:

> Dear listers,
>
> I would like to write jpeg (or png, etc.) maps using the function 
> graphics:jpeg (png, etc.), but with the device window exactly fitting the map 
> (= no white strip above, below or on the sides of the map). The map is 
> derived from a SpatialPolygonsDataFrame (WGS84)
>
> However, even if I set
>
> par(mar=c(0,0,0,0) and pass xaxs='i', yaxs='i' in the plot, there are still 
> white strips
>
> I tried several tricks but none are fully satisfying.
>
> Trial #1: I computed the ratio of the bounding box and using this ratio in 
> the function jpeg with the height and width arguments. I did it both in WGS84 
> and in UTM47:
>
> ratio<-(max(bbox(ChinaUTM)[1,])-min(bbox(ChinaUTM)[1,]))/(max(bbox(ChinaUTM)[2,])-min(bbox(ChinaUTM)[2,])) 
> # ratio width/height
>
> then
>
> jpeg("./MapChina.jpeg",height=8,width=8*ratio,res=300,units="in")
>
> par(mar=c(0,0,0,0)) # no margin
> plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')
>
> dev.off()
>
> The best is obtained with UTM47 (planar projection), however I get (almost) 
> rid of any white strip only if I add an additionnal 1.04 coefficient 
> (obtained by trial-error)
>
> Hence:
>
> jpeg("./MapChina.jpeg",height=8,width=8*ratio*1.04,res=300,units="in")
>
> par(mar=c(0,0,0,0)) # no margin
> plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')
>
> dev.off()
>
> Trial #2: The other way has been to pick up the parameter values like that:
>
> par(mar=c(0,0,0,0)) # no margin
> plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')
> pt<-par()$pin
>
> ratio<-pt[2]/pt[1]
>
> jpeg("./MapChina.jpeg",height=8,width=8*ratio,res=300,units="in")
> par(mar=c(0,0,0,0)) # no margin
> plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i') #
>
> dev.off()
>
> Does not work either
>
> Any idea about how to deal with this ? In short how to get the exact size of 
> a SpatialPolygonDataFrame plot to make the device window exactly this? size?

This feels like a section in ASDAR, ch. 3, and code chunks 24-27 in 
https://asdar-book.org/book2ed/vis_mod.R. Could you please try that first 
by way of something reproducible?

Best wishes,

Roger

>
> Best,
>
> Patrick
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r  Sun Jul 28 17:15:53 2019
From: p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r (Patrick Giraudoux)
Date: Sun, 28 Jul 2019 17:15:53 +0200
Subject: [R-sig-Geo] how to specify the size of device so that it fits a
 SpatialPolygonsDataFrame plot exactly
In-Reply-To: <alpine.LFD.2.21.1907281705240.1553@reclus.nhh.no>
References: <04cc3d06-a85d-427a-17a2-40988643f03e@univ-fcomte.fr>
 <alpine.LFD.2.21.1907281705240.1553@reclus.nhh.no>
Message-ID: <a759a48e-0ecf-aeb2-884d-d60bb39e9994@univ-fcomte.fr>

Le 28/07/2019 ? 17:08, Roger Bivand a ?crit?:
> On Sat, 27 Jul 2019, Patrick Giraudoux wrote:
>
>> Dear listers,
>>
>> I would like to write jpeg (or png, etc.) maps using the function 
>> graphics:jpeg (png, etc.), but with the device window exactly fitting 
>> the map (= no white strip above, below or on the sides of the map). 
>> The map is derived from a SpatialPolygonsDataFrame (WGS84)
>>
>> However, even if I set
>>
>> par(mar=c(0,0,0,0) and pass xaxs='i', yaxs='i' in the plot, there are 
>> still white strips
>>
>> I tried several tricks but none are fully satisfying.
>>
>> Trial #1: I computed the ratio of the bounding box and using this 
>> ratio in the function jpeg with the height and width arguments. I did 
>> it both in WGS84 and in UTM47:
>>
>> ratio<-(max(bbox(ChinaUTM)[1,])-min(bbox(ChinaUTM)[1,]))/(max(bbox(ChinaUTM)[2,])-min(bbox(ChinaUTM)[2,])) 
>> # ratio width/height
>>
>> then
>>
>> jpeg("./MapChina.jpeg",height=8,width=8*ratio,res=300,units="in")
>>
>> par(mar=c(0,0,0,0)) # no margin
>> plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')
>>
>> dev.off()
>>
>> The best is obtained with UTM47 (planar projection), however I get 
>> (almost) rid of any white strip only if I add an additionnal 1.04 
>> coefficient (obtained by trial-error)
>>
>> Hence:
>>
>> jpeg("./MapChina.jpeg",height=8,width=8*ratio*1.04,res=300,units="in")
>>
>> par(mar=c(0,0,0,0)) # no margin
>> plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')
>>
>> dev.off()
>>
>> Trial #2: The other way has been to pick up the parameter values like 
>> that:
>>
>> par(mar=c(0,0,0,0)) # no margin
>> plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i')
>> pt<-par()$pin
>>
>> ratio<-pt[2]/pt[1]
>>
>> jpeg("./MapChina.jpeg",height=8,width=8*ratio,res=300,units="in")
>> par(mar=c(0,0,0,0)) # no margin
>> plot(China,col="grey",border="white",lwd=2,xaxs='i', yaxs='i') #
>>
>> dev.off()
>>
>> Does not work either
>>
>> Any idea about how to deal with this ? In short how to get the exact 
>> size of a SpatialPolygonDataFrame plot to make the device window 
>> exactly this? size?
>
> This feels like a section in ASDAR, ch. 3, and code chunks 24-27 in 
> https://asdar-book.org/book2ed/vis_mod.R. Could you please try that 
> first by way of something reproducible?
>
> Best wishes,
>
> Roger 


Ups... How? can I have missed this chapter of my bible ;-) ? (must admit 
I had been on google first). Will re-read it carefully and come back to 
the list with a solution or a reproducible example, indeed.

Best,

Patrick


From p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r  Sun Jul 28 18:00:30 2019
From: p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r (Patrick Giraudoux)
Date: Sun, 28 Jul 2019 18:00:30 +0200
Subject: [R-sig-Geo] how to specify the size of device so that it fits a
 SpatialPolygonsDataFrame plot exactly
In-Reply-To: <a759a48e-0ecf-aeb2-884d-d60bb39e9994@univ-fcomte.fr>
References: <04cc3d06-a85d-427a-17a2-40988643f03e@univ-fcomte.fr>
 <alpine.LFD.2.21.1907281705240.1553@reclus.nhh.no>
 <a759a48e-0ecf-aeb2-884d-d60bb39e9994@univ-fcomte.fr>
Message-ID: <e2010eea-1789-2a2c-652a-4df2cfbad1a1@univ-fcomte.fr>

> Le 28/07/2019 ? 17:08, Roger Bivand a ?crit?: 
> This feels like a section in ASDAR, ch. 3, and code chunks 24-27 in 
> https://asdar-book.org/book2ed/vis_mod.R. Could you please try that 
> first by way of something reproducible?

Le 28/07/2019 ? 17:15, Patrick Giraudoux a ?crit?:
>
> Ups... How? can I have missed this chapter of my bible ;-) ? (must 
> admit I had been on google first). Will re-read it carefully and come 
> back to the list with a solution or a reproducible example, indeed.


OK. Read it again, I was not totally lost. Here is a reproducible 
example. To ease reproducibility with simple objects, I will use two 
bounding boxes.? bbChina in WGS84, bbChinaUTM47 in UTM47. I want a 
window fitting the WGS84, and you'll see I get it through a strange way.

bbChina <- new("SpatialPolygons", polygons = list(new("Polygons", 
Polygons = list( new("Polygon", labpt = c(104.8, 35.95), area = 2372.28, 
hole = FALSE, ringDir = 1L, coords = structure(c(73, 73, 136.6, 136.6, 
73, 17.3, 54.6, 54.6, 17.3, 17.3), .Dim = c(5L, 2L)))), plotOrder = 1L, 
labpt = c(104.8, 35.95), ID = "1", area = 2372.28)), plotOrder = 1L, 
bbox = structure(c(73, 17.3, 136.6, 54.6), .Dim = c(2L, 2L), .Dimnames = 
list(c("x", "y"), c("min", "max"))), proj4string = new("CRS", projargs = 
"+init=epsg:4326 +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 
+towgs84=0,0,0"))

bbChinaUTM47 <- new("SpatialPolygons", polygons = list(new("Polygons", 
Polygons = list( new("Polygon", labpt = c(856106.391943348, 
4317264.60126758 ), area = 30651262771540.1, hole = FALSE, ringDir = 1L, 
coords = structure(c(-2331000.09677063, -2331000.09677063, 
4043212.88065733, 4043212.88065733, -2331000.09677063, 1912947.1678777, 
6721582.03465746, 6721582.03465746, 1912947.1678777, 1912947.1678777), 
.Dim = c(5L, 2L)))), plotOrder = 1L, labpt = c(856106.391943348, 
4317264.60126758), ID = "1", area = 30651262771540.1)), plotOrder = 1L, 
bbox = structure(c(-2331000.09677063, 1912947.1678777, 4043212.88065733, 
6721582.03465746), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"), 
c("min", "max"))), proj4string = new("CRS", projargs = "+init=epsg:4326 
+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))

Then let's go:

Example #1: here, being straightforward we get two indesirable white 
strips on the sides:

width1<-max(bbox(bbChina)[1,])-min(bbox(bbChina)[1,])
height1<-max(bbox(bbChina)[2,])-min(bbox(bbChina)[2,])
ratio<-width1/height1
ratio
windows(height=8,width=8*ratio)
par(mar=c(0,0,0,0))
plot(bbChina,col="grey",xaxs='i', yaxs='i')
dev.off()

Example #2: computing the ratio on UTM47, but plotting WGS84 (strange), 
I get a better fit but with still two small white strips up and down.

width1<-max(bbox(bbChinaUTM47)[1,])-min(bbox(bbChinaUTM47)[1,])
height1<-max(bbox(bbChinaUTM47)[2,])-min(bbox(bbChinaUTM47)[2,])
ratio<-width1/height1
ratio
windows(height=8,width=8*ratio)
par(mar=c(0,0,0,0))
plot(bbChina,col="grey",xaxs='i', yaxs='i') # no data range extention 
(xaxs and yaxs parameter)

dev.off()

Example #3: multiplying the ratio by 1.04, I get a good fit

width1<-max(bbox(bbChinaUTM47)[1,])-min(bbox(bbChinaUTM47)[1,])
height1<-max(bbox(bbChinaUTM47)[2,])-min(bbox(bbChinaUTM47)[2,])
ratio<-width1/height1
ratio
windows(height=8,width=8*ratio*1.04)
par(mar=c(0,0,0,0))
plot(bbChina,col="grey",xaxs='i', yaxs='i')

dev.off()

Looks like the issue has something to do with the way CRS are handled 
when plotting objects, mmmh ? Tricky isn't it ?




	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Jul 29 11:12:57 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 29 Jul 2019 11:12:57 +0200
Subject: [R-sig-Geo] how to specify the size of device so that it fits a
 SpatialPolygonsDataFrame plot exactly
In-Reply-To: <e2010eea-1789-2a2c-652a-4df2cfbad1a1@univ-fcomte.fr>
References: <04cc3d06-a85d-427a-17a2-40988643f03e@univ-fcomte.fr>
 <alpine.LFD.2.21.1907281705240.1553@reclus.nhh.no>
 <a759a48e-0ecf-aeb2-884d-d60bb39e9994@univ-fcomte.fr>
 <e2010eea-1789-2a2c-652a-4df2cfbad1a1@univ-fcomte.fr>
Message-ID: <alpine.LFD.2.21.1907291056470.13109@reclus.nhh.no>

On Sun, 28 Jul 2019, Patrick Giraudoux wrote:

>>  Le 28/07/2019 ? 17:08, Roger Bivand a ?crit?: This feels like a section in
>>  ASDAR, ch. 3, and code chunks 24-27 in
>>  https://asdar-book.org/book2ed/vis_mod.R. Could you please try that first
>>  by way of something reproducible?
>
> Le 28/07/2019 ? 17:15, Patrick Giraudoux a ?crit?:
>>
>>  Ups... How? can I have missed this chapter of my bible ;-) ? (must admit I
>>  had been on google first). Will re-read it carefully and come back to the
>>  list with a solution or a reproducible example, indeed.
>
>
> OK. Read it again, I was not totally lost. Here is a reproducible example. To 
> ease reproducibility with simple objects, I will use two bounding boxes.? 
> bbChina in WGS84, bbChinaUTM47 in UTM47. I want a window fitting the WGS84, 
> and you'll see I get it through a strange way.
>
> bbChina <- new("SpatialPolygons", polygons = list(new("Polygons", Polygons = 
> list( new("Polygon", labpt = c(104.8, 35.95), area = 2372.28, hole = FALSE, 
> ringDir = 1L, coords = structure(c(73, 73, 136.6, 136.6, 73, 17.3, 54.6, 
> 54.6, 17.3, 17.3), .Dim = c(5L, 2L)))), plotOrder = 1L, labpt = c(104.8, 
> 35.95), ID = "1", area = 2372.28)), plotOrder = 1L, bbox = structure(c(73, 
> 17.3, 136.6, 54.6), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"), c("min", 
> "max"))), proj4string = new("CRS", projargs = "+init=epsg:4326 +proj=longlat 
> +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
>
> bbChinaUTM47 <- new("SpatialPolygons", polygons = list(new("Polygons", 
> Polygons = list( new("Polygon", labpt = c(856106.391943348, 4317264.60126758 
> ), area = 30651262771540.1, hole = FALSE, ringDir = 1L, coords = 
> structure(c(-2331000.09677063, -2331000.09677063, 4043212.88065733, 
> 4043212.88065733, -2331000.09677063, 1912947.1678777, 6721582.03465746, 
> 6721582.03465746, 1912947.1678777, 1912947.1678777), .Dim = c(5L, 2L)))), 
> plotOrder = 1L, labpt = c(856106.391943348, 4317264.60126758), ID = "1", area 
> = 30651262771540.1)), plotOrder = 1L, bbox = structure(c(-2331000.09677063, 
> 1912947.1678777, 4043212.88065733, 6721582.03465746), .Dim = c(2L, 2L), 
> .Dimnames = list(c("x", "y"), c("min", "max"))), proj4string = new("CRS", 
> projargs = "+init=epsg:4326 +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 
> +towgs84=0,0,0"))
>
> Then let's go:
>
> Example #1: here, being straightforward we get two indesirable white strips 
> on the sides:
>
> width1<-max(bbox(bbChina)[1,])-min(bbox(bbChina)[1,])
> height1<-max(bbox(bbChina)[2,])-min(bbox(bbChina)[2,])
> ratio<-width1/height1
> ratio
> windows(height=8,width=8*ratio)
> par(mar=c(0,0,0,0))
> plot(bbChina,col="grey",xaxs='i', yaxs='i')
> dev.off()
>
> Example #2: computing the ratio on UTM47, but plotting WGS84 (strange), I get 
> a better fit but with still two small white strips up and down.
>
> width1<-max(bbox(bbChinaUTM47)[1,])-min(bbox(bbChinaUTM47)[1,])
> height1<-max(bbox(bbChinaUTM47)[2,])-min(bbox(bbChinaUTM47)[2,])
> ratio<-width1/height1
> ratio
> windows(height=8,width=8*ratio)
> par(mar=c(0,0,0,0))
> plot(bbChina,col="grey",xaxs='i', yaxs='i') # no data range extention (xaxs 
> and yaxs parameter)
>
> dev.off()
>
> Example #3: multiplying the ratio by 1.04, I get a good fit
>
> width1<-max(bbox(bbChinaUTM47)[1,])-min(bbox(bbChinaUTM47)[1,])
> height1<-max(bbox(bbChinaUTM47)[2,])-min(bbox(bbChinaUTM47)[2,])
> ratio<-width1/height1
> ratio
> windows(height=8,width=8*ratio*1.04)
> par(mar=c(0,0,0,0))
> plot(bbChina,col="grey",xaxs='i', yaxs='i')
>
> dev.off()
>
> Looks like the issue has something to do with the way CRS are handled when 
> plotting objects, mmmh ? Tricky isn't it ?
>

Yes, and the section in the book only discusses projected objects, as 
geographical coordinates are stretched N-S proportionally to the distance 
from the Equator. For the UTM47 object, I have:

library(sp)
bbChinaUTM47 <- 
SpatialPolygons(list(Polygons(list(Polygon(matrix(c(-2331000.09677063, 
-2331000.09677063, 4043212.88065733, 4043212.88065733, -2331000.09677063, 
1912947.1678777, 6721582.03465746, 6721582.03465746, 1912947.1678777, 
1912947.1678777), ncol=2))), ID="1")), proj4string=CRS("+proj=utm 
+zone=47")) # you had longlat, so triggering the stretching.

x11() # or equivalent
dxy <- apply(bbox(bbChinaUTM47), 1, diff)
dxy
ratio <- dxy[1]/dxy[2]
ratio
pin <- par("pin")
pin
par(pin=c(ratio * pin[2], pin[2]), xaxs="i", yaxs="i")
plot(bbChinaUTM47)
box()

where the box overlaps the SP object. To finesse:

c(ratio * pin[2], pin[2])
dev.off()
X11(width=6.85, height=5.2)
par(mar=c(0,0,0,0)+0.1)
pin <- par("pin")
par(pin=c(ratio * pin[2], pin[2]), xaxs="i", yaxs="i")
plot(bbChinaUTM47)
box()
dev.off()

From plot.Spatial(), asp is set to 1/cos((mean(ylim) * pi)/180 for 
geographical coordinates, where ylim is a possibly modified version of the 
N-S bounding box. This makes it harder to automate, as you'd need to 
manipulate dxy[2] above to match. So for projected objects, the book 
approach works, for non-projected objects you'd need an extra step.

Hope this helps,

Roger

>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r  Mon Jul 29 12:11:13 2019
From: p@tr|ck@g|r@udoux @end|ng |rom un|v-|comte@|r (Patrick Giraudoux)
Date: Mon, 29 Jul 2019 12:11:13 +0200
Subject: [R-sig-Geo] how to specify the size of device so that it fits a
 SpatialPolygonsDataFrame plot exactly
In-Reply-To: <alpine.LFD.2.21.1907291056470.13109@reclus.nhh.no>
References: <04cc3d06-a85d-427a-17a2-40988643f03e@univ-fcomte.fr>
 <alpine.LFD.2.21.1907281705240.1553@reclus.nhh.no>
 <a759a48e-0ecf-aeb2-884d-d60bb39e9994@univ-fcomte.fr>
 <e2010eea-1789-2a2c-652a-4df2cfbad1a1@univ-fcomte.fr>
 <alpine.LFD.2.21.1907291056470.13109@reclus.nhh.no>
Message-ID: <b688d8f2-ef7b-11c8-a265-f724d3717311@univ-fcomte.fr>

Le 29/07/2019 ? 11:12, Roger Bivand a ?crit?:
> On Sun, 28 Jul 2019, Patrick Giraudoux wrote:
>
>>> ?Le 28/07/2019 ? 17:08, Roger Bivand a ?crit?: This feels like a 
>>> section in
>>> ?ASDAR, ch. 3, and code chunks 24-27 in
>>> ?https://asdar-book.org/book2ed/vis_mod.R. Could you please try that 
>>> first
>>> ?by way of something reproducible?
>>
>> Le 28/07/2019 ? 17:15, Patrick Giraudoux a ?crit?:
>>>
>>> ?Ups... How? can I have missed this chapter of my bible ;-) ? (must 
>>> admit I
>>> ?had been on google first). Will re-read it carefully and come back 
>>> to the
>>> ?list with a solution or a reproducible example, indeed.
>>
>>
>> OK. Read it again, I was not totally lost. Here is a reproducible 
>> example. To ease reproducibility with simple objects, I will use two 
>> bounding boxes.? bbChina in WGS84, bbChinaUTM47 in UTM47. I want a 
>> window fitting the WGS84, and you'll see I get it through a strange way.
>>
>> bbChina <- new("SpatialPolygons", polygons = list(new("Polygons", 
>> Polygons = list( new("Polygon", labpt = c(104.8, 35.95), area = 
>> 2372.28, hole = FALSE, ringDir = 1L, coords = structure(c(73, 73, 
>> 136.6, 136.6, 73, 17.3, 54.6, 54.6, 17.3, 17.3), .Dim = c(5L, 2L)))), 
>> plotOrder = 1L, labpt = c(104.8, 35.95), ID = "1", area = 2372.28)), 
>> plotOrder = 1L, bbox = structure(c(73, 17.3, 136.6, 54.6), .Dim = 
>> c(2L, 2L), .Dimnames = list(c("x", "y"), c("min", "max"))), 
>> proj4string = new("CRS", projargs = "+init=epsg:4326 +proj=longlat 
>> +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
>>
>> bbChinaUTM47 <- new("SpatialPolygons", polygons = 
>> list(new("Polygons", Polygons = list( new("Polygon", labpt = 
>> c(856106.391943348, 4317264.60126758 ), area = 30651262771540.1, hole 
>> = FALSE, ringDir = 1L, coords = structure(c(-2331000.09677063, 
>> -2331000.09677063, 4043212.88065733, 4043212.88065733, 
>> -2331000.09677063, 1912947.1678777, 6721582.03465746, 
>> 6721582.03465746, 1912947.1678777, 1912947.1678777), .Dim = c(5L, 
>> 2L)))), plotOrder = 1L, labpt = c(856106.391943348, 
>> 4317264.60126758), ID = "1", area = 30651262771540.1)), plotOrder = 
>> 1L, bbox = structure(c(-2331000.09677063, 1912947.1678777, 
>> 4043212.88065733, 6721582.03465746), .Dim = c(2L, 2L), .Dimnames = 
>> list(c("x", "y"), c("min", "max"))), proj4string = new("CRS", 
>> projargs = "+init=epsg:4326 +proj=longlat +datum=WGS84 +no_defs 
>> +ellps=WGS84 +towgs84=0,0,0"))
>>
>> Then let's go:
>>
>> Example #1: here, being straightforward we get two indesirable white 
>> strips on the sides:
>>
>> width1<-max(bbox(bbChina)[1,])-min(bbox(bbChina)[1,])
>> height1<-max(bbox(bbChina)[2,])-min(bbox(bbChina)[2,])
>> ratio<-width1/height1
>> ratio
>> windows(height=8,width=8*ratio)
>> par(mar=c(0,0,0,0))
>> plot(bbChina,col="grey",xaxs='i', yaxs='i')
>> dev.off()
>>
>> Example #2: computing the ratio on UTM47, but plotting WGS84 
>> (strange), I get a better fit but with still two small white strips 
>> up and down.
>>
>> width1<-max(bbox(bbChinaUTM47)[1,])-min(bbox(bbChinaUTM47)[1,])
>> height1<-max(bbox(bbChinaUTM47)[2,])-min(bbox(bbChinaUTM47)[2,])
>> ratio<-width1/height1
>> ratio
>> windows(height=8,width=8*ratio)
>> par(mar=c(0,0,0,0))
>> plot(bbChina,col="grey",xaxs='i', yaxs='i') # no data range extention 
>> (xaxs and yaxs parameter)
>>
>> dev.off()
>>
>> Example #3: multiplying the ratio by 1.04, I get a good fit
>>
>> width1<-max(bbox(bbChinaUTM47)[1,])-min(bbox(bbChinaUTM47)[1,])
>> height1<-max(bbox(bbChinaUTM47)[2,])-min(bbox(bbChinaUTM47)[2,])
>> ratio<-width1/height1
>> ratio
>> windows(height=8,width=8*ratio*1.04)
>> par(mar=c(0,0,0,0))
>> plot(bbChina,col="grey",xaxs='i', yaxs='i')
>>
>> dev.off()
>>
>> Looks like the issue has something to do with the way CRS are handled 
>> when plotting objects, mmmh ? Tricky isn't it ?
>>
>
> Yes, and the section in the book only discusses projected objects, as 
> geographical coordinates are stretched N-S proportionally to the 
> distance from the Equator. For the UTM47 object, I have:
>
> library(sp)
> bbChinaUTM47 <- 
> SpatialPolygons(list(Polygons(list(Polygon(matrix(c(-2331000.09677063, 
> -2331000.09677063, 4043212.88065733, 4043212.88065733, 
> -2331000.09677063, 1912947.1678777, 6721582.03465746, 
> 6721582.03465746, 1912947.1678777, 1912947.1678777), ncol=2))), 
> ID="1")), proj4string=CRS("+proj=utm +zone=47")) # you had longlat, so 
> triggering the stretching.
>
> x11() # or equivalent
> dxy <- apply(bbox(bbChinaUTM47), 1, diff)
> dxy
> ratio <- dxy[1]/dxy[2]
> ratio
> pin <- par("pin")
> pin
> par(pin=c(ratio * pin[2], pin[2]), xaxs="i", yaxs="i")
> plot(bbChinaUTM47)
> box()
>
> where the box overlaps the SP object. To finesse:
>
> c(ratio * pin[2], pin[2])
> dev.off()
> X11(width=6.85, height=5.2)
> par(mar=c(0,0,0,0)+0.1)
> pin <- par("pin")
> par(pin=c(ratio * pin[2], pin[2]), xaxs="i", yaxs="i")
> plot(bbChinaUTM47)
> box()
> dev.off()
>
> From plot.Spatial(), asp is set to 1/cos((mean(ylim) * pi)/180 for 
> geographical coordinates, where ylim is a possibly modified version of 
> the N-S bounding box. This makes it harder to automate, as you'd need 
> to manipulate dxy[2] above to match. So for projected objects, the 
> book approach works, for non-projected objects you'd need an extra step.
>
> Hope this helps,
>
> Roger 


Yes, indeed. Thanks. When one understands fully what's happens, the 
easier to adapt...

Now I can go ahead cleanly...

Best,

Patrick


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Mon Jul 29 23:12:00 2019
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Mon, 29 Jul 2019 17:12:00 -0400
Subject: [R-sig-Geo] 
 Simple Ripley's CRS test for market point patters --- addendum.
In-Reply-To: <449bd5c0-a6b7-ecf7-4553-ea4d33c13c82@auckland.ac.nz>
References: <1075794206.5056673.1563831392519.ref@mail.yahoo.com>
 <1075794206.5056673.1563831392519@mail.yahoo.com>
 <8a7d0858-37ea-de97-b988-5f29dfcc38c8@auckland.ac.nz>
 <dd69fad4-192c-ae1c-5096-65b9c2e7f29f@yahoo.com.br>
 <3d447698-95dc-24e9-9e60-333042d1c3e9@auckland.ac.nz>
 <449bd5c0-a6b7-ecf7-4553-ea4d33c13c82@auckland.ac.nz>
Message-ID: <6a542aad-c547-390c-0f86-3cefa3fdbd55@yahoo.com.br>

Dr. Rolf Turner,

 ???? Thanks again and your explanations as a personal spatial course for 
me :) You are very clear, but there are some questions yet. First, these 
data set is not artificial, I have real coordinates and area of nests of 
leaf-cutting ants in my example (in my complete data set I have more 
than 3,000 coordinates and areas). The problem of overlap is because we 
represent a ants nests area as a circle (for an easy comprehension and 
modeling), but in the real world this areas was elliptical and some 
times we have an eccentricity close to 1 (in this case I have two nests 
close but not in overlap condition in the field). Based on our comments, 
I used the CSRI test, but first I try to explore more the question about 
intensities and size categories (s, m, and l), for this,could I test 
intensities by pair-wise comparison? For the last, do you have any 
solution for my overlap problem, if my objective is the study of my 
problem as a realization of a Poisson process?

Please see my code below:

#Packages
require(spatstat)
require(sp)

# Create some points that represent ant nests

xp<-c(371278.588,371250.722,371272.618,371328.421,371349.974,
371311.95,371296.265,371406.46,371411.551,371329.041,371338.081,
371334.182,371333.756,371299.818,371254.374,371193.673,371172.836,
371173.803,371153.73,371165.051,371140.417,371168.279,371166.367,
371180.575,371132.664,371129.791,371232.919,371208.502,371289.462,
371207.595,371219.008,371139.921,371133.215,371061.467,371053.69,
371099.897,371108.782,371112.52,371114.241,371176.236,371159.185,
371159.291,371158.552,370978.252,371120.03,371116.993)

yp<-c(8246507.94,8246493.176,8246465.974,8246464.413,8246403.465,
8246386.098,8246432.144,8246394.827,8246366.201,8246337.626,
8246311.125,8246300.039,8246299.594,8246298.072,8246379.351,
8246431.998,8246423.913,8246423.476,8246431.658,8246418.226,
8246400.161,8246396.891,8246394.225,8246400.391,8246370.244,
8246367.019,8246311.075,8246255.174,8246255.085,8246226.514,
8246215.847,8246337.316,8246330.197,8246311.197,8246304.183,
8246239.282,8246239.887,8246241.678,8246240.361,8246167.364,
8246171.581,8246171.803,8246169.807,8246293.57,8246183.194,8246189.926)

#Now I have the size of each nest (marked process)

area<-c(117,30,4,341,15,160,35,280,108,168,63,143,2,48,182,42,
88,56,27,156,288,45,49,234,72,270,91,40,304,56,35,4,56.7,9,4.6,
105,133,135,23.92,190,12.9,15.2,192.78,104,255,24)

# Make a contour for the window creation
W <- convexhull.xy(xp,yp)


# Class of nests size - large, medium and small
acat <- cut(area,breaks=c(-Inf,25,55,Inf),right=FALSE, 
labels=c("s","m","l"))

#Create a ppp object

syn.ppp <- ppp(x=xp,y=yp,window=W, marks=acat)

#Test initial hypothesis
f1 <- ppm(syn.ppp) # Same (constant) intensity for each area category.
f2 <- ppm(syn.ppp ~ marks) # Allows different (constant) intensity for 
each area category.
anova(f1,f2,test="Chi") #The test?? rejects the hypothesis that the 
intensities are the same - p =0.002015 **

*#First question, could I test intensities by pair-wise comparison?*

# Small vs medium sizes
acat2<-acat
levels(acat2)
levels(acat2)[1]<-"s_m"
levels(acat2)[2]<-"s_m"
levels(acat2)
syn.ppp2 <- ppp(x=xp,y=yp,window=W, marks=acat2)
f3 <- ppm(syn.ppp2 ~ marks)
f4 <- ppm(syn.ppp2)
anova(f4,f3,test="Chi") #Intensities of small and medium size are the 
same - p=0.237

# Medium vs large sizes
acat3<-acat
levels(acat3)
levels(acat3)[2]<-"m_l"
levels(acat3)[3]<-"m_l"
levels(acat3)
syn.ppp3 <- ppp(x=xp,y=yp,window=W, marks=acat3)
f5 <- ppm(syn.ppp3 ~ marks)
f6 <- ppm(syn.ppp3)
anova(f5,f6,test="Chi") #Intensities of medium and large sizes are the 
different - p=7.827e-05 ***

Finally small and medium sizes has the same intensity but is different 
of large ants size!!! With this condition I will test the CSRI:

*#Now testing CSRI by simulation*

foo <- function(W){
s_m <- runifpoint(length(area<55),win=W)
l <- runifpoint(length(area>=55),win=W)
superimpose(s_m=s_m,l=l)
}
simex <- expression(foo(W))

#and then

set.seed(12)
E <- envelope(syn.ppp2,simulate=simex,nsim=999, savefuns=TRUE)
plot(E)
dtst <- dclf.test(E)
plot(dtst)
mtst <- mad.test(E)
plot(mtst)
# now gives p-value = 0.003 for dtst and p-value = 0.002 for mtst

Thanks in advanced,

Alexandre

-- 
======================================================================
Alexandre dos Santos
Prote????o Florestal
IFMT - Instituto Federal de Educa????o, Ci??ncia e Tecnologia de Mato Grosso
Campus C??ceres
Caixa Postal 244
Avenida dos Ramires, s/n
Bairro: Distrito Industrial
C??ceres - MT                      CEP: 78.200-000
Fone: (+55) 65 99686-6970 (VIVO) (+55) 65 3221-2674 (FIXO)

         alexandre.santos at cas.ifmt.edu.br
Lattes: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
Researchgate: www.researchgate.net/profile/Alexandre_Santos10
LinkedIn: br.linkedin.com/in/alexandre-dos-santos-87961635
Mendeley:www.mendeley.com/profiles/alexandre-dos-santos6/
======================================================================

Em 26/07/2019 17:29, Rolf Turner escreveu:
>
> I have realised that there were a couple of oversights in my previous 
> posting on this issue.?? One is a bit subtle; the other is a bit of a 
> blunder on my part.
>
> First the "subtle" one. The test that I proposed for CSRI is a test 
> done using the estimated parameters of the proposed model to generate 
> realisations of data sets under the null hypothesis. Such tests tend 
> to be conservative.?? (See section 10.6.3, p. 388 ff., in [1].)
>
> In the current instance (testing for CSRI) the conservatism can be 
> overcome by simulating data conditional on the numbers of points of 
> each type in the "real" data.?? This can be done here via:
>
> foo <- function(W){
> s <- runifpoint(10,win=W)
> m <- runifpoint(9,win=W)
> l <- runifpoint(27,win=W)
> superimpose(s=s,m=m,l=l)
> }
> simex <- expression(foo(W))
>
> and then
>
> set.seed(42)
> E <- envelope(syn.ppp,simulate=simex,savefuns=TRUE)
> dtst <- dclf.test(E)
> mtst <- mad.test(E)
>
> This gives p-values of 0.06 from the dclf test and 0.09 from the mad 
> test.?? Thus there appears to be some slight evidence against the null 
> hypothesis.?? (Evidence at the 0.10 significance level.)
>
> That this should be so is *OBVIOUS* (!!!) if we turn to the unsubtle 
> point that I overlooked.?? It is clear that the pattern of ants' nests 
> cannot be truly a realisation of a Poisson process since there must be 
> a bit of a "hard core" effect.?? Two ants' nests cannot overlap.?? Thus 
> if we approximate the shape of each nest by a disc, points i and j 
> must be a distance of at least r_i + r_j from each other, where r_i = 
> sqrt(area_i/pi), and similarly for r_j.
>
> However I note that the data provided seem to violate this principle 
> in several instances.?? E.g. points 41 and 42 are a distance of only 
> 0.2460 metres apart but areas 41 and 42 are 12.9 and 15.2 square 
> metres, yielding putative radii of 3.5917 and 3.8987 metres, whence 
> the closest
> these points could possibly be (under the "disc-shaped assumption") is
> 7.4904 metres, far larger than 0.2460.???? So something is a bit out of 
> whack here.?? Perhaps these are made-up ("synthetic") data and the 
> process of making up the data did not take account of the minimum 
> distance constraint.
>
> How to incorporate the "hard core" aspect of your (real?) data into 
> the modelling exercise, and what the impact of it is upon your 
> research question(s), is unclear to me and is likely to be complicated.
>
> cheers,
>
> Rolf
>

	[[alternative HTML version deleted]]


From ty|er@j@|r@z|er @end|ng |rom |c|oud@com  Tue Jul 30 00:25:11 2019
From: ty|er@j@|r@z|er @end|ng |rom |c|oud@com (Tyler Frazier)
Date: Mon, 29 Jul 2019 18:25:11 -0400
Subject: [R-sig-Geo] st_sample with raster
Message-ID: <E697FE84-B258-4A29-A068-A73FB52DAC88@icloud.com>

Hi All,

Spatstat::rpoint has an argument f that accepts a pixel image object (class .im) to generate points based on the proportionate probability density of that raster.  Just wondering, does sf::st_sample or perhaps another function have a similar available method?

Thanks so much!
Ty

Tyler Frazier, Ph.D.
Lecturer of Interdisciplinary Studies
Data Science Program
College of William and Mary
Webpage: http://tjfrazier.people.wm.edu
Phone: +01 (757) 386-1269

From m||uj|@b @end|ng |rom gm@||@com  Wed Jul 31 16:44:18 2019
From: m||uj|@b @end|ng |rom gm@||@com (Miluji Sb)
Date: Wed, 31 Jul 2019 16:44:18 +0200
Subject: [R-sig-Geo] Convert data.frame/SpatialPointsDataFrame to raster
Message-ID: <CAMLwc7N9FGwF0rVB1eL1CGWpyUFU73B9tU7nQ2vE0_OamHrMZw@mail.gmail.com>

Dear all,

I have georeferenced dataset with multiple variables and years. The data is
at ~100 km (1? ? 1?) spatial resolution. I would like to convert this into
a raster.

I have filtered the data for one year and one variable and did the
following;

try <- subset(df, year==2010)
try <- try[,c(1,2,4)]
try$lon <- round(try$lon)
try$lat <- round(try$lat)
r_imp <- rasterFromXYZ(try)

Two issues; is it possible to convert the original dataset with the
multiple variables and years to a raster? If not, how can I avoid rounding
the coordinates? Currently, I get this error "Error in rasterFromXYZ(try) :
x cell sizes are not regular" without rounding.

Any help will be greatly appreciated. Thank you!

Sincerely,

Shouro

## Data
df <- structure(list(lon = c(180, 179.762810919291, 179.523658017568,
179.311342656601, 179.067616041778, 178.851382109362, 178.648816406322,
178.501097394651, 178.662722495847, 178.860599151485), lat =
c(-16.1529296875,
-16.21659020822, -16.266117894201, -16.393550535614, -16.4457378034442,
-16.561653799838, -16.6533087696649, -16.7741069281329, -16.914110607613,
-16.9049389730284), nsdec = structure(c(1L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 2L), .Label = c("1 of 10", "10 of 10", "2 of 10",
"3 of 10", "4 of 10", "5 of 10", "6 of 10", "7 of 10", "8 of 10",
"9 of 10"), class = "factor"), TWL_5 = c(2.13810426616849,
2.16767864033646,
2.16881240361846, 2.20727073247015, 2.27771608519709, 2.3649601141941,
2.44210984856767, 2.52466349543977, 2.63982954290745, 2.71828906773926
), TWL_50 = c(2.38302354555823, 2.43142793944275, 2.45733044901087,
2.53057109758284, 2.61391337469939, 2.71040967066483, 2.82546443373866,
2.9709907727849, 3.1785797371187, 3.33227647990861), TWL_95 =
c(2.63753852023063,
2.7080249053612, 2.75483681166049, 2.86893038433795, 2.97758282474101,
3.14541928966618, 3.3986143008625, 3.68043269045659, 4.09571655859075,
4.57299670034984), year = c(2010, 2020, 2030, 2040, 2050, 2060,
2070, 2080, 2090, 2100)), row.names = c(NA, 10L), class = "data.frame")

	[[alternative HTML version deleted]]


From v|j@y|u||@ @end|ng |rom gm@||@com  Wed Jul 31 21:20:47 2019
From: v|j@y|u||@ @end|ng |rom gm@||@com (Vijay Lulla)
Date: Wed, 31 Jul 2019 15:20:47 -0400
Subject: [R-sig-Geo] Convert data.frame/SpatialPointsDataFrame to raster
In-Reply-To: <CAMLwc7N9FGwF0rVB1eL1CGWpyUFU73B9tU7nQ2vE0_OamHrMZw@mail.gmail.com>
References: <CAMLwc7N9FGwF0rVB1eL1CGWpyUFU73B9tU7nQ2vE0_OamHrMZw@mail.gmail.com>
Message-ID: <CAKkiGbs6D6ah3oGVWJWff9ihf3sr4zXbV263JBYBv_GcdjJzJw@mail.gmail.com>

?`rasterFromXYZ` states that "x and y represent spatial coordinates and
must be on a regular grid."  And, it appears to me that you might be losing
values by rounding lon/lat values.  The help file further suggests that
`rasterize` might be the function you're looking for.  List members will
(certainly I will) find it more helpful to propose other solutions if you
post a small reproducible example of your original georeferenced dataset so
that we get an idea of what data you're using.

Sorry, I cannot be of more help.

On Wed, Jul 31, 2019 at 10:45 AM Miluji Sb <milujisb at gmail.com> wrote:

> Dear all,
>
> I have georeferenced dataset with multiple variables and years. The data is
> at ~100 km (1? ? 1?) spatial resolution. I would like to convert this into
> a raster.
>
> I have filtered the data for one year and one variable and did the
> following;
>
> try <- subset(df, year==2010)
> try <- try[,c(1,2,4)]
> try$lon <- round(try$lon)
> try$lat <- round(try$lat)
> r_imp <- rasterFromXYZ(try)
>
> Two issues; is it possible to convert the original dataset with the
> multiple variables and years to a raster? If not, how can I avoid rounding
> the coordinates? Currently, I get this error "Error in rasterFromXYZ(try) :
> x cell sizes are not regular" without rounding.
>
> Any help will be greatly appreciated. Thank you!
>
> Sincerely,
>
> Shouro
>
> ## Data
> df <- structure(list(lon = c(180, 179.762810919291, 179.523658017568,
> 179.311342656601, 179.067616041778, 178.851382109362, 178.648816406322,
> 178.501097394651, 178.662722495847, 178.860599151485), lat =
> c(-16.1529296875,
> -16.21659020822, -16.266117894201, -16.393550535614, -16.4457378034442,
> -16.561653799838, -16.6533087696649, -16.7741069281329, -16.914110607613,
> -16.9049389730284), nsdec = structure(c(1L, 3L, 4L, 5L, 6L, 7L,
> 8L, 9L, 10L, 2L), .Label = c("1 of 10", "10 of 10", "2 of 10",
> "3 of 10", "4 of 10", "5 of 10", "6 of 10", "7 of 10", "8 of 10",
> "9 of 10"), class = "factor"), TWL_5 = c(2.13810426616849,
> 2.16767864033646,
> 2.16881240361846, 2.20727073247015, 2.27771608519709, 2.3649601141941,
> 2.44210984856767, 2.52466349543977, 2.63982954290745, 2.71828906773926
> ), TWL_50 = c(2.38302354555823, 2.43142793944275, 2.45733044901087,
> 2.53057109758284, 2.61391337469939, 2.71040967066483, 2.82546443373866,
> 2.9709907727849, 3.1785797371187, 3.33227647990861), TWL_95 =
> c(2.63753852023063,
> 2.7080249053612, 2.75483681166049, 2.86893038433795, 2.97758282474101,
> 3.14541928966618, 3.3986143008625, 3.68043269045659, 4.09571655859075,
> 4.57299670034984), year = c(2010, 2020, 2030, 2040, 2050, 2060,
> 2070, 2080, 2090, 2100)), row.names = c(NA, 10L), class = "data.frame")
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From m||uj|@b @end|ng |rom gm@||@com  Wed Jul 31 22:15:55 2019
From: m||uj|@b @end|ng |rom gm@||@com (Miluji Sb)
Date: Wed, 31 Jul 2019 22:15:55 +0200
Subject: [R-sig-Geo] Convert data.frame/SpatialPointsDataFrame to raster
In-Reply-To: <CAKkiGbs6D6ah3oGVWJWff9ihf3sr4zXbV263JBYBv_GcdjJzJw@mail.gmail.com>
References: <CAMLwc7N9FGwF0rVB1eL1CGWpyUFU73B9tU7nQ2vE0_OamHrMZw@mail.gmail.com>
 <CAKkiGbs6D6ah3oGVWJWff9ihf3sr4zXbV263JBYBv_GcdjJzJw@mail.gmail.com>
Message-ID: <CAMLwc7Nqer4HUBK_AoiFHvAsx=d+Md0EMhODBcrFDePVe6WRcA@mail.gmail.com>

Hello,

Thank you for your kind reply.  Here is a snapshot of the original data. I
had pasted it at the bottom of my first email but forgot to mention it.
Thanks again!

df <- structure(list(lon = c(180, 179.762810919291, 179.523658017568,
179.311342656601, 179.067616041778, 178.851382109362, 178.648816406322,
178.501097394651, 178.662722495847, 178.860599151485), lat =
c(-16.1529296875,
-16.21659020822, -16.266117894201, -16.393550535614, -16.4457378034442,
-16.561653799838, -16.6533087696649, -16.7741069281329, -16.914110607613,
-16.9049389730284), nsdec = structure(c(1L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 2L), .Label = c("1 of 10", "10 of 10", "2 of 10",
"3 of 10", "4 of 10", "5 of 10", "6 of 10", "7 of 10", "8 of 10",
"9 of 10"), class = "factor"), TWL_5 = c(2.13810426616849,
2.16767864033646,
2.16881240361846, 2.20727073247015, 2.27771608519709, 2.3649601141941,
2.44210984856767, 2.52466349543977, 2.63982954290745, 2.71828906773926
), TWL_50 = c(2.38302354555823, 2.43142793944275, 2.45733044901087,
2.53057109758284, 2.61391337469939, 2.71040967066483, 2.82546443373866,
2.9709907727849, 3.1785797371187, 3.33227647990861), TWL_95 =
c(2.63753852023063,
2.7080249053612, 2.75483681166049, 2.86893038433795, 2.97758282474101,
3.14541928966618, 3.3986143008625, 3.68043269045659, 4.09571655859075,
4.57299670034984), year = c(2010, 2020, 2030, 2040, 2050, 2060,
2070, 2080, 2090, 2100)), row.names = c(NA, 10L), class = "data.frame")

Sincerely,

Milu

On Wed, Jul 31, 2019 at 9:20 PM Vijay Lulla <vijaylulla at gmail.com> wrote:

> ?`rasterFromXYZ` states that "x and y represent spatial coordinates and
> must be on a regular grid."  And, it appears to me that you might be losing
> values by rounding lon/lat values.  The help file further suggests that
> `rasterize` might be the function you're looking for.  List members will
> (certainly I will) find it more helpful to propose other solutions if you
> post a small reproducible example of your original georeferenced dataset so
> that we get an idea of what data you're using.
>
> Sorry, I cannot be of more help.
>
> On Wed, Jul 31, 2019 at 10:45 AM Miluji Sb <milujisb at gmail.com> wrote:
>
>> Dear all,
>>
>> I have georeferenced dataset with multiple variables and years. The data
>> is
>> at ~100 km (1? ? 1?) spatial resolution. I would like to convert this into
>> a raster.
>>
>> I have filtered the data for one year and one variable and did the
>> following;
>>
>> try <- subset(df, year==2010)
>> try <- try[,c(1,2,4)]
>> try$lon <- round(try$lon)
>> try$lat <- round(try$lat)
>> r_imp <- rasterFromXYZ(try)
>>
>> Two issues; is it possible to convert the original dataset with the
>> multiple variables and years to a raster? If not, how can I avoid rounding
>> the coordinates? Currently, I get this error "Error in rasterFromXYZ(try)
>> :
>> x cell sizes are not regular" without rounding.
>>
>> Any help will be greatly appreciated. Thank you!
>>
>> Sincerely,
>>
>> Shouro
>>
>> ## Data
>> df <- structure(list(lon = c(180, 179.762810919291, 179.523658017568,
>> 179.311342656601, 179.067616041778, 178.851382109362, 178.648816406322,
>> 178.501097394651, 178.662722495847, 178.860599151485), lat =
>> c(-16.1529296875,
>> -16.21659020822, -16.266117894201, -16.393550535614, -16.4457378034442,
>> -16.561653799838, -16.6533087696649, -16.7741069281329, -16.914110607613,
>> -16.9049389730284), nsdec = structure(c(1L, 3L, 4L, 5L, 6L, 7L,
>> 8L, 9L, 10L, 2L), .Label = c("1 of 10", "10 of 10", "2 of 10",
>> "3 of 10", "4 of 10", "5 of 10", "6 of 10", "7 of 10", "8 of 10",
>> "9 of 10"), class = "factor"), TWL_5 = c(2.13810426616849,
>> 2.16767864033646,
>> 2.16881240361846, 2.20727073247015, 2.27771608519709, 2.3649601141941,
>> 2.44210984856767, 2.52466349543977, 2.63982954290745, 2.71828906773926
>> ), TWL_50 = c(2.38302354555823, 2.43142793944275, 2.45733044901087,
>> 2.53057109758284, 2.61391337469939, 2.71040967066483, 2.82546443373866,
>> 2.9709907727849, 3.1785797371187, 3.33227647990861), TWL_95 =
>> c(2.63753852023063,
>> 2.7080249053612, 2.75483681166049, 2.86893038433795, 2.97758282474101,
>> 3.14541928966618, 3.3986143008625, 3.68043269045659, 4.09571655859075,
>> 4.57299670034984), year = c(2010, 2020, 2030, 2040, 2050, 2060,
>> 2070, 2080, 2090, 2100)), row.names = c(NA, 10L), class = "data.frame")
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
>

	[[alternative HTML version deleted]]


From v|j@y|u||@ @end|ng |rom gm@||@com  Wed Jul 31 22:45:48 2019
From: v|j@y|u||@ @end|ng |rom gm@||@com (Vijay Lulla)
Date: Wed, 31 Jul 2019 16:45:48 -0400
Subject: [R-sig-Geo] Convert data.frame/SpatialPointsDataFrame to raster
In-Reply-To: <CAMLwc7Nqer4HUBK_AoiFHvAsx=d+Md0EMhODBcrFDePVe6WRcA@mail.gmail.com>
References: <CAMLwc7N9FGwF0rVB1eL1CGWpyUFU73B9tU7nQ2vE0_OamHrMZw@mail.gmail.com>
 <CAKkiGbs6D6ah3oGVWJWff9ihf3sr4zXbV263JBYBv_GcdjJzJw@mail.gmail.com>
 <CAMLwc7Nqer4HUBK_AoiFHvAsx=d+Md0EMhODBcrFDePVe6WRcA@mail.gmail.com>
Message-ID: <CAKkiGbtOK6A80P7azAQMdt-aNdjXgJQDpnsP2jO76jx=TnOmCQ@mail.gmail.com>

Hmm...I had seen your data and thought that it was just some sample that
you'd shared.  If this is your whole data then I don't know how to create a
raster from just one row that is returned from subsetting the dataframe.

Sorry for the noise.

On Wed, Jul 31, 2019 at 4:16 PM Miluji Sb <milujisb at gmail.com> wrote:

> Hello,
>
> Thank you for your kind reply.  Here is a snapshot of the original data. I
> had pasted it at the bottom of my first email but forgot to mention it.
> Thanks again!
>
> df <- structure(list(lon = c(180, 179.762810919291, 179.523658017568,
> 179.311342656601, 179.067616041778, 178.851382109362, 178.648816406322,
> 178.501097394651, 178.662722495847, 178.860599151485), lat =
> c(-16.1529296875,
> -16.21659020822, -16.266117894201, -16.393550535614, -16.4457378034442,
> -16.561653799838, -16.6533087696649, -16.7741069281329, -16.914110607613,
> -16.9049389730284), nsdec = structure(c(1L, 3L, 4L, 5L, 6L, 7L,
> 8L, 9L, 10L, 2L), .Label = c("1 of 10", "10 of 10", "2 of 10",
> "3 of 10", "4 of 10", "5 of 10", "6 of 10", "7 of 10", "8 of 10",
> "9 of 10"), class = "factor"), TWL_5 = c(2.13810426616849,
> 2.16767864033646,
> 2.16881240361846, 2.20727073247015, 2.27771608519709, 2.3649601141941,
> 2.44210984856767, 2.52466349543977, 2.63982954290745, 2.71828906773926
> ), TWL_50 = c(2.38302354555823, 2.43142793944275, 2.45733044901087,
> 2.53057109758284, 2.61391337469939, 2.71040967066483, 2.82546443373866,
> 2.9709907727849, 3.1785797371187, 3.33227647990861), TWL_95 =
> c(2.63753852023063,
> 2.7080249053612, 2.75483681166049, 2.86893038433795, 2.97758282474101,
> 3.14541928966618, 3.3986143008625, 3.68043269045659, 4.09571655859075,
> 4.57299670034984), year = c(2010, 2020, 2030, 2040, 2050, 2060,
> 2070, 2080, 2090, 2100)), row.names = c(NA, 10L), class = "data.frame")
>
> Sincerely,
>
> Milu
>
> On Wed, Jul 31, 2019 at 9:20 PM Vijay Lulla <vijaylulla at gmail.com> wrote:
>
>> ?`rasterFromXYZ` states that "x and y represent spatial coordinates and
>> must be on a regular grid."  And, it appears to me that you might be losing
>> values by rounding lon/lat values.  The help file further suggests that
>> `rasterize` might be the function you're looking for.  List members will
>> (certainly I will) find it more helpful to propose other solutions if you
>> post a small reproducible example of your original georeferenced dataset so
>> that we get an idea of what data you're using.
>>
>> Sorry, I cannot be of more help.
>>
>> On Wed, Jul 31, 2019 at 10:45 AM Miluji Sb <milujisb at gmail.com> wrote:
>>
>>> Dear all,
>>>
>>> I have georeferenced dataset with multiple variables and years. The data
>>> is
>>> at ~100 km (1? ? 1?) spatial resolution. I would like to convert this
>>> into
>>> a raster.
>>>
>>> I have filtered the data for one year and one variable and did the
>>> following;
>>>
>>> try <- subset(df, year==2010)
>>> try <- try[,c(1,2,4)]
>>> try$lon <- round(try$lon)
>>> try$lat <- round(try$lat)
>>> r_imp <- rasterFromXYZ(try)
>>>
>>> Two issues; is it possible to convert the original dataset with the
>>> multiple variables and years to a raster? If not, how can I avoid
>>> rounding
>>> the coordinates? Currently, I get this error "Error in
>>> rasterFromXYZ(try) :
>>> x cell sizes are not regular" without rounding.
>>>
>>> Any help will be greatly appreciated. Thank you!
>>>
>>> Sincerely,
>>>
>>> Shouro
>>>
>>> ## Data
>>> df <- structure(list(lon = c(180, 179.762810919291, 179.523658017568,
>>> 179.311342656601, 179.067616041778, 178.851382109362, 178.648816406322,
>>> 178.501097394651, 178.662722495847, 178.860599151485), lat =
>>> c(-16.1529296875,
>>> -16.21659020822, -16.266117894201, -16.393550535614, -16.4457378034442,
>>> -16.561653799838, -16.6533087696649, -16.7741069281329, -16.914110607613,
>>> -16.9049389730284), nsdec = structure(c(1L, 3L, 4L, 5L, 6L, 7L,
>>> 8L, 9L, 10L, 2L), .Label = c("1 of 10", "10 of 10", "2 of 10",
>>> "3 of 10", "4 of 10", "5 of 10", "6 of 10", "7 of 10", "8 of 10",
>>> "9 of 10"), class = "factor"), TWL_5 = c(2.13810426616849,
>>> 2.16767864033646,
>>> 2.16881240361846, 2.20727073247015, 2.27771608519709, 2.3649601141941,
>>> 2.44210984856767, 2.52466349543977, 2.63982954290745, 2.71828906773926
>>> ), TWL_50 = c(2.38302354555823, 2.43142793944275, 2.45733044901087,
>>> 2.53057109758284, 2.61391337469939, 2.71040967066483, 2.82546443373866,
>>> 2.9709907727849, 3.1785797371187, 3.33227647990861), TWL_95 =
>>> c(2.63753852023063,
>>> 2.7080249053612, 2.75483681166049, 2.86893038433795, 2.97758282474101,
>>> 3.14541928966618, 3.3986143008625, 3.68043269045659, 4.09571655859075,
>>> 4.57299670034984), year = c(2010, 2020, 2030, 2040, 2050, 2060,
>>> 2070, 2080, 2090, 2100)), row.names = c(NA, 10L), class = "data.frame")
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>>
>>

	[[alternative HTML version deleted]]


