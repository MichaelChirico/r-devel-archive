From Roger.Bivand at nhh.no  Mon May  1 11:23:07 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 1 May 2017 11:23:07 +0200
Subject: [R-sig-Geo] sp::disaggregate() does not seem to add the
 necessary "comment" attribute to identify holes if Polygons does not
 contain exactly 1 polygon with 0 or 1 holes
In-Reply-To: <30d71999-b99a-dabd-8de9-709b782fae03@ofb.net>
References: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>
 <alpine.LFD.2.20.1704281804080.27020@reclus.nhh.no>
 <78988b99-67ef-8129-6622-aad0a4946920@ofb.net>
 <alpine.LFD.2.20.1704291551270.27676@reclus.nhh.no>
 <dbf4f008-23ae-3767-efae-6df317dc21c9@ofb.net>
 <30d71999-b99a-dabd-8de9-709b782fae03@ofb.net>
Message-ID: <alpine.LFD.2.20.1705011103180.6598@reclus.nhh.no>

On Sun, 30 Apr 2017, Anne C. Hanna wrote:

> Okay, I think I've tracked it down.  This is... absurdly complicated and I
> don't know what the actual correct resolution is, but I'm pretty sure that at
> least I know what's going wrong.

Good, and thanks for staying with this. I've pushed a fix to my github 
repo, and pull request #29 to edzer/sp.

Note that this also works out of the box:

library(sf)
sf <- st_read(dsn = "2011 Voting District Boundary Shapefiles", layer = 
"MCDS", stringsAsFactors = FALSE)
sfd <- st_cast(sf, "POLYGON")

However, using sf::st_touches() is a bit less intuitive for now than the 
spdep "nb" object generated by spdep::poly2nb(), which also provides Queen 
(touch at one or more boundary points) and Rook (touch at more than one 
boundary point), and sf::st_touches() is seven times slower than 
spdep::poly2nb() now. sf::st_touches() could be speeded up using an STR 
tree (spdep::poly2nb() can do this, but in any case only looks at 
candidate neighbours with intersecting bounding boxes). So:

sp_sfd <- as(sfd, "Spatial")
library(spdep)
sp_nb <- poly2nb(sp_sfd)

seems a reasonable workflow to find the adjacencies under Queen/Rook and 
snapping control.

Please try out the disaggregate fix and report back - and consider 
shifting to sf.

Roger

>
> Basically it comes down to the fact that createSPComment() does not actually
> check whether each individual polygon in the SpatialPolygons object has a
> comment (much less a correct comment).  Instead, it looks at a top-level
> "comment" attribute.  If that top-level comment attribute (i.e. sf at comment for
> my SpatialPolygonsDataFrame object stored as "sf") is "TRUE", then
> createSPComment() does nothing.
>
> And here's where the second layer of the problem comes in for my dataset ---
> my original data, pre-disaggregation, has complete and correct comments.  Some
> of those comments (for objects where it's a single polygon with 0 or 1 holes)
> are being retained by the basic disaggregation algorithm, while others are
> not.  Your patch attempts to fill in the holes by running createSPComment() on
> the final results.  But... the thing you are running createSPComment() on is
> SpatialPolygons(p), where p is the list of disaggregated polygons.  And the
> SpatialPolygons() constructor looks at the list of the polygons you feed it,
> and if *any* of those polygons have comments, it sets the top-level "comment"
> attribute of its output to "TRUE", even if other polygons are missing
> comments.  (And it also doesn't check the comments it gets for correctness.)
> So the fact that some polygon comments are retained in p means that
> SpatialPolygons() lies to createSPComment() about whether it has any missing
> comments that need to be fixed.
>
> On the other hand, when I manually run createPolygonComment() on the
> individual Polygons objects in disaggregate()'s output, I don't even look at
> the top-level "comment" attribute, so it works just fine.
>
> My little toy test polygons, on the other hand, were just not complex enough
> to end up with partial comments in them, so they also didn't experience this
> error.
>
> I am not familiar enough with the design philosophy behind sp to suggest at
> what level this issue should be fixed, but I hope this is at least enough info
> to help those who do know what they're doing get it corrected.
>
> - Anne
>
>
> On 04/30/2017 12:18 PM, Anne C. Hanna wrote:
>> Roger,
>>
>> Unfortunately I have a use case for you of createSPComment() not working.
>>
>> I tried your new version of disaggregate() on the original test script I sent
>> you, and it does seem to have fixed all the cases in there.  However, when I
>> tried it on my actual data, it had the same failure mode as before. The
>> original dataset has what appear to be correct comments, but the
>> disaggregate() output is full of NULLs in place of comments for the multi-part
>> and multi-hole polygons.
>>
>> createSPComment() appears to be what's failing, rather than just some logic in
>> your disaggregate() function --- when I try to run createSPComment() on the
>> disaggregate() output, I still get the same set of NULLs.  I can fix it if I
>> run createPolygonsComment() individually on every Polygons object in my
>> SpatialPolygonsDataFrame.
>>
>> I'm not sure what is different between my dataset and the test polygons I was
>> using earlier, as createSPComment() seems to handle the test shapes just fine
>> (and converting my SpatialPolygonsDataFrame into just a set of SpatialPolygons
>> also doesn't help).  But presumably it is better to have createSPComment()
>> fixed than to have to work around it in disaggregate().  So I guess I'll look
>> at the code for that and see if I can find what's wrong.
>>
>> Just in case you want to see this in action, I've attached a little test
>> script.  The data set I am working with is available at:
>>
>> http://aws.redistricting.state.pa.us/Redistricting/Resources/GISData/2011-Voting-District-Boundary-Shapefiles.zip
>>
>>  - Anne
>>
>>
>> On 04/29/2017 10:06 AM, Roger Bivand wrote:
>>> On Sat, 29 Apr 2017, Anne C. Hanna wrote:
>>>
>>>> Roger,
>>>>
>>>> This looks great, and I will try it out ASAP.  I do have one reservation
>>>> though --- it seems you are using createSPComment() to reconstruct the
>>>> comments, and I have seen some discussion that that may not be reliable in
>>>> all cases (e.g. if the initial polygons are wonky in some way).  I don't
>>>> know a lot about this, but is it possible that it would be preferable to
>>>> parse and appropriately disaggregate the original comments strings (if they
>>>> exist), so as to deal slightly more smoothly with such cases (i.e., the
>>>> polygons would still be wonky, but at least the hole/polygon matching would
>>>> track with whatever was in the original data)?
>>>
>>> Contributions very welcome - this was code moved from the raster package, so I
>>> really have little feeling for why it is necessary.
>>>
>>> Please provide references and use cases. GEOS is strict in its treatments of
>>> geometries, but does work in most cases. People just expressing opinions
>>> doesn't help, and may well be misleading. It is possible to clean geometries
>>> too, see the cleangeo package. createPolygonsComment probably isn't foolproof,
>>> but specific reports help, because then it is possible to do something about
>>> it. sp classes didn't use simple features when written because sf was not
>>> widely used then - introduced in 2004, when sp was being developed. Using sf
>>> helps, and rgeos::createPolygonsComment was a work-around from 2010 when rgeos
>>> was written.
>>>
>>> Even when you use sf in the sf package, you will still run into invalid
>>> geometries because the geometries are in fact invalid, the sf package also
>>> uses GEOS.
>>>
>>> Roger
>>>
>>>>
>>>> I also had no success using createSPComment to fix disaggregate()'s output
>>>> previously, even though my polygons are perfectly non-wonky, so I am perhaps
>>>> a little more untrusting of it than I should be.  But I'll let you know how
>>>> this version works with my data.  Thanks for addressing this so quickly!
>>>>
>>>> - Anne
>>>>
>>>>
>>>> On 04/28/2017 12:11 PM, Roger Bivand wrote:
>>>>> I've pushed the fix to my fork:
>>>>>
>>>>> https://github.com/rsbivand/sp
>>>>>
>>>>> and created a pull request:
>>>>>
>>>>> https://github.com/edzer/sp/pull/28
>>>>>
>>>>> Only one part of a complicated set if nested if() in disaggregate() was adding
>>>>> comments, but in some settings the existing comments survived the
>>>>> disaggregation. Now the Polygons object comment attribute is re-created for
>>>>> all Polygons objects. This is version 1.2-6, also including code changes that
>>>>> internally affect rgdal and rgeos - you may like to re-install them from
>>>>> source after installing this sp version (shouldn't matter).
>>>>>
>>>>> Roger
>>>>>
>>>>> On Fri, 28 Apr 2017, Anne C. Hanna wrote:
>>>>>
>>>>>> Hello.  I first posted this issue report on the sp GitHub repo
>>>>>> (https://github.com/edzer/sp/issues/27) and it was suggested that I redirect
>>>>>> it to here.
>>>>>>
>>>>>> I am working with a geographic dataset with complex borders. The data is
>>>>>> stored as a SpatialPolygonsDataFrame.  Each Polygons object in the data frame
>>>>>> may be composed of multiple disjoint polygons, and each polygon may have
>>>>>> multiple holes.  I want to disaggregate each of the Polygons objects into its
>>>>>> individual disjoint polygons and construct an adjacency matrix for all the
>>>>>> disjoint components, and I was using disaggregate() to do this.  However,
>>>>>> when
>>>>>> I run gTouches() on the disaggregated data, in order to compute the
>>>>>> adjacencies, I get a number of warnings like this:
>>>>>>
>>>>>> Error in RGEOSBinPredFunc(spgeom1, spgeom2, byid, "rgeos_touches") : Polygons
>>>>>> object missing comment attribute ignoring hole(s).  See function
>>>>>> createSPComment.
>>>>>>
>>>>>> Looking at the Polygons "comment" attributes in the SpatialPolygonsDataFrame
>>>>>> output by disaggregate(), I see that the only comment values are "0"
>>>>>> (indicating a single polygon with no holes), "0 1" (indicating a single
>>>>>> polygon with a single hole), and NULL (apparently no comment was written).
>>>>>> Since I know my dataset contains several Polygons objects which are composed
>>>>>> of multiple disjoint regions, and also several Polygons which contain more
>>>>>> than one hole, this is not the expected result.  In reading the
>>>>>> disaggregate()
>>>>>> code in the sp GitHub repository (specifically, explodePolygons()), I also
>>>>>> can't see anywhere the comment is being added for the cases where a Polygons
>>>>>> object has more than two parts or more than two holes.  It actually seems
>>>>>> like
>>>>>> it's getting carried along almost accidentally in the few cases that do get
>>>>>> comments, and neglected otherwise.
>>>>>>
>>>>>> Assuming I'm not failing to understand the code and the desired behavior
>>>>>> (entirely possible, as I am new at working with this software!), this seems
>>>>>> suboptimal to me.  My dataset is pretty well-behaved (despite its
>>>>>> complexity),
>>>>>> so I should be able to fix my issues with judicious application of
>>>>>> createPolygonsComment.  But I had a heck of a time figuring out what was
>>>>>> going
>>>>>> wrong with gTouches, since Polygons comment management appears to be a pretty
>>>>>> obscure field (and createSPComment wasn't working for me, for whatever
>>>>>> reason).  So it seems like it might be better if disaggregate() just parses
>>>>>> and passes along the comments from its input correctly, or, if it's
>>>>>> absolutely
>>>>>> necessary to not create comments, passes nothing and warns clearly in the
>>>>>> manual that comments and associated hole information are being lost.  Passing
>>>>>> along comments in some cases while silently dropping them in others seems
>>>>>> like
>>>>>> kind of the worst of both worlds.
>>>>>>
>>>>>> I've attached a set of tests I wrote to demonstrate desired/undesired
>>>>>> behavior: disaggregate_comment_tests.R.  My R version is 3.4.0, my sp version
>>>>>> is 1.2-4, my rgeos version is 0.3-23 (SVN revision 546), and my GEOS runtime
>>>>>> version is 3.5.1-CAPI-1.9.1 r4246.  I am using Debian Release 9.0 with kernel
>>>>>> version 4.9.0-2-amd64.  I hope this is useful; please let me know if you need
>>>>>> more info or if there is a better place to post this issue.
>>>>>>
>>>>>> - Anne
>>>>>>
>>>>>
>>>>
>>>>
>>>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From HodgessE at uhd.edu  Mon May  1 16:06:23 2017
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Mon, 1 May 2017 14:06:23 +0000
Subject: [R-sig-Geo] authorized model formulas for spatio-temporal models,
	please
Message-ID: <1493647582628.86532@uhd.edu>

Hello everyone.


Where would I find the authorized model formulas for spatio-temporal models, please?


?Thanks,

Erin



Erin M. Hodgess
Associate Professor
Department of Mathematics and Statistics
University of Houston - Downtown
mailto: hodgesse at uhd.edu

	[[alternative HTML version deleted]]


From v.m.vanzoest at utwente.nl  Mon May  1 16:30:51 2017
From: v.m.vanzoest at utwente.nl (v.m.vanzoest at utwente.nl)
Date: Mon, 1 May 2017 14:30:51 +0000
Subject: [R-sig-Geo] Cross-validation for kriging in R (package geoR): how
 to include the trend while reestimate is TRUE?
Message-ID: <376be5b9a46d4c619bc99507368fbe19@EXMBX31.ad.utwente.nl>

Dear all,

I have a question related to the function xvalid (package geoR), which I asked on StackOverflow before but unfortunately did not get answered, probably because it is too specifically related to spatial statistics and this specific function (http://stackoverflow.com/questions/43520716/cross-validation-for-kriging-in-r-how-to-include-the-trend-while-reestimating-t). I hope anyone of you is able to answer it.

I would like to compute a variogram, fit it, and then perform cross-validation. Function xvalid seems to work pretty nice to do the cross-validation. It works when I set reestimate=TRUE (so it reestimates the variogram for every point removed from the dataset in cross-validation) and it also works when using a trend. However, it does not seem to work when combining these two. Here is a reproducible example using the Meuse example dataset:

library(geoR)
library(sp)
data(meuse) # import data
coordinates(meuse) = ~x+y # make spatialpointsdataframe
meuse at proj4string <- CRS("+init=epsg:28992") # add projection
meuse_geo <- as.geodata(meuse) # create object of class geodata for geoR compatibility
meuse_geo$data <- meuse at data # attach all data (incl. covariates) to meuse_geo
meuse_vario <- variog(geodata=meuse_geo, data=meuse_geo$data$lead, trend= ~meuse_geo$data$elev) # variogram
meuse_vfit <- variofit(meuse_vario, nugget=0.1, fix.nugget=T) # fit
# cross-validation works fine:
xvalid(geodata=meuse_geo, data=meuse_geo$data$lead, model=meuse_vfit, variog.obj = meuse_vario, reestimate=F)
# cross-validation does not work when reestimate = T:
xvalid(geodata=meuse_geo, data=meuse_geo$data$lead, model=meuse_vfit, variog.obj = meuse_vario, reestimate=T)

The error I get is:

Error in variog(coords = cv.coords, data = cv.data, uvec = variog.obj$uvec,  : coords and trend have incompatible sizes

It seems to remove the point from the dataset during cross-validation, but it doesn't seem to remove the point from the covariates/trend data. Any ideas on solving this / work-arounds? Thanks a lot in advance for thinking along.

Best, Vera

---

V.M. (Vera) van Zoest, MSc | PhD candidate |
University of Twente<http://www.utwente.nl/> | Faculty ITC<http://www.itc.nl/> | Department Earth Observation Science (EOS)<https://www.itc.nl/EOS> |
ITC Building, room 2-038 | T: +31 (0)53 - 489 4412 | v.m.vanzoest at utwente.nl<mailto:v.m.vanzoest at utwente.nl> |

Study Geoinformatics: www.itc.nl/geoinformatics<http://www.itc.nl/geoinformatics>


	[[alternative HTML version deleted]]


From patrick.schratz at gmail.com  Mon May  1 20:04:16 2017
From: patrick.schratz at gmail.com (Patrick Schratz)
Date: Mon, 1 May 2017 20:04:16 +0200
Subject: [R-sig-Geo] Cross-validation for kriging in R (package geoR):
 how to include the trend while reestimate is TRUE?
In-Reply-To: <376be5b9a46d4c619bc99507368fbe19@EXMBX31.ad.utwente.nl>
References: <376be5b9a46d4c619bc99507368fbe19@EXMBX31.ad.utwente.nl>
Message-ID: <9b2af622-ebd1-4918-8da7-01f75b9d0f0a@Spark>

Hi Vera,

I started debugging a bit and the error is in line 192 of the `xvalid()` function which uses a subfunction `cv.f`

? ? res <- as.data.frame(t(apply(matrix(locations.xvalid),
? ? ? 1, cv.f)))

which then does the call to `vario()` in lines 125-141.
Here, the error appears because coords (which is created in line 90) seems to be of different length then trend (which is taken from your provided variog.obj).

if (is.null(variog.obj))
? ? ? ? ? ? stop("xvalid: when argument reestimate = TRUE an object with the fitted variogram model must be provided in the argument variog.obj ")
? ? ? ? ? CVvar <- variog(coords = cv.coords, data = cv.data,
? ? ? ? ? ? uvec = variog.obj$uvec, trend = variog.obj$trend,
? ? ? ? ? ? lambda = variog.obj$lambda, option = variog.obj$output.type,
? ? ? ? ? ? estimator.type = variog.obj$estimator.type,
? ? ? ? ? ? nugget.tolerance = variog.obj$nugget.tolerance,
? ? ? ? ? ? max.dist = max(variog.obj$u), pairs.min = 2,
? ? ? ? ? ? bin.cloud = FALSE, direction = variog.obj$direction,
? ? ? ? ? ? tolerance = variog.obj$tolerance, unit.angle = "radians",
? ? ? ? ? ? messages = FALSE, ...)
? ? ? ? ? CVmod <- variofit(vario = CVvar, ini.cov.pars = model$cov.pars,
? ? ? ? ? ? cov.model = model$cov.model, fix.nugget = model$fix.nugget,
? ? ? ? ? ? nugget = model$nugget, fix.kappa = model$fix.kappa,
? ? ? ? ? ? kappa = model$kappa, max.dist = model$max.dist,
? ? ? ? ? ? minimisation.function = model$minimisation.function,
? ? ? ? ? ? weights = model$weights, messages = FALSE,
? ? ? ? ? ???)

Because we are debugging somewhat deep here and the issue might be quickly solved by contacting the package authors (they should get it working quickly since they provide the option ?reestimate = TRUE'), I would try to do so first before doing any more detailed inspection of the error.

Cheers, Patrick

PhD Student at Department of Geography - GIScience group
Friedrich-Schiller-University Jena, Germany
Tel.: +49-3641-9-48973
Web: https://pat-s.github.io

On 1. May 2017, 16:31 +0200, wrote:
>
> http://stackoverflow.com/questions/43520716/cross-validation-for-kriging-in-r-how-to-include-the-trend-while-reestimating-t

	[[alternative HTML version deleted]]


From orion at ofb.net  Mon May  1 20:42:37 2017
From: orion at ofb.net (Anne C. Hanna)
Date: Mon, 1 May 2017 14:42:37 -0400
Subject: [R-sig-Geo] sp::disaggregate() does not seem to add the
 necessary "comment" attribute to identify holes if Polygons does not
 contain exactly 1 polygon with 0 or 1 holes
In-Reply-To: <alpine.LFD.2.20.1705011103180.6598@reclus.nhh.no>
References: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>
 <alpine.LFD.2.20.1704281804080.27020@reclus.nhh.no>
 <78988b99-67ef-8129-6622-aad0a4946920@ofb.net>
 <alpine.LFD.2.20.1704291551270.27676@reclus.nhh.no>
 <dbf4f008-23ae-3767-efae-6df317dc21c9@ofb.net>
 <30d71999-b99a-dabd-8de9-709b782fae03@ofb.net>
 <alpine.LFD.2.20.1705011103180.6598@reclus.nhh.no>
Message-ID: <e8468d6e-f8c0-44a2-f1ce-68ff48a48cd6@ofb.net>

Roger,

Works for me now.  Thanks for your patience with this.

I'll definitely look at sf for future projects, but for now, I just want to
analyze my darn data!  :)

 - Anne


On 05/01/2017 05:23 AM, Roger Bivand wrote:
> On Sun, 30 Apr 2017, Anne C. Hanna wrote:
> 
>> Okay, I think I've tracked it down.  This is... absurdly complicated and I
>> don't know what the actual correct resolution is, but I'm pretty sure that at
>> least I know what's going wrong.
> 
> Good, and thanks for staying with this. I've pushed a fix to my github repo,
> and pull request #29 to edzer/sp.
> 
> Note that this also works out of the box:
> 
> library(sf)
> sf <- st_read(dsn = "2011 Voting District Boundary Shapefiles", layer =
> "MCDS", stringsAsFactors = FALSE)
> sfd <- st_cast(sf, "POLYGON")
> 
> However, using sf::st_touches() is a bit less intuitive for now than the spdep
> "nb" object generated by spdep::poly2nb(), which also provides Queen (touch at
> one or more boundary points) and Rook (touch at more than one boundary point),
> and sf::st_touches() is seven times slower than spdep::poly2nb() now.
> sf::st_touches() could be speeded up using an STR tree (spdep::poly2nb() can
> do this, but in any case only looks at candidate neighbours with intersecting
> bounding boxes). So:
> 
> sp_sfd <- as(sfd, "Spatial")
> library(spdep)
> sp_nb <- poly2nb(sp_sfd)
> 
> seems a reasonable workflow to find the adjacencies under Queen/Rook and
> snapping control.
> 
> Please try out the disaggregate fix and report back - and consider shifting to
> sf.
> 
> Roger
> 
>>
>> Basically it comes down to the fact that createSPComment() does not actually
>> check whether each individual polygon in the SpatialPolygons object has a
>> comment (much less a correct comment).  Instead, it looks at a top-level
>> "comment" attribute.  If that top-level comment attribute (i.e. sf at comment for
>> my SpatialPolygonsDataFrame object stored as "sf") is "TRUE", then
>> createSPComment() does nothing.
>>
>> And here's where the second layer of the problem comes in for my dataset ---
>> my original data, pre-disaggregation, has complete and correct comments.  Some
>> of those comments (for objects where it's a single polygon with 0 or 1 holes)
>> are being retained by the basic disaggregation algorithm, while others are
>> not.  Your patch attempts to fill in the holes by running createSPComment() on
>> the final results.  But... the thing you are running createSPComment() on is
>> SpatialPolygons(p), where p is the list of disaggregated polygons.  And the
>> SpatialPolygons() constructor looks at the list of the polygons you feed it,
>> and if *any* of those polygons have comments, it sets the top-level "comment"
>> attribute of its output to "TRUE", even if other polygons are missing
>> comments.  (And it also doesn't check the comments it gets for correctness.)
>> So the fact that some polygon comments are retained in p means that
>> SpatialPolygons() lies to createSPComment() about whether it has any missing
>> comments that need to be fixed.
>>
>> On the other hand, when I manually run createPolygonComment() on the
>> individual Polygons objects in disaggregate()'s output, I don't even look at
>> the top-level "comment" attribute, so it works just fine.
>>
>> My little toy test polygons, on the other hand, were just not complex enough
>> to end up with partial comments in them, so they also didn't experience this
>> error.
>>
>> I am not familiar enough with the design philosophy behind sp to suggest at
>> what level this issue should be fixed, but I hope this is at least enough info
>> to help those who do know what they're doing get it corrected.
>>
>> - Anne
>>
>>
>> On 04/30/2017 12:18 PM, Anne C. Hanna wrote:
>>> Roger,
>>>
>>> Unfortunately I have a use case for you of createSPComment() not working.
>>>
>>> I tried your new version of disaggregate() on the original test script I sent
>>> you, and it does seem to have fixed all the cases in there.  However, when I
>>> tried it on my actual data, it had the same failure mode as before. The
>>> original dataset has what appear to be correct comments, but the
>>> disaggregate() output is full of NULLs in place of comments for the multi-part
>>> and multi-hole polygons.
>>>
>>> createSPComment() appears to be what's failing, rather than just some logic in
>>> your disaggregate() function --- when I try to run createSPComment() on the
>>> disaggregate() output, I still get the same set of NULLs.  I can fix it if I
>>> run createPolygonsComment() individually on every Polygons object in my
>>> SpatialPolygonsDataFrame.
>>>
>>> I'm not sure what is different between my dataset and the test polygons I was
>>> using earlier, as createSPComment() seems to handle the test shapes just fine
>>> (and converting my SpatialPolygonsDataFrame into just a set of SpatialPolygons
>>> also doesn't help).  But presumably it is better to have createSPComment()
>>> fixed than to have to work around it in disaggregate().  So I guess I'll look
>>> at the code for that and see if I can find what's wrong.
>>>
>>> Just in case you want to see this in action, I've attached a little test
>>> script.  The data set I am working with is available at:
>>>
>>> http://aws.redistricting.state.pa.us/Redistricting/Resources/GISData/2011-Voting-District-Boundary-Shapefiles.zip
>>>
>>>
>>>  - Anne
>>>
>>>
>>> On 04/29/2017 10:06 AM, Roger Bivand wrote:
>>>> On Sat, 29 Apr 2017, Anne C. Hanna wrote:
>>>>
>>>>> Roger,
>>>>>
>>>>> This looks great, and I will try it out ASAP.  I do have one reservation
>>>>> though --- it seems you are using createSPComment() to reconstruct the
>>>>> comments, and I have seen some discussion that that may not be reliable in
>>>>> all cases (e.g. if the initial polygons are wonky in some way).  I don't
>>>>> know a lot about this, but is it possible that it would be preferable to
>>>>> parse and appropriately disaggregate the original comments strings (if they
>>>>> exist), so as to deal slightly more smoothly with such cases (i.e., the
>>>>> polygons would still be wonky, but at least the hole/polygon matching would
>>>>> track with whatever was in the original data)?
>>>>
>>>> Contributions very welcome - this was code moved from the raster package,
>>>> so I
>>>> really have little feeling for why it is necessary.
>>>>
>>>> Please provide references and use cases. GEOS is strict in its treatments of
>>>> geometries, but does work in most cases. People just expressing opinions
>>>> doesn't help, and may well be misleading. It is possible to clean geometries
>>>> too, see the cleangeo package. createPolygonsComment probably isn't
>>>> foolproof,
>>>> but specific reports help, because then it is possible to do something about
>>>> it. sp classes didn't use simple features when written because sf was not
>>>> widely used then - introduced in 2004, when sp was being developed. Using sf
>>>> helps, and rgeos::createPolygonsComment was a work-around from 2010 when
>>>> rgeos
>>>> was written.
>>>>
>>>> Even when you use sf in the sf package, you will still run into invalid
>>>> geometries because the geometries are in fact invalid, the sf package also
>>>> uses GEOS.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> I also had no success using createSPComment to fix disaggregate()'s output
>>>>> previously, even though my polygons are perfectly non-wonky, so I am perhaps
>>>>> a little more untrusting of it than I should be.  But I'll let you know how
>>>>> this version works with my data.  Thanks for addressing this so quickly!
>>>>>
>>>>> - Anne
>>>>>
>>>>>
>>>>> On 04/28/2017 12:11 PM, Roger Bivand wrote:
>>>>>> I've pushed the fix to my fork:
>>>>>>
>>>>>> https://github.com/rsbivand/sp
>>>>>>
>>>>>> and created a pull request:
>>>>>>
>>>>>> https://github.com/edzer/sp/pull/28
>>>>>>
>>>>>> Only one part of a complicated set if nested if() in disaggregate() was
>>>>>> adding
>>>>>> comments, but in some settings the existing comments survived the
>>>>>> disaggregation. Now the Polygons object comment attribute is re-created for
>>>>>> all Polygons objects. This is version 1.2-6, also including code changes
>>>>>> that
>>>>>> internally affect rgdal and rgeos - you may like to re-install them from
>>>>>> source after installing this sp version (shouldn't matter).
>>>>>>
>>>>>> Roger
>>>>>>
>>>>>> On Fri, 28 Apr 2017, Anne C. Hanna wrote:
>>>>>>
>>>>>>> Hello.  I first posted this issue report on the sp GitHub repo
>>>>>>> (https://github.com/edzer/sp/issues/27) and it was suggested that I
>>>>>>> redirect
>>>>>>> it to here.
>>>>>>>
>>>>>>> I am working with a geographic dataset with complex borders. The data is
>>>>>>> stored as a SpatialPolygonsDataFrame.  Each Polygons object in the data
>>>>>>> frame
>>>>>>> may be composed of multiple disjoint polygons, and each polygon may have
>>>>>>> multiple holes.  I want to disaggregate each of the Polygons objects
>>>>>>> into its
>>>>>>> individual disjoint polygons and construct an adjacency matrix for all the
>>>>>>> disjoint components, and I was using disaggregate() to do this.  However,
>>>>>>> when
>>>>>>> I run gTouches() on the disaggregated data, in order to compute the
>>>>>>> adjacencies, I get a number of warnings like this:
>>>>>>>
>>>>>>> Error in RGEOSBinPredFunc(spgeom1, spgeom2, byid, "rgeos_touches") :
>>>>>>> Polygons
>>>>>>> object missing comment attribute ignoring hole(s).  See function
>>>>>>> createSPComment.
>>>>>>>
>>>>>>> Looking at the Polygons "comment" attributes in the
>>>>>>> SpatialPolygonsDataFrame
>>>>>>> output by disaggregate(), I see that the only comment values are "0"
>>>>>>> (indicating a single polygon with no holes), "0 1" (indicating a single
>>>>>>> polygon with a single hole), and NULL (apparently no comment was written).
>>>>>>> Since I know my dataset contains several Polygons objects which are
>>>>>>> composed
>>>>>>> of multiple disjoint regions, and also several Polygons which contain more
>>>>>>> than one hole, this is not the expected result.  In reading the
>>>>>>> disaggregate()
>>>>>>> code in the sp GitHub repository (specifically, explodePolygons()), I also
>>>>>>> can't see anywhere the comment is being added for the cases where a
>>>>>>> Polygons
>>>>>>> object has more than two parts or more than two holes.  It actually seems
>>>>>>> like
>>>>>>> it's getting carried along almost accidentally in the few cases that do
>>>>>>> get
>>>>>>> comments, and neglected otherwise.
>>>>>>>
>>>>>>> Assuming I'm not failing to understand the code and the desired behavior
>>>>>>> (entirely possible, as I am new at working with this software!), this
>>>>>>> seems
>>>>>>> suboptimal to me.  My dataset is pretty well-behaved (despite its
>>>>>>> complexity),
>>>>>>> so I should be able to fix my issues with judicious application of
>>>>>>> createPolygonsComment.  But I had a heck of a time figuring out what was
>>>>>>> going
>>>>>>> wrong with gTouches, since Polygons comment management appears to be a
>>>>>>> pretty
>>>>>>> obscure field (and createSPComment wasn't working for me, for whatever
>>>>>>> reason).  So it seems like it might be better if disaggregate() just
>>>>>>> parses
>>>>>>> and passes along the comments from its input correctly, or, if it's
>>>>>>> absolutely
>>>>>>> necessary to not create comments, passes nothing and warns clearly in the
>>>>>>> manual that comments and associated hole information are being lost. 
>>>>>>> Passing
>>>>>>> along comments in some cases while silently dropping them in others seems
>>>>>>> like
>>>>>>> kind of the worst of both worlds.
>>>>>>>
>>>>>>> I've attached a set of tests I wrote to demonstrate desired/undesired
>>>>>>> behavior: disaggregate_comment_tests.R.  My R version is 3.4.0, my sp
>>>>>>> version
>>>>>>> is 1.2-4, my rgeos version is 0.3-23 (SVN revision 546), and my GEOS
>>>>>>> runtime
>>>>>>> version is 3.5.1-CAPI-1.9.1 r4246.  I am using Debian Release 9.0 with
>>>>>>> kernel
>>>>>>> version 4.9.0-2-amd64.  I hope this is useful; please let me know if
>>>>>>> you need
>>>>>>> more info or if there is a better place to post this issue.
>>>>>>>
>>>>>>> - Anne
>>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>
>>
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170501/fdf0ed47/attachment.sig>

From v.m.vanzoest at utwente.nl  Thu May  4 11:55:44 2017
From: v.m.vanzoest at utwente.nl (v.m.vanzoest at utwente.nl)
Date: Thu, 4 May 2017 09:55:44 +0000
Subject: [R-sig-Geo] Cross-validation for kriging in R (package geoR):
 how to include the trend while reestimate is TRUE?
In-Reply-To: <9b2af622-ebd1-4918-8da7-01f75b9d0f0a@Spark>
References: <376be5b9a46d4c619bc99507368fbe19@EXMBX31.ad.utwente.nl>
 <9b2af622-ebd1-4918-8da7-01f75b9d0f0a@Spark>
Message-ID: <cf4b7dd5e7464365b9a2d037a7edef98@EXMBX31.ad.utwente.nl>

Dear Patrick,

Thanks a lot for your quick reply and taking the time for digging deeper into the function! I will contact the package authors about the issue.

Best, Vera

---

V.M. (Vera) van Zoest, MSc | PhD candidate |
University of Twente<http://www.utwente.nl/> | Faculty ITC<http://www.itc.nl/> | Department Earth Observation Science (EOS)<https://www.itc.nl/EOS> |
ITC Building, room 2-038 | T: +31 (0)53 ? 489 4412 | v.m.vanzoest at utwente.nl<mailto:v.m.vanzoest at utwente.nl> |

Study Geoinformatics: www.itc.nl/geoinformatics<http://www.itc.nl/geoinformatics>


From: Patrick Schratz [mailto:patrick.schratz at gmail.com]
Sent: maandag 1 mei 2017 20:04
To: r-sig-geo at r-project.org; Zoest, V.M. van (ITC) <v.m.vanzoest at utwente.nl>
Subject: Re: [R-sig-Geo] Cross-validation for kriging in R (package geoR): how to include the trend while reestimate is TRUE?

Hi Vera,

I started debugging a bit and the error is in line 192 of the `xvalid()` function which uses a subfunction `cv.f`

    res <- as.data.frame(t(apply(matrix(locations.xvalid),
      1, cv.f)))

which then does the call to `vario()` in lines 125-141.
Here, the error appears because coords (which is created in line 90) seems to be of different length then trend (which is taken from your provided variog.obj).

if (is.null(variog.obj))
            stop("xvalid: when argument reestimate = TRUE an object with the fitted variogram model must be provided in the argument variog.obj ")
          CVvar <- variog(coords = cv.coords, data = cv.data,
            uvec = variog.obj$uvec, trend = variog.obj$trend,
            lambda = variog.obj$lambda, option = variog.obj$output.type,
            estimator.type = variog.obj$estimator.type,
            nugget.tolerance = variog.obj$nugget.tolerance,
            max.dist = max(variog.obj$u), pairs.min = 2,
            bin.cloud = FALSE, direction = variog.obj$direction,
            tolerance = variog.obj$tolerance, unit.angle = "radians",
            messages = FALSE, ...)
          CVmod <- variofit(vario = CVvar, ini.cov.pars = model$cov.pars,
            cov.model = model$cov.model, fix.nugget = model$fix.nugget,
            nugget = model$nugget, fix.kappa = model$fix.kappa,
            kappa = model$kappa, max.dist = model$max.dist,
            minimisation.function = model$minimisation.function,
            weights = model$weights, messages = FALSE,
            ?)

Because we are debugging somewhat deep here and the issue might be quickly solved by contacting the package authors (they should get it working quickly since they provide the option ?reestimate = TRUE'), I would try to do so first before doing any more detailed inspection of the error.

Cheers, Patrick

PhD Student at Department of Geography - GIScience group
Friedrich-Schiller-University Jena, Germany
Tel.: +49-3641-9-48973
Web: https://pat-s.github.io<https://pat-s.github.io/>

On 1. May 2017, 16:31 +0200, wrote:


http://stackoverflow.com/questions/43520716/cross-validation-for-kriging-in-r-how-to-include-the-trend-while-reestimating-t

	[[alternative HTML version deleted]]


From ankur.sum13 at yahoo.com  Sat May  6 01:22:11 2017
From: ankur.sum13 at yahoo.com (Ankur Sarker)
Date: Fri, 5 May 2017 23:22:11 +0000 (UTC)
Subject: [R-sig-Geo] KrigeST function giving errors
References: <622429782.4790075.1494026531668.ref@mail.yahoo.com>
Message-ID: <622429782.4790075.1494026531668@mail.yahoo.com>

Hi,

If I applies a variogram model into KrigeST fucntion, I get the error as follow:
> DE_kriged <- krigeST(count~1, data=stfdf, newdata=stf,modelList=sumMetricVgm)
Error in chol.default(A) :?? the leading minor of order 16 is not positive definiteIn addition: Warning message:In krigeST(count ~ 1, data = stfdf, newdata = stf, modelList = sumMetricVgm) :? The spatio-temporal variogram model does not carry a time unit attribute: krisgeST cannot check whether the temporal distance metrics coincide.

I have tried applying different variogram models and different points.
https://drive.google.com/open?id=0B3tdiSZ9hBj4dVZXOXItUjdrRlE

Best,Ankur
	[[alternative HTML version deleted]]


From payneb at post.bgu.ac.il  Sat May  6 22:56:52 2017
From: payneb at post.bgu.ac.il (Brandon Payne)
Date: Sat, 06 May 2017 23:56:52 +0300
Subject: [R-sig-Geo] extract lat-long pairs from sf::st_read MULTIPOLYGON
	format
In-Reply-To: <mailman.7.1494064801.12302.r-sig-geo@r-project.org>
References: <mailman.7.1494064801.12302.r-sig-geo@r-project.org>
Message-ID: <m2shkh4sbv.fsf@post.bgu.ac.il>


## I am trying to modify the

install_github("choroplethrAdmin1", "arilamstein")

package.  It can show a map by

choroplethrAdmin1::admin1_map("israel")

The shape data is hidden in .Rdata, (principle of encapsulation)
so to get at it I had to

```{bash}
git clone git at github.com:arilamstein/choroplethrAdmin1.git
cd chor*
```
```{r}
load("/Users/AbuDavid/scratch/R/choroplethrAdmin1/data/admin1.regions.rdata")
ilmap<-admin1.map[which(admin1.map$admin == name), ]
View(ilmap)
#write.csv(ilmap, file = "israel.csv", row.names=FALSE)

```

the format of this data looks like

35.13070357	32.70585928	israel	haifa district	1879.1	272218	FALSE	1	1879
35.12837813	32.70314627	israel	haifa district	1879.1	272219	FALSE	1	1879
35.12574263	32.7008725	israel	haifa district	1879.1	272220	FALSE	1	1879
35.12419234	32.69989065	israel	haifa district	1879.1	272221	FALSE	1	1879
35.12248701	32.69919302	israel	haifa district	1879.1	272222	FALSE	1	1879
35.12078169	32.69911551	israel	haifa district	1879.1	272223	FALSE	1	1879
35.02130456	32.38117626	israel	central district	1880.1	272295	FALSE	1	1880
35.0327303	32.38220026	israel	central district	1880.1	272296	FALSE	1	1880
35.02879684	32.36515737	israel	central district	1880.1	272297	FALSE	1	1880
35.02156213	32.34459015	israel	central district	1880.1	272298	FALSE	1	1880
35.01784143	32.34221303	israel	central district	1880.1	272299	FALSE
1	1880

## Question:

How can I get the data from my shape file as 2 columns: c(lat, long) ?

I don't mind using something other than sf:st_read.

```{r importshapes}
editedmap <- sf::st_read("../includes/ISR_adm_edited/ISR_adm1.shp")
golan <- editedmap[which(editedmap$NAME_1 == "Golan"), ]
names(editedmap)
```

> names(editedmap)
 [1] "ID_0"      "ISO"       "NAME_0"    "ID_1"      "NAME_1"    "TYPE_1"   

 [7] "ENGTYPE_1" "NL_NAME_1" "VARNAME_1" "geometry"


From roman.lustrik at gmail.com  Sun May  7 09:49:29 2017
From: roman.lustrik at gmail.com (=?UTF-8?Q?Roman_Lu=C5=A1trik?=)
Date: Sun, 7 May 2017 09:49:29 +0200
Subject: [R-sig-Geo] extract lat-long pairs from sf::st_read
	MULTIPOLYGON format
In-Reply-To: <m2shkh4sbv.fsf@post.bgu.ac.il>
References: <mailman.7.1494064801.12302.r-sig-geo@r-project.org>
 <m2shkh4sbv.fsf@post.bgu.ac.il>
Message-ID: <CAHT1vpg_9Yw5RDqRsYK_5pOaW_8wsbk4GJ7ZPA-2kY8dDavXDA@mail.gmail.com>

After downloading
https://github.com/trulia/choroplethrAdmin1/blob/master/data/admin1.map.rdata
and

load("admin1.map.rdata")
ilmap<-admin1.map[which(admin1.map$admin == "slovenia"), ]


I get a data.frame

> names(ilmap)
 [1] "long"       "lat"        "order"      "hole"       "piece"
 "group"
 [7] "id"         "adm1_code"  "OBJECTID_1" "diss_me"    "adm1_cod_1"
"iso_3166_2"
[13] "wikipedia"  "iso_a2"     "adm0_sr"    "name"       "name_alt"
"name_local"
[19] "type"       "type_en"    "code_local" "code_hasc"  "note"
"hasc_maybe"
[25] "region"     "region_cod" "provnum_ne" "gadm_level" "check_me"
"scalerank"
[31] "datarank"   "abbrev"     "postal"     "area_sqkm"  "sameascity"
"labelrank"
[37] "featurecla" "name_len"   "mapcolor9"  "mapcolor13" "fips"
"fips_alt"
[43] "woe_id"     "woe_label"  "woe_name"   "latitude"   "longitude"
 "sov_a3"
[49] "adm0_a3"    "adm0_label" "admin"      "geonunit"   "gu_a3"
 "gn_id"
[55] "gn_name"    "gns_id"     "gns_name"   "gn_level"   "gn_region"
 "gn_a1_code"
[61] "region_sub" "sub_code"   "gns_level"  "gns_lang"   "gns_adm1"
"gns_region"


Notice the first two columns. If this isn't what you're after, perhaps try
coordinates().

Cheers,
Roman


On Sat, May 6, 2017 at 10:56 PM, Brandon Payne <payneb at post.bgu.ac.il>
wrote:

>
> ## I am trying to modify the
>
> install_github("choroplethrAdmin1", "arilamstein")
>
> package.  It can show a map by
>
> choroplethrAdmin1::admin1_map("israel")
>
> The shape data is hidden in .Rdata, (principle of encapsulation)
> so to get at it I had to
>
> ```{bash}
> git clone git at github.com:arilamstein/choroplethrAdmin1.git
> cd chor*
> ```
> ```{r}
> load("/Users/AbuDavid/scratch/R/choroplethrAdmin1/data/
> admin1.regions.rdata")
> ilmap<-admin1.map[which(admin1.map$admin == name), ]
> View(ilmap)
> #write.csv(ilmap, file = "israel.csv", row.names=FALSE)
>
> ```
>
> the format of this data looks like
>
> 35.13070357     32.70585928     israel  haifa district  1879.1  272218
> FALSE   1       1879
> 35.12837813     32.70314627     israel  haifa district  1879.1  272219
> FALSE   1       1879
> 35.12574263     32.7008725      israel  haifa district  1879.1  272220
> FALSE   1       1879
> 35.12419234     32.69989065     israel  haifa district  1879.1  272221
> FALSE   1       1879
> 35.12248701     32.69919302     israel  haifa district  1879.1  272222
> FALSE   1       1879
> 35.12078169     32.69911551     israel  haifa district  1879.1  272223
> FALSE   1       1879
> 35.02130456     32.38117626     israel  central district        1880.1
> 272295  FALSE   1       1880
> 35.0327303      32.38220026     israel  central district        1880.1
> 272296  FALSE   1       1880
> 35.02879684     32.36515737     israel  central district        1880.1
> 272297  FALSE   1       1880
> 35.02156213     32.34459015     israel  central district        1880.1
> 272298  FALSE   1       1880
> 35.01784143     32.34221303     israel  central district        1880.1
> 272299  FALSE
> 1       1880
>
> ## Question:
>
> How can I get the data from my shape file as 2 columns: c(lat, long) ?
>
> I don't mind using something other than sf:st_read.
>
> ```{r importshapes}
> editedmap <- sf::st_read("../includes/ISR_adm_edited/ISR_adm1.shp")
> golan <- editedmap[which(editedmap$NAME_1 == "Golan"), ]
> names(editedmap)
> ```
>
> > names(editedmap)
>  [1] "ID_0"      "ISO"       "NAME_0"    "ID_1"      "NAME_1"    "TYPE_1"
>
>  [7] "ENGTYPE_1" "NL_NAME_1" "VARNAME_1" "geometry"
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
In God we trust, all others bring data.

	[[alternative HTML version deleted]]


From payneb at post.bgu.ac.il  Sun May  7 10:17:01 2017
From: payneb at post.bgu.ac.il (Brandon Payne)
Date: Sun, 07 May 2017 11:17:01 +0300
Subject: [R-sig-Geo] extract lat-long pairs from sf::st_read
	MULTIPOLYGON format
In-Reply-To: <CAHT1vpg_9Yw5RDqRsYK_5pOaW_8wsbk4GJ7ZPA-2kY8dDavXDA@mail.gmail.com>
References: <mailman.7.1494064801.12302.r-sig-geo@r-project.org>
 <m2shkh4sbv.fsf@post.bgu.ac.il>
 <CAHT1vpg_9Yw5RDqRsYK_5pOaW_8wsbk4GJ7ZPA-2kY8dDavXDA@mail.gmail.com>
Message-ID: <m2shkh3wua.fsf@post.bgu.ac.il>


Roman,

I am looking for advice on how to convert one (extra, missing, not-included) region from the shape file
"ISR_adm1.shp" @ "http://biogeo.ucdavis.edu/data/diva/adm/ISR_adm.zip", 

into that ( https://github.com/trulia/choroplethrAdmin1/blob/master/data/admin1.map.rdata ) data frame format.


From tim.appelhans at gmail.com  Sun May  7 10:17:54 2017
From: tim.appelhans at gmail.com (Tim Appelhans)
Date: Sun, 7 May 2017 10:17:54 +0200
Subject: [R-sig-Geo] extract lat-long pairs from sf::st_read
 MULTIPOLYGON format
In-Reply-To: <m2shkh3wua.fsf@post.bgu.ac.il>
References: <mailman.7.1494064801.12302.r-sig-geo@r-project.org>
 <m2shkh4sbv.fsf@post.bgu.ac.il>
 <CAHT1vpg_9Yw5RDqRsYK_5pOaW_8wsbk4GJ7ZPA-2kY8dDavXDA@mail.gmail.com>
 <m2shkh3wua.fsf@post.bgu.ac.il>
Message-ID: <95ac0a80-57b9-3c1d-37d6-1960b7782d8e@gmail.com>

Brandon,

sf has sf::st_coordinates which gives a matrix or list of matrices of X 
and Y coordinates depending on the feature type.

Tim


On 07.05.2017 10:17, Brandon Payne wrote:
> Roman,
>
> I am looking for advice on how to convert one (extra, missing, not-included) region from the shape file
> "ISR_adm1.shp" @ "http://biogeo.ucdavis.edu/data/diva/adm/ISR_adm.zip",
>
> into that ( https://github.com/trulia/choroplethrAdmin1/blob/master/data/admin1.map.rdata ) data frame format.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Tim Appelhans
Research Specialist, Geomarketing
GfK | Bamberger Str. 6, 90425 N?rnberg | Germany
Postal address: Nordwestring 101 | 90419 N?rnberg


From payneb at post.bgu.ac.il  Sun May  7 11:35:15 2017
From: payneb at post.bgu.ac.il (Brandon Payne)
Date: Sun, 07 May 2017 12:35:15 +0300
Subject: [R-sig-Geo] extract lat-long pairs - slot "coords"
In-Reply-To: <CAHT1vpg_9Yw5RDqRsYK_5pOaW_8wsbk4GJ7ZPA-2kY8dDavXDA@mail.gmail.com>
References: <mailman.7.1494064801.12302.r-sig-geo@r-project.org>
 <m2shkh4sbv.fsf@post.bgu.ac.il>
 <CAHT1vpg_9Yw5RDqRsYK_5pOaW_8wsbk4GJ7ZPA-2kY8dDavXDA@mail.gmail.com>
Message-ID: <m2tw4xt3fw.fsf@post.bgu.ac.il>


I was able to get the coords to print to the R-console.  Still need to
read the docs on the Polygon class to access them.
(It's not  Golan$coords   )

```{pseudo}
# download
"http://biogeo.ucdavis.edu/data/diva/adm/ISR_adm.zip",
# extract
unzip *.zip
```

```{r}
states <- readOGR("../includes/ISR_adm/ISR_adm1.shp", "ISR_adm1")
Golan <- subset(states, NAME_1 == "Golan")
## check
## based on http://stackoverflow.com/questions/25985395/how-to-subset-a-shapefile
dim(states)
dim(Golan) 
Golan

```
>
An object of class "Polygon"
Slot "labpt":
[1] 35.64728 33.24579

Slot "area":
[1] 6.914819e-05

Slot "hole":
[1] FALSE

Slot "ringDir":
[1] 1

Slot "coords":
          [,1]     [,2]
 [1,] 35.63564 33.24520
 [2,] 35.63712 33.24554
 [3,] 35.64099 33.24652
 [4,] 35.64528 33.24749
 [5,] 35.65008 33.24818
 [6,] 35.65404 33.24831
 [7,] 35.65889 33.24823
 [8,] 35.65992 33.24816
 [9,] 35.65163 33.24443
[10,] 35.64420 33.24282
[11,] 35.63676 33.24432
[12,] 35.63564 33.24520


From ankur.sum13 at yahoo.com  Mon May  8 06:33:28 2017
From: ankur.sum13 at yahoo.com (Ankur Sarker)
Date: Mon, 8 May 2017 04:33:28 +0000 (UTC)
Subject: [R-sig-Geo] Appropriate variogram model
References: <770069364.6168148.1494218008878.ref@mail.yahoo.com>
Message-ID: <770069364.6168148.1494218008878@mail.yahoo.com>

Hi,
Can anyone suggest me the most appropriate variogram model to fit my data??
I have tried three different models and results are too bad. Here is the comparison of different variogram models as attached.
Thanks,Ankur
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170508/6d240960/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Different models.pdf
Type: application/pdf
Size: 28828 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170508/6d240960/attachment.pdf>

From tim.appelhans at gmail.com  Mon May  8 08:03:03 2017
From: tim.appelhans at gmail.com (Tim Appelhans)
Date: Mon, 8 May 2017 08:03:03 +0200
Subject: [R-sig-Geo] mapview 2.0.1 on CRAN
Message-ID: <a12f549e-f2ff-c7af-bdbb-aae61b6d4267@gmail.com>

Dear list,

we have released mapview version 2.0.1 to CRAN yesterday. Windows 
binaries have already been built and MacOSX binaries should follow soon.

As indicated by the major version bump, this release entails a few major 
changes. The two most user-relevant changes are:

1. All vector data processing internally is now done using the simple 
features package. Apart from providing full support for the most common 
sf types this also means that mapview now returns data of class sf/sfc 
even when passing a sp object to mapview. I am unsure whether anyone 
ever uses the @object slot of mapview but in case people do, this is a 
major change.

2. The github repository has been move from 
environmentalinformatics-marburg to 
https://github.com/r-spatial/mapview. So to install the develop version 
use devtools::install_github("r-spatial/mapview at develop")

3. There are new vector data sets (breweries, trails, franconia). The 
old data sets (breweries91, gadmCHE, astStorms2005) have been moved to 
library(leaflet). Thus, examples using the old data hopefully should 
still work.

All changes are outlined here (still listed as as version 1.2.79)

https://r-spatial.github.io/mapview/news/index.html

We also have a new online documentation which can be found here

https://r-spatial.github.io/mapview/

As always, feedback, suggestions, feature requests and bug reports 
should be filed at https://github.com/r-spatial/mapview/issues

Happy mapping,
Tim

-- 
Tim Appelhans
Research Specialist, Geomarketing
GfK | Bamberger Str. 6, 90425 N?rnberg | Germany
Postal address: Nordwestring 101 | 90419 N?rnberg


From mark.graham at oii.ox.ac.uk  Tue May  9 08:38:08 2017
From: mark.graham at oii.ox.ac.uk (Mark Graham)
Date: Tue, 9 May 2017 06:38:08 +0000
Subject: [R-sig-Geo] Job vacancy for a 'Digital Geographer' at the Oxford
	Internet Institute
Message-ID: <CADPBXSZ0Zov9kxyXYQYYyhK2wLcvqWZaceoHerWY4TOWpp6Dsw@mail.gmail.com>

Researcher in Digital Geography
<https://www.recruit.ox.ac.uk/pls/hrisliverecruit/erq_jobspec_details_form.jobspec?p_id=126609>
Oxford Internet Institute, 1 St Giles, Oxford
Grade 7: ?31,076 ? ?38,183 p.a. (pro rata for part-time)

The Oxford Internet Institute is a leading centre for research into
individual, collective and institutional behaviour on the Internet. We are
looking for a Researcher to work with Professor Mark Graham on a two-year
project to better understand the geographies of the Internet.

Our existing research has uncovered highly uneven digital geographies: with
some parts of the world far more like to produce, and be represented by,
digital content than others.

We seek to hire a Researcher to continue some of this research, to ask what
has changed, and to ask new questions about digital inequalities at not
just the global, but also the local scale. We plan to ask and answer
questions such as: what are the contemporary geographies of the production
and consumption of digital knowledge-based economic activities?; what are
the geographies of digital representations (such as content in Wikipedia or
Google)?; how likely is digital content to be locally or non-locally
produced?; and do digital representations produce or reproduce social and
economic inequalities and divisions in our urban environments. If we accept
that our cities are made up of digital as well as physical raw materials ?
we need to better understand who owns, controls, shapes, can access, and
can remake the digital layers of place.

We plan on answering the above questions using methods from computational
social science and GIS: scraping, mapping, and statistically analysing a
diverse range of datasets. The position is suited to candidates who have
recently completed a doctorate in Quantitative Geography, GIScience,
Computer Science, Economics, Sociology or other relevant discipline (i.e.
postdocs), but we also welcome applications from qualified individuals
without a doctorate (e.g. candidates with industry experience). Programming
skills, and experience with GIS are required. The successful candidate will
ultimately work with Professor Graham to produce a full-length monograph on
the topic.

Based at the Oxford Internet Institute, this position is available
immediately for 24 months in the first instance, with the possibility of
renewal thereafter, funding permitting.

This job advertisement is for a full-time researcher, but part-time
applications of more than 22.5 hours a week will also be considered.

*Recruitment link and further details about the position:*
https://www.recruit.ox.ac.uk/pls/hrisliverecruit/erq_jobspec
_details_form.jobspec?p_id=126609

*Other relevant links:*
http://www.markgraham.space/blog/2017/5/5/work-with-me-at-ox
ford-im-hiring-a-digital-geographer
http://www.markgraham.space/internet-information-geography/
http://geography.oii.ox.ac.uk/
http://geonet.oii.ox.ac.uk/mapping/
http://cii.oii.ox.ac.uk/


/apologies for cross-posting
------------------------------------------
Mark Graham

Professor of Internet Geography
Oxford Internet Institute
University of Oxford

Faculty Fellow
The Alan Turing Institute

Research Affiliate
School of Geography and the Environment
University of Oxford

markgraham.space | @geoplace <http://twitter.com/geoplace>
<http://twitter.com/geoplace>

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Tue May  9 15:32:32 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 9 May 2017 15:32:32 +0200
Subject: [R-sig-Geo] How does sf do rgeos::gUnaryUnion or
 maptools::unionSpatialPolygons?
Message-ID: <alpine.LFD.2.20.1705091451420.23667@reclus.nhh.no>

While https://github.com/edzer/sfr/wiki/migrating is very helpful, 
rgeos::gUnaryUnion is not the typical use case of sf::st_union. The 
typical use case, from ASDAR 1st edition, chapter 5 and the maptools 
"combine_maptools" vignette (the shapefiles are shipped with maptools), is 
grouping features that should belong to the same statistical entity:

library(maptools)
vignette("combine_maptools")

###################################################
### chunk number 16:
###################################################

library(sf)
nc90 <- st_read(system.file("shapes/co37_d90.shp", package = "maptools"))
st_crs(nc90) <- "+proj=longlat +datum=NAD27"
table(table(paste(nc90$ST, nc90$CO, sep="")))

The point is that two counties are represented by multiple features of 
"POLYGON" objects, rather than single "MULTIPOLYGON" objects, in the input 
shapefile. I've tried doing:

ids <- factor(paste(nc90$ST, nc90$CO, sep=""))
nc90a <- st_cast(nc90, to="MULTIPOLYGON", ids=as.integer(ids))

but:

> dim(nc90a)
[1] 104   9

all turned into "MULTIPOLYGON", but not grouped, even though I think ids= 
are as they should be:

> table(table(as.integer(ids)))

  1  2  4
98  1  1

This may be to avoid dropping data.frame rows. It looks as though I can 
get there using:

nc90a <- st_cast(st_geometry(nc90), to="MULTIPOLYGON", ids=as.integer(ids))

> length(nc90a)
[1] 100

but all are "MULTIPOLYGON", not a mixture of "POLYGON" and "MULTIPOLYGON" 
features, and I've no idea which is which - that is how the order of nc90a 
relates to the counties of nc90. How do I associate the features in 
nc90a with their county ids? Where do i find the ids I gave to st_cast in 
nc90a?

What am I missing? I'm writing here rather than raising an issue on GH 
because others may want to know too, and Edzer needs others to reply for 
him if they know the answer.

Puzzled,

Roger

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From edzer.pebesma at uni-muenster.de  Tue May  9 19:11:23 2017
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Tue, 9 May 2017 19:11:23 +0200
Subject: [R-sig-Geo] How does sf do rgeos::gUnaryUnion or
 maptools::unionSpatialPolygons?
In-Reply-To: <alpine.LFD.2.20.1705091451420.23667@reclus.nhh.no>
References: <alpine.LFD.2.20.1705091451420.23667@reclus.nhh.no>
Message-ID: <245d1d6d-46ff-1078-3efd-de63849c4e26@uni-muenster.de>



On 09/05/17 15:32, Roger Bivand wrote:
> While https://github.com/edzer/sfr/wiki/migrating is very helpful,
> rgeos::gUnaryUnion is not the typical use case of sf::st_union. The
> typical use case, from ASDAR 1st edition, chapter 5 and the maptools
> "combine_maptools" vignette (the shapefiles are shipped with maptools),
> is grouping features that should belong to the same statistical entity:
> 
> library(maptools)
> vignette("combine_maptools")
> 
> ###################################################
> ### chunk number 16:
> ###################################################
> 
> library(sf)
> nc90 <- st_read(system.file("shapes/co37_d90.shp", package = "maptools"))
> st_crs(nc90) <- "+proj=longlat +datum=NAD27"
> table(table(paste(nc90$ST, nc90$CO, sep="")))
> 
> The point is that two counties are represented by multiple features of
> "POLYGON" objects, rather than single "MULTIPOLYGON" objects, in the
> input shapefile. I've tried doing:
> 
> ids <- factor(paste(nc90$ST, nc90$CO, sep=""))
> nc90a <- st_cast(nc90, to="MULTIPOLYGON", ids=as.integer(ids))
> 
> but:
> 
>> dim(nc90a)
> [1] 104   9
> 
> all turned into "MULTIPOLYGON", but not grouped, even though I think
> ids= are as they should be:
> 
>> table(table(as.integer(ids)))
> 
>  1  2  4
> 98  1  1

this is at least documented: nc90 is of class `sf`, and st_cast.sf
has no ids argument; ... is ignored. If it would merge, it'd need
some guidance what to do with non-geometry feature attributes.

> 
> This may be to avoid dropping data.frame rows. It looks as though I can
> get there using:
> 
> nc90a <- st_cast(st_geometry(nc90), to="MULTIPOLYGON", ids=as.integer(ids))
> 
>> length(nc90a)
> [1] 100
> 
> but all are "MULTIPOLYGON", not a mixture of "POLYGON" and
> "MULTIPOLYGON" features, and I've no idea which is which - that is how
> the order of nc90a relates to the counties of nc90. How do I associate
> the features in nc90a with their county ids? Where do i find the ids I
> gave to st_cast in nc90a?

st_cast uses base::split, which converts ids to a factor,
and returns a list with the order of the levels of that factor
(alphabetically?). Neither convenient, nor documented. I guess more
intuitive would be to squeeze, but keep original order, right?

I'd suggest to use instead

nc90a = aggregate(nc90, list(ids = ids), head, n = 1)

which returns a GEOMETRY sf, with 2 MULTIPOLYGON and 98 POLYGON. You
could st_cast that to MULTIPOLYGON.

Alternatively:

library(dplyr)
bind_cols(nc90, ids=ids) %>%
	group_by(ids) %>%
	summarise(ST=head(ST,1), CO=head(CO,1), do_union=FALSE)

converts straight into MULTIPOLYGON.

I'll update the sp -> sf migration wiki.

> 
> What am I missing? I'm writing here rather than raising an issue on GH
> because others may want to know too, and Edzer needs others to reply for
> him if they know the answer.
> 
> Puzzled,
> 
> Roger
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170509/aef97118/attachment.sig>

From Roger.Bivand at nhh.no  Tue May  9 20:39:45 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 9 May 2017 20:39:45 +0200
Subject: [R-sig-Geo] How does sf do rgeos::gUnaryUnion or
 maptools::unionSpatialPolygons?
In-Reply-To: <245d1d6d-46ff-1078-3efd-de63849c4e26@uni-muenster.de>
References: <alpine.LFD.2.20.1705091451420.23667@reclus.nhh.no>
 <245d1d6d-46ff-1078-3efd-de63849c4e26@uni-muenster.de>
Message-ID: <alpine.LFD.2.20.1705092012450.19856@reclus.nhh.no>

On Tue, 9 May 2017, Edzer Pebesma wrote:

>
>
> On 09/05/17 15:32, Roger Bivand wrote:
>> While https://github.com/edzer/sfr/wiki/migrating is very helpful,
>> rgeos::gUnaryUnion is not the typical use case of sf::st_union. The
>> typical use case, from ASDAR 1st edition, chapter 5 and the maptools
>> "combine_maptools" vignette (the shapefiles are shipped with maptools),
>> is grouping features that should belong to the same statistical entity:
>>
>> library(maptools)
>> vignette("combine_maptools")
>>
>> ###################################################
>> ### chunk number 16:
>> ###################################################
>>
>> library(sf)
>> nc90 <- st_read(system.file("shapes/co37_d90.shp", package = "maptools"))
>> st_crs(nc90) <- "+proj=longlat +datum=NAD27"
>> table(table(paste(nc90$ST, nc90$CO, sep="")))
>>
>> The point is that two counties are represented by multiple features of
>> "POLYGON" objects, rather than single "MULTIPOLYGON" objects, in the
>> input shapefile. I've tried doing:
>>
>> ids <- factor(paste(nc90$ST, nc90$CO, sep=""))
>> nc90a <- st_cast(nc90, to="MULTIPOLYGON", ids=as.integer(ids))
>>
>> but:
>>
>>> dim(nc90a)
>> [1] 104   9
>>
>> all turned into "MULTIPOLYGON", but not grouped, even though I think
>> ids= are as they should be:
>>
>>> table(table(as.integer(ids)))
>>
>>  1  2  4
>> 98  1  1
>
> this is at least documented: nc90 is of class `sf`, and st_cast.sf
> has no ids argument; ... is ignored. If it would merge, it'd need
> some guidance what to do with non-geometry feature attributes.
>

OK, thanks! I was hoping one or other of the many contributors to sf would 
jump in, given the amount of time you commit to moving sf forward!

>>
>> This may be to avoid dropping data.frame rows. It looks as though I can
>> get there using:
>>
>> nc90a <- st_cast(st_geometry(nc90), to="MULTIPOLYGON", ids=as.integer(ids))
>>
>>> length(nc90a)
>> [1] 100
>>
>> but all are "MULTIPOLYGON", not a mixture of "POLYGON" and
>> "MULTIPOLYGON" features, and I've no idea which is which - that is how
>> the order of nc90a relates to the counties of nc90. How do I associate
>> the features in nc90a with their county ids? Where do i find the ids I
>> gave to st_cast in nc90a?
>
> st_cast uses base::split, which converts ids to a factor,
> and returns a list with the order of the levels of that factor
> (alphabetically?). Neither convenient, nor documented. I guess more
> intuitive would be to squeeze, but keep original order, right?
>
> I'd suggest to use instead
>
> nc90a = aggregate(nc90, list(ids = ids), head, n = 1)
>
> which returns a GEOMETRY sf, with 2 MULTIPOLYGON and 98 POLYGON. You
> could st_cast that to MULTIPOLYGON.

This works and keeps the output as c("sf", "data.frame"):

ids <- factor(paste(nc90$ST, nc90$CO, sep=""))
t0 <- aggregate(nc90, list(ids = ids), head, n = 1)
nc90a <- t0[, c("ids", attr(t0, "sf_column"))]
rm(t0)
# n is an argument to head saying take the first value in each ids-group

>
> Alternatively:
>
> library(dplyr)
> bind_cols(nc90, ids=ids) %>%
> 	group_by(ids) %>%
> 	summarise(ST=head(ST,1), CO=head(CO,1), do_union=FALSE)

This works but returns an object of: c("sf", "tbl_df", "tbl", "data.frame"):

library(dplyr)
nc90b <- summarise(group_by(bind_cols(nc90, data.frame(ids=ids)), ids),
   do_union=FALSE)

or equivalently (for released dplyr):

bind_cols(nc90, data.frame(ids=ids)) %>%
         group_by(ids) %>%
         summarise(do_union=FALSE) -> nc90b

This fails in:

all.equal(nc90b, nc90a, check.attributes=FALSE)
# Error in equal_data_frame(target, current, ignore_col_order = 
# ignore_col_order,  :
#  Can't join on 'geometry' x 'geometry' because of incompatible types 
# (sfc_GEOMETRY, sfc / sfc_GEOMETRY, sfc)

but:

> all.equal(nc90a, nc90b, check.attributes=FALSE)
[1] TRUE

so being c("tbl_df", "tbl") before "data.frame" makes a difference.

Since I always try to check intermediate results, I only found the missing 
data.frame() in bind_cols() by stepping through - is that implicit in the 
development version of dplyr (or sf)?

Sometime the maptools vignette will migrate, when I get so far ...

Thanks again,

Roger

>
> converts straight into MULTIPOLYGON.
>
> I'll update the sp -> sf migration wiki.
>
>>
>> What am I missing? I'm writing here rather than raising an issue on GH
>> because others may want to know too, and Edzer needs others to reply for
>> him if they know the answer.
>>
>> Puzzled,
>>
>> Roger
>>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From tirico85 at gmail.com  Wed May 10 10:17:42 2017
From: tirico85 at gmail.com (=?UTF-8?Q?Andr=C3=A9s_Peralta?=)
Date: Wed, 10 May 2017 10:17:42 +0200
Subject: [R-sig-Geo]  Problems With Cartography - Help
Message-ID: <CAK_Cwd_FZWevo=G9RJ7swvXG8zYBkTNrd-fgjqUL684A7ShaAQ@mail.gmail.com>

Hi to everyone,

I`m working with the second administrative level cartography from Ecuador
(available in:
http://www.ecuadorencifras.gob.ec//documentos/web-inec/Cartografia/2015/Clasificador_Geografico/2012/SHP.zip).
The shape file is called nxcantones.shp. I`ve been having a lot of problems
opening and working with this cartography in R; but i can open it in Q-GIS
and ARCGIS without any problem (I have opened it in both programs and saved
it again). As the cartography has various objects with the same ID -
aparently the same areas but repeated; we had to run the following sintax
in order to have one ID in each area:

*carto2012 <- readOGR("CANTONES2012/2012CLEAN.shp", "2012CLEAN",
stringsAsFactors=F) *
*proj4string(carto2012) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
+units=m +no_defs")*

*carto2012x <- unionSpatialPolygons(carto2012, IDs=carto2012$DPA_CANTON)*
*proj4string(carto2012x) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
+units=m +no_defs")*


*datos <- data.frame(ID=row.names(carto2012x), stringsAsFactors = F)*
*row.names(datos) <- row.names(carto2012x)*
*carto2012x2 <- SpatialPolygonsDataFrame(carto2012x, data=datos)*

*carto2012 <- carto2012x2 *

Either using the original (nxcantones) or the modified cartography
(carto2012), I can plot the map in R, but I can?t plot it in R Leaflet. It
just opens the base map. I seems as the projection is lost in the way or
that the cartography has a problem.

Any ideas?

-- 

*             Andr?s Peralta*
        Pre-doctoral Researcher
     GREDS/EMCONET / ASPB
<https://www.upf.edu/greds-emconet/en/>   <http://aspb.cat/>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed May 10 10:25:37 2017
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 10 May 2017 10:25:37 +0200
Subject: [R-sig-Geo] Problems With Cartography - Help
In-Reply-To: <CAK_Cwd_FZWevo=G9RJ7swvXG8zYBkTNrd-fgjqUL684A7ShaAQ@mail.gmail.com>
References: <CAK_Cwd_FZWevo=G9RJ7swvXG8zYBkTNrd-fgjqUL684A7ShaAQ@mail.gmail.com>
Message-ID: <CAJuCY5xJmgLSxU3rO5sOu_ih712AWNuS1+YMRh0Z7_0z9+-XBw@mail.gmail.com>

Dear Andres,

You'll need spTransform() to reproject the layer into WGS84

carto2012x <- unionSpatialPolygons(carto2012, IDs=carto2012$DPA_CANTON)
spTransform(carto2012x, CRS("+proj=longlat"))

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2017-05-10 10:17 GMT+02:00 Andr?s Peralta <tirico85 at gmail.com>:

> Hi to everyone,
>
> I`m working with the second administrative level cartography from Ecuador
> (available in:
> http://www.ecuadorencifras.gob.ec//documentos/web-inec/
> Cartografia/2015/Clasificador_Geografico/2012/SHP.zip).
> The shape file is called nxcantones.shp. I`ve been having a lot of problems
> opening and working with this cartography in R; but i can open it in Q-GIS
> and ARCGIS without any problem (I have opened it in both programs and saved
> it again). As the cartography has various objects with the same ID -
> aparently the same areas but repeated; we had to run the following sintax
> in order to have one ID in each area:
>
> *carto2012 <- readOGR("CANTONES2012/2012CLEAN.shp", "2012CLEAN",
> stringsAsFactors=F) *
> *proj4string(carto2012) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
> +units=m +no_defs")*
>
> *carto2012x <- unionSpatialPolygons(carto2012, IDs=carto2012$DPA_CANTON)*
> *proj4string(carto2012x) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
> +units=m +no_defs")*
>
>
> *datos <- data.frame(ID=row.names(carto2012x), stringsAsFactors = F)*
> *row.names(datos) <- row.names(carto2012x)*
> *carto2012x2 <- SpatialPolygonsDataFrame(carto2012x, data=datos)*
>
> *carto2012 <- carto2012x2 *
>
> Either using the original (nxcantones) or the modified cartography
> (carto2012), I can plot the map in R, but I can?t plot it in R Leaflet. It
> just opens the base map. I seems as the projection is lost in the way or
> that the cartography has a problem.
>
> Any ideas?
>
> --
>
> *             Andr?s Peralta*
>         Pre-doctoral Researcher
>      GREDS/EMCONET / ASPB
> <https://www.upf.edu/greds-emconet/en/>   <http://aspb.cat/>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From tirico85 at gmail.com  Wed May 10 10:27:02 2017
From: tirico85 at gmail.com (=?UTF-8?Q?Andr=C3=A9s_Peralta?=)
Date: Wed, 10 May 2017 10:27:02 +0200
Subject: [R-sig-Geo]  Problems With Cartography 2 - Help
Message-ID: <CAK_Cwd_Jwr4Uv6zoJ1qLgc9WpO3-ocnYXm5TVTgkgt11q5=WAg@mail.gmail.com>

Hi to everyone,

I`m working with the second administrative level cartography from Ecuador
(available in: http://www.ecuadorencifras.gob.ec//documentos/web-inec/
Cartografia/2015/Clasificador_Geografico/2012/SHP.zip). The shape file is
called nxcantones.shp. I`ve been having a lot of problems opening and
working with this cartography in R; but i can open it in Q-GIS and ARCGIS
without any problem (I have opened it in both programs and saved it again).
As the cartography has various objects with the same ID - aparently the
same areas but repeated; we had to run the following sintax in order to
have one ID in each area:

*carto2012 <- readOGR("CANTONES2012/2012CLEAN.shp", "2012CLEAN",
stringsAsFactors=F) *
*proj4string(carto2012) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
+units=m +no_defs")*

*carto2012x <- unionSpatialPolygons(carto2012, IDs=carto2012$DPA_CANTON)*
*proj4string(carto2012x) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
+units=m +no_defs")*


*datos <- data.frame(ID=row.names(carto2012x), stringsAsFactors = F)*
*row.names(datos) <- row.names(carto2012x)*
*carto2012x2 <- SpatialPolygonsDataFrame(carto2012x, data=datos)*

*carto2012 <- carto2012x2 *

For our work, we had to make a neighborhood matrix of the cartography.

*x.nb <- poly2nb(carto2012u2) # U2 is the cartography after the recode of
some areas that had to be joined.*

*summary(x.nb)*

This seems to work fine. The problem is we need to connect the three island
areas (the Galapagos) but when we try to do so, it connects many areas
along all the cartography. I've tried to do it manually using *edit.nb* but
it does not work. I?ve also tried the following sintaxes:

x.nb1 <- x.nb
which(card(x.nb1)==0) #to discover the id of these areas without connections
id <- function(x){which(carto2012u2$ID==x)}

x.nb1[[id(2001)]] = unique(as.integer(sort(c(x.nb1[[id(2001)]], id(2002)))))
x.nb1[[id(2002)]] = unique(as.integer(sort(c(x.nb1[[id(2002)]], id(2001)))))

x.nb1[[id(2001)]] = unique(as.integer(sort(c(x.nb1[[id(2001)]], id(2003)))))
x.nb1[[id(2003)]] = unique(as.integer(sort(c(x.nb1[[id(2003)]], id(2001)))))

x.nb1[[id(2002)]] = unique(as.integer(sort(c(x.nb1[[id(2002)]], id(2003)))))
x.nb1[[id(2003)]] = unique(as.integer(sort(c(x.nb1[[id(2003)]], id(2002)))))

####
x.nb1[[id("2001")]] = unique(as.integer(sort(c(x.nb1[[id("2001")]],
id("2002")))))
x.nb1[[id("2002")]] = unique(as.integer(sort( id("2001"))))

x.nb1[[id("2001")]] = unique(as.integer(sort(c(x.nb1[[id("2001")]],
id("2003")))))
x.nb1[[id("2003")]] = unique(as.integer(sort( id("2001"))))

x.nb1[[id("2002")]] = unique(as.integer(sort(c(x.nb1[[id("2002")]],
id("2003")))))
x.nb1[[id("2003")]] = unique(as.integer(sort( id("2002"))))

Any ideas or suggestions? Your help will really be appreciated.


-- 

*             Andr?s Peralta*
        Pre-doctoral Researcher
     GREDS/EMCONET / ASPB
<https://www.upf.edu/greds-emconet/en/>   <http://aspb.cat/>

	[[alternative HTML version deleted]]


From tirico85 at gmail.com  Wed May 10 12:01:18 2017
From: tirico85 at gmail.com (=?UTF-8?Q?Andr=C3=A9s_Peralta?=)
Date: Wed, 10 May 2017 12:01:18 +0200
Subject: [R-sig-Geo] Problems With Cartography - Help
In-Reply-To: <CAJuCY5xJmgLSxU3rO5sOu_ih712AWNuS1+YMRh0Z7_0z9+-XBw@mail.gmail.com>
References: <CAK_Cwd_FZWevo=G9RJ7swvXG8zYBkTNrd-fgjqUL684A7ShaAQ@mail.gmail.com>
 <CAJuCY5xJmgLSxU3rO5sOu_ih712AWNuS1+YMRh0Z7_0z9+-XBw@mail.gmail.com>
Message-ID: <CAK_Cwd9_ThY_oy0O92XwnAGsheV7j8NCCqVekunyO1ZF4Xd6Qw@mail.gmail.com>

Thank you Thierry, it worked perfectly.

Best regards,

Andr?s

On Wed, May 10, 2017 at 10:25 AM, Thierry Onkelinx <thierry.onkelinx at inbo.be
> wrote:

> Dear Andres,
>
> You'll need spTransform() to reproject the layer into WGS84
>
> carto2012x <- unionSpatialPolygons(carto2012, IDs=carto2012$DPA_CANTON)
> spTransform(carto2012x, CRS("+proj=longlat"))
>
> Best regards,
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2017-05-10 10:17 GMT+02:00 Andr?s Peralta <tirico85 at gmail.com>:
>
>> Hi to everyone,
>>
>> I`m working with the second administrative level cartography from Ecuador
>> (available in:
>> http://www.ecuadorencifras.gob.ec//documentos/web-inec/Carto
>> grafia/2015/Clasificador_Geografico/2012/SHP.zip).
>> The shape file is called nxcantones.shp. I`ve been having a lot of
>> problems
>> opening and working with this cartography in R; but i can open it in Q-GIS
>> and ARCGIS without any problem (I have opened it in both programs and
>> saved
>> it again). As the cartography has various objects with the same ID -
>> aparently the same areas but repeated; we had to run the following sintax
>> in order to have one ID in each area:
>>
>> *carto2012 <- readOGR("CANTONES2012/2012CLEAN.shp", "2012CLEAN",
>> stringsAsFactors=F) *
>> *proj4string(carto2012) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
>> +units=m +no_defs")*
>>
>> *carto2012x <- unionSpatialPolygons(carto2012, IDs=carto2012$DPA_CANTON)*
>> *proj4string(carto2012x) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
>> +units=m +no_defs")*
>>
>>
>> *datos <- data.frame(ID=row.names(carto2012x), stringsAsFactors = F)*
>> *row.names(datos) <- row.names(carto2012x)*
>> *carto2012x2 <- SpatialPolygonsDataFrame(carto2012x, data=datos)*
>>
>> *carto2012 <- carto2012x2 *
>>
>> Either using the original (nxcantones) or the modified cartography
>> (carto2012), I can plot the map in R, but I can?t plot it in R Leaflet. It
>> just opens the base map. I seems as the projection is lost in the way or
>> that the cartography has a problem.
>>
>> Any ideas?
>>
>> --
>>
>> *             Andr?s Peralta*
>>         Pre-doctoral Researcher
>>      GREDS/EMCONET / ASPB
>> <https://www.upf.edu/greds-emconet/en/>   <http://aspb.cat/>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>


-- 

*             Andr?s Peralta*
        Pre-doctoral Researcher
     GREDS/EMCONET / ASPB
<https://www.upf.edu/greds-emconet/en/>   <http://aspb.cat/>

	[[alternative HTML version deleted]]


From mercetadzio at gmail.com  Wed May 10 14:37:03 2017
From: mercetadzio at gmail.com (=?UTF-8?Q?Mercedes_Rom=C3=A1n?=)
Date: Wed, 10 May 2017 14:37:03 +0200
Subject: [R-sig-Geo] Fwd: help - MC simulations at universal co-kriging
In-Reply-To: <CAAOWriOGAC5jhZcO2bVPti57HeVAaDXmLU0y8HZyk1dWYgRCDA@mail.gmail.com>
References: <CAAOWriOGAC5jhZcO2bVPti57HeVAaDXmLU0y8HZyk1dWYgRCDA@mail.gmail.com>
Message-ID: <CAAOWriNai_JaFv1xPY7p7eO-2TeWSk0dQCi+fDc8RhZTvNr2yQ@mail.gmail.com>

Dear r-sig-geo members,

I am trying to perform Monte-Carlo simulations for a universal co-kriging
system. The script I am using would work with a small dataset, as it is
with the meuse dataset. However, my calibration dataset has ~ 37000
observations and four variables (two providing the trend, two to be
modelled). The dataset where I am trying to do the simulation consists
aproximatelly of 2000 locations. Despite using the closest 20-30
observations for local co-kriging, and testing first with just 10 of the
2000 locations, and even for 2-5 simulations, R seems to not finish the
task.

Should I change anything else to speed up the calculations, or complement
gstat with another package?

Thanks in advance for your help,

Mercedes



data(meuse)
data(meuse.grid)

### check correlation with covariables
cor.test(log10(meuse$lead), log10(meuse$zinc))
cor.test(log10(meuse$lead), (meuse$dist))
cor.test(log10(meuse$zinc), (meuse$dist))

meuse$ltpb <- log10(meuse$lead)
meuse$ltzn <- log10(meuse$zinc)

### Transofrm to spatial
coordinates(meuse) <- ~ x + y
coordinates(meuse.grid) <- ~ x + y


# experimental variogam
v.ltpb <- variogram(ltpb ~ dist, data=meuse, cutoff=1800, width=200)
plot(v.ltpb, pl=T)
# estimate variogram model form and parameters by eye
m.ltpb <- vgm(0.035,"Sph",800,0.015)
plot(v.ltpb, pl=T, model=m.ltpb)
# fit model parameters
m.ltpb.f <- fit.variogram(v.ltpb, m.ltpb)
plot(v.ltpb, pl=T, model=m.ltpb.f)

### Set  local kriging in the gstat object
g <- gstat(NULL, id = "ltpb", form = ltpb ~ dist, data=meuse, nmax=30)
g <- gstat(g, id = "ltzn", form = ltzn ~ dist, data=meuse, nmax=30)

v.cross <- variogram(g)
str(v.cross)
plot(v.cross, pl=F)

g <- gstat(g, id = "ltpb", model = m.ltpb.f, fill.all=T, nmax=30)

g <- fit.lmc(v.cross, g, correct.diagonal = 1.01)
plot(v.cross, model=g$model)

# Universal co-kriging
k.c <- predict(g, meuse.grid)
str(k.c)
plot.kresults(k.c, "ltpb", meuse, "lead", meuse, "UcK with Zn covariable")

#### try MC simulation
MC <- 100
sims <- predict(g, meuse.grid, nsim = MC)

	[[alternative HTML version deleted]]


From englishchristophera at gmail.com  Thu May 11 15:25:45 2017
From: englishchristophera at gmail.com (chris english)
Date: Thu, 11 May 2017 09:25:45 -0400
Subject: [R-sig-Geo] Problems With Cartography 2 - Help
In-Reply-To: <CAK_Cwd_Jwr4Uv6zoJ1qLgc9WpO3-ocnYXm5TVTgkgt11q5=WAg@mail.gmail.com>
References: <CAK_Cwd_Jwr4Uv6zoJ1qLgc9WpO3-ocnYXm5TVTgkgt11q5=WAg@mail.gmail.com>
Message-ID: <CAASFQpQHSKU+8VeY78-xRvyZzKFti=fi67__8MRvzSE8Gvyfhw@mail.gmail.com>

Andres,

I'm thinking you want to 'unsimplify' the topology prior to your poly2nb by
a slight negative gBuffer on the Galapagos polygons,
reduce the Galapagos polygons by the buffer and un-share the boundaries.
I'm trying some code, but looking at a mix of these as
guidance:

https://www.rdocumentation.org/packages/rgeos/versions/0.3-22/topics/gBuffer
https://gist.github.com/mstrimas/1b4a4b93a9d4a158bce4
https://gis.stackexchange.com/questions/93096/how-to-perform-a-true-gis-clip-of-polygons-layer-using-a-polygon-layer-in-r

Saludos,
Chris

On Wed, May 10, 2017 at 4:27 AM, Andr?s Peralta <tirico85 at gmail.com> wrote:

> Hi to everyone,
>
> I`m working with the second administrative level cartography from Ecuador
> (available in: http://www.ecuadorencifras.gob.ec//documentos/web-inec/
> Cartografia/2015/Clasificador_Geografico/2012/SHP.zip). The shape file is
> called nxcantones.shp. I`ve been having a lot of problems opening and
> working with this cartography in R; but i can open it in Q-GIS and ARCGIS
> without any problem (I have opened it in both programs and saved it again).
> As the cartography has various objects with the same ID - aparently the
> same areas but repeated; we had to run the following sintax in order to
> have one ID in each area:
>
> *carto2012 <- readOGR("CANTONES2012/2012CLEAN.shp", "2012CLEAN",
> stringsAsFactors=F) *
> *proj4string(carto2012) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
> +units=m +no_defs")*
>
> *carto2012x <- unionSpatialPolygons(carto2012, IDs=carto2012$DPA_CANTON)*
> *proj4string(carto2012x) <- CRS("+proj=utm +zone=17 +south +datum=WGS84
> +units=m +no_defs")*
>
>
> *datos <- data.frame(ID=row.names(carto2012x), stringsAsFactors = F)*
> *row.names(datos) <- row.names(carto2012x)*
> *carto2012x2 <- SpatialPolygonsDataFrame(carto2012x, data=datos)*
>
> *carto2012 <- carto2012x2 *
>
> For our work, we had to make a neighborhood matrix of the cartography.
>
> *x.nb <- poly2nb(carto2012u2) # U2 is the cartography after the recode of
> some areas that had to be joined.*
>
> *summary(x.nb)*
>
> This seems to work fine. The problem is we need to connect the three island
> areas (the Galapagos) but when we try to do so, it connects many areas
> along all the cartography. I've tried to do it manually using *edit.nb* but
> it does not work. I?ve also tried the following sintaxes:
>
> x.nb1 <- x.nb
> which(card(x.nb1)==0) #to discover the id of these areas without
> connections
> id <- function(x){which(carto2012u2$ID==x)}
>
> x.nb1[[id(2001)]] = unique(as.integer(sort(c(x.nb1[[id(2001)]],
> id(2002)))))
> x.nb1[[id(2002)]] = unique(as.integer(sort(c(x.nb1[[id(2002)]],
> id(2001)))))
>
> x.nb1[[id(2001)]] = unique(as.integer(sort(c(x.nb1[[id(2001)]],
> id(2003)))))
> x.nb1[[id(2003)]] = unique(as.integer(sort(c(x.nb1[[id(2003)]],
> id(2001)))))
>
> x.nb1[[id(2002)]] = unique(as.integer(sort(c(x.nb1[[id(2002)]],
> id(2003)))))
> x.nb1[[id(2003)]] = unique(as.integer(sort(c(x.nb1[[id(2003)]],
> id(2002)))))
>
> ####
> x.nb1[[id("2001")]] = unique(as.integer(sort(c(x.nb1[[id("2001")]],
> id("2002")))))
> x.nb1[[id("2002")]] = unique(as.integer(sort( id("2001"))))
>
> x.nb1[[id("2001")]] = unique(as.integer(sort(c(x.nb1[[id("2001")]],
> id("2003")))))
> x.nb1[[id("2003")]] = unique(as.integer(sort( id("2001"))))
>
> x.nb1[[id("2002")]] = unique(as.integer(sort(c(x.nb1[[id("2002")]],
> id("2003")))))
> x.nb1[[id("2003")]] = unique(as.integer(sort( id("2002"))))
>
> Any ideas or suggestions? Your help will really be appreciated.
>
>
> --
>
> *             Andr?s Peralta*
>         Pre-doctoral Researcher
>      GREDS/EMCONET / ASPB
> <https://www.upf.edu/greds-emconet/en/>   <http://aspb.cat/>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From miriam.puets at thuenen.de  Fri May 12 10:21:03 2017
From: miriam.puets at thuenen.de (Miriam =?utf-8?B?UMO8dHM=?=)
Date: Fri, 12 May 2017 10:21:03 +0200 (CEST)
Subject: [R-sig-Geo] Fill raster with information if it intersects with a
 Polygon for ASCii File
Message-ID: <872168617.11574780.1494577263463.JavaMail.zimbra@thuenen.de>

Hi everyone,

I am fairly new at working with raster in R. I have combed through the threats about raster and shape files, but I could find completely what I was looking for. 

May I start with the task at hand:

I want to create a raster with geographic choordinates (done with raster function). 
Next I would like to fill this raster with the information, if each cell is intersecting with a shape file (Polygon) or not. 
Next I need to write an ASCii file with the choords, and 1 or 0 for intersecting or not. 

The reason why I am doing this is because I am working with Ecospace and I would like to include marine protected areas not by drawing but by reading an ASCii file. 

I hope I made clear what I am going for.

Thanks for information!


<?)))>< >?)))>< >?)))>< >?)))><

Miriam P?ts
Marine Lebende Ressourcen/ Marine Living Resources
Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries
Palmaille 9
22767 Hamburg (Germany)

Tel:  +49 40 38905-105 
Mail: miriam.puets at thuenen.de


From tech_dev at wildintellect.com  Fri May 12 17:30:25 2017
From: tech_dev at wildintellect.com (Alex Mandel)
Date: Fri, 12 May 2017 08:30:25 -0700
Subject: [R-sig-Geo] Fill raster with information if it intersects with
 a Polygon for ASCii File
In-Reply-To: <872168617.11574780.1494577263463.JavaMail.zimbra@thuenen.de>
References: <872168617.11574780.1494577263463.JavaMail.zimbra@thuenen.de>
Message-ID: <a11e947c-aa2b-6ac7-faac-e15c394d0a2d@wildintellect.com>

Does the output need to be an ASCII Raster or a text file with center
coordinates of each cell and a value?

#Make a raster of value 1
#Mask that raster with the polygon layer, updatevalue=0 to set the area
outside the polygon to 0
mask()
https://www.rdocumentation.org/packages/raster/versions/2.5-8/topics/mask

To save as ASCII raster check the writeRaster (rgdal parameters).

To write a text file 1 line with center coordinates and and values,
rasterToPoints()
 https://www.rdocumentation.org/packages/raster/versions/2.5-8/topics/rasterToPoints
Then put the coords in the DataFrame and write.csv, or use writeOGR with
csv as the output format.


You could also rasterize the polygons, and reclassify. Or make the point
layer from the raster and use over, or intersect to pull values into the
points.

Hope that gives you some options.

-Alex

On 05/12/2017 01:21 AM, Miriam P?ts wrote:
> Hi everyone,
> 
> I am fairly new at working with raster in R. I have combed through the threats about raster and shape files, but I could find completely what I was looking for. 
> 
> May I start with the task at hand:
> 
> I want to create a raster with geographic choordinates (done with raster function). 
> Next I would like to fill this raster with the information, if each cell is intersecting with a shape file (Polygon) or not. 
> Next I need to write an ASCii file with the choords, and 1 or 0 for intersecting or not. 
> 
> The reason why I am doing this is because I am working with Ecospace and I would like to include marine protected areas not by drawing but by reading an ASCii file. 
> 
> I hope I made clear what I am going for.
> 
> Thanks for information!
> 
> 
> <?)))>< >?)))>< >?)))>< >?)))><
> 
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources
> Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries
> Palmaille 9
> 22767 Hamburg (Germany)
> 
> Tel:  +49 40 38905-105 
> Mail: miriam.puets at thuenen.de
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From tiernanmartin at gmail.com  Fri May 12 20:22:22 2017
From: tiernanmartin at gmail.com (Tiernan Martin)
Date: Fri, 12 May 2017 18:22:22 +0000
Subject: [R-sig-Geo] Is a simple feature -friendly version of spdep being
	developed?
Message-ID: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>

Is anyone thinking about creating an adaptation of the `spdep` package that
expects sf-class inputs and works well in a pipeline?

I understand that there is skepticism about the wisdom of adopting the
?tidyverse? principles throughout the R package ecosystem, and I share the
concern that an over-reliance on any single paradigm could reduce the
resilience and diversity of the system as a whole.

That said, I believe that the enthusiastic adoption of the `sf` package and
the package's connections with widely-used tidyverse packages like `dplyr`
and `ggplot2` may result in increased demand for sf-friendly spatial
analysis tools. As an amateur who recently started using R as my primary
GIS tool, it seems like the tidyverse's preference for dataframes, S3
objects, list columns, and pipeline workflows would be well-suited to the
field of spatial analysis. Are there some fundamental reasons why the
`spdep` tools cannot (or should not) be adapted to the tidyverse "dialect"?

Let me put the question in the context of an actual analysis: in February
2017, the pop culture infovis website The Pudding (https://pudding.cool/)
published an analysis of regional preferences for Oscar-nominated films in
the US (https://pudding.cool/2017/02/oscars_so_mapped/). A few days ago,
the author posted a tutorial explaining the method of ?regional smoothing?
used to create the article?s choropleths (
https://pudding.cool/process/regional_smoothing/).

The method relies on several `spdep` functions (
https://github.com/polygraph-cool/smoothing_tutorial/blob/master/smoothing_tutorial.R).
In the code below, I provide reprex with a smaller dataset included in the
`sf` package:

library(sf)
library(spdep)

nc <- st_read(system.file("shape/nc.shp", package = "sf"))  # North
Carolina counties
nc_shp <- as(nc,'Spatial')

coords <- coordinates(nc_shp)
IDs<-row.names(as(nc_shp, "data.frame"))

knn5 <- knn2nb(knearneigh(coords, k = 5), row.names = IDs)  # find the
nearest neighbors for each county
knn5 <- include.self(knn5)

localGvalues <- localG(x = as.numeric(nc_shp at data$NWBIR74), listw =
nb2listw(knn5, style = "B"), zero.policy = TRUE) # calculate the G scores
localGvalues <- round(localGvalues,3)

nc_shp at data$LOCAL_G <- as.numeric(localGvalues)

p1 <- spplot(nc_shp, c('NWBIR74'))
p2 <- spplot(nc_shp, c('LOCAL_G'))
plot(p1, split=c(1,1,2,2), more=TRUE)
plot(p2, split=c(1,2,2,2), more=TRUE)

Here?s what I imagine that would look like in a tidyverse pipeline (please
note that this code is for illustrative purposes and will not run):

library(tidyverse)
library(purrr)
library(sf)
library(sfdep) # this package doesn't exist (yet)

nc <- st_read(system.file("shape/nc.shp", package = "sf"))

nc_g <-
  nc %>%
  mutate(KNN = map(.x = geometry, ~ sfdep::st_knn(.x, k = 5, include.self =
TRUE)),  # find the nearest neighbors for each county
         NB_LIST = map(.x = KNN, ~ sfdep::st_nb_list(.x, style = 'B')),  #
make a list of the neighbors using the binary method
         LOCAL_G = sfdep::st_localG(x = NWBIR74, listw = NB_LIST,
zero.policy = TRUE),  # calculate the G scores
         LOCAL_G = round(LOCAL_G,3))

We can see that the (hypothetical) tidyverse version reduces the amount of
intermediate objects and wraps the creation of the G scores into a single
code chunk with clear steps.

I'd be grateful to hear from the users and developers of the `spdep` and
`sf` packages about this topic!

Tiernan Martin

	[[alternative HTML version deleted]]


From bogaso.christofer at gmail.com  Fri May 12 21:36:49 2017
From: bogaso.christofer at gmail.com (Christofer Bogaso)
Date: Sat, 13 May 2017 01:06:49 +0530
Subject: [R-sig-Geo] Drawing World map divided into 6 economic regions
Message-ID: <CA+dpOJ=65DWSkc8tVzDmWCwVMs6nbcGcRUehHRf=LY1u-PXUnQ@mail.gmail.com>

Hi,

I previously posted this in General R forum, however experts there
suggested to post in this forum.

Below is the description of my issue :

I am trying to draw a World map which is divided into 6 Economic
regions as available in below link

http://www.worldbank.org/en/about/annual-report/regions

I am aware of various R ways to draw World map based on Countries like
one available in

http://stackoverflow.com/questions/24136868/plot-map-with-values-for-countries-as-color-in-r

However I do not want to put any individual Country boundaries,
instead the individual boundaries of 6 Economic regions in the World.

Can you please suggest how can I achieve such a World map. Any pointer
will be highly appreciated.

Thanks for your time.


From tech_dev at wildintellect.com  Sat May 13 04:08:58 2017
From: tech_dev at wildintellect.com (Alex Mandel)
Date: Fri, 12 May 2017 19:08:58 -0700
Subject: [R-sig-Geo] Drawing World map divided into 6 economic regions
In-Reply-To: <CA+dpOJ=65DWSkc8tVzDmWCwVMs6nbcGcRUehHRf=LY1u-PXUnQ@mail.gmail.com>
References: <CA+dpOJ=65DWSkc8tVzDmWCwVMs6nbcGcRUehHRf=LY1u-PXUnQ@mail.gmail.com>
Message-ID: <668dfd4b-5dd1-9e4b-f1b6-faec313738d4@wildintellect.com>

On 05/12/2017 12:36 PM, Christofer Bogaso wrote:
> Hi,
> 
> I previously posted this in General R forum, however experts there
> suggested to post in this forum.
> 
> Below is the description of my issue :
> 
> I am trying to draw a World map which is divided into 6 Economic
> regions as available in below link
> 
> http://www.worldbank.org/en/about/annual-report/regions
> 
> I am aware of various R ways to draw World map based on Countries like
> one available in
> 
> http://stackoverflow.com/questions/24136868/plot-map-with-values-for-countries-as-color-in-r
> 
> However I do not want to put any individual Country boundaries,
> instead the individual boundaries of 6 Economic regions in the World.
> 
> Can you please suggest how can I achieve such a World map. Any pointer
> will be highly appreciated.
> 
> Thanks for your time.
> 

You need a column in your data that indicates the economic zone, then
you aggregate (aka dissolve in other GIS systems)*.

* The raster package has this function.

I usually use http://www.naturalearthdata.com/downloads/ for world scale
maps. It has a column with the UN defined regions.

Enjoy,
Alex


From Roger.Bivand at nhh.no  Sat May 13 22:39:10 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 13 May 2017 22:39:10 +0200
Subject: [R-sig-Geo] Is a simple feature -friendly version of spdep
 being developed?
In-Reply-To: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>
References: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1705132138000.2107@reclus.nhh.no>

On Fri, 12 May 2017, Tiernan Martin wrote:

> Is anyone thinking about creating an adaptation of the `spdep` package that
> expects sf-class inputs and works well in a pipeline?

I assume that "this is not a pipeline" is a joke for insiders (in the 
pipe)?

No, there is your issue on the sfr github repository that is relevant for 
contiguous neighbours, but not beyond that:

https://github.com/edzer/sfr/issues/234

An sf is a data.frame, and as such should "just work", like "Spatial" 
objects have, in formula/data settings. The problem is (of course) the 
weights matrix.

Should it be a list column (each row has a list nesting two lists, first 
indices, second non-zero weights), or remain separate as it has been for 
20 years, or become a column-oriented representation (Matrix package) - a 
nested list like a list column or a listw obeject is row-oriented. I had 
started thinking about using a sparse column-oriented representation, but 
have not implemented functions accepting them instead of listw objects.

I am very cautious about creating classes for data that were data.frame, 
then sf, and then have the weights built-in. In the simple case it would 
work, but all you have to do is re-order the rows and the link between the 
neighbour ids and row order breaks down; the same applies to subsetting.

The problems to solve first are related the workflows, and easiest to look 
at in the univariate case (Moran, Geary, join-count, Mantel, G, ...) for 
global and local tests. I think that all or almost all of the NSE verbs 
will cause chaos (mutate, select, group) once weights have been created. 
If there is a way to bind the neighour IDs to the input sf object rows, a 
list column might be possible, but we have to permit multiple such columns 
(Queen, Rook, ...), and ensure that subsetting and row-reordering keep the 
graph relations intact (and their modifications, like row-standardisation)

If you've seen any examples of how tidy relates to graphs (adjacency lists 
are like nb objects), we could learn from that.

How should we go about this technically? spdep is on R-Forge and is happy 
there. Its present functionality has to be maintained as it stands, it has 
too many reverse dependencies to break. Should it be re-written from 
scratch (with more sparse matrices internally for computation)?

Your example creates weights on the fly for a local G* map, but has n=100, 
not say n=90000 (LA census blocks). Using the sf_ geos based methods does 
not use STRtrees, which cut the time needed to find contigious neighbours 
from days to seconds. We ought to pre-compute weights, but this messes up 
the flow of data, because a second lot of data (the weights) have to enter 
the pipe, and be matched with the row-ids.

We'd need proof of concept with realistically sized data sets (not yet NY 
taxis, but maybe later ...). spdep started as spweights, sptests and 
spdep, and the first two got folded into the third when stable. If weights 
are the first thing to go for, sfweights is where to go first (and port 
the STRtree envelope intersections for contiguities). It could define the 
new classes, and tests and modelling would use them.

Thanks for starting this discussion.

Roger

>
> I understand that there is skepticism about the wisdom of adopting the
> ?tidyverse? principles throughout the R package ecosystem, and I share the
> concern that an over-reliance on any single paradigm could reduce the
> resilience and diversity of the system as a whole.
>
> That said, I believe that the enthusiastic adoption of the `sf` package and
> the package's connections with widely-used tidyverse packages like `dplyr`
> and `ggplot2` may result in increased demand for sf-friendly spatial
> analysis tools. As an amateur who recently started using R as my primary
> GIS tool, it seems like the tidyverse's preference for dataframes, S3
> objects, list columns, and pipeline workflows would be well-suited to the
> field of spatial analysis. Are there some fundamental reasons why the
> `spdep` tools cannot (or should not) be adapted to the tidyverse "dialect"?
>
> Let me put the question in the context of an actual analysis: in February
> 2017, the pop culture infovis website The Pudding (https://pudding.cool/)
> published an analysis of regional preferences for Oscar-nominated films in
> the US (https://pudding.cool/2017/02/oscars_so_mapped/). A few days ago,
> the author posted a tutorial explaining the method of ?regional smoothing?
> used to create the article?s choropleths (
> https://pudding.cool/process/regional_smoothing/).
>
> The method relies on several `spdep` functions (
> https://github.com/polygraph-cool/smoothing_tutorial/blob/master/smoothing_tutorial.R).
> In the code below, I provide reprex with a smaller dataset included in the
> `sf` package:
>
> library(sf)
> library(spdep)
>
> nc <- st_read(system.file("shape/nc.shp", package = "sf"))  # North
> Carolina counties
> nc_shp <- as(nc,'Spatial')
>
> coords <- coordinates(nc_shp)
> IDs<-row.names(as(nc_shp, "data.frame"))
>
> knn5 <- knn2nb(knearneigh(coords, k = 5), row.names = IDs)  # find the
> nearest neighbors for each county
> knn5 <- include.self(knn5)
>
> localGvalues <- localG(x = as.numeric(nc_shp at data$NWBIR74), listw =
> nb2listw(knn5, style = "B"), zero.policy = TRUE) # calculate the G scores
> localGvalues <- round(localGvalues,3)
>
> nc_shp at data$LOCAL_G <- as.numeric(localGvalues)
>
> p1 <- spplot(nc_shp, c('NWBIR74'))
> p2 <- spplot(nc_shp, c('LOCAL_G'))
> plot(p1, split=c(1,1,2,2), more=TRUE)
> plot(p2, split=c(1,2,2,2), more=TRUE)
>
> Here?s what I imagine that would look like in a tidyverse pipeline (please
> note that this code is for illustrative purposes and will not run):
>
> library(tidyverse)
> library(purrr)
> library(sf)
> library(sfdep) # this package doesn't exist (yet)
>
> nc <- st_read(system.file("shape/nc.shp", package = "sf"))
>
> nc_g <-
>  nc %>%
>  mutate(KNN = map(.x = geometry, ~ sfdep::st_knn(.x, k = 5, include.self =
> TRUE)),  # find the nearest neighbors for each county
>         NB_LIST = map(.x = KNN, ~ sfdep::st_nb_list(.x, style = 'B')),  #
> make a list of the neighbors using the binary method
>         LOCAL_G = sfdep::st_localG(x = NWBIR74, listw = NB_LIST,
> zero.policy = TRUE),  # calculate the G scores
>         LOCAL_G = round(LOCAL_G,3))
>
> We can see that the (hypothetical) tidyverse version reduces the amount of
> intermediate objects and wraps the creation of the G scores into a single
> code chunk with clear steps.
>
> I'd be grateful to hear from the users and developers of the `spdep` and
> `sf` packages about this topic!
>
> Tiernan Martin
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From tiernanmartin at gmail.com  Sun May 14 21:39:53 2017
From: tiernanmartin at gmail.com (Tiernan Martin)
Date: Sun, 14 May 2017 19:39:53 +0000
Subject: [R-sig-Geo] Is a simple feature -friendly version of spdep
	being developed?
In-Reply-To: <alpine.LFD.2.20.1705132138000.2107@reclus.nhh.no>
References: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>
 <alpine.LFD.2.20.1705132138000.2107@reclus.nhh.no>
Message-ID: <CAEo23y6CVLU9NKV0zDWW4vYjyRU8Z8u6xR0UyaS_Eg+S4pWsUg@mail.gmail.com>

You?re right about one thing: those of us introduced to the pipe in our
formative years find it hard to escape (everything is better here inside
the pipe - right?) ;)

Thank you for your very informative response. Before reading it, I had a
vague inkling that matrices were part of the challenge of adapting these
tools to work with simple features, but I also thought perhaps all this
issue needed was a little nudge from a user like me. Your response made it
clear that there is a series of technical decisions that need to be made,
and in order to do that I imagine some exploratory testing is in order. Of
course, it is possible that the data.frame + list-col foundation is just
not well-suited to the needs of spatial weighting/testing tools, but I
suspect that there is some valuable insight to be gained from exploring
this possibility further. And as I stated in my first post, going forward I
expect more users will be interested in seeing this sort of integration
happen.

> How should we go about this technically? spdep is on R-Forge and is happy
> there. Its present functionality has to be maintained as it stands, it
has
> too many reverse dependencies to break. Should it be re-written from
> scratch (with more sparse matrices internally for computation)?
> ...
> We'd need proof of concept with realistically sized data sets (not yet NY
> taxis, but maybe later ...). spdep started as spweights, sptests and
> spdep, and the first two got folded into the third when stable. If
weights
> are the first thing to go for, sfweights is where to go first (and port
> the STRtree envelope intersections for contiguities). It could define the
> new classes, and tests and modelling would use them.

Sounds to me like this package would need to be built from the ground up,
possibly following a similar path to the `sp` development process as you
mentioned. Maybe that presents a challenge with regards to resources (i.e.,
funding, time, etc.). Perhaps project this is a candidate for future
proposals to the R Consortium or other funding sources. I hope that this
post is useful in documenting user interest in seeing the `sf` protocol
integrated into other spatial tools and workflows, and I look forward to
hearing others' thoughts on the matter.

Thanks again,

Tiernan





On Sat, May 13, 2017 at 1:39 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Fri, 12 May 2017, Tiernan Martin wrote:
>
> > Is anyone thinking about creating an adaptation of the `spdep` package
> that
> > expects sf-class inputs and works well in a pipeline?
>
> I assume that "this is not a pipeline" is a joke for insiders (in the
> pipe)?
>
> No, there is your issue on the sfr github repository that is relevant for
> contiguous neighbours, but not beyond that:
>
> https://github.com/edzer/sfr/issues/234
>
> An sf is a data.frame, and as such should "just work", like "Spatial"
> objects have, in formula/data settings. The problem is (of course) the
> weights matrix.
>
> Should it be a list column (each row has a list nesting two lists, first
> indices, second non-zero weights), or remain separate as it has been for
> 20 years, or become a column-oriented representation (Matrix package) - a
> nested list like a list column or a listw obeject is row-oriented. I had
> started thinking about using a sparse column-oriented representation, but
> have not implemented functions accepting them instead of listw objects.
>
> I am very cautious about creating classes for data that were data.frame,
> then sf, and then have the weights built-in. In the simple case it would
> work, but all you have to do is re-order the rows and the link between the
> neighbour ids and row order breaks down; the same applies to subsetting.
>
> The problems to solve first are related the workflows, and easiest to look
> at in the univariate case (Moran, Geary, join-count, Mantel, G, ...) for
> global and local tests. I think that all or almost all of the NSE verbs
> will cause chaos (mutate, select, group) once weights have been created.
> If there is a way to bind the neighour IDs to the input sf object rows, a
> list column might be possible, but we have to permit multiple such columns
> (Queen, Rook, ...), and ensure that subsetting and row-reordering keep the
> graph relations intact (and their modifications, like row-standardisation)
>
> If you've seen any examples of how tidy relates to graphs (adjacency lists
> are like nb objects), we could learn from that.
>
> How should we go about this technically? spdep is on R-Forge and is happy
> there. Its present functionality has to be maintained as it stands, it has
> too many reverse dependencies to break. Should it be re-written from
> scratch (with more sparse matrices internally for computation)?
>
> Your example creates weights on the fly for a local G* map, but has n=100,
> not say n=90000 (LA census blocks). Using the sf_ geos based methods does
> not use STRtrees, which cut the time needed to find contigious neighbours
> from days to seconds. We ought to pre-compute weights, but this messes up
> the flow of data, because a second lot of data (the weights) have to enter
> the pipe, and be matched with the row-ids.
>
> We'd need proof of concept with realistically sized data sets (not yet NY
> taxis, but maybe later ...). spdep started as spweights, sptests and
> spdep, and the first two got folded into the third when stable. If weights
> are the first thing to go for, sfweights is where to go first (and port
> the STRtree envelope intersections for contiguities). It could define the
> new classes, and tests and modelling would use them.
>
> Thanks for starting this discussion.
>
> Roger
>
> >
> > I understand that there is skepticism about the wisdom of adopting the
> > ?tidyverse? principles throughout the R package ecosystem, and I share
> the
> > concern that an over-reliance on any single paradigm could reduce the
> > resilience and diversity of the system as a whole.
> >
> > That said, I believe that the enthusiastic adoption of the `sf` package
> and
> > the package's connections with widely-used tidyverse packages like
> `dplyr`
> > and `ggplot2` may result in increased demand for sf-friendly spatial
> > analysis tools. As an amateur who recently started using R as my primary
> > GIS tool, it seems like the tidyverse's preference for dataframes, S3
> > objects, list columns, and pipeline workflows would be well-suited to the
> > field of spatial analysis. Are there some fundamental reasons why the
> > `spdep` tools cannot (or should not) be adapted to the tidyverse
> "dialect"?
> >
> > Let me put the question in the context of an actual analysis: in February
> > 2017, the pop culture infovis website The Pudding (https://pudding.cool/
> )
> > published an analysis of regional preferences for Oscar-nominated films
> in
> > the US (https://pudding.cool/2017/02/oscars_so_mapped/). A few days ago,
> > the author posted a tutorial explaining the method of ?regional
> smoothing?
> > used to create the article?s choropleths (
> > https://pudding.cool/process/regional_smoothing/).
> >
> > The method relies on several `spdep` functions (
> >
> https://github.com/polygraph-cool/smoothing_tutorial/blob/master/smoothing_tutorial.R
> ).
> > In the code below, I provide reprex with a smaller dataset included in
> the
> > `sf` package:
> >
> > library(sf)
> > library(spdep)
> >
> > nc <- st_read(system.file("shape/nc.shp", package = "sf"))  # North
> > Carolina counties
> > nc_shp <- as(nc,'Spatial')
> >
> > coords <- coordinates(nc_shp)
> > IDs<-row.names(as(nc_shp, "data.frame"))
> >
> > knn5 <- knn2nb(knearneigh(coords, k = 5), row.names = IDs)  # find the
> > nearest neighbors for each county
> > knn5 <- include.self(knn5)
> >
> > localGvalues <- localG(x = as.numeric(nc_shp at data$NWBIR74), listw =
> > nb2listw(knn5, style = "B"), zero.policy = TRUE) # calculate the G scores
> > localGvalues <- round(localGvalues,3)
> >
> > nc_shp at data$LOCAL_G <- as.numeric(localGvalues)
> >
> > p1 <- spplot(nc_shp, c('NWBIR74'))
> > p2 <- spplot(nc_shp, c('LOCAL_G'))
> > plot(p1, split=c(1,1,2,2), more=TRUE)
> > plot(p2, split=c(1,2,2,2), more=TRUE)
> >
> > Here?s what I imagine that would look like in a tidyverse pipeline
> (please
> > note that this code is for illustrative purposes and will not run):
> >
> > library(tidyverse)
> > library(purrr)
> > library(sf)
> > library(sfdep) # this package doesn't exist (yet)
> >
> > nc <- st_read(system.file("shape/nc.shp", package = "sf"))
> >
> > nc_g <-
> >  nc %>%
> >  mutate(KNN = map(.x = geometry, ~ sfdep::st_knn(.x, k = 5, include.self
> =
> > TRUE)),  # find the nearest neighbors for each county
> >         NB_LIST = map(.x = KNN, ~ sfdep::st_nb_list(.x, style = 'B')),  #
> > make a list of the neighbors using the binary method
> >         LOCAL_G = sfdep::st_localG(x = NWBIR74, listw = NB_LIST,
> > zero.policy = TRUE),  # calculate the G scores
> >         LOCAL_G = round(LOCAL_G,3))
> >
> > We can see that the (hypothetical) tidyverse version reduces the amount
> of
> > intermediate objects and wraps the creation of the G scores into a single
> > code chunk with clear steps.
> >
> > I'd be grateful to hear from the users and developers of the `spdep` and
> > `sf` packages about this topic!
> >
> > Tiernan Martin
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
> Roger.Bivand at nhh.no
> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Mon May 15 14:16:57 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 15 May 2017 14:16:57 +0200
Subject: [R-sig-Geo] Is a simple feature -friendly version of spdep
 being developed?
In-Reply-To: <CAEo23y6CVLU9NKV0zDWW4vYjyRU8Z8u6xR0UyaS_Eg+S4pWsUg@mail.gmail.com>
References: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>
 <alpine.LFD.2.20.1705132138000.2107@reclus.nhh.no>
 <CAEo23y6CVLU9NKV0zDWW4vYjyRU8Z8u6xR0UyaS_Eg+S4pWsUg@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1705151357190.2983@reclus.nhh.no>

On Sun, 14 May 2017, Tiernan Martin wrote:

> You?re right about one thing: those of us introduced to the pipe in our
> formative years find it hard to escape (everything is better here inside
> the pipe - right?) ;)

Pipes can be great if they don't involve extra inputs at a later stage in 
the pipe, but as:

https://github.com/thomasp85/tidygraph

suggests, it isn't very easy, and may be forced.

>
> Thank you for your very informative response. Before reading it, I had a
> vague inkling that matrices were part of the challenge of adapting these
> tools to work with simple features, but I also thought perhaps all this
> issue needed was a little nudge from a user like me. Your response made it
> clear that there is a series of technical decisions that need to be made,
> and in order to do that I imagine some exploratory testing is in order. Of
> course, it is possible that the data.frame + list-col foundation is just
> not well-suited to the needs of spatial weighting/testing tools, but I
> suspect that there is some valuable insight to be gained from exploring
> this possibility further. And as I stated in my first post, going forward I
> expect more users will be interested in seeing this sort of integration
> happen.

I think that there are two tasks - one to create neighbour objects from sf 
objects, then another to see whether the costs of putting the spatial 
weights into a data.frame (or even more challenging, a tibble) to test for 
spatial autocorrelation or model spatial dependence. Should the weights be 
squirreled away inside the object flowing down the pipe?

sf_object %>% add_weights(...) -> sf_object_with_weights
moran.test(sf_object_with_weights, ...)
geary.test(sf_object_with_weights, ...)

or

sf_object %>% add_weights(...) %>% moran.test(sf_object_with_weights, ...) 
-> output

all in one (output is a htest object). It gets more complex in:

lm.morantest(lm(formula, sf_object), sf_object_with_weights)

as we need the weights and the lm output object.

>
>> How should we go about this technically? spdep is on R-Forge and is happy
>> there. Its present functionality has to be maintained as it stands, it
> has
>> too many reverse dependencies to break. Should it be re-written from
>> scratch (with more sparse matrices internally for computation)?
>> ...
>> We'd need proof of concept with realistically sized data sets (not yet NY
>> taxis, but maybe later ...). spdep started as spweights, sptests and
>> spdep, and the first two got folded into the third when stable. If
> weights
>> are the first thing to go for, sfweights is where to go first (and port
>> the STRtree envelope intersections for contiguities). It could define the
>> new classes, and tests and modelling would use them.
>
> Sounds to me like this package would need to be built from the ground up,
> possibly following a similar path to the `sp` development process as you
> mentioned. Maybe that presents a challenge with regards to resources (i.e.,
> funding, time, etc.). Perhaps project this is a candidate for future
> proposals to the R Consortium or other funding sources. I hope that this
> post is useful in documenting user interest in seeing the `sf` protocol
> integrated into other spatial tools and workflows, and I look forward to
> hearing others' thoughts on the matter.

Making spatial weights from sf objects is just expanding spdep, and is 
feasible. There is no point looking for funding, all it needs is knowledge 
of how things have been done in spdep. Contiguity, knn, distance, 
graph-based neighbours are all feasible.

Extending this to adding weights to sf objects may be going too far. It 
seems slightly forced to - say - attach a sparse matrix to an sf geometry 
column as an attribute, or to add a neighbour nested list column when 
writing subsetting protection is very far from obvious. Like time series 
data (not time stamped observations, but real ordered time series), 
spatial data have implicit order that is tidy in a different way than tidy 
(graph dependencies between observation rows). Maybe mapped features with 
attached attribute data are "tidier" than a table, because they would 
preserve their relative positions?

Roger

>
> Thanks again,
>
> Tiernan
>
>
>
>
>
> On Sat, May 13, 2017 at 1:39 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Fri, 12 May 2017, Tiernan Martin wrote:
>>
>>> Is anyone thinking about creating an adaptation of the `spdep` package
>> that
>>> expects sf-class inputs and works well in a pipeline?
>>
>> I assume that "this is not a pipeline" is a joke for insiders (in the
>> pipe)?
>>
>> No, there is your issue on the sfr github repository that is relevant for
>> contiguous neighbours, but not beyond that:
>>
>> https://github.com/edzer/sfr/issues/234
>>
>> An sf is a data.frame, and as such should "just work", like "Spatial"
>> objects have, in formula/data settings. The problem is (of course) the
>> weights matrix.
>>
>> Should it be a list column (each row has a list nesting two lists, first
>> indices, second non-zero weights), or remain separate as it has been for
>> 20 years, or become a column-oriented representation (Matrix package) - a
>> nested list like a list column or a listw obeject is row-oriented. I had
>> started thinking about using a sparse column-oriented representation, but
>> have not implemented functions accepting them instead of listw objects.
>>
>> I am very cautious about creating classes for data that were data.frame,
>> then sf, and then have the weights built-in. In the simple case it would
>> work, but all you have to do is re-order the rows and the link between the
>> neighbour ids and row order breaks down; the same applies to subsetting.
>>
>> The problems to solve first are related the workflows, and easiest to look
>> at in the univariate case (Moran, Geary, join-count, Mantel, G, ...) for
>> global and local tests. I think that all or almost all of the NSE verbs
>> will cause chaos (mutate, select, group) once weights have been created.
>> If there is a way to bind the neighour IDs to the input sf object rows, a
>> list column might be possible, but we have to permit multiple such columns
>> (Queen, Rook, ...), and ensure that subsetting and row-reordering keep the
>> graph relations intact (and their modifications, like row-standardisation)
>>
>> If you've seen any examples of how tidy relates to graphs (adjacency lists
>> are like nb objects), we could learn from that.
>>
>> How should we go about this technically? spdep is on R-Forge and is happy
>> there. Its present functionality has to be maintained as it stands, it has
>> too many reverse dependencies to break. Should it be re-written from
>> scratch (with more sparse matrices internally for computation)?
>>
>> Your example creates weights on the fly for a local G* map, but has n=100,
>> not say n=90000 (LA census blocks). Using the sf_ geos based methods does
>> not use STRtrees, which cut the time needed to find contigious neighbours
>> from days to seconds. We ought to pre-compute weights, but this messes up
>> the flow of data, because a second lot of data (the weights) have to enter
>> the pipe, and be matched with the row-ids.
>>
>> We'd need proof of concept with realistically sized data sets (not yet NY
>> taxis, but maybe later ...). spdep started as spweights, sptests and
>> spdep, and the first two got folded into the third when stable. If weights
>> are the first thing to go for, sfweights is where to go first (and port
>> the STRtree envelope intersections for contiguities). It could define the
>> new classes, and tests and modelling would use them.
>>
>> Thanks for starting this discussion.
>>
>> Roger
>>
>>>
>>> I understand that there is skepticism about the wisdom of adopting the
>>> ?tidyverse? principles throughout the R package ecosystem, and I share
>> the
>>> concern that an over-reliance on any single paradigm could reduce the
>>> resilience and diversity of the system as a whole.
>>>
>>> That said, I believe that the enthusiastic adoption of the `sf` package
>> and
>>> the package's connections with widely-used tidyverse packages like
>> `dplyr`
>>> and `ggplot2` may result in increased demand for sf-friendly spatial
>>> analysis tools. As an amateur who recently started using R as my primary
>>> GIS tool, it seems like the tidyverse's preference for dataframes, S3
>>> objects, list columns, and pipeline workflows would be well-suited to the
>>> field of spatial analysis. Are there some fundamental reasons why the
>>> `spdep` tools cannot (or should not) be adapted to the tidyverse
>> "dialect"?
>>>
>>> Let me put the question in the context of an actual analysis: in February
>>> 2017, the pop culture infovis website The Pudding (https://pudding.cool/
>> )
>>> published an analysis of regional preferences for Oscar-nominated films
>> in
>>> the US (https://pudding.cool/2017/02/oscars_so_mapped/). A few days ago,
>>> the author posted a tutorial explaining the method of ?regional
>> smoothing?
>>> used to create the article?s choropleths (
>>> https://pudding.cool/process/regional_smoothing/).
>>>
>>> The method relies on several `spdep` functions (
>>>
>> https://github.com/polygraph-cool/smoothing_tutorial/blob/master/smoothing_tutorial.R
>> ).
>>> In the code below, I provide reprex with a smaller dataset included in
>> the
>>> `sf` package:
>>>
>>> library(sf)
>>> library(spdep)
>>>
>>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))  # North
>>> Carolina counties
>>> nc_shp <- as(nc,'Spatial')
>>>
>>> coords <- coordinates(nc_shp)
>>> IDs<-row.names(as(nc_shp, "data.frame"))
>>>
>>> knn5 <- knn2nb(knearneigh(coords, k = 5), row.names = IDs)  # find the
>>> nearest neighbors for each county
>>> knn5 <- include.self(knn5)
>>>
>>> localGvalues <- localG(x = as.numeric(nc_shp at data$NWBIR74), listw =
>>> nb2listw(knn5, style = "B"), zero.policy = TRUE) # calculate the G scores
>>> localGvalues <- round(localGvalues,3)
>>>
>>> nc_shp at data$LOCAL_G <- as.numeric(localGvalues)
>>>
>>> p1 <- spplot(nc_shp, c('NWBIR74'))
>>> p2 <- spplot(nc_shp, c('LOCAL_G'))
>>> plot(p1, split=c(1,1,2,2), more=TRUE)
>>> plot(p2, split=c(1,2,2,2), more=TRUE)
>>>
>>> Here?s what I imagine that would look like in a tidyverse pipeline
>> (please
>>> note that this code is for illustrative purposes and will not run):
>>>
>>> library(tidyverse)
>>> library(purrr)
>>> library(sf)
>>> library(sfdep) # this package doesn't exist (yet)
>>>
>>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))
>>>
>>> nc_g <-
>>>  nc %>%
>>>  mutate(KNN = map(.x = geometry, ~ sfdep::st_knn(.x, k = 5, include.self
>> =
>>> TRUE)),  # find the nearest neighbors for each county
>>>         NB_LIST = map(.x = KNN, ~ sfdep::st_nb_list(.x, style = 'B')),  #
>>> make a list of the neighbors using the binary method
>>>         LOCAL_G = sfdep::st_localG(x = NWBIR74, listw = NB_LIST,
>>> zero.policy = TRUE),  # calculate the G scores
>>>         LOCAL_G = round(LOCAL_G,3))
>>>
>>> We can see that the (hypothetical) tidyverse version reduces the amount
>> of
>>> intermediate objects and wraps the creation of the G scores into a single
>>> code chunk with clear steps.
>>>
>>> I'd be grateful to hear from the users and developers of the `spdep` and
>>> `sf` packages about this topic!
>>>
>>> Tiernan Martin
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
>> Roger.Bivand at nhh.no
>> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
>> http://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From tiernanmartin at gmail.com  Tue May 16 01:27:29 2017
From: tiernanmartin at gmail.com (Tiernan Martin)
Date: Mon, 15 May 2017 23:27:29 +0000
Subject: [R-sig-Geo] Is a simple feature -friendly version of spdep
	being developed?
In-Reply-To: <alpine.LFD.2.20.1705151357190.2983@reclus.nhh.no>
References: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>
 <alpine.LFD.2.20.1705132138000.2107@reclus.nhh.no>
 <CAEo23y6CVLU9NKV0zDWW4vYjyRU8Z8u6xR0UyaS_Eg+S4pWsUg@mail.gmail.com>
 <alpine.LFD.2.20.1705151357190.2983@reclus.nhh.no>
Message-ID: <CAEo23y5H92BTeSxB13_vU5MnXo_y9xVpr143zWAuUHXMkbvGLQ@mail.gmail.com>

> Should the weights be
> squirreled away inside the object flowing down the pipe?

I'd propose that the that functions be written in such a way that the
neighbors, weights, and test results can be stored in list cols within the
data.frame.

This way multiple tests (and their inputs) can be stored in a single,
easily comprehensible, rectangular data object.

Here's some more pretend code to illustrate that concept with regard to sf:

  nc %>%
    group_by(NAME) %>%
    nest %>%
    mutate(
           geometry = map(.x = data, ~.x[['geometry']]),
           neighbors = map(.x = data, .f = my_nb), # creates a list col of
the neighbors
           weights = map(.x = data, .f = add_weights), # creates a weights
object (class=wt, is this a sparse matrix in spdep?)
           test_moran = map2(.x = data, .y =  weights, .f = my_moran), #
creates list of Moran's I and sample kurtosis of x
           test_geary = map2(.x = data, .y = weights, .f = my_geary) #
creates list of Geary's C and sample kurtosis of x
               )

#> # A tibble: 100 x 7
#>           NAME              data         geometry  neighbors  weights
test_moran test_geary
#>        <fctr>            <list>           <list>     <list>   <list>
<list>     <list>
#>  1        Ashe <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
<list [2]> <list [2]>
#>  2   Alleghany <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
<list [2]> <list [2]>
#>  3       Surry <tibble [1 x 14]> <simple_feature> <list [4]> <S3: wt>
<list [2]> <list [2]>
#>  4   Currituck <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
<list [2]> <list [2]>
#>  5 Northampton <tibble [1 x 14]> <simple_feature> <list [3]> <S3: wt>
<list [2]> <list [2]>
#>  6    Hertford <tibble [1 x 14]> <simple_feature> <list [6]> <S3: wt>
<list [2]> <list [2]>
#>  7      Camden <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
<list [2]> <list [2]>
#>  8       Gates <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
<list [2]> <list [2]>
#>  9      Warren <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
<list [2]> <list [2]>
#> 10      Stokes <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
<list [2]> <list [2]>
#> # ... with 90 more rows

 Examples of this nested data.frame / list col approach can be found here:
http://r4ds.had.co.nz/many-models.html#nested-data

> Extending this to adding weights to sf objects may be going too far. It
> seems slightly forced to - say - attach a sparse matrix to an sf geometry
> column as an attribute, or to add a neighbour nested list column when
> writing subsetting protection is very far from obvious. Like time series
> data (not time stamped observations, but real ordered time series),
> spatial data have implicit order that is tidy in a different way than tidy
> (graph dependencies between observation rows). Maybe mapped features with
> attached attribute data are "tidier" than a table, because they would
> preserve their relative positions?

This seems like the crux of the problem. Are you saying that the orthogonal
"tidyness" of the tidyverse is fundamentally different from the underlying
relationships of spatial data, and therefore there's a limit to the
usefulness of tidyverse approach to spatial analysis?

On Mon, May 15, 2017 at 5:17 AM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Sun, 14 May 2017, Tiernan Martin wrote:
>
> > You?re right about one thing: those of us introduced to the pipe in our
> > formative years find it hard to escape (everything is better here inside
> > the pipe - right?) ;)
>
> Pipes can be great if they don't involve extra inputs at a later stage in
> the pipe, but as:
>
> https://github.com/thomasp85/tidygraph
>
> suggests, it isn't very easy, and may be forced.
>
> >
> > Thank you for your very informative response. Before reading it, I had a
> > vague inkling that matrices were part of the challenge of adapting these
> > tools to work with simple features, but I also thought perhaps all this
> > issue needed was a little nudge from a user like me. Your response made
> it
> > clear that there is a series of technical decisions that need to be made,
> > and in order to do that I imagine some exploratory testing is in order.
> Of
> > course, it is possible that the data.frame + list-col foundation is just
> > not well-suited to the needs of spatial weighting/testing tools, but I
> > suspect that there is some valuable insight to be gained from exploring
> > this possibility further. And as I stated in my first post, going
> forward I
> > expect more users will be interested in seeing this sort of integration
> > happen.
>
> I think that there are two tasks - one to create neighbour objects from sf
> objects, then another to see whether the costs of putting the spatial
> weights into a data.frame (or even more challenging, a tibble) to test for
> spatial autocorrelation or model spatial dependence. Should the weights be
> squirreled away inside the object flowing down the pipe?
>
> sf_object %>% add_weights(...) -> sf_object_with_weights
> moran.test(sf_object_with_weights, ...)
> geary.test(sf_object_with_weights, ...)
>
> or
>
> sf_object %>% add_weights(...) %>% moran.test(sf_object_with_weights, ...)
> -> output
>
> all in one (output is a htest object). It gets more complex in:
>
> lm.morantest(lm(formula, sf_object), sf_object_with_weights)
>
> as we need the weights and the lm output object.
>
> >
> >> How should we go about this technically? spdep is on R-Forge and is
> happy
> >> there. Its present functionality has to be maintained as it stands, it
> > has
> >> too many reverse dependencies to break. Should it be re-written from
> >> scratch (with more sparse matrices internally for computation)?
> >> ...
> >> We'd need proof of concept with realistically sized data sets (not yet
> NY
> >> taxis, but maybe later ...). spdep started as spweights, sptests and
> >> spdep, and the first two got folded into the third when stable. If
> > weights
> >> are the first thing to go for, sfweights is where to go first (and port
> >> the STRtree envelope intersections for contiguities). It could define
> the
> >> new classes, and tests and modelling would use them.
> >
> > Sounds to me like this package would need to be built from the ground up,
> > possibly following a similar path to the `sp` development process as you
> > mentioned. Maybe that presents a challenge with regards to resources
> (i.e.,
> > funding, time, etc.). Perhaps project this is a candidate for future
> > proposals to the R Consortium or other funding sources. I hope that this
> > post is useful in documenting user interest in seeing the `sf` protocol
> > integrated into other spatial tools and workflows, and I look forward to
> > hearing others' thoughts on the matter.
>
> Making spatial weights from sf objects is just expanding spdep, and is
> feasible. There is no point looking for funding, all it needs is knowledge
> of how things have been done in spdep. Contiguity, knn, distance,
> graph-based neighbours are all feasible.
>
> Extending this to adding weights to sf objects may be going too far. It
> seems slightly forced to - say - attach a sparse matrix to an sf geometry
> column as an attribute, or to add a neighbour nested list column when
> writing subsetting protection is very far from obvious. Like time series
> data (not time stamped observations, but real ordered time series),
> spatial data have implicit order that is tidy in a different way than tidy
> (graph dependencies between observation rows). Maybe mapped features with
> attached attribute data are "tidier" than a table, because they would
> preserve their relative positions?
>
> Roger
>
> >
> > Thanks again,
> >
> > Tiernan
> >
> >
> >
> >
> >
> > On Sat, May 13, 2017 at 1:39 PM Roger Bivand <Roger.Bivand at nhh.no>
> wrote:
> >
> >> On Fri, 12 May 2017, Tiernan Martin wrote:
> >>
> >>> Is anyone thinking about creating an adaptation of the `spdep` package
> >> that
> >>> expects sf-class inputs and works well in a pipeline?
> >>
> >> I assume that "this is not a pipeline" is a joke for insiders (in the
> >> pipe)?
> >>
> >> No, there is your issue on the sfr github repository that is relevant
> for
> >> contiguous neighbours, but not beyond that:
> >>
> >> https://github.com/edzer/sfr/issues/234
> >>
> >> An sf is a data.frame, and as such should "just work", like "Spatial"
> >> objects have, in formula/data settings. The problem is (of course) the
> >> weights matrix.
> >>
> >> Should it be a list column (each row has a list nesting two lists, first
> >> indices, second non-zero weights), or remain separate as it has been for
> >> 20 years, or become a column-oriented representation (Matrix package) -
> a
> >> nested list like a list column or a listw obeject is row-oriented. I had
> >> started thinking about using a sparse column-oriented representation,
> but
> >> have not implemented functions accepting them instead of listw objects.
> >>
> >> I am very cautious about creating classes for data that were data.frame,
> >> then sf, and then have the weights built-in. In the simple case it would
> >> work, but all you have to do is re-order the rows and the link between
> the
> >> neighbour ids and row order breaks down; the same applies to subsetting.
> >>
> >> The problems to solve first are related the workflows, and easiest to
> look
> >> at in the univariate case (Moran, Geary, join-count, Mantel, G, ...) for
> >> global and local tests. I think that all or almost all of the NSE verbs
> >> will cause chaos (mutate, select, group) once weights have been created.
> >> If there is a way to bind the neighour IDs to the input sf object rows,
> a
> >> list column might be possible, but we have to permit multiple such
> columns
> >> (Queen, Rook, ...), and ensure that subsetting and row-reordering keep
> the
> >> graph relations intact (and their modifications, like
> row-standardisation)
> >>
> >> If you've seen any examples of how tidy relates to graphs (adjacency
> lists
> >> are like nb objects), we could learn from that.
> >>
> >> How should we go about this technically? spdep is on R-Forge and is
> happy
> >> there. Its present functionality has to be maintained as it stands, it
> has
> >> too many reverse dependencies to break. Should it be re-written from
> >> scratch (with more sparse matrices internally for computation)?
> >>
> >> Your example creates weights on the fly for a local G* map, but has
> n=100,
> >> not say n=90000 (LA census blocks). Using the sf_ geos based methods
> does
> >> not use STRtrees, which cut the time needed to find contigious
> neighbours
> >> from days to seconds. We ought to pre-compute weights, but this messes
> up
> >> the flow of data, because a second lot of data (the weights) have to
> enter
> >> the pipe, and be matched with the row-ids.
> >>
> >> We'd need proof of concept with realistically sized data sets (not yet
> NY
> >> taxis, but maybe later ...). spdep started as spweights, sptests and
> >> spdep, and the first two got folded into the third when stable. If
> weights
> >> are the first thing to go for, sfweights is where to go first (and port
> >> the STRtree envelope intersections for contiguities). It could define
> the
> >> new classes, and tests and modelling would use them.
> >>
> >> Thanks for starting this discussion.
> >>
> >> Roger
> >>
> >>>
> >>> I understand that there is skepticism about the wisdom of adopting the
> >>> ?tidyverse? principles throughout the R package ecosystem, and I share
> >> the
> >>> concern that an over-reliance on any single paradigm could reduce the
> >>> resilience and diversity of the system as a whole.
> >>>
> >>> That said, I believe that the enthusiastic adoption of the `sf` package
> >> and
> >>> the package's connections with widely-used tidyverse packages like
> >> `dplyr`
> >>> and `ggplot2` may result in increased demand for sf-friendly spatial
> >>> analysis tools. As an amateur who recently started using R as my
> primary
> >>> GIS tool, it seems like the tidyverse's preference for dataframes, S3
> >>> objects, list columns, and pipeline workflows would be well-suited to
> the
> >>> field of spatial analysis. Are there some fundamental reasons why the
> >>> `spdep` tools cannot (or should not) be adapted to the tidyverse
> >> "dialect"?
> >>>
> >>> Let me put the question in the context of an actual analysis: in
> February
> >>> 2017, the pop culture infovis website The Pudding (
> https://pudding.cool/
> >> )
> >>> published an analysis of regional preferences for Oscar-nominated films
> >> in
> >>> the US (https://pudding.cool/2017/02/oscars_so_mapped/). A few days
> ago,
> >>> the author posted a tutorial explaining the method of ?regional
> >> smoothing?
> >>> used to create the article?s choropleths (
> >>> https://pudding.cool/process/regional_smoothing/).
> >>>
> >>> The method relies on several `spdep` functions (
> >>>
> >>
> https://github.com/polygraph-cool/smoothing_tutorial/blob/master/smoothing_tutorial.R
> >> ).
> >>> In the code below, I provide reprex with a smaller dataset included in
> >> the
> >>> `sf` package:
> >>>
> >>> library(sf)
> >>> library(spdep)
> >>>
> >>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))  # North
> >>> Carolina counties
> >>> nc_shp <- as(nc,'Spatial')
> >>>
> >>> coords <- coordinates(nc_shp)
> >>> IDs<-row.names(as(nc_shp, "data.frame"))
> >>>
> >>> knn5 <- knn2nb(knearneigh(coords, k = 5), row.names = IDs)  # find the
> >>> nearest neighbors for each county
> >>> knn5 <- include.self(knn5)
> >>>
> >>> localGvalues <- localG(x = as.numeric(nc_shp at data$NWBIR74), listw =
> >>> nb2listw(knn5, style = "B"), zero.policy = TRUE) # calculate the G
> scores
> >>> localGvalues <- round(localGvalues,3)
> >>>
> >>> nc_shp at data$LOCAL_G <- as.numeric(localGvalues)
> >>>
> >>> p1 <- spplot(nc_shp, c('NWBIR74'))
> >>> p2 <- spplot(nc_shp, c('LOCAL_G'))
> >>> plot(p1, split=c(1,1,2,2), more=TRUE)
> >>> plot(p2, split=c(1,2,2,2), more=TRUE)
> >>>
> >>> Here?s what I imagine that would look like in a tidyverse pipeline
> >> (please
> >>> note that this code is for illustrative purposes and will not run):
> >>>
> >>> library(tidyverse)
> >>> library(purrr)
> >>> library(sf)
> >>> library(sfdep) # this package doesn't exist (yet)
> >>>
> >>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))
> >>>
> >>> nc_g <-
> >>>  nc %>%
> >>>  mutate(KNN = map(.x = geometry, ~ sfdep::st_knn(.x, k = 5,
> include.self
> >> =
> >>> TRUE)),  # find the nearest neighbors for each county
> >>>         NB_LIST = map(.x = KNN, ~ sfdep::st_nb_list(.x, style =
> 'B')),  #
> >>> make a list of the neighbors using the binary method
> >>>         LOCAL_G = sfdep::st_localG(x = NWBIR74, listw = NB_LIST,
> >>> zero.policy = TRUE),  # calculate the G scores
> >>>         LOCAL_G = round(LOCAL_G,3))
> >>>
> >>> We can see that the (hypothetical) tidyverse version reduces the amount
> >> of
> >>> intermediate objects and wraps the creation of the G scores into a
> single
> >>> code chunk with clear steps.
> >>>
> >>> I'd be grateful to hear from the users and developers of the `spdep`
> and
> >>> `sf` packages about this topic!
> >>>
> >>> Tiernan Martin
> >>>
> >>>       [[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-Geo mailing list
> >>> R-sig-Geo at r-project.org
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >> --
> >> Roger Bivand
> >> Department of Economics, Norwegian School of Economics,
> >> Helleveien 30, N-5045 Bergen, Norway.
> >> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>
> <+47%2055%2095%2093%2055>; e-mail:
> >> Roger.Bivand at nhh.no
> >> Editor-in-Chief of The R Journal,
> https://journal.r-project.org/index.html
> >> http://orcid.org/0000-0003-2392-6140
> >> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
> Roger.Bivand at nhh.no
> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Tue May 16 10:25:04 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 16 May 2017 10:25:04 +0200
Subject: [R-sig-Geo] Is a simple feature -friendly version of spdep
 being developed?
In-Reply-To: <CAEo23y5H92BTeSxB13_vU5MnXo_y9xVpr143zWAuUHXMkbvGLQ@mail.gmail.com>
References: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>
 <alpine.LFD.2.20.1705132138000.2107@reclus.nhh.no>
 <CAEo23y6CVLU9NKV0zDWW4vYjyRU8Z8u6xR0UyaS_Eg+S4pWsUg@mail.gmail.com>
 <alpine.LFD.2.20.1705151357190.2983@reclus.nhh.no>
 <CAEo23y5H92BTeSxB13_vU5MnXo_y9xVpr143zWAuUHXMkbvGLQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1705161004070.28163@reclus.nhh.no>

On Tue, 16 May 2017, Tiernan Martin wrote:

>> Should the weights be
>> squirreled away inside the object flowing down the pipe?
>
> I'd propose that the that functions be written in such a way that the
> neighbors, weights, and test results can be stored in list cols within the
> data.frame.
>
> This way multiple tests (and their inputs) can be stored in a single,
> easily comprehensible, rectangular data object.
>
> Here's some more pretend code to illustrate that concept with regard to sf:
>
>  nc %>%
>    group_by(NAME) %>%
>    nest %>%
>    mutate(
>           geometry = map(.x = data, ~.x[['geometry']]),
>           neighbors = map(.x = data, .f = my_nb), # creates a list col of
> the neighbors
>           weights = map(.x = data, .f = add_weights), # creates a weights
> object (class=wt, is this a sparse matrix in spdep?)
>           test_moran = map2(.x = data, .y =  weights, .f = my_moran), #
> creates list of Moran's I and sample kurtosis of x
>           test_geary = map2(.x = data, .y = weights, .f = my_geary) #
> creates list of Geary's C and sample kurtosis of x
>               )
>
> #> # A tibble: 100 x 7
> #>           NAME              data         geometry  neighbors  weights
> test_moran test_geary
> #>        <fctr>            <list>           <list>     <list>   <list>
> <list>     <list>
> #>  1        Ashe <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
> <list [2]> <list [2]>
> #>  2   Alleghany <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
> <list [2]> <list [2]>
> #>  3       Surry <tibble [1 x 14]> <simple_feature> <list [4]> <S3: wt>
> <list [2]> <list [2]>
> #>  4   Currituck <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
> <list [2]> <list [2]>
> #>  5 Northampton <tibble [1 x 14]> <simple_feature> <list [3]> <S3: wt>
> <list [2]> <list [2]>
> #>  6    Hertford <tibble [1 x 14]> <simple_feature> <list [6]> <S3: wt>
> <list [2]> <list [2]>
> #>  7      Camden <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
> <list [2]> <list [2]>
> #>  8       Gates <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
> <list [2]> <list [2]>
> #>  9      Warren <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
> <list [2]> <list [2]>
> #> 10      Stokes <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
> <list [2]> <list [2]>
> #> # ... with 90 more rows
>
> Examples of this nested data.frame / list col approach can be found here:
> http://r4ds.had.co.nz/many-models.html#nested-data

Thanks, interesting but really scary. You get numeric output for local 
Moran's I and local Geary, but what will happen next?

All of these tests assume a correctly specified mean model. Without this 
assumption being met (absence of trends, absence of global spatial 
autocorrelation in the local test case, absence of missing right-hand-side 
variables and absence of incorrect functional forms for these, ...), a map 
pretending to invite "inference" of "hotspots" etc. - collections of 
observations with similar values of the local statistic, will most often 
be highly misleading. This isn't only about whether it is possible to do 
technically, but also about not making it simpler than reason would 
suggest. With a correct mean model, there may not be any (local) spatial 
autocorrelation left. See ?localmoran.sad and ?localmoran.exact, and their 
references, and McMillen (2003) (the McSpatial author). I won't even 
mention the need to adjust p.values for multiple comparisons ...

>
>> Extending this to adding weights to sf objects may be going too far. It
>> seems slightly forced to - say - attach a sparse matrix to an sf geometry
>> column as an attribute, or to add a neighbour nested list column when
>> writing subsetting protection is very far from obvious. Like time series
>> data (not time stamped observations, but real ordered time series),
>> spatial data have implicit order that is tidy in a different way than tidy
>> (graph dependencies between observation rows). Maybe mapped features with
>> attached attribute data are "tidier" than a table, because they would
>> preserve their relative positions?
>
> This seems like the crux of the problem. Are you saying that the orthogonal
> "tidyness" of the tidyverse is fundamentally different from the underlying
> relationships of spatial data, and therefore there's a limit to the
> usefulness of tidyverse approach to spatial analysis?

Right, this is central and open. Spatial objects can be tidy in the 
tidyverse sense - sf shows this can be done. The open question is whether 
this extends to the analysis of the data. From looking around, it seems 
that time series and graphs suffer from the same kinds of questions, and 
it will be interesting to see what stars turns up for spatio-temporal 
data:

https://github.com/edzer/stars

I see that stars wisely does not include trip/trajectory data structures, 
but analysis might involve reading the array-based representation of 
spatio-temporal environmental drivers for an ensemble of buffers around 
trajectories (say animal movement), then modelling.

Maybe: "everything should be as tidy as it can be but not tidier"?

Roger

>
> On Mon, May 15, 2017 at 5:17 AM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Sun, 14 May 2017, Tiernan Martin wrote:
>>
>>> You?re right about one thing: those of us introduced to the pipe in our
>>> formative years find it hard to escape (everything is better here inside
>>> the pipe - right?) ;)
>>
>> Pipes can be great if they don't involve extra inputs at a later stage in
>> the pipe, but as:
>>
>> https://github.com/thomasp85/tidygraph
>>
>> suggests, it isn't very easy, and may be forced.
>>
>>>
>>> Thank you for your very informative response. Before reading it, I had a
>>> vague inkling that matrices were part of the challenge of adapting these
>>> tools to work with simple features, but I also thought perhaps all this
>>> issue needed was a little nudge from a user like me. Your response made
>> it
>>> clear that there is a series of technical decisions that need to be made,
>>> and in order to do that I imagine some exploratory testing is in order.
>> Of
>>> course, it is possible that the data.frame + list-col foundation is just
>>> not well-suited to the needs of spatial weighting/testing tools, but I
>>> suspect that there is some valuable insight to be gained from exploring
>>> this possibility further. And as I stated in my first post, going
>> forward I
>>> expect more users will be interested in seeing this sort of integration
>>> happen.
>>
>> I think that there are two tasks - one to create neighbour objects from sf
>> objects, then another to see whether the costs of putting the spatial
>> weights into a data.frame (or even more challenging, a tibble) to test for
>> spatial autocorrelation or model spatial dependence. Should the weights be
>> squirreled away inside the object flowing down the pipe?
>>
>> sf_object %>% add_weights(...) -> sf_object_with_weights
>> moran.test(sf_object_with_weights, ...)
>> geary.test(sf_object_with_weights, ...)
>>
>> or
>>
>> sf_object %>% add_weights(...) %>% moran.test(sf_object_with_weights, ...)
>> -> output
>>
>> all in one (output is a htest object). It gets more complex in:
>>
>> lm.morantest(lm(formula, sf_object), sf_object_with_weights)
>>
>> as we need the weights and the lm output object.
>>
>>>
>>>> How should we go about this technically? spdep is on R-Forge and is
>> happy
>>>> there. Its present functionality has to be maintained as it stands, it
>>> has
>>>> too many reverse dependencies to break. Should it be re-written from
>>>> scratch (with more sparse matrices internally for computation)?
>>>> ...
>>>> We'd need proof of concept with realistically sized data sets (not yet
>> NY
>>>> taxis, but maybe later ...). spdep started as spweights, sptests and
>>>> spdep, and the first two got folded into the third when stable. If
>>> weights
>>>> are the first thing to go for, sfweights is where to go first (and port
>>>> the STRtree envelope intersections for contiguities). It could define
>> the
>>>> new classes, and tests and modelling would use them.
>>>
>>> Sounds to me like this package would need to be built from the ground up,
>>> possibly following a similar path to the `sp` development process as you
>>> mentioned. Maybe that presents a challenge with regards to resources
>> (i.e.,
>>> funding, time, etc.). Perhaps project this is a candidate for future
>>> proposals to the R Consortium or other funding sources. I hope that this
>>> post is useful in documenting user interest in seeing the `sf` protocol
>>> integrated into other spatial tools and workflows, and I look forward to
>>> hearing others' thoughts on the matter.
>>
>> Making spatial weights from sf objects is just expanding spdep, and is
>> feasible. There is no point looking for funding, all it needs is knowledge
>> of how things have been done in spdep. Contiguity, knn, distance,
>> graph-based neighbours are all feasible.
>>
>> Extending this to adding weights to sf objects may be going too far. It
>> seems slightly forced to - say - attach a sparse matrix to an sf geometry
>> column as an attribute, or to add a neighbour nested list column when
>> writing subsetting protection is very far from obvious. Like time series
>> data (not time stamped observations, but real ordered time series),
>> spatial data have implicit order that is tidy in a different way than tidy
>> (graph dependencies between observation rows). Maybe mapped features with
>> attached attribute data are "tidier" than a table, because they would
>> preserve their relative positions?
>>
>> Roger
>>
>>>
>>> Thanks again,
>>>
>>> Tiernan
>>>
>>>
>>>
>>>
>>>
>>> On Sat, May 13, 2017 at 1:39 PM Roger Bivand <Roger.Bivand at nhh.no>
>> wrote:
>>>
>>>> On Fri, 12 May 2017, Tiernan Martin wrote:
>>>>
>>>>> Is anyone thinking about creating an adaptation of the `spdep` package
>>>> that
>>>>> expects sf-class inputs and works well in a pipeline?
>>>>
>>>> I assume that "this is not a pipeline" is a joke for insiders (in the
>>>> pipe)?
>>>>
>>>> No, there is your issue on the sfr github repository that is relevant
>> for
>>>> contiguous neighbours, but not beyond that:
>>>>
>>>> https://github.com/edzer/sfr/issues/234
>>>>
>>>> An sf is a data.frame, and as such should "just work", like "Spatial"
>>>> objects have, in formula/data settings. The problem is (of course) the
>>>> weights matrix.
>>>>
>>>> Should it be a list column (each row has a list nesting two lists, first
>>>> indices, second non-zero weights), or remain separate as it has been for
>>>> 20 years, or become a column-oriented representation (Matrix package) -
>> a
>>>> nested list like a list column or a listw obeject is row-oriented. I had
>>>> started thinking about using a sparse column-oriented representation,
>> but
>>>> have not implemented functions accepting them instead of listw objects.
>>>>
>>>> I am very cautious about creating classes for data that were data.frame,
>>>> then sf, and then have the weights built-in. In the simple case it would
>>>> work, but all you have to do is re-order the rows and the link between
>> the
>>>> neighbour ids and row order breaks down; the same applies to subsetting.
>>>>
>>>> The problems to solve first are related the workflows, and easiest to
>> look
>>>> at in the univariate case (Moran, Geary, join-count, Mantel, G, ...) for
>>>> global and local tests. I think that all or almost all of the NSE verbs
>>>> will cause chaos (mutate, select, group) once weights have been created.
>>>> If there is a way to bind the neighour IDs to the input sf object rows,
>> a
>>>> list column might be possible, but we have to permit multiple such
>> columns
>>>> (Queen, Rook, ...), and ensure that subsetting and row-reordering keep
>> the
>>>> graph relations intact (and their modifications, like
>> row-standardisation)
>>>>
>>>> If you've seen any examples of how tidy relates to graphs (adjacency
>> lists
>>>> are like nb objects), we could learn from that.
>>>>
>>>> How should we go about this technically? spdep is on R-Forge and is
>> happy
>>>> there. Its present functionality has to be maintained as it stands, it
>> has
>>>> too many reverse dependencies to break. Should it be re-written from
>>>> scratch (with more sparse matrices internally for computation)?
>>>>
>>>> Your example creates weights on the fly for a local G* map, but has
>> n=100,
>>>> not say n=90000 (LA census blocks). Using the sf_ geos based methods
>> does
>>>> not use STRtrees, which cut the time needed to find contigious
>> neighbours
>>>> from days to seconds. We ought to pre-compute weights, but this messes
>> up
>>>> the flow of data, because a second lot of data (the weights) have to
>> enter
>>>> the pipe, and be matched with the row-ids.
>>>>
>>>> We'd need proof of concept with realistically sized data sets (not yet
>> NY
>>>> taxis, but maybe later ...). spdep started as spweights, sptests and
>>>> spdep, and the first two got folded into the third when stable. If
>> weights
>>>> are the first thing to go for, sfweights is where to go first (and port
>>>> the STRtree envelope intersections for contiguities). It could define
>> the
>>>> new classes, and tests and modelling would use them.
>>>>
>>>> Thanks for starting this discussion.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> I understand that there is skepticism about the wisdom of adopting the
>>>>> ?tidyverse? principles throughout the R package ecosystem, and I share
>>>> the
>>>>> concern that an over-reliance on any single paradigm could reduce the
>>>>> resilience and diversity of the system as a whole.
>>>>>
>>>>> That said, I believe that the enthusiastic adoption of the `sf` package
>>>> and
>>>>> the package's connections with widely-used tidyverse packages like
>>>> `dplyr`
>>>>> and `ggplot2` may result in increased demand for sf-friendly spatial
>>>>> analysis tools. As an amateur who recently started using R as my
>> primary
>>>>> GIS tool, it seems like the tidyverse's preference for dataframes, S3
>>>>> objects, list columns, and pipeline workflows would be well-suited to
>> the
>>>>> field of spatial analysis. Are there some fundamental reasons why the
>>>>> `spdep` tools cannot (or should not) be adapted to the tidyverse
>>>> "dialect"?
>>>>>
>>>>> Let me put the question in the context of an actual analysis: in
>> February
>>>>> 2017, the pop culture infovis website The Pudding (
>> https://pudding.cool/
>>>> )
>>>>> published an analysis of regional preferences for Oscar-nominated films
>>>> in
>>>>> the US (https://pudding.cool/2017/02/oscars_so_mapped/). A few days
>> ago,
>>>>> the author posted a tutorial explaining the method of ?regional
>>>> smoothing?
>>>>> used to create the article?s choropleths (
>>>>> https://pudding.cool/process/regional_smoothing/).
>>>>>
>>>>> The method relies on several `spdep` functions (
>>>>>
>>>>
>> https://github.com/polygraph-cool/smoothing_tutorial/blob/master/smoothing_tutorial.R
>>>> ).
>>>>> In the code below, I provide reprex with a smaller dataset included in
>>>> the
>>>>> `sf` package:
>>>>>
>>>>> library(sf)
>>>>> library(spdep)
>>>>>
>>>>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))  # North
>>>>> Carolina counties
>>>>> nc_shp <- as(nc,'Spatial')
>>>>>
>>>>> coords <- coordinates(nc_shp)
>>>>> IDs<-row.names(as(nc_shp, "data.frame"))
>>>>>
>>>>> knn5 <- knn2nb(knearneigh(coords, k = 5), row.names = IDs)  # find the
>>>>> nearest neighbors for each county
>>>>> knn5 <- include.self(knn5)
>>>>>
>>>>> localGvalues <- localG(x = as.numeric(nc_shp at data$NWBIR74), listw =
>>>>> nb2listw(knn5, style = "B"), zero.policy = TRUE) # calculate the G
>> scores
>>>>> localGvalues <- round(localGvalues,3)
>>>>>
>>>>> nc_shp at data$LOCAL_G <- as.numeric(localGvalues)
>>>>>
>>>>> p1 <- spplot(nc_shp, c('NWBIR74'))
>>>>> p2 <- spplot(nc_shp, c('LOCAL_G'))
>>>>> plot(p1, split=c(1,1,2,2), more=TRUE)
>>>>> plot(p2, split=c(1,2,2,2), more=TRUE)
>>>>>
>>>>> Here?s what I imagine that would look like in a tidyverse pipeline
>>>> (please
>>>>> note that this code is for illustrative purposes and will not run):
>>>>>
>>>>> library(tidyverse)
>>>>> library(purrr)
>>>>> library(sf)
>>>>> library(sfdep) # this package doesn't exist (yet)
>>>>>
>>>>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))
>>>>>
>>>>> nc_g <-
>>>>>  nc %>%
>>>>>  mutate(KNN = map(.x = geometry, ~ sfdep::st_knn(.x, k = 5,
>> include.self
>>>> =
>>>>> TRUE)),  # find the nearest neighbors for each county
>>>>>         NB_LIST = map(.x = KNN, ~ sfdep::st_nb_list(.x, style =
>> 'B')),  #
>>>>> make a list of the neighbors using the binary method
>>>>>         LOCAL_G = sfdep::st_localG(x = NWBIR74, listw = NB_LIST,
>>>>> zero.policy = TRUE),  # calculate the G scores
>>>>>         LOCAL_G = round(LOCAL_G,3))
>>>>>
>>>>> We can see that the (hypothetical) tidyverse version reduces the amount
>>>> of
>>>>> intermediate objects and wraps the creation of the G scores into a
>> single
>>>>> code chunk with clear steps.
>>>>>
>>>>> I'd be grateful to hear from the users and developers of the `spdep`
>> and
>>>>> `sf` packages about this topic!
>>>>>
>>>>> Tiernan Martin
>>>>>
>>>>>       [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>
>> <+47%2055%2095%2093%2055>; e-mail:
>>>> Roger.Bivand at nhh.no
>>>> Editor-in-Chief of The R Journal,
>> https://journal.r-project.org/index.html
>>>> http://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
>> Roger.Bivand at nhh.no
>> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
>> http://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From mdsumner at gmail.com  Tue May 16 14:27:30 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Tue, 16 May 2017 12:27:30 +0000
Subject: [R-sig-Geo] Is a simple feature -friendly version of spdep
	being developed?
In-Reply-To: <alpine.LFD.2.20.1705161004070.28163@reclus.nhh.no>
References: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>
 <alpine.LFD.2.20.1705132138000.2107@reclus.nhh.no>
 <CAEo23y6CVLU9NKV0zDWW4vYjyRU8Z8u6xR0UyaS_Eg+S4pWsUg@mail.gmail.com>
 <alpine.LFD.2.20.1705151357190.2983@reclus.nhh.no>
 <CAEo23y5H92BTeSxB13_vU5MnXo_y9xVpr143zWAuUHXMkbvGLQ@mail.gmail.com>
 <alpine.LFD.2.20.1705161004070.28163@reclus.nhh.no>
Message-ID: <CAAcGz9_PwYcJCoz5A-1LQkAzG8YEOa8VLgGMYw6T3+7DB986iA@mail.gmail.com>

Hello, I'm following this conversation with great interest. There's a lot
here for me
to work with, it's an active pursuit and I don't have much to offer yet but
I have a couple of inline responses below.  I've learnt a lot about spdep
from this discussion, something I've meant to explore for a long time
but my work has never had the same modelling focus).

On Tue, 16 May 2017 at 18:25 Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Tue, 16 May 2017, Tiernan Martin wrote:
>
> >> Should the weights be
> >> squirreled away inside the object flowing down the pipe?
> >
> > I'd propose that the that functions be written in such a way that the
> > neighbors, weights, and test results can be stored in list cols within
> the
> > data.frame.
> >
> > This way multiple tests (and their inputs) can be stored in a single,
> > easily comprehensible, rectangular data object.
> >
> > Here's some more pretend code to illustrate that concept with regard to
> sf:
> >
> >  nc %>%
> >    group_by(NAME) %>%
> >    nest %>%
> >    mutate(
> >           geometry = map(.x = data, ~.x[['geometry']]),
> >           neighbors = map(.x = data, .f = my_nb), # creates a list col of
> > the neighbors
> >           weights = map(.x = data, .f = add_weights), # creates a weights
> > object (class=wt, is this a sparse matrix in spdep?)
> >           test_moran = map2(.x = data, .y =  weights, .f = my_moran), #
> > creates list of Moran's I and sample kurtosis of x
> >           test_geary = map2(.x = data, .y = weights, .f = my_geary) #
> > creates list of Geary's C and sample kurtosis of x
> >               )
> >
> > #> # A tibble: 100 x 7
> > #>           NAME              data         geometry  neighbors  weights
> > test_moran test_geary
> > #>        <fctr>            <list>           <list>     <list>   <list>
> > <list>     <list>
> > #>  1        Ashe <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
> > <list [2]> <list [2]>
> > #>  2   Alleghany <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
> > <list [2]> <list [2]>
> > #>  3       Surry <tibble [1 x 14]> <simple_feature> <list [4]> <S3: wt>
> > <list [2]> <list [2]>
> > #>  4   Currituck <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
> > <list [2]> <list [2]>
> > #>  5 Northampton <tibble [1 x 14]> <simple_feature> <list [3]> <S3: wt>
> > <list [2]> <list [2]>
> > #>  6    Hertford <tibble [1 x 14]> <simple_feature> <list [6]> <S3: wt>
> > <list [2]> <list [2]>
> > #>  7      Camden <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
> > <list [2]> <list [2]>
> > #>  8       Gates <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
> > <list [2]> <list [2]>
> > #>  9      Warren <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
> > <list [2]> <list [2]>
> > #> 10      Stokes <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
> > <list [2]> <list [2]>
> > #> # ... with 90 more rows
> >
> > Examples of this nested data.frame / list col approach can be found here:
> > http://r4ds.had.co.nz/many-models.html#nested-data
>
> Thanks, interesting but really scary. You get numeric output for local
> Moran's I and local Geary, but what will happen next?
>
> All of these tests assume a correctly specified mean model. Without this
> assumption being met (absence of trends, absence of global spatial
> autocorrelation in the local test case, absence of missing right-hand-side
> variables and absence of incorrect functional forms for these, ...), a map
> pretending to invite "inference" of "hotspots" etc. - collections of
> observations with similar values of the local statistic, will most often
> be highly misleading. This isn't only about whether it is possible to do
> technically, but also about not making it simpler than reason would
> suggest. With a correct mean model, there may not be any (local) spatial
> autocorrelation left. See ?localmoran.sad and ?localmoran.exact, and their
> references, and McMillen (2003) (the McSpatial author). I won't even
> mention the need to adjust p.values for multiple comparisons ...
>
> >
> >> Extending this to adding weights to sf objects may be going too far. It
> >> seems slightly forced to - say - attach a sparse matrix to an sf
> geometry
> >> column as an attribute, or to add a neighbour nested list column when
> >> writing subsetting protection is very far from obvious. Like time series
> >> data (not time stamped observations, but real ordered time series),
> >> spatial data have implicit order that is tidy in a different way than
> tidy
> >> (graph dependencies between observation rows). Maybe mapped features
> with
> >> attached attribute data are "tidier" than a table, because they would
> >> preserve their relative positions?
> >
> > This seems like the crux of the problem. Are you saying that the
> orthogonal
> > "tidyness" of the tidyverse is fundamentally different from the
> underlying
> > relationships of spatial data, and therefore there's a limit to the
> > usefulness of tidyverse approach to spatial analysis?
>
> Right, this is central and open. Spatial objects can be tidy in the
> tidyverse sense - sf shows this can be done. The open question is whether
> this extends to the analysis of the data.



I think the structures are still wide open, unless you think the goals of sf
encompass everything. The simple features standard is one corner of the
available ways to structure spatial data. It pointedly ignores relations
between objects, and heavily relies on 2D-specific optimizations. All
the topology is done "on the fly" and is treated as expendable. I personally
want structures that treat simple features and similar formats as
expendable,
but that maintain the topology as a first-class citizen. I don't say this
because I think simple features is not important, it's just not my
main focus. I want time, depth, temperature, salinity, anything and
everything
storable on features, parts, edges, vertices and nodes. Lots of tools
do this or aspects of it, and none of them can be shoe-horned into simple
features, and
they don't have a unifying framework. (It's called "mesh structures", or
the simplicial
complex, or "a database" in other contexts, but none of those are really
enough).

 (I'm dismayed by GIS ideas when they are spoken about as if it's the end
of the story. Sf is really great, but it's now more than the standard and
it's kind
of unique in the available tools for the standard itself - but most of them
are pretty specialized and different to each other, probably unavoidably).

ggplot2 and tidygraph still have a long way to go, but in there I believe
is a language not just of graphics and of converting to and from igraph,
but of flexible ways of specifying how spatial data are constructed,
analysed
and interacted with.
Ggplot builds geometries, but it has no output other than a plot, and the
object that can
modify that plot. Select, group-by, arrange, filter, nest, and normalization
into multiple tables are all part of the powerful ways in which raw data
can be given structure. "Spatial" is just one small part of that structure,
and
unfortunately the 'geographic spatial', the 2D optimizations baked in
by decades old GIS practice seems to have the most sway in discussions.

This community could find a way to bake-in the geometries from ggplot
constructions as sf objects, i.e. convert gg-plot into sf, not into a plot.
I think that's a
digestable project that would really provide some valuable insights.


> From looking around, it seems
> that time series and graphs suffer from the same kinds of questions, and
> it will be interesting to see what stars turns up for spatio-temporal
> data:
>
> https://github.com/edzer/stars
>
> I see that stars wisely does not include trip/trajectory data structures,
> but analysis might involve reading the array-based representation of
> spatio-temporal environmental drivers for an ensemble of buffers around
> trajectories (say animal movement), then modelling.
>
>
Interesting that you mention this, I agree it's a crux area in modern
spatial,
tracking has been neglected for years but it's starting to be looked at.

 I don't see this as terribly difficult, in fact we do this routinely
for all kinds of tracking data both as-measured and modelled. The
difficulty is having
access to the data as well as the motivation in the same place. We have
in-house tools
that sit on large collections of remotely sensed time series data:

https://github.com/AustralianAntarcticDataCentre/raadsync

When you have both the motivation (lots of animal tracking and voyage data)
and
the data on hand it's relatively easy (we have a ready audience at in
Southern Ocean
ecosystems research here).  First step is to write a read-data tool as a
function
of time. Then the problem of matching tracking data to the right space-time
window is easily broken down into the intervals between the time
series of environmental data. At that level, you can employ interpolation
between the layers,
aggregation to compromise resolution with coverage, and even
bring user-defined calculations to the smallish windows to calculate
derived products (rugosity, slope, distance to thresholds etc.)

In our modelled track outputs with tripEstimation/SGAT and bsam[1] we can
use the more probabilistic
estimates of location as more nuanced "query buckets" against the
same environmental data, it's the same issue with matching time with
all the different options you might want. What is hard for us is to share
the tools,
because first you need to arrange access to quite a lot of data, before
you get to see the magic in action.

I do think that stars provides a great opportunity though, and my thoughts
along
those lines are here:

https://github.com/mdsumner/stars.use.cases/blob/master/Use-cases.rmd#animal-tracking-mcmc-estimation


[1] https://CRAN.R-project.org/package=bsam



> Maybe: "everything should be as tidy as it can be but not tidier"?
>
>
I don't think we've even started with the housekeeping here. Some early
thoughts are here:

http://rpubs.com/cyclemumner/sc-rationale

I'm interested to help find ways to nest the spdep idioms in "tidy" ways,
but I'm also not sure
how valuable that is yet. When you convert the structures to primitives the
edge and vertex relations
are inherent already, and it's a mater of standard database
join/select/filter idioms
to link things up:

http://rpubs.com/cyclemumner/neighbours01

http://rpubs.com/cyclemumner/neighbours02

(those examples use https://github.com/mdsumner/scsf for the PRIMITIVE
model
but it's really too raw to use yet). This is all fine and dandy, but then
you still have a list-bag
- i.e. database  - of tables with no strong idioms for treating them as a
single object -
that's the real loftier goal  of tidygraph, it's certainly a clear goal of
the tidyverse to be a
general master of data).

Is this a serious option worth pursuing for spdep? I don't know, but I'm
pursuing it for my own
reasons as detailed above. I don't think modernizing spdep would be that
difficult.  At the least the limitations and
obstacles faced when using it  should be described.

I'm happy to help and I might have a go at it at some point, and I'd
encourage
anyone to get in and try it out.With rlang and the imminent release of the
greatly
improved dplyr it's a good time to start.

Cheers, Mike.



> Roger
>
> >
> > On Mon, May 15, 2017 at 5:17 AM Roger Bivand <Roger.Bivand at nhh.no>
> wrote:
> >
> >> On Sun, 14 May 2017, Tiernan Martin wrote:
> >>
> >>> You?re right about one thing: those of us introduced to the pipe in our
> >>> formative years find it hard to escape (everything is better here
> inside
> >>> the pipe - right?) ;)
> >>
> >> Pipes can be great if they don't involve extra inputs at a later stage
> in
> >> the pipe, but as:
> >>
> >> https://github.com/thomasp85/tidygraph
> >>
> >> suggests, it isn't very easy, and may be forced.
> >>
> >>>
> >>> Thank you for your very informative response. Before reading it, I had
> a
> >>> vague inkling that matrices were part of the challenge of adapting
> these
> >>> tools to work with simple features, but I also thought perhaps all this
> >>> issue needed was a little nudge from a user like me. Your response made
> >> it
> >>> clear that there is a series of technical decisions that need to be
> made,
> >>> and in order to do that I imagine some exploratory testing is in order.
> >> Of
> >>> course, it is possible that the data.frame + list-col foundation is
> just
> >>> not well-suited to the needs of spatial weighting/testing tools, but I
> >>> suspect that there is some valuable insight to be gained from exploring
> >>> this possibility further. And as I stated in my first post, going
> >> forward I
> >>> expect more users will be interested in seeing this sort of integration
> >>> happen.
> >>
> >> I think that there are two tasks - one to create neighbour objects from
> sf
> >> objects, then another to see whether the costs of putting the spatial
> >> weights into a data.frame (or even more challenging, a tibble) to test
> for
> >> spatial autocorrelation or model spatial dependence. Should the weights
> be
> >> squirreled away inside the object flowing down the pipe?
> >>
> >> sf_object %>% add_weights(...) -> sf_object_with_weights
> >> moran.test(sf_object_with_weights, ...)
> >> geary.test(sf_object_with_weights, ...)
> >>
> >> or
> >>
> >> sf_object %>% add_weights(...) %>% moran.test(sf_object_with_weights,
> ...)
> >> -> output
> >>
> >> all in one (output is a htest object). It gets more complex in:
> >>
> >> lm.morantest(lm(formula, sf_object), sf_object_with_weights)
> >>
> >> as we need the weights and the lm output object.
> >>
> >>>
> >>>> How should we go about this technically? spdep is on R-Forge and is
> >> happy
> >>>> there. Its present functionality has to be maintained as it stands, it
> >>> has
> >>>> too many reverse dependencies to break. Should it be re-written from
> >>>> scratch (with more sparse matrices internally for computation)?
> >>>> ...
> >>>> We'd need proof of concept with realistically sized data sets (not yet
> >> NY
> >>>> taxis, but maybe later ...). spdep started as spweights, sptests and
> >>>> spdep, and the first two got folded into the third when stable. If
> >>> weights
> >>>> are the first thing to go for, sfweights is where to go first (and
> port
> >>>> the STRtree envelope intersections for contiguities). It could define
> >> the
> >>>> new classes, and tests and modelling would use them.
> >>>
> >>> Sounds to me like this package would need to be built from the ground
> up,
> >>> possibly following a similar path to the `sp` development process as
> you
> >>> mentioned. Maybe that presents a challenge with regards to resources
> >> (i.e.,
> >>> funding, time, etc.). Perhaps project this is a candidate for future
> >>> proposals to the R Consortium or other funding sources. I hope that
> this
> >>> post is useful in documenting user interest in seeing the `sf` protocol
> >>> integrated into other spatial tools and workflows, and I look forward
> to
> >>> hearing others' thoughts on the matter.
> >>
> >> Making spatial weights from sf objects is just expanding spdep, and is
> >> feasible. There is no point looking for funding, all it needs is
> knowledge
> >> of how things have been done in spdep. Contiguity, knn, distance,
> >> graph-based neighbours are all feasible.
> >>
> >> Extending this to adding weights to sf objects may be going too far. It
> >> seems slightly forced to - say - attach a sparse matrix to an sf
> geometry
> >> column as an attribute, or to add a neighbour nested list column when
> >> writing subsetting protection is very far from obvious. Like time series
> >> data (not time stamped observations, but real ordered time series),
> >> spatial data have implicit order that is tidy in a different way than
> tidy
> >> (graph dependencies between observation rows). Maybe mapped features
> with
> >> attached attribute data are "tidier" than a table, because they would
> >> preserve their relative positions?
> >>
> >> Roger
> >>
> >>>
> >>> Thanks again,
> >>>
> >>> Tiernan
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> On Sat, May 13, 2017 at 1:39 PM Roger Bivand <Roger.Bivand at nhh.no>
> >> wrote:
> >>>
> >>>> On Fri, 12 May 2017, Tiernan Martin wrote:
> >>>>
> >>>>> Is anyone thinking about creating an adaptation of the `spdep`
> package
> >>>> that
> >>>>> expects sf-class inputs and works well in a pipeline?
> >>>>
> >>>> I assume that "this is not a pipeline" is a joke for insiders (in the
> >>>> pipe)?
> >>>>
> >>>> No, there is your issue on the sfr github repository that is relevant
> >> for
> >>>> contiguous neighbours, but not beyond that:
> >>>>
> >>>> https://github.com/edzer/sfr/issues/234
> >>>>
> >>>> An sf is a data.frame, and as such should "just work", like "Spatial"
> >>>> objects have, in formula/data settings. The problem is (of course) the
> >>>> weights matrix.
> >>>>
> >>>> Should it be a list column (each row has a list nesting two lists,
> first
> >>>> indices, second non-zero weights), or remain separate as it has been
> for
> >>>> 20 years, or become a column-oriented representation (Matrix package)
> -
> >> a
> >>>> nested list like a list column or a listw obeject is row-oriented. I
> had
> >>>> started thinking about using a sparse column-oriented representation,
> >> but
> >>>> have not implemented functions accepting them instead of listw
> objects.
> >>>>
> >>>> I am very cautious about creating classes for data that were
> data.frame,
> >>>> then sf, and then have the weights built-in. In the simple case it
> would
> >>>> work, but all you have to do is re-order the rows and the link between
> >> the
> >>>> neighbour ids and row order breaks down; the same applies to
> subsetting.
> >>>>
> >>>> The problems to solve first are related the workflows, and easiest to
> >> look
> >>>> at in the univariate case (Moran, Geary, join-count, Mantel, G, ...)
> for
> >>>> global and local tests. I think that all or almost all of the NSE
> verbs
> >>>> will cause chaos (mutate, select, group) once weights have been
> created.
> >>>> If there is a way to bind the neighour IDs to the input sf object
> rows,
> >> a
> >>>> list column might be possible, but we have to permit multiple such
> >> columns
> >>>> (Queen, Rook, ...), and ensure that subsetting and row-reordering keep
> >> the
> >>>> graph relations intact (and their modifications, like
> >> row-standardisation)
> >>>>
> >>>> If you've seen any examples of how tidy relates to graphs (adjacency
> >> lists
> >>>> are like nb objects), we could learn from that.
> >>>>
> >>>> How should we go about this technically? spdep is on R-Forge and is
> >> happy
> >>>> there. Its present functionality has to be maintained as it stands, it
> >> has
> >>>> too many reverse dependencies to break. Should it be re-written from
> >>>> scratch (with more sparse matrices internally for computation)?
> >>>>
> >>>> Your example creates weights on the fly for a local G* map, but has
> >> n=100,
> >>>> not say n=90000 (LA census blocks). Using the sf_ geos based methods
> >> does
> >>>> not use STRtrees, which cut the time needed to find contigious
> >> neighbours
> >>>> from days to seconds. We ought to pre-compute weights, but this messes
> >> up
> >>>> the flow of data, because a second lot of data (the weights) have to
> >> enter
> >>>> the pipe, and be matched with the row-ids.
> >>>>
> >>>> We'd need proof of concept with realistically sized data sets (not yet
> >> NY
> >>>> taxis, but maybe later ...). spdep started as spweights, sptests and
> >>>> spdep, and the first two got folded into the third when stable. If
> >> weights
> >>>> are the first thing to go for, sfweights is where to go first (and
> port
> >>>> the STRtree envelope intersections for contiguities). It could define
> >> the
> >>>> new classes, and tests and modelling would use them.
> >>>>
> >>>> Thanks for starting this discussion.
> >>>>
> >>>> Roger
> >>>>
> >>>>>
> >>>>> I understand that there is skepticism about the wisdom of adopting
> the
> >>>>> ?tidyverse? principles throughout the R package ecosystem, and I
> share
> >>>> the
> >>>>> concern that an over-reliance on any single paradigm could reduce the
> >>>>> resilience and diversity of the system as a whole.
> >>>>>
> >>>>> That said, I believe that the enthusiastic adoption of the `sf`
> package
> >>>> and
> >>>>> the package's connections with widely-used tidyverse packages like
> >>>> `dplyr`
> >>>>> and `ggplot2` may result in increased demand for sf-friendly spatial
> >>>>> analysis tools. As an amateur who recently started using R as my
> >> primary
> >>>>> GIS tool, it seems like the tidyverse's preference for dataframes, S3
> >>>>> objects, list columns, and pipeline workflows would be well-suited to
> >> the
> >>>>> field of spatial analysis. Are there some fundamental reasons why the
> >>>>> `spdep` tools cannot (or should not) be adapted to the tidyverse
> >>>> "dialect"?
> >>>>>
> >>>>> Let me put the question in the context of an actual analysis: in
> >> February
> >>>>> 2017, the pop culture infovis website The Pudding (
> >> https://pudding.cool/
> >>>> )
> >>>>> published an analysis of regional preferences for Oscar-nominated
> films
> >>>> in
> >>>>> the US (https://pudding.cool/2017/02/oscars_so_mapped/). A few days
> >> ago,
> >>>>> the author posted a tutorial explaining the method of ?regional
> >>>> smoothing?
> >>>>> used to create the article?s choropleths (
> >>>>> https://pudding.cool/process/regional_smoothing/).
> >>>>>
> >>>>> The method relies on several `spdep` functions (
> >>>>>
> >>>>
> >>
> https://github.com/polygraph-cool/smoothing_tutorial/blob/master/smoothing_tutorial.R
> >>>> ).
> >>>>> In the code below, I provide reprex with a smaller dataset included
> in
> >>>> the
> >>>>> `sf` package:
> >>>>>
> >>>>> library(sf)
> >>>>> library(spdep)
> >>>>>
> >>>>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))  # North
> >>>>> Carolina counties
> >>>>> nc_shp <- as(nc,'Spatial')
> >>>>>
> >>>>> coords <- coordinates(nc_shp)
> >>>>> IDs<-row.names(as(nc_shp, "data.frame"))
> >>>>>
> >>>>> knn5 <- knn2nb(knearneigh(coords, k = 5), row.names = IDs)  # find
> the
> >>>>> nearest neighbors for each county
> >>>>> knn5 <- include.self(knn5)
> >>>>>
> >>>>> localGvalues <- localG(x = as.numeric(nc_shp at data$NWBIR74), listw =
> >>>>> nb2listw(knn5, style = "B"), zero.policy = TRUE) # calculate the G
> >> scores
> >>>>> localGvalues <- round(localGvalues,3)
> >>>>>
> >>>>> nc_shp at data$LOCAL_G <- as.numeric(localGvalues)
> >>>>>
> >>>>> p1 <- spplot(nc_shp, c('NWBIR74'))
> >>>>> p2 <- spplot(nc_shp, c('LOCAL_G'))
> >>>>> plot(p1, split=c(1,1,2,2), more=TRUE)
> >>>>> plot(p2, split=c(1,2,2,2), more=TRUE)
> >>>>>
> >>>>> Here?s what I imagine that would look like in a tidyverse pipeline
> >>>> (please
> >>>>> note that this code is for illustrative purposes and will not run):
> >>>>>
> >>>>> library(tidyverse)
> >>>>> library(purrr)
> >>>>> library(sf)
> >>>>> library(sfdep) # this package doesn't exist (yet)
> >>>>>
> >>>>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))
> >>>>>
> >>>>> nc_g <-
> >>>>>  nc %>%
> >>>>>  mutate(KNN = map(.x = geometry, ~ sfdep::st_knn(.x, k = 5,
> >> include.self
> >>>> =
> >>>>> TRUE)),  # find the nearest neighbors for each county
> >>>>>         NB_LIST = map(.x = KNN, ~ sfdep::st_nb_list(.x, style =
> >> 'B')),  #
> >>>>> make a list of the neighbors using the binary method
> >>>>>         LOCAL_G = sfdep::st_localG(x = NWBIR74, listw = NB_LIST,
> >>>>> zero.policy = TRUE),  # calculate the G scores
> >>>>>         LOCAL_G = round(LOCAL_G,3))
> >>>>>
> >>>>> We can see that the (hypothetical) tidyverse version reduces the
> amount
> >>>> of
> >>>>> intermediate objects and wraps the creation of the G scores into a
> >> single
> >>>>> code chunk with clear steps.
> >>>>>
> >>>>> I'd be grateful to hear from the users and developers of the `spdep`
> >> and
> >>>>> `sf` packages about this topic!
> >>>>>
> >>>>> Tiernan Martin
> >>>>>
> >>>>>       [[alternative HTML version deleted]]
> >>>>>
> >>>>> _______________________________________________
> >>>>> R-sig-Geo mailing list
> >>>>> R-sig-Geo at r-project.org
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>>>
> >>>> --
> >>>> Roger Bivand
> >>>> Department of Economics, Norwegian School of Economics,
> >>>> Helleveien 30, N-5045 Bergen, Norway.
> >>>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>
> <+47%2055%2095%2093%2055>
> >> <+47%2055%2095%2093%2055>; e-mail:
> >>>> Roger.Bivand at nhh.no
> >>>> Editor-in-Chief of The R Journal,
> >> https://journal.r-project.org/index.html
> >>>> http://orcid.org/0000-0003-2392-6140
> >>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> >>>
> >>
> >> --
> >> Roger Bivand
> >> Department of Economics, Norwegian School of Economics,
> >> Helleveien 30, N-5045 Bergen, Norway.
> >> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>
> <+47%2055%2095%2093%2055>; e-mail:
> >> Roger.Bivand at nhh.no
> >> Editor-in-Chief of The R Journal,
> https://journal.r-project.org/index.html
> >> http://orcid.org/0000-0003-2392-6140
> >> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
> Roger.Bivand at nhh.no
> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From michael.koehler at nw-fva.de  Wed May 17 10:21:26 2017
From: michael.koehler at nw-fva.de (Michael Koehler)
Date: Wed, 17 May 2017 10:21:26 +0200
Subject: [R-sig-Geo] Cannot correctly assign Data type of upper left cell in
 ascii grid with writeRaster
Message-ID: <96c97825-7f3a-627e-c677-8bb03eaf4b77@nw-fva.de>

Dear list,

  I encountered a problem when trying to write an Ascii-grid of integers 
with raster::writeRaster.
When using that function, the upper left cell (only that cell!) in the 
resulting ascii grid will always
  be a floating point number, even if I convert the whole raster to 
integers before.

  However, when I convert the raster to a SpatialPixelDataFrame and 
write the Ascii file with
  maptools::writeAsciiGrid, the resulting upper left cell will be 
correctly saved as an integer.
So Is this a bug or am I doing something wrong here?

  Here is a reproducible example:

library(raster)
library(maptools)
library(SDMTools)
library(adehabitat)

# create raster

m <- matrix(c(-999,-999,-999,-999,-999,-999.999,-999.9,-999,1,6,
               -999,-999,-999,7,1,6,7,2,6,3,
               4,7,3,4,5,3,7,9,3,8,
               9,3,6,8,3,4,7,3,7,8,
               3,3,-999.999,7,5,3,2,8,9,8,
               7,6,2,6,5,2,2,7,7,7,
               4,7,2,5,7,7,7,3,3,5,
               7,6,7,5,-999,6,5,2,3,2,
               4,9,2,5,5,8,3,3,1,2,
               5,2,6,5,1,5,3,7,7,2),nrow=10, ncol=10, byrow = T)

r <- raster(m)
values(r)<-as.integer(values(r))

#------------------------------------------------------
# write the raster and read the resulting file in again
writeRaster(r, "Test_writeRaster.asc",NAflag =-999,overwrite=T)
a<-read.table("Test_writeRaster.asc",stringsAsFactors = F)

a[7,] # upper left cell in the ascii file is a floating point number!

#------------------------------------------------------
# convert, write with writeAsciiGrid and read the resulting file in again
asc <- asc.from.raster(r) # convert to asc
rasc <- asc2spixdf(asc)   # convert to SpatialPixelDataFrame

writeAsciiGrid(rasc,"Test_writeAsciiGrid.asc", na.value =-999)
a<-read.table("Test_writeAsciiGrid.asc",stringsAsFactors = F)

a[7,] # upper left cell in the ascii file is a

# R version 3.2.5 (2016-04-14)
# Platform: i386-w64-mingw32/i386 (32-bit)
# Running under: Windows 7 x64 (build 7601) Service Pack 1
#
# locale:
# [1] LC_COLLATE=English_United States.1252 LC_CTYPE=English_United 
States.1252
# [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
# [5] LC_TIME=English_United States.1252
#
# attached base packages:
# [1] stats     graphics  grDevices utils     datasets  methods base
#
# loaded via a namespace (and not attached):
# [1] tools_3.2.5

-- 
Michael K?hler, PhD


From b.graeler at 52north.org  Wed May 17 12:10:28 2017
From: b.graeler at 52north.org (=?UTF-8?Q?Dr._Benedikt_Gr=c3=a4ler?=)
Date: Wed, 17 May 2017 12:10:28 +0200
Subject: [R-sig-Geo] Appropriate variogram model
In-Reply-To: <770069364.6168148.1494218008878@mail.yahoo.com>
References: <770069364.6168148.1494218008878.ref@mail.yahoo.com>
 <770069364.6168148.1494218008878@mail.yahoo.com>
Message-ID: <a8dbf504-ff10-90b7-73ce-2d6424d54acd@52north.org>

Dear Ankur,

I see two issues from your plot:
i) there seems to be a seasonality in your data (maybe daily?) as the 
sample variogram grows up to half the length of your temporal axis and 
then drops again
ii) the starting values of your fitting routine are far off from what 
can be deduced from the sample variogram. Hence, the numerical routines 
have a hard time to identify the correct values. Additionally, the 
parameters are of different orders of magnitude further complicating the 
numerical fit. Manual re-scaling might help.

Try adding the parameter "scales=list(arrows=F)" for a visual assessment 
of the empirical variogram; in your case

plot(var, wireframe=T, scales=list(arrows=F))

This reveals that the variogram indicates the same strength of 
dependence for values 24 and 0 hours apart -> daily cycle? I'd suggest 
to model the daily cycle first and then to model the spatio-temporal 
variogram of the residuals.

HTH,

  Ben



On 08/05/2017 06:33, Ankur Sarker via R-sig-Geo wrote:
> Hi,
> 
> Can anyone suggest me the most appropriate variogram model to fit my data?
> 
> I have tried three different models and results are too bad. Here is the 
> comparison of different variogram models as attached.
> 
> Thanks,
> Ankur
> 
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Dr. Benedikt Gr?ler
52?North Initiative for Geospatial Open Source Software GmbH
Martin-Luther-King-Weg 24
48155 Muenster, Germany

E-Mail: b.graeler at 52north.org
Fon: +49-(0)-251/396371-39
Fax: +49-(0)-251/396371-11

http://52north.org/
Twitter: @FiveTwoN

General Managers: Dr. Albert Remke, Dr. Andreas Wytzisk
Local Court Muenster HRB 10849
-------------- next part --------------
A non-text attachment was scrubbed...
Name: b_graeler.vcf
Type: text/x-vcard
Size: 422 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170517/c2549081/attachment.vcf>

From edzer.pebesma at uni-muenster.de  Wed May 17 12:41:06 2017
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 17 May 2017 12:41:06 +0200
Subject: [R-sig-Geo] Is a simple feature -friendly version of spdep
 being developed?
In-Reply-To: <CAAcGz9_PwYcJCoz5A-1LQkAzG8YEOa8VLgGMYw6T3+7DB986iA@mail.gmail.com>
References: <CAEo23y5FMrUTdzTruh9QQjv5VD99=2nyGtX-PoZCiz3J2GYWmg@mail.gmail.com>
 <alpine.LFD.2.20.1705132138000.2107@reclus.nhh.no>
 <CAEo23y6CVLU9NKV0zDWW4vYjyRU8Z8u6xR0UyaS_Eg+S4pWsUg@mail.gmail.com>
 <alpine.LFD.2.20.1705151357190.2983@reclus.nhh.no>
 <CAEo23y5H92BTeSxB13_vU5MnXo_y9xVpr143zWAuUHXMkbvGLQ@mail.gmail.com>
 <alpine.LFD.2.20.1705161004070.28163@reclus.nhh.no>
 <CAAcGz9_PwYcJCoz5A-1LQkAzG8YEOa8VLgGMYw6T3+7DB986iA@mail.gmail.com>
Message-ID: <1d982349-f5c4-1842-14d4-44ccc3b5729e@uni-muenster.de>

Interesting discussion, thanks for bringing this up here. Some ideas:

I agree that there's more than one answer to the question what is tidy
in the context of analysing spatial data. Compared to the S4 classes in
sp, with the simplicity of S3 and data.frame's, there's also a certain
risk/loss, two examples came up the last few days in [1] and [2].

I don't think that the risks of adding neighbourhood lists or sparse
weights matrices to sf/data.frame objects are really large - right now
I'd guess (untried!) it is also possible to create nb lists, then subset
data and this list or sparse matrix, and end up with a meaningless mess
relatively unnoticed (some luck needed).

It might be a good idea to give these lists a certain class (right now
`st_intersects` or `st_rook` return an unclassed list), and modify the
`filter.sf` method such that it will warn when these objects are
subsetted - in which case they would loose their meaning.

I'm not exactly sure how to pack a sparse weights matrix in a
list-column, but that can't be hard; somehow I feel that sparse matrices
from Matrix would not fit in, since they're not lists - convert on the fly?

[1] https://github.com/edzer/sfr/issues/343
[2] https://github.com/edzer/sfr/issues/340

On 16/05/17 14:27, Michael Sumner wrote:
> Hello, I'm following this conversation with great interest. There's a lot
> here for me
> to work with, it's an active pursuit and I don't have much to offer yet but
> I have a couple of inline responses below.  I've learnt a lot about spdep
> from this discussion, something I've meant to explore for a long time
> but my work has never had the same modelling focus).
> 
> On Tue, 16 May 2017 at 18:25 Roger Bivand <Roger.Bivand at nhh.no> wrote:
> 
>> On Tue, 16 May 2017, Tiernan Martin wrote:
>>
>>>> Should the weights be
>>>> squirreled away inside the object flowing down the pipe?
>>>
>>> I'd propose that the that functions be written in such a way that the
>>> neighbors, weights, and test results can be stored in list cols within
>> the
>>> data.frame.
>>>
>>> This way multiple tests (and their inputs) can be stored in a single,
>>> easily comprehensible, rectangular data object.
>>>
>>> Here's some more pretend code to illustrate that concept with regard to
>> sf:
>>>
>>>  nc %>%
>>>    group_by(NAME) %>%
>>>    nest %>%
>>>    mutate(
>>>           geometry = map(.x = data, ~.x[['geometry']]),
>>>           neighbors = map(.x = data, .f = my_nb), # creates a list col of
>>> the neighbors
>>>           weights = map(.x = data, .f = add_weights), # creates a weights
>>> object (class=wt, is this a sparse matrix in spdep?)
>>>           test_moran = map2(.x = data, .y =  weights, .f = my_moran), #
>>> creates list of Moran's I and sample kurtosis of x
>>>           test_geary = map2(.x = data, .y = weights, .f = my_geary) #
>>> creates list of Geary's C and sample kurtosis of x
>>>               )
>>>
>>> #> # A tibble: 100 x 7
>>> #>           NAME              data         geometry  neighbors  weights
>>> test_moran test_geary
>>> #>        <fctr>            <list>           <list>     <list>   <list>
>>> <list>     <list>
>>> #>  1        Ashe <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #>  2   Alleghany <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #>  3       Surry <tibble [1 x 14]> <simple_feature> <list [4]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #>  4   Currituck <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #>  5 Northampton <tibble [1 x 14]> <simple_feature> <list [3]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #>  6    Hertford <tibble [1 x 14]> <simple_feature> <list [6]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #>  7      Camden <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #>  8       Gates <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #>  9      Warren <tibble [1 x 14]> <simple_feature> <list [5]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #> 10      Stokes <tibble [1 x 14]> <simple_feature> <list [2]> <S3: wt>
>>> <list [2]> <list [2]>
>>> #> # ... with 90 more rows
>>>
>>> Examples of this nested data.frame / list col approach can be found here:
>>> http://r4ds.had.co.nz/many-models.html#nested-data
>>
>> Thanks, interesting but really scary. You get numeric output for local
>> Moran's I and local Geary, but what will happen next?
>>
>> All of these tests assume a correctly specified mean model. Without this
>> assumption being met (absence of trends, absence of global spatial
>> autocorrelation in the local test case, absence of missing right-hand-side
>> variables and absence of incorrect functional forms for these, ...), a map
>> pretending to invite "inference" of "hotspots" etc. - collections of
>> observations with similar values of the local statistic, will most often
>> be highly misleading. This isn't only about whether it is possible to do
>> technically, but also about not making it simpler than reason would
>> suggest. With a correct mean model, there may not be any (local) spatial
>> autocorrelation left. See ?localmoran.sad and ?localmoran.exact, and their
>> references, and McMillen (2003) (the McSpatial author). I won't even
>> mention the need to adjust p.values for multiple comparisons ...
>>
>>>
>>>> Extending this to adding weights to sf objects may be going too far. It
>>>> seems slightly forced to - say - attach a sparse matrix to an sf
>> geometry
>>>> column as an attribute, or to add a neighbour nested list column when
>>>> writing subsetting protection is very far from obvious. Like time series
>>>> data (not time stamped observations, but real ordered time series),
>>>> spatial data have implicit order that is tidy in a different way than
>> tidy
>>>> (graph dependencies between observation rows). Maybe mapped features
>> with
>>>> attached attribute data are "tidier" than a table, because they would
>>>> preserve their relative positions?
>>>
>>> This seems like the crux of the problem. Are you saying that the
>> orthogonal
>>> "tidyness" of the tidyverse is fundamentally different from the
>> underlying
>>> relationships of spatial data, and therefore there's a limit to the
>>> usefulness of tidyverse approach to spatial analysis?
>>
>> Right, this is central and open. Spatial objects can be tidy in the
>> tidyverse sense - sf shows this can be done. The open question is whether
>> this extends to the analysis of the data.
> 
> 
> 
> I think the structures are still wide open, unless you think the goals of sf
> encompass everything. The simple features standard is one corner of the
> available ways to structure spatial data. It pointedly ignores relations
> between objects, and heavily relies on 2D-specific optimizations. All
> the topology is done "on the fly" and is treated as expendable. I personally
> want structures that treat simple features and similar formats as
> expendable,
> but that maintain the topology as a first-class citizen. I don't say this
> because I think simple features is not important, it's just not my
> main focus. I want time, depth, temperature, salinity, anything and
> everything
> storable on features, parts, edges, vertices and nodes. Lots of tools
> do this or aspects of it, and none of them can be shoe-horned into simple
> features, and
> they don't have a unifying framework. (It's called "mesh structures", or
> the simplicial
> complex, or "a database" in other contexts, but none of those are really
> enough).
> 
>  (I'm dismayed by GIS ideas when they are spoken about as if it's the end
> of the story. Sf is really great, but it's now more than the standard and
> it's kind
> of unique in the available tools for the standard itself - but most of them
> are pretty specialized and different to each other, probably unavoidably).
> 
> ggplot2 and tidygraph still have a long way to go, but in there I believe
> is a language not just of graphics and of converting to and from igraph,
> but of flexible ways of specifying how spatial data are constructed,
> analysed
> and interacted with.
> Ggplot builds geometries, but it has no output other than a plot, and the
> object that can
> modify that plot. Select, group-by, arrange, filter, nest, and normalization
> into multiple tables are all part of the powerful ways in which raw data
> can be given structure. "Spatial" is just one small part of that structure,
> and
> unfortunately the 'geographic spatial', the 2D optimizations baked in
> by decades old GIS practice seems to have the most sway in discussions.
> 
> This community could find a way to bake-in the geometries from ggplot
> constructions as sf objects, i.e. convert gg-plot into sf, not into a plot.
> I think that's a
> digestable project that would really provide some valuable insights.
> 
> 
>> From looking around, it seems
>> that time series and graphs suffer from the same kinds of questions, and
>> it will be interesting to see what stars turns up for spatio-temporal
>> data:
>>
>> https://github.com/edzer/stars
>>
>> I see that stars wisely does not include trip/trajectory data structures,
>> but analysis might involve reading the array-based representation of
>> spatio-temporal environmental drivers for an ensemble of buffers around
>> trajectories (say animal movement), then modelling.
>>
>>
> Interesting that you mention this, I agree it's a crux area in modern
> spatial,
> tracking has been neglected for years but it's starting to be looked at.
> 
>  I don't see this as terribly difficult, in fact we do this routinely
> for all kinds of tracking data both as-measured and modelled. The
> difficulty is having
> access to the data as well as the motivation in the same place. We have
> in-house tools
> that sit on large collections of remotely sensed time series data:
> 
> https://github.com/AustralianAntarcticDataCentre/raadsync
> 
> When you have both the motivation (lots of animal tracking and voyage data)
> and
> the data on hand it's relatively easy (we have a ready audience at in
> Southern Ocean
> ecosystems research here).  First step is to write a read-data tool as a
> function
> of time. Then the problem of matching tracking data to the right space-time
> window is easily broken down into the intervals between the time
> series of environmental data. At that level, you can employ interpolation
> between the layers,
> aggregation to compromise resolution with coverage, and even
> bring user-defined calculations to the smallish windows to calculate
> derived products (rugosity, slope, distance to thresholds etc.)
> 
> In our modelled track outputs with tripEstimation/SGAT and bsam[1] we can
> use the more probabilistic
> estimates of location as more nuanced "query buckets" against the
> same environmental data, it's the same issue with matching time with
> all the different options you might want. What is hard for us is to share
> the tools,
> because first you need to arrange access to quite a lot of data, before
> you get to see the magic in action.
> 
> I do think that stars provides a great opportunity though, and my thoughts
> along
> those lines are here:
> 
> https://github.com/mdsumner/stars.use.cases/blob/master/Use-cases.rmd#animal-tracking-mcmc-estimation
> 
> 
> [1] https://CRAN.R-project.org/package=bsam
> 
> 
> 
>> Maybe: "everything should be as tidy as it can be but not tidier"?
>>
>>
> I don't think we've even started with the housekeeping here. Some early
> thoughts are here:
> 
> http://rpubs.com/cyclemumner/sc-rationale
> 
> I'm interested to help find ways to nest the spdep idioms in "tidy" ways,
> but I'm also not sure
> how valuable that is yet. When you convert the structures to primitives the
> edge and vertex relations
> are inherent already, and it's a mater of standard database
> join/select/filter idioms
> to link things up:
> 
> http://rpubs.com/cyclemumner/neighbours01
> 
> http://rpubs.com/cyclemumner/neighbours02
> 
> (those examples use https://github.com/mdsumner/scsf for the PRIMITIVE
> model
> but it's really too raw to use yet). This is all fine and dandy, but then
> you still have a list-bag
> - i.e. database  - of tables with no strong idioms for treating them as a
> single object -
> that's the real loftier goal  of tidygraph, it's certainly a clear goal of
> the tidyverse to be a
> general master of data).
> 
> Is this a serious option worth pursuing for spdep? I don't know, but I'm
> pursuing it for my own
> reasons as detailed above. I don't think modernizing spdep would be that
> difficult.  At the least the limitations and
> obstacles faced when using it  should be described.
> 
> I'm happy to help and I might have a go at it at some point, and I'd
> encourage
> anyone to get in and try it out.With rlang and the imminent release of the
> greatly
> improved dplyr it's a good time to start.
> 
> Cheers, Mike.
> 
> 
> 
>> Roger
>>
>>>
>>> On Mon, May 15, 2017 at 5:17 AM Roger Bivand <Roger.Bivand at nhh.no>
>> wrote:
>>>
>>>> On Sun, 14 May 2017, Tiernan Martin wrote:
>>>>
>>>>> You?re right about one thing: those of us introduced to the pipe in our
>>>>> formative years find it hard to escape (everything is better here
>> inside
>>>>> the pipe - right?) ;)
>>>>
>>>> Pipes can be great if they don't involve extra inputs at a later stage
>> in
>>>> the pipe, but as:
>>>>
>>>> https://github.com/thomasp85/tidygraph
>>>>
>>>> suggests, it isn't very easy, and may be forced.
>>>>
>>>>>
>>>>> Thank you for your very informative response. Before reading it, I had
>> a
>>>>> vague inkling that matrices were part of the challenge of adapting
>> these
>>>>> tools to work with simple features, but I also thought perhaps all this
>>>>> issue needed was a little nudge from a user like me. Your response made
>>>> it
>>>>> clear that there is a series of technical decisions that need to be
>> made,
>>>>> and in order to do that I imagine some exploratory testing is in order.
>>>> Of
>>>>> course, it is possible that the data.frame + list-col foundation is
>> just
>>>>> not well-suited to the needs of spatial weighting/testing tools, but I
>>>>> suspect that there is some valuable insight to be gained from exploring
>>>>> this possibility further. And as I stated in my first post, going
>>>> forward I
>>>>> expect more users will be interested in seeing this sort of integration
>>>>> happen.
>>>>
>>>> I think that there are two tasks - one to create neighbour objects from
>> sf
>>>> objects, then another to see whether the costs of putting the spatial
>>>> weights into a data.frame (or even more challenging, a tibble) to test
>> for
>>>> spatial autocorrelation or model spatial dependence. Should the weights
>> be
>>>> squirreled away inside the object flowing down the pipe?
>>>>
>>>> sf_object %>% add_weights(...) -> sf_object_with_weights
>>>> moran.test(sf_object_with_weights, ...)
>>>> geary.test(sf_object_with_weights, ...)
>>>>
>>>> or
>>>>
>>>> sf_object %>% add_weights(...) %>% moran.test(sf_object_with_weights,
>> ...)
>>>> -> output
>>>>
>>>> all in one (output is a htest object). It gets more complex in:
>>>>
>>>> lm.morantest(lm(formula, sf_object), sf_object_with_weights)
>>>>
>>>> as we need the weights and the lm output object.
>>>>
>>>>>
>>>>>> How should we go about this technically? spdep is on R-Forge and is
>>>> happy
>>>>>> there. Its present functionality has to be maintained as it stands, it
>>>>> has
>>>>>> too many reverse dependencies to break. Should it be re-written from
>>>>>> scratch (with more sparse matrices internally for computation)?
>>>>>> ...
>>>>>> We'd need proof of concept with realistically sized data sets (not yet
>>>> NY
>>>>>> taxis, but maybe later ...). spdep started as spweights, sptests and
>>>>>> spdep, and the first two got folded into the third when stable. If
>>>>> weights
>>>>>> are the first thing to go for, sfweights is where to go first (and
>> port
>>>>>> the STRtree envelope intersections for contiguities). It could define
>>>> the
>>>>>> new classes, and tests and modelling would use them.
>>>>>
>>>>> Sounds to me like this package would need to be built from the ground
>> up,
>>>>> possibly following a similar path to the `sp` development process as
>> you
>>>>> mentioned. Maybe that presents a challenge with regards to resources
>>>> (i.e.,
>>>>> funding, time, etc.). Perhaps project this is a candidate for future
>>>>> proposals to the R Consortium or other funding sources. I hope that
>> this
>>>>> post is useful in documenting user interest in seeing the `sf` protocol
>>>>> integrated into other spatial tools and workflows, and I look forward
>> to
>>>>> hearing others' thoughts on the matter.
>>>>
>>>> Making spatial weights from sf objects is just expanding spdep, and is
>>>> feasible. There is no point looking for funding, all it needs is
>> knowledge
>>>> of how things have been done in spdep. Contiguity, knn, distance,
>>>> graph-based neighbours are all feasible.
>>>>
>>>> Extending this to adding weights to sf objects may be going too far. It
>>>> seems slightly forced to - say - attach a sparse matrix to an sf
>> geometry
>>>> column as an attribute, or to add a neighbour nested list column when
>>>> writing subsetting protection is very far from obvious. Like time series
>>>> data (not time stamped observations, but real ordered time series),
>>>> spatial data have implicit order that is tidy in a different way than
>> tidy
>>>> (graph dependencies between observation rows). Maybe mapped features
>> with
>>>> attached attribute data are "tidier" than a table, because they would
>>>> preserve their relative positions?
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> Thanks again,
>>>>>
>>>>> Tiernan
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Sat, May 13, 2017 at 1:39 PM Roger Bivand <Roger.Bivand at nhh.no>
>>>> wrote:
>>>>>
>>>>>> On Fri, 12 May 2017, Tiernan Martin wrote:
>>>>>>
>>>>>>> Is anyone thinking about creating an adaptation of the `spdep`
>> package
>>>>>> that
>>>>>>> expects sf-class inputs and works well in a pipeline?
>>>>>>
>>>>>> I assume that "this is not a pipeline" is a joke for insiders (in the
>>>>>> pipe)?
>>>>>>
>>>>>> No, there is your issue on the sfr github repository that is relevant
>>>> for
>>>>>> contiguous neighbours, but not beyond that:
>>>>>>
>>>>>> https://github.com/edzer/sfr/issues/234
>>>>>>
>>>>>> An sf is a data.frame, and as such should "just work", like "Spatial"
>>>>>> objects have, in formula/data settings. The problem is (of course) the
>>>>>> weights matrix.
>>>>>>
>>>>>> Should it be a list column (each row has a list nesting two lists,
>> first
>>>>>> indices, second non-zero weights), or remain separate as it has been
>> for
>>>>>> 20 years, or become a column-oriented representation (Matrix package)
>> -
>>>> a
>>>>>> nested list like a list column or a listw obeject is row-oriented. I
>> had
>>>>>> started thinking about using a sparse column-oriented representation,
>>>> but
>>>>>> have not implemented functions accepting them instead of listw
>> objects.
>>>>>>
>>>>>> I am very cautious about creating classes for data that were
>> data.frame,
>>>>>> then sf, and then have the weights built-in. In the simple case it
>> would
>>>>>> work, but all you have to do is re-order the rows and the link between
>>>> the
>>>>>> neighbour ids and row order breaks down; the same applies to
>> subsetting.
>>>>>>
>>>>>> The problems to solve first are related the workflows, and easiest to
>>>> look
>>>>>> at in the univariate case (Moran, Geary, join-count, Mantel, G, ...)
>> for
>>>>>> global and local tests. I think that all or almost all of the NSE
>> verbs
>>>>>> will cause chaos (mutate, select, group) once weights have been
>> created.
>>>>>> If there is a way to bind the neighour IDs to the input sf object
>> rows,
>>>> a
>>>>>> list column might be possible, but we have to permit multiple such
>>>> columns
>>>>>> (Queen, Rook, ...), and ensure that subsetting and row-reordering keep
>>>> the
>>>>>> graph relations intact (and their modifications, like
>>>> row-standardisation)
>>>>>>
>>>>>> If you've seen any examples of how tidy relates to graphs (adjacency
>>>> lists
>>>>>> are like nb objects), we could learn from that.
>>>>>>
>>>>>> How should we go about this technically? spdep is on R-Forge and is
>>>> happy
>>>>>> there. Its present functionality has to be maintained as it stands, it
>>>> has
>>>>>> too many reverse dependencies to break. Should it be re-written from
>>>>>> scratch (with more sparse matrices internally for computation)?
>>>>>>
>>>>>> Your example creates weights on the fly for a local G* map, but has
>>>> n=100,
>>>>>> not say n=90000 (LA census blocks). Using the sf_ geos based methods
>>>> does
>>>>>> not use STRtrees, which cut the time needed to find contigious
>>>> neighbours
>>>>>> from days to seconds. We ought to pre-compute weights, but this messes
>>>> up
>>>>>> the flow of data, because a second lot of data (the weights) have to
>>>> enter
>>>>>> the pipe, and be matched with the row-ids.
>>>>>>
>>>>>> We'd need proof of concept with realistically sized data sets (not yet
>>>> NY
>>>>>> taxis, but maybe later ...). spdep started as spweights, sptests and
>>>>>> spdep, and the first two got folded into the third when stable. If
>>>> weights
>>>>>> are the first thing to go for, sfweights is where to go first (and
>> port
>>>>>> the STRtree envelope intersections for contiguities). It could define
>>>> the
>>>>>> new classes, and tests and modelling would use them.
>>>>>>
>>>>>> Thanks for starting this discussion.
>>>>>>
>>>>>> Roger
>>>>>>
>>>>>>>
>>>>>>> I understand that there is skepticism about the wisdom of adopting
>> the
>>>>>>> ?tidyverse? principles throughout the R package ecosystem, and I
>> share
>>>>>> the
>>>>>>> concern that an over-reliance on any single paradigm could reduce the
>>>>>>> resilience and diversity of the system as a whole.
>>>>>>>
>>>>>>> That said, I believe that the enthusiastic adoption of the `sf`
>> package
>>>>>> and
>>>>>>> the package's connections with widely-used tidyverse packages like
>>>>>> `dplyr`
>>>>>>> and `ggplot2` may result in increased demand for sf-friendly spatial
>>>>>>> analysis tools. As an amateur who recently started using R as my
>>>> primary
>>>>>>> GIS tool, it seems like the tidyverse's preference for dataframes, S3
>>>>>>> objects, list columns, and pipeline workflows would be well-suited to
>>>> the
>>>>>>> field of spatial analysis. Are there some fundamental reasons why the
>>>>>>> `spdep` tools cannot (or should not) be adapted to the tidyverse
>>>>>> "dialect"?
>>>>>>>
>>>>>>> Let me put the question in the context of an actual analysis: in
>>>> February
>>>>>>> 2017, the pop culture infovis website The Pudding (
>>>> https://pudding.cool/
>>>>>> )
>>>>>>> published an analysis of regional preferences for Oscar-nominated
>> films
>>>>>> in
>>>>>>> the US (https://pudding.cool/2017/02/oscars_so_mapped/). A few days
>>>> ago,
>>>>>>> the author posted a tutorial explaining the method of ?regional
>>>>>> smoothing?
>>>>>>> used to create the article?s choropleths (
>>>>>>> https://pudding.cool/process/regional_smoothing/).
>>>>>>>
>>>>>>> The method relies on several `spdep` functions (
>>>>>>>
>>>>>>
>>>>
>> https://github.com/polygraph-cool/smoothing_tutorial/blob/master/smoothing_tutorial.R
>>>>>> ).
>>>>>>> In the code below, I provide reprex with a smaller dataset included
>> in
>>>>>> the
>>>>>>> `sf` package:
>>>>>>>
>>>>>>> library(sf)
>>>>>>> library(spdep)
>>>>>>>
>>>>>>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))  # North
>>>>>>> Carolina counties
>>>>>>> nc_shp <- as(nc,'Spatial')
>>>>>>>
>>>>>>> coords <- coordinates(nc_shp)
>>>>>>> IDs<-row.names(as(nc_shp, "data.frame"))
>>>>>>>
>>>>>>> knn5 <- knn2nb(knearneigh(coords, k = 5), row.names = IDs)  # find
>> the
>>>>>>> nearest neighbors for each county
>>>>>>> knn5 <- include.self(knn5)
>>>>>>>
>>>>>>> localGvalues <- localG(x = as.numeric(nc_shp at data$NWBIR74), listw =
>>>>>>> nb2listw(knn5, style = "B"), zero.policy = TRUE) # calculate the G
>>>> scores
>>>>>>> localGvalues <- round(localGvalues,3)
>>>>>>>
>>>>>>> nc_shp at data$LOCAL_G <- as.numeric(localGvalues)
>>>>>>>
>>>>>>> p1 <- spplot(nc_shp, c('NWBIR74'))
>>>>>>> p2 <- spplot(nc_shp, c('LOCAL_G'))
>>>>>>> plot(p1, split=c(1,1,2,2), more=TRUE)
>>>>>>> plot(p2, split=c(1,2,2,2), more=TRUE)
>>>>>>>
>>>>>>> Here?s what I imagine that would look like in a tidyverse pipeline
>>>>>> (please
>>>>>>> note that this code is for illustrative purposes and will not run):
>>>>>>>
>>>>>>> library(tidyverse)
>>>>>>> library(purrr)
>>>>>>> library(sf)
>>>>>>> library(sfdep) # this package doesn't exist (yet)
>>>>>>>
>>>>>>> nc <- st_read(system.file("shape/nc.shp", package = "sf"))
>>>>>>>
>>>>>>> nc_g <-
>>>>>>>  nc %>%
>>>>>>>  mutate(KNN = map(.x = geometry, ~ sfdep::st_knn(.x, k = 5,
>>>> include.self
>>>>>> =
>>>>>>> TRUE)),  # find the nearest neighbors for each county
>>>>>>>         NB_LIST = map(.x = KNN, ~ sfdep::st_nb_list(.x, style =
>>>> 'B')),  #
>>>>>>> make a list of the neighbors using the binary method
>>>>>>>         LOCAL_G = sfdep::st_localG(x = NWBIR74, listw = NB_LIST,
>>>>>>> zero.policy = TRUE),  # calculate the G scores
>>>>>>>         LOCAL_G = round(LOCAL_G,3))
>>>>>>>
>>>>>>> We can see that the (hypothetical) tidyverse version reduces the
>> amount
>>>>>> of
>>>>>>> intermediate objects and wraps the creation of the G scores into a
>>>> single
>>>>>>> code chunk with clear steps.
>>>>>>>
>>>>>>> I'd be grateful to hear from the users and developers of the `spdep`
>>>> and
>>>>>>> `sf` packages about this topic!
>>>>>>>
>>>>>>> Tiernan Martin
>>>>>>>
>>>>>>>       [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-Geo mailing list
>>>>>>> R-sig-Geo at r-project.org
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>>
>>>>>> --
>>>>>> Roger Bivand
>>>>>> Department of Economics, Norwegian School of Economics,
>>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>
>> <+47%2055%2095%2093%2055>
>>>> <+47%2055%2095%2093%2055>; e-mail:
>>>>>> Roger.Bivand at nhh.no
>>>>>> Editor-in-Chief of The R Journal,
>>>> https://journal.r-project.org/index.html
>>>>>> http://orcid.org/0000-0003-2392-6140
>>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>
>> <+47%2055%2095%2093%2055>; e-mail:
>>>> Roger.Bivand at nhh.no
>>>> Editor-in-Chief of The R Journal,
>> https://journal.r-project.org/index.html
>>>> http://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
>> Roger.Bivand at nhh.no
>> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
>> http://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170517/3ff0d738/attachment.sig>

From abm.firoz at th-koeln.de  Wed May 17 16:04:35 2017
From: abm.firoz at th-koeln.de (ABM Firoz)
Date: Wed, 17 May 2017 16:04:35 +0200
Subject: [R-sig-Geo] Nested Variogram Model for 3D data using gstat
Message-ID: <000201d2cf16$84834030$8d89c090$@th-koeln.de>

Dear All,

 

I am facing problem to obtain the nested variogram model for the 3D
lithological data. I have 3D data, in which the coordinate are x,y,z (z
corresponds to the depth) and the lithofacie information for different
materials as a binary values (Ind01, Ind02, Ind03, Ind04, Ind05) . Now I
would like to develop sequential indicator simulation to obtain the facies
distribution for each of the category. I have used gstat to develop the
vertical and horizontal varigram but unable to fit as a nested variogram for
both vertical and horizontal direction. Is there any one help me to combine
the vertical and horizontal variogram in one nested structure. 

The data I have used from the greengoblin data. A dropbox link has been
placed at the end of this post to obtain the data.

 

The code I have used as follows to obtain the vertical and horizontal
variogram for the Ind03 facies (medium sand)

green_gstat <- as.data.frame(green_goblin)

coordinates(green_gstat) = ~X+Y+Z

names (green_gstat)

class(green_gstat) 

# vertical variogram 

ind03_vert <- variogram((Ind03)~X+Y+Z, green_gstat, cutoff = 250, width= 2,
alpha= 0, beta = 90)

# fitting the variogram model

ind03_fit <- fit.variogram (ind03_vert, vgm(1, "Exp", 500,1))

# plotting

plot(ind03_vert, ind03_fit)



# estimating the horizontal variogram in omni direction

Ind03_hori_omni <- variogram(ind03, cutoff = 15000, width = 1000, alpha = 0)

# fiting variogram mdoel

ind03_fit_dir <- vgm(.188, "Exp", 1500, .05, anis = c (45, .25))

# obtain the fit model informaiotn 

Ind03_fit_vrio_hor <- fit.variogram(Ind03_hori_omni,vgm(.188, "Exp", 1500,
.05, anis = c (45, .25)))

# plotting the data

plot(Ind03_hori_omni,ind03_fit_dir)



# now I have tried to go for the nested variogram approach, typically what I
have found there, that one can create the nested model like this from the
same variogram, but using diffent fitting options. 

ind03_vertnested <- fit.variogram (ind03_vert, vgm (1, "Exp", 500, add.to =
vgm (.188, "Exp", .05, anis = c(45, 0.25))))

here I have only used the ind03_vert (variogram), but I also have a
horizontal variogram model (Ind03_hori_omni). So, how I can incorporate two
model into a single variogram model. A feedback in this regard would be
highly appreciate. Or, is there any other option to create a 3D variogram
modeling. 

 

To have the data, please follow this dropbox link for the easy access. 

 

Link to the data-
https://www.dropbox.com/s/6xl8zh7ofntlna6/GreenGoblin.csv?dl=0

 

 

 

 

--

A.B.M Firoz
Researcher- GIS & Hydrological Modeling

 

ITT- Institute for Technology and Resources Management 

in the Tropics and Subtropics

Technology Arts Sciences
TH K?ln - University of Applied Sciences 

 

T:  +49 221 8275-2059

F: +49 221 8275-2736
E :  <mailto:abm.firoz at fh-koeln.de> abm.firoz at fh-koeln.de 

 

Kalk- Campus
RobertStrasse  2
51105 K?ln Germany

 <http://www.tt.fh-koeln.de/> www.tt.fh-koeln.de
 <http://www.th-koeln.de/> www.th-koeln.de

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170517/c569c5d5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.jpg
Type: image/jpeg
Size: 12079 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170517/c569c5d5/attachment.jpg>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 11162 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170517/c569c5d5/attachment-0001.jpg>

From loic.dutrieux at conabio.gob.mx  Wed May 17 16:34:32 2017
From: loic.dutrieux at conabio.gob.mx (=?UTF-8?Q?Lo=c3=afc_Dutrieux?=)
Date: Wed, 17 May 2017 09:34:32 -0500
Subject: [R-sig-Geo] Cannot correctly assign Data type of upper left
 cell in ascii grid with writeRaster
In-Reply-To: <96c97825-7f3a-627e-c677-8bb03eaf4b77@nw-fva.de>
References: <96c97825-7f3a-627e-c677-8bb03eaf4b77@nw-fva.de>
Message-ID: <fbabbf0a-cf88-f77a-78b5-294d44e79282@conabio.gob.mx>

writeRaster() defaults to Float 8, regardless of the data type of your
input matrix. You can set it explicitly to write to integer by using the
datatype= argument.

writeRaster(r, "Test_writeRaster.asc",NAflag =-999,overwrite=T,
datatype='INT2S')

Cheers,
Lo?c

On 17/05/2017 03:21, Michael Koehler wrote:
> Dear list,
> 
>  I encountered a problem when trying to write an Ascii-grid of integers
> with raster::writeRaster.
> When using that function, the upper left cell (only that cell!) in the
> resulting ascii grid will always
>  be a floating point number, even if I convert the whole raster to
> integers before.
> 
>  However, when I convert the raster to a SpatialPixelDataFrame and write
> the Ascii file with
>  maptools::writeAsciiGrid, the resulting upper left cell will be
> correctly saved as an integer.
> So Is this a bug or am I doing something wrong here?
> 
>  Here is a reproducible example:
> 
> library(raster)
> library(maptools)
> library(SDMTools)
> library(adehabitat)
> 
> # create raster
> 
> m <- matrix(c(-999,-999,-999,-999,-999,-999.999,-999.9,-999,1,6,
>               -999,-999,-999,7,1,6,7,2,6,3,
>               4,7,3,4,5,3,7,9,3,8,
>               9,3,6,8,3,4,7,3,7,8,
>               3,3,-999.999,7,5,3,2,8,9,8,
>               7,6,2,6,5,2,2,7,7,7,
>               4,7,2,5,7,7,7,3,3,5,
>               7,6,7,5,-999,6,5,2,3,2,
>               4,9,2,5,5,8,3,3,1,2,
>               5,2,6,5,1,5,3,7,7,2),nrow=10, ncol=10, byrow = T)
> 
> r <- raster(m)
> values(r)<-as.integer(values(r))
> 
> #------------------------------------------------------
> # write the raster and read the resulting file in again
> writeRaster(r, "Test_writeRaster.asc",NAflag =-999,overwrite=T)
> a<-read.table("Test_writeRaster.asc",stringsAsFactors = F)
> 
> a[7,] # upper left cell in the ascii file is a floating point number!
> 
> #------------------------------------------------------
> # convert, write with writeAsciiGrid and read the resulting file in again
> asc <- asc.from.raster(r) # convert to asc
> rasc <- asc2spixdf(asc)   # convert to SpatialPixelDataFrame
> 
> writeAsciiGrid(rasc,"Test_writeAsciiGrid.asc", na.value =-999)
> a<-read.table("Test_writeAsciiGrid.asc",stringsAsFactors = F)
> 
> a[7,] # upper left cell in the ascii file is a
> 
> # R version 3.2.5 (2016-04-14)
> # Platform: i386-w64-mingw32/i386 (32-bit)
> # Running under: Windows 7 x64 (build 7601) Service Pack 1
> #
> # locale:
> # [1] LC_COLLATE=English_United States.1252 LC_CTYPE=English_United
> States.1252
> # [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
> # [5] LC_TIME=English_United States.1252
> #
> # attached base packages:
> # [1] stats     graphics  grDevices utils     datasets  methods base
> #
> # loaded via a namespace (and not attached):
> # [1] tools_3.2.5
>


From thi_veloso at yahoo.com.br  Wed May 17 19:25:13 2017
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Wed, 17 May 2017 17:25:13 +0000 (UTC)
Subject: [R-sig-Geo] Specify color for "zero" raster values using levelplot
References: <1249795416.1781620.1495041913673.ref@mail.yahoo.com>
Message-ID: <1249795416.1781620.1495041913673@mail.yahoo.com>

Dear all,

I am trying to change the color for zero values in a map produced using levelplot to plot a raster file. Specifically, I want to reproduce this figure: http://i.imgur.com/mjXxZhO.png, where a red to blue scale is used, but notice that zero values have been replaced by grey.

As an example, let's use an adapted version of the August irradiation code from the rasterVis webpage:

################################
library(raster)
library(ncdf4)
library(rasterVis)

##Solar irradiation data from CMSAF 
old <- setwd(tempdir())
download.file('https://raw.github.com/oscarperpinan/spacetime-vis/master/data/SISmm2008_CMSAF.zip',
'SISmm2008_CMSAF.zip', method='wget')
unzip('SISmm2008_CMSAF.zip')

listFich <- dir(pattern='\\.nc')
stackSIS <- stack(listFich)
stackSIS <- stackSIS * 24 ##from irradiance (W/m2) to irradiation Wh/m2
idx <- seq(as.Date('2008-01-15'), as.Date('2008-12-15'), 'month')

SISmm <- setZ(stackSIS, idx)
names(SISmm) <- month.abb

setwd(old)

# Set color palette
myTheme=rasterTheme(region=brewer.pal('RdBu', n=11))

Aug <- raster(SISmm, 8)
meanAug <- cellStats(Aug, mean)
levelplot(Aug - meanAug, par.settings = myTheme, margin=FALSE)

################################

In the example above, how can I replace the color of "zero values" with grey?
 
Thanks,
 -- Thiago V. dos Santos

PhD student
Land and Atmospheric Science
University of Minnesota


From sarosama at feps.edu.eg  Wed May 17 20:56:02 2017
From: sarosama at feps.edu.eg (sara osama)
Date: Wed, 17 May 2017 21:56:02 +0300
Subject: [R-sig-Geo] units for spatial range in stvariogram gstat
Message-ID: <CAJsLiQRP+zTS-2W9dmfHAkSspe9OwDu21C_0MMyejJaqxxXoOQ@mail.gmail.com>

Dear All,
I have question concerning the unit of measurement for the range parameter
in the fit.stvariogram in gstat, i have fitted the following spatio
temporal variogram for data on monthly averages pf pm10 and the longitude
and latitude are in decimel format

> pm10_sp
SpatialPoints:
     longitude latitude
1     30.92500 29.95300
61    31.36000 30.26000
121   31.33300 29.84720
 and this is the commands for fitting the variogram
> varpm10 <-
variogramST(lnpm10only~1,data=lnpm10_stfdf,cutoff=0.8,assumeRegular=T,na.omit=T)
> plot(varpm10,map=F)
> sumMetric <- vgmST("sumMetric", space = vgm(psill=5,"Mat", range=50,
nugget=0),time = vgm(psill=500,"Mat", range=50, nugget=0,kappa=0.3), joint
= vgm(psill=10,"Mat", range=100, nugget=1), stAni=200)
> sumMetric_Vgm <- fit.StVariogram(varpm10, sumMetric,
method="L-BFGS-B",lower=pars.l,upper=pars.u)
> plot(varpm10,sumMetric_Vgm,map=F)

> extractPar(sumMetric_Vgm)
      sill.s      range.s     nugget.s       sill.t      range.t
nugget.t
  1.73598444  50.63845035   0.03212857   0.01570379  49.99811448
0.00000000
     sill.st     range.st    nugget.st         anis
  0.00000000 100.00004052   0.09151787 200.00000000

-- 
now i want to know the units for these parameters in order to be able to
interpret it for example is it in km or m.

I really appreciate any help that I deeply need

Best Regards
Sara

Faculty of Economics & Political Science
Cairo University

Tel:(202)35728055-(202)35728116-(202)35736608-(202)35736605
Fax:(202)35711020
Follow us on twitter:https://twitter.com/fepsnews
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170517/35f109d4/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: variogram.png
Type: image/png
Size: 13032 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170517/35f109d4/attachment.png>

From edzer.pebesma at uni-muenster.de  Wed May 17 21:08:44 2017
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 17 May 2017 21:08:44 +0200
Subject: [R-sig-Geo] units for spatial range in stvariogram gstat
In-Reply-To: <CAJsLiQRP+zTS-2W9dmfHAkSspe9OwDu21C_0MMyejJaqxxXoOQ@mail.gmail.com>
References: <CAJsLiQRP+zTS-2W9dmfHAkSspe9OwDu21C_0MMyejJaqxxXoOQ@mail.gmail.com>
Message-ID: <b9cf1e86-2a41-5ef4-d697-a27390db5475@uni-muenster.de>

Your email is not nearly enough for a reader to figure out what you did.
However, if your data are long/lat, and you instructed gstat about that
(i.e., proj4string(pm10_sp) indicates coordinates are degrees) then
gstat computes and returns distances in km.

On 17/05/17 20:56, sara osama wrote:
> 
> Dear All,
> I have question concerning the unit of measurement for the range
> parameter in the fit.stvariogram in gstat, i have fitted the following
> spatio temporal variogram for data on monthly averages pf pm10 and the
> longitude and latitude are in decimel format
>  
>> pm10_sp
> SpatialPoints:
>      longitude latitude
> 1     30.92500 29.95300
> 61    31.36000 30.26000
> 121   31.33300 29.84720
>  and this is the commands for fitting the variogram
>> varpm10 <- variogramST(lnpm10only~1,data=lnpm10_stfdf,cutoff=0.8,assumeRegular=T,na.omit=T)
>> plot(varpm10,map=F)
>> sumMetric <- vgmST("sumMetric", space = vgm(psill=5,"Mat", range=50, nugget=0),time = vgm(psill=500,"Mat", range=50, nugget=0,kappa=0.3), joint = vgm(psill=10,"Mat", range=100, nugget=1), stAni=200)
>> sumMetric_Vgm <- fit.StVariogram(varpm10, sumMetric, method="L-BFGS-B",lower=pars.l,upper=pars.u)
>> plot(varpm10,sumMetric_Vgm,map=F)
>  
>> extractPar(sumMetric_Vgm)
>       sill.s      range.s     nugget.s       sill.t      range.t    
> nugget.t
>   1.73598444  50.63845035   0.03212857   0.01570379  49.99811448  
> 0.00000000
>      sill.st <http://sill.st>     range.st <http://range.st>   
> nugget.st <http://nugget.st>         anis
>   0.00000000 100.00004052   0.09151787 200.00000000
>  
> -- 
> now i want to know the units for these parameters in order to be able to
> interpret it for example is it in km or m.
>  
> I really appreciate any help that I deeply need
>  
> Best Regards
> Sara
> 
> 	Faculty of Economics & Political Science
> Cairo University
> 
> 
> Tel:(202)35728055-(202)35728116-(202)35736608-(202)35736605
> Fax:(202)35711020
> Follow us on twitter:https://twitter.com/fepsnews
> 
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170517/9818c946/attachment.sig>

From sarosama at feps.edu.eg  Wed May 17 21:40:43 2017
From: sarosama at feps.edu.eg (sara osama)
Date: Wed, 17 May 2017 22:40:43 +0300
Subject: [R-sig-Geo] units for spatial range in stvariogram gstat
In-Reply-To: <b9cf1e86-2a41-5ef4-d697-a27390db5475@uni-muenster.de>
References: <CAJsLiQRP+zTS-2W9dmfHAkSspe9OwDu21C_0MMyejJaqxxXoOQ@mail.gmail.com>
 <b9cf1e86-2a41-5ef4-d697-a27390db5475@uni-muenster.de>
Message-ID: <CAJsLiQS1Se6AeCUoT0CDQu0et+M-4hrnSCrqChATm9RpxY4xmA@mail.gmail.com>

Dear Prof. Pebesma,

Thanks for reply I will try to make it  more clear

the coordinates are converted to decimal degree format using the equation
DD = d + (min/60) + (sec/3600)

and then projected using

> llCRS <- CRS("+proj=longlat +ellps=WGS84")
> pm10_sp <- SpatialPoints(coords, proj4string = llCRS)
> pm10_sp

SpatialPoints:
     longitude latitude
1     30.92500 29.95300
61    31.36000 30.26000
 and the variogram is fitted using

>> varpm10 <- variogramST(lnpm10only~1,data=lnpm10_stfdf,cutoff=0.8,
assumeRegular=T,na.omit=T)

>> sumMetric <- vgmST("sumMetric", space = vgm(psill=5,"Mat", range=50,
nugget=0),time = vgm(psill=500,"Mat", range=50, nugget=0,kappa=0.3), joint
= vgm(psill=10,"Mat", range=100, nugget=1), stAni=200)

>> sumMetric_Vgm <- fit.StVariogram(varpm10, sumMetric,
method="L-BFGS-B",lower=pars.l,upper=pars.u)

*the s.range parameter extracted was 50.6 but if you take a look on  the
figure attached the largest distance on the x-axis is 0.6 ??????*

*so i think the unit on the figure is not the unit of the extracted
parameter. *

by the way the largest ditance between any points in my data set is
approximately 190 km


Thanks again




On Wed, May 17, 2017 at 10:08 PM, Edzer Pebesma <
edzer.pebesma at uni-muenster.de> wrote:

> Your email is not nearly enough for a reader to figure out what you did.
> However, if your data are long/lat, and you instructed gstat about that
> (i.e., proj4string(pm10_sp) indicates coordinates are degrees) then
> gstat computes and returns distances in km.
>
> On 17/05/17 20:56, sara osama wrote:
> >
> > Dear All,
> > I have question concerning the unit of measurement for the range
> > parameter in the fit.stvariogram in gstat, i have fitted the following
> > spatio temporal variogram for data on monthly averages pf pm10 and the
> > longitude and latitude are in decimel format
> >
> >> pm10_sp
> > SpatialPoints:
> >      longitude latitude
> > 1     30.92500 29.95300
> > 61    31.36000 30.26000
> > 121   31.33300 29.84720
> >  and this is the commands for fitting the variogram
> >> varpm10 <- variogramST(lnpm10only~1,data=lnpm10_stfdf,cutoff=0.8,
> assumeRegular=T,na.omit=T)
> >> plot(varpm10,map=F)
> >> sumMetric <- vgmST("sumMetric", space = vgm(psill=5,"Mat", range=50,
> nugget=0),time = vgm(psill=500,"Mat", range=50, nugget=0,kappa=0.3), joint
> = vgm(psill=10,"Mat", range=100, nugget=1), stAni=200)
> >> sumMetric_Vgm <- fit.StVariogram(varpm10, sumMetric,
> method="L-BFGS-B",lower=pars.l,upper=pars.u)
> >> plot(varpm10,sumMetric_Vgm,map=F)
> >
> >> extractPar(sumMetric_Vgm)
> >       sill.s      range.s     nugget.s       sill.t      range.t
> > nugget.t
> >   1.73598444  50.63845035   0.03212857   0.01570379  49.99811448
> > 0.00000000
> >      sill.st <http://sill.st>     range.st <http://range.st>
> > nugget.st <http://nugget.st>         anis
> >   0.00000000 100.00004052   0.09151787 200.00000000
> >
> > --
> > now i want to know the units for these parameters in order to be able to
> > interpret it for example is it in km or m.
> >
> > I really appreciate any help that I deeply need
> >
> > Best Regards
> > Sara
> >
> >       Faculty of Economics & Political Science
> > Cairo University
> >
> >
> > Tel:(202)35728055-(202)35728116-(202)35736608-(202)35736605
> > Fax:(202)35711020
> > Follow us on twitter:https://twitter.com/fepsnews
> >
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Edzer Pebesma
> Institute for Geoinformatics  (ifgi),  University of M?nster
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Best Regards

Faculty of Economics & Political Science
Cairo University
Tel:(202)35728055-(202)35728116-(202)35736608-(202)35736605
Fax:(202)35711020
Follow us on twitter:https://twitter.com/fepsnews

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Thu May 18 09:32:01 2017
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Thu, 18 May 2017 09:32:01 +0200
Subject: [R-sig-Geo] Nested Variogram Model for 3D data using gstat
In-Reply-To: <000201d2cf16$84834030$8d89c090$@th-koeln.de>
References: <000201d2cf16$84834030$8d89c090$@th-koeln.de>
Message-ID: <0f0eb2b5-e2be-f5c1-6c61-06bd59f8d1bd@uni-muenster.de>

Dear A.B.M.,

fit.variogram will not help you fit directional variograms. In
principle, you could write your own optimizer using e.g. stats::optim,
and gstat:;variogramLine to compute semivariances give the model for
different directions and distances.

I'd welcome a pull request with a well-tested function that would make
this easy for end users.

On 17/05/17 16:04, ABM Firoz wrote:
> Dear All,
> 
>  
> 
> I am facing problem to obtain the nested variogram model for the 3D
> lithological data. I have 3D data, in which the coordinate are x,y,z (z
> corresponds to the depth) and the lithofacie information for different
> materials as a binary values (Ind01, Ind02, Ind03, Ind04, Ind05) . Now I
> would like to develop sequential indicator simulation to obtain the
> facies distribution for each of the category. I have used gstat to
> develop the vertical and horizontal varigram but unable to fit as a
> nested variogram for both vertical and horizontal direction. Is there
> any one help me to combine the vertical and horizontal variogram in one
> nested structure.
> 
> The data I have used from the greengoblin data. A dropbox link has been
> placed at the end of this post to obtain the data.
> 
>  
> 
> The code I have used as follows to obtain the vertical and horizontal
> variogram for the Ind03 facies (medium sand)
> 
> green_gstat <- as.data.frame(green_goblin)
> 
> coordinates(green_gstat) = ~X+Y+Z
> 
> names (green_gstat)
> 
> class(green_gstat)
> 
> # vertical variogram
> 
> ind03_vert <- variogram((Ind03)~X+Y+Z, green_gstat, cutoff = 250, width=
> 2, alpha= 0, beta = 90)
> 
> # fitting the variogram model
> 
> ind03_fit <- fit.variogram (ind03_vert, vgm(1, "Exp", 500,1))
> 
> # plotting
> 
> plot(ind03_vert, ind03_fit)
> 
> Rplot01
> 
> # estimating the horizontal variogram in omni direction
> 
> Ind03_hori_omni <- variogram(ind03, cutoff = 15000, width = 1000, alpha = 0)
> 
> # fiting variogram mdoel
> 
> ind03_fit_dir <- vgm(.188, "Exp", 1500, .05, anis = c (45, .25))
> 
> # obtain the fit model informaiotn
> 
> Ind03_fit_vrio_hor <- fit.variogram(Ind03_hori_omni,vgm(.188, "Exp",
> 1500, .05, anis = c (45, .25)))
> 
> # plotting the data
> 
> plot(Ind03_hori_omni,ind03_fit_dir)
> 
> hori
> 
> # now I have tried to go for the nested variogram approach, typically
> what I have found there, that one can create the nested model like this
> from the same variogram, but using diffent fitting options.
> 
> ind03_vertnested <- fit.variogram (*ind03_vert*, vgm (1, "Exp", 500,
> add.to = vgm (.188, "Exp", .05, anis = c(45, 0.25))))
> 
> here I have only used the ind03_vert (variogram), but I also have a
> horizontal variogram model (Ind03_hori_omni). So, how I can incorporate
> two model into a single variogram model. A feedback in this regard would
> be highly appreciate. Or, is there any other option to create a 3D
> variogram modeling.
> 
>  
> 
> To have the data, please follow this dropbox link for the easy access.
> 
>  
> 
> Link to the data-
> https://www.dropbox.com/s/6xl8zh7ofntlna6/GreenGoblin.csv?dl=0
> 
>  
> 
>  
> 
>  
> 
>  
> 
> *--*
> 
> *A.B.M Firoz*
> *Researcher- GIS & Hydrological Modeling*
> 
> * *
> 
> *ITT*- Institute for Technology and Resources Management
> 
> in the Tropics and Subtropics
> 
> *Technology* *Arts** **Sciences*
> *TH K?ln* - University of Applied Sciences 
> 
>  
> 
> T:  +49 221 8275-2059
> 
> F: +49 221 8275-2736
> E : abm.firoz at fh-koeln.de <mailto:abm.firoz at fh-koeln.de>
> 
>  
> 
> Kalk- Campus
> RobertStrasse  2
> 51105 K?ln Germany**
> 
> www.tt.fh-koeln.de <http://www.tt.fh-koeln.de/>
> www.th-koeln.de <http://www.th-koeln.de/>
> 
>  
> 
> 
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170518/bebf7571/attachment.sig>

From mel at mbacou.com  Thu May 18 10:06:37 2017
From: mel at mbacou.com (Melanie Bacou)
Date: Thu, 18 May 2017 04:06:37 -0400
Subject: [R-sig-Geo] Specify color for "zero" raster values using
 levelplot
In-Reply-To: <1249795416.1781620.1495041913673@mail.yahoo.com>
References: <1249795416.1781620.1495041913673.ref@mail.yahoo.com>
 <1249795416.1781620.1495041913673@mail.yahoo.com>
Message-ID: <090ef682-d385-69c6-856a-acb106643842@mbacou.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170518/0455f0cd/attachment.html>

From ankur.sum13 at yahoo.com  Thu May 18 17:37:54 2017
From: ankur.sum13 at yahoo.com (Ankur Sarker)
Date: Thu, 18 May 2017 15:37:54 +0000 (UTC)
Subject: [R-sig-Geo] Appropriate variogram model
In-Reply-To: <a8dbf504-ff10-90b7-73ce-2d6424d54acd@52north.org>
References: <770069364.6168148.1494218008878.ref@mail.yahoo.com>
 <770069364.6168148.1494218008878@mail.yahoo.com>
 <a8dbf504-ff10-90b7-73ce-2d6424d54acd@52north.org>
Message-ID: <218830669.885794.1495121874585@mail.yahoo.com>

Dear?Dr. Benedikt Gr?ler,
Thank you for your advice. Now, I have found some directs to move on.?
Actually, I do not have enough background to analyze by myself. Based on your suggestions, I have plotted the variogram. I think there are daily and weekly seasonality.
According to your suggestions, I am trying to model daily seasonality. It would be great if you can give me more advice.
With Best Regards,Ankur 

    On Wednesday, May 17, 2017 6:10 AM, Dr. Benedikt Gr?ler <b.graeler at 52north.org> wrote:
 

 Dear Ankur,

I see two issues from your plot:
i) there seems to be a seasonality in your data (maybe daily?) as the 
sample variogram grows up to half the length of your temporal axis and 
then drops again
ii) the starting values of your fitting routine are far off from what 
can be deduced from the sample variogram. Hence, the numerical routines 
have a hard time to identify the correct values. Additionally, the 
parameters are of different orders of magnitude further complicating the 
numerical fit. Manual re-scaling might help.

Try adding the parameter "scales=list(arrows=F)" for a visual assessment 
of the empirical variogram; in your case

plot(var, wireframe=T, scales=list(arrows=F))

This reveals that the variogram indicates the same strength of 
dependence for values 24 and 0 hours apart -> daily cycle? I'd suggest 
to model the daily cycle first and then to model the spatio-temporal 
variogram of the residuals.

HTH,

? Ben



On 08/05/2017 06:33, Ankur Sarker via R-sig-Geo wrote:
> Hi,
> 
> Can anyone suggest me the most appropriate variogram model to fit my data?
> 
> I have tried three different models and results are too bad. Here is the 
> comparison of different variogram models as attached.
> 
> Thanks,
> Ankur
> 
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Dr. Benedikt Gr?ler
52?North Initiative for Geospatial Open Source Software GmbH
Martin-Luther-King-Weg 24
48155 Muenster, Germany

E-Mail: b.graeler at 52north.org
Fon: +49-(0)-251/396371-39
Fax: +49-(0)-251/396371-11

http://52north.org/
Twitter: @FiveTwoN

General Managers: Dr. Albert Remke, Dr. Andreas Wytzisk
Local Court Muenster HRB 10849
_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170518/f11c6f9c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: variogram.pdf
Type: application/pdf
Size: 12868 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170518/f11c6f9c/attachment.pdf>

From perry.beasley-hall at sydney.edu.au  Fri May 19 08:51:15 2017
From: perry.beasley-hall at sydney.edu.au (Perry Beasley-hall)
Date: Fri, 19 May 2017 06:51:15 +0000
Subject: [R-sig-Geo] Problem in ENFA: infinite or missing values
Message-ID: <7B16C76E8F4ABD4B8F0D70668E32D2CB96CFC3@ex-mbx-pro-01>

Hello everyone,
I'm trying to do an ENFA using the package adehabitatHS. I'm following the example code and am creating the 'pr' file using:

pr <- slot(count.points(species, map), "data")[,1]

Where 'species' is a .csv file with two columns of latitude and longitude, and 'map' is a SpatialGridDataFrame file of 23 .asc inputs. For reference, I'm importing my .asc files using readGDAL and then using the cbind command to put them into the 'map' file.

I encounter the following warning message when trying to create 'pr':

1: In points2grid(points, tolerance, round) :
  grid has empty column/rows in dimension 1
2: In points2grid(points, tolerance, round) :
  grid has empty column/rows in dimension 2

I assumed this would be alright, but I'm getting the subsequent error when I attempt the enfa command:

enfa1 <- enfa(pc, pr, scannf = FALSE)

Error in eigen(Se) : infinite or missing values in 'x'

Some searching online has told me that this might mean not all of my rows in the 'species' .csv correspond to variables in the 'map' file, but I don't think this is the case for my dataset.

Can anyone help me with this issue? Many thanks in advance.
____

Perry Beasley-Hall
PhD Candidate
School of Life and Environmental Sciences

THE UNIVERSITY OF SYDNEY
Room 329, Edgeworth David Building A11
Sydney NSW 2006, Australia
E-mail: perry.beasley-hall at sydney.edu.au
Phone: +61 420 494 584

	[[alternative HTML version deleted]]


From thi_veloso at yahoo.com.br  Thu May 18 18:27:12 2017
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Thu, 18 May 2017 16:27:12 +0000 (UTC)
Subject: [R-sig-Geo] Specify color for "zero" raster values using
 levelplot
In-Reply-To: <090ef682-d385-69c6-856a-acb106643842@mbacou.com>
References: <1249795416.1781620.1495041913673.ref@mail.yahoo.com>
 <1249795416.1781620.1495041913673@mail.yahoo.com>
 <090ef682-d385-69c6-856a-acb106643842@mbacou.com>
Message-ID: <1571909681.837951.1495124832162@mail.yahoo.com>

Hi Mel,
Thank you ver much for the suggestion. I've reproduced your palette, which looks like exactly what I was looking for. However, the grey color is still not associated to zero values in the map.
After plotting my raster with your palette as an argument:
levelplot(annual.mask, cuts=14, col.regions=myPal(15), margin=F)

I ended up with a figure like this:?http://i.imgur.com/aqZkCGZ.png, where I was hoping to have all white (i.e. zero values) in the map filled with grey.
Any other ideas to achieve that?
This is the file and code to reproduce the figure:
https://dl.dropboxusercontent.com/content_link/jnv58wx0QN8ObKaiqigVckXtDmanOVYlgBxXoiym4oty5MS93xHolItNZ8tJ5gVF/file?dl=1
########################library(rasterVis)
myPal <- read.table(sep=",", text="155,29,32,Brown238,36,37,Tomato238,77,34,Tomato250,143,35,Dark.orange254,215,24,Gold214,223,47,Bitter.lemon188,218,112,Sulu179,179,179,Dark.gray124,199,177,Keppel105,202,229,Viking72,156,211,Curious.blue69,101,173,Chetwode.blue58,86,164,Governor.bay58,72,155,Dark.slate.blue44,46,118,Blue.bell")
myPal <- colorRampPalette(rgb(myPal[, 1:3], names=as.character(myPal$V4), maxColorValue=255))
r.annual <- raster("Desktop/r.annual.tif")levelplot(r.annual, cuts=14, col.regions=myPal(15), margin=F)########################?Greetings,?-- Thiago V. dos Santos
PhD studentLand and Atmospheric ScienceUniversity of Minnesota 

    On Thursday, May 18, 2017 3:06 AM, Melanie Bacou <mel at mbacou.com> wrote:
 

  Simply use the same palette as in the example: library(lattice)
 
 myPal <- read.table(sep="\t", text="
 155? ?? 29? ?? 32??? Brown
 238?? ? 36?? ? 37??? Tomato
 238???? 77???? 34??? Tomato
 250??? 143???? 35??? Dark orange
 254??? 215 ? ? 24??? Gold
 214??? 223???? 47??? Bitter lemon
 188??? 218??? 112??? Sulu
 179??? 179??? 179??? Dark gray
 124??? 199??? 177??? Keppel
 105??? 202??? 229??? Viking
 72???? 156??? 211??? Curious blue
 69???? 101??? 173??? Chetwode blue
 58???? 86???? 164??? Governor bay
 58???? 72???? 155??? Dark slate blue
 44???? 46???? 118??? Blue bell")
 
 myPal <- colorRampPalette(rgb(myPal[, 1:3], names=as.character(myPal$V4), maxColorValue=255))
 
 x <- seq(pi/4, 5 * pi, length.out = 100)
 y <- seq(pi/4, 5 * pi, length.out = 100)
 r <- as.vector(sqrt(outer(x^2, y^2, "+")))
 grid <- expand.grid(x=x, y=y)
 grid$z <- cos(r^2) * exp(-r/(pi^3))
 levelplot(z~x*y, grid, cuts=14, col.regions=myPal(15), margin=FALSE)
 
 --Mel.
 
 On 05/17/2017 01:25 PM, Thiago V. dos Santos via R-sig-Geo wrote:
  
 Dear all,

I am trying to change the color for zero values in a map produced using levelplot to plot a raster file. Specifically, I want to reproduce this figure: http://i.imgur.com/mjXxZhO.png, where a red to blue scale is used, but notice that zero values have been replaced by grey.

As an example, let's use an adapted version of the August irradiation code from the rasterVis webpage:

################################
library(raster)
library(ncdf4)
library(rasterVis)

##Solar irradiation data from CMSAF 
old <- setwd(tempdir())
download.file('https://raw.github.com/oscarperpinan/spacetime-vis/master/data/SISmm2008_CMSAF.zip',
'SISmm2008_CMSAF.zip', method='wget')
unzip('SISmm2008_CMSAF.zip')

listFich <- dir(pattern='\\.nc')
stackSIS <- stack(listFich)
stackSIS <- stackSIS * 24 ##from irradiance (W/m2) to irradiation Wh/m2
idx <- seq(as.Date('2008-01-15'), as.Date('2008-12-15'), 'month')

SISmm <- setZ(stackSIS, idx)
names(SISmm) <- month.abb

setwd(old)

# Set color palette
myTheme=rasterTheme(region=brewer.pal('RdBu', n=11))

Aug <- raster(SISmm, 8)
meanAug <- cellStats(Aug, mean)
levelplot(Aug - meanAug, par.settings = myTheme, margin=FALSE)

################################

In the example above, how can I replace the color of "zero values" with grey?

Thanks,
 -- Thiago V. dos Santos

PhD student
Land and Atmospheric Science
University of Minnesota

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo




   
	[[alternative HTML version deleted]]


From mel at mbacou.com  Thu May 18 19:36:02 2017
From: mel at mbacou.com (Melanie Bacou)
Date: Thu, 18 May 2017 13:36:02 -0400
Subject: [R-sig-Geo] Specify color for "zero" raster values using
 levelplot
In-Reply-To: <1571909681.837951.1495124832162@mail.yahoo.com>
References: <1249795416.1781620.1495041913673.ref@mail.yahoo.com>
 <1249795416.1781620.1495041913673@mail.yahoo.com>
 <090ef682-d385-69c6-856a-acb106643842@mbacou.com>
 <1571909681.837951.1495124832162@mail.yahoo.com>
Message-ID: <8330ac1d-c047-b76f-b66e-e6ea6ef2d662@mbacou.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170518/2a197305/attachment.html>

From jecogeo at gmail.com  Mon May 22 04:38:45 2017
From: jecogeo at gmail.com (Jefferson Ferreira-Ferreira)
Date: Sun, 21 May 2017 22:38:45 -0400
Subject: [R-sig-Geo] spgwr note on require the package
Message-ID: <CAFFT+Y5f4vBxqi+UbVoYTo4n4jkTdNN3mZCao=ikEoqD5w3-uw@mail.gmail.com>

Dears, when we require spgwr package, a note is showed "NOTE: This package
does not constitute approval of GWR as a method of spatial analysis".
I tried to investigate example(spgwr) and cannot figure out. A search for
this note in google doesn't return answers as well.

Are some of you aware of what this note means, specifically? As long as I
now, Geographically weighted regressions are solid techniques. So this note
is confusing me.

Best,


*Jefferson Ferreira-Ferreira, **PhD (abd)*

*Geographer*



*Ecosystem Dynamics Observatory <http://tscanada.wix.com/ecodyn> -
EcoDyn/UNESP*
*Department of **Geography *
*Institute of Geosciences and Exact Sciences** (IGCE)*
*S?o Paulo State University (UNESP)*
*Rio Claro, SP - Brazil*

	[[alternative HTML version deleted]]


From kozielec at gmail.com  Mon May 22 16:07:41 2017
From: kozielec at gmail.com (Slawomir Kozielec)
Date: Mon, 22 May 2017 15:07:41 +0100
Subject: [R-sig-Geo] what Manifold does that Rgdal fails? Projection
	transformation R
Message-ID: <CAC4aLfyDaoUxbLHAvSZfP+3HZJ1Z8xEeQdYECdfi1x=c6aKVjw@mail.gmail.com>

Dear friends,

pretty new to the topic so let me know if i do not follow and reproducible
examples convention. I am OK with R, but spatial data is still terra
incognita

I got a Mapinfo file and i had to convert it to ESRI shp and then within r
into spatial lines data frame and use with leaflet+OS maps epsg:3857

whatever i tried within R with spTransform (rgdal) it did not work - either
was not visible or was visible with offset (approx 2 meters). Proj4 did not
want to work with SLDF.
I took the file into Manifold, converted it to 3857, returned to R,
transformed it to epsg:4269 and it works perfectly OK with leaflet + OS map
3857. But i have to automate this process to work within R, without any
Manifold intervention.

I checked how the files differed:
i selected one item (the same of course) from the file and to make it easy
to present created a centroid for it. Then I checked projection, xy, and
latlong.

the item created from the file without using Manifold:
proj4string

    "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000
+y_0=-100000 +ellps=airy +units=m +no_defs"

centroid:

    528685 181836.2

centroid after spTransform to 4269

    -0.1450149 51.52028

the item from the file pre-processed by Manifold:
proj4string

    "+proj=merc +lon_0=0 +lat_ts=0 +x_0=0 +y_0=0 +a=6378137 +b=6378137
+units=m +no_defs"

centroid:

    -16321.64 6713938

centroid after spTransform to 4269:

    -0.1466198 51.52079

the latter is correct of course. Just to emphasize - it is the same file
and the same item

Few questions:

1. any idea what happens in Manifold that fails in rgdal?

2. is there any mechanism in R that I can apply to fix the error (to match
the 'with Manifold' output)? Maybe a kind of offset or work on prj files?

I tried to use the latter CRS for transformation

    P5 <-spTransform(ptp.points1P, CRS( "+proj=merc +lon_0=0 +lat_ts=0
+x_0=0 +y_0=0 +a=6378137 +b=6378137 +units=m +no_defs"))

and it returned still incorrect values
the xy is t least inverted but shifted

     -16142.98 6713847

the latlong is of course incorrect, but the same as without any attempt to
transform

    -0.1450149 51.52028


content of prj file not transformed - failing to correctly show

    PROJCS["Transverse_Mercator",GEOGCS["GCS_Airy
1830",DATUM["D_unknown",SPHEROID["airy",6377563.396,299.3249646]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],PROJECTION["Transverse_Mercator"],PARAMETER["latitude_of_origin",49],PARAMETER["central_meridian",-2],PARAMETER["scale_factor",0.9996012717],PARAMETER["false_easting",400000],PARAMETER["false_northing",-100000],UNIT["Meter",1]]

content of prj manifold transformed file - good to go

    PROJCS["Mercator_2SP",GEOGCS["GCS_unnamed
ellipse",DATUM["D_unknown",SPHEROID["Unknown",6378137,0]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],PROJECTION["Mercator"],PARAMETER["standard_parallel_1",0],PARAMETER["central_meridian",0],PARAMETER["false_easting",0],PARAMETER["false_northing",0],UNIT["Meter",1]]

thanks in advance

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Tue May 23 09:13:09 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 23 May 2017 09:13:09 +0200
Subject: [R-sig-Geo] what Manifold does that Rgdal fails? Projection
 transformation R
In-Reply-To: <CAC4aLfyDaoUxbLHAvSZfP+3HZJ1Z8xEeQdYECdfi1x=c6aKVjw@mail.gmail.com>
References: <CAC4aLfyDaoUxbLHAvSZfP+3HZJ1Z8xEeQdYECdfi1x=c6aKVjw@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1705230905220.17071@reclus.nhh.no>

On Mon, 22 May 2017, Slawomir Kozielec wrote:

> Dear friends,
>
> pretty new to the topic so let me know if i do not follow and reproducible
> examples convention. I am OK with R, but spatial data is still terra
> incognita

Please find out about ellipsoids and datums (sorry, not plural data). They 
describe the assumed shape of the world, and the relative distance of a 
point on the surface to an assumed 3D origin (roughly).

In one of your CRS, the ellipsoid is Airy, in another where a=b it is a 
sphere. In addition, EPSG 4269 presupposes NAD 1983, equivalent to GRS 
1980 and effectively WGS84. What you do not have are +towgs84= 
definitions, but if these are not given, they are assumed by proj in rgdal 
to be WGS84.

None of these are "correct", all are based on chosen assumptions. Why 
Manifold is assuming a spheroid is unknown. Define your datums correctly, 
and things should clear up.

Hope this clarifies,

Roger

>
> I got a Mapinfo file and i had to convert it to ESRI shp and then within r
> into spatial lines data frame and use with leaflet+OS maps epsg:3857
>
> whatever i tried within R with spTransform (rgdal) it did not work - either
> was not visible or was visible with offset (approx 2 meters). Proj4 did not
> want to work with SLDF.
> I took the file into Manifold, converted it to 3857, returned to R,
> transformed it to epsg:4269 and it works perfectly OK with leaflet + OS map
> 3857. But i have to automate this process to work within R, without any
> Manifold intervention.
>
> I checked how the files differed:
> i selected one item (the same of course) from the file and to make it easy
> to present created a centroid for it. Then I checked projection, xy, and
> latlong.
>
> the item created from the file without using Manifold:
> proj4string
>
>    "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000
> +y_0=-100000 +ellps=airy +units=m +no_defs"
>
> centroid:
>
>    528685 181836.2
>
> centroid after spTransform to 4269
>
>    -0.1450149 51.52028
>
> the item from the file pre-processed by Manifold:
> proj4string
>
>    "+proj=merc +lon_0=0 +lat_ts=0 +x_0=0 +y_0=0 +a=6378137 +b=6378137
> +units=m +no_defs"
>
> centroid:
>
>    -16321.64 6713938
>
> centroid after spTransform to 4269:
>
>    -0.1466198 51.52079
>
> the latter is correct of course. Just to emphasize - it is the same file
> and the same item
>
> Few questions:
>
> 1. any idea what happens in Manifold that fails in rgdal?
>
> 2. is there any mechanism in R that I can apply to fix the error (to match
> the 'with Manifold' output)? Maybe a kind of offset or work on prj files?
>
> I tried to use the latter CRS for transformation
>
>    P5 <-spTransform(ptp.points1P, CRS( "+proj=merc +lon_0=0 +lat_ts=0
> +x_0=0 +y_0=0 +a=6378137 +b=6378137 +units=m +no_defs"))
>
> and it returned still incorrect values
> the xy is t least inverted but shifted
>
>     -16142.98 6713847
>
> the latlong is of course incorrect, but the same as without any attempt to
> transform
>
>    -0.1450149 51.52028
>
>
> content of prj file not transformed - failing to correctly show
>
>    PROJCS["Transverse_Mercator",GEOGCS["GCS_Airy
> 1830",DATUM["D_unknown",SPHEROID["airy",6377563.396,299.3249646]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],PROJECTION["Transverse_Mercator"],PARAMETER["latitude_of_origin",49],PARAMETER["central_meridian",-2],PARAMETER["scale_factor",0.9996012717],PARAMETER["false_easting",400000],PARAMETER["false_northing",-100000],UNIT["Meter",1]]
>
> content of prj manifold transformed file - good to go
>
>    PROJCS["Mercator_2SP",GEOGCS["GCS_unnamed
> ellipse",DATUM["D_unknown",SPHEROID["Unknown",6378137,0]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],PROJECTION["Mercator"],PARAMETER["standard_parallel_1",0],PARAMETER["central_meridian",0],PARAMETER["false_easting",0],PARAMETER["false_northing",0],UNIT["Meter",1]]
>
> thanks in advance
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From mdsumner at gmail.com  Tue May 23 09:40:19 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Tue, 23 May 2017 07:40:19 +0000
Subject: [R-sig-Geo] what Manifold does that Rgdal fails? Projection
 transformation R
In-Reply-To: <alpine.LFD.2.20.1705230905220.17071@reclus.nhh.no>
References: <CAC4aLfyDaoUxbLHAvSZfP+3HZJ1Z8xEeQdYECdfi1x=c6aKVjw@mail.gmail.com>
 <alpine.LFD.2.20.1705230905220.17071@reclus.nhh.no>
Message-ID: <CAAcGz9-QA_RS40ERBnBLdPxbXc2SLghN8ucwHPLm11CqYASY_Q@mail.gmail.com>

Quickly, there is no need to convert to shapefile. MapInfo TAB or MIF are
both vastly superior to SHP and are well supported by rgdal and sf.

That takes out one weak link. I'm happy to explore with Manifold if you
provide reproducible examples. Also I would ask on georeference.org too,
though you'll get a very disparaging (of open source) perspective from some
of their staff. It works both ways.

Cheers, Mike

On Tue, 23 May 2017, 17:13 Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Mon, 22 May 2017, Slawomir Kozielec wrote:
>
> > Dear friends,
> >
> > pretty new to the topic so let me know if i do not follow and
> reproducible
> > examples convention. I am OK with R, but spatial data is still terra
> > incognita
>
> Please find out about ellipsoids and datums (sorry, not plural data). They
> describe the assumed shape of the world, and the relative distance of a
> point on the surface to an assumed 3D origin (roughly).
>
> In one of your CRS, the ellipsoid is Airy, in another where a=b it is a
> sphere. In addition, EPSG 4269 presupposes NAD 1983, equivalent to GRS
> 1980 and effectively WGS84. What you do not have are +towgs84=
> definitions, but if these are not given, they are assumed by proj in rgdal
> to be WGS84.
>
> None of these are "correct", all are based on chosen assumptions. Why
> Manifold is assuming a spheroid is unknown. Define your datums correctly,
> and things should clear up.
>
> Hope this clarifies,
>
> Roger
>
> >
> > I got a Mapinfo file and i had to convert it to ESRI shp and then within
> r
> > into spatial lines data frame and use with leaflet+OS maps epsg:3857
> >
> > whatever i tried within R with spTransform (rgdal) it did not work -
> either
> > was not visible or was visible with offset (approx 2 meters). Proj4 did
> not
> > want to work with SLDF.
> > I took the file into Manifold, converted it to 3857, returned to R,
> > transformed it to epsg:4269 and it works perfectly OK with leaflet + OS
> map
> > 3857. But i have to automate this process to work within R, without any
> > Manifold intervention.
> >
> > I checked how the files differed:
> > i selected one item (the same of course) from the file and to make it
> easy
> > to present created a centroid for it. Then I checked projection, xy, and
> > latlong.
> >
> > the item created from the file without using Manifold:
> > proj4string
> >
> >    "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000
> > +y_0=-100000 +ellps=airy +units=m +no_defs"
> >
> > centroid:
> >
> >    528685 181836.2
> >
> > centroid after spTransform to 4269
> >
> >    -0.1450149 51.52028
> >
> > the item from the file pre-processed by Manifold:
> > proj4string
> >
> >    "+proj=merc +lon_0=0 +lat_ts=0 +x_0=0 +y_0=0 +a=6378137 +b=6378137
> > +units=m +no_defs"
> >
> > centroid:
> >
> >    -16321.64 6713938
> >
> > centroid after spTransform to 4269:
> >
> >    -0.1466198 51.52079
> >
> > the latter is correct of course. Just to emphasize - it is the same file
> > and the same item
> >
> > Few questions:
> >
> > 1. any idea what happens in Manifold that fails in rgdal?
> >
> > 2. is there any mechanism in R that I can apply to fix the error (to
> match
> > the 'with Manifold' output)? Maybe a kind of offset or work on prj files?
> >
> > I tried to use the latter CRS for transformation
> >
> >    P5 <-spTransform(ptp.points1P, CRS( "+proj=merc +lon_0=0 +lat_ts=0
> > +x_0=0 +y_0=0 +a=6378137 +b=6378137 +units=m +no_defs"))
> >
> > and it returned still incorrect values
> > the xy is t least inverted but shifted
> >
> >     -16142.98 6713847
> >
> > the latlong is of course incorrect, but the same as without any attempt
> to
> > transform
> >
> >    -0.1450149 51.52028
> >
> >
> > content of prj file not transformed - failing to correctly show
> >
> >    PROJCS["Transverse_Mercator",GEOGCS["GCS_Airy
> >
> 1830",DATUM["D_unknown",SPHEROID["airy",6377563.396,299.3249646]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],PROJECTION["Transverse_Mercator"],PARAMETER["latitude_of_origin",49],PARAMETER["central_meridian",-2],PARAMETER["scale_factor",0.9996012717],PARAMETER["false_easting",400000],PARAMETER["false_northing",-100000],UNIT["Meter",1]]
> >
> > content of prj manifold transformed file - good to go
> >
> >    PROJCS["Mercator_2SP",GEOGCS["GCS_unnamed
> >
> ellipse",DATUM["D_unknown",SPHEROID["Unknown",6378137,0]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],PROJECTION["Mercator"],PARAMETER["standard_parallel_1",0],PARAMETER["central_meridian",0],PARAMETER["false_easting",0],PARAMETER["false_northing",0],UNIT["Meter",1]]
> >
> > thanks in advance
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From kozielec at gmail.com  Tue May 23 10:48:31 2017
From: kozielec at gmail.com (Slawomir Kozielec)
Date: Tue, 23 May 2017 09:48:31 +0100
Subject: [R-sig-Geo] what Manifold does that Rgdal fails? Projection
 transformation R
In-Reply-To: <CAAcGz9-QA_RS40ERBnBLdPxbXc2SLghN8ucwHPLm11CqYASY_Q@mail.gmail.com>
References: <CAC4aLfyDaoUxbLHAvSZfP+3HZJ1Z8xEeQdYECdfi1x=c6aKVjw@mail.gmail.com>
 <alpine.LFD.2.20.1705230905220.17071@reclus.nhh.no>
 <CAAcGz9-QA_RS40ERBnBLdPxbXc2SLghN8ucwHPLm11CqYASY_Q@mail.gmail.com>
Message-ID: <CAC4aLfyhat1Neb011C_WVh+4P9BqsRSqvzFdzmQmeuw8-MmF0g@mail.gmail.com>

all, Thank you for your assistance,
I resolved the issue, though I am not sure how to 'call off' my request for
advice.
Basically the MapInfo was projected to BNG (27700), so rgdal tried to
transform already projected file. On Manifold I most likely and purely by
chance set it to initial BNG and this is how it was produced correctly (i
have no practical experience with Manifold - I got it once upon a time with
intention to learn but never had time to). Probably Proj4 could have worked
as well (as you provide the initial projection), but it refuses to work
with spatial dataframes, especially lines (as points you can just extract
as coords and transform).
What I did was basically to replace by substitution (not transformation)
the proj4file attribute to BNG and then transformed it - everything went
smoothly from then on

Mike, the file format is not my choice - I am preparing an application for
my team and shp is our internal standard. Alas our GIS team leader took few
months off, some project to clean oceans off plastic, and his deputees did
not know how to arrange the file for me - so i took a 'raw' file from our
vendor. The file was in TAB format. I am not disputing whatever is better
as I am a proper layman in the order of datums, elipsums and all other
Latin words I would love to know:)


On Tue, May 23, 2017 at 8:40 AM, Michael Sumner <mdsumner at gmail.com> wrote:

> Quickly, there is no need to convert to shapefile. MapInfo TAB or MIF are
> both vastly superior to SHP and are well supported by rgdal and sf.
>
> That takes out one weak link. I'm happy to explore with Manifold if you
> provide reproducible examples. Also I would ask on georeference.org too,
> though you'll get a very disparaging (of open source) perspective from some
> of their staff. It works both ways.
>
> Cheers, Mike
>
> On Tue, 23 May 2017, 17:13 Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Mon, 22 May 2017, Slawomir Kozielec wrote:
>>
>> > Dear friends,
>> >
>> > pretty new to the topic so let me know if i do not follow and
>> reproducible
>> > examples convention. I am OK with R, but spatial data is still terra
>> > incognita
>>
>> Please find out about ellipsoids and datums (sorry, not plural data). They
>> describe the assumed shape of the world, and the relative distance of a
>> point on the surface to an assumed 3D origin (roughly).
>>
>> In one of your CRS, the ellipsoid is Airy, in another where a=b it is a
>> sphere. In addition, EPSG 4269 presupposes NAD 1983, equivalent to GRS
>> 1980 and effectively WGS84. What you do not have are +towgs84=
>> definitions, but if these are not given, they are assumed by proj in rgdal
>> to be WGS84.
>>
>> None of these are "correct", all are based on chosen assumptions. Why
>> Manifold is assuming a spheroid is unknown. Define your datums correctly,
>> and things should clear up.
>>
>> Hope this clarifies,
>>
>> Roger
>>
>> >
>> > I got a Mapinfo file and i had to convert it to ESRI shp and then
>> within r
>> > into spatial lines data frame and use with leaflet+OS maps epsg:3857
>> >
>> > whatever i tried within R with spTransform (rgdal) it did not work -
>> either
>> > was not visible or was visible with offset (approx 2 meters). Proj4 did
>> not
>> > want to work with SLDF.
>> > I took the file into Manifold, converted it to 3857, returned to R,
>> > transformed it to epsg:4269 and it works perfectly OK with leaflet + OS
>> map
>> > 3857. But i have to automate this process to work within R, without any
>> > Manifold intervention.
>> >
>> > I checked how the files differed:
>> > i selected one item (the same of course) from the file and to make it
>> easy
>> > to present created a centroid for it. Then I checked projection, xy, and
>> > latlong.
>> >
>> > the item created from the file without using Manifold:
>> > proj4string
>> >
>> >    "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000
>> > +y_0=-100000 +ellps=airy +units=m +no_defs"
>> >
>> > centroid:
>> >
>> >    528685 181836.2
>> >
>> > centroid after spTransform to 4269
>> >
>> >    -0.1450149 51.52028
>> >
>> > the item from the file pre-processed by Manifold:
>> > proj4string
>> >
>> >    "+proj=merc +lon_0=0 +lat_ts=0 +x_0=0 +y_0=0 +a=6378137 +b=6378137
>> > +units=m +no_defs"
>> >
>> > centroid:
>> >
>> >    -16321.64 6713938
>> >
>> > centroid after spTransform to 4269:
>> >
>> >    -0.1466198 51.52079
>> >
>> > the latter is correct of course. Just to emphasize - it is the same file
>> > and the same item
>> >
>> > Few questions:
>> >
>> > 1. any idea what happens in Manifold that fails in rgdal?
>> >
>> > 2. is there any mechanism in R that I can apply to fix the error (to
>> match
>> > the 'with Manifold' output)? Maybe a kind of offset or work on prj
>> files?
>> >
>> > I tried to use the latter CRS for transformation
>> >
>> >    P5 <-spTransform(ptp.points1P, CRS( "+proj=merc +lon_0=0 +lat_ts=0
>> > +x_0=0 +y_0=0 +a=6378137 +b=6378137 +units=m +no_defs"))
>> >
>> > and it returned still incorrect values
>> > the xy is t least inverted but shifted
>> >
>> >     -16142.98 6713847
>> >
>> > the latlong is of course incorrect, but the same as without any attempt
>> to
>> > transform
>> >
>> >    -0.1450149 51.52028
>> >
>> >
>> > content of prj file not transformed - failing to correctly show
>> >
>> >    PROJCS["Transverse_Mercator",GEOGCS["GCS_Airy
>> > 1830",DATUM["D_unknown",SPHEROID["airy",6377563.396,
>> 299.3249646]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],
>> PROJECTION["Transverse_Mercator"],PARAMETER["latitude_of_origin",49],
>> PARAMETER["central_meridian",-2],PARAMETER["scale_factor",0.
>> 9996012717],PARAMETER["false_easting",400000],PARAMETER["
>> false_northing",-100000],UNIT["Meter",1]]
>> >
>> > content of prj manifold transformed file - good to go
>> >
>> >    PROJCS["Mercator_2SP",GEOGCS["GCS_unnamed
>> > ellipse",DATUM["D_unknown",SPHEROID["Unknown",6378137,0]]
>> ,PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]]
>> ,PROJECTION["Mercator"],PARAMETER["standard_parallel_
>> 1",0],PARAMETER["central_meridian",0],PARAMETER["false_
>> easting",0],PARAMETER["false_northing",0],UNIT["Meter",1]]
>> >
>> > thanks in advance
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-Geo mailing list
>> > R-sig-Geo at r-project.org
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
>> Roger.Bivand at nhh.no
>> Editor-in-Chief of The R Journal, https://journal.r-project.org/
>> index.html
>> http://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Tue May 23 11:11:08 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Tue, 23 May 2017 09:11:08 +0000
Subject: [R-sig-Geo] what Manifold does that Rgdal fails? Projection
 transformation R
In-Reply-To: <CAC4aLfyhat1Neb011C_WVh+4P9BqsRSqvzFdzmQmeuw8-MmF0g@mail.gmail.com>
References: <CAC4aLfyDaoUxbLHAvSZfP+3HZJ1Z8xEeQdYECdfi1x=c6aKVjw@mail.gmail.com>
 <alpine.LFD.2.20.1705230905220.17071@reclus.nhh.no>
 <CAAcGz9-QA_RS40ERBnBLdPxbXc2SLghN8ucwHPLm11CqYASY_Q@mail.gmail.com>
 <CAC4aLfyhat1Neb011C_WVh+4P9BqsRSqvzFdzmQmeuw8-MmF0g@mail.gmail.com>
Message-ID: <CAAcGz985hWOcYS8Y6kuJ0NfAx_rVT5HCKKj5XJLvmik=iYRDWQ@mail.gmail.com>

Appreciate the feedback, cheers

(proj4 can be willed into use for decomposed sp/sf/etc but beware of
internal transpose assumption in current version, going rogue is generally
not advisable)


On Tue, 23 May 2017, 18:48 Slawomir Kozielec <kozielec at gmail.com> wrote:

> all, Thank you for your assistance,
> I resolved the issue, though I am not sure how to 'call off' my request
> for advice.
> Basically the MapInfo was projected to BNG (27700), so rgdal tried to
> transform already projected file. On Manifold I most likely and purely by
> chance set it to initial BNG and this is how it was produced correctly (i
> have no practical experience with Manifold - I got it once upon a time with
> intention to learn but never had time to). Probably Proj4 could have worked
> as well (as you provide the initial projection), but it refuses to work
> with spatial dataframes, especially lines (as points you can just extract
> as coords and transform).
> What I did was basically to replace by substitution (not transformation)
> the proj4file attribute to BNG and then transformed it - everything went
> smoothly from then on
>
> Mike, the file format is not my choice - I am preparing an application for
> my team and shp is our internal standard. Alas our GIS team leader took few
> months off, some project to clean oceans off plastic, and his deputees did
> not know how to arrange the file for me - so i took a 'raw' file from our
> vendor. The file was in TAB format. I am not disputing whatever is better
> as I am a proper layman in the order of datums, elipsums and all other
> Latin words I would love to know:)
>
>
> On Tue, May 23, 2017 at 8:40 AM, Michael Sumner <mdsumner at gmail.com>
> wrote:
>
>> Quickly, there is no need to convert to shapefile. MapInfo TAB or MIF are
>> both vastly superior to SHP and are well supported by rgdal and sf.
>>
>> That takes out one weak link. I'm happy to explore with Manifold if you
>> provide reproducible examples. Also I would ask on georeference.org too,
>> though you'll get a very disparaging (of open source) perspective from some
>> of their staff. It works both ways.
>>
>> Cheers, Mike
>>
>> On Tue, 23 May 2017, 17:13 Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>
>>> On Mon, 22 May 2017, Slawomir Kozielec wrote:
>>>
>>> > Dear friends,
>>> >
>>> > pretty new to the topic so let me know if i do not follow and
>>> reproducible
>>> > examples convention. I am OK with R, but spatial data is still terra
>>> > incognita
>>>
>>> Please find out about ellipsoids and datums (sorry, not plural data).
>>> They
>>> describe the assumed shape of the world, and the relative distance of a
>>> point on the surface to an assumed 3D origin (roughly).
>>>
>>> In one of your CRS, the ellipsoid is Airy, in another where a=b it is a
>>> sphere. In addition, EPSG 4269 presupposes NAD 1983, equivalent to GRS
>>> 1980 and effectively WGS84. What you do not have are +towgs84=
>>> definitions, but if these are not given, they are assumed by proj in
>>> rgdal
>>> to be WGS84.
>>>
>>> None of these are "correct", all are based on chosen assumptions. Why
>>> Manifold is assuming a spheroid is unknown. Define your datums correctly,
>>> and things should clear up.
>>>
>>> Hope this clarifies,
>>>
>>> Roger
>>>
>>> >
>>> > I got a Mapinfo file and i had to convert it to ESRI shp and then
>>> within r
>>> > into spatial lines data frame and use with leaflet+OS maps epsg:3857
>>> >
>>> > whatever i tried within R with spTransform (rgdal) it did not work -
>>> either
>>> > was not visible or was visible with offset (approx 2 meters). Proj4
>>> did not
>>> > want to work with SLDF.
>>> > I took the file into Manifold, converted it to 3857, returned to R,
>>> > transformed it to epsg:4269 and it works perfectly OK with leaflet +
>>> OS map
>>> > 3857. But i have to automate this process to work within R, without any
>>> > Manifold intervention.
>>> >
>>> > I checked how the files differed:
>>> > i selected one item (the same of course) from the file and to make it
>>> easy
>>> > to present created a centroid for it. Then I checked projection, xy,
>>> and
>>> > latlong.
>>> >
>>> > the item created from the file without using Manifold:
>>> > proj4string
>>> >
>>> >    "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000
>>> > +y_0=-100000 +ellps=airy +units=m +no_defs"
>>> >
>>> > centroid:
>>> >
>>> >    528685 181836.2
>>> >
>>> > centroid after spTransform to 4269
>>> >
>>> >    -0.1450149 51.52028
>>> >
>>> > the item from the file pre-processed by Manifold:
>>> > proj4string
>>> >
>>> >    "+proj=merc +lon_0=0 +lat_ts=0 +x_0=0 +y_0=0 +a=6378137 +b=6378137
>>> > +units=m +no_defs"
>>> >
>>> > centroid:
>>> >
>>> >    -16321.64 6713938
>>> >
>>> > centroid after spTransform to 4269:
>>> >
>>> >    -0.1466198 51.52079
>>> >
>>> > the latter is correct of course. Just to emphasize - it is the same
>>> file
>>> > and the same item
>>> >
>>> > Few questions:
>>> >
>>> > 1. any idea what happens in Manifold that fails in rgdal?
>>> >
>>> > 2. is there any mechanism in R that I can apply to fix the error (to
>>> match
>>> > the 'with Manifold' output)? Maybe a kind of offset or work on prj
>>> files?
>>> >
>>> > I tried to use the latter CRS for transformation
>>> >
>>> >    P5 <-spTransform(ptp.points1P, CRS( "+proj=merc +lon_0=0 +lat_ts=0
>>> > +x_0=0 +y_0=0 +a=6378137 +b=6378137 +units=m +no_defs"))
>>> >
>>> > and it returned still incorrect values
>>> > the xy is t least inverted but shifted
>>> >
>>> >     -16142.98 6713847
>>> >
>>> > the latlong is of course incorrect, but the same as without any
>>> attempt to
>>> > transform
>>> >
>>> >    -0.1450149 51.52028
>>> >
>>> >
>>> > content of prj file not transformed - failing to correctly show
>>> >
>>> >    PROJCS["Transverse_Mercator",GEOGCS["GCS_Airy
>>> >
>>> 1830",DATUM["D_unknown",SPHEROID["airy",6377563.396,299.3249646]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],PROJECTION["Transverse_Mercator"],PARAMETER["latitude_of_origin",49],PARAMETER["central_meridian",-2],PARAMETER["scale_factor",0.9996012717],PARAMETER["false_easting",400000],PARAMETER["false_northing",-100000],UNIT["Meter",1]]
>>> >
>>> > content of prj manifold transformed file - good to go
>>> >
>>> >    PROJCS["Mercator_2SP",GEOGCS["GCS_unnamed
>>> >
>>> ellipse",DATUM["D_unknown",SPHEROID["Unknown",6378137,0]],PRIMEM["Greenwich",0],UNIT["Degree",0.017453292519943295]],PROJECTION["Mercator"],PARAMETER["standard_parallel_1",0],PARAMETER["central_meridian",0],PARAMETER["false_easting",0],PARAMETER["false_northing",0],UNIT["Meter",1]]
>>> >
>>> > thanks in advance
>>> >
>>> >       [[alternative HTML version deleted]]
>>> >
>>> > _______________________________________________
>>> > R-sig-Geo mailing list
>>> > R-sig-Geo at r-project.org
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>> >
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
>>> Roger.Bivand at nhh.no
>>> Editor-in-Chief of The R Journal,
>>> https://journal.r-project.org/index.html
>>> http://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>> --
>> Dr. Michael Sumner
>> Software and Database Engineer
>> Australian Antarctic Division
>> 203 Channel Highway
>> Kingston Tasmania 7050 Australia
>>
>>
> --
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From dmwarner at usgs.gov  Tue May 23 15:58:17 2017
From: dmwarner at usgs.gov (Warner, David)
Date: Tue, 23 May 2017 09:58:17 -0400
Subject: [R-sig-Geo] netcdf file mapping and dimensions issue
Message-ID: <CA+Y23taE+BO3iTB7gqzoPeJU1D=_h5hu7-5bAstHRERq0uCKzA@mail.gmail.com>

Greetings all

I am working nc files from the Ocean Biology Procesing Group.  The files
are MODIS aqua L2 ocean color files.

These files are not mapped or perhaps some would say orthorectified such
that when you create a rasterLayer from one of the contained variables and
plot it, the location and orientation of features is incorrect.  For
example, Georgian Bay, a large eastern subset of Lake Huron in the US and
Canada, may well show up west of the lake.

The dimensions of the data are read as column and row numbers rather than
latitude and longitude.

I am looking for a way to map these data in R other than my current
method.  The current method for a single nc file is shown below.  I run
this modified to fit into a parallelized foreach() loop that is used to
process folders of nc files.

The  basic steps that map the nc file data are 1) create a data.frame from
the nc files measured variable(s) and the latitude and longitude then
rasterize using vect2rast.SpatialPoints() and 2) resample the rasterLayer
created from the data.frame using resample() to an nc file I reprojected
using SeaDAS.  This seems overly complicated considering the fact that I
start with a raster and it is pretty slow (30-40 seconds per nc file in
parallel, nearly two minutes per file in the script example here).

I have two questions are 1) is there a way to eliminate the used of
vect2rast.SpatialPoints(), which is the slowest part of the process and 2)
are there any other ways to perhaps speed the process up if I can't
eliminate vect2rast.SpatialPoints().  I have tried rasterize() and that
does not provide the correct product.  It recreates what I started with,
unmapped data.  What am I missing, or is this just a function of the data I
am using?


Script is below.  I have included links to the two data files required in
the process.  One is the nc file,
https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.L2_LAC_OC.x.nc
the other is the reprojected version of this file from SeaDAS.
https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.reprojected.tif
To use them you will have to change the path to the file locations.

Thanks for your patience with my writing and poorly written code.  Any
ideas would be helpful.

Dave

library(raster)
library(plotKML)

rasterOptions(maxmemory = 1e+09)
start <- Sys.time()
#Create rasterLayers from the bands of nc file that are required for
mapping, with r412 being the measured variable of interest
r412<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_412')
r443<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_443')
r469<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_469')
r488<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_488')
r531<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_531')
r547<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_547')
r555<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_555')
r645<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_645')
r667<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_667')
r678<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var='geophysical_data/Rrs_678')

longitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var="navigation_data/longitude")
latitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
var="navigation_data/latitude")
r412  # shows that dimensions are row and column numbers, not lat and long


#Import single reprojected band of the nc file as geotif, with reprojection
done manually in SeaDAS software
correct<-raster("/Users/dmwarner/Documents/MODIS/OC/
correct/Huron/A2011202183000.reprojected.tif")


#now rasterize and map the nc file after

d <- data.frame(x = values(longitude), y = values(latitude), r412 =
values(r412))
d$r412[is.na(d$r412)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r412', cell.size=0.009337697,
method='raster')
ras.412 <- raster(dd)
ras.412.crop<-crop(ras.412, myext)
#ras.412.crop[ras.412.crop<0]<-NA
ras.412.crop[ras.412.crop>90000]<- NA


d <- data.frame(x = values(longitude), y = values(latitude), r443 =
values(r443))
d$r443[is.na(d$r443)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r443', cell.size=0.009337697,
method='raster')
ras.443 <- raster(dd)
ras.443.crop<-crop(ras.443, myext)
ras.443.crop[ras.443.crop<0]<-NA
ras.443.crop[ras.443.crop>90000]<- NA

d <- data.frame(x = values(longitude), y = values(latitude), r469 =
values(r469))
d$r469[is.na(d$r469)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r469', cell.size=0.009337697,
method='raster')
ras.469 <- raster(dd)
ras.469.crop<-crop(ras.469, myext)
ras.469.crop[ras.469.crop<0]<-NA
ras.469.crop[ras.469.crop>90000]<- NA

d <- data.frame(x = values(longitude), y = values(latitude), r488 =
values(r488))
d$r488[is.na(d$r488)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r488', cell.size=0.009337697,
method='raster')
ras.488 <- raster(dd)
ras.488.crop<-crop(ras.488, myext)
ras.488.crop[ras.488.crop<0]<-NA
ras.488.crop[ras.488.crop>90000]<- NA

d <- data.frame(x = values(longitude), y = values(latitude), r531 =
values(r531))
d$r531[is.na(d$r531)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r531', cell.size=0.009337697,
method='raster')
ras.531 <- raster(dd)
ras.531.crop<-crop(ras.531, myext)
ras.531.crop[ras.531.crop<0]<-NA
ras.531.crop[ras.531.crop>90000]<- NA

d <- data.frame(x = values(longitude), y = values(latitude), r547 =
values(r547))
d$r547[is.na(d$r547)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r547', cell.size=0.009337697,
method='raster')
ras.547 <- raster(dd)
ras.547.crop<-crop(ras.547, myext)
ras.547.crop[ras.547.crop<0]<-NA
ras.547.crop[ras.547.crop>90000]<- NA

d <- data.frame(x = values(longitude), y = values(latitude), r555 =
values(r555))
d$r555[is.na(d$r555)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r555', cell.size=0.009337697,
method='raster')
ras.555 <- raster(dd)
ras.555.crop<-crop(ras.555, myext)
ras.555.crop[ras.555.crop<0]<-NA
ras.555.crop[ras.555.crop>90000]<- NA

d <- data.frame(x = values(longitude), y = values(latitude), r645 =
values(r645))
d$r645[is.na(d$r645)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r645', cell.size=0.009337697,
method='raster')
ras.645 <- raster(dd)
ras.645.crop<-crop(ras.645, myext)
ras.645.crop[ras.645.crop<0]<-NA
ras.645.crop[ras.645.crop>90000]<- NA

d <- data.frame(x = values(longitude), y = values(latitude), r667 =
values(r667))
d$r667[is.na(d$r667)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r667', cell.size=0.009337697,
method='raster')
ras.667 <- raster(dd)
ras.667.crop<-crop(ras.667, myext)
ras.667.crop[ras.667.crop<0]<-NA
ras.667.crop[ras.667.crop>90000]<- NA

d <- data.frame(x = values(longitude), y = values(latitude), r678 =
values(r678))
d$r678[is.na(d$r678)]<- 99999
coordinates(d)<-~x+y
dd<-vect2rast.SpatialPoints(d, fname ='r678', cell.size=0.009337697,
method='raster')
ras.678 <- raster(dd)
ras.678.crop<-crop(ras.678, myext)
ras.678.crop[ras.678.crop<0]<-NA
ras.678.crop[ras.678.crop>90000]<- NA

myext <- extent(-84.9, -79.6, 42.9, 46.6) # Approx. extent that will cover
all of Lake Huron
correct.crop<-crop(correct, myext)
ras.412.r<-resample(ras.412.crop, correct.crop, method='bilinear')
ras.443.r<-resample(ras.443.crop, correct.crop, method='bilinear')
ras.469.r<-resample(ras.469.crop, correct.crop, method='bilinear')
ras.488.r<-resample(ras.488.crop, correct.crop, method='bilinear')
ras.531.r<-resample(ras.531.crop, correct.crop, method='bilinear')
ras.547.r<-resample(ras.547.crop, correct.crop, method='bilinear')
ras.555.r<-resample(ras.555.crop, correct.crop, method='bilinear')
ras.645.r<-resample(ras.645.crop, correct.crop, method='bilinear')
ras.667.r<-resample(ras.667.crop, correct.crop, method='bilinear')
ras.678.r<-resample(ras.678.crop, correct.crop, method='bilinear')
End<-Sys.time()
difftime(End, start)

#plot original unmapped nc band Rrs_412 versus mapped version for gross
comparison
par(mfrow=c(2,1), mar=c(2,2,2,2))
my.orig.extent<-extent(100,475,200,600)
r412.crop<-crop(r412, my.orig.extent)
plot(r412.crop, main='Original unmapped')
plot(ras.412.r, main='Mapped in R')

-- 
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392

	[[alternative HTML version deleted]]


From javiermoreira at gmail.com  Tue May 23 17:23:08 2017
From: javiermoreira at gmail.com (Javier Moreira)
Date: Tue, 23 May 2017 12:23:08 -0300
Subject: [R-sig-Geo] gstat or geoR create variogram with sets of bounding
	boxes
Message-ID: <CAEyHP-0NAmTBHsdqjCgtVQOcUmyj=eU5WnAqBD98wqzacMurow@mail.gmail.com>

hi,
im trying to make a variogram that consider a limit made by a bounding box,
but not just one, but dividing the area of the hole set of points in
different subzones (treatments in this case).
I have tried to do so with bounding box in geoR, but i cant do it.
Also, another way its to use anisotropy, but when try to put an alpha
parameter in geostat, doesnt make any change that without it.
bottom line, i have to perform a variogram that includes only the points in
1 direction, or, tell the variogram to limit the surface to a polygon (
several ones)

if any one can help,
thanks

-- 
Javier Moreira de Souza
Ingeniero Agr?nomo
099 406 006

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Tue May 23 17:39:49 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Tue, 23 May 2017 15:39:49 +0000
Subject: [R-sig-Geo] netcdf file mapping and dimensions issue
In-Reply-To: <CA+Y23taE+BO3iTB7gqzoPeJU1D=_h5hu7-5bAstHRERq0uCKzA@mail.gmail.com>
References: <CA+Y23taE+BO3iTB7gqzoPeJU1D=_h5hu7-5bAstHRERq0uCKzA@mail.gmail.com>
Message-ID: <CAAcGz9-GUVB4HgQDhR6doUS5pFww3TCCxcA_x7hKwBiCoDKDRQ@mail.gmail.com>

I tried a few pretty unpromising things and then pointed gdalwarp at it.
Seems fine, gdalwarp knows how to find geolocation arrays and use them, and
it will auto-choose an output grid, and you can provide a custom one (i.e.
with a local projection.

The SDS (subdataset) naming thing is a bit awkward until you get used to
it.  This was just GDAL 2.1.2 on Ubuntu.


f <- "
https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.L2_LAC_OC.x.nc"
download.file(f, basename(f), mode = "wb")

system('gdalwarp
HDF5:"A2011202183000.L2_LAC_OC.x.nc"://geophysical_data/chl_ocx
chl_ocx .tif')
Creating output file that is 783P x 530L.
Processing input file HDF5:A2011202183000.L2_LAC_OC.x.nc
://geophysical_data/chl_ocx.
0...10...20...30...40...50...60...70...80...90...100 - done.
r <- raster("chl_ocx ")
r
class       : RasterLayer
dimensions  : 530, 783, 414990  (nrow, ncol, ncell)
resolution  : 0.01224719, 0.01224719  (x, y)
extent      : -86.99178, -77.40223, 40.73677, 47.22778  (xmin, xmax, ymin,
ymax)
coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84
+towgs84=0,0,0

#plot(r)
#library(mapdata)
#map("worldHires", add = TRUE)

Cheers, Mike.


On Tue, 23 May 2017 at 23:58 Warner, David <dmwarner at usgs.gov> wrote:

> Greetings all
>
> I am working nc files from the Ocean Biology Procesing Group.  The files
> are MODIS aqua L2 ocean color files.
>
> These files are not mapped or perhaps some would say orthorectified such
> that when you create a rasterLayer from one of the contained variables and
> plot it, the location and orientation of features is incorrect.  For
> example, Georgian Bay, a large eastern subset of Lake Huron in the US and
> Canada, may well show up west of the lake.
>
> The dimensions of the data are read as column and row numbers rather than
> latitude and longitude.
>
> I am looking for a way to map these data in R other than my current
> method.  The current method for a single nc file is shown below.  I run
> this modified to fit into a parallelized foreach() loop that is used to
> process folders of nc files.
>
> The  basic steps that map the nc file data are 1) create a data.frame from
> the nc files measured variable(s) and the latitude and longitude then
> rasterize using vect2rast.SpatialPoints() and 2) resample the rasterLayer
> created from the data.frame using resample() to an nc file I reprojected
> using SeaDAS.  This seems overly complicated considering the fact that I
> start with a raster and it is pretty slow (30-40 seconds per nc file in
> parallel, nearly two minutes per file in the script example here).
>
> I have two questions are 1) is there a way to eliminate the used of
> vect2rast.SpatialPoints(), which is the slowest part of the process and 2)
> are there any other ways to perhaps speed the process up if I can't
> eliminate vect2rast.SpatialPoints().  I have tried rasterize() and that
> does not provide the correct product.  It recreates what I started with,
> unmapped data.  What am I missing, or is this just a function of the data I
> am using?
>
>
> Script is below.  I have included links to the two data files required in
> the process.  One is the nc file,
> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.L2_LAC_OC.x.nc
> the other is the reprojected version of this file from SeaDAS.
>
> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.reprojected.tif
> To use them you will have to change the path to the file locations.
>
> Thanks for your patience with my writing and poorly written code.  Any
> ideas would be helpful.
>
> Dave
>
> library(raster)
> library(plotKML)
>
> rasterOptions(maxmemory = 1e+09)
> start <- Sys.time()
> #Create rasterLayers from the bands of nc file that are required for
> mapping, with r412 being the measured variable of interest
> r412<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_412')
> r443<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_443')
> r469<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_469')
> r488<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_488')
> r531<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_531')
> r547<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_547')
> r555<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_555')
> r645<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_645')
> r667<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_667')
> r678<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var='geophysical_data/Rrs_678')
>
> longitude <-
> raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var="navigation_data/longitude")
> latitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
> var="navigation_data/latitude")
> r412  # shows that dimensions are row and column numbers, not lat and long
>
>
> #Import single reprojected band of the nc file as geotif, with reprojection
> done manually in SeaDAS software
> correct<-raster("/Users/dmwarner/Documents/MODIS/OC/
> correct/Huron/A2011202183000.reprojected.tif")
>
>
> #now rasterize and map the nc file after
>
> d <- data.frame(x = values(longitude), y = values(latitude), r412 =
> values(r412))
> d$r412[is.na(d$r412)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r412', cell.size=0.009337697,
> method='raster')
> ras.412 <- raster(dd)
> ras.412.crop<-crop(ras.412, myext)
> #ras.412.crop[ras.412.crop<0]<-NA
> ras.412.crop[ras.412.crop>90000]<- NA
>
>
> d <- data.frame(x = values(longitude), y = values(latitude), r443 =
> values(r443))
> d$r443[is.na(d$r443)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r443', cell.size=0.009337697,
> method='raster')
> ras.443 <- raster(dd)
> ras.443.crop<-crop(ras.443, myext)
> ras.443.crop[ras.443.crop<0]<-NA
> ras.443.crop[ras.443.crop>90000]<- NA
>
> d <- data.frame(x = values(longitude), y = values(latitude), r469 =
> values(r469))
> d$r469[is.na(d$r469)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r469', cell.size=0.009337697,
> method='raster')
> ras.469 <- raster(dd)
> ras.469.crop<-crop(ras.469, myext)
> ras.469.crop[ras.469.crop<0]<-NA
> ras.469.crop[ras.469.crop>90000]<- NA
>
> d <- data.frame(x = values(longitude), y = values(latitude), r488 =
> values(r488))
> d$r488[is.na(d$r488)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r488', cell.size=0.009337697,
> method='raster')
> ras.488 <- raster(dd)
> ras.488.crop<-crop(ras.488, myext)
> ras.488.crop[ras.488.crop<0]<-NA
> ras.488.crop[ras.488.crop>90000]<- NA
>
> d <- data.frame(x = values(longitude), y = values(latitude), r531 =
> values(r531))
> d$r531[is.na(d$r531)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r531', cell.size=0.009337697,
> method='raster')
> ras.531 <- raster(dd)
> ras.531.crop<-crop(ras.531, myext)
> ras.531.crop[ras.531.crop<0]<-NA
> ras.531.crop[ras.531.crop>90000]<- NA
>
> d <- data.frame(x = values(longitude), y = values(latitude), r547 =
> values(r547))
> d$r547[is.na(d$r547)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r547', cell.size=0.009337697,
> method='raster')
> ras.547 <- raster(dd)
> ras.547.crop<-crop(ras.547, myext)
> ras.547.crop[ras.547.crop<0]<-NA
> ras.547.crop[ras.547.crop>90000]<- NA
>
> d <- data.frame(x = values(longitude), y = values(latitude), r555 =
> values(r555))
> d$r555[is.na(d$r555)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r555', cell.size=0.009337697,
> method='raster')
> ras.555 <- raster(dd)
> ras.555.crop<-crop(ras.555, myext)
> ras.555.crop[ras.555.crop<0]<-NA
> ras.555.crop[ras.555.crop>90000]<- NA
>
> d <- data.frame(x = values(longitude), y = values(latitude), r645 =
> values(r645))
> d$r645[is.na(d$r645)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r645', cell.size=0.009337697,
> method='raster')
> ras.645 <- raster(dd)
> ras.645.crop<-crop(ras.645, myext)
> ras.645.crop[ras.645.crop<0]<-NA
> ras.645.crop[ras.645.crop>90000]<- NA
>
> d <- data.frame(x = values(longitude), y = values(latitude), r667 =
> values(r667))
> d$r667[is.na(d$r667)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r667', cell.size=0.009337697,
> method='raster')
> ras.667 <- raster(dd)
> ras.667.crop<-crop(ras.667, myext)
> ras.667.crop[ras.667.crop<0]<-NA
> ras.667.crop[ras.667.crop>90000]<- NA
>
> d <- data.frame(x = values(longitude), y = values(latitude), r678 =
> values(r678))
> d$r678[is.na(d$r678)]<- 99999
> coordinates(d)<-~x+y
> dd<-vect2rast.SpatialPoints(d, fname ='r678', cell.size=0.009337697,
> method='raster')
> ras.678 <- raster(dd)
> ras.678.crop<-crop(ras.678, myext)
> ras.678.crop[ras.678.crop<0]<-NA
> ras.678.crop[ras.678.crop>90000]<- NA
>
> myext <- extent(-84.9, -79.6, 42.9, 46.6) # Approx. extent that will cover
> all of Lake Huron
> correct.crop<-crop(correct, myext)
> ras.412.r<-resample(ras.412.crop, correct.crop, method='bilinear')
> ras.443.r<-resample(ras.443.crop, correct.crop, method='bilinear')
> ras.469.r<-resample(ras.469.crop, correct.crop, method='bilinear')
> ras.488.r<-resample(ras.488.crop, correct.crop, method='bilinear')
> ras.531.r<-resample(ras.531.crop, correct.crop, method='bilinear')
> ras.547.r<-resample(ras.547.crop, correct.crop, method='bilinear')
> ras.555.r<-resample(ras.555.crop, correct.crop, method='bilinear')
> ras.645.r<-resample(ras.645.crop, correct.crop, method='bilinear')
> ras.667.r<-resample(ras.667.crop, correct.crop, method='bilinear')
> ras.678.r<-resample(ras.678.crop, correct.crop, method='bilinear')
> End<-Sys.time()
> difftime(End, start)
>
> #plot original unmapped nc band Rrs_412 versus mapped version for gross
> comparison
> par(mfrow=c(2,1), mar=c(2,2,2,2))
> my.orig.extent<-extent(100,475,200,600)
> r412.crop<-crop(r412, my.orig.extent)
> plot(r412.crop, main='Original unmapped')
> plot(ras.412.r, main='Mapped in R')
>
> --
> David Warner
> Research Fisheries Biologist
> U.S.G.S. Great Lakes Science Center
> 1451 Green Road
> Ann Arbor, MI 48105
> 734-214-9392 <(734)%20214-9392>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From dmwarner at usgs.gov  Tue May 23 17:43:39 2017
From: dmwarner at usgs.gov (Warner, David)
Date: Tue, 23 May 2017 11:43:39 -0400
Subject: [R-sig-Geo] netcdf file mapping and dimensions issue
In-Reply-To: <CAAcGz9-GUVB4HgQDhR6doUS5pFww3TCCxcA_x7hKwBiCoDKDRQ@mail.gmail.com>
References: <CA+Y23taE+BO3iTB7gqzoPeJU1D=_h5hu7-5bAstHRERq0uCKzA@mail.gmail.com>
 <CAAcGz9-GUVB4HgQDhR6doUS5pFww3TCCxcA_x7hKwBiCoDKDRQ@mail.gmail.com>
Message-ID: <CA+Y23tavtC33ErC8L1TsnbXTD=jDTNVmeRK5-NoSjdVsAQ6iPg@mail.gmail.com>

This looks promising!  Thank you!
Dave

On Tue, May 23, 2017 at 11:39 AM, Michael Sumner <mdsumner at gmail.com> wrote:

>
> I tried a few pretty unpromising things and then pointed gdalwarp at it.
> Seems fine, gdalwarp knows how to find geolocation arrays and use them, and
> it will auto-choose an output grid, and you can provide a custom one (i.e.
> with a local projection.
>
> The SDS (subdataset) naming thing is a bit awkward until you get used to
> it.  This was just GDAL 2.1.2 on Ubuntu.
>
>
> f <- "https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
> A2011202183000.L2_LAC_OC.x.nc"
> download.file(f, basename(f), mode = "wb")
>
> system('gdalwarp HDF5:"A2011202183000.L2_LAC_OC.x.nc"://geophysical_data/chl_ocx
> chl_ocx .tif')
> Creating output file that is 783P x 530L.
> Processing input file HDF5:A2011202183000.L2_LAC_OC.x.nc
> ://geophysical_data/chl_ocx.
> 0...10...20...30...40...50...60...70...80...90...100 - done.
> r <- raster("chl_ocx ")
> r
> class       : RasterLayer
> dimensions  : 530, 783, 414990  (nrow, ncol, ncell)
> resolution  : 0.01224719, 0.01224719  (x, y)
> extent      : -86.99178, -77.40223, 40.73677, 47.22778  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84
> +towgs84=0,0,0
>
> #plot(r)
> #library(mapdata)
> #map("worldHires", add = TRUE)
>
> Cheers, Mike.
>
>
> On Tue, 23 May 2017 at 23:58 Warner, David <dmwarner at usgs.gov> wrote:
>
>> Greetings all
>>
>> I am working nc files from the Ocean Biology Procesing Group.  The files
>> are MODIS aqua L2 ocean color files.
>>
>> These files are not mapped or perhaps some would say orthorectified such
>> that when you create a rasterLayer from one of the contained variables and
>> plot it, the location and orientation of features is incorrect.  For
>> example, Georgian Bay, a large eastern subset of Lake Huron in the US and
>> Canada, may well show up west of the lake.
>>
>> The dimensions of the data are read as column and row numbers rather than
>> latitude and longitude.
>>
>> I am looking for a way to map these data in R other than my current
>> method.  The current method for a single nc file is shown below.  I run
>> this modified to fit into a parallelized foreach() loop that is used to
>> process folders of nc files.
>>
>> The  basic steps that map the nc file data are 1) create a data.frame from
>> the nc files measured variable(s) and the latitude and longitude then
>> rasterize using vect2rast.SpatialPoints() and 2) resample the rasterLayer
>> created from the data.frame using resample() to an nc file I reprojected
>> using SeaDAS.  This seems overly complicated considering the fact that I
>> start with a raster and it is pretty slow (30-40 seconds per nc file in
>> parallel, nearly two minutes per file in the script example here).
>>
>> I have two questions are 1) is there a way to eliminate the used of
>> vect2rast.SpatialPoints(), which is the slowest part of the process and 2)
>> are there any other ways to perhaps speed the process up if I can't
>> eliminate vect2rast.SpatialPoints().  I have tried rasterize() and that
>> does not provide the correct product.  It recreates what I started with,
>> unmapped data.  What am I missing, or is this just a function of the data
>> I
>> am using?
>>
>>
>> Script is below.  I have included links to the two data files required in
>> the process.  One is the nc file,
>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
>> A2011202183000.L2_LAC_OC.x.nc
>> the other is the reprojected version of this file from SeaDAS.
>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
>> A2011202183000.reprojected.tif
>> To use them you will have to change the path to the file locations.
>>
>> Thanks for your patience with my writing and poorly written code.  Any
>> ideas would be helpful.
>>
>> Dave
>>
>> library(raster)
>> library(plotKML)
>>
>> rasterOptions(maxmemory = 1e+09)
>> start <- Sys.time()
>> #Create rasterLayers from the bands of nc file that are required for
>> mapping, with r412 being the measured variable of interest
>> r412<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_412')
>> r443<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_443')
>> r469<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_469')
>> r488<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_488')
>> r531<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_531')
>> r547<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_547')
>> r555<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_555')
>> r645<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_645')
>> r667<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_667')
>> r678<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_678')
>>
>> longitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/
>> Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var="navigation_data/longitude")
>> latitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/
>> Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var="navigation_data/latitude")
>> r412  # shows that dimensions are row and column numbers, not lat and long
>>
>>
>> #Import single reprojected band of the nc file as geotif, with
>> reprojection
>> done manually in SeaDAS software
>> correct<-raster("/Users/dmwarner/Documents/MODIS/OC/
>> correct/Huron/A2011202183000.reprojected.tif")
>>
>>
>> #now rasterize and map the nc file after
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r412 =
>> values(r412))
>> d$r412[is.na(d$r412)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r412', cell.size=0.009337697,
>> method='raster')
>> ras.412 <- raster(dd)
>> ras.412.crop<-crop(ras.412, myext)
>> #ras.412.crop[ras.412.crop<0]<-NA
>> ras.412.crop[ras.412.crop>90000]<- NA
>>
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r443 =
>> values(r443))
>> d$r443[is.na(d$r443)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r443', cell.size=0.009337697,
>> method='raster')
>> ras.443 <- raster(dd)
>> ras.443.crop<-crop(ras.443, myext)
>> ras.443.crop[ras.443.crop<0]<-NA
>> ras.443.crop[ras.443.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r469 =
>> values(r469))
>> d$r469[is.na(d$r469)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r469', cell.size=0.009337697,
>> method='raster')
>> ras.469 <- raster(dd)
>> ras.469.crop<-crop(ras.469, myext)
>> ras.469.crop[ras.469.crop<0]<-NA
>> ras.469.crop[ras.469.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r488 =
>> values(r488))
>> d$r488[is.na(d$r488)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r488', cell.size=0.009337697,
>> method='raster')
>> ras.488 <- raster(dd)
>> ras.488.crop<-crop(ras.488, myext)
>> ras.488.crop[ras.488.crop<0]<-NA
>> ras.488.crop[ras.488.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r531 =
>> values(r531))
>> d$r531[is.na(d$r531)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r531', cell.size=0.009337697,
>> method='raster')
>> ras.531 <- raster(dd)
>> ras.531.crop<-crop(ras.531, myext)
>> ras.531.crop[ras.531.crop<0]<-NA
>> ras.531.crop[ras.531.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r547 =
>> values(r547))
>> d$r547[is.na(d$r547)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r547', cell.size=0.009337697,
>> method='raster')
>> ras.547 <- raster(dd)
>> ras.547.crop<-crop(ras.547, myext)
>> ras.547.crop[ras.547.crop<0]<-NA
>> ras.547.crop[ras.547.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r555 =
>> values(r555))
>> d$r555[is.na(d$r555)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r555', cell.size=0.009337697,
>> method='raster')
>> ras.555 <- raster(dd)
>> ras.555.crop<-crop(ras.555, myext)
>> ras.555.crop[ras.555.crop<0]<-NA
>> ras.555.crop[ras.555.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r645 =
>> values(r645))
>> d$r645[is.na(d$r645)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r645', cell.size=0.009337697,
>> method='raster')
>> ras.645 <- raster(dd)
>> ras.645.crop<-crop(ras.645, myext)
>> ras.645.crop[ras.645.crop<0]<-NA
>> ras.645.crop[ras.645.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r667 =
>> values(r667))
>> d$r667[is.na(d$r667)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r667', cell.size=0.009337697,
>> method='raster')
>> ras.667 <- raster(dd)
>> ras.667.crop<-crop(ras.667, myext)
>> ras.667.crop[ras.667.crop<0]<-NA
>> ras.667.crop[ras.667.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r678 =
>> values(r678))
>> d$r678[is.na(d$r678)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r678', cell.size=0.009337697,
>> method='raster')
>> ras.678 <- raster(dd)
>> ras.678.crop<-crop(ras.678, myext)
>> ras.678.crop[ras.678.crop<0]<-NA
>> ras.678.crop[ras.678.crop>90000]<- NA
>>
>> myext <- extent(-84.9, -79.6, 42.9, 46.6) # Approx. extent that will cover
>> all of Lake Huron
>> correct.crop<-crop(correct, myext)
>> ras.412.r<-resample(ras.412.crop, correct.crop, method='bilinear')
>> ras.443.r<-resample(ras.443.crop, correct.crop, method='bilinear')
>> ras.469.r<-resample(ras.469.crop, correct.crop, method='bilinear')
>> ras.488.r<-resample(ras.488.crop, correct.crop, method='bilinear')
>> ras.531.r<-resample(ras.531.crop, correct.crop, method='bilinear')
>> ras.547.r<-resample(ras.547.crop, correct.crop, method='bilinear')
>> ras.555.r<-resample(ras.555.crop, correct.crop, method='bilinear')
>> ras.645.r<-resample(ras.645.crop, correct.crop, method='bilinear')
>> ras.667.r<-resample(ras.667.crop, correct.crop, method='bilinear')
>> ras.678.r<-resample(ras.678.crop, correct.crop, method='bilinear')
>> End<-Sys.time()
>> difftime(End, start)
>>
>> #plot original unmapped nc band Rrs_412 versus mapped version for gross
>> comparison
>> par(mfrow=c(2,1), mar=c(2,2,2,2))
>> my.orig.extent<-extent(100,475,200,600)
>> r412.crop<-crop(r412, my.orig.extent)
>> plot(r412.crop, main='Original unmapped')
>> plot(ras.412.r, main='Mapped in R')
>>
>> --
>> David Warner
>> Research Fisheries Biologist
>> U.S.G.S. Great Lakes Science Center
>> 1451 Green Road
>> Ann Arbor, MI 48105
>> 734-214-9392 <(734)%20214-9392>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>


-- 
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392

	[[alternative HTML version deleted]]


From dmwarner at usgs.gov  Tue May 23 18:33:40 2017
From: dmwarner at usgs.gov (Warner, David)
Date: Tue, 23 May 2017 12:33:40 -0400
Subject: [R-sig-Geo] netcdf file mapping and dimensions issue
In-Reply-To: <CAAcGz9-GUVB4HgQDhR6doUS5pFww3TCCxcA_x7hKwBiCoDKDRQ@mail.gmail.com>
References: <CA+Y23taE+BO3iTB7gqzoPeJU1D=_h5hu7-5bAstHRERq0uCKzA@mail.gmail.com>
 <CAAcGz9-GUVB4HgQDhR6doUS5pFww3TCCxcA_x7hKwBiCoDKDRQ@mail.gmail.com>
Message-ID: <CA+Y23taFuhZTeP8QWyh3T7m_tki1z-Xz_9jY7THRsdssNH5SaA@mail.gmail.com>

Now if I can just get calls to gdal functions to work from within R....


On Tue, May 23, 2017 at 11:39 AM, Michael Sumner <mdsumner at gmail.com> wrote:

>
> I tried a few pretty unpromising things and then pointed gdalwarp at it.
> Seems fine, gdalwarp knows how to find geolocation arrays and use them, and
> it will auto-choose an output grid, and you can provide a custom one (i.e.
> with a local projection.
>
> The SDS (subdataset) naming thing is a bit awkward until you get used to
> it.  This was just GDAL 2.1.2 on Ubuntu.
>
>
> f <- "https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
> A2011202183000.L2_LAC_OC.x.nc"
> download.file(f, basename(f), mode = "wb")
>
> system('gdalwarp HDF5:"A2011202183000.L2_LAC_OC.x.nc"://geophysical_data/chl_ocx
> chl_ocx .tif')
> Creating output file that is 783P x 530L.
> Processing input file HDF5:A2011202183000.L2_LAC_OC.x.nc
> ://geophysical_data/chl_ocx.
> 0...10...20...30...40...50...60...70...80...90...100 - done.
> r <- raster("chl_ocx ")
> r
> class       : RasterLayer
> dimensions  : 530, 783, 414990  (nrow, ncol, ncell)
> resolution  : 0.01224719, 0.01224719  (x, y)
> extent      : -86.99178, -77.40223, 40.73677, 47.22778  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84
> +towgs84=0,0,0
>
> #plot(r)
> #library(mapdata)
> #map("worldHires", add = TRUE)
>
> Cheers, Mike.
>
>
> On Tue, 23 May 2017 at 23:58 Warner, David <dmwarner at usgs.gov> wrote:
>
>> Greetings all
>>
>> I am working nc files from the Ocean Biology Procesing Group.  The files
>> are MODIS aqua L2 ocean color files.
>>
>> These files are not mapped or perhaps some would say orthorectified such
>> that when you create a rasterLayer from one of the contained variables and
>> plot it, the location and orientation of features is incorrect.  For
>> example, Georgian Bay, a large eastern subset of Lake Huron in the US and
>> Canada, may well show up west of the lake.
>>
>> The dimensions of the data are read as column and row numbers rather than
>> latitude and longitude.
>>
>> I am looking for a way to map these data in R other than my current
>> method.  The current method for a single nc file is shown below.  I run
>> this modified to fit into a parallelized foreach() loop that is used to
>> process folders of nc files.
>>
>> The  basic steps that map the nc file data are 1) create a data.frame from
>> the nc files measured variable(s) and the latitude and longitude then
>> rasterize using vect2rast.SpatialPoints() and 2) resample the rasterLayer
>> created from the data.frame using resample() to an nc file I reprojected
>> using SeaDAS.  This seems overly complicated considering the fact that I
>> start with a raster and it is pretty slow (30-40 seconds per nc file in
>> parallel, nearly two minutes per file in the script example here).
>>
>> I have two questions are 1) is there a way to eliminate the used of
>> vect2rast.SpatialPoints(), which is the slowest part of the process and 2)
>> are there any other ways to perhaps speed the process up if I can't
>> eliminate vect2rast.SpatialPoints().  I have tried rasterize() and that
>> does not provide the correct product.  It recreates what I started with,
>> unmapped data.  What am I missing, or is this just a function of the data
>> I
>> am using?
>>
>>
>> Script is below.  I have included links to the two data files required in
>> the process.  One is the nc file,
>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
>> A2011202183000.L2_LAC_OC.x.nc
>> the other is the reprojected version of this file from SeaDAS.
>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
>> A2011202183000.reprojected.tif
>> To use them you will have to change the path to the file locations.
>>
>> Thanks for your patience with my writing and poorly written code.  Any
>> ideas would be helpful.
>>
>> Dave
>>
>> library(raster)
>> library(plotKML)
>>
>> rasterOptions(maxmemory = 1e+09)
>> start <- Sys.time()
>> #Create rasterLayers from the bands of nc file that are required for
>> mapping, with r412 being the measured variable of interest
>> r412<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_412')
>> r443<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_443')
>> r469<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_469')
>> r488<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_488')
>> r531<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_531')
>> r547<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_547')
>> r555<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_555')
>> r645<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_645')
>> r667<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_667')
>> r678<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var='geophysical_data/Rrs_678')
>>
>> longitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/
>> Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var="navigation_data/longitude")
>> latitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/
>> Huron/2011/
>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>> var="navigation_data/latitude")
>> r412  # shows that dimensions are row and column numbers, not lat and long
>>
>>
>> #Import single reprojected band of the nc file as geotif, with
>> reprojection
>> done manually in SeaDAS software
>> correct<-raster("/Users/dmwarner/Documents/MODIS/OC/
>> correct/Huron/A2011202183000.reprojected.tif")
>>
>>
>> #now rasterize and map the nc file after
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r412 =
>> values(r412))
>> d$r412[is.na(d$r412)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r412', cell.size=0.009337697,
>> method='raster')
>> ras.412 <- raster(dd)
>> ras.412.crop<-crop(ras.412, myext)
>> #ras.412.crop[ras.412.crop<0]<-NA
>> ras.412.crop[ras.412.crop>90000]<- NA
>>
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r443 =
>> values(r443))
>> d$r443[is.na(d$r443)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r443', cell.size=0.009337697,
>> method='raster')
>> ras.443 <- raster(dd)
>> ras.443.crop<-crop(ras.443, myext)
>> ras.443.crop[ras.443.crop<0]<-NA
>> ras.443.crop[ras.443.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r469 =
>> values(r469))
>> d$r469[is.na(d$r469)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r469', cell.size=0.009337697,
>> method='raster')
>> ras.469 <- raster(dd)
>> ras.469.crop<-crop(ras.469, myext)
>> ras.469.crop[ras.469.crop<0]<-NA
>> ras.469.crop[ras.469.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r488 =
>> values(r488))
>> d$r488[is.na(d$r488)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r488', cell.size=0.009337697,
>> method='raster')
>> ras.488 <- raster(dd)
>> ras.488.crop<-crop(ras.488, myext)
>> ras.488.crop[ras.488.crop<0]<-NA
>> ras.488.crop[ras.488.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r531 =
>> values(r531))
>> d$r531[is.na(d$r531)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r531', cell.size=0.009337697,
>> method='raster')
>> ras.531 <- raster(dd)
>> ras.531.crop<-crop(ras.531, myext)
>> ras.531.crop[ras.531.crop<0]<-NA
>> ras.531.crop[ras.531.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r547 =
>> values(r547))
>> d$r547[is.na(d$r547)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r547', cell.size=0.009337697,
>> method='raster')
>> ras.547 <- raster(dd)
>> ras.547.crop<-crop(ras.547, myext)
>> ras.547.crop[ras.547.crop<0]<-NA
>> ras.547.crop[ras.547.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r555 =
>> values(r555))
>> d$r555[is.na(d$r555)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r555', cell.size=0.009337697,
>> method='raster')
>> ras.555 <- raster(dd)
>> ras.555.crop<-crop(ras.555, myext)
>> ras.555.crop[ras.555.crop<0]<-NA
>> ras.555.crop[ras.555.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r645 =
>> values(r645))
>> d$r645[is.na(d$r645)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r645', cell.size=0.009337697,
>> method='raster')
>> ras.645 <- raster(dd)
>> ras.645.crop<-crop(ras.645, myext)
>> ras.645.crop[ras.645.crop<0]<-NA
>> ras.645.crop[ras.645.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r667 =
>> values(r667))
>> d$r667[is.na(d$r667)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r667', cell.size=0.009337697,
>> method='raster')
>> ras.667 <- raster(dd)
>> ras.667.crop<-crop(ras.667, myext)
>> ras.667.crop[ras.667.crop<0]<-NA
>> ras.667.crop[ras.667.crop>90000]<- NA
>>
>> d <- data.frame(x = values(longitude), y = values(latitude), r678 =
>> values(r678))
>> d$r678[is.na(d$r678)]<- 99999
>> coordinates(d)<-~x+y
>> dd<-vect2rast.SpatialPoints(d, fname ='r678', cell.size=0.009337697,
>> method='raster')
>> ras.678 <- raster(dd)
>> ras.678.crop<-crop(ras.678, myext)
>> ras.678.crop[ras.678.crop<0]<-NA
>> ras.678.crop[ras.678.crop>90000]<- NA
>>
>> myext <- extent(-84.9, -79.6, 42.9, 46.6) # Approx. extent that will cover
>> all of Lake Huron
>> correct.crop<-crop(correct, myext)
>> ras.412.r<-resample(ras.412.crop, correct.crop, method='bilinear')
>> ras.443.r<-resample(ras.443.crop, correct.crop, method='bilinear')
>> ras.469.r<-resample(ras.469.crop, correct.crop, method='bilinear')
>> ras.488.r<-resample(ras.488.crop, correct.crop, method='bilinear')
>> ras.531.r<-resample(ras.531.crop, correct.crop, method='bilinear')
>> ras.547.r<-resample(ras.547.crop, correct.crop, method='bilinear')
>> ras.555.r<-resample(ras.555.crop, correct.crop, method='bilinear')
>> ras.645.r<-resample(ras.645.crop, correct.crop, method='bilinear')
>> ras.667.r<-resample(ras.667.crop, correct.crop, method='bilinear')
>> ras.678.r<-resample(ras.678.crop, correct.crop, method='bilinear')
>> End<-Sys.time()
>> difftime(End, start)
>>
>> #plot original unmapped nc band Rrs_412 versus mapped version for gross
>> comparison
>> par(mfrow=c(2,1), mar=c(2,2,2,2))
>> my.orig.extent<-extent(100,475,200,600)
>> r412.crop<-crop(r412, my.orig.extent)
>> plot(r412.crop, main='Original unmapped')
>> plot(ras.412.r, main='Mapped in R')
>>
>> --
>> David Warner
>> Research Fisheries Biologist
>> U.S.G.S. Great Lakes Science Center
>> 1451 Green Road
>> Ann Arbor, MI 48105
>> 734-214-9392 <(734)%20214-9392>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>


-- 
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Wed May 24 00:04:36 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Tue, 23 May 2017 22:04:36 +0000
Subject: [R-sig-Geo] netcdf file mapping and dimensions issue
In-Reply-To: <CA+Y23taFuhZTeP8QWyh3T7m_tki1z-Xz_9jY7THRsdssNH5SaA@mail.gmail.com>
References: <CA+Y23taE+BO3iTB7gqzoPeJU1D=_h5hu7-5bAstHRERq0uCKzA@mail.gmail.com>
 <CAAcGz9-GUVB4HgQDhR6doUS5pFww3TCCxcA_x7hKwBiCoDKDRQ@mail.gmail.com>
 <CA+Y23taFuhZTeP8QWyh3T7m_tki1z-Xz_9jY7THRsdssNH5SaA@mail.gmail.com>
Message-ID: <CAAcGz988p+Dp-aY1Z-Zb0Rrn108AKXO77S33jMNHNRdzSKcCMA@mail.gmail.com>

See gdalUtils for a wrapper, or ncdump to easily get the names of variables
and other metadata. I have workers L2 here that might help you batch this
and I'm happy to help:

https://github.com/mdsumner/roc/blob/master/R/readL2.R

We want to get deeper into this soon so I'll be looking at it.

There's possibly a way to do all the sds in one GDAL call too.

Cheers, Mike

On Wed, 24 May 2017, 02:34 Warner, David <dmwarner at usgs.gov> wrote:

> Now if I can just get calls to gdal functions to work from within R....
>
>
> On Tue, May 23, 2017 at 11:39 AM, Michael Sumner <mdsumner at gmail.com>
> wrote:
>
>>
>> I tried a few pretty unpromising things and then pointed gdalwarp at it.
>> Seems fine, gdalwarp knows how to find geolocation arrays and use them, and
>> it will auto-choose an output grid, and you can provide a custom one (i.e.
>> with a local projection.
>>
>> The SDS (subdataset) naming thing is a bit awkward until you get used to
>> it.  This was just GDAL 2.1.2 on Ubuntu.
>>
>>
>> f <- "
>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.L2_LAC_OC.x.nc
>> "
>> download.file(f, basename(f), mode = "wb")
>>
>> system('gdalwarp HDF5:"A2011202183000.L2_LAC_OC.x.nc"://geophysical_data/chl_ocx
>> chl_ocx .tif')
>> Creating output file that is 783P x 530L.
>> Processing input file HDF5:A2011202183000.L2_LAC_OC.x.nc
>> ://geophysical_data/chl_ocx.
>> 0...10...20...30...40...50...60...70...80...90...100 - done.
>> r <- raster("chl_ocx ")
>> r
>> class       : RasterLayer
>> dimensions  : 530, 783, 414990  (nrow, ncol, ncell)
>> resolution  : 0.01224719, 0.01224719  (x, y)
>> extent      : -86.99178, -77.40223, 40.73677, 47.22778  (xmin, xmax,
>> ymin, ymax)
>> coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84
>> +towgs84=0,0,0
>>
>> #plot(r)
>> #library(mapdata)
>> #map("worldHires", add = TRUE)
>>
>> Cheers, Mike.
>>
>>
>> On Tue, 23 May 2017 at 23:58 Warner, David <dmwarner at usgs.gov> wrote:
>>
>>> Greetings all
>>>
>>> I am working nc files from the Ocean Biology Procesing Group.  The files
>>> are MODIS aqua L2 ocean color files.
>>>
>>> These files are not mapped or perhaps some would say orthorectified such
>>> that when you create a rasterLayer from one of the contained variables
>>> and
>>> plot it, the location and orientation of features is incorrect.  For
>>> example, Georgian Bay, a large eastern subset of Lake Huron in the US and
>>> Canada, may well show up west of the lake.
>>>
>>> The dimensions of the data are read as column and row numbers rather than
>>> latitude and longitude.
>>>
>>> I am looking for a way to map these data in R other than my current
>>> method.  The current method for a single nc file is shown below.  I run
>>> this modified to fit into a parallelized foreach() loop that is used to
>>> process folders of nc files.
>>>
>>> The  basic steps that map the nc file data are 1) create a data.frame
>>> from
>>> the nc files measured variable(s) and the latitude and longitude then
>>> rasterize using vect2rast.SpatialPoints() and 2) resample the rasterLayer
>>> created from the data.frame using resample() to an nc file I reprojected
>>> using SeaDAS.  This seems overly complicated considering the fact that I
>>> start with a raster and it is pretty slow (30-40 seconds per nc file in
>>> parallel, nearly two minutes per file in the script example here).
>>>
>>> I have two questions are 1) is there a way to eliminate the used of
>>> vect2rast.SpatialPoints(), which is the slowest part of the process and
>>> 2)
>>> are there any other ways to perhaps speed the process up if I can't
>>> eliminate vect2rast.SpatialPoints().  I have tried rasterize() and that
>>> does not provide the correct product.  It recreates what I started with,
>>> unmapped data.  What am I missing, or is this just a function of the
>>> data I
>>> am using?
>>>
>>>
>>> Script is below.  I have included links to the two data files required in
>>> the process.  One is the nc file,
>>>
>>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.L2_LAC_OC.x.nc
>>> the other is the reprojected version of this file from SeaDAS.
>>>
>>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.reprojected.tif
>>> To use them you will have to change the path to the file locations.
>>>
>>> Thanks for your patience with my writing and poorly written code.  Any
>>> ideas would be helpful.
>>>
>>> Dave
>>>
>>> library(raster)
>>> library(plotKML)
>>>
>>> rasterOptions(maxmemory = 1e+09)
>>> start <- Sys.time()
>>> #Create rasterLayers from the bands of nc file that are required for
>>> mapping, with r412 being the measured variable of interest
>>> r412<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_412')
>>> r443<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_443')
>>> r469<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_469')
>>> r488<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_488')
>>> r531<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_531')
>>> r547<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_547')
>>> r555<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_555')
>>> r645<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_645')
>>> r667<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_667')
>>> r678<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var='geophysical_data/Rrs_678')
>>>
>>> longitude <-
>>> raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var="navigation_data/longitude")
>>> latitude <-
>>> raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>> var="navigation_data/latitude")
>>> r412  # shows that dimensions are row and column numbers, not lat and
>>> long
>>>
>>>
>>> #Import single reprojected band of the nc file as geotif, with
>>> reprojection
>>> done manually in SeaDAS software
>>> correct<-raster("/Users/dmwarner/Documents/MODIS/OC/
>>> correct/Huron/A2011202183000.reprojected.tif")
>>>
>>>
>>> #now rasterize and map the nc file after
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r412 =
>>> values(r412))
>>> d$r412[is.na(d$r412)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r412', cell.size=0.009337697,
>>> method='raster')
>>> ras.412 <- raster(dd)
>>> ras.412.crop<-crop(ras.412, myext)
>>> #ras.412.crop[ras.412.crop<0]<-NA
>>> ras.412.crop[ras.412.crop>90000]<- NA
>>>
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r443 =
>>> values(r443))
>>> d$r443[is.na(d$r443)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r443', cell.size=0.009337697,
>>> method='raster')
>>> ras.443 <- raster(dd)
>>> ras.443.crop<-crop(ras.443, myext)
>>> ras.443.crop[ras.443.crop<0]<-NA
>>> ras.443.crop[ras.443.crop>90000]<- NA
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r469 =
>>> values(r469))
>>> d$r469[is.na(d$r469)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r469', cell.size=0.009337697,
>>> method='raster')
>>> ras.469 <- raster(dd)
>>> ras.469.crop<-crop(ras.469, myext)
>>> ras.469.crop[ras.469.crop<0]<-NA
>>> ras.469.crop[ras.469.crop>90000]<- NA
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r488 =
>>> values(r488))
>>> d$r488[is.na(d$r488)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r488', cell.size=0.009337697,
>>> method='raster')
>>> ras.488 <- raster(dd)
>>> ras.488.crop<-crop(ras.488, myext)
>>> ras.488.crop[ras.488.crop<0]<-NA
>>> ras.488.crop[ras.488.crop>90000]<- NA
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r531 =
>>> values(r531))
>>> d$r531[is.na(d$r531)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r531', cell.size=0.009337697,
>>> method='raster')
>>> ras.531 <- raster(dd)
>>> ras.531.crop<-crop(ras.531, myext)
>>> ras.531.crop[ras.531.crop<0]<-NA
>>> ras.531.crop[ras.531.crop>90000]<- NA
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r547 =
>>> values(r547))
>>> d$r547[is.na(d$r547)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r547', cell.size=0.009337697,
>>> method='raster')
>>> ras.547 <- raster(dd)
>>> ras.547.crop<-crop(ras.547, myext)
>>> ras.547.crop[ras.547.crop<0]<-NA
>>> ras.547.crop[ras.547.crop>90000]<- NA
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r555 =
>>> values(r555))
>>> d$r555[is.na(d$r555)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r555', cell.size=0.009337697,
>>> method='raster')
>>> ras.555 <- raster(dd)
>>> ras.555.crop<-crop(ras.555, myext)
>>> ras.555.crop[ras.555.crop<0]<-NA
>>> ras.555.crop[ras.555.crop>90000]<- NA
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r645 =
>>> values(r645))
>>> d$r645[is.na(d$r645)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r645', cell.size=0.009337697,
>>> method='raster')
>>> ras.645 <- raster(dd)
>>> ras.645.crop<-crop(ras.645, myext)
>>> ras.645.crop[ras.645.crop<0]<-NA
>>> ras.645.crop[ras.645.crop>90000]<- NA
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r667 =
>>> values(r667))
>>> d$r667[is.na(d$r667)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r667', cell.size=0.009337697,
>>> method='raster')
>>> ras.667 <- raster(dd)
>>> ras.667.crop<-crop(ras.667, myext)
>>> ras.667.crop[ras.667.crop<0]<-NA
>>> ras.667.crop[ras.667.crop>90000]<- NA
>>>
>>> d <- data.frame(x = values(longitude), y = values(latitude), r678 =
>>> values(r678))
>>> d$r678[is.na(d$r678)]<- 99999
>>> coordinates(d)<-~x+y
>>> dd<-vect2rast.SpatialPoints(d, fname ='r678', cell.size=0.009337697,
>>> method='raster')
>>> ras.678 <- raster(dd)
>>> ras.678.crop<-crop(ras.678, myext)
>>> ras.678.crop[ras.678.crop<0]<-NA
>>> ras.678.crop[ras.678.crop>90000]<- NA
>>>
>>> myext <- extent(-84.9, -79.6, 42.9, 46.6) # Approx. extent that will
>>> cover
>>> all of Lake Huron
>>> correct.crop<-crop(correct, myext)
>>> ras.412.r<-resample(ras.412.crop, correct.crop, method='bilinear')
>>> ras.443.r<-resample(ras.443.crop, correct.crop, method='bilinear')
>>> ras.469.r<-resample(ras.469.crop, correct.crop, method='bilinear')
>>> ras.488.r<-resample(ras.488.crop, correct.crop, method='bilinear')
>>> ras.531.r<-resample(ras.531.crop, correct.crop, method='bilinear')
>>> ras.547.r<-resample(ras.547.crop, correct.crop, method='bilinear')
>>> ras.555.r<-resample(ras.555.crop, correct.crop, method='bilinear')
>>> ras.645.r<-resample(ras.645.crop, correct.crop, method='bilinear')
>>> ras.667.r<-resample(ras.667.crop, correct.crop, method='bilinear')
>>> ras.678.r<-resample(ras.678.crop, correct.crop, method='bilinear')
>>> End<-Sys.time()
>>> difftime(End, start)
>>>
>>> #plot original unmapped nc band Rrs_412 versus mapped version for gross
>>> comparison
>>> par(mfrow=c(2,1), mar=c(2,2,2,2))
>>> my.orig.extent<-extent(100,475,200,600)
>>> r412.crop<-crop(r412, my.orig.extent)
>>> plot(r412.crop, main='Original unmapped')
>>> plot(ras.412.r, main='Mapped in R')
>>>
>>> --
>>> David Warner
>>> Research Fisheries Biologist
>>> U.S.G.S. Great Lakes Science Center
>>> 1451 Green Road
>>> Ann Arbor, MI 48105
>>> 734-214-9392 <(734)%20214-9392>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>> --
>> Dr. Michael Sumner
>> Software and Database Engineer
>> Australian Antarctic Division
>> 203 Channel Highway
>> Kingston Tasmania 7050 Australia
>>
>>
>
>
> --
> David Warner
> Research Fisheries Biologist
> U.S.G.S. Great Lakes Science Center
> 1451 Green Road
> Ann Arbor, MI 48105
> 734-214-9392
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From miriam.puets at thuenen.de  Wed May 24 09:12:22 2017
From: miriam.puets at thuenen.de (Miriam =?utf-8?B?UMO8dHM=?=)
Date: Wed, 24 May 2017 09:12:22 +0200 (CEST)
Subject: [R-sig-Geo] Deal with multiple factorlevel in one grid square
Message-ID: <1857226423.12600330.1495609942428.JavaMail.zimbra@thuenen.de>

Hi everyone,

I have the following problem: I have a personalized grid and a shape file with polygons representing sediment types. Now I would like to apply this grid to the Polygons to identify the sediment type most common within each grid. I tried it with rasterize, but here I can only chose last or first. Die you have any suggestions how I might get the sediment type for each raster cell?

Thank you for your help!

<?)))>< >?)))>< >?)))>< >?)))><

Miriam P?ts
Marine Lebende Ressourcen/ Marine Living Resources
Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries
Palmaille 9
22767 Hamburg (Germany)

Tel:  +49 40 38905-105 
Mail: miriam.puets at thuenen.de


From mdsumner at gmail.com  Wed May 24 11:29:02 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 24 May 2017 09:29:02 +0000
Subject: [R-sig-Geo] Problem in ENFA: infinite or missing values
In-Reply-To: <7B16C76E8F4ABD4B8F0D70668E32D2CB96CFC3@ex-mbx-pro-01>
References: <7B16C76E8F4ABD4B8F0D70668E32D2CB96CFC3@ex-mbx-pro-01>
Message-ID: <CAAcGz98BNdiZpMRsDZHjithD7Xy4xnCyhsAiHG66wrH0Ww3hjA@mail.gmail.com>

Can you you put together a reproducible example ( reprex)? I'm happy to
help but the set up is too onerous.

I'd recommend using raster to count points and for gridded data generally
but the advice is perhaps too abstract without an example. cellFromXY gives
the right output for a point in cell count as a grouped aggregate summary,
 but it's a module requiring to be embedded in a workflow (which means it's
super powerful, efficient and flexible) rather than a high level solution
requiring no further composition.

Cheers, Mike


On Fri, 19 May 2017, 16:51 Perry Beasley-hall <
perry.beasley-hall at sydney.edu.au> wrote:

> Hello everyone,
> I'm trying to do an ENFA using the package adehabitatHS. I'm following the
> example code and am creating the 'pr' file using:
>
> pr <- slot(count.points(species, map), "data")[,1]
>
> Where 'species' is a .csv file with two columns of latitude and longitude,
> and 'map' is a SpatialGridDataFrame file of 23 .asc inputs. For reference,
> I'm importing my .asc files using readGDAL and then using the cbind command
> to put them into the 'map' file.
>
> I encounter the following warning message when trying to create 'pr':
>
> 1: In points2grid(points, tolerance, round) :
>   grid has empty column/rows in dimension 1
> 2: In points2grid(points, tolerance, round) :
>   grid has empty column/rows in dimension 2
>
> I assumed this would be alright, but I'm getting the subsequent error when
> I attempt the enfa command:
>
> enfa1 <- enfa(pc, pr, scannf = FALSE)
>
> Error in eigen(Se) : infinite or missing values in 'x'
>
> Some searching online has told me that this might mean not all of my rows
> in the 'species' .csv correspond to variables in the 'map' file, but I
> don't think this is the case for my dataset.
>
> Can anyone help me with this issue? Many thanks in advance.
> ____
>
> Perry Beasley-Hall
> PhD Candidate
> School of Life and Environmental Sciences
>
> THE UNIVERSITY OF SYDNEY
> Room 329, Edgeworth David Building A11
> Sydney NSW 2006, Australia
> E-mail: perry.beasley-hall at sydney.edu.au
> Phone: +61 420 494 584
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From dmwarner at usgs.gov  Wed May 24 11:42:36 2017
From: dmwarner at usgs.gov (Warner, David)
Date: Wed, 24 May 2017 05:42:36 -0400
Subject: [R-sig-Geo] netcdf file mapping and dimensions issue
In-Reply-To: <CAAcGz988p+Dp-aY1Z-Zb0Rrn108AKXO77S33jMNHNRdzSKcCMA@mail.gmail.com>
References: <CA+Y23taE+BO3iTB7gqzoPeJU1D=_h5hu7-5bAstHRERq0uCKzA@mail.gmail.com>
 <CAAcGz9-GUVB4HgQDhR6doUS5pFww3TCCxcA_x7hKwBiCoDKDRQ@mail.gmail.com>
 <CA+Y23taFuhZTeP8QWyh3T7m_tki1z-Xz_9jY7THRsdssNH5SaA@mail.gmail.com>
 <CAAcGz988p+Dp-aY1Z-Zb0Rrn108AKXO77S33jMNHNRdzSKcCMA@mail.gmail.com>
Message-ID: <CA+Y23tb_Q+=MXM2WW591EuwG9GOt0v0JTUdFS5dk-Ft9JkmHig@mail.gmail.com>

Thanks a lot!  I need to fix the basics first though.  For example, even
though I can run the command and warp files at will, there is still an
issue I have yet to figure out.

The code below produces a nicely mapped file that looks like there is a
problem with the srcnodata or possibly dstnodata values as all of the pixel
values are less than zero for a scene that clearly has nonzero data.  I am
pretty stumped as I have looked at the documentation for the ocean color
data to get the srcnodata value of -32767s.  Also tried -32767, which is
shown as the fill value in SeaDAS band attributes table.  The link below
shows the result.  The data look quite like the original file as viewed in
SeaDAS but the values are clearly wrong.

r412<-raster('/Users/dmwarner/A2011202183000.L2_LAC_OC.x.nc',
var='geophysical_data/chl_ocx')
f <- "
https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/A2011202183000.L2_LAC_OC.x.nc"
download.file(f, basename(f), mode = "wb")
system('gdalwarp -srcnodata "None" -dstnodata None -srcnodata -32767s HDF5:"
A2011202183000.L2_LAC_OC.x.nc"://geophysical_data/Rrs_412 output.tif')
test<-raster('/Users/dmwarner/output.tif')
plot(test)

https://cdn.rawgit.com/dmwarn/Tethys/34794e44/warprrs412.png

On Tue, May 23, 2017 at 6:04 PM, Michael Sumner <mdsumner at gmail.com> wrote:

> See gdalUtils for a wrapper, or ncdump to easily get the names of
> variables and other metadata. I have workers L2 here that might help you
> batch this and I'm happy to help:
>
> https://github.com/mdsumner/roc/blob/master/R/readL2.R
>
> We want to get deeper into this soon so I'll be looking at it.
>
> There's possibly a way to do all the sds in one GDAL call too.
>
> Cheers, Mike
>
> On Wed, 24 May 2017, 02:34 Warner, David <dmwarner at usgs.gov> wrote:
>
>> Now if I can just get calls to gdal functions to work from within R....
>>
>>
>> On Tue, May 23, 2017 at 11:39 AM, Michael Sumner <mdsumner at gmail.com>
>> wrote:
>>
>>>
>>> I tried a few pretty unpromising things and then pointed gdalwarp at it.
>>> Seems fine, gdalwarp knows how to find geolocation arrays and use them, and
>>> it will auto-choose an output grid, and you can provide a custom one (i.e.
>>> with a local projection.
>>>
>>> The SDS (subdataset) naming thing is a bit awkward until you get used to
>>> it.  This was just GDAL 2.1.2 on Ubuntu.
>>>
>>>
>>> f <- "https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
>>> A2011202183000.L2_LAC_OC.x.nc"
>>> download.file(f, basename(f), mode = "wb")
>>>
>>> system('gdalwarp HDF5:"A2011202183000.L2_LAC_OC.x.nc
>>> "://geophysical_data/chl_ocx chl_ocx .tif')
>>> Creating output file that is 783P x 530L.
>>> Processing input file HDF5:A2011202183000.L2_LAC_OC.x.nc
>>> ://geophysical_data/chl_ocx.
>>> 0...10...20...30...40...50...60...70...80...90...100 - done.
>>> r <- raster("chl_ocx ")
>>> r
>>> class       : RasterLayer
>>> dimensions  : 530, 783, 414990  (nrow, ncol, ncell)
>>> resolution  : 0.01224719, 0.01224719  (x, y)
>>> extent      : -86.99178, -77.40223, 40.73677, 47.22778  (xmin, xmax,
>>> ymin, ymax)
>>> coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84
>>> +towgs84=0,0,0
>>>
>>> #plot(r)
>>> #library(mapdata)
>>> #map("worldHires", add = TRUE)
>>>
>>> Cheers, Mike.
>>>
>>>
>>> On Tue, 23 May 2017 at 23:58 Warner, David <dmwarner at usgs.gov> wrote:
>>>
>>>> Greetings all
>>>>
>>>> I am working nc files from the Ocean Biology Procesing Group.  The files
>>>> are MODIS aqua L2 ocean color files.
>>>>
>>>> These files are not mapped or perhaps some would say orthorectified such
>>>> that when you create a rasterLayer from one of the contained variables
>>>> and
>>>> plot it, the location and orientation of features is incorrect.  For
>>>> example, Georgian Bay, a large eastern subset of Lake Huron in the US
>>>> and
>>>> Canada, may well show up west of the lake.
>>>>
>>>> The dimensions of the data are read as column and row numbers rather
>>>> than
>>>> latitude and longitude.
>>>>
>>>> I am looking for a way to map these data in R other than my current
>>>> method.  The current method for a single nc file is shown below.  I run
>>>> this modified to fit into a parallelized foreach() loop that is used to
>>>> process folders of nc files.
>>>>
>>>> The  basic steps that map the nc file data are 1) create a data.frame
>>>> from
>>>> the nc files measured variable(s) and the latitude and longitude then
>>>> rasterize using vect2rast.SpatialPoints() and 2) resample the
>>>> rasterLayer
>>>> created from the data.frame using resample() to an nc file I reprojected
>>>> using SeaDAS.  This seems overly complicated considering the fact that I
>>>> start with a raster and it is pretty slow (30-40 seconds per nc file in
>>>> parallel, nearly two minutes per file in the script example here).
>>>>
>>>> I have two questions are 1) is there a way to eliminate the used of
>>>> vect2rast.SpatialPoints(), which is the slowest part of the process and
>>>> 2)
>>>> are there any other ways to perhaps speed the process up if I can't
>>>> eliminate vect2rast.SpatialPoints().  I have tried rasterize() and that
>>>> does not provide the correct product.  It recreates what I started with,
>>>> unmapped data.  What am I missing, or is this just a function of the
>>>> data I
>>>> am using?
>>>>
>>>>
>>>> Script is below.  I have included links to the two data files required
>>>> in
>>>> the process.  One is the nc file,
>>>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
>>>> A2011202183000.L2_LAC_OC.x.nc
>>>> the other is the reprojected version of this file from SeaDAS.
>>>> https://cdn.rawgit.com/dmwarn/Tethys/5ce3aad2/
>>>> A2011202183000.reprojected.tif
>>>> To use them you will have to change the path to the file locations.
>>>>
>>>> Thanks for your patience with my writing and poorly written code.  Any
>>>> ideas would be helpful.
>>>>
>>>> Dave
>>>>
>>>> library(raster)
>>>> library(plotKML)
>>>>
>>>> rasterOptions(maxmemory = 1e+09)
>>>> start <- Sys.time()
>>>> #Create rasterLayers from the bands of nc file that are required for
>>>> mapping, with r412 being the measured variable of interest
>>>> r412<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_412')
>>>> r443<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_443')
>>>> r469<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_469')
>>>> r488<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_488')
>>>> r531<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_531')
>>>> r547<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_547')
>>>> r555<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_555')
>>>> r645<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_645')
>>>> r667<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_667')
>>>> r678<-raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var='geophysical_data/Rrs_678')
>>>>
>>>> longitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/
>>>> Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var="navigation_data/longitude")
>>>> latitude <- raster('/Users/dmwarner/Documents/MODIS/MODIS/L2v73/
>>>> Huron/2011/
>>>> A2011202183000.L2_LAC_OC.x.nc <http://a2011202183000.l2_lac_oc.x.nc/>',
>>>> var="navigation_data/latitude")
>>>> r412  # shows that dimensions are row and column numbers, not lat and
>>>> long
>>>>
>>>>
>>>> #Import single reprojected band of the nc file as geotif, with
>>>> reprojection
>>>> done manually in SeaDAS software
>>>> correct<-raster("/Users/dmwarner/Documents/MODIS/OC/
>>>> correct/Huron/A2011202183000.reprojected.tif")
>>>>
>>>>
>>>> #now rasterize and map the nc file after
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r412 =
>>>> values(r412))
>>>> d$r412[is.na(d$r412)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r412', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.412 <- raster(dd)
>>>> ras.412.crop<-crop(ras.412, myext)
>>>> #ras.412.crop[ras.412.crop<0]<-NA
>>>> ras.412.crop[ras.412.crop>90000]<- NA
>>>>
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r443 =
>>>> values(r443))
>>>> d$r443[is.na(d$r443)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r443', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.443 <- raster(dd)
>>>> ras.443.crop<-crop(ras.443, myext)
>>>> ras.443.crop[ras.443.crop<0]<-NA
>>>> ras.443.crop[ras.443.crop>90000]<- NA
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r469 =
>>>> values(r469))
>>>> d$r469[is.na(d$r469)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r469', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.469 <- raster(dd)
>>>> ras.469.crop<-crop(ras.469, myext)
>>>> ras.469.crop[ras.469.crop<0]<-NA
>>>> ras.469.crop[ras.469.crop>90000]<- NA
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r488 =
>>>> values(r488))
>>>> d$r488[is.na(d$r488)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r488', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.488 <- raster(dd)
>>>> ras.488.crop<-crop(ras.488, myext)
>>>> ras.488.crop[ras.488.crop<0]<-NA
>>>> ras.488.crop[ras.488.crop>90000]<- NA
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r531 =
>>>> values(r531))
>>>> d$r531[is.na(d$r531)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r531', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.531 <- raster(dd)
>>>> ras.531.crop<-crop(ras.531, myext)
>>>> ras.531.crop[ras.531.crop<0]<-NA
>>>> ras.531.crop[ras.531.crop>90000]<- NA
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r547 =
>>>> values(r547))
>>>> d$r547[is.na(d$r547)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r547', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.547 <- raster(dd)
>>>> ras.547.crop<-crop(ras.547, myext)
>>>> ras.547.crop[ras.547.crop<0]<-NA
>>>> ras.547.crop[ras.547.crop>90000]<- NA
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r555 =
>>>> values(r555))
>>>> d$r555[is.na(d$r555)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r555', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.555 <- raster(dd)
>>>> ras.555.crop<-crop(ras.555, myext)
>>>> ras.555.crop[ras.555.crop<0]<-NA
>>>> ras.555.crop[ras.555.crop>90000]<- NA
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r645 =
>>>> values(r645))
>>>> d$r645[is.na(d$r645)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r645', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.645 <- raster(dd)
>>>> ras.645.crop<-crop(ras.645, myext)
>>>> ras.645.crop[ras.645.crop<0]<-NA
>>>> ras.645.crop[ras.645.crop>90000]<- NA
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r667 =
>>>> values(r667))
>>>> d$r667[is.na(d$r667)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r667', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.667 <- raster(dd)
>>>> ras.667.crop<-crop(ras.667, myext)
>>>> ras.667.crop[ras.667.crop<0]<-NA
>>>> ras.667.crop[ras.667.crop>90000]<- NA
>>>>
>>>> d <- data.frame(x = values(longitude), y = values(latitude), r678 =
>>>> values(r678))
>>>> d$r678[is.na(d$r678)]<- 99999
>>>> coordinates(d)<-~x+y
>>>> dd<-vect2rast.SpatialPoints(d, fname ='r678', cell.size=0.009337697,
>>>> method='raster')
>>>> ras.678 <- raster(dd)
>>>> ras.678.crop<-crop(ras.678, myext)
>>>> ras.678.crop[ras.678.crop<0]<-NA
>>>> ras.678.crop[ras.678.crop>90000]<- NA
>>>>
>>>> myext <- extent(-84.9, -79.6, 42.9, 46.6) # Approx. extent that will
>>>> cover
>>>> all of Lake Huron
>>>> correct.crop<-crop(correct, myext)
>>>> ras.412.r<-resample(ras.412.crop, correct.crop, method='bilinear')
>>>> ras.443.r<-resample(ras.443.crop, correct.crop, method='bilinear')
>>>> ras.469.r<-resample(ras.469.crop, correct.crop, method='bilinear')
>>>> ras.488.r<-resample(ras.488.crop, correct.crop, method='bilinear')
>>>> ras.531.r<-resample(ras.531.crop, correct.crop, method='bilinear')
>>>> ras.547.r<-resample(ras.547.crop, correct.crop, method='bilinear')
>>>> ras.555.r<-resample(ras.555.crop, correct.crop, method='bilinear')
>>>> ras.645.r<-resample(ras.645.crop, correct.crop, method='bilinear')
>>>> ras.667.r<-resample(ras.667.crop, correct.crop, method='bilinear')
>>>> ras.678.r<-resample(ras.678.crop, correct.crop, method='bilinear')
>>>> End<-Sys.time()
>>>> difftime(End, start)
>>>>
>>>> #plot original unmapped nc band Rrs_412 versus mapped version for gross
>>>> comparison
>>>> par(mfrow=c(2,1), mar=c(2,2,2,2))
>>>> my.orig.extent<-extent(100,475,200,600)
>>>> r412.crop<-crop(r412, my.orig.extent)
>>>> plot(r412.crop, main='Original unmapped')
>>>> plot(ras.412.r, main='Mapped in R')
>>>>
>>>> --
>>>> David Warner
>>>> Research Fisheries Biologist
>>>> U.S.G.S. Great Lakes Science Center
>>>> 1451 Green Road
>>>> Ann Arbor, MI 48105
>>>> 734-214-9392 <(734)%20214-9392>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>> --
>>> Dr. Michael Sumner
>>> Software and Database Engineer
>>> Australian Antarctic Division
>>> 203 Channel Highway
>>> Kingston Tasmania 7050 Australia
>>>
>>>
>>
>>
>> --
>> David Warner
>> Research Fisheries Biologist
>> U.S.G.S. Great Lakes Science Center
>> 1451 Green Road
>> Ann Arbor, MI 48105
>> 734-214-9392
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>


-- 
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Wed May 24 11:55:14 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 24 May 2017 09:55:14 +0000
Subject: [R-sig-Geo] Deal with multiple factorlevel in one grid square
In-Reply-To: <1857226423.12600330.1495609942428.JavaMail.zimbra@thuenen.de>
References: <1857226423.12600330.1495609942428.JavaMail.zimbra@thuenen.de>
Message-ID: <CAAcGz98HrNL8pNn--G2vY9RKaHMZtABtixScf18L6FTLWH7PTA@mail.gmail.com>

raster::extract(grid, poly, weights = TRUE) is a start. It returns a list
which is painful to deal with at first, but can be collected into one data
frame for standard summarizing.

I'm still a bit confused about whether you want an estimate of a cell
overlap in a polygon or something else.

Cheers, Mike

On Wed, 24 May 2017, 17:12 Miriam P?ts <miriam.puets at thuenen.de> wrote:

> Hi everyone,
>
> I have the following problem: I have a personalized grid and a shape file
> with polygons representing sediment types. Now I would like to apply this
> grid to the Polygons to identify the sediment type most common within each
> grid. I tried it with rasterize, but here I can only chose last or first.
> Die you have any suggestions how I might get the sediment type for each
> raster cell?
>
> Thank you for your help!
>
> <?)))>< >?)))>< >?)))>< >?)))><
>
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources
> Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries
> Palmaille 9
> 22767 Hamburg (Germany)
>
> Tel:  +49 40 38905-105
> Mail: miriam.puets at thuenen.de
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From b.graeler at 52north.org  Wed May 24 12:17:02 2017
From: b.graeler at 52north.org (=?UTF-8?Q?Dr._Benedikt_Gr=c3=a4ler?=)
Date: Wed, 24 May 2017 12:17:02 +0200
Subject: [R-sig-Geo] gstat or geoR create variogram with sets of
 bounding boxes
In-Reply-To: <CAEyHP-0NAmTBHsdqjCgtVQOcUmyj=eU5WnAqBD98wqzacMurow@mail.gmail.com>
References: <CAEyHP-0NAmTBHsdqjCgtVQOcUmyj=eU5WnAqBD98wqzacMurow@mail.gmail.com>
Message-ID: <b3589919-f033-92c2-4bc2-233feb98815c@52north.org>

Dear Javier,

if I got you right, you seem to be looking for a "pooled estimation of 
within-strata variograms" as the help page on the function "variogram()" 
of gstat calls it. At first, define an indicator for each of your points 
that identifies the ID of the polygon it belongs to, then include it as 
regressor and set dX=0.5:

Try the example below for the meuse data set where only point-pairs of 
the same soil type (corresponding to your polygon Ids) are considered 
and then a pooled variogram is calculated.

HTH,

  Ben


library(sp)
library(gstat)

data(meuse)
coordinates(meuse) <- ~x+y

spplot(meuse, "zinc")
spplot(meuse, "soil")

vAll <- variogram(zinc~soil, meuse)
plot(vAll)

vWithinSoils <- variogram(zinc~soil, meuse, dX=0.5)
plot(vWithinSoils)




On 23/05/2017 17:23, Javier Moreira wrote:
> hi,
> im trying to make a variogram that consider a limit made by a bounding box,
> but not just one, but dividing the area of the hole set of points in
> different subzones (treatments in this case).
> I have tried to do so with bounding box in geoR, but i cant do it.
> Also, another way its to use anisotropy, but when try to put an alpha
> parameter in geostat, doesnt make any change that without it.
> bottom line, i have to perform a variogram that includes only the points in
> 1 direction, or, tell the variogram to limit the surface to a polygon (
> several ones)
> 
> if any one can help,
> thanks
> 

-- 
Dr. Benedikt Gr?ler
52?North Initiative for Geospatial Open Source Software GmbH
Martin-Luther-King-Weg 24
48155 Muenster, Germany

E-Mail: b.graeler at 52north.org
Fon: +49-(0)-251/396371-39
Fax: +49-(0)-251/396371-11

http://52north.org/
Twitter: @FiveTwoN

General Managers: Dr. Albert Remke, Dr. Andreas Wytzisk
Local Court Muenster HRB 10849
-------------- next part --------------
A non-text attachment was scrubbed...
Name: b_graeler.vcf
Type: text/x-vcard
Size: 422 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170524/c913cb64/attachment.vcf>

From miriam.puets at thuenen.de  Wed May 24 12:35:37 2017
From: miriam.puets at thuenen.de (Miriam =?utf-8?B?UMO8dHM=?=)
Date: Wed, 24 May 2017 12:35:37 +0200 (CEST)
Subject: [R-sig-Geo] Deal with multiple factorlevel in one grid square
In-Reply-To: <CAAcGz98HrNL8pNn--G2vY9RKaHMZtABtixScf18L6FTLWH7PTA@mail.gmail.com>
References: <1857226423.12600330.1495609942428.JavaMail.zimbra@thuenen.de>
 <CAAcGz98HrNL8pNn--G2vY9RKaHMZtABtixScf18L6FTLWH7PTA@mail.gmail.com>
Message-ID: <85201758.12620837.1495622137454.JavaMail.zimbra@thuenen.de>

Hi Mike, 

I will try to explain it a bit more in detail. Maybe it is easier to understand if I start from the end. In the end I would like to have an ASCII file to read into Ecospace, which has the same extensions and coordinates as other files I already created. This ASCII should contain information about the sediment type within each predefined cell. To create this ASCII file I have a shape file with the polygons representing the sediment type and my grid which I applied to other variables to have the same extend. Now I would like to create a grid containing the information on sediment. Here, per grid cell the sediment type which covers the most of the cell should be defined and connected with the coordinates for the grid cell. 

I hope this makes it more clear... 


<?)))>< >?)))>< >?)))>< >?)))>< 

Miriam P?ts 
Marine Lebende Ressourcen/ Marine Living Resources 
Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries 
Palmaille 9 
22767 Hamburg (Germany) 

Tel: +49 40 38905-105 
Mail: miriam.puets at thuenen.de 


Von: "Michael Sumner" <mdsumner at gmail.com> 
An: "Miriam P?ts" <miriam.puets at thuenen.de>, r-sig-geo at r-project.org 
Gesendet: Mittwoch, 24. Mai 2017 11:55:14 
Betreff: Re: [R-sig-Geo] Deal with multiple factorlevel in one grid square 



raster::extract(grid, poly, weights = TRUE) is a start. It returns a list which is painful to deal with at first, but can be collected into one data frame for standard summarizing. 

I'm still a bit confused about whether you want an estimate of a cell overlap in a polygon or something else. 

Cheers, Mike 
On Wed, 24 May 2017, 17:12 Miriam P?ts < [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ] > wrote: 


Hi everyone, 

I have the following problem: I have a personalized grid and a shape file with polygons representing sediment types. Now I would like to apply this grid to the Polygons to identify the sediment type most common within each grid. I tried it with rasterize, but here I can only chose last or first. Die you have any suggestions how I might get the sediment type for each raster cell? 

Thank you for your help! 

<?)))>< >?)))>< >?)))>< >?)))>< 

Miriam P?ts 
Marine Lebende Ressourcen/ Marine Living Resources 
Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries 
Palmaille 9 
22767 Hamburg (Germany) 

Tel: +49 40 38905-105 
Mail: [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ] 

_______________________________________________ 
R-sig-Geo mailing list 
[ mailto:R-sig-Geo at r-project.org | R-sig-Geo at r-project.org ] 
[ https://stat.ethz.ch/mailman/listinfo/r-sig-geo | https://stat.ethz.ch/mailman/listinfo/r-sig-geo ] 



-- 
Dr. Michael Sumner 
Software and Database Engineer 
Australian Antarctic Division 
203 Channel Highway 
Kingston Tasmania 7050 Australia 


	[[alternative HTML version deleted]]


From patrick.schratz at gmail.com  Wed May 24 14:48:02 2017
From: patrick.schratz at gmail.com (Patrick Schratz)
Date: Wed, 24 May 2017 14:48:02 +0200
Subject: [R-sig-Geo] Deal with multiple factorlevel in one grid square
In-Reply-To: <85201758.12620837.1495622137454.JavaMail.zimbra@thuenen.de>
References: <1857226423.12600330.1495609942428.JavaMail.zimbra@thuenen.de>
 <CAAcGz98HrNL8pNn--G2vY9RKaHMZtABtixScf18L6FTLWH7PTA@mail.gmail.com>
 <85201758.12620837.1495622137454.JavaMail.zimbra@thuenen.de>
Message-ID: <dba2a2eb-4efb-462c-85cd-a32ffa51152e@Spark>

Do you want to do what is called ?zonal statistics? in ArcGIS with ?majority" option?

You may check out this SO question and try the ?mode? function within raster::extract() - maybe it does what you need.

(I?m also unsure if I understand the question correctly)

Cheers, Pat

PhD Student at Department of Geography - GIScience group
Friedrich-Schiller-University Jena, Germany
Tel.: +49-3641-9-48973
Web: https://pat-s.github.io

On 24. May 2017, 12:35 +0200, Miriam P?ts <miriam.puets at thuenen.de>, wrote:
> Hi Mike,
>
> I will try to explain it a bit more in detail. Maybe it is easier to understand if I start from the end. In the end I would like to have an ASCII file to read into Ecospace, which has the same extensions and coordinates as other files I already created. This ASCII should contain information about the sediment type within each predefined cell. To create this ASCII file I have a shape file with the polygons representing the sediment type and my grid which I applied to other variables to have the same extend. Now I would like to create a grid containing the information on sediment. Here, per grid cell the sediment type which covers the most of the cell should be defined and connected with the coordinates for the grid cell.
>
> I hope this makes it more clear...
>
>
> <?)))>< >?)))>< >?)))>< >?)))><
>
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources
> Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries
> Palmaille 9
> 22767 Hamburg (Germany)
>
> Tel: +49 40 38905-105
> Mail: miriam.puets at thuenen.de
>
>
> Von: "Michael Sumner" <mdsumner at gmail.com
> An: "Miriam P?ts" <miriam.puets at thuenen.de>, r-sig-geo at r-project.org
> Gesendet: Mittwoch, 24. Mai 2017 11:55:14
> Betreff: Re: [R-sig-Geo] Deal with multiple factorlevel in one grid square
>
>
>
> raster::extract(grid, poly, weights = TRUE) is a start. It returns a list which is painful to deal with at first, but can be collected into one data frame for standard summarizing.
>
> I'm still a bit confused about whether you want an estimate of a cell overlap in a polygon or something else.
>
> Cheers, Mike
> On Wed, 24 May 2017, 17:12 Miriam P?ts < [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ] > wrote:
>
>
> Hi everyone,
>
> I have the following problem: I have a personalized grid and a shape file with polygons representing sediment types. Now I would like to apply this grid to the Polygons to identify the sediment type most common within each grid. I tried it with rasterize, but here I can only chose last or first. Die you have any suggestions how I might get the sediment type for each raster cell?
>
> Thank you for your help!
>
> <?)))>< >?)))>< >?)))>< >?)))><
>
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources
> Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries
> Palmaille 9
> 22767 Hamburg (Germany)
>
> Tel: +49 40 38905-105
> Mail: [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ]
>
> _______________________________________________
> R-sig-Geo mailing list
> [ mailto:R-sig-Geo at r-project.org | R-sig-Geo at r-project.org ]
> [ https://stat.ethz.ch/mailman/listinfo/r-sig-geo | https://stat.ethz.ch/mailman/listinfo/r-sig-geo ]
>
>
>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From javiermoreira at gmail.com  Wed May 24 15:35:42 2017
From: javiermoreira at gmail.com (Javier Moreira)
Date: Wed, 24 May 2017 10:35:42 -0300
Subject: [R-sig-Geo] gstat or geoR create variogram with sets of
	bounding boxes
In-Reply-To: <b3589919-f033-92c2-4bc2-233feb98815c@52north.org>
References: <CAEyHP-0NAmTBHsdqjCgtVQOcUmyj=eU5WnAqBD98wqzacMurow@mail.gmail.com>
 <b3589919-f033-92c2-4bc2-233feb98815c@52north.org>
Message-ID: <CAEyHP-0v6gA3ozEa+ca4_zzRsY9bxtw_9CtPy-MbOjHvPuyqZw@mail.gmail.com>

THAT WORK FINE,
THANKS!

2017-05-24 7:17 GMT-03:00 Dr. Benedikt Gr?ler <b.graeler at 52north.org>:

> Dear Javier,
>
> if I got you right, you seem to be looking for a "pooled estimation of
> within-strata variograms" as the help page on the function "variogram()" of
> gstat calls it. At first, define an indicator for each of your points that
> identifies the ID of the polygon it belongs to, then include it as
> regressor and set dX=0.5:
>
> Try the example below for the meuse data set where only point-pairs of the
> same soil type (corresponding to your polygon Ids) are considered and then
> a pooled variogram is calculated.
>
> HTH,
>
>  Ben
>
>
> library(sp)
> library(gstat)
>
> data(meuse)
> coordinates(meuse) <- ~x+y
>
> spplot(meuse, "zinc")
> spplot(meuse, "soil")
>
> vAll <- variogram(zinc~soil, meuse)
> plot(vAll)
>
> vWithinSoils <- variogram(zinc~soil, meuse, dX=0.5)
> plot(vWithinSoils)
>
>
>
>
> On 23/05/2017 17:23, Javier Moreira wrote:
>
>> hi,
>> im trying to make a variogram that consider a limit made by a bounding
>> box,
>> but not just one, but dividing the area of the hole set of points in
>> different subzones (treatments in this case).
>> I have tried to do so with bounding box in geoR, but i cant do it.
>> Also, another way its to use anisotropy, but when try to put an alpha
>> parameter in geostat, doesnt make any change that without it.
>> bottom line, i have to perform a variogram that includes only the points
>> in
>> 1 direction, or, tell the variogram to limit the surface to a polygon (
>> several ones)
>>
>> if any one can help,
>> thanks
>>
>>
> --
> Dr. Benedikt Gr?ler
> 52?North Initiative for Geospatial Open Source Software GmbH
> Martin-Luther-King-Weg 24
> 48155 Muenster, Germany
>
> E-Mail: b.graeler at 52north.org
> Fon: +49-(0)-251/396371-39
> Fax: +49-(0)-251/396371-11
>
> http://52north.org/
> Twitter: @FiveTwoN
>
> General Managers: Dr. Albert Remke, Dr. Andreas Wytzisk
> Local Court Muenster HRB 10849
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Javier Moreira de Souza
Ingeniero Agr?nomo
099 406 006

	[[alternative HTML version deleted]]


From Andy.Bunn at wwu.edu  Wed May 24 21:22:56 2017
From: Andy.Bunn at wwu.edu (Andy Bunn)
Date: Wed, 24 May 2017 19:22:56 +0000
Subject: [R-sig-Geo] Easiest way to get a a subset from MODIS
Message-ID: <D54B2B97.69867%andy.bunn@wwu.edu>

Hi all, It's been years since I've used any MODIS data. Looking around I see there are many R packages that purport to interface nicely with MODIS servers and deliver you a subset of data.

I have an EarthData login and can (and have) download the tile I want via http://search.earthdata.nasa.gov but getting the hdf, converting to tif, clipping etc. is tedious and I'm wondering what the best way to do this in R is. So,  let's imagine I would like to get a class(raster) object of the "250m_16_days_NDVI" band from "MYD13Q1" product for a 10x10km area around the point c(68.75,161.42) for the period of nearest July 1, 2015. What package and approach is the easiest in R?

Is it something like this?

library(MODISTools)
dat2get <- data.frame(lat=68.75,long=161.42,
                      start.date=as.POSIXlt("2015-07-01"),
                      end.date=as.POSIXlt("2015-07-01"),
                      ID=1)

MODISSubsets(LoadDat = dat2get,
             Products = "MYD13Q1",
             Bands = "250m_16_days_NDVI",
             Size = c(10,10),
             StartDate = TRUE)

Regards, Andy



	[[alternative HTML version deleted]]


From maloneymc at appstate.edu  Wed May 24 21:49:59 2017
From: maloneymc at appstate.edu (Megan Maloney)
Date: Wed, 24 May 2017 15:49:59 -0400
Subject: [R-sig-Geo] Recs for R tools that subset/tile raster bricks?
Message-ID: <CAE2v7awxOAOBzyJF8X5jbh_6eaiUrco8FDSunFZ_fR7CH2nkFQ@mail.gmail.com>

Hello all,

Does anyone know of a good R tool to subset/tile raster bricks?

I'd like to break a raster brick into smaller pieces to process. I'm
finding functions to subset or tile a single raster layer, but not for
multi-layer raster bricks.

The brick input is hyperspectral flightlines - very large, 1 m^2 resampling
covering a watershed, 340 layers. My code currently takes in a raster
brick, extracts values per cell, applies beta coefficients, and outputs a
nitrogen value to a raster.

I don't need the subsets/tiles to be a particular size or number, just
smaller. I was warned against dropping a whole flightline into R. I'm new to
R, so I'm not sure what limitations I'm running into except processing time.
All advice is welcome.

Thank you very much for your help.
Megan

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Thu May 25 00:33:01 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 24 May 2017 22:33:01 +0000
Subject: [R-sig-Geo] Recs for R tools that subset/tile raster bricks?
In-Reply-To: <CAE2v7awxOAOBzyJF8X5jbh_6eaiUrco8FDSunFZ_fR7CH2nkFQ@mail.gmail.com>
References: <CAE2v7awxOAOBzyJF8X5jbh_6eaiUrco8FDSunFZ_fR7CH2nkFQ@mail.gmail.com>
Message-ID: <CAAcGz9-8FO=YvwRPbSik3watSTsuTrHZ8SDbTVjqp3N8swQtPg@mail.gmail.com>

Try crop() (or more abstractly extract(raster, cells) once you've
determined which cells x-y are needed).

Getting the most efficient way to do this depends on the source of the
brick. It's not possible to know without knowing details on your setup. The
file format, method of creation and dimensions of the brick and sizes of
your subset query can all affect the best ways to go.

Happy to help but it might take a bit of conversation to figure out what's
best. If you can share the output of print on the brick anf the extent of a
subset that is a good start.

Cheers, Mike

On Thu, 25 May 2017, 05:50 Megan Maloney <maloneymc at appstate.edu> wrote:

> Hello all,
>
> Does anyone know of a good R tool to subset/tile raster bricks?
>
> I'd like to break a raster brick into smaller pieces to process. I'm
> finding functions to subset or tile a single raster layer, but not for
> multi-layer raster bricks.
>
> The brick input is hyperspectral flightlines - very large, 1 m^2 resampling
> covering a watershed, 340 layers. My code currently takes in a raster
> brick, extracts values per cell, applies beta coefficients, and outputs a
> nitrogen value to a raster.
>
> I don't need the subsets/tiles to be a particular size or number, just
> smaller. I was warned against dropping a whole flightline into R. I'm new
> to
> R, so I'm not sure what limitations I'm running into except processing
> time.
> All advice is welcome.
>
> Thank you very much for your help.
> Megan
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Thu May 25 00:58:47 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 24 May 2017 22:58:47 +0000
Subject: [R-sig-Geo] Recs for R tools that subset/tile raster bricks?
In-Reply-To: <CAAcGz9-8FO=YvwRPbSik3watSTsuTrHZ8SDbTVjqp3N8swQtPg@mail.gmail.com>
References: <CAE2v7awxOAOBzyJF8X5jbh_6eaiUrco8FDSunFZ_fR7CH2nkFQ@mail.gmail.com>
 <CAAcGz9-8FO=YvwRPbSik3watSTsuTrHZ8SDbTVjqp3N8swQtPg@mail.gmail.com>
Message-ID: <CAAcGz999uwiWWsArPE5dom=OktWpOYS_5cXSAFSA=74YeUbv1Q@mail.gmail.com>

Do also see the raster vignettes, there is one on performance and using
line- or block- based chunks for large jobs.

Cheers, Mike

On Thu, 25 May 2017, 08:32 Michael Sumner <mdsumner at gmail.com> wrote:

> Try crop() (or more abstractly extract(raster, cells) once you've
> determined which cells x-y are needed).
>
> Getting the most efficient way to do this depends on the source of the
> brick. It's not possible to know without knowing details on your setup. The
> file format, method of creation and dimensions of the brick and sizes of
> your subset query can all affect the best ways to go.
>
> Happy to help but it might take a bit of conversation to figure out what's
> best. If you can share the output of print on the brick anf the extent of a
> subset that is a good start.
>
> Cheers, Mike
>
> On Thu, 25 May 2017, 05:50 Megan Maloney <maloneymc at appstate.edu> wrote:
>
>> Hello all,
>>
>> Does anyone know of a good R tool to subset/tile raster bricks?
>>
>> I'd like to break a raster brick into smaller pieces to process. I'm
>> finding functions to subset or tile a single raster layer, but not for
>> multi-layer raster bricks.
>>
>> The brick input is hyperspectral flightlines - very large, 1 m^2
>> resampling
>> covering a watershed, 340 layers. My code currently takes in a raster
>> brick, extracts values per cell, applies beta coefficients, and outputs a
>> nitrogen value to a raster.
>>
>> I don't need the subsets/tiles to be a particular size or number, just
>> smaller. I was warned against dropping a whole flightline into R. I'm new
>> to
>> R, so I'm not sure what limitations I'm running into except processing
>> time.
>> All advice is welcome.
>>
>> Thank you very much for your help.
>> Megan
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
> --
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From maloneymc at appstate.edu  Thu May 25 01:18:10 2017
From: maloneymc at appstate.edu (Megan Maloney)
Date: Wed, 24 May 2017 19:18:10 -0400
Subject: [R-sig-Geo] Recs for R tools that subset/tile raster bricks?
In-Reply-To: <CAAcGz999uwiWWsArPE5dom=OktWpOYS_5cXSAFSA=74YeUbv1Q@mail.gmail.com>
References: <CAE2v7awxOAOBzyJF8X5jbh_6eaiUrco8FDSunFZ_fR7CH2nkFQ@mail.gmail.com>
 <CAAcGz9-8FO=YvwRPbSik3watSTsuTrHZ8SDbTVjqp3N8swQtPg@mail.gmail.com>
 <CAAcGz999uwiWWsArPE5dom=OktWpOYS_5cXSAFSA=74YeUbv1Q@mail.gmail.com>
Message-ID: <CAE2v7azrjHxVUh2KQCtKWX2j8s-37Q4zZA0ZqrhmjF2ds47JvA@mail.gmail.com>

Thanks for your response, Mike! The files are ENVI images (*.dat). The
output of print() on one is copied below. The other images have slightly
different row/col sizes but the same number of layers.

Thanks for the tip on the vignettes. It looks like "Writing functions with
the ?raster? package" is the one you're referring to? blockSize() looks
helpful as I'm not sure what size would be appropriate to crop to.

--

class       : RasterBrick
dimensions  : 24390, 2032, 49560480, 373  (nrow, ncol, ncell, nlayers)
resolution  : 1, 1  (x, y)
extent      : 520554.6, 522586.6, 4766639, 4791029  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=utm +zone=11 +datum=NAD83 +units=m +no_defs
+ellps=GRS80 +towgs84=0,0,0
data source : E:\RCEW_CoReg_2014\183421\183421_Georegistered
names       :
Warp..Band.8.ang20140919t183421_corr_v1c_img...381.360260.Nanometers.,
Warp..Band.9.ang20140919t183421_corr_v1c_img...386.368927.Nanometers.,
Warp..Band.10.ang20140919t183421_corr_v1c_img...391.377594.Nanometers.,
Warp..Band.11.ang20140919t183421_corr_v1c_img...396.386292.Nanometers.,
Warp..Band.12.ang20140919t183421_corr_v1c_img...401.394958.Nanometers.,
Warp..Band.13.ang20140919t183421_corr_v1c_img...406.403625.Nanometers.,
Warp..Band.14.ang20140919t183421_corr_v1c_img...411.412292.Nanometers.,
Warp..Band.15.ang20140919t183421_corr_v1c_img...416.420959.Nanometers.,
Warp..Band.16.ang20140919t183421_corr_v1c_img...421.429626.Nanometers.,
Warp..Band.17.ang20140919t183421_corr_v1c_img...426.438293.Nanometers.,
Warp..Band.18.ang20140919t183421_corr_v1c_img...431.446960.Nanometers.,
Warp..Band.19.ang20140919t183421_corr_v1c_img...436.455627.Nanometers.,
Warp..Band.20.ang20140919t183421_corr_v1c_img...441.464294.Nanometers.,
Warp..Band.21.ang20140919t183421_corr_v1c_img...446.... <truncated>

Thanks,
Megan

On Wed, May 24, 2017 at 6:58 PM, Michael Sumner <mdsumner at gmail.com> wrote:

> Do also see the raster vignettes, there is one on performance and using
> line- or block- based chunks for large jobs.
>
> Cheers, Mike
>
>
> On Thu, 25 May 2017, 08:32 Michael Sumner <mdsumner at gmail.com> wrote:
>
>> Try crop() (or more abstractly extract(raster, cells) once you've
>> determined which cells x-y are needed).
>>
>> Getting the most efficient way to do this depends on the source of the
>> brick. It's not possible to know without knowing details on your setup. The
>> file format, method of creation and dimensions of the brick and sizes of
>> your subset query can all affect the best ways to go.
>>
>> Happy to help but it might take a bit of conversation to figure out
>> what's best. If you can share the output of print on the brick anf the
>> extent of a subset that is a good start.
>>
>> Cheers, Mike
>>
>> On Thu, 25 May 2017, 05:50 Megan Maloney <maloneymc at appstate.edu> wrote:
>>
>>> Hello all,
>>>
>>> Does anyone know of a good R tool to subset/tile raster bricks?
>>>
>>> I'd like to break a raster brick into smaller pieces to process. I'm
>>> finding functions to subset or tile a single raster layer, but not for
>>> multi-layer raster bricks.
>>>
>>> The brick input is hyperspectral flightlines - very large, 1 m^2
>>> resampling
>>> covering a watershed, 340 layers. My code currently takes in a raster
>>> brick, extracts values per cell, applies beta coefficients, and outputs a
>>> nitrogen value to a raster.
>>>
>>> I don't need the subsets/tiles to be a particular size or number, just
>>> smaller. I was warned against dropping a whole flightline into R. I'm
>>> new to
>>> R, so I'm not sure what limitations I'm running into except processing
>>> time.
>>> All advice is welcome.
>>>
>>> Thank you very much for your help.
>>> Megan
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>> --
>> Dr. Michael Sumner
>> Software and Database Engineer
>> Australian Antarctic Division
>> 203 Channel Highway
>> Kingston Tasmania 7050 Australia
>>
>> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>

	[[alternative HTML version deleted]]


From Alexander.Herr at csiro.au  Thu May 25 02:28:40 2017
From: Alexander.Herr at csiro.au (Alexander.Herr at csiro.au)
Date: Thu, 25 May 2017 00:28:40 +0000
Subject: [R-sig-Geo] Deal with multiple factorlevel in one grid square
In-Reply-To: <dba2a2eb-4efb-462c-85cd-a32ffa51152e@Spark>
References: <1857226423.12600330.1495609942428.JavaMail.zimbra@thuenen.de>
 <CAAcGz98HrNL8pNn--G2vY9RKaHMZtABtixScf18L6FTLWH7PTA@mail.gmail.com>
 <85201758.12620837.1495622137454.JavaMail.zimbra@thuenen.de>
 <dba2a2eb-4efb-462c-85cd-a32ffa51152e@Spark>
Message-ID: <327c98be68434232881786fecbc53d6d@exch1-mel.nexus.csiro.au>

Hiya,

Do you want a raster with attribute information (ie several attributes from your  shape file)? You can achieve this with a workflow that uses 
raster::extract() to assigne unique IDs of polygon to raster cells
create a RAT for you raster
assign to the unique IDs of raster RAT the attributes of your polygon unique IDs (many to one relationship) with something like merge() or functions in libraries like diplyr or data.table

Cheers
Herry

-----Original Message-----
From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Patrick Schratz
Sent: Wednesday, 24 May 2017 10:48 PM
To: Michael Sumner <mdsumner at gmail.com>; Miriam P?ts <miriam.puets at thuenen.de>
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Deal with multiple factorlevel in one grid square

Do you want to do what is called ?zonal statistics? in ArcGIS with ?majority" option?

You may check out this SO question and try the ?mode? function within raster::extract() - maybe it does what you need.

(I?m also unsure if I understand the question correctly)

Cheers, Pat

PhD Student at Department of Geography - GIScience group Friedrich-Schiller-University Jena, Germany
Tel.: +49-3641-9-48973
Web: https://pat-s.github.io

On 24. May 2017, 12:35 +0200, Miriam P?ts <miriam.puets at thuenen.de>, wrote:
> Hi Mike,
>
> I will try to explain it a bit more in detail. Maybe it is easier to understand if I start from the end. In the end I would like to have an ASCII file to read into Ecospace, which has the same extensions and coordinates as other files I already created. This ASCII should contain information about the sediment type within each predefined cell. To create this ASCII file I have a shape file with the polygons representing the sediment type and my grid which I applied to other variables to have the same extend. Now I would like to create a grid containing the information on sediment. Here, per grid cell the sediment type which covers the most of the cell should be defined and connected with the coordinates for the grid cell.
>
> I hope this makes it more clear...
>
>
> <?)))>< >?)))>< >?)))>< >?)))><
>
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources Th?nen-Institut f?r 
> Seefischerei/ Th?nen Institute of Sea fisheries Palmaille 9
> 22767 Hamburg (Germany)
>
> Tel: +49 40 38905-105
> Mail: miriam.puets at thuenen.de
>
>
> Von: "Michael Sumner" <mdsumner at gmail.com
> An: "Miriam P?ts" <miriam.puets at thuenen.de>, r-sig-geo at r-project.org
> Gesendet: Mittwoch, 24. Mai 2017 11:55:14
> Betreff: Re: [R-sig-Geo] Deal with multiple factorlevel in one grid 
> square
>
>
>
> raster::extract(grid, poly, weights = TRUE) is a start. It returns a list which is painful to deal with at first, but can be collected into one data frame for standard summarizing.
>
> I'm still a bit confused about whether you want an estimate of a cell overlap in a polygon or something else.
>
> Cheers, Mike
> On Wed, 24 May 2017, 17:12 Miriam P?ts < [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ] > wrote:
>
>
> Hi everyone,
>
> I have the following problem: I have a personalized grid and a shape file with polygons representing sediment types. Now I would like to apply this grid to the Polygons to identify the sediment type most common within each grid. I tried it with rasterize, but here I can only chose last or first. Die you have any suggestions how I might get the sediment type for each raster cell?
>
> Thank you for your help!
>
> <?)))>< >?)))>< >?)))>< >?)))><
>
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources Th?nen-Institut f?r 
> Seefischerei/ Th?nen Institute of Sea fisheries Palmaille 9
> 22767 Hamburg (Germany)
>
> Tel: +49 40 38905-105
> Mail: [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ]
>
> _______________________________________________
> R-sig-Geo mailing list
> [ mailto:R-sig-Geo at r-project.org | R-sig-Geo at r-project.org ] [ 
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo | 
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo ]
>
>
>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

From Andy.Bunn at wwu.edu  Thu May 25 06:34:44 2017
From: Andy.Bunn at wwu.edu (Andy Bunn)
Date: Thu, 25 May 2017 04:34:44 +0000
Subject: [R-sig-Geo] Easiest way to get a a subset from MODIS
In-Reply-To: <D54B2B97.69867%andy.bunn@wwu.edu>
References: <D54B2B97.69867%andy.bunn@wwu.edu>
Message-ID: <D54B8B23.698D2%andy.bunn@wwu.edu>

Answering my own question.

I was able to do what I wanted using the MODIS package. The runGdal function does what I want. Thanks to those who coded it!

-A

From: Andy Bunn <andy.bunn at wwu.edu<mailto:andy.bunn at wwu.edu>>
Date: Wednesday, May 24, 2017 at 12:22 PM
To: R-sig-Geo <r-sig-geo at r-project.org<mailto:r-sig-geo at r-project.org>>
Subject: Easiest way to get a a subset from MODIS

Hi all, It's been years since I've used any MODIS data. Looking around I see there are many R packages that purport to interface nicely with MODIS servers and deliver you a subset of data.

I have an EarthData login and can (and have) download the tile I want via http://search.earthdata.nasa.gov but getting the hdf, converting to tif, clipping etc. is tedious and I'm wondering what the best way to do this in R is. So,  let's imagine I would like to get a class(raster) object of the "250m_16_days_NDVI" band from "MYD13Q1" product for a 10x10km area around the point c(68.75,161.42) for the period of nearest July 1, 2015. What package and approach is the easiest in R?

Is it something like this?

library(MODISTools)
dat2get <- data.frame(lat=68.75,long=161.42,
                      start.date=as.POSIXlt("2015-07-01"),
                      end.date=as.POSIXlt("2015-07-01"),
                      ID=1)

MODISSubsets(LoadDat = dat2get,
             Products = "MYD13Q1",
             Bands = "250m_16_days_NDVI",
             Size = c(10,10),
             StartDate = TRUE)

Regards, Andy



	[[alternative HTML version deleted]]


From malNamalJa at gmx.de  Fri May 26 10:06:15 2017
From: malNamalJa at gmx.de (=?UTF-8?Q?=22Jannes_M=C3=BCnchow=22?=)
Date: Fri, 26 May 2017 10:06:15 +0200
Subject: [R-sig-Geo] release of RQGIS 1.0.0
Message-ID: <trinity-ffe762f4-ec96-4890-a3f8-0cc9bdc735fa-1495785975967@3capp-gmx-bs55>

Today we are proud to announce a major RQGIS release (already on CRAN). We have completely rewritten RQGIS now using the reticulate package to access the QGIS Python API. Overall, RQGIS is now even more user-friendly. Find more information on new features, what has changed and how to use RQGIS on following pages:
- https://jannesm.wordpress.com/category/r/
- https://github.com/jannes-m/RQGIS/releases
- https://github.com/jannes-m/RQGIS


From Andy.Bunn at wwu.edu  Sat May 27 00:25:12 2017
From: Andy.Bunn at wwu.edu (Andy Bunn)
Date: Fri, 26 May 2017 22:25:12 +0000
Subject: [R-sig-Geo] release of RQGIS 1.0.0
In-Reply-To: <trinity-ffe762f4-ec96-4890-a3f8-0cc9bdc735fa-1495785975967@3capp-gmx-bs55>
References: <trinity-ffe762f4-ec96-4890-a3f8-0cc9bdc735fa-1495785975967@3capp-gmx-bs55>
Message-ID: <D54DF927.69A94%andy.bunn@wwu.edu>

Wonderful work! Thank you for your contributions!

On 5/26/17, 1:06 AM, "R-sig-Geo on behalf of "Jannes M?nchow""
<r-sig-geo-bounces at r-project.org on behalf of malNamalJa at gmx.de> wrote:

>Today we are proud to announce a major RQGIS release (already on CRAN).
>We have completely rewritten RQGIS now using the reticulate package to
>access the QGIS Python API. Overall, RQGIS is now even more
>user-friendly. Find more information on new features, what has changed
>and how to use RQGIS on following pages:
>- https://jannesm.wordpress.com/category/r/
>- https://github.com/jannes-m/RQGIS/releases
>- https://github.com/jannes-m/RQGIS
>
>_______________________________________________
>R-sig-Geo mailing list
>R-sig-Geo at r-project.org
>https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From Andy.Bunn at wwu.edu  Sat May 27 00:34:05 2017
From: Andy.Bunn at wwu.edu (Andy Bunn)
Date: Fri, 26 May 2017 22:34:05 +0000
Subject: [R-sig-Geo] Google Earth Engine?
Message-ID: <D54DFB5F.69AA9%andy.bunn@wwu.edu>

Does anybody out there interface with the google earth engine from R? I'm too old a dog to learn python. -Andy

	[[alternative HTML version deleted]]


From b.rowlingson at lancaster.ac.uk  Sat May 27 01:02:11 2017
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Sat, 27 May 2017 00:02:11 +0100
Subject: [R-sig-Geo] Google Earth Engine?
In-Reply-To: <D54DFB5F.69AA9%andy.bunn@wwu.edu>
References: <D54DFB5F.69AA9%andy.bunn@wwu.edu>
Message-ID: <CANVKczNmepxdwHivb6_h+2vqM1p0OppLYf3m-uWc5SY5idtycg@mail.gmail.com>

On Fri, May 26, 2017 at 11:34 PM, Andy Bunn <Andy.Bunn at wwu.edu> wrote:
> Does anybody out there interface with the google earth engine from R? I'm too old a dog to learn python. -Andy

Too old? Never! See: https://www.xkcd.com/353/

Given that the other supported option is Javascript....

I suspect a solution using one of the R-Python interfaces
("reticulate" perhaps) might be the best solution, but you still might
have to learn python to construct the analysis jobs.

Barry



>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From c.cavalieri at me.com  Mon May 29 20:13:27 2017
From: c.cavalieri at me.com (Carlo Cavalieri)
Date: Mon, 29 May 2017 20:13:27 +0200
Subject: [R-sig-Geo] How to fit a pure spatial variogram on a
	spatio-temporal empirical one
Message-ID: <2D22C648-971F-410F-A662-261A66C475CC@me.com>

Hi, I am using the R package GSTAT to make a spatio-temporal interpolation for my thesis and I wanted to know if it was possible to obtain the pure spatial empirical variogram from the spatio-temporal so that I can use it to fit a pure spatial variogram, for example exponential.
Unfortunately fit.variogram only accepts objects output of variogram, not of variogramST. One possible solution could be to extract tlag=0 from the StVariogram and convert the output to class variogramModel, but I have no idea on how to do this. 
I look for a way to do this because fit the spatial variogram for each day separately is not a good idea given the small number of observation stations.

One way is definitely possible since the authors of the paper "Spatio-Temporal Interpolation using gstat? managed to compare the results of pure spatial and spatio-temporal interpolation (that is what I want to do): Below a quotation from that paper.

"For comparison with classical approaches, we interpolate across Germany iteratively for each single day using all available data for variogram estimation. The purely spatial empirical variogram can directly be obtained from the empirical spatio-temporal variogram, by ?xing the temporal lag at 0 separation. From the same set of variogram models as investigated for the spatio-temporal models, the exponential model (partial sill: 66.5, range: 224 km, nugget: 13.5) is the best suited based on the optimisation criterion.?

Does anyone have any idea?
Thank you
Carlo
	[[alternative HTML version deleted]]


From b.graeler at 52north.org  Tue May 30 10:48:37 2017
From: b.graeler at 52north.org (=?UTF-8?Q?Dr._Benedikt_Gr=c3=a4ler?=)
Date: Tue, 30 May 2017 10:48:37 +0200
Subject: [R-sig-Geo] How to fit a pure spatial variogram on a
 spatio-temporal empirical one
In-Reply-To: <2D22C648-971F-410F-A662-261A66C475CC@me.com>
References: <2D22C648-971F-410F-A662-261A66C475CC@me.com>
Message-ID: <45f0ab87-51c9-dc24-5193-00bd08ebc2fe@52north.org>

Dear Carlo,

the code below is a bit of a hack, but does what you are asking for. The 
classes "gstatVariogram" and "StVariogram" have slight different design 
and so do the functions fit.variogram and fit.StVariogram. Note that 
spVv is now a pooled variogram across all time steps of your dataset 
treating each time slice as an independent copy of the same pure spatial 
process (i.e. strong temporal autocorrelation might influence your 
estimation).

HTH,

  Ben


library(gstat)
data("vv")
plot(vv)

spaceOnly <- vv$timelag == 0

spVv <- cbind(vv[spaceOnly,],
               data.frame(dir.hor=rep(0, sum(spaceOnly)),
                          dir.ver=rep(0, sum(spaceOnly))))

# drop empty (NA) first row
spVv <- spVv[-1, ]

# manually re-class
class(spVv) <- c("gstatVariogram","data.frame")

plot(spVv)

fitSpVgm <- fit.variogram(spVv, vgm(30, "Exp", 150, 10))
plot(spVv, fitSpVgm)



On 29/05/2017 20:13, Carlo Cavalieri wrote:
> Hi, I am using the R package GSTAT to make a spatio-temporal interpolation for my thesis and I wanted to know if it was possible to obtain the pure spatial empirical variogram from the spatio-temporal so that I can use it to fit a pure spatial variogram, for example exponential.
> Unfortunately fit.variogram only accepts objects output of variogram, not of variogramST. One possible solution could be to extract tlag=0 from the StVariogram and convert the output to class variogramModel, but I have no idea on how to do this.
> I look for a way to do this because fit the spatial variogram for each day separately is not a good idea given the small number of observation stations.
> 
> One way is definitely possible since the authors of the paper "Spatio-Temporal Interpolation using gstat? managed to compare the results of pure spatial and spatio-temporal interpolation (that is what I want to do): Below a quotation from that paper.
> 
> "For comparison with classical approaches, we interpolate across Germany iteratively for each single day using all available data for variogram estimation. The purely spatial empirical variogram can directly be obtained from the empirical spatio-temporal variogram, by ?xing the temporal lag at 0 separation. From the same set of variogram models as investigated for the spatio-temporal models, the exponential model (partial sill: 66.5, range: 224 km, nugget: 13.5) is the best suited based on the optimisation criterion.?
> 
> Does anyone have any idea?
> Thank you
> Carlo
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Dr. Benedikt Gr?ler
52?North Initiative for Geospatial Open Source Software GmbH
Martin-Luther-King-Weg 24
48155 Muenster, Germany

E-Mail: b.graeler at 52north.org
Fon: +49-(0)-251/396371-39
Fax: +49-(0)-251/396371-11

http://52north.org/
Twitter: @FiveTwoN

General Managers: Dr. Albert Remke, Dr. Andreas Wytzisk
Local Court Muenster HRB 10849
-------------- next part --------------
A non-text attachment was scrubbed...
Name: b_graeler.vcf
Type: text/x-vcard
Size: 422 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170530/a7bc1969/attachment.vcf>

From miriam.puets at thuenen.de  Tue May 30 13:55:36 2017
From: miriam.puets at thuenen.de (Miriam =?utf-8?B?UMO8dHM=?=)
Date: Tue, 30 May 2017 13:55:36 +0200 (CEST)
Subject: [R-sig-Geo] Deal with multiple factorlevel in one grid square
In-Reply-To: <327c98be68434232881786fecbc53d6d@exch1-mel.nexus.csiro.au>
References: <1857226423.12600330.1495609942428.JavaMail.zimbra@thuenen.de>
 <CAAcGz98HrNL8pNn--G2vY9RKaHMZtABtixScf18L6FTLWH7PTA@mail.gmail.com>
 <85201758.12620837.1495622137454.JavaMail.zimbra@thuenen.de>
 <dba2a2eb-4efb-462c-85cd-a32ffa51152e@Spark>
 <327c98be68434232881786fecbc53d6d@exch1-mel.nexus.csiro.au>
Message-ID: <1509510501.13989268.1496145336605.JavaMail.zimbra@thuenen.de>

Hi all,

@Pat: yes, that is almost what I need. If I would do it in ArcGIS I would use Polygon to raster (with a second raster as reference) and then choose majority.  there a way to do it in R? If I use raster::extract() I would extract data from the raster object for the locations of other spatial data, but I need spatial data for the raster object...


Cheers, 
Miriam

<?)))>< >?)))>< >?)))>< >?)))><

Miriam P?ts
Marine Lebende Ressourcen/ Marine Living Resources
Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries
Palmaille 9
22767 Hamburg (Germany)

Tel:  +49 40 38905-105 
Mail: miriam.puets at thuenen.de

----- Urspr?ngliche Mail -----
Von: "Alexander Herr" <Alexander.Herr at csiro.au>
An: "patrick schratz" <patrick.schratz at gmail.com>, mdsumner at gmail.com, "miriam puets" <miriam.puets at thuenen.de>
CC: r-sig-geo at r-project.org
Gesendet: Donnerstag, 25. Mai 2017 02:28:40
Betreff: RE: [R-sig-Geo] Deal with multiple factorlevel in one grid square

Hiya,

Do you want a raster with attribute information (ie several attributes from your  shape file)? You can achieve this with a workflow that uses 
raster::extract() to assigne unique IDs of polygon to raster cells
create a RAT for you raster
assign to the unique IDs of raster RAT the attributes of your polygon unique IDs (many to one relationship) with something like merge() or functions in libraries like diplyr or data.table

Cheers
Herry

-----Original Message-----
From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Patrick Schratz
Sent: Wednesday, 24 May 2017 10:48 PM
To: Michael Sumner <mdsumner at gmail.com>; Miriam P?ts <miriam.puets at thuenen.de>
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Deal with multiple factorlevel in one grid square

Do you want to do what is called ?zonal statistics? in ArcGIS with ?majority" option?

You may check out this SO question and try the ?mode? function within raster::extract() - maybe it does what you need.

(I?m also unsure if I understand the question correctly)

Cheers, Pat

PhD Student at Department of Geography - GIScience group Friedrich-Schiller-University Jena, Germany
Tel.: +49-3641-9-48973
Web: https://pat-s.github.io

On 24. May 2017, 12:35 +0200, Miriam P?ts <miriam.puets at thuenen.de>, wrote:
> Hi Mike,
>
> I will try to explain it a bit more in detail. Maybe it is easier to understand if I start from the end. In the end I would like to have an ASCII file to read into Ecospace, which has the same extensions and coordinates as other files I already created. This ASCII should contain information about the sediment type within each predefined cell. To create this ASCII file I have a shape file with the polygons representing the sediment type and my grid which I applied to other variables to have the same extend. Now I would like to create a grid containing the information on sediment. Here, per grid cell the sediment type which covers the most of the cell should be defined and connected with the coordinates for the grid cell.
>
> I hope this makes it more clear...
>
>
> <?)))>< >?)))>< >?)))>< >?)))><
>
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources Th?nen-Institut f?r 
> Seefischerei/ Th?nen Institute of Sea fisheries Palmaille 9
> 22767 Hamburg (Germany)
>
> Tel: +49 40 38905-105
> Mail: miriam.puets at thuenen.de
>
>
> Von: "Michael Sumner" <mdsumner at gmail.com
> An: "Miriam P?ts" <miriam.puets at thuenen.de>, r-sig-geo at r-project.org
> Gesendet: Mittwoch, 24. Mai 2017 11:55:14
> Betreff: Re: [R-sig-Geo] Deal with multiple factorlevel in one grid 
> square
>
>
>
> raster::extract(grid, poly, weights = TRUE) is a start. It returns a list which is painful to deal with at first, but can be collected into one data frame for standard summarizing.
>
> I'm still a bit confused about whether you want an estimate of a cell overlap in a polygon or something else.
>
> Cheers, Mike
> On Wed, 24 May 2017, 17:12 Miriam P?ts < [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ] > wrote:
>
>
> Hi everyone,
>
> I have the following problem: I have a personalized grid and a shape file with polygons representing sediment types. Now I would like to apply this grid to the Polygons to identify the sediment type most common within each grid. I tried it with rasterize, but here I can only chose last or first. Die you have any suggestions how I might get the sediment type for each raster cell?
>
> Thank you for your help!
>
> <?)))>< >?)))>< >?)))>< >?)))><
>
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources Th?nen-Institut f?r 
> Seefischerei/ Th?nen Institute of Sea fisheries Palmaille 9
> 22767 Hamburg (Germany)
>
> Tel: +49 40 38905-105
> Mail: [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ]
>
> _______________________________________________
> R-sig-Geo mailing list
> [ mailto:R-sig-Geo at r-project.org | R-sig-Geo at r-project.org ] [ 
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo | 
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo ]
>
>
>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From mdsumner at gmail.com  Tue May 30 14:07:39 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Tue, 30 May 2017 12:07:39 +0000
Subject: [R-sig-Geo] Deal with multiple factorlevel in one grid square
In-Reply-To: <1509510501.13989268.1496145336605.JavaMail.zimbra@thuenen.de>
References: <1857226423.12600330.1495609942428.JavaMail.zimbra@thuenen.de>
 <CAAcGz98HrNL8pNn--G2vY9RKaHMZtABtixScf18L6FTLWH7PTA@mail.gmail.com>
 <85201758.12620837.1495622137454.JavaMail.zimbra@thuenen.de>
 <dba2a2eb-4efb-462c-85cd-a32ffa51152e@Spark>
 <327c98be68434232881786fecbc53d6d@exch1-mel.nexus.csiro.au>
 <1509510501.13989268.1496145336605.JavaMail.zimbra@thuenen.de>
Message-ID: <CAAcGz98ceCFLXX+FAsx1njY1HEQS=tw=Q+9WXQ3L9SunyP+L8w@mail.gmail.com>

maybe fasterize? Only on GitHub, and requires sf.

(Mixing raster and sf is piece-meal, but very doable).

https://github.com/ecohealthalliance/fasterize

On Tue, 30 May 2017, 21:55 Miriam P?ts <miriam.puets at thuenen.de> wrote:

> Hi all,
>
> @Pat: yes, that is almost what I need. If I would do it in ArcGIS I would
> use Polygon to raster (with a second raster as reference) and then choose
> majority.  there a way to do it in R? If I use raster::extract() I would
> extract data from the raster object for the locations of other spatial
> data, but I need spatial data for the raster object...
>
>
> Cheers,
> Miriam
>
> <?)))>< >?)))>< >?)))>< >?)))><
>
> Miriam P?ts
> Marine Lebende Ressourcen/ Marine Living Resources
> Th?nen-Institut f?r Seefischerei/ Th?nen Institute of Sea fisheries
> Palmaille 9
> 22767 Hamburg (Germany)
>
> Tel:  +49 40 38905-105
> Mail: miriam.puets at thuenen.de
>
> ----- Urspr?ngliche Mail -----
> Von: "Alexander Herr" <Alexander.Herr at csiro.au>
> An: "patrick schratz" <patrick.schratz at gmail.com>, mdsumner at gmail.com,
> "miriam puets" <miriam.puets at thuenen.de>
> CC: r-sig-geo at r-project.org
> Gesendet: Donnerstag, 25. Mai 2017 02:28:40
> Betreff: RE: [R-sig-Geo] Deal with multiple factorlevel in one grid square
>
> Hiya,
>
> Do you want a raster with attribute information (ie several attributes
> from your  shape file)? You can achieve this with a workflow that uses
> raster::extract() to assigne unique IDs of polygon to raster cells
> create a RAT for you raster
> assign to the unique IDs of raster RAT the attributes of your polygon
> unique IDs (many to one relationship) with something like merge() or
> functions in libraries like diplyr or data.table
>
> Cheers
> Herry
>
> -----Original Message-----
> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
> Patrick Schratz
> Sent: Wednesday, 24 May 2017 10:48 PM
> To: Michael Sumner <mdsumner at gmail.com>; Miriam P?ts <
> miriam.puets at thuenen.de>
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Deal with multiple factorlevel in one grid square
>
> Do you want to do what is called ?zonal statistics? in ArcGIS with
> ?majority" option?
>
> You may check out this SO question and try the ?mode? function within
> raster::extract() - maybe it does what you need.
>
> (I?m also unsure if I understand the question correctly)
>
> Cheers, Pat
>
> PhD Student at Department of Geography - GIScience group
> Friedrich-Schiller-University Jena, Germany
> Tel.: +49-3641-9-48973
> Web: https://pat-s.github.io
>
> On 24. May 2017, 12:35 +0200, Miriam P?ts <miriam.puets at thuenen.de>,
> wrote:
> > Hi Mike,
> >
> > I will try to explain it a bit more in detail. Maybe it is easier to
> understand if I start from the end. In the end I would like to have an
> ASCII file to read into Ecospace, which has the same extensions and
> coordinates as other files I already created. This ASCII should contain
> information about the sediment type within each predefined cell. To create
> this ASCII file I have a shape file with the polygons representing the
> sediment type and my grid which I applied to other variables to have the
> same extend. Now I would like to create a grid containing the information
> on sediment. Here, per grid cell the sediment type which covers the most of
> the cell should be defined and connected with the coordinates for the grid
> cell.
> >
> > I hope this makes it more clear...
> >
> >
> > <?)))>< >?)))>< >?)))>< >?)))><
> >
> > Miriam P?ts
> > Marine Lebende Ressourcen/ Marine Living Resources Th?nen-Institut f?r
> > Seefischerei/ Th?nen Institute of Sea fisheries Palmaille 9
> > 22767 Hamburg (Germany)
> >
> > Tel: +49 40 38905-105
> > Mail: miriam.puets at thuenen.de
> >
> >
> > Von: "Michael Sumner" <mdsumner at gmail.com
> > An: "Miriam P?ts" <miriam.puets at thuenen.de>, r-sig-geo at r-project.org
> > Gesendet: Mittwoch, 24. Mai 2017 11:55:14
> > Betreff: Re: [R-sig-Geo] Deal with multiple factorlevel in one grid
> > square
> >
> >
> >
> > raster::extract(grid, poly, weights = TRUE) is a start. It returns a
> list which is painful to deal with at first, but can be collected into one
> data frame for standard summarizing.
> >
> > I'm still a bit confused about whether you want an estimate of a cell
> overlap in a polygon or something else.
> >
> > Cheers, Mike
> > On Wed, 24 May 2017, 17:12 Miriam P?ts < [ mailto:
> miriam.puets at thuenen.de | miriam.puets at thuenen.de ] > wrote:
> >
> >
> > Hi everyone,
> >
> > I have the following problem: I have a personalized grid and a shape
> file with polygons representing sediment types. Now I would like to apply
> this grid to the Polygons to identify the sediment type most common within
> each grid. I tried it with rasterize, but here I can only chose last or
> first. Die you have any suggestions how I might get the sediment type for
> each raster cell?
> >
> > Thank you for your help!
> >
> > <?)))>< >?)))>< >?)))>< >?)))><
> >
> > Miriam P?ts
> > Marine Lebende Ressourcen/ Marine Living Resources Th?nen-Institut f?r
> > Seefischerei/ Th?nen Institute of Sea fisheries Palmaille 9
> > 22767 Hamburg (Germany)
> >
> > Tel: +49 40 38905-105
> > Mail: [ mailto:miriam.puets at thuenen.de | miriam.puets at thuenen.de ]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > [ mailto:R-sig-Geo at r-project.org | R-sig-Geo at r-project.org ] [
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo |
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo ]
> >
> >
> >
> > --
> > Dr. Michael Sumner
> > Software and Database Engineer
> > Australian Antarctic Division
> > 203 Channel Highway
> > Kingston Tasmania 7050 Australia
> >
> >
> > [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From c.cavalieri at me.com  Tue May 30 14:26:37 2017
From: c.cavalieri at me.com (Carlo Cavalieri)
Date: Tue, 30 May 2017 14:26:37 +0200
Subject: [R-sig-Geo] How to fit a pure spatial variogram on a
 spatio-temporal empirical one
In-Reply-To: <45f0ab87-51c9-dc24-5193-00bd08ebc2fe@52north.org>
References: <2D22C648-971F-410F-A662-261A66C475CC@me.com>
 <45f0ab87-51c9-dc24-5193-00bd08ebc2fe@52north.org>
Message-ID: <88656E23-2D64-4D87-B6BC-4544BDAE41C7@me.com>

Thank you very much Ben, the code works perfectly.
I have 2 years time series, one for each station, of daily mean average pm10 which I have detrended wrt mean and standard deviation by using land use coefficients.
The problem I am trying to face is to distinguish between the spatial and the temporal components of the correlation among time series, the former due to distance, the latter mostly due to seasonality. 
Does the pooled variogram obtained with the code you posted here isolates the temporal component of the correlation and only assume constant covariance structure over time or is it influenced by seasonality?

> Il giorno 30 mag 2017, alle ore 10:48, Dr. Benedikt Gr?ler <b.graeler at 52north.org> ha scritto:
> 
> Dear Carlo,
> 
> the code below is a bit of a hack, but does what you are asking for. The classes "gstatVariogram" and "StVariogram" have slight different design and so do the functions fit.variogram and fit.StVariogram. Note that spVv is now a pooled variogram across all time steps of your dataset treating each time slice as an independent copy of the same pure spatial process (i.e. strong temporal autocorrelation might influence your estimation).
> 
> HTH,
> 
> Ben
> 
> 
> library(gstat)
> data("vv")
> plot(vv)
> 
> spaceOnly <- vv$timelag == 0
> 
> spVv <- cbind(vv[spaceOnly,],
>              data.frame(dir.hor=rep(0, sum(spaceOnly)),
>                         dir.ver=rep(0, sum(spaceOnly))))
> 
> # drop empty (NA) first row
> spVv <- spVv[-1, ]
> 
> # manually re-class
> class(spVv) <- c("gstatVariogram","data.frame")
> 
> plot(spVv)
> 
> fitSpVgm <- fit.variogram(spVv, vgm(30, "Exp", 150, 10))
> plot(spVv, fitSpVgm)
> 
> 
> 
>> On 29/05/2017 20:13, Carlo Cavalieri wrote:
>> Hi, I am using the R package GSTAT to make a spatio-temporal interpolation for my thesis and I wanted to know if it was possible to obtain the pure spatial empirical variogram from the spatio-temporal so that I can use it to fit a pure spatial variogram, for example exponential.
>> Unfortunately fit.variogram only accepts objects output of variogram, not of variogramST. One possible solution could be to extract tlag=0 from the StVariogram and convert the output to class variogramModel, but I have no idea on how to do this.
>> I look for a way to do this because fit the spatial variogram for each day separately is not a good idea given the small number of observation stations.
>> One way is definitely possible since the authors of the paper "Spatio-Temporal Interpolation using gstat? managed to compare the results of pure spatial and spatio-temporal interpolation (that is what I want to do): Below a quotation from that paper.
>> "For comparison with classical approaches, we interpolate across Germany iteratively for each single day using all available data for variogram estimation. The purely spatial empirical variogram can directly be obtained from the empirical spatio-temporal variogram, by ?xing the temporal lag at 0 separation. From the same set of variogram models as investigated for the spatio-temporal models, the exponential model (partial sill: 66.5, range: 224 km, nugget: 13.5) is the best suited based on the optimisation criterion.?
>> Does anyone have any idea?
>> Thank you
>> Carlo
>>    [[alternative HTML version deleted]]
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 
> -- 
> Dr. Benedikt Gr?ler
> 52?North Initiative for Geospatial Open Source Software GmbH
> Martin-Luther-King-Weg 24
> 48155 Muenster, Germany
> 
> E-Mail: b.graeler at 52north.org
> Fon: +49-(0)-251/396371-39
> Fax: +49-(0)-251/396371-11
> 
> http://52north.org/
> Twitter: @FiveTwoN
> 
> General Managers: Dr. Albert Remke, Dr. Andreas Wytzisk
> Local Court Muenster HRB 10849
> <b_graeler.vcf>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From Andy.Bunn at wwu.edu  Tue May 30 19:45:53 2017
From: Andy.Bunn at wwu.edu (Andy Bunn)
Date: Tue, 30 May 2017 17:45:53 +0000
Subject: [R-sig-Geo] Google Earth Engine?
In-Reply-To: <CANVKczNmepxdwHivb6_h+2vqM1p0OppLYf3m-uWc5SY5idtycg@mail.gmail.com>
References: <D54DFB5F.69AA9%andy.bunn@wwu.edu>
 <CANVKczNmepxdwHivb6_h+2vqM1p0OppLYf3m-uWc5SY5idtycg@mail.gmail.com>
Message-ID: <D552FDA2.69CBE%andy.bunn@wwu.edu>


On 5/26/17, 4:02 PM, "b.rowlingson at gmail.com on behalf of Barry
Rowlingson" <b.rowlingson at gmail.com on behalf of
b.rowlingson at lancaster.ac.uk> wrote:

>On Fri, May 26, 2017 at 11:34 PM, Andy Bunn <Andy.Bunn at wwu.edu> wrote:
>> Does anybody out there interface with the google earth engine from R?
>>I'm too old a dog to learn python. -Andy
>
>Too old? Never! See: https://www.xkcd.com/353/


Perfect! I had forgotten that one. Well, I think I might have found a new
project. The GEE seems like the perfect tool for Landsat products. Thanks,
A



>
>Given that the other supported option is Javascript....
>
>I suspect a solution using one of the R-Python interfaces
>("reticulate" perhaps) might be the best solution, but you still might
>have to learn python to construct the analysis jobs.
>
>Barry
>
>
>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


