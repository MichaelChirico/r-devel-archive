From Roger.Bivand at nhh.no  Sat Sep  1 00:48:31 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 1 Sep 2007 00:48:31 +0200 (CEST)
Subject: [R-sig-Geo] shapefile errors and kriging limits
In-Reply-To: <DA0943DAEC5E044096035B01B6C542631C7756@ci04.mail2.esa.ipvc.pt>
References: <DA0943DAEC5E044096035B01B6C542631C7755@ci04.mail2.esa.ipvc.pt>
	<DA0943DAEC5E044096035B01B6C542631C7756@ci04.mail2.esa.ipvc.pt>
Message-ID: <Pine.LNX.4.64.0709010043530.18662@reclus.nhh.no>

On Fri, 31 Aug 2007, Carlos A. Bastos M.Guerra wrote:

>
> Dear Roger,
>
> Regarding the shapefile question I think it was a memory problem because 
> I had 88000 points and wen I reduced the number of points to half it 
> worked fine.
>
> Regarding the second question: thanks for the help and your solution 
> almost solves my problems, but I forgot to tell you that I am running 
> this on Mac OS, so rgdal doesn't run (at least that I know of). Because 
> of this I am also strugling to find the correct method to optmise the 
> parameters to do the fiting of the variogram (sill, range and nugget), 
> that rgdal facilitates...

rdal is not a geostatistics package, so this is unclear. With this many 
observations, you will need to be careful with the maximum number of 
points included and/or the maximum distance.

>
> Can you recomend another package beside rgdal, or do you know of a 
> version that runs on Mac??

The rgdal package runs fine on OSX, but like other Unix platforms, you 
have to fulfill its external dependencies and then install from source. 
There are details for getting hold of the external dependencies on the 
Rgeo website. For your size of data, you should take the necessary time to 
do that.

Best wishes,

Roger

>
> Best regards,
> Carlos
>
>
>
>
>
> Em 30/08/2007, ?s 23:16, Roger Bivand escreveu:
>
> On Thu, 30 Aug 2007, Carlos Guerra wrote:
>
> Dear useRs,
>
> I am trying to convert the predictions of a kriging model into a
> shapefile but I am getting some errors and I am getting nowere with
> my solutions...
>
> Without the error messages and the output of sessionInfo(), it isn't easy to say.
>
>
> This is the code I am running:
>
> a <- data.frame(Id=seq(1,length(pred.grid[,1]),1),X=pred.grid[,
> 1],Y=pred.grid[,2])
> a_dbf <- data.frame(Id=seq(1,length(pred.grid[,1]),1), data=kc2$predict)
> shp_1 <- convert.to.shapefile(a, a_dbf, field="Id", type=1)
>
> were pre.grid is determined by this code:
>
> min_x <- 142794
> max_x <- 152121
> min_y <- 485080
> max_y <- 508887
> pred.grid <- expand.grid(seq(min_x, max_x, 50), seq(min_y, max_y, 50))
>
> and kc2 is an object returned by the aplication of the function
> krige.conv:
>
> kc2 <- krige.conv(dados_g, loc=pred.grid, krige=krige.control
> (obj.m=vario_fit2))
>
>
> Although you are free to use the shapefiles package, your questions suggest that you might be better served by using sp classes and either maptools or rgdal to read and write your files. If you read the points data with readOGR() or readShapePoints(), created a GridTopology for your grid, and used the overlay() method from sp to cookie-cut the grid with a SpatialPolygonsDataFrame object read by readShapePoly() or readOGR(), and then simply passed coordinates() of the SpatialPixelsDataFrame object created after the overlaying to the loc= argument, you would be very close. Use bbox() of the imported SpatialPointsDataFrame object to find out what the grid should be. Consider reading the vignette of the sp package - and finally just output the SpatialPixelsDataFrame augmented with the prediction as a new column as a GeoTiff file using writeGDAL() in rgdal, choosing only the predictions.
>
> There are a number of steps to take, but it does work.
>
> Hope this helps,
>
> Roger
>
>
>
>
> Another question:
>
> as you can see I am predicting my values to a square set of points...
> my question is how can I can I generate a set of point that match a
> specific area (because I have a specific limite(area) in an esri's
> shapefile).
>
>
>
> Thanks in advance.
> Carlos
>
>
>
> Carlos GUERRA
>
> Gabinete de Sistemas de Informacao Geografica
> Escola Superior Agraria de Ponte de Lima
> Mosteiro de Refoios do Lima
> 4990-706 Ponte de Lima
>
> Tlm: +351 91 2407109
> Tlf: +351 258 909779
>
> Be a Mac user... update your self!
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From Roger.Bivand at nhh.no  Sat Sep  1 00:56:27 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 1 Sep 2007 00:56:27 +0200 (CEST)
Subject: [R-sig-Geo] center of gravity
In-Reply-To: <46D85F0A.2080101@nceas.ucsb.edu>
References: <BAY137-W257FFAB121F472F50C4A6EE1CE0@phx.gbl>
	<46D85F0A.2080101@nceas.ucsb.edu>
Message-ID: <Pine.LNX.4.64.0709010051090.18662@reclus.nhh.no>

On Fri, 31 Aug 2007, Rick Reeves wrote:

> Hi: look for Centroid calculations:
>
>> help.search("centroid") will show:
>
> Help files with alias or concept or title matching 'centroid' using fuzzy 
> matching:
>
>
>
> apply.polygon(maps)                        Polygon functions
> get.Pcent(maptools)                        Polygon centroids
> calcCentroid(PBSmapping)                   Calculate the Centroids of 
> Polygons
> centroid.owin(spatstat)                    Centroid of a window
> affinexy(spatstat)                         Internal spatstat functions
>
> I suspect that the best one for your case would be centroid.owin, since it 
> has the most
> straightforward input data requirements.

I agree, but am not sure that the question is about centroids. It might be 
the "centre" of a point pattern, where the highest density location(s) 
would not often be the centroid of the convex hull. If the data are 
SpatialPolygons read using readShapePoly() or readOGR(), the coordnates() 
method for the imported object returns a matrix of label points, which are 
the centroids of the largest member Polygon object of each Polygons 
object, so they can be retrieved without any more trouble, but I'm not 
sure that the points needed are area weighted.

Roger

>
> Hope this helps,
> Rick Reeves
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From paulojus at c3sl.ufpr.br  Sat Sep  1 01:24:28 2007
From: paulojus at c3sl.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 31 Aug 2007 20:24:28 -0300 (BRT)
Subject: [R-sig-Geo] Gaussian Variogram Positive Definite?
In-Reply-To: <46D88595.40602@email.unc.edu>
References: <46D88595.40602@email.unc.edu>
Message-ID: <Pine.LNX.4.58.0708312019490.6716@dalmore.c3sl.ufpr.br>

Brian

Gaussian variograms are known to generate numeric problems
in case you have the nugget parameter equals to zero.
This occours because the almost flat
and with points very close to each other the covariance matrix will be
nearly-singular -- numerically singular.

Soma alternatives are:
1. choose another covariance model:
   for instance a Matern model with smoothness parameter  to ensure a
behaviour which is similar to the gaussian (e.g. kappa = 4 in the
parametrisation used in geoR

2. add a small nugget to the model to make the covariance matrix
diagonally dominant

hope this helps

best
P.J.




Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus AT  ufpr  br
http://www.leg.ufpr.br/~paulojus

On Fri, 31 Aug 2007, Brian J. Lopes wrote:

> Hello All:
>
> I've been banging my head against the wall about this for quite some
> time now, and I can't seem to find any reference on the matter.  I'm
> trying to calculate MLE estimates for the Gaussian variogram, but it
> seems that I consistently reach the point where the covariance matrix is
> not positive definite, as dictated by the Cholesky decomposition, even
> though the range parameter is indeed positive (note that I am also
> incorporating a nugget to sill ratio as well).  Has anybody else
> experienced this problem?  Better yet, does anybody have any references
> that discuss the situation, or how I can avoid it?
>
> The data is a bit large, so if an example is necessary I'll try to see
> if I can come up with something reasonable.
>
> Thanks,
> Brian
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From marco.helbich at gmx.at  Sat Sep  1 11:29:43 2007
From: marco.helbich at gmx.at (Marco Helbich)
Date: Sat, 1 Sep 2007 11:29:43 +0200
Subject: [R-sig-Geo] allocate error Kenv.csr()
Message-ID: <000301c7ec7a$a13c3030$0300a8c0@mirk>

Dear List,

I want to calculate envelopes of Khat from simulations of csr with the 
Kenv.csr(splancs) function. My dataset contains around 300 cases. Using the 
Kenv.csr function (testing only 3 simulation runs) I got following error 
message:

> UL.khat <- Kenv.csr(pts.dat, stadtreg.pol, nsim=3, s)
Doing simulation  1
Fehler: kann Vektor der Gr??e 2.2 MB nicht allozieren
Zus?tzlich: Es gab 50 oder mehr Warnungen (Anzeige der ersten 50 mit 
warnings())

Enclosed please find the output from the sessionInfo(), traceback() and 
gc():

> sessionInfo()
R version 2.5.0 (2007-04-23)
i386-pc-mingw32

locale:
LC_COLLATE=German_Austria.1252;LC_CTYPE=German_Austria.1252;LC_MONETARY=German_Austria.1252;LC_NUMERIC=C;LC_TIME=German_Austria.1252

attached base packages:
[1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
[7] "base"

other attached packages:
    rgdal  maptools   foreign   splancs        sp
  "0.5-9"  "0.6-12"  "0.8-20" "2.01-22"  "0.9-13"
> traceback()
427: rbind(pts, genpts)
426: ranpts(pts, poly, nprq - npgen)
425: ranpts(pts, poly, nprq - npgen)
424: ranpts(pts, poly, nprq - npgen)
.....
.....
5: ranpts(pts, poly, nprq - npgen)
4: ranpts(NULL, poly, npoints)
3: csr(poly, nptg)
2: khat(csr(poly, nptg), poly, s)
1: Kenv.csr(pts.dat, stadtreg.pol, nsim = 3, s)
> gc()
         used (Mb) gc trigger  (Mb) max used  (Mb)
Ncells 433048 11.6     741108  19.8   661425  17.7
Vcells 219455  1.7   52498390 400.6 62525504 477.1
>

Best regards
Marco
______________________________

Marco Helbich
ISR
Postgasse 7/4/2
A-1010, Vienna
Tel.: +43 (0) 699 11 71 44 02
mail (a): marco.helbich at univie.ac.at
mail (b): marco.helbich at gmx.at



From Roger.Bivand at nhh.no  Sun Sep  2 00:09:17 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 2 Sep 2007 00:09:17 +0200 (CEST)
Subject: [R-sig-Geo] allocate error Kenv.csr()
In-Reply-To: <000301c7ec7a$a13c3030$0300a8c0@mirk>
References: <000301c7ec7a$a13c3030$0300a8c0@mirk>
Message-ID: <Pine.LNX.4.64.0709012354370.21864@reclus.nhh.no>

On Sat, 1 Sep 2007, Marco Helbich wrote:

> Dear List,
>
> I want to calculate envelopes of Khat from simulations of csr with the
> Kenv.csr(splancs) function. My dataset contains around 300 cases. Using the
> Kenv.csr function (testing only 3 simulation runs) I got following error
> message:
>
>> UL.khat <- Kenv.csr(pts.dat, stadtreg.pol, nsim=3, s)

?Kenv.csr

The first argument, nptg: Number of points to generate in each simulation, 
is not the matrix of points, just the number of points to simulate. Since 
we do not know what the first value of pts.dat is, I can only guess that 
it is very large, and setting the first value of the erroneous points to 
17158834, I can reproduce your problem. If you set it to scalar 300, 
everything will work.

Hope this helps,

Roger

PS. The warnings do say that the condition has length > 1 and only the 
first element will be used in:  if (npgen < nprq) {  ...
where nprg is the number of points required.


> Doing simulation  1
> Fehler: kann Vektor der Gr??e 2.2 MB nicht allozieren
> Zus?tzlich: Es gab 50 oder mehr Warnungen (Anzeige der ersten 50 mit
> warnings())
>
> Enclosed please find the output from the sessionInfo(), traceback() and
> gc():
>
>> sessionInfo()
> R version 2.5.0 (2007-04-23)
> i386-pc-mingw32
>
> locale:
> LC_COLLATE=German_Austria.1252;LC_CTYPE=German_Austria.1252;LC_MONETARY=German_Austria.1252;LC_NUMERIC=C;LC_TIME=German_Austria.1252
>
> attached base packages:
> [1] "stats"     "graphics"  "grDevices" "utils"     "datasets"  "methods"
> [7] "base"
>
> other attached packages:
>    rgdal  maptools   foreign   splancs        sp
>  "0.5-9"  "0.6-12"  "0.8-20" "2.01-22"  "0.9-13"
>> traceback()
> 427: rbind(pts, genpts)
> 426: ranpts(pts, poly, nprq - npgen)
> 425: ranpts(pts, poly, nprq - npgen)
> 424: ranpts(pts, poly, nprq - npgen)
> .....
> .....
> 5: ranpts(pts, poly, nprq - npgen)
> 4: ranpts(NULL, poly, npoints)
> 3: csr(poly, nptg)
> 2: khat(csr(poly, nptg), poly, s)
> 1: Kenv.csr(pts.dat, stadtreg.pol, nsim = 3, s)
>> gc()
>         used (Mb) gc trigger  (Mb) max used  (Mb)
> Ncells 433048 11.6     741108  19.8   661425  17.7
> Vcells 219455  1.7   52498390 400.6 62525504 477.1
>>
>
> Best regards
> Marco
> ______________________________
>
> Marco Helbich
> ISR
> Postgasse 7/4/2
> A-1010, Vienna
> Tel.: +43 (0) 699 11 71 44 02
> mail (a): marco.helbich at univie.ac.at
> mail (b): marco.helbich at gmx.at
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From paulojus at c3sl.ufpr.br  Sun Sep  2 02:32:02 2007
From: paulojus at c3sl.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Sat, 1 Sep 2007 21:32:02 -0300 (BRT)
Subject: [R-sig-Geo] geoR likfit message
In-Reply-To: <46D6A973.CF06.00FB.0@uaex.edu>
References: <46D6A973.CF06.00FB.0@uaex.edu>
Message-ID: <Pine.LNX.4.58.0709012130520.10126@dalmore.c3sl.ufpr.br>

Not really Terry

tha kappa parameter is an "extra"parameters used in some but not all
models.
Spherical end exponential models for the correlation functions are cases
where kappa is note used and, therfore, the argument makes no diference

best
P.J.


Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus AT  ufpr  br
http://www.leg.ufpr.br/~paulojus

On Thu, 30 Aug 2007, Terry Griffin wrote:

> Greetings,
>
> I am revisiting some simulations that I ran about a year ago.  When I run the likfit function from geoR; a message is returned that reads "kappa not used for the spherical correlation function".  The simulations still run, but wasn't sure if this message indicates another problem that I need to be aware of.  I'm not using "fix.kappa=TRUE" or other references to kappa.  Below is the line in the code that is returning the message.  The "cressie" refers to the variofit function.
>
> geo.reml<-likfit(data.geo,coords=coords.geo,data=y,cov.model="spherical",trend=~s2+s3+s4+t+ts2+ts3+ts4,method.lik="REML",ini=c(cressie[2]$cov.pars[1],cressie[2]$cov.pars[2]), messages=FALSE)#
>
> Is this anything to be concerned about?
>
> Thank you,
>
> Terry
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From epistat at gmail.com  Sun Sep  2 18:06:16 2007
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 3 Sep 2007 00:06:16 +0800
Subject: [R-sig-Geo] Calculate the angles for a spatial point patern?
Message-ID: <2fc17e30709020906t15325dc9na865e790fd6fbf9b@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070903/eb56d0fe/attachment.pl>

From peter.adler at usu.edu  Sun Sep  2 19:43:17 2007
From: peter.adler at usu.edu (Peter Adler)
Date: Sun, 02 Sep 2007 11:43:17 -0600
Subject: [R-sig-Geo] polygon buffers?
Message-ID: <46DAF635.60705@usu.edu>

Dear R gurus,

I am very excited about all the spatials tools in R because they have 
almost liberated me from ESRI products. One of the last things I have 
yet to figure out how to do in R is to create buffers around polygons 
(or just expand polygons). I searched the R-sig-geo archives (using 
Google) for "polygon buffer" and "transform polygon" but haven't found 
anything to get me started. Thanks in advance for your ideas,

Peter

-- 
Peter Adler
Department of Wildland Resources
5230 Old Main Hill
Utah State University
Logan, UT 84322
tel: (435) 797-1021 / email: peter.adler at usu.edu



From tkeitt at gmail.com  Mon Sep  3 03:45:51 2007
From: tkeitt at gmail.com (Tim Keitt)
Date: Sun, 2 Sep 2007 20:45:51 -0500
Subject: [R-sig-Geo] polygon buffers?
In-Reply-To: <46DAF635.60705@usu.edu>
References: <46DAF635.60705@usu.edu>
Message-ID: <6262c54c0709021845q3df4bbacyfd6e7f4ccdb49f12@mail.gmail.com>

Hi Peter,

I've been doing this in postgis. Once you get used to it, its not too
difficult. Load data with shp2pgsql (or use rgdal + RODBC or similar
if you want to stay in R) and then create a new table to hold the
results. After that you can do "insert into restab select
buffer(the_geom, dist) from sourcetab" (that's from memory, so check
the postgis docs).

THK

On 9/2/07, Peter Adler <peter.adler at usu.edu> wrote:
> Dear R gurus,
>
> I am very excited about all the spatials tools in R because they have
> almost liberated me from ESRI products. One of the last things I have
> yet to figure out how to do in R is to create buffers around polygons
> (or just expand polygons). I searched the R-sig-geo archives (using
> Google) for "polygon buffer" and "transform polygon" but haven't found
> anything to get me started. Thanks in advance for your ideas,
>
> Peter
>
> --
> Peter Adler
> Department of Wildland Resources
> 5230 Old Main Hill
> Utah State University
> Logan, UT 84322
> tel: (435) 797-1021 / email: peter.adler at usu.edu
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


-- 
Timothy H. Keitt, University of Texas at Austin
Contact info and schedule at http://www.keittlab.org/tkeitt/
Reprints at http://www.keittlab.org/tkeitt/papers/
ODF attachment? See http://www.openoffice.org/



From paulojus at c3sl.ufpr.br  Mon Sep  3 04:12:55 2007
From: paulojus at c3sl.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Sun, 2 Sep 2007 23:12:55 -0300 (BRT)
Subject: [R-sig-Geo] polygon buffers?
In-Reply-To: <46DAF635.60705@usu.edu>
References: <46DAF635.60705@usu.edu>
Message-ID: <Pine.LNX.4.58.0709022311450.30656@dalmore.c3sl.ufpr.br>

Peter
There may be other alternatives, but
this is one of the functionalities in an external software (terralib) that
the aRT package tries to make usage.

The aRT web page has one example:
www.leg.ufpr.br/aRT


Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus AT  ufpr  br
http://www.leg.ufpr.br/~paulojus

On Sun, 2 Sep 2007, Peter Adler wrote:

> Dear R gurus,
>
> I am very excited about all the spatials tools in R because they have
> almost liberated me from ESRI products. One of the last things I have
> yet to figure out how to do in R is to create buffers around polygons
> (or just expand polygons). I searched the R-sig-geo archives (using
> Google) for "polygon buffer" and "transform polygon" but haven't found
> anything to get me started. Thanks in advance for your ideas,
>
> Peter
>
> --
> Peter Adler
> Department of Wildland Resources
> 5230 Old Main Hill
> Utah State University
> Logan, UT 84322
> tel: (435) 797-1021 / email: peter.adler at usu.edu
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From Agustin.Lobo at ija.csic.es  Mon Sep  3 07:40:06 2007
From: Agustin.Lobo at ija.csic.es (Agustin Lobo)
Date: Mon, 03 Sep 2007 07:40:06 +0200
Subject: [R-sig-Geo] polygon buffers?
In-Reply-To: <Pine.LNX.4.58.0709022311450.30656@dalmore.c3sl.ufpr.br>
References: <46DAF635.60705@usu.edu>
	<Pine.LNX.4.58.0709022311450.30656@dalmore.c3sl.ufpr.br>
Message-ID: <46DB9E36.9040809@ija.csic.es>

Very interesting work, Paulo.
Can you display from R on the TerraView window?

I dream with a GIS tool (A GIS display tool for R? An R plugging for 
Qgis? etc)
that could display spatial R objects. This is beyond current
tools, based on exporting (hence duplicating space and troubles)
R objects to other formats.

Agus

Paulo Justiniano Ribeiro Jr escribi?:
> Peter
> There may be other alternatives, but
> this is one of the functionalities in an external software (terralib) that
> the aRT package tries to make usage.
> 
> The aRT web page has one example:
> www.leg.ufpr.br/aRT
> 
> 
> Paulo Justiniano Ribeiro Jr
> LEG (Laborat?rio de Estat?stica e Geoinforma??o)
> Universidade Federal do Paran?
> Caixa Postal 19.081
> CEP 81.531-990
> Curitiba, PR  -  Brasil
> Tel: (+55) 41 3361 3573
> Fax: (+55) 41 3361 3141
> e-mail: paulojus AT  ufpr  br
> http://www.leg.ufpr.br/~paulojus
> 
> On Sun, 2 Sep 2007, Peter Adler wrote:
> 
>> Dear R gurus,
>>
>> I am very excited about all the spatials tools in R because they have
>> almost liberated me from ESRI products. One of the last things I have
>> yet to figure out how to do in R is to create buffers around polygons
>> (or just expand polygons). I searched the R-sig-geo archives (using
>> Google) for "polygon buffer" and "transform polygon" but haven't found
>> anything to get me started. Thanks in advance for your ideas,
>>
>> Peter
>>
>> --
>> Peter Adler
>> Department of Wildland Resources
>> 5230 Old Main Hill
>> Utah State University
>> Logan, UT 84322
>> tel: (435) 797-1021 / email: peter.adler at usu.edu
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Dr. Agustin Lobo
Institut de Ciencies de la Terra "Jaume Almera" (CSIC)
LLuis Sole Sabaris s/n
08028 Barcelona
Spain
Tel. 34 934095410
Fax. 34 934110012
email: Agustin.Lobo at ija.csic.es
http://www.ija.csic.es/gt/obster



From joshua_palmer at ncsu.edu  Mon Sep  3 08:03:43 2007
From: joshua_palmer at ncsu.edu (Joshua Palmer)
Date: Mon, 3 Sep 2007 02:03:43 -0400
Subject: [R-sig-Geo] geoR Beta Estimates Not Equal to lm Coefficients: Why?
Message-ID: <002e01c7edf0$2f3317c0$640fa8c0@noreaster2005>

Hello everyone,

I am using the excellent geoR package to perform Kriging with External
Drift.  As to be expected, for any given set of covariates, trend
coefficients I obtain performing least-squares regression in SAS equal those
coefficients obtained by using R's lm function.  However, those trend
coefficients do NOT match the estimates for the beta values when I run
krige.conv (or likfit or variofit) in geoR.  The coefficients are often
similar, but they are never nearly or exactly the same.  I was under the
assumption that geoR utilizes least-squares regression (on the trend.d
matrix) a la R's lm function to derive beta estimates.  This appears to be
incorrect.

Would anyone be able to explain to me why I am noting this difference or
should I be noting a difference?  I have researched R mailing lists and geoR
documentation but have been unable to find an answer.  Please let me know if
additional specificity is needed.  This dilemma is universal across
different sets of covariates and covariance models or parameters.

Any assistance is greatly appreciated and I thank you for your time.
Joshua Palmer
Meteorologist
Atlanta, GA, USA



From e.pebesma at geo.uu.nl  Mon Sep  3 08:47:03 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Mon, 03 Sep 2007 08:47:03 +0200
Subject: [R-sig-Geo] geoR Beta Estimates Not Equal to lm Coefficients:
 Why?
In-Reply-To: <002e01c7edf0$2f3317c0$640fa8c0@noreaster2005>
References: <002e01c7edf0$2f3317c0$640fa8c0@noreaster2005>
Message-ID: <46DBADE7.7070802@geo.uu.nl>

Joshua,

My guess is that to estimate beta geoR uses generalized least squares 
and lm and SAS ordinary least squares. If you use a pure nugget model,  
or some other model with a range parameter sufficiently close to zero, 
i.e. model the observations as independent, the estimates should be the 
same.
--
Edzer

Joshua Palmer wrote:
> Hello everyone,
>
> I am using the excellent geoR package to perform Kriging with External
> Drift.  As to be expected, for any given set of covariates, trend
> coefficients I obtain performing least-squares regression in SAS equal those
> coefficients obtained by using R's lm function.  However, those trend
> coefficients do NOT match the estimates for the beta values when I run
> krige.conv (or likfit or variofit) in geoR.  The coefficients are often
> similar, but they are never nearly or exactly the same.  I was under the
> assumption that geoR utilizes least-squares regression (on the trend.d
> matrix) a la R's lm function to derive beta estimates.  This appears to be
> incorrect.
>
> Would anyone be able to explain to me why I am noting this difference or
> should I be noting a difference?  I have researched R mailing lists and geoR
> documentation but have been unable to find an answer.  Please let me know if
> additional specificity is needed.  This dilemma is universal across
> different sets of covariates and covariance models or parameters.
>
> Any assistance is greatly appreciated and I thank you for your time.
> Joshua Palmer
> Meteorologist
> Atlanta, GA, USA
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From b.rowlingson at lancaster.ac.uk  Mon Sep  3 09:22:43 2007
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 03 Sep 2007 08:22:43 +0100
Subject: [R-sig-Geo] polygon buffers?
In-Reply-To: <46DB9E36.9040809@ija.csic.es>
References: <46DAF635.60705@usu.edu>	<Pine.LNX.4.58.0709022311450.30656@dalmore.c3sl.ufpr.br>
	<46DB9E36.9040809@ija.csic.es>
Message-ID: <46DBB643.70403@lancaster.ac.uk>

Agustin Lobo wrote:

> I dream with a GIS tool (A GIS display tool for R? An R plugging for 
> Qgis? etc)
> that could display spatial R objects. This is beyond current
> tools, based on exporting (hence duplicating space and troubles)
> R objects to other formats.

  Oooh, you are giving me ideas! I've been working with R and QGIS for 
some time, linked by Python plugins.

  For example, my Basic Stats plugin for QGIS lets you do data summaries 
of QGIS layers (histograms of attributes, x-y plots etc):

http://www.maths.lancs.ac.uk/~rowlings/Software/Spqr/

  This is only a small step from writing what's called a 'Provider' for 
QGIS, that could display sp-style data objects from R on the QGIS map 
canvas...

  If my Java skills were more developed I'd be working on something 
similar for OpenJUMP, which I think has all the topology operators like 
buffering that the original poster on this thread was interested in.

Barry



From paulojus at c3sl.ufpr.br  Mon Sep  3 18:42:05 2007
From: paulojus at c3sl.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Mon, 3 Sep 2007 13:42:05 -0300 (BRT)
Subject: [R-sig-Geo] polygon buffers?
In-Reply-To: <46DB9E36.9040809@ija.csic.es>
References: <46DAF635.60705@usu.edu>
	<Pine.LNX.4.58.0709022311450.30656@dalmore.c3sl.ufpr.br>
	<46DB9E36.9040809@ija.csic.es>
Message-ID: <Pine.LNX.4.58.0709031336470.30656@dalmore.c3sl.ufpr.br>

Hi Agustin

The short answer is not.
However this mechanism in somewhat in the plans
in something we've called temporalilly "myR"
in the sense of R from my application.

The "inners" would use the same mecanism and possibilities are to
add R capabilities in a customized TerraView and/or call R directly.
It has been tested as proof of concept and works
as described in the "papers and presentation" section of the aRT page,
but polish and expend this is necessary:

  A Process and Environment for Embedding The R Software into TerraLib
  P. R. Andrade Neto, P. J. Ribeiro Junior
  GeoInfo 2005 [pdf, 185k] [presentation, 593k]

The email is cc to Pedro, the project maintaner who will be able to
provide more details if necessary

best
P.J.



On Mon, 3 Sep 2007, Agustin Lobo wrote:

> Very interesting work, Paulo.
> Can you display from R on the TerraView window?
>
> I dream with a GIS tool (A GIS display tool for R? An R plugging for
> Qgis? etc)
> that could display spatial R objects. This is beyond current
> tools, based on exporting (hence duplicating space and troubles)
> R objects to other formats.
>
> Agus
>
> Paulo Justiniano Ribeiro Jr escribi?:
> > Peter
> > There may be other alternatives, but
> > this is one of the functionalities in an external software (terralib) that
> > the aRT package tries to make usage.
> >
> > The aRT web page has one example:
> > www.leg.ufpr.br/aRT
> >
> >
> > Paulo Justiniano Ribeiro Jr
> > LEG (Laborat?rio de Estat?stica e Geoinforma??o)
> > Universidade Federal do Paran?
> > Caixa Postal 19.081
> > CEP 81.531-990
> > Curitiba, PR  -  Brasil
> > Tel: (+55) 41 3361 3573
> > Fax: (+55) 41 3361 3141
> > e-mail: paulojus AT  ufpr  br
> > http://www.leg.ufpr.br/~paulojus
> >
> > On Sun, 2 Sep 2007, Peter Adler wrote:
> >
> >> Dear R gurus,
> >>
> >> I am very excited about all the spatials tools in R because they have
> >> almost liberated me from ESRI products. One of the last things I have
> >> yet to figure out how to do in R is to create buffers around polygons
> >> (or just expand polygons). I searched the R-sig-geo archives (using
> >> Google) for "polygon buffer" and "transform polygon" but haven't found
> >> anything to get me started. Thanks in advance for your ideas,
> >>
> >> Peter
> >>
> >> --
> >> Peter Adler
> >> Department of Wildland Resources
> >> 5230 Old Main Hill
> >> Utah State University
> >> Logan, UT 84322
> >> tel: (435) 797-1021 / email: peter.adler at usu.edu
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at stat.math.ethz.ch
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at stat.math.ethz.ch
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Dr. Agustin Lobo
> Institut de Ciencies de la Terra "Jaume Almera" (CSIC)
> LLuis Sole Sabaris s/n
> 08028 Barcelona
> Spain
> Tel. 34 934095410
> Fax. 34 934110012
> email: Agustin.Lobo at ija.csic.es
> http://www.ija.csic.es/gt/obster
>



From paulojus at c3sl.ufpr.br  Mon Sep  3 18:44:55 2007
From: paulojus at c3sl.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Mon, 3 Sep 2007 13:44:55 -0300 (BRT)
Subject: [R-sig-Geo] geoR Beta Estimates Not Equal to lm Coefficients:
 Why?
In-Reply-To: <002e01c7edf0$2f3317c0$640fa8c0@noreaster2005>
References: <002e01c7edf0$2f3317c0$640fa8c0@noreaster2005>
Message-ID: <Pine.LNX.4.58.0709031342450.30656@dalmore.c3sl.ufpr.br>

Josha

If I've got oyur question correctly the diference is because geoR is
estimating both, regression coeficientes and covariance parameters and
therefore the estimates ar not the same as lm() which uses
ordinary least squares.
In other words the regression parameters on geoR are estimated
considering the spatial correlation between the sample points
as described by the assumed model

Hope this helps
P.J.


Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o)
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus AT  ufpr  br
http://www.leg.ufpr.br/~paulojus

On Mon, 3 Sep 2007, Joshua Palmer wrote:

> Hello everyone,
>
> I am using the excellent geoR package to perform Kriging with External
> Drift.  As to be expected, for any given set of covariates, trend
> coefficients I obtain performing least-squares regression in SAS equal those
> coefficients obtained by using R's lm function.  However, those trend
> coefficients do NOT match the estimates for the beta values when I run
> krige.conv (or likfit or variofit) in geoR.  The coefficients are often
> similar, but they are never nearly or exactly the same.  I was under the
> assumption that geoR utilizes least-squares regression (on the trend.d
> matrix) a la R's lm function to derive beta estimates.  This appears to be
> incorrect.
>
> Would anyone be able to explain to me why I am noting this difference or
> should I be noting a difference?  I have researched R mailing lists and geoR
> documentation but have been unable to find an answer.  Please let me know if
> additional specificity is needed.  This dilemma is universal across
> different sets of covariates and covariance models or parameters.
>
> Any assistance is greatly appreciated and I thank you for your time.
> Joshua Palmer
> Meteorologist
> Atlanta, GA, USA
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From Roger.Bivand at nhh.no  Mon Sep  3 20:59:04 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 3 Sep 2007 20:59:04 +0200 (CEST)
Subject: [R-sig-Geo] polygon buffers?
In-Reply-To: <46DAF635.60705@usu.edu>
References: <46DAF635.60705@usu.edu>
Message-ID: <Pine.LNX.4.64.0709032025080.24931@reclus.nhh.no>

On Sun, 2 Sep 2007, Peter Adler wrote:

> Dear R gurus,
>
> I am very excited about all the spatials tools in R because they have
> almost liberated me from ESRI products. One of the last things I have
> yet to figure out how to do in R is to create buffers around polygons
> (or just expand polygons). I searched the R-sig-geo archives (using
> Google) for "polygon buffer" and "transform polygon" but haven't found
> anything to get me started. Thanks in advance for your ideas,

Vector export and import in the spgrass6 package and v.buffer in GRASS 
works too. Using QGIS under Windows, it is pretty painless, starting Rgui 
in the GRASS shell; if the geometries are very complex, it takes some 
time. readVECT6() on the output geometries does need a fix (in the next 
release 0.4-2), because the imported data frame only has a single column 
and the data frame dimension gets dropped. The attached image is from a 
detailed bit of the North Carolina coast from the data sets supporting the 
forthcoming third edition of the GRASS book.

Roger

>
> Peter
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From mgallay01 at qub.ac.uk  Mon Sep  3 21:15:16 2007
From: mgallay01 at qub.ac.uk (Michal Gallay)
Date: 03 Sep 2007 19:15:16 +0000
Subject: [R-sig-Geo] Image colours based on certain values
In-Reply-To: <Pine.LNX.4.58.0709031336470.30656@dalmore.c3sl.ufpr.br>
References: <46DAF635.60705@usu.edu>
	<Pine.LNX.4.58.0709031336470.30656@dalmore.c3sl.ufpr.br>
Message-ID: <Prayer.1.0.12.0709031915160.20358@amos.qub.ac.uk>

Dear R Spatial Users,

I have many DEM ASCII grid files and have produced maps(images) via 
image(). The maps display e.g. slope angle, profile curvature for one area, 
but calculated via different methods.

I have searched R help, the R group, but I couldn't find out how to make R 
use certain colours for certain image values, in order to be able to 
compare the maps, for instance to have the same color for 0 curvature 
values, no matter what is the range of values.

If it helps, the following is applied in a 'for' loop for 20 files: 

raster.values <- read.asciigrid(fname=Files[i], as.image = FALSE,  
                          plot.image = FALSE, colname = Files[i])

min.val <- min(raster.values at data, na.rm=TRUE)
max.val <- max(raster.values at data, na.rm=TRUE)
 
image(raster.values, zlim=c(min.val,max.val), col=bpy.colors(255, 
cutoff.tails = 0.1), axes=TRUE)

I appreciate and thank you for help.

Michal

-- 
Michal Gallay

Postgraduate Research Student
School of Geography, Archaeology and Palaeoecology
Queen's University
Belfast BT7 1NN
Northern Ireland

Tel: +44(0)2890 273929
Fax: +44(0)2890 973212
email: mgallay01 at qub.ac.uk
www: www.qub.ac.uk/geog



From Roger.Bivand at nhh.no  Mon Sep  3 21:29:18 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 3 Sep 2007 21:29:18 +0200 (CEST)
Subject: [R-sig-Geo] Image colours based on certain values
In-Reply-To: <Prayer.1.0.12.0709031915160.20358@amos.qub.ac.uk>
References: <46DAF635.60705@usu.edu>
	<Pine.LNX.4.58.0709031336470.30656@dalmore.c3sl.ufpr.br>
	<Prayer.1.0.12.0709031915160.20358@amos.qub.ac.uk>
Message-ID: <Pine.LNX.4.64.0709032127340.24931@reclus.nhh.no>

On Mon, 3 Sep 2007, Michal Gallay wrote:

> Dear R Spatial Users,
>
> I have many DEM ASCII grid files and have produced maps(images) via
> image(). The maps display e.g. slope angle, profile curvature for one area,
> but calculated via different methods.
>
> I have searched R help, the R group, but I couldn't find out how to make R
> use certain colours for certain image values, in order to be able to
> compare the maps, for instance to have the same color for 0 curvature
> values, no matter what is the range of values.
>
> If it helps, the following is applied in a 'for' loop for 20 files:
>
> raster.values <- read.asciigrid(fname=Files[i], as.image = FALSE,
>                          plot.image = FALSE, colname = Files[i])
>
> min.val <- min(raster.values at data, na.rm=TRUE)
> max.val <- max(raster.values at data, na.rm=TRUE)
>
> image(raster.values, zlim=c(min.val,max.val), col=bpy.colors(255,
> cutoff.tails = 0.1), axes=TRUE)

Set breaks= explicity, it will be passed through to image.default(); 
perhaps do not set zlim=, and note that there should be one more break 
than the number of colours.

Roger

>
> I appreciate and thank you for help.
>
> Michal
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From rainer at krugs.de  Tue Sep  4 10:27:40 2007
From: rainer at krugs.de (rainer at krugs.de)
Date: Tue, 04 Sep 2007 10:27:40 +0200
Subject: [R-sig-Geo] polygon buffers?
Message-ID: <28740409.804541188894460178.JavaMail.servlet@kundenserver>

Just to add to this: QGIS and R under Linux works as well - But as there seem to be slight problems with the grass console under QGRIS, I have the following procedure (QGIS already connected to grass): I start xterm from the grass console in QGIS and in this console R (or emacs...). Works nicely. But it also works directly in the QGIS grass console

Rainer


>On Sun, 2 Sep 2007, Peter Adler wrote:
>
>> Dear R gurus,
>>
>> I am very excited about all the spatials tools in R because they have
>> almost liberated me from ESRI products. One of the last things I have
>> yet to figure out how to do in R is to create buffers around polygons
>> (or just expand polygons). I searched the R-sig-geo archives (using
>> Google) for "polygon buffer" and "transform polygon" but haven't found
>> anything to get me started. Thanks in advance for your ideas,
>
>Vector export and import in the spgrass6 package and v.buffer in GRASS 
>works too. Using QGIS under Windows, it is pretty painless, starting Rgui 
>in the GRASS shell; if the geometries are very complex, it takes some 
>time. readVECT6() on the output geometries does need a fix (in the next 
>release 0.4-2), because the imported data frame only has a single column 
>and the data frame dimension gets dropped. The attached image is from a 
>detailed bit of the North Carolina coast from the data sets supporting the 
>forthcoming third edition of the GRASS book.
>
>Roger
>
>>
>> Peter
>>
>>
>
>-- 
>Roger Bivand
>Economic Geography Section, Department of Economics, Norwegian School of
>Economics and Business Administration, Helleveien 30, N-5045 Bergen,
>Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
>e-mail: Roger.Bivand at nhh.no
>
>_______________________________________________
>R-sig-Geo mailing list
>R-sig-Geo at stat.math.ethz.ch
>https://stat.ethz.ch/mailman/listinfo/r-sig-geo



From jolma at cc.hut.fi  Tue Sep  4 11:33:23 2007
From: jolma at cc.hut.fi (Ari Jolma)
Date: Tue, 04 Sep 2007 12:33:23 +0300
Subject: [R-sig-Geo] polygon buffers?
In-Reply-To: <46DAF635.60705@usu.edu>
References: <46DAF635.60705@usu.edu>
Message-ID: <fe72f86d1bf19.1bf19fe72f86d@cc.hut.fi>



----- Original Message -----
From: Peter Adler <peter.adler at usu.edu>
Date: Sunday, September 2, 2007 8:45 pm
Subject: [R-sig-Geo] polygon buffers?
To: r-sig-geo at stat.math.ethz.ch

> Dear R gurus,
> 
> I am very excited about all the spatials tools in R because they 
> have 
> almost liberated me from ESRI products. One of the last things I 
> have 
> yet to figure out how to do in R is to create buffers around 
> polygons 
> (or just expand polygons). I searched the R-sig-geo archives (using 
> Google) for "polygon buffer" and "transform polygon" but haven't 
> found 
> anything to get me started. Thanks in advance for your ideas,


I'm a novice when it comes to R but since GDAL and OGR have been
interfaced to R, there should/might be possibility to access the GEOS
functions through OGR (PostGIS also uses GEOS, which is a C++ port of
the Java Topology Suite) if GEOS was linked to OGR when it was built.

This page http://www.gdal.org/ogr/classOGRGeometry.html lists the GEOS
methods, which include Buffer.

Cheers,

Ari

>



> 
> Peter
> 
> -- 
> Peter Adler
> Department of Wildland Resources
> 5230 Old Main Hill
> Utah State University
> Logan, UT 84322
> tel: (435) 797-1021 / email: peter.adler at usu.edu
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From caroline.keef at jbaconsulting.co.uk  Tue Sep  4 13:44:17 2007
From: caroline.keef at jbaconsulting.co.uk (Caroline Keef)
Date: Tue, 4 Sep 2007 12:44:17 +0100
Subject: [R-sig-Geo] Large Data Sets
Message-ID: <660594358408634983549249D0B0057D5259B2@mail-ski1.jbanorthwest.co.uk>

I am having trouble manipulating a large data set to change a spatial
polygons data frame

I have read in an ESRI polygon shapefile using rgdal and readOGR that
contains 18804 polygons.  I am trying to add two variables into the
spatial data frame but it's not working.  I'm doing this using

cbind(SpatialPolygonsDataFrame,new variable,rank(new variable)).

Before I killed R Task Manager said it was using more than 1,050,000K
virtual memory, which given my whole computer ground to a halt I'm
guessing is the limit!

I regularly handle larger data frames than this using R without any
problem so I don't think the problem is R in general.  

Does anyone have any suggestions as to either the size of the data set
that it is possible to handle as a spatial data frame or a better way of
combining spatial data frames?

Thank you

Caroline
 
JBA Consulting - Engineers and Scientists
South Barn, Broughton Hall, Skipton, North Yorkshire, BD23 3AE, UK
t: +44 (0)1756 799919   f: +44 (0)1756 799449
 
JBA is a Carbon Neutral Company.  Please don't print this e-mail unless you really need to.
This email is covered by JBA Consulting's email disclaimer at www.jbaconsulting.co.uk/emaildisclaimer



From mdsumner at utas.edu.au  Tue Sep  4 14:21:11 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Tue, 04 Sep 2007 22:21:11 +1000
Subject: [R-sig-Geo] Large Data Sets
In-Reply-To: <660594358408634983549249D0B0057D5259B2@mail-ski1.jbanorthwest.co.uk>
References: <660594358408634983549249D0B0057D5259B2@mail-ski1.jbanorthwest.co.uk>
Message-ID: <46DD4DB7.7040904@utas.edu.au>

Caroline Keef wrote:
> I am having trouble manipulating a large data set to change a spatial
> polygons data frame
>
> I have read in an ESRI polygon shapefile using rgdal and readOGR that
> contains 18804 polygons.  I am trying to add two variables into the
> spatial data frame but it's not working.  I'm doing this using
>
> cbind(SpatialPolygonsDataFrame,new variable,rank(new variable)).
>
>   
Try

spdf$newvariable <- newvariable
spdf$ranknewvariable <- rank(newvariable)

or similar, depending on your actual variable and column names, and the 
workflow used to generate your vectors.

This would be identical, exploiting the features of data frames via SPDF 
methods (again, depending on your actual names):

spdf[["newvariable"]] <- newvariable
spdf[["ranknewvariable"]] <- rank(newvariable)


Using cbind like that looks like it will coerce to matrix, but it's not 
going to do what you want. You could  access the underlying spdf at data 
more directly,  but using the method syntax for data frames is advisable.

Cheers, Mike.
> Before I killed R Task Manager said it was using more than 1,050,000K
> virtual memory, which given my whole computer ground to a halt I'm
> guessing is the limit!
>
> I regularly handle larger data frames than this using R without any
> problem so I don't think the problem is R in general.  
>
> Does anyone have any suggestions as to either the size of the data set
> that it is possible to handle as a spatial data frame or a better way of
> combining spatial data frames?
>
> Thank you
>
> Caroline
>  
> JBA Consulting - Engineers and Scientists
> South Barn, Broughton Hall, Skipton, North Yorkshire, BD23 3AE, UK
> t: +44 (0)1756 799919   f: +44 (0)1756 799449
>  
> JBA is a Carbon Neutral Company.  Please don't print this e-mail unless you really need to.
> This email is covered by JBA Consulting's email disclaimer at www.jbaconsulting.co.uk/emaildisclaimer
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>



From Roger.Bivand at nhh.no  Tue Sep  4 14:27:09 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 4 Sep 2007 14:27:09 +0200 (CEST)
Subject: [R-sig-Geo] Large Data Sets
In-Reply-To: <660594358408634983549249D0B0057D5259B2@mail-ski1.jbanorthwest.co.uk>
References: <660594358408634983549249D0B0057D5259B2@mail-ski1.jbanorthwest.co.uk>
Message-ID: <Pine.LNX.4.64.0709041407360.29743@reclus.nhh.no>

On Tue, 4 Sep 2007, Caroline Keef wrote:

> I am having trouble manipulating a large data set to change a spatial
> polygons data frame
>
> I have read in an ESRI polygon shapefile using rgdal and readOGR that
> contains 18804 polygons.  I am trying to add two variables into the
> spatial data frame but it's not working.  I'm doing this using
>
> cbind(SpatialPolygonsDataFrame,new variable,rank(new variable)).

If we call your SpatialPolygonsDataFrame "x", then why not do:

x$nv <- new_variable
x$rnv <- rank(new_variable)

cbind is an S3 generic function, but no methods are defined for the 
SpatialPolygonsDataFrame class.

You can use the spCbind() method in the maptools package, but first you 
need to put your new variables into a data frame, and make sure that the 
set of row names of the data frame in the data slot of the 
SpatialPolygonsDataFrame object and the new data frame agree. You can use 
spCbind on a single vector:

x1 <- spCbind(x, new_variable)

but

x$nv <- new_variable

seems easier.

cbind methods do exist for SpatialGridDataFrame objects, but are for 
cbind'ing two SpatialGridDataFrame objects together, rather than for 
arguments of arbitrary classes; it stops you if the GridTopologies are 
not identical.

Hope this helps,

Roger

>
> Before I killed R Task Manager said it was using more than 1,050,000K
> virtual memory, which given my whole computer ground to a halt I'm
> guessing is the limit!

The default cbind method had got completely lost in the input object.

>
> I regularly handle larger data frames than this using R without any
> problem so I don't think the problem is R in general.
>
> Does anyone have any suggestions as to either the size of the data set
> that it is possible to handle as a spatial data frame or a better way of
> combining spatial data frames?
>
> Thank you
>
> Caroline
>
> JBA Consulting - Engineers and Scientists
> South Barn, Broughton Hall, Skipton, North Yorkshire, BD23 3AE, UK
> t: +44 (0)1756 799919   f: +44 (0)1756 799449
>
> JBA is a Carbon Neutral Company.  Please don't print this e-mail unless you really need to.
> This email is covered by JBA Consulting's email disclaimer at www.jbaconsulting.co.uk/emaildisclaimer
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From adrian at maths.uwa.edu.au  Wed Sep  5 05:08:37 2007
From: adrian at maths.uwa.edu.au (adrian at maths.uwa.edu.au)
Date: Wed, 5 Sep 2007 11:08:37 +0800 (WST)
Subject: [R-sig-Geo] Buffers around polygons
Message-ID: <61152.121.221.161.23.1188961717.squirrel@121.221.161.23>

Peter Adler asked about constructing a `buffer' around a polygon.

In the spatstat package the function 'dilate.owin' will compute the
morphological dilation (the region of space consisting of all locations
that are less than or equal to d units distant from the original polygon).
The result is not a polygon; it is represented in spatstat as a binary
image.

spatstat will also rescale and translate polygons, etc

spatstat does not have code to construct a *polygonal* buffer (a polygon
with a margin of *at least* distance d from the original polygon) and I
suspect that it's not well defined if the original polygon is not convex.

Adrian Baddeley



From Nachman.Keeve at epamail.epa.gov  Wed Sep  5 16:33:50 2007
From: Nachman.Keeve at epamail.epa.gov (Nachman.Keeve at epamail.epa.gov)
Date: Wed, 5 Sep 2007 10:33:50 -0400
Subject: [R-sig-Geo] deleting duplicate observations in spatial dataset?
Message-ID: <OF9269166F.618D2C40-ON8525734D.004E83CF-8525734D.0050009E@epamail.epa.gov>


Hello-

I apologize in advance for what is likely a newbie question, but here
goes:

In the geoR package, after using dup.coords to identify co-located data
points, is there an easy way to delete duplicate observations?  If it
makes things any easier, my duplicate observations/co-located points are
adjacent in the dataset.

For example, the command

dup.coords(spadata)

    dup   longfix   lat   data
977   1 -87.79917 42.14 0.0443
978   2 -87.79917 42.14 0.0352

And to be a bit more specific, rather than deleting duplicate
observations outright, I would prefer to establish some sort of criteria
for removal of duplicates - in the above case, I would like to keep the
observation with the larger data value.  Of course, if this isn't
possible, I'd be happy with any method to delete duplicates, and I can
deal with the selective deletion through a prior sort.

Any help would be much appreciated!

Keeve

Keeve Nachman, PhD, MHS
Public Health and Environmental Policy Team
National Center for Environmental Economics
US Environmental Protection Agency
1200 Pennsylvania Ave NW (1809T)
Washington, DC  20460
(O) 202-566-2279
(F) 202-566-2336



From viton.1 at osu.edu  Wed Sep  5 18:42:11 2007
From: viton.1 at osu.edu (Philip A. Viton)
Date: Wed, 05 Sep 2007 13:42:11 -0300
Subject: [R-sig-Geo] nb file using spdep tools only
Message-ID: <200709051743.l85HhHYw028654@defang10.it.ohio-state.edu>


(Very new-user problem, sorry) Can anyone help me with generating an 
nb file using only R/spdep tools, ie without using GeoDa as an 
intermediate step? I have a shape file, say "myshape.shp" with its 
associated (eg) dbf files. My idea is to read it with maptools, then 
convert to polygons, neighbors, then finally weights.

In R (2.5.1 under Winxp):

library(spdep)

map <-readshape("myshape")
# seems OK

mappoly <-Map2poly(map,region.id="OBJECTID")
# also seems OK, though it takes a LONG time

mapnb = poly2nb(mappoly,row.names = NULL, 
snap=sqrt(.Machine$double.eps), queen=TRUE)

# problem is, this hangs R (GUI says "Not responding")

(then, if I ever get this far, use nb2listw to generate the weights). 
The shape file is a US-48-states file, so not, I would assume, too 
big; but obviously something is going wrong. Can anyone suggest a better way?


Thanks!

------------------------
Philip A. Viton
City Planning, Ohio State University
275 West Woodruff Avenue, Columbus OH 43210
viton.1 at osu.edu



From rss10 at duke.edu  Wed Sep  5 20:00:57 2007
From: rss10 at duke.edu (Rob Schick)
Date: Wed, 5 Sep 2007 14:00:57 -0400
Subject: [R-sig-Geo] Sampling a grid from a single polygon
Message-ID: <4ae03def0709051100y205fba9bi103dcd95b4a79258@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070905/7385a258/attachment.pl>

From dooher1 at llnl.gov  Wed Sep  5 20:17:13 2007
From: dooher1 at llnl.gov (dooher1 at llnl.gov)
Date: Wed, 05 Sep 2007 11:17:13 -0700
Subject: [R-sig-Geo] Optimized search routine
In-Reply-To: <4ae03def0709051100y205fba9bi103dcd95b4a79258@mail.gmail.co
 m>
References: <4ae03def0709051100y205fba9bi103dcd95b4a79258@mail.gmail.com>
Message-ID: <7.0.0.16.2.20070905111135.028456a8@llnl.gov>

I am looking at different population density routines, running 
cumulative populations within a certain radial distance (as well as 
other type functions).  Is there a way to optimize the search, ie, 
setting up something like ESRI's SDE (which is basically a database 
that relates nearby grids to one another to optimize search routines 
and other queries?
Brendan



From Roger.Bivand at nhh.no  Wed Sep  5 21:45:29 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 5 Sep 2007 21:45:29 +0200 (CEST)
Subject: [R-sig-Geo] Sampling a grid from a single polygon
In-Reply-To: <4ae03def0709051100y205fba9bi103dcd95b4a79258@mail.gmail.com>
References: <4ae03def0709051100y205fba9bi103dcd95b4a79258@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0709052133110.9950@reclus.nhh.no>

On Wed, 5 Sep 2007, Rob Schick wrote:

> Hi - I'm trying to write a simulation that loops over an animal movement
> track, creates a buffer at each x,y,t, and samples an underlying grid (sort
> of like Zonal Stats in ArcInfo).
>
> To create the polygon buffer centered on the current position and with
> radius r, I've written:
>
> poly <- function(x,y,r){
>    phi <- seq(0,2*pi,length.out=30)
>    xy <- matrix(NA, nrow=length(phi),ncol=2)
>        for(i in 1:length(phi)){
>            xy[i,1] <- x + (r * cos(phi[i]))
>            xy[i,2] <- y + (r * sin(phi[i]))
>        }
>    xy[i,1:2] <- xy[1,1:2]
>    xysp <- Polygon(xy,hole=F)
>    return(xysp)
> }
>
> Then calling it:
>
> test <- poly(xy[t,1], xy[t,2], 2)
>
> But when I try overlay(df,test), I get
>
>> overlay(df,test, fn=mean)
> Error in function (classes, fdef, mtable)  :
>        unable to find an inherited method for function "overlay", for
> signature "SpatialGridDataFrame", "Polygon"
>
>> class(df)
> [1] "SpatialGridDataFrame"
> attr(,"package")
> [1] "sp"
>
>> class(test)
> [1] "Polygon"
> attr(,"package")
> [1] "sp"
>
> I thought the error might be in the way I am calling df within overlay, but
> it's not clear from overlay-methods if this even possible. To test this, I
> tried
>
> overlay(as(df,"SpatialPoints"),test)
> Error in function (classes, fdef, mtable)  :
>        unable to find an inherited method for function "overlay", for
> signature "SpatialPoints", "Polygon"
>
> I also thought it might be a problem with using Polygon, instead of
> SpatialPolygons, but seems like I can't use the latter with a single
> polygon. Am I missing something easy?

Possibly:

SP <- SpatialPolygons(list(Polygons(list(test), ID="poly")))

that is, it needs to be a SpatialPolygons object. Since each Polygons 
object can contain one or more Polygon objects, and each SpatialPolygons
object can contain one or more Polygons objects, they need wrapping up in 
lists.

The overlay() methods should now work. If you want to use a canned circle, 
try disc() in the spatstat package - look for the bdry component 
containing a list of list of coordinates, which need closing.

Hope this helps,

Roger

>
> Thanks,
> Rob
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Wed Sep  5 22:00:11 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 5 Sep 2007 22:00:11 +0200 (CEST)
Subject: [R-sig-Geo] nb file using spdep tools only
In-Reply-To: <200709051743.l85HhHYw028654@defang10.it.ohio-state.edu>
References: <200709051743.l85HhHYw028654@defang10.it.ohio-state.edu>
Message-ID: <Pine.LNX.4.64.0709052149010.9950@reclus.nhh.no>

On Wed, 5 Sep 2007, Philip A. Viton wrote:

>
> (Very new-user problem, sorry) Can anyone help me with generating an
> nb file using only R/spdep tools, ie without using GeoDa as an
> intermediate step? I have a shape file, say "myshape.shp" with its
> associated (eg) dbf files. My idea is to read it with maptools, then
> convert to polygons, neighbors, then finally weights.
>
> In R (2.5.1 under Winxp):
>
> library(spdep)
>
> map <-readshape("myshape")
> # seems OK
>
> mappoly <-Map2poly(map,region.id="OBJECTID")
> # also seems OK, though it takes a LONG time
>
> mapnb = poly2nb(mappoly,row.names = NULL,
> snap=sqrt(.Machine$double.eps), queen=TRUE)
>
> # problem is, this hangs R (GUI says "Not responding")

This suggests that the coastline is very detailed. R is just working on 
all the coastal points, it will get there in the end.

Nowadays you can use either:

getinfo.shape("myshape.shp") # to check type of shapes
mapSP <- readShapePoly("myshape.shp")
mapnb <- poly2nb(mapSP) # other arguments are default values

or

library(rgdal)
mapSP <- readOGR(dsn=".", layer="myshape") # sets the shape type itself
                                            # and reads *.prj

but most likely myshape.shp is large and detailed, it isn't so much the 
number of polygons that impacts performance, rather the numbers of 
coordinates in the polygons being matched. If the coastlines are very 
detailed, all the islands will be being checked too, so just leave it to 
complete without watching (runs faster if nobody is looking!).

Hope this helps,

Roger

>
> (then, if I ever get this far, use nb2listw to generate the weights).
> The shape file is a US-48-states file, so not, I would assume, too
> big; but obviously something is going wrong. Can anyone suggest a better way?
>
>
> Thanks!
>
> ------------------------
> Philip A. Viton
> City Planning, Ohio State University
> 275 West Woodruff Avenue, Columbus OH 43210
> viton.1 at osu.edu
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From viton.1 at osu.edu  Thu Sep  6 15:33:44 2007
From: viton.1 at osu.edu (Philip A. Viton)
Date: Thu, 06 Sep 2007 10:33:44 -0300
Subject: [R-sig-Geo] nb file using spdep tools only
Message-ID: <200709061434.l86EYnGp020440@defang9.it.ohio-state.edu>


One small follow-up question, if I may: I converted the wts to a 
matrix using listw2mat and then wrote it out as a csv file. The csv 
has column headers "V1", "V2" etc. Is there some way I could have 
matched these up to the "OBJECTID" in the original shape file?  That 
is, I'm not sure to just what, in my context where the shapes are 
shapes of US states, the V's refer.

Thanks!

------------------------
Philip A. Viton
City Planning, Ohio State University
275 West Woodruff Avenue, Columbus OH 43210
viton.1 at osu.edu



From viton.1 at osu.edu  Thu Sep  6 15:25:39 2007
From: viton.1 at osu.edu (Philip A. Viton)
Date: Thu, 06 Sep 2007 10:25:39 -0300
Subject: [R-sig-Geo] nb file using spdep tools only
Message-ID: <200709061426.l86EQjv0007425@defang10.it.ohio-state.edu>


Roger: many thanks! I let it run overnight, and it seems to have 
worked! Thanks again!!

Regards,

------------------------
Philip A. Viton
City Planning, Ohio State University
275 West Woodruff Avenue, Columbus OH 43210
viton.1 at osu.edu



From peter.adler at usu.edu  Thu Sep  6 16:46:01 2007
From: peter.adler at usu.edu (Peter Adler)
Date: Thu, 06 Sep 2007 08:46:01 -0600
Subject: [R-sig-Geo] summary of polygon buffer thread
Message-ID: <46E012A9.9040502@usu.edu>

Here is a summary of the responses to my question about buffering 
polygons. Thanks to everyone for quick and creative replies.

------------------------------

I've been doing this in postgis. Once you get used to it, its not too
difficult. Load data with shp2pgsql (or use rgdal + RODBC or similar
if you want to stay in R) and then create a new table to hold the
results. After that you can do "insert into restab select
buffer(the_geom, dist) from sourcetab" (that's from memory, so check
the postgis docs).
-Tim Keitt
------------------------------
There may be other alternatives, but
this is one of the functionalities in an external software (terralib) that
the aRT package tries to make usage.

The aRT web page has one example:
www.leg.ufpr.br/aRT


Paulo Justiniano Ribeiro Jr
------------------------------
Vector export and import in the spgrass6 package and v.buffer in GRASS
works too. Using QGIS under Windows, it is pretty painless, starting Rgui
in the GRASS shell; if the geometries are very complex, it takes some
time. readVECT6() on the output geometries does need a fix (in the next
release 0.4-2), because the imported data frame only has a single column
and the data frame dimension gets dropped. The attached image is from a
detailed bit of the North Carolina coast from the data sets supporting the
forthcoming third edition of the GRASS book.

Roger Bivand
------------------------------

I'm a novice when it comes to R but since GDAL and OGR have been
interfaced to R, there should/might be possibility to access the GEOS
functions through OGR (PostGIS also uses GEOS, which is a C++ port of
the Java Topology Suite) if GEOS was linked to OGR when it was built.

This page http://www.gdal.org/ogr/classOGRGeometry.html lists the GEOS
methods, which include Buffer.

Cheers,

Ari Jolma

[I found the following link which gives an example of a related approach:
http://wiki.intamap.org/index.php/PostGIS]
------------------------------

In the spatstat package the function 'dilate.owin' will compute the
morphological dilation (the region of space consisting of all locations
that are less than or equal to d units distant from the original polygon).
The result is not a polygon; it is represented in spatstat as a binary
image.

spatstat will also rescale and translate polygons, etc

spatstat does not have code to construct a *polygonal* buffer (a polygon
with a margin of *at least* distance d from the original polygon) and I
suspect that it's not well defined if the original polygon is not convex.

Adrian Baddeley


-- 
Peter Adler
Dept. of Wildland Resources and the Ecology Center
5230 Old Main
Utah State University
Logan, UT 84322
tel: (435) 797-1021  email: peter.adler at usu.edu
http://www.cnr.usu.edu/faculty/adler/



From JBaker at MSA.com  Fri Sep  7 16:34:43 2007
From: JBaker at MSA.com (Baker, John)
Date: Fri, 7 Sep 2007 10:34:43 -0400 
Subject: [R-sig-Geo] Chongqing Province is missing in latest China mapdata.
Message-ID: <9F646553D72D154BB319DCFA6107FEBD1E5A5CEB@YUZU.msais.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070907/39104cb6/attachment.pl>

From yud at mail.montclair.edu  Fri Sep  7 17:56:12 2007
From: yud at mail.montclair.edu (Danlin Yu)
Date: Fri, 07 Sep 2007 11:56:12 -0400
Subject: [R-sig-Geo] Chongqing Province is missing in latest China
	mapdata.
In-Reply-To: <9F646553D72D154BB319DCFA6107FEBD1E5A5CEB@YUZU.msais.com>
References: <9F646553D72D154BB319DCFA6107FEBD1E5A5CEB@YUZU.msais.com>
Message-ID: <46E1749C.6080109@mail.montclair.edu>

John:

Chongqing is a provincial  municipality, not a province. That said, I 
would actually suggest you to find a county map of China (which should 
be easily available), and manually identify the counties that now belong 
to Chongqing, and dissolve them into the municipality. The rest would be 
easy.

Hope this helps,

Best,
Danlin Yu

Baker, John wrote:
> I was able to create a Chinese map using the appropriate libraries (i.e.
> maps and mapdata).  Unfortunately, the Chinese map does not show the
> Chongqing province which was added in 1998.  Does anyone have an idea when
> the china map might be updated - or - any advice on how to update the
> package manually.  In the package, it seems that there are a lot of files
> for china and I'm not sure which ones to update. 
>
>  
>
> Thanks in advance,
>
>  
>
> John Baker
>
>  
>
>  
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>   

-- 
___________________________________________
Danlin Yu, Ph.D.
Assistant Professor
Department of Earth & Environmental Studies
Montclair State University
Montclair, NJ, 07043
Tel: 973-655-4313
Fax: 973-655-4072
email: yud at mail.montclair.edu
webpage: csam.montclair.edu/~yu



From joshua_palmer at ncsu.edu  Sun Sep  9 00:38:25 2007
From: joshua_palmer at ncsu.edu (Joshua Palmer)
Date: Sat, 8 Sep 2007 18:38:25 -0400
Subject: [R-sig-Geo] geoR Beta Estimates Not Equal to lm Coefficients:
	Why?
In-Reply-To: <Pine.LNX.4.58.0709031342450.30656@dalmore.c3sl.ufpr.br>
References: <002e01c7edf0$2f3317c0$640fa8c0@noreaster2005>
	<Pine.LNX.4.58.0709031342450.30656@dalmore.c3sl.ufpr.br>
Message-ID: <002a01c7f268$f81b2e80$640fa8c0@noreaster2005>


Thank you very much Paulo and Edzer; between your responses and a little
brushing up on my geostatistics it now makes perfect sense that geoR uses a
GLS approach to removing the trend in likfit or krige.conv.  Otherwise, by
relying on OLS, I am not accounting for the spatial correlations in my
dataset (as you have suggested Paulo).  As Edzer has explained, if I were to
rely on OLS I am assuming there is spatial independence between my
measurements; therefore, my semivariogram model would be a pure nugget model
with zero range and zero partial sill.  My data clearly shows this is not
the case, which is why I cannot match my coefficients obtained from OLS with
those obtained by GLS.

Best Regards,
Joshua Palmer
Meteorologist
Atlanta, GA, USA

-----Original Message-----
From: Paulo Justiniano Ribeiro Jr [mailto:paulojus at c3sl.ufpr.br] 
Sent: Monday, September 03, 2007 12:45 PM
To: Joshua Palmer
Cc: r-sig-geo at stat.math.ethz.ch
Subject: Re: [R-sig-Geo] geoR Beta Estimates Not Equal to lm Coefficients:
Why?

Josha

If I've got oyur question correctly the diference is because geoR is
estimating both, regression coeficientes and covariance parameters and
therefore the estimates ar not the same as lm() which uses ordinary least
squares.
In other words the regression parameters on geoR are estimated considering
the spatial correlation between the sample points as described by the
assumed model

Hope this helps
P.J.


Paulo Justiniano Ribeiro Jr
LEG (Laborat?rio de Estat?stica e Geoinforma??o) Universidade Federal do
Paran? Caixa Postal 19.081 CEP 81.531-990 Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
Fax: (+55) 41 3361 3141
e-mail: paulojus AT  ufpr  br
http://www.leg.ufpr.br/~paulojus

-----Original Message-----
From: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
Sent: Monday, September 03, 2007 2:47 AM
To: Joshua Palmer
Cc: r-sig-geo at stat.math.ethz.ch
Subject: Re: [R-sig-Geo] geoR Beta Estimates Not Equal to lm Coefficients:
Why?

Joshua,

My guess is that to estimate beta geoR uses generalized least squares and lm
and SAS ordinary least squares. If you use a pure nugget model, or some
other model with a range parameter sufficiently close to zero, i.e. model
the observations as independent, the estimates should be the same.
--
Edzer

On Mon, 3 Sep 2007, Joshua Palmer wrote:

> Hello everyone,
>
> I am using the excellent geoR package to perform Kriging with External 
> Drift.  As to be expected, for any given set of covariates, trend 
> coefficients I obtain performing least-squares regression in SAS equal 
> those coefficients obtained by using R's lm function.  However, those 
> trend coefficients do NOT match the estimates for the beta values when 
> I run krige.conv (or likfit or variofit) in geoR.  The coefficients 
> are often similar, but they are never nearly or exactly the same.  I 
> was under the assumption that geoR utilizes least-squares regression 
> (on the trend.d
> matrix) a la R's lm function to derive beta estimates.  This appears 
> to be incorrect.
>
> Would anyone be able to explain to me why I am noting this difference 
> or should I be noting a difference?  I have researched R mailing lists 
> and geoR documentation but have been unable to find an answer.  Please 
> let me know if additional specificity is needed.  This dilemma is 
> universal across different sets of covariates and covariance models or
parameters.
>
> Any assistance is greatly appreciated and I thank you for your time.
> Joshua Palmer
> Meteorologist
> Atlanta, GA, USA
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From eaeff at mtsu.edu  Mon Sep 10 19:39:14 2007
From: eaeff at mtsu.edu (E. Anthon Eff)
Date: Mon, 10 Sep 2007 12:39:14 -0500
Subject: [R-sig-Geo] biparametric model
Message-ID: <46E58142.80505@mtsu.edu>

Please forgive what may be a naive question. I'm working on a model that 
requires two different weight matrices, as in the "biparametric" model 
introduced by Brandsma and Ketellapper in 1979. I haven't had any luck 
finding a way to do this in R. Any suggestions?
Thanks!
Anthon

Brandsma A S, Ketellapper R H, 1979, "A biparametric approach to spatial 
autocorrelation" /Environment and Planning A/ *11*(1) 51 ? 58

-- 
E. Anthon Eff, Associate Professor
Dept. of Economics, Middle Tennessee State University
Box X050, Murfreesboro TN 37132
Phone: 615.898.2387
http://www.mtsu.edu/~eaeff/

Natura non facit saltum.



From duncan at duncanelkins.com  Mon Sep 10 21:22:56 2007
From: duncan at duncanelkins.com (Duncan Elkins)
Date: Mon, 10 Sep 2007 15:22:56 -0400
Subject: [R-sig-Geo] centroids in 3-space?
Message-ID: <2cd6ab3d0709101222v57ca06abvab8ad66d90dd6a74@mail.gmail.com>

Hello, list-
  Forgive me if this has been covered before- I've just joined and
can't seem to find a way to browse the list archives by keyword rather
than date.
     I'm trying to analyze a bunch of location data from lab
experiments that's roughly in the form of
[ObservationID,x,y,z, subjectID, treatment]

What I'd like to do is plot the cloud of observations for each fish in
3-space, subset that by treatment (or day of the trial), determine
some summary stats on the enclosing polygons and the centroids for
each, then calculate the distance matrix from each centroid to the
others.  This would be to test the hypothesis that the average
subject-subject distance is less under some treatments than others.
     Some of this looks pretty straightforward to do in 2-space,
especially if I use something like LoCoH to get the polygons, but I'm
wondering if there's something canned to do it and use the z-values,
too.  Can someone point me in the direction of a good resource for
this sort of analysis, either in R, ArcGIS, or both?

Thanks,
Duncan Elkins



From didier.leibovici at nottingham.ac.uk  Tue Sep 11 12:29:35 2007
From: didier.leibovici at nottingham.ac.uk (Didier Leibovici)
Date: Tue, 11 Sep 2007 11:29:35 +0100
Subject: [R-sig-Geo] memory Usage setting
Message-ID: <46E66E0F.3000704@nottingham.ac.uk>

These days in GIS on may have to manipulate big datasets or arrays.

What is the best way of tuning the R memory ?
or how can we not worry about the memory usage
i.e. could it that if it is needed it will swap on the disk (the system 
I suppose)
That means declaring even more RAM than existing ...

Here I am on WINDOWS I have a 4Gb
my aim was to have an array of dim 298249 12 10 22 but that's 2.9Gb

Here is some tries to find the size I could have :

First of all I did:
 > memory.limit(size=3500)
NULL
 > memory.limit(size=NA)/2^20
[1] 3500

 > memory.size(max=TRUE)/2^20
[1] 341.125


 >ZZ=array(rep(0,round(298249/16)*12*10*22),c(round(298249/16),12,10,22))
 > gc()
            used  (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells   835490  22.4    1368491   36.6    954139   25.5
Vcells 90663806 691.8  154550742 1179.2 139876061 1067.2
 >
 >  298249 *12*10*22
[1] 787377360
 >  dd=as.vector(rep(0,298249 *12*10*22))
Error: cannot allocate vector of size 2.9 Gb
In addition: Warning messages:
1: Reached total allocation of 3500Mb: see help(memory.size)

 >  dd=as.vector(rep(0,round(298249/4) *12*10*22))
Error: cannot allocate vector of size 750.9 Mb
 >  dd=as.vector(rep(0,round(298249/2) *12*10*22))
Error: cannot allocate vector of size 1.5 Gb
 >  dd=as.vector(rep(0,round(298249/9) *12*10*22))
Error: cannot allocate vector of size 333.7 Mb
 >  dd=as.vector(rep(0,round(298249/10) *12*10*22))
Error: cannot allocate vector of size 300.4 Mb
 > object.size(ZZ)
[1] 393698040
 > object.size(ZZ)/2^20
[1] 375.4597
 > gc()
            used  (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells   835535  22.4    1368491   36.6    954139   25.5
Vcells 90663830 691.8  162358279 1238.7 139876061 1067.2
 > rm(ZZ)
 > gc()
            used  (Mb) gc trigger  (Mb)  max used   (Mb)
Ncells   835622  22.4    1368491  36.6    954139   25.5
Vcells 41451603 316.3  129886623 991.0 139876061 1067.2

 >  dd=as.vector(rep(0,round(298249/10) *12*10*22))
Error: cannot allocate vector of size 600.7 Mb
 >
It is also strange that once a dd needed 300.4Mb and then 600.7Mb (?) as 
also I made some room in removing ZZ?

I had already modiied the starting of windows by putting

[operating systems]
multi(0)disk(0)rdisk(0)partition(1)\WINDOWS="Windows Server 2003, 
Standard" /noexecute=optout /fastdetect /redirect /5GB
in the boot.ini
which I don't really know if it took into account as the limit is 
greater than the physical RAM of 4GB. ...?

would it be easier using Linux ?

Thanks

-- 
Dr Didier Leibovici
	http://www.nottingham.ac.uk/cgs/leibovici.shtml
Centre for Geospatial Science
Sir Clive Granger Building
University of Nottingham,
University Park
Nottingham NG7 2RD, UK
	Tel: +44 - (0)115 84 66058   Fax: +44 (0)115 95 15249

This message has been checked for viruses but the contents of an attachment
may still contain software viruses, which could damage your computer system:
you are advised to perform your own checks. Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From elw at stderr.org  Tue Sep 11 14:20:39 2007
From: elw at stderr.org (elw at stderr.org)
Date: Tue, 11 Sep 2007 07:20:39 -0500 (CDT)
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <46E66E0F.3000704@nottingham.ac.uk>
References: <46E66E0F.3000704@nottingham.ac.uk>
Message-ID: <Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>


> These days in GIS on may have to manipulate big datasets or arrays.
>
> Here I am on WINDOWS I have a 4Gb
> my aim was to have an array of dim 298249 12 10 22 but that's 2.9Gb


It used to be (maybe still is?) the case that a single process could only 
'claim' a chunk of max size 2GB on Windows.


Also remember to compute overhead for R objects... 58 bytes per object, I 
think it is.


> It is also strange that once a dd needed 300.4Mb and then 600.7Mb (?) as 
> also I made some room in removing ZZ?


Approximately double size - many things the interpreter does involve 
making an additional copy of the data and then working with *that*.  This 
might be happening here, though I didn't read your code carefully enough 
to be able to be certain.


> which I don't really know if it took into account as the limit is 
> greater than the physical RAM of 4GB. ...?

:)

> would it be easier using Linux ?

possibly a little bit - on a linux machine you can at least run a PAE 
kernel (giving you a lot more address space to work with) and have the 
ability to turn on a bit more virtual memory.

usually with data of the size you're trying to work with, i try to find a 
way to preprocess the data a bit more before i apply R's tools to it. 
sometimes we stick it into a database (postgres) and select out the bits 
we want our inferences to be sourced from.  ;)

it might be simplest to just hunt up a machine with 8 or 16GB of memory in 
it, and run those bits of the analysis that really need memory on that 
machine...

--e



From keith at statkingconsulting.com  Tue Sep 11 16:26:53 2007
From: keith at statkingconsulting.com (Keith Dunnigan)
Date: Tue, 11 Sep 2007 10:26:53 -0400
Subject: [R-sig-Geo] Positive Definite Covariance Matrix for Grid Sampled
	Data
Message-ID: <221470FD0F9FD24ABE3A2F5569FF54250B38F9@statserversbs.Statkingconsulting.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070911/377abee5/attachment.pl>

From Roger.Bivand at nhh.no  Tue Sep 11 16:32:01 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 11 Sep 2007 16:32:01 +0200 (CEST)
Subject: [R-sig-Geo] biparametric model
In-Reply-To: <46E58142.80505@mtsu.edu>
References: <46E58142.80505@mtsu.edu>
Message-ID: <Pine.LNX.4.64.0709111601330.18811@reclus.nhh.no>

On Mon, 10 Sep 2007, E. Anthon Eff wrote:

> Please forgive what may be a naive question. I'm working on a model that
> requires two different weight matrices, as in the "biparametric" model
> introduced by Brandsma and Ketellapper in 1979. I haven't had any luck
> finding a way to do this in R. Any suggestions?

The spatial error model fitting functions spautolm() and errorsarlm(), and 
the spatial lag model fitting function lagsarlm() only fit a single set of 
spatial weights, by optimising in one dimension.

It would be possible to generalise them to optimise in more than one 
dimension, but is it justified? Is there a very clear behavioural model 
that requires the fitting of more than one spatial regression coefficient 
that is driving thw question? Is it going to be practical to fit more than 
one coefficient on a probably rather flat surface?

The slm() function in S-Plus SpatialStats module can do this for the error 
model if need be, but there is a case to be made for why it is necessary, 
unless there is a clear behavioural model.

Roger

> Thanks!
> Anthon
>
> Brandsma A S, Ketellapper R H, 1979, "A biparametric approach to spatial
> autocorrelation" /Environment and Planning A/ *11*(1) 51 ? 58
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Tue Sep 11 16:50:03 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 11 Sep 2007 16:50:03 +0200 (CEST)
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>
References: <46E66E0F.3000704@nottingham.ac.uk>
	<Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>
Message-ID: <Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no>

On Tue, 11 Sep 2007, elw at stderr.org wrote:

>
>> These days in GIS on may have to manipulate big datasets or arrays.
>>
>> Here I am on WINDOWS I have a 4Gb
>> my aim was to have an array of dim 298249 12 10 22 but that's 2.9Gb
>

Assuming double precision (no single precision in R), 5.8Gb.

>
> It used to be (maybe still is?) the case that a single process could only
> 'claim' a chunk of max size 2GB on Windows.
>
>
> Also remember to compute overhead for R objects... 58 bytes per object, I
> think it is.
>
>
>> It is also strange that once a dd needed 300.4Mb and then 600.7Mb (?) as
>> also I made some room in removing ZZ?
>
>
> Approximately double size - many things the interpreter does involve
> making an additional copy of the data and then working with *that*.  This
> might be happening here, though I didn't read your code carefully enough
> to be able to be certain.
>
>
>> which I don't really know if it took into account as the limit is
>> greater than the physical RAM of 4GB. ...?
>
> :)
>
>> would it be easier using Linux ?
>
> possibly a little bit - on a linux machine you can at least run a PAE
> kernel (giving you a lot more address space to work with) and have the
> ability to turn on a bit more virtual memory.
>
> usually with data of the size you're trying to work with, i try to find a
> way to preprocess the data a bit more before i apply R's tools to it.
> sometimes we stick it into a database (postgres) and select out the bits
> we want our inferences to be sourced from.  ;)
>
> it might be simplest to just hunt up a machine with 8 or 16GB of memory in
> it, and run those bits of the analysis that really need memory on that
> machine...

Yes, if there is no other way, a 64bit machine with lots of RAM would not 
be so contrained, but maybe this is a matter of first deciding why doing 
statistics on that much data is worth the effort? It may be, but just 
trying to read large amounts of data into memory is perhaps not justified 
in itself.

Can you tile or subset the data, accumulating intermediate results? This 
is the approach the biglm package takes, and the R/GDAL interface also 
supports subsetting from an external file.

Depending on the input format of the data, you should be able to do all 
you need provided that you do not try to keep all the data in memory. 
Using a database may be a good idea, or if the data are multiple remote 
sensing images, subsetting and accumulating results.

Roger

>
> --e
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From eaeff at mtsu.edu  Tue Sep 11 20:21:29 2007
From: eaeff at mtsu.edu (eaeff at mtsu.edu)
Date: Tue, 11 Sep 2007 13:21:29 -0500 (CDT)
Subject: [R-sig-Geo] biparametric model
Message-ID: <20070911132129.APV48019@mtsu125.mtsu.edu>

Roger,
Thanks for your response--as always (and I've lurked here a while), you were very helpful. 
There are three contexts I can think of in which two weight matrices would be desirable:
1. For a gravity model, where each observation is a link between source region and destination region, one spatially lagged variable for the source and one spatially lagged variable for the destination (this is the original Brandsma and Ketellapper application).
2. There is a recent paper by Donald J. LaCombe (?Does Econometric Methodology Matter? An Analysis of Public Policy Using Spatial Econometric Techniques?, Geographical Analysis, vol. 36, no. 2, April 2004, pp 105?118.) in which he uses two weight matrices to disentangle border effects.
3. There is a literature in anthropology and cross-cultural research, pretty much fully developed by the mid-1980s, in which ?Galton's Problem? is addressed by introducing two kinds of spatial dependence: via physical proximity and via cultural proximity. (e.g., Malcolm M. Dow, Michael L. Burton, Douglas R. White, Karl P. Reitz. (1984) ?Galton's Problem as Network  Autocorrelation.? American Ethnologist. 11(4):754-770 )

My own research interest is actually the third option above. 

best,
Anthon


<Anthon Eff>
Please forgive what may be a naive question. I'm working on a model that requires two different weight matrices, as in the "biparametric" model introduced by Brandsma and Ketellapper in 1979. I haven't had any luck finding a way to do this in R. Any suggestions?

<Roger Bivand>
The spatial error model fitting functions spautolm() and errorsarlm(), and the spatial lag model fitting function lagsarlm() only fit a single set of spatial weights, by optimising in one dimension.

It would be possible to generalise them to optimise in more than one dimension, but is it justified? Is there a very clear behavioural model that requires the fitting of more than one spatial regression coefficient that is driving thw question? Is it going to be practical to fit more than one coefficient on a probably rather flat surface?

The slm() function in S-Plus SpatialStats module can do this for the error model if need be, but there is a case to be made for why it is necessary, unless there is a clear behavioural model.

Roger



From e.pebesma at geo.uu.nl  Tue Sep 11 20:30:24 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Tue, 11 Sep 2007 20:30:24 +0200
Subject: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
 Sampled Data
In-Reply-To: <221470FD0F9FD24ABE3A2F5569FF54250B38F9@statserversbs.Statkingconsulting.local>
References: <221470FD0F9FD24ABE3A2F5569FF54250B38F9@statserversbs.Statkingconsulting.local>
Message-ID: <46E6DEC0.5010201@geo.uu.nl>

Keith,

indeed kriging usually fails when one or more point pairs have zero 
distance. One solution in terms of distances would be to shift these 
points a bit, such that no zero distances occur anymore. In terms of the 
covariances, the solution would be to lower the corresponding 
off-diagonal entries with a small amount.

If you have measurements with a known measurement error variance, it may 
make sense to use this variance as the amount to subtract from all  
off-diagonal elements of the covariance matrix.

Hope this helps,
--
Edzer

Keith Dunnigan wrote:
> Hello all,
>
>  
>
>   First I would like to apologize if this question is inappropriate for
> this list.  I am new here, I found this list doing a web search and it
> seemed like the members here would have knowledge in this area.  If
> there are more appropriate lists of forums for this question, I would
> appreciate that information.
>
>  
>
>   I do the majority of my work as a biostatistician in the
> pharmaceutical industry, so I am new to this area.  I am working on a
> couple of small projects in this area though.  I have consulted a couple
> of basic texts ("Introduction to Geostatistics" by Kitanidis, and "An
> Introduction to Applied Geostatistics" by Isaaks & Srivastava).
>
>  
>
>   The gist of what I have gathered from my reading is that standard
> practice is not to use the actual covariance matrix calculated from the
> data.  This is because this matrix may in general not be positive
> definite.  Instead standard practice seems to be to pick from one of
> several standard covariance models, which are guaranteed to be positive
> definite.  After fitting the most appropriate model then, one generates
> the covariance matrix from this model and the distance matrix.  The
> resulting matrix should be positive definite.
>
>  
>
>   The only problem is, I am not finding that to be true.  For instance,
> when I apply the exponential model to my distance matrix and calculate
> the eigenvalues, I find that some of them are negative.  Very, very
> small, but negative (For example -1.2 x 10exp-13).  I applied a couple
> of models and found this to be true. Could someone help me with this?
>
>  
>
>   This is a small data set.  I have a distance matrix that is 20 by 20.
> The exponential model I have used has range parameter R = 14 and sigma
> squared parameter 86.618.  Letting the distance be x, the exponential
> model then is c(x) = sigmasq * exp( ((-3)*x)/R .  
>
>  
>
>   My distance matrix is such that most of the covariances have very
> small values (effectively zero), except for the first couple of
> distances.  That may be the trouble, what do geo folks usually do in
> situations such as this?  I have copied the distance matrix below in the
> case any of you wants to take a look at this.
>
>  
>
>                  0 162 232 246 474   0 162 232 246 474   0 162 232 246
> 474   0 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312 162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242 232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228 246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0 474
> 312 242 228   0
>
>            0 162 232 246 474   0 162 232 246 474   0 162 232 246 474   0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312 162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242 232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228 246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0 474
> 312 242 228   0
>
>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474   0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312 162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242 232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228 246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0 474
> 312 242 228   0
>
>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474   0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312 162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242 232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228 246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0 474
> 312 242 228   0
>
>  
>
>   Thanks in advance for any help you can provide!  Warmest Regards,
>
>  
>
>     Keith Dunnigan
>
>     Statking Consulting
>
>     Cincinnati Ohio
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From keith at statkingconsulting.com  Tue Sep 11 22:57:37 2007
From: keith at statkingconsulting.com (Keith Dunnigan)
Date: Tue, 11 Sep 2007 16:57:37 -0400
Subject: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
	Sampled Data
In-Reply-To: <46E6DEC0.5010201@geo.uu.nl>
References: <221470FD0F9FD24ABE3A2F5569FF54250B38F9@statserversbs.Statkingconsulting.local>
	<46E6DEC0.5010201@geo.uu.nl>
Message-ID: <221470FD0F9FD24ABE3A2F5569FF54250B3904@statserversbs.Statkingconsulting.local>

Edzer,

  Thanks for your help!  I tried the first suggestion.., I replaced all
the zero's in the distance matrix with a different value.., but I still
have the same problem.  I get negative eigenvalues.  I tried various
constants, replacing the zero with positive numbers up to 5, with no
luck.

  Anyone have any other ideas?

  Keith

-----Original Message-----
From: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
Sent: Tuesday, September 11, 2007 2:30 PM
To: Keith Dunnigan
Cc: r-sig-geo at stat.math.ethz.ch
Subject: Re: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
Sampled Data

Keith,

indeed kriging usually fails when one or more point pairs have zero 
distance. One solution in terms of distances would be to shift these 
points a bit, such that no zero distances occur anymore. In terms of the

covariances, the solution would be to lower the corresponding 
off-diagonal entries with a small amount.

If you have measurements with a known measurement error variance, it may

make sense to use this variance as the amount to subtract from all  
off-diagonal elements of the covariance matrix.

Hope this helps,
--
Edzer

Keith Dunnigan wrote:
> Hello all,
>
>  
>
>   First I would like to apologize if this question is inappropriate
for
> this list.  I am new here, I found this list doing a web search and it
> seemed like the members here would have knowledge in this area.  If
> there are more appropriate lists of forums for this question, I would
> appreciate that information.
>
>  
>
>   I do the majority of my work as a biostatistician in the
> pharmaceutical industry, so I am new to this area.  I am working on a
> couple of small projects in this area though.  I have consulted a
couple
> of basic texts ("Introduction to Geostatistics" by Kitanidis, and "An
> Introduction to Applied Geostatistics" by Isaaks & Srivastava).
>
>  
>
>   The gist of what I have gathered from my reading is that standard
> practice is not to use the actual covariance matrix calculated from
the
> data.  This is because this matrix may in general not be positive
> definite.  Instead standard practice seems to be to pick from one of
> several standard covariance models, which are guaranteed to be
positive
> definite.  After fitting the most appropriate model then, one
generates
> the covariance matrix from this model and the distance matrix.  The
> resulting matrix should be positive definite.
>
>  
>
>   The only problem is, I am not finding that to be true.  For
instance,
> when I apply the exponential model to my distance matrix and calculate
> the eigenvalues, I find that some of them are negative.  Very, very
> small, but negative (For example -1.2 x 10exp-13).  I applied a couple
> of models and found this to be true. Could someone help me with this?
>
>  
>
>   This is a small data set.  I have a distance matrix that is 20 by
20.
> The exponential model I have used has range parameter R = 14 and sigma
> squared parameter 86.618.  Letting the distance be x, the exponential
> model then is c(x) = sigmasq * exp( ((-3)*x)/R .  
>
>  
>
>   My distance matrix is such that most of the covariances have very
> small values (effectively zero), except for the first couple of
> distances.  That may be the trouble, what do geo folks usually do in
> situations such as this?  I have copied the distance matrix below in
the
> case any of you wants to take a look at this.
>
>  
>
>                  0 162 232 246 474   0 162 232 246 474   0 162 232 246
> 474   0 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
474
> 312 242 228   0
>
>            0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
474
> 312 242 228   0
>
>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
474
> 312 242 228   0
>
>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
474
> 312 242 228   0
>
>  
>
>   Thanks in advance for any help you can provide!  Warmest Regards,
>
>  
>
>     Keith Dunnigan
>
>     Statking Consulting
>
>     Cincinnati Ohio
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From paciorek at hsph.harvard.edu  Wed Sep 12 00:18:15 2007
From: paciorek at hsph.harvard.edu (Christopher Paciorek)
Date: Tue, 11 Sep 2007 18:18:15 -0400
Subject: [R-sig-Geo] Positive Definite Covariance Matrix for
	Grid	Sampled Data
In-Reply-To: <221470FD0F9FD24ABE3A2F5569FF54250B3904@statserversbs.Statkingconsulting.local>
References: <221470FD0F9FD24ABE3A2F5569FF54250B38F9@statserversbs.Statkingconsulting.local>
	<46E6DEC0.5010201@geo.uu.nl>
	<221470FD0F9FD24ABE3A2F5569FF54250B3904@statserversbs.Statkingconsulting.local>
Message-ID: <46E6DB85.10CD.002E.0@hsph.harvard.edu>

If you want to avoid zero distances, what Edzer meant is to actually jitter the points and then recalculate the distance matrix based on the new locations.  Replacing the zeros doesn't give you a valid distance matrix because it doesn't account for having moved the two points with respect to all the other points.

For an exponential covariance that will probably be sufficient. For the Matern or squared exponential (Gaussian), if you have small distances, you may need to add a small amount of variance to the diagonals to make it numerically stable (essentially a fake nugget).  Alternatively, if you fit the covariance (based on maximum likelihood or fitting the semivariogram) and you get a non-zero nugget, that has the same effect but in that case the added variance may be substantial.

chris

 
 
>>> "Keith Dunnigan" <keith at statkingconsulting.com> 09/11/07 4:57 PM >>> 
Edzer,

  Thanks for your help!  I tried the first suggestion.., I replaced all
the zero's in the distance matrix with a different value.., but I still
have the same problem.  I get negative eigenvalues.  I tried various
constants, replacing the zero with positive numbers up to 5, with no
luck.

  Anyone have any other ideas?

  Keith

----- Original Message-----
From: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
Sent: Tuesday, September 11, 2007 2:30 PM
To: Keith Dunnigan
Cc: r- sig- geo at stat.math.ethz.ch
Subject: Re: [R- sig- Geo] Positive Definite Covariance Matrix for Grid
Sampled Data

Keith,

indeed kriging usually fails when one or more point pairs have zero 
distance. One solution in terms of distances would be to shift these 
points a bit, such that no zero distances occur anymore. In terms of the

covariances, the solution would be to lower the corresponding 
off- diagonal entries with a small amount.

If you have measurements with a known measurement error variance, it may

make sense to use this variance as the amount to subtract from all  
off- diagonal elements of the covariance matrix.

Hope this helps,
--
Edzer

Keith Dunnigan wrote:
> Hello all,
>
>  
>
>   First I would like to apologize if this question is inappropriate
for
> this list.  I am new here, I found this list doing a web search and it
> seemed like the members here would have knowledge in this area.  If
> there are more appropriate lists of forums for this question, I would
> appreciate that information.
>
>  
>
>   I do the majority of my work as a biostatistician in the
> pharmaceutical industry, so I am new to this area.  I am working on a
> couple of small projects in this area though.  I have consulted a
couple
> of basic texts ("Introduction to Geostatistics" by Kitanidis, and "An
> Introduction to Applied Geostatistics" by Isaaks & Srivastava).
>
>  
>
>   The gist of what I have gathered from my reading is that standard
> practice is not to use the actual covariance matrix calculated from
the
> data.  This is because this matrix may in general not be positive
> definite.  Instead standard practice seems to be to pick from one of
> several standard covariance models, which are guaranteed to be
positive
> definite.  After fitting the most appropriate model then, one
generates
> the covariance matrix from this model and the distance matrix.  The
> resulting matrix should be positive definite.
>
>  
>
>   The only problem is, I am not finding that to be true.  For
instance,
> when I apply the exponential model to my distance matrix and calculate
> the eigenvalues, I find that some of them are negative.  Very, very
> small, but negative (For example - 1.2 x 10exp- 13).  I applied a couple
> of models and found this to be true. Could someone help me with this?
>
>  
>
>   This is a small data set.  I have a distance matrix that is 20 by
20.
> The exponential model I have used has range parameter R = 14 and sigma
> squared parameter 86.618.  Letting the distance be x, the exponential
> model then is c(x) = sigmasq * exp( ((- 3)*x)/R .  
>
>  
>
>   My distance matrix is such that most of the covariances have very
> small values (effectively zero), except for the first couple of
> distances.  That may be the trouble, what do geo folks usually do in
> situations such as this?  I have copied the distance matrix below in
the
> case any of you wants to take a look at this.
>
>  
>
>                  0 162 232 246 474   0 162 232 246 474   0 162 232 246
> 474   0 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
474
> 312 242 228   0
>
>            0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
474
> 312 242 228   0
>
>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
474
> 312 242 228   0
>
>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
0
> 162 232 246 474
>
>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
162
> 0  70  84 312
>
>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
232
> 70   0  14 242
>
>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
246
> 84  14   0 228
>
>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
474
> 312 242 228   0
>
>  
>
>   Thanks in advance for any help you can provide!  Warmest Regards,
>
>  
>
>     Keith Dunnigan
>
>     Statking Consulting
>
>     Cincinnati Ohio
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R- sig- Geo mailing list
> R- sig- Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r- sig- geo
>

_______________________________________________
R- sig- Geo mailing list
R- sig- Geo at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r- sig- geo



From e.pebesma at geo.uu.nl  Wed Sep 12 09:01:10 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Wed, 12 Sep 2007 09:01:10 +0200
Subject: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
 Sampled Data
In-Reply-To: <221470FD0F9FD24ABE3A2F5569FF54250B3904@statserversbs.Statkingconsulting.local>
References: <221470FD0F9FD24ABE3A2F5569FF54250B38F9@statserversbs.Statkingconsulting.local>
	<46E6DEC0.5010201@geo.uu.nl>
	<221470FD0F9FD24ABE3A2F5569FF54250B3904@statserversbs.Statkingconsulting.local>
Message-ID: <46E78EB6.2030104@geo.uu.nl>

Keith, read my suggestion good; I didn't suggest replacing all zeroes in 
the distance matrix! You either rearrange points and recompute 
distances, or modify off-diagonal zero-distance entries in the 
covariance matrix.

Sounds like you modified the zero entries on the diagonal as well.
--
Edzer

Keith Dunnigan wrote:
> Edzer,
>
>   Thanks for your help!  I tried the first suggestion.., I replaced all
> the zero's in the distance matrix with a different value.., but I still
> have the same problem.  I get negative eigenvalues.  I tried various
> constants, replacing the zero with positive numbers up to 5, with no
> luck.
>
>   Anyone have any other ideas?
>
>   Keith
>
> -----Original Message-----
> From: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
> Sent: Tuesday, September 11, 2007 2:30 PM
> To: Keith Dunnigan
> Cc: r-sig-geo at stat.math.ethz.ch
> Subject: Re: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
> Sampled Data
>
> Keith,
>
> indeed kriging usually fails when one or more point pairs have zero 
> distance. One solution in terms of distances would be to shift these 
> points a bit, such that no zero distances occur anymore. In terms of the
>
> covariances, the solution would be to lower the corresponding 
> off-diagonal entries with a small amount.
>
> If you have measurements with a known measurement error variance, it may
>
> make sense to use this variance as the amount to subtract from all  
> off-diagonal elements of the covariance matrix.
>
> Hope this helps,
> --
> Edzer
>
> Keith Dunnigan wrote:
>   
>> Hello all,
>>
>>  
>>
>>   First I would like to apologize if this question is inappropriate
>>     
> for
>   
>> this list.  I am new here, I found this list doing a web search and it
>> seemed like the members here would have knowledge in this area.  If
>> there are more appropriate lists of forums for this question, I would
>> appreciate that information.
>>
>>  
>>
>>   I do the majority of my work as a biostatistician in the
>> pharmaceutical industry, so I am new to this area.  I am working on a
>> couple of small projects in this area though.  I have consulted a
>>     
> couple
>   
>> of basic texts ("Introduction to Geostatistics" by Kitanidis, and "An
>> Introduction to Applied Geostatistics" by Isaaks & Srivastava).
>>
>>  
>>
>>   The gist of what I have gathered from my reading is that standard
>> practice is not to use the actual covariance matrix calculated from
>>     
> the
>   
>> data.  This is because this matrix may in general not be positive
>> definite.  Instead standard practice seems to be to pick from one of
>> several standard covariance models, which are guaranteed to be
>>     
> positive
>   
>> definite.  After fitting the most appropriate model then, one
>>     
> generates
>   
>> the covariance matrix from this model and the distance matrix.  The
>> resulting matrix should be positive definite.
>>
>>  
>>
>>   The only problem is, I am not finding that to be true.  For
>>     
> instance,
>   
>> when I apply the exponential model to my distance matrix and calculate
>> the eigenvalues, I find that some of them are negative.  Very, very
>> small, but negative (For example -1.2 x 10exp-13).  I applied a couple
>> of models and found this to be true. Could someone help me with this?
>>
>>  
>>
>>   This is a small data set.  I have a distance matrix that is 20 by
>>     
> 20.
>   
>> The exponential model I have used has range parameter R = 14 and sigma
>> squared parameter 86.618.  Letting the distance be x, the exponential
>> model then is c(x) = sigmasq * exp( ((-3)*x)/R .  
>>
>>  
>>
>>   My distance matrix is such that most of the covariances have very
>> small values (effectively zero), except for the first couple of
>> distances.  That may be the trouble, what do geo folks usually do in
>> situations such as this?  I have copied the distance matrix below in
>>     
> the
>   
>> case any of you wants to take a look at this.
>>
>>  
>>
>>                  0 162 232 246 474   0 162 232 246 474   0 162 232 246
>> 474   0 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>            0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>  
>>
>>   Thanks in advance for any help you can provide!  Warmest Regards,
>>
>>  
>>
>>     Keith Dunnigan
>>
>>     Statking Consulting
>>
>>     Cincinnati Ohio
>>
>>  
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>   
>>     
>
>
>



From ajung at gfz-potsdam.de  Wed Sep 12 14:48:52 2007
From: ajung at gfz-potsdam.de (Andre Jung)
Date: Wed, 12 Sep 2007 14:48:52 +0200
Subject: [R-sig-Geo] plot contour map for irregular data points
Message-ID: <web-22594776@cgp1.gfz-potsdam.de>

Hello,

I'm dealing with the following problem:

I have a table with x and y coordinates and corresponding values of a 
mineral concentration, let's call it z.

Can someone provide me a short step-by-step manual for the steps 
necessary to get a contour map?

How to sort and interpolate my matrix to an equidistant grid which can 
afterwards be plotted by contour(x,y,z)? (e.g. fill the missing 
locations with NAs.)

Thanks, guys.



From keith at statkingconsulting.com  Wed Sep 12 15:00:39 2007
From: keith at statkingconsulting.com (Keith Dunnigan)
Date: Wed, 12 Sep 2007 09:00:39 -0400
Subject: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
	Sampled Data
In-Reply-To: <46E78EB6.2030104@geo.uu.nl>
References: <221470FD0F9FD24ABE3A2F5569FF54250B38F9@statserversbs.Statkingconsulting.local>
	<46E6DEC0.5010201@geo.uu.nl>
	<221470FD0F9FD24ABE3A2F5569FF54250B3904@statserversbs.Statkingconsulting.local>
	<46E78EB6.2030104@geo.uu.nl>
Message-ID: <221470FD0F9FD24ABE3A2F5569FF54250B3905@statserversbs.Statkingconsulting.local>

Edzer,

  Thanks again, sorry if I misread you.  Thanks also to James Holland
Jones and Christopher Paciorek who responded with helpful comments.  I
will read over all of these and give it another try.

  Keith

-----Original Message-----
From: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
Sent: Wednesday, September 12, 2007 3:01 AM
To: Keith Dunnigan
Cc: r-sig-geo at stat.math.ethz.ch
Subject: Re: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
Sampled Data

Keith, read my suggestion good; I didn't suggest replacing all zeroes in

the distance matrix! You either rearrange points and recompute 
distances, or modify off-diagonal zero-distance entries in the 
covariance matrix.

Sounds like you modified the zero entries on the diagonal as well.
--
Edzer

Keith Dunnigan wrote:
> Edzer,
>
>   Thanks for your help!  I tried the first suggestion.., I replaced
all
> the zero's in the distance matrix with a different value.., but I
still
> have the same problem.  I get negative eigenvalues.  I tried various
> constants, replacing the zero with positive numbers up to 5, with no
> luck.
>
>   Anyone have any other ideas?
>
>   Keith
>
> -----Original Message-----
> From: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
> Sent: Tuesday, September 11, 2007 2:30 PM
> To: Keith Dunnigan
> Cc: r-sig-geo at stat.math.ethz.ch
> Subject: Re: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
> Sampled Data
>
> Keith,
>
> indeed kriging usually fails when one or more point pairs have zero 
> distance. One solution in terms of distances would be to shift these 
> points a bit, such that no zero distances occur anymore. In terms of
the
>
> covariances, the solution would be to lower the corresponding 
> off-diagonal entries with a small amount.
>
> If you have measurements with a known measurement error variance, it
may
>
> make sense to use this variance as the amount to subtract from all  
> off-diagonal elements of the covariance matrix.
>
> Hope this helps,
> --
> Edzer
>
> Keith Dunnigan wrote:
>   
>> Hello all,
>>
>>  
>>
>>   First I would like to apologize if this question is inappropriate
>>     
> for
>   
>> this list.  I am new here, I found this list doing a web search and
it
>> seemed like the members here would have knowledge in this area.  If
>> there are more appropriate lists of forums for this question, I would
>> appreciate that information.
>>
>>  
>>
>>   I do the majority of my work as a biostatistician in the
>> pharmaceutical industry, so I am new to this area.  I am working on a
>> couple of small projects in this area though.  I have consulted a
>>     
> couple
>   
>> of basic texts ("Introduction to Geostatistics" by Kitanidis, and "An
>> Introduction to Applied Geostatistics" by Isaaks & Srivastava).
>>
>>  
>>
>>   The gist of what I have gathered from my reading is that standard
>> practice is not to use the actual covariance matrix calculated from
>>     
> the
>   
>> data.  This is because this matrix may in general not be positive
>> definite.  Instead standard practice seems to be to pick from one of
>> several standard covariance models, which are guaranteed to be
>>     
> positive
>   
>> definite.  After fitting the most appropriate model then, one
>>     
> generates
>   
>> the covariance matrix from this model and the distance matrix.  The
>> resulting matrix should be positive definite.
>>
>>  
>>
>>   The only problem is, I am not finding that to be true.  For
>>     
> instance,
>   
>> when I apply the exponential model to my distance matrix and
calculate
>> the eigenvalues, I find that some of them are negative.  Very, very
>> small, but negative (For example -1.2 x 10exp-13).  I applied a
couple
>> of models and found this to be true. Could someone help me with this?
>>
>>  
>>
>>   This is a small data set.  I have a distance matrix that is 20 by
>>     
> 20.
>   
>> The exponential model I have used has range parameter R = 14 and
sigma
>> squared parameter 86.618.  Letting the distance be x, the exponential
>> model then is c(x) = sigmasq * exp( ((-3)*x)/R .  
>>
>>  
>>
>>   My distance matrix is such that most of the covariances have very
>> small values (effectively zero), except for the first couple of
>> distances.  That may be the trouble, what do geo folks usually do in
>> situations such as this?  I have copied the distance matrix below in
>>     
> the
>   
>> case any of you wants to take a look at this.
>>
>>  
>>
>>                  0 162 232 246 474   0 162 232 246 474   0 162 232
246
>> 474   0 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>            0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>  
>>
>>   Thanks in advance for any help you can provide!  Warmest Regards,
>>
>>  
>>
>>     Keith Dunnigan
>>
>>     Statking Consulting
>>
>>     Cincinnati Ohio
>>
>>  
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>   
>>     
>
>
>



From keith at statkingconsulting.com  Wed Sep 12 16:58:50 2007
From: keith at statkingconsulting.com (Keith Dunnigan)
Date: Wed, 12 Sep 2007 10:58:50 -0400
Subject: [R-sig-Geo] Positive Definite Covariance Matrix for GridSampled
	Data
In-Reply-To: <221470FD0F9FD24ABE3A2F5569FF54250B3905@statserversbs.Statkingconsulting.local>
References: <221470FD0F9FD24ABE3A2F5569FF54250B38F9@statserversbs.Statkingconsulting.local><46E6DEC0.5010201@geo.uu.nl><221470FD0F9FD24ABE3A2F5569FF54250B3904@statserversbs.Statkingconsulting.local><46E78EB6.2030104@geo.uu.nl>
	<221470FD0F9FD24ABE3A2F5569FF54250B3905@statserversbs.Statkingconsulting.local>
Message-ID: <221470FD0F9FD24ABE3A2F5569FF54250B3907@statserversbs.Statkingconsulting.local>

Edzer and List,

  Success!  Thanks so much to Edzer and all who took the time to write.


  I adjusted the x coordinates by a small amount such that no two x
coordinates were exactly the same, then recalculated the covariance
matrix and the negative eigenvalues disappeared.

  However several cases of duplicate eigenvalues appeared.  My strong
suspicion is that they were not truly identical, but only identical to
the resolution given by my software (8 places behind the decimal).  This
especially so as most values in the covariance matrix were zero to
several places behind the decimal.  Nonetheless to make myself feel
better, I tweaked the adjustment of the x coordinates a bit further and
that did the trick.  No duplicate or negative eigenvalues.


  So thanks again, your suggestions were right on and helped me out a
great deal!

  Warmest Regards,

  Keith Dunnigan
  Statking Consulting
  Cincinnati Ohio


-----Original Message-----
From: r-sig-geo-bounces at stat.math.ethz.ch
[mailto:r-sig-geo-bounces at stat.math.ethz.ch] On Behalf Of Keith Dunnigan
Sent: Wednesday, September 12, 2007 9:01 AM
To: Edzer J. Pebesma
Cc: r-sig-geo at stat.math.ethz.ch
Subject: Re: [R-sig-Geo] Positive Definite Covariance Matrix for
GridSampled Data

Edzer,

  Thanks again, sorry if I misread you.  Thanks also to James Holland
Jones and Christopher Paciorek who responded with helpful comments.  I
will read over all of these and give it another try.

  Keith

-----Original Message-----
From: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
Sent: Wednesday, September 12, 2007 3:01 AM
To: Keith Dunnigan
Cc: r-sig-geo at stat.math.ethz.ch
Subject: Re: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
Sampled Data

Keith, read my suggestion good; I didn't suggest replacing all zeroes in

the distance matrix! You either rearrange points and recompute 
distances, or modify off-diagonal zero-distance entries in the 
covariance matrix.

Sounds like you modified the zero entries on the diagonal as well.
--
Edzer

Keith Dunnigan wrote:
> Edzer,
>
>   Thanks for your help!  I tried the first suggestion.., I replaced
all
> the zero's in the distance matrix with a different value.., but I
still
> have the same problem.  I get negative eigenvalues.  I tried various
> constants, replacing the zero with positive numbers up to 5, with no
> luck.
>
>   Anyone have any other ideas?
>
>   Keith
>
> -----Original Message-----
> From: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
> Sent: Tuesday, September 11, 2007 2:30 PM
> To: Keith Dunnigan
> Cc: r-sig-geo at stat.math.ethz.ch
> Subject: Re: [R-sig-Geo] Positive Definite Covariance Matrix for Grid
> Sampled Data
>
> Keith,
>
> indeed kriging usually fails when one or more point pairs have zero 
> distance. One solution in terms of distances would be to shift these 
> points a bit, such that no zero distances occur anymore. In terms of
the
>
> covariances, the solution would be to lower the corresponding 
> off-diagonal entries with a small amount.
>
> If you have measurements with a known measurement error variance, it
may
>
> make sense to use this variance as the amount to subtract from all  
> off-diagonal elements of the covariance matrix.
>
> Hope this helps,
> --
> Edzer
>
> Keith Dunnigan wrote:
>   
>> Hello all,
>>
>>  
>>
>>   First I would like to apologize if this question is inappropriate
>>     
> for
>   
>> this list.  I am new here, I found this list doing a web search and
it
>> seemed like the members here would have knowledge in this area.  If
>> there are more appropriate lists of forums for this question, I would
>> appreciate that information.
>>
>>  
>>
>>   I do the majority of my work as a biostatistician in the
>> pharmaceutical industry, so I am new to this area.  I am working on a
>> couple of small projects in this area though.  I have consulted a
>>     
> couple
>   
>> of basic texts ("Introduction to Geostatistics" by Kitanidis, and "An
>> Introduction to Applied Geostatistics" by Isaaks & Srivastava).
>>
>>  
>>
>>   The gist of what I have gathered from my reading is that standard
>> practice is not to use the actual covariance matrix calculated from
>>     
> the
>   
>> data.  This is because this matrix may in general not be positive
>> definite.  Instead standard practice seems to be to pick from one of
>> several standard covariance models, which are guaranteed to be
>>     
> positive
>   
>> definite.  After fitting the most appropriate model then, one
>>     
> generates
>   
>> the covariance matrix from this model and the distance matrix.  The
>> resulting matrix should be positive definite.
>>
>>  
>>
>>   The only problem is, I am not finding that to be true.  For
>>     
> instance,
>   
>> when I apply the exponential model to my distance matrix and
calculate
>> the eigenvalues, I find that some of them are negative.  Very, very
>> small, but negative (For example -1.2 x 10exp-13).  I applied a
couple
>> of models and found this to be true. Could someone help me with this?
>>
>>  
>>
>>   This is a small data set.  I have a distance matrix that is 20 by
>>     
> 20.
>   
>> The exponential model I have used has range parameter R = 14 and
sigma
>> squared parameter 86.618.  Letting the distance be x, the exponential
>> model then is c(x) = sigmasq * exp( ((-3)*x)/R .  
>>
>>  
>>
>>   My distance matrix is such that most of the covariances have very
>> small values (effectively zero), except for the first couple of
>> distances.  That may be the trouble, what do geo folks usually do in
>> situations such as this?  I have copied the distance matrix below in
>>     
> the
>   
>> case any of you wants to take a look at this.
>>
>>  
>>
>>                  0 162 232 246 474   0 162 232 246 474   0 162 232
246
>> 474   0 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>            0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>           0 162 232 246 474   0 162 232 246 474   0 162 232 246 474
>>     
> 0
>   
>> 162 232 246 474
>>
>>          162   0  70  84 312 162   0  70  84 312 162   0  70  84 312
>>     
> 162
>   
>> 0  70  84 312
>>
>>          232  70   0  14 242 232  70   0  14 242 232  70   0  14 242
>>     
> 232
>   
>> 70   0  14 242
>>
>>          246  84  14   0 228 246  84  14   0 228 246  84  14   0 228
>>     
> 246
>   
>> 84  14   0 228
>>
>>          474 312 242 228   0 474 312 242 228   0 474 312 242 228   0
>>     
> 474
>   
>> 312 242 228   0
>>
>>  
>>
>>   Thanks in advance for any help you can provide!  Warmest Regards,
>>
>>  
>>
>>     Keith Dunnigan
>>
>>     Statking Consulting
>>
>>     Cincinnati Ohio
>>
>>  
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>   
>>     
>
>
>

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-geo



From macq at llnl.gov  Thu Sep 13 01:09:32 2007
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 12 Sep 2007 16:09:32 -0700
Subject: [R-sig-Geo] plot contour map for irregular data points
In-Reply-To: <web-22594776@cgp1.gfz-potsdam.de>
References: <web-22594776@cgp1.gfz-potsdam.de>
Message-ID: <p06230909c30e21c2f112@[128.115.153.6]>

See the help for the interp() function in the akima package. It has 
some examples that should be reasonably easy to follow.

-Don

At 2:48 PM +0200 9/12/07, Andre Jung wrote:
>Hello,
>
>I'm dealing with the following problem:
>
>I have a table with x and y coordinates and corresponding values of a
>mineral concentration, let's call it z.
>
>Can someone provide me a short step-by-step manual for the steps
>necessary to get a contour map?
>
>How to sort and interpolate my matrix to an equidistant grid which can
>afterwards be plotted by contour(x,y,z)? (e.g. fill the missing
>locations with NAs.)
>
>Thanks, guys.
>
>_______________________________________________
>R-sig-Geo mailing list
>R-sig-Geo at stat.math.ethz.ch
>https://stat.ethz.ch/mailman/listinfo/r-sig-geo


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA
925-423-1062



From epistat at gmail.com  Thu Sep 13 08:32:37 2007
From: epistat at gmail.com (zhijie zhang)
Date: Thu, 13 Sep 2007 14:32:37 +0800
Subject: [R-sig-Geo] Functions for spatio-temporal/space-time analysis?
Message-ID: <2fc17e30709122332m512100b6vfb8253c14cdc14cc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070913/5b8edd9d/attachment.pl>

From Roger.Bivand at nhh.no  Thu Sep 13 09:00:22 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 13 Sep 2007 09:00:22 +0200 (CEST)
Subject: [R-sig-Geo] Functions for spatio-temporal/space-time analysis?
In-Reply-To: <2fc17e30709122332m512100b6vfb8253c14cdc14cc@mail.gmail.com>
References: <2fc17e30709122332m512100b6vfb8253c14cdc14cc@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0709130855530.32739@reclus.nhh.no>

On Thu, 13 Sep 2007, zhijie zhang wrote:

> Dear friends,
>  I'm conducting some spatio-temporal/space-time analysis for point pattern
> data, and the following functions maybe used to do it.
> Anybody knows some other functions/methods for spatio-temporal/space-time
> analysis  in R?
>
>
> stkhat (splancs)
>
> Space-time K-functions
>
> stmctest (splancs)
>
> Monte-Carlo test of space-time clustering
>
> stsecal (splancs)
>
> Standard error for space-time clustering
>
> stvmat (splancs)
>
> Variance matrix for space-time clustering
> I appreciate your help/suggestions very much.

Roger Peng's ptproc package was originally on CRAN, version 1.0 is in the 
contributed package source archive. A more recent source package version 
is on his home repository:

http://www.biostat.jhsph.edu/~rpeng/software/src/contrib/ptproc_1.5-1.tar.gz

The package is described in:

http://www.jstatsoft.org/v08/i16/ptprocR102.pdf

His CV also has links to other papers related to space-time point 
processes, I believe based on his doctoral research.

Hope this helps,

Roger

>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From didier.leibovici at nottingham.ac.uk  Thu Sep 13 11:48:09 2007
From: didier.leibovici at nottingham.ac.uk (Didier Leibovici)
Date: Thu, 13 Sep 2007 10:48:09 +0100
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no>
References: <46E66E0F.3000704@nottingham.ac.uk>
	<Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>
	<Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no>
Message-ID: <46E90759.5060301@nottingham.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070913/dfad5304/attachment.pl>

From e.pebesma at geo.uu.nl  Thu Sep 13 12:14:49 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Thu, 13 Sep 2007 12:14:49 +0200
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <46E90759.5060301@nottingham.ac.uk>
References: <46E66E0F.3000704@nottingham.ac.uk>	<Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>	<Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no>
	<46E90759.5060301@nottingham.ac.uk>
Message-ID: <46E90D99.4030100@geo.uu.nl>

I think R will never do it's own memory swapping, as that is a typical 
OS task. There are however several developments (provided in add-on 
packages) that will not load all data in memory at start-up, but instead 
call some data base whenever a data element is needed. You might search 
r-help for rsqlite or biglm, and there are others; also look at the 
award winners at useR this year.

Here, we've run pretty successful R sessions needing 10-11 Gb of memory 
on a 8Gb RAM 64 bits linux machine with lots of swap space. Needs some 
patience, and still R might crash other parts of the system when memory 
usage becomes too excessive.

Best regards,
--
Edzer

Didier Leibovici wrote:
> Thanks Roger
>
> I feel we've got a low RAM machine which would need a bit of an uplift 
> (recent server though)!
> The linux machine is unfortunately also with 4Gb of RAM
> But  I persist to say it would be interesting to have within R a way of 
> automatically performing swapping memory if needed ...
>
> Didier
>
> Roger Bivand wrote:
>   
>> On Tue, 11 Sep 2007, elw at stderr.org wrote:
>>
>>     
>>>> These days in GIS on may have to manipulate big datasets or arrays.
>>>>
>>>> Here I am on WINDOWS I have a 4Gb
>>>> my aim was to have an array of dim 298249 12 10 22 but that's 2.9Gb
>>>>         
>> Assuming double precision (no single precision in R), 5.8Gb.
>>
>>     
>>> It used to be (maybe still is?) the case that a single process could 
>>> only
>>> 'claim' a chunk of max size 2GB on Windows.
>>>
>>>
>>> Also remember to compute overhead for R objects... 58 bytes per 
>>> object, I
>>> think it is.
>>>
>>>
>>>       
>>>> It is also strange that once a dd needed 300.4Mb and then 600.7Mb 
>>>> (?) as
>>>> also I made some room in removing ZZ?
>>>>         
>>> Approximately double size - many things the interpreter does involve
>>> making an additional copy of the data and then working with *that*.  
>>> This
>>> might be happening here, though I didn't read your code carefully enough
>>> to be able to be certain.
>>>
>>>
>>>       
>>>> which I don't really know if it took into account as the limit is
>>>> greater than the physical RAM of 4GB. ...?
>>>>         
>>> :)
>>>
>>>       
>>>> would it be easier using Linux ?
>>>>         
>>> possibly a little bit - on a linux machine you can at least run a PAE
>>> kernel (giving you a lot more address space to work with) and have the
>>> ability to turn on a bit more virtual memory.
>>>
>>> usually with data of the size you're trying to work with, i try to 
>>> find a
>>> way to preprocess the data a bit more before i apply R's tools to it.
>>> sometimes we stick it into a database (postgres) and select out the bits
>>> we want our inferences to be sourced from.  ;)
>>>
>>> it might be simplest to just hunt up a machine with 8 or 16GB of 
>>> memory in
>>> it, and run those bits of the analysis that really need memory on that
>>> machine...
>>>       
>> Yes, if there is no other way, a 64bit machine with lots of RAM would 
>> not be so contrained, but maybe this is a matter of first deciding why 
>> doing statistics on that much data is worth the effort? It may be, but 
>> just trying to read large amounts of data into memory is perhaps not 
>> justified in itself.
>>
>> Can you tile or subset the data, accumulating intermediate results? 
>> This is the approach the biglm package takes, and the R/GDAL interface 
>> also supports subsetting from an external file.
>>
>> Depending on the input format of the data, you should be able to do 
>> all you need provided that you do not try to keep all the data in 
>> memory. Using a database may be a good idea, or if the data are 
>> multiple remote sensing images, subsetting and accumulating results.
>>
>> Roger
>>
>>     
>>> --e
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at stat.math.ethz.ch
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>       
>
>
>



From Roger.Bivand at nhh.no  Thu Sep 13 12:22:59 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 13 Sep 2007 12:22:59 +0200 (CEST)
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <46E90759.5060301@nottingham.ac.uk>
References: <46E66E0F.3000704@nottingham.ac.uk>
	<Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>
	<Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no>
	<46E90759.5060301@nottingham.ac.uk>
Message-ID: <Pine.LNX.4.64.0709131214120.32739@reclus.nhh.no>

On Thu, 13 Sep 2007, Didier Leibovici wrote:

>
> Thanks Roger
>
> I feel we've got a low RAM machine which would need a bit of an uplift 
> (recent server though)!
> The linux machine is unfortunately also with 4Gb of RAM
> But  I persist to say it would be interesting to have within R a way of 
> automatically performing swapping memory if needed ...

On many OS, virtual memory will cut in at a certain point, but here the 
object is in any case too large to represent on a 32-bit system, and 
indeed R sets a limit on single object size of roughly 2Gb - for full 
details see:

?"Memory-limits"

However, the real question remains why you cannot save time by subsetting 
first, which is equivalent to swapping to virtual memory, but under your 
own control. There were some comments on this in my earlier reply, and 
other replies make similar suggestions. You still have not said why all 
the data must be in memory at the same time, and I strongly doubt that 
there are no viable or even superior alternatives.

Roger

>
> Didier
>
> Roger Bivand wrote:
>>  On Tue, 11 Sep 2007, elw at stderr.org wrote:
>> 
>> > 
>> > >  These days in GIS on may have to manipulate big datasets or arrays.
>> > > 
>> > >  Here I am on WINDOWS I have a 4Gb
>> > >  my aim was to have an array of dim 298249 12 10 22 but that's 2.9Gb
>> > 
>>
>>  Assuming double precision (no single precision in R), 5.8Gb.
>> 
>> > 
>> >  It used to be (maybe still is?) the case that a single process could 
>> >  only
>> >  'claim' a chunk of max size 2GB on Windows.
>> > 
>> > 
>> >  Also remember to compute overhead for R objects... 58 bytes per object, 
>> >  I
>> >  think it is.
>> > 
>> > 
>> > >  It is also strange that once a dd needed 300.4Mb and then 600.7Mb (?) 
>> > >  as
>> > >  also I made some room in removing ZZ?
>> > 
>> > 
>> >  Approximately double size - many things the interpreter does involve
>> >  making an additional copy of the data and then working with *that*. 
>> >  This
>> >  might be happening here, though I didn't read your code carefully enough
>> >  to be able to be certain.
>> > 
>> > 
>> > >  which I don't really know if it took into account as the limit is
>> > >  greater than the physical RAM of 4GB. ...?
>> > 
>> > :) 
>> > 
>> > >  would it be easier using Linux ?
>> > 
>> >  possibly a little bit - on a linux machine you can at least run a PAE
>> >  kernel (giving you a lot more address space to work with) and have the
>> >  ability to turn on a bit more virtual memory.
>> > 
>> >  usually with data of the size you're trying to work with, i try to find 
>> >  a
>> >  way to preprocess the data a bit more before i apply R's tools to it.
>> >  sometimes we stick it into a database (postgres) and select out the bits
>> >  we want our inferences to be sourced from.  ;)
>> > 
>> >  it might be simplest to just hunt up a machine with 8 or 16GB of memory 
>> >  in
>> >  it, and run those bits of the analysis that really need memory on that
>> >  machine...
>>
>>  Yes, if there is no other way, a 64bit machine with lots of RAM would not
>>  be so contrained, but maybe this is a matter of first deciding why doing
>>  statistics on that much data is worth the effort? It may be, but just
>>  trying to read large amounts of data into memory is perhaps not justified
>>  in itself.
>>
>>  Can you tile or subset the data, accumulating intermediate results? This
>>  is the approach the biglm package takes, and the R/GDAL interface also
>>  supports subsetting from an external file.
>>
>>  Depending on the input format of the data, you should be able to do all
>>  you need provided that you do not try to keep all the data in memory.
>>  Using a database may be a good idea, or if the data are multiple remote
>>  sensing images, subsetting and accumulating results.
>>
>>  Roger
>> 
>> > 
>> >  --e
>> > 
>>> _______________________________________________
>> >  R-sig-Geo mailing list
>> >  R-sig-Geo at stat.math.ethz.ch
>> >  https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> > 
>> 
>
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Agustin.Lobo at ija.csic.es  Thu Sep 13 13:27:41 2007
From: Agustin.Lobo at ija.csic.es (Agustin Lobo)
Date: Thu, 13 Sep 2007 13:27:41 +0200
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <46E90D99.4030100@geo.uu.nl>
References: <46E66E0F.3000704@nottingham.ac.uk>	<Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>	<Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no>	<46E90759.5060301@nottingham.ac.uk>
	<46E90D99.4030100@geo.uu.nl>
Message-ID: <46E91EAD.20209@ija.csic.es>

Any particular advice for setting up the kernel
(or other things) for such a machine (i.e., the PAE kernel)?

Agus

Edzer J. Pebesma escribi?:
> I think R will never do it's own memory swapping, as that is a typical 
> OS task. There are however several developments (provided in add-on 
> packages) that will not load all data in memory at start-up, but instead 
> call some data base whenever a data element is needed. You might search 
> r-help for rsqlite or biglm, and there are others; also look at the 
> award winners at useR this year.
> 
> Here, we've run pretty successful R sessions needing 10-11 Gb of memory 
> on a 8Gb RAM 64 bits linux machine with lots of swap space. Needs some 
> patience, and still R might crash other parts of the system when memory 
> usage becomes too excessive.
> 
> Best regards,
> --
> Edzer
> 
> Didier Leibovici wrote:
>> Thanks Roger
>>
>> I feel we've got a low RAM machine which would need a bit of an uplift 
>> (recent server though)!
>> The linux machine is unfortunately also with 4Gb of RAM
>> But  I persist to say it would be interesting to have within R a way of 
>> automatically performing swapping memory if needed ...
>>
>> Didier
>>
>> Roger Bivand wrote:
>>   
>>> On Tue, 11 Sep 2007, elw at stderr.org wrote:
>>>
>>>     
>>>>> These days in GIS on may have to manipulate big datasets or arrays.
>>>>>
>>>>> Here I am on WINDOWS I have a 4Gb
>>>>> my aim was to have an array of dim 298249 12 10 22 but that's 2.9Gb
>>>>>         
>>> Assuming double precision (no single precision in R), 5.8Gb.
>>>
>>>     
>>>> It used to be (maybe still is?) the case that a single process could 
>>>> only
>>>> 'claim' a chunk of max size 2GB on Windows.
>>>>
>>>>
>>>> Also remember to compute overhead for R objects... 58 bytes per 
>>>> object, I
>>>> think it is.
>>>>
>>>>
>>>>       
>>>>> It is also strange that once a dd needed 300.4Mb and then 600.7Mb 
>>>>> (?) as
>>>>> also I made some room in removing ZZ?
>>>>>         
>>>> Approximately double size - many things the interpreter does involve
>>>> making an additional copy of the data and then working with *that*.  
>>>> This
>>>> might be happening here, though I didn't read your code carefully enough
>>>> to be able to be certain.
>>>>
>>>>
>>>>       
>>>>> which I don't really know if it took into account as the limit is
>>>>> greater than the physical RAM of 4GB. ...?
>>>>>         
>>>> :)
>>>>
>>>>       
>>>>> would it be easier using Linux ?
>>>>>         
>>>> possibly a little bit - on a linux machine you can at least run a PAE
>>>> kernel (giving you a lot more address space to work with) and have the
>>>> ability to turn on a bit more virtual memory.
>>>>
>>>> usually with data of the size you're trying to work with, i try to 
>>>> find a
>>>> way to preprocess the data a bit more before i apply R's tools to it.
>>>> sometimes we stick it into a database (postgres) and select out the bits
>>>> we want our inferences to be sourced from.  ;)
>>>>
>>>> it might be simplest to just hunt up a machine with 8 or 16GB of 
>>>> memory in
>>>> it, and run those bits of the analysis that really need memory on that
>>>> machine...
>>>>       
>>> Yes, if there is no other way, a 64bit machine with lots of RAM would 
>>> not be so contrained, but maybe this is a matter of first deciding why 
>>> doing statistics on that much data is worth the effort? It may be, but 
>>> just trying to read large amounts of data into memory is perhaps not 
>>> justified in itself.
>>>
>>> Can you tile or subset the data, accumulating intermediate results? 
>>> This is the approach the biglm package takes, and the R/GDAL interface 
>>> also supports subsetting from an external file.
>>>
>>> Depending on the input format of the data, you should be able to do 
>>> all you need provided that you do not try to keep all the data in 
>>> memory. Using a database may be a good idea, or if the data are 
>>> multiple remote sensing images, subsetting and accumulating results.
>>>
>>> Roger
>>>
>>>     
>>>> --e
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at stat.math.ethz.ch
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>       
>>
>>
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Dr. Agustin Lobo
Institut de Ciencies de la Terra "Jaume Almera" (CSIC)
LLuis Sole Sabaris s/n
08028 Barcelona
Spain
Tel. 34 934095410
Fax. 34 934110012
email: Agustin.Lobo at ija.csic.es
http://www.ija.csic.es/gt/obster



From e.pebesma at geo.uu.nl  Thu Sep 13 14:05:28 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Thu, 13 Sep 2007 14:05:28 +0200
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <46E91EAD.20209@ija.csic.es>
References: <46E66E0F.3000704@nottingham.ac.uk>	<Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>	<Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no>	<46E90759.5060301@nottingham.ac.uk>
	<46E90D99.4030100@geo.uu.nl> <46E91EAD.20209@ija.csic.es>
Message-ID: <46E92788.5030002@geo.uu.nl>

Agus, not 100% sure but I believe the PAE kernel allows the OS to use
more than 4 Gb, but not more than 4 Gb for any single process (such as R
is) to be used. So, 2 R processes could each use max 4 Gb RAM without
swapping on an 8 Gb RAM machine with PAE kernel.

No, I installed a full 64-bit kernel, where the 4 Gb address space limit
is not present because pointers are 8 byte. It was as simple as anything
else; there's just some things that may not work (OpenOffice?) without a
large of effort. Since this was a server, it didn't matter.
--
Edzer

Agustin Lobo wrote:
> Any particular advice for setting up the kernel
> (or other things) for such a machine (i.e., the PAE kernel)?
>
> Agus
>
> Edzer J. Pebesma escribi?:
>> I think R will never do it's own memory swapping, as that is a 
>> typical OS task. There are however several developments (provided in 
>> add-on packages) that will not load all data in memory at start-up, 
>> but instead call some data base whenever a data element is needed. 
>> You might search r-help for rsqlite or biglm, and there are others; 
>> also look at the award winners at useR this year.
>>
>> Here, we've run pretty successful R sessions needing 10-11 Gb of 
>> memory on a 8Gb RAM 64 bits linux machine with lots of swap space. 
>> Needs some patience, and still R might crash other parts of the 
>> system when memory usage becomes too excessive.
>>
>> Best regards,
>> -- 
>> Edzer
>>
>> Didier Leibovici wrote:
>>> Thanks Roger
>>>
>>> I feel we've got a low RAM machine which would need a bit of an 
>>> uplift (recent server though)!
>>> The linux machine is unfortunately also with 4Gb of RAM
>>> But  I persist to say it would be interesting to have within R a way 
>>> of automatically performing swapping memory if needed ...
>>>
>>> Didier
>>>
>>> Roger Bivand wrote:
>>>  
>>>> On Tue, 11 Sep 2007, elw at stderr.org wrote:
>>>>
>>>>    
>>>>>> These days in GIS on may have to manipulate big datasets or arrays.
>>>>>>
>>>>>> Here I am on WINDOWS I have a 4Gb
>>>>>> my aim was to have an array of dim 298249 12 10 22 but that's 2.9Gb
>>>>>>         
>>>> Assuming double precision (no single precision in R), 5.8Gb.
>>>>
>>>>    
>>>>> It used to be (maybe still is?) the case that a single process 
>>>>> could only
>>>>> 'claim' a chunk of max size 2GB on Windows.
>>>>>
>>>>>
>>>>> Also remember to compute overhead for R objects... 58 bytes per 
>>>>> object, I
>>>>> think it is.
>>>>>
>>>>>
>>>>>      
>>>>>> It is also strange that once a dd needed 300.4Mb and then 600.7Mb 
>>>>>> (?) as
>>>>>> also I made some room in removing ZZ?
>>>>>>         
>>>>> Approximately double size - many things the interpreter does involve
>>>>> making an additional copy of the data and then working with 
>>>>> *that*.  This
>>>>> might be happening here, though I didn't read your code carefully 
>>>>> enough
>>>>> to be able to be certain.
>>>>>
>>>>>
>>>>>      
>>>>>> which I don't really know if it took into account as the limit is
>>>>>> greater than the physical RAM of 4GB. ...?
>>>>>>         
>>>>> :)
>>>>>
>>>>>      
>>>>>> would it be easier using Linux ?
>>>>>>         
>>>>> possibly a little bit - on a linux machine you can at least run a PAE
>>>>> kernel (giving you a lot more address space to work with) and have 
>>>>> the
>>>>> ability to turn on a bit more virtual memory.
>>>>>
>>>>> usually with data of the size you're trying to work with, i try to 
>>>>> find a
>>>>> way to preprocess the data a bit more before i apply R's tools to it.
>>>>> sometimes we stick it into a database (postgres) and select out 
>>>>> the bits
>>>>> we want our inferences to be sourced from.  ;)
>>>>>
>>>>> it might be simplest to just hunt up a machine with 8 or 16GB of 
>>>>> memory in
>>>>> it, and run those bits of the analysis that really need memory on 
>>>>> that
>>>>> machine...
>>>>>       
>>>> Yes, if there is no other way, a 64bit machine with lots of RAM 
>>>> would not be so contrained, but maybe this is a matter of first 
>>>> deciding why doing statistics on that much data is worth the 
>>>> effort? It may be, but just trying to read large amounts of data 
>>>> into memory is perhaps not justified in itself.
>>>>
>>>> Can you tile or subset the data, accumulating intermediate results? 
>>>> This is the approach the biglm package takes, and the R/GDAL 
>>>> interface also supports subsetting from an external file.
>>>>
>>>> Depending on the input format of the data, you should be able to do 
>>>> all you need provided that you do not try to keep all the data in 
>>>> memory. Using a database may be a good idea, or if the data are 
>>>> multiple remote sensing images, subsetting and accumulating results.
>>>>
>>>> Roger
>>>>
>>>>    
>>>>> --e
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at stat.math.ethz.ch
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>
>>>>>       
>>>
>>>
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>



From tomislav.hengl at jrc.it  Thu Sep 13 14:50:24 2007
From: tomislav.hengl at jrc.it (Tomislav Hengl)
Date: Thu, 13 Sep 2007 14:50:24 +0200
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <46E90D99.4030100@geo.uu.nl>
References: <46E66E0F.3000704@nottingham.ac.uk>	<Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>	<Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no><46E90759.5060301@nottingham.ac.uk>
	<46E90D99.4030100@geo.uu.nl>
Message-ID: <001301c7f604$a8c3ba10$1c10bf8b@H07.jrc.it>


Edzer,

This is also my biggest frustration with R at the moment. I can not load
grids that are bigger than 2M pixels and run geostatistics on them (if I run
the same analysis in a GIS software such as SAGA GIS, then I do not get this
problems). I often get messages such as:

"Reached total allocation of 1536Mb: see help(memory.size)"
"Error: cannot allocate vector of size 101.1Mb"

I still did not figure out how to avoid this obstacle.

FYI, I just came back from the geocomputation conference in Ireland where I
met some  Colleagues from the Centre for e-Science in Lancaster
(http://e-science.lancs.ac.uk). They are just about to release an R package
called MultiR that should be able to significantly speed up R calculations
by employing the grid computing facilities. Daniel Grose
(http://e-science.lancs.ac.uk/personnel.html#daniel) mentioned that they
plan to organize a workshop on MultiR in November 2007.

Grose, D. et al., 2006. sabreR: Grid-Enabling the Analysis of MultiProcess
Random Effect Response Data in R. In: P. Halfpenny (Editor), Second
International Conference on e-Social Science. National Centre for e-Social
Science, Manchester, UK, pp. 12.
http://epubs.cclrc.ac.uk/work-details?w=36493

Tom Hengl
http://spatial-analyst.net

-----Original Message-----
From: r-sig-geo-bounces at stat.math.ethz.ch
[mailto:r-sig-geo-bounces at stat.math.ethz.ch] On Behalf Of Edzer J. Pebesma
Sent: Thursday, September 13, 2007 12:15 PM
To: didier.leibovici at nottingham.ac.uk
Cc: r-sig-geo at stat.math.ethz.ch
Subject: Re: [R-sig-Geo] memory Usage setting

I think R will never do it's own memory swapping, as that is a typical OS
task. There are however several developments (provided in add-on
packages) that will not load all data in memory at start-up, but instead
call some data base whenever a data element is needed. You might search
r-help for rsqlite or biglm, and there are others; also look at the award
winners at useR this year.

Here, we've run pretty successful R sessions needing 10-11 Gb of memory on a
8Gb RAM 64 bits linux machine with lots of swap space. Needs some patience,
and still R might crash other parts of the system when memory usage becomes
too excessive.

Best regards,
--
Edzer

Didier Leibovici wrote:
> Thanks Roger
>
> I feel we've got a low RAM machine which would need a bit of an uplift 
> (recent server though)!
> The linux machine is unfortunately also with 4Gb of RAM But  I persist 
> to say it would be interesting to have within R a way of automatically 
> performing swapping memory if needed ...
>
> Didier
>
> Roger Bivand wrote:
>   
>> On Tue, 11 Sep 2007, elw at stderr.org wrote:
>>
>>     
>>>> These days in GIS on may have to manipulate big datasets or arrays.
>>>>
>>>> Here I am on WINDOWS I have a 4Gb
>>>> my aim was to have an array of dim 298249 12 10 22 but that's 2.9Gb
>>>>         
>> Assuming double precision (no single precision in R), 5.8Gb.
>>
>>     
>>> It used to be (maybe still is?) the case that a single process could 
>>> only 'claim' a chunk of max size 2GB on Windows.
>>>
>>>
>>> Also remember to compute overhead for R objects... 58 bytes per 
>>> object, I
>>> think it is.
>>>
>>>
>>>       
>>>> It is also strange that once a dd needed 300.4Mb and then 600.7Mb 
>>>> (?) as
>>>> also I made some room in removing ZZ?
>>>>         
>>> Approximately double size - many things the interpreter does involve
>>> making an additional copy of the data and then working with *that*.  
>>> This
>>> might be happening here, though I didn't read your code carefully enough
>>> to be able to be certain.
>>>
>>>
>>>       
>>>> which I don't really know if it took into account as the limit is
>>>> greater than the physical RAM of 4GB. ...?
>>>>         
>>> :)
>>>
>>>       
>>>> would it be easier using Linux ?
>>>>         
>>> possibly a little bit - on a linux machine you can at least run a PAE
>>> kernel (giving you a lot more address space to work with) and have the
>>> ability to turn on a bit more virtual memory.
>>>
>>> usually with data of the size you're trying to work with, i try to 
>>> find a
>>> way to preprocess the data a bit more before i apply R's tools to it.
>>> sometimes we stick it into a database (postgres) and select out the bits
>>> we want our inferences to be sourced from.  ;)
>>>
>>> it might be simplest to just hunt up a machine with 8 or 16GB of 
>>> memory in
>>> it, and run those bits of the analysis that really need memory on that
>>> machine...
>>>       
>> Yes, if there is no other way, a 64bit machine with lots of RAM would 
>> not be so contrained, but maybe this is a matter of first deciding why 
>> doing statistics on that much data is worth the effort? It may be, but 
>> just trying to read large amounts of data into memory is perhaps not 
>> justified in itself.
>>
>> Can you tile or subset the data, accumulating intermediate results? 
>> This is the approach the biglm package takes, and the R/GDAL interface 
>> also supports subsetting from an external file.
>>
>> Depending on the input format of the data, you should be able to do 
>> all you need provided that you do not try to keep all the data in 
>> memory. Using a database may be a good idea, or if the data are 
>> multiple remote sensing images, subsetting and accumulating results.
>>
>> Roger
>>
>>     
>>> --e
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at stat.math.ethz.ch
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>       
>
>
>

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-sig-geo



From didier.leibovici at nottingham.ac.uk  Thu Sep 13 16:40:23 2007
From: didier.leibovici at nottingham.ac.uk (Didier Leibovici)
Date: Thu, 13 Sep 2007 15:40:23 +0100
Subject: [R-sig-Geo] memory Usage setting
In-Reply-To: <Pine.LNX.4.64.0709131214120.32739@reclus.nhh.no>
References: <46E66E0F.3000704@nottingham.ac.uk>
	<Pine.LNX.4.64.0709110713320.31397@illuminati.stderr.org>
	<Pine.LNX.4.64.0709111636170.18811@reclus.nhh.no>
	<46E90759.5060301@nottingham.ac.uk>
	<Pine.LNX.4.64.0709131214120.32739@reclus.nhh.no>
Message-ID: <46E94BD7.2090303@nottingham.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070913/bf2edad7/attachment.pl>

From tkobayas at indiana.edu  Thu Sep 13 18:35:01 2007
From: tkobayas at indiana.edu (Takatsugu Kobayashi)
Date: Thu, 13 Sep 2007 12:35:01 -0400
Subject: [R-sig-Geo] Shortest network path distances
Message-ID: <46E966B5.7020107@indiana.edu>

Hi,

I would like to know if there are packages to compute shortest network 
path distances, something that can be down on TransCAD and ArcGIS 
network analyst. I have cencus tract centroids and network line 
shapefiles for 126 US urbanized areas. Basically, I would like to move 
the census tract centroids a bit to see how much it changes the shortest 
path distances.

If not possible, I guess I should program on ArcGIS using Python.

Thank you very much

Taka



From Roger.Bivand at nhh.no  Thu Sep 13 18:57:59 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 13 Sep 2007 18:57:59 +0200 (CEST)
Subject: [R-sig-Geo] Shortest network path distances
In-Reply-To: <46E966B5.7020107@indiana.edu>
References: <46E966B5.7020107@indiana.edu>
Message-ID: <Pine.LNX.4.64.0709131854560.5071@reclus.nhh.no>

On Thu, 13 Sep 2007, Takatsugu Kobayashi wrote:

> Hi,
>
> I would like to know if there are packages to compute shortest network
> path distances, something that can be down on TransCAD and ArcGIS
> network analyst. I have cencus tract centroids and network line
> shapefiles for 126 US urbanized areas. Basically, I would like to move
> the census tract centroids a bit to see how much it changes the shortest
> path distances.

There are a couple of pointers in this earlier thread:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/96654.html

to the e1071 package, and it seems as though isomap in the vegan package 
is another possibility. How well they scale is a different question, 
though. I would also think that there might be something on Bioconductor.

Roger

PS. Remember RSiteSearch() - it usually comes up with something!

>
> If not possible, I guess I should program on ArcGIS using Python.
>
> Thank you very much
>
> Taka
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From danbebber at yahoo.co.uk  Fri Sep 14 12:22:52 2007
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Fri, 14 Sep 2007 11:22:52 +0100
Subject: [R-sig-Geo] Shortest network path distances
In-Reply-To: <mailman.7.1189764002.11444.r-sig-geo@stat.math.ethz.ch>
References: <mailman.7.1189764002.11444.r-sig-geo@stat.math.ethz.ch>
Message-ID: <001e01c7f6b9$35497e20$6d00070a@earthwatchewe.lan>

Dear Taka,

Anything is possible on R!
I do all my shortest path calculations with RBGL and graph packages from
Bioconductor.
RBGL (based on the C++ Boost Graph Library) has several shortest path
algorithms that perform better on graphs of different types.
I usually use the johnson.all.pairs.sp() function
http://www.bioconductor.org/packages/release/GraphsAndNetworks.html


Yours,
Dan Bebber
_________________________________________
Dr Daniel P Bebber, MA, DPhil
 
Head of Climate Change Research
Earthwatch Institute
 
Junior Research Fellow in Biology
St. Peter's College, University of Oxford
 
Earthwatch Institute (Europe)
267 Banbury Road
Oxford OX2 7HT
UK
T. +44(0)1865 318842
F. +44(0)1865 311383
dbebber at earthwatch.org.uk
www.earthwatch.org


On Thu, 13 Sep 2007, Takatsugu Kobayashi wrote:

> Hi,
>
> I would like to know if there are packages to compute shortest network
> path distances, something that can be down on TransCAD and ArcGIS
> network analyst. I have cencus tract centroids and network line
> shapefiles for 126 US urbanized areas. Basically, I would like to move
> the census tract centroids a bit to see how much it changes the shortest
> path distances.



From v.gomezrubio at imperial.ac.uk  Fri Sep 14 13:13:43 2007
From: v.gomezrubio at imperial.ac.uk (Virgilio Gomez-Rubio)
Date: Fri, 14 Sep 2007 12:13:43 +0100
Subject: [R-sig-Geo] Shortest network path distances
In-Reply-To: <001e01c7f6b9$35497e20$6d00070a@earthwatchewe.lan>
References: <mailman.7.1189764002.11444.r-sig-geo@stat.math.ethz.ch>
	<001e01c7f6b9$35497e20$6d00070a@earthwatchewe.lan>
Message-ID: <1189768423.22226.16.camel@fh-vrubio>

Dear Dan,


> Anything is possible on R!
> I do all my shortest path calculations with RBGL and graph packages from
> Bioconductor.

I am working on something that requires finding the minimum set of nodes
to split a graph in n components and I am wondering whether there is
something on CRAN or Bioconductor to make that. Basically, I would like,
for example, get the minimum number of (adjacent) nodes that split the
graph in two components. Is it possible to do that with R?

Best regards,

Virgilio



From danbebber at yahoo.co.uk  Fri Sep 14 15:00:33 2007
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Fri, 14 Sep 2007 14:00:33 +0100
Subject: [R-sig-Geo] Shortest network path distances
In-Reply-To: <1189768423.22226.16.camel@fh-vrubio>
References: <mailman.7.1189764002.11444.r-sig-geo@stat.math.ethz.ch>
	<001e01c7f6b9$35497e20$6d00070a@earthwatchewe.lan>
	<1189768423.22226.16.camel@fh-vrubio>
Message-ID: <002201c7f6cf$3c6dcb00$6d00070a@earthwatchewe.lan>

Dear Virgilio,

Try the minCut() and edgeConnectivity() functions in RBGL.
You may need to adapt them to your problem.

Dan

-----Original Message-----
From: Virgilio Gomez-Rubio [mailto:v.gomezrubio at imperial.ac.uk] 
Sent: 14 September 2007 12:14
To: danbebber at forestecology.co.uk
Cc: R-sig-geo
Subject: Re: [R-sig-Geo] Shortest network path distances

Dear Dan,


> Anything is possible on R!
> I do all my shortest path calculations with RBGL and graph packages 
> from Bioconductor.

I am working on something that requires finding the minimum set of nodes to
split a graph in n components and I am wondering whether there is something
on CRAN or Bioconductor to make that. Basically, I would like, for example,
get the minimum number of (adjacent) nodes that split the graph in two
components. Is it possible to do that with R?

Best regards,

Virgilio



From danbebber at yahoo.co.uk  Fri Sep 14 17:26:37 2007
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Fri, 14 Sep 2007 16:26:37 +0100
Subject: [R-sig-Geo] Shortest network path distances
In-Reply-To: <46EAA48B.2090802@ija.csic.es>
References: <mailman.7.1189764002.11444.r-sig-geo@stat.math.ethz.ch>
	<001e01c7f6b9$35497e20$6d00070a@earthwatchewe.lan>
	<46EAA48B.2090802@ija.csic.es>
Message-ID: <000001c7f6e3$a41754b0$44754381@earthwatchewe.lan>

Dear Agus

Use the graphNEL class: details are in the graph library documentation
?ftM2graphNEL shows how to convert a (weighted) link list to this class.

Dan

-----Original Message-----
From: Agustin Lobo [mailto:aloboaleu at gmail.com] On Behalf Of Agustin Lobo
Sent: 14 September 2007 16:11
To: danbebber at forestecology.co.uk
Subject: Re: [R-sig-Geo] Shortest network path distances

Dan,

I've given a look. How can I actually import my data into a RGBL graph? What
I get out of my program is an R list of adjacencies. Each element in the
list is a node, and for each node I have a vector of adjacent nodes. The
examples in RGBL.pdf start from RGBL graphs in gxl format.

Thanks!

Agus



From knussear at usgs.gov  Fri Sep 14 20:48:58 2007
From: knussear at usgs.gov (Ken Nussear)
Date: Fri, 14 Sep 2007 11:48:58 -0700
Subject: [R-sig-Geo] help with glsm parameter estimation
Message-ID: <8F14E572-9ACC-4DA6-B055-BCB9CE850F32@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070914/0f229e89/attachment.pl>

From tkobayas at indiana.edu  Sun Sep 16 04:59:14 2007
From: tkobayas at indiana.edu (Takatsugu Kobayashi)
Date: Sat, 15 Sep 2007 22:59:14 -0400
Subject: [R-sig-Geo] Shortest network path distances
In-Reply-To: <001e01c7f6b9$35497e20$6d00070a@earthwatchewe.lan>
References: <mailman.7.1189764002.11444.r-sig-geo@stat.math.ethz.ch>
	<001e01c7f6b9$35497e20$6d00070a@earthwatchewe.lan>
Message-ID: <46EC9C02.2010507@indiana.edu>

Dear Dan,

Apologies for not responding quickly.

I will look into Bioconductor. I assume I can easily change the data 
attached to nodes and arcs. I have used R for 1 1/2 years and this 
computing language has grown on me more and more! I migrated from 
Matlab. Now a challenge is how to export network datasets readable into 
Bioconductor...

Thank you very much!!!

Taka

Dan Bebber wrote:
> Dear Taka,
>
> Anything is possible on R!
> I do all my shortest path calculations with RBGL and graph packages from
> Bioconductor.
> RBGL (based on the C++ Boost Graph Library) has several shortest path
> algorithms that perform better on graphs of different types.
> I usually use the johnson.all.pairs.sp() function
> http://www.bioconductor.org/packages/release/GraphsAndNetworks.html
>
>
> Yours,
> Dan Bebber
> _________________________________________
> Dr Daniel P Bebber, MA, DPhil
>  
> Head of Climate Change Research
> Earthwatch Institute
>  
> Junior Research Fellow in Biology
> St. Peter's College, University of Oxford
>  
> Earthwatch Institute (Europe)
> 267 Banbury Road
> Oxford OX2 7HT
> UK
> T. +44(0)1865 318842
> F. +44(0)1865 311383
> dbebber at earthwatch.org.uk
> www.earthwatch.org
>
>
> On Thu, 13 Sep 2007, Takatsugu Kobayashi wrote:
>
>   
>> Hi,
>>
>> I would like to know if there are packages to compute shortest network
>> path distances, something that can be down on TransCAD and ArcGIS
>> network analyst. I have cencus tract centroids and network line
>> shapefiles for 126 US urbanized areas. Basically, I would like to move
>> the census tract centroids a bit to see how much it changes the shortest
>> path distances.
>>     
>
>



From bardan_gh at yahoo.com  Sun Sep 16 08:52:24 2007
From: bardan_gh at yahoo.com (Bardan Ghimire)
Date: Sat, 15 Sep 2007 23:52:24 -0700 (PDT)
Subject: [R-sig-Geo] incorporating spatial dependence in non-spatial models
Message-ID: <405166.95449.qm@web38907.mail.mud.yahoo.com>

Hi,

I am using classification trees (non-spatial model)
and thinking of incorporating spatial terms,
variables, etc. I want to use both a geostatistical
and spatial econometric approach. Could anyone suggest
the best way to go about this?

Best wishes,
Bardan Ghimire
PhD Student
Graduate School of Geography
Clark University



From marcelino.delacruz at upm.es  Mon Sep 17 10:32:14 2007
From: marcelino.delacruz at upm.es (Marcelino de la Cruz)
Date: Mon, 17 Sep 2007 10:32:14 +0200
Subject: [R-sig-Geo] incorporating spatial dependence in non-spatial
 models
In-Reply-To: <405166.95449.qm@web38907.mail.mud.yahoo.com>
References: <405166.95449.qm@web38907.mail.mud.yahoo.com>
Message-ID: <6.1.2.0.1.20070917102211.0211b0f8@correo.upm.es>


McDonald RI, Urban DL
<http://sauwok.fecyt.es/apps/WoS/CIW.cgi?SID=R14HC at pCgFbn5eo8i2K&Func=Abstract&doc=2/10>Spatially 
varying rules of landscape change: lessons from a case study
LANDSCAPE AND URBAN PLANNING 74 (1): 7-20 JAN 1 2006



You probably would find this reference useful.

Marcelino



At 08:52 16/09/2007, Bardan Ghimire wrote:
>Hi,
>
>I am using classification trees (non-spatial model)
>and thinking of incorporating spatial terms,
>variables, etc. I want to use both a geostatistical
>and spatial econometric approach. Could anyone suggest
>the best way to go about this?
>
>Best wishes,
>Bardan Ghimire
>PhD Student
>Graduate School of Geography
>Clark University
>
>_______________________________________________
>R-sig-Geo mailing list
>R-sig-Geo at stat.math.ethz.ch
>https://stat.ethz.ch/mailman/listinfo/r-sig-geo

________________________________

Marcelino de la Cruz Rot

Departamento de  Biolog?a Vegetal
E.U.T.I. Agr?cola
Universidad Polit?cnica de Madrid
28040-Madrid
Tel.: 91 336 54 35
Fax: 91 336 56 56
marcelino.delacruz at upm.es



From Roger.Bivand at nhh.no  Mon Sep 17 11:02:15 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 17 Sep 2007 11:02:15 +0200 (CEST)
Subject: [R-sig-Geo] incorporating spatial dependence in non-spatial
 models
In-Reply-To: <6.1.2.0.1.20070917102211.0211b0f8@correo.upm.es>
References: <405166.95449.qm@web38907.mail.mud.yahoo.com>
	<6.1.2.0.1.20070917102211.0211b0f8@correo.upm.es>
Message-ID: <Pine.LNX.4.64.0709171101440.5105@reclus.nhh.no>

On Mon, 17 Sep 2007, Marcelino de la Cruz wrote:

>
> McDonald RI, Urban DL
> <http://sauwok.fecyt.es/apps/WoS/CIW.cgi?SID=R14HC at pCgFbn5eo8i2K&Func=Abstract&doc=2/10>Spatially
> varying rules of landscape change: lessons from a case study
> LANDSCAPE AND URBAN PLANNING 74 (1): 7-20 JAN 1 2006

This link gets to the original:

http://harvardforest.fas.harvard.edu/publications/pdfs/McDonald_LandscapeAndUrban_2006.pdf

Roger

>
>
>
> You probably would find this reference useful.
>
> Marcelino
>
>
>
> At 08:52 16/09/2007, Bardan Ghimire wrote:
>> Hi,
>>
>> I am using classification trees (non-spatial model)
>> and thinking of incorporating spatial terms,
>> variables, etc. I want to use both a geostatistical
>> and spatial econometric approach. Could anyone suggest
>> the best way to go about this?
>>
>> Best wishes,
>> Bardan Ghimire
>> PhD Student
>> Graduate School of Geography
>> Clark University
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> ________________________________
>
> Marcelino de la Cruz Rot
>
> Departamento de  Biolog?a Vegetal
> E.U.T.I. Agr?cola
> Universidad Polit?cnica de Madrid
> 28040-Madrid
> Tel.: 91 336 54 35
> Fax: 91 336 56 56
> marcelino.delacruz at upm.es
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From susana.costa at esri-portugal.pt  Mon Sep 17 18:12:33 2007
From: susana.costa at esri-portugal.pt (Susana Costa)
Date: Mon, 17 Sep 2007 17:12:33 +0100
Subject: [R-sig-Geo] Help - gdal_translate HDF5 problems
Message-ID: <FC3E2365FE096449AD07A1D5A17F6EAA0228B59A@Teris.esript.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070917/9e8d7148/attachment.pl>

From p.hiemstra at geo.uu.nl  Tue Sep 18 11:15:02 2007
From: p.hiemstra at geo.uu.nl (Paul Hiemstra)
Date: Tue, 18 Sep 2007 11:15:02 +0200
Subject: [R-sig-Geo] Help - gdal_translate HDF5 problems
In-Reply-To: <FC3E2365FE096449AD07A1D5A17F6EAA0228B59A@Teris.esript.local>
References: <FC3E2365FE096449AD07A1D5A17F6EAA0228B59A@Teris.esript.local>
Message-ID: <46EF9716.2050201@geo.uu.nl>

Dear Susana,

What OS are you using and what version of gdal are you using (gdalinfo 
--version)? Maybe upgrading to a newer version could solve your problem. 
And are you sure that the space before hydrodynamic should be there?

cheers,

Paul

Susana Costa schreef:
>  
>
> Hello,
>
>  
>
> I am trying to convert a file in HDF5 format to ArcInfo Grid format. But I am having a problem in using gdal_translate and gdalinfo in accessing the subdataset in the HDF5 file. After running the command on my HDF5 file ( gdalinfo Hydrodynamic_2.hdf5), the name of the subdataset I want to access is written out as 
>
>  
>
> SUBDATASET_57_NAME=HDF5:" Hydrodynamic_2.hdf5"://Results/water_level/water_level_00002
>
>  
>
> So I tried to access this dataset by typing HDF5:" Hydrodynamic_2.hdf5"://Results/water_level/water_level_00002 as in
>
>  
>
> gdalinfo  " Hydrodynamic_2.hdf5"://Results/water_level/water_level_00002
>
> gdal_translate  -of  AAIGrid  -sds HDF5:" Hydrodynamic_2.hdf5"://Results/water_level/water_level_00002 HDFTest.ascii
>
>  
>
> Unfortunately I get this error message for both commands and the program crashes
>
>  
>
> szFilenname Hydrodynamic_2.hdf5
>
>  
>
> Thank you very much 
>
>  
>
>  
>
> Susana Costa
>
> Consultora T?cnica S?nior
>
>  
>
> ESRI Portugal
>
> Tel: 912266080
>
> susana.costa at esri-portugal.pt <mailto:susana.costa at esri-portugal.pt> 
>
> www.esri-portugal.pt <http://www.esri-portugal.pt/> 
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>   


-- 
Drs. Paul Hiemstra
Department of Physical Geography
Faculty of Geosciences
University of Utrecht
Heidelberglaan 2
P.O. Box 80.115
3508 TC Utrecht
Phone: 	+31302535773
Fax:	+31302531145
http://intamap.geo.uu.nl/~paul



From MMSullivan at ric.edu  Tue Sep 18 19:30:34 2007
From: MMSullivan at ric.edu (Sullivan, Mary M)
Date: Tue, 18 Sep 2007 13:30:34 -0400
Subject: [R-sig-Geo] minimum distance for event locations
Message-ID: <38329C144184084CA2E572C51002D8FEDAA7C6@mailsvr1.RICOL.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070918/91ea67eb/attachment.pl>

From yud at mail.montclair.edu  Tue Sep 18 19:46:21 2007
From: yud at mail.montclair.edu (Danlin Yu)
Date: Tue, 18 Sep 2007 13:46:21 -0400
Subject: [R-sig-Geo] minimum distance for event locations
In-Reply-To: <38329C144184084CA2E572C51002D8FEDAA7C6@mailsvr1.RICOL.EDU>
References: <38329C144184084CA2E572C51002D8FEDAA7C6@mailsvr1.RICOL.EDU>
Message-ID: <46F00EED.1000901@mail.montclair.edu>

Mary:

I would suggest you to take a look at the coordinates of your locations 
- it might not be the minimum distance issue allowed in geoR or any 
other packages, but due to truncation of decimals, some of the very 
close locations might share the same coordinates hence give you such 
warnings.

Hope this helps.

Danlin

Sullivan, Mary M wrote:
> Hi,
>
>  
>
> I got a warning when I requested a posting in geoR, saying that there
> are duplicate locations.  There are some locations that are very
> close...can you tell me what the minimum distance allowed by geoR is for
> processes to run correctly?
>
>  
>
> Thanks.
>
> Mary Sullivan
>
>  
>
>  
>
> marymsullivan at comcast.net
>
>  
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>   

-- 
___________________________________________
Danlin Yu, Ph.D.
Assistant Professor
Department of Earth & Environmental Studies
Montclair State University
Montclair, NJ, 07043
Tel: 973-655-4313
Fax: 973-655-4072
email: yud at mail.montclair.edu
webpage: csam.montclair.edu/~yu



From Giovanna.Jonalasinio at uniroma1.it  Wed Sep 19 16:00:16 2007
From: Giovanna.Jonalasinio at uniroma1.it (Giovanna.Jonalasinio at uniroma1.it)
Date: Wed, 19 Sep 2007 16:00:16 +0200
Subject: [R-sig-Geo] =?iso-8859-1?q?Giovanna_Jonalasinio_=E8_fuori_ufficio?=
Message-ID: <OFF309626F.DC495B86-ONC125735B.004CEDCB-C125735B.004CEDCB@Uniroma1.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070919/73f9abed/attachment.pl>

From keith at statkingconsulting.com  Wed Sep 19 16:55:59 2007
From: keith at statkingconsulting.com (Keith Dunnigan)
Date: Wed, 19 Sep 2007 10:55:59 -0400
Subject: [R-sig-Geo] Publication for applied geo/environmental paper?
Message-ID: <221470FD0F9FD24ABE3A2F5569FF54250B3918@statserversbs.Statkingconsulting.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070919/fd50cd8b/attachment.pl>

From blopes at email.unc.edu  Wed Sep 19 22:03:10 2007
From: blopes at email.unc.edu (Brian J. Lopes)
Date: Wed, 19 Sep 2007 16:03:10 -0400
Subject: [R-sig-Geo] Gaussian Variogram Positive Definite?
In-Reply-To: <Pine.LNX.4.58.0708312019490.6716@dalmore.c3sl.ufpr.br>
References: <46D88595.40602@email.unc.edu>
	<Pine.LNX.4.58.0708312019490.6716@dalmore.c3sl.ufpr.br>
Message-ID: <46F1807E.4080100@email.unc.edu>

Paulo,

Sorry for the delay in response, but making the changes you recommended 
opened a whole bunch of other bugs in my C code.  Thank you very much 
for the advice, it indeed did help me to get my MLE to converge 
consistently by incorporating a small nugget.  I'm still having trouble 
getting consistent estimates with the Matern model, but that's beyond 
the scope of the email.

I should also thank Terry for the advice as well.

Thanks,
Brian

Paulo Justiniano Ribeiro Jr wrote:
> Brian
> 
> Gaussian variograms are known to generate numeric problems
> in case you have the nugget parameter equals to zero.
> This occours because the almost flat
> and with points very close to each other the covariance matrix will be
> nearly-singular -- numerically singular.
> 
> Soma alternatives are:
> 1. choose another covariance model:
>    for instance a Matern model with smoothness parameter  to ensure a
> behaviour which is similar to the gaussian (e.g. kappa = 4 in the
> parametrisation used in geoR
> 
> 2. add a small nugget to the model to make the covariance matrix
> diagonally dominant
> 
> hope this helps
> 
> best
> P.J.
> 
> 
> 
> 
> Paulo Justiniano Ribeiro Jr
> LEG (Laborat?rio de Estat?stica e Geoinforma??o)
> Universidade Federal do Paran?
> Caixa Postal 19.081
> CEP 81.531-990
> Curitiba, PR  -  Brasil
> Tel: (+55) 41 3361 3573
> Fax: (+55) 41 3361 3141
> e-mail: paulojus AT  ufpr  br
> http://www.leg.ufpr.br/~paulojus
> 
> On Fri, 31 Aug 2007, Brian J. Lopes wrote:
> 
>> Hello All:
>>
>> I've been banging my head against the wall about this for quite some
>> time now, and I can't seem to find any reference on the matter.  I'm
>> trying to calculate MLE estimates for the Gaussian variogram, but it
>> seems that I consistently reach the point where the covariance matrix is
>> not positive definite, as dictated by the Cholesky decomposition, even
>> though the range parameter is indeed positive (note that I am also
>> incorporating a nugget to sill ratio as well).  Has anybody else
>> experienced this problem?  Better yet, does anybody have any references
>> that discuss the situation, or how I can avoid it?
>>
>> The data is a bit large, so if an example is necessary I'll try to see
>> if I can come up with something reasonable.
>>
>> Thanks,
>> Brian
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>


-- 
Brian J. Lopes
PhD Student
Department of Statistics and Operations Research
University of North Carolina at Chapel Hill

To know that we know what we know, and that we do not know what we do
not know, that is true knowledge --Henry David Thoreau (quoting
Confucius): Walden



From p.hiemstra at geo.uu.nl  Thu Sep 20 09:57:03 2007
From: p.hiemstra at geo.uu.nl (Paul Hiemstra)
Date: Thu, 20 Sep 2007 09:57:03 +0200
Subject: [R-sig-Geo] Publication for applied geo/environmental paper?
In-Reply-To: <221470FD0F9FD24ABE3A2F5569FF54250B3918@statserversbs.Statkingconsulting.local>
References: <221470FD0F9FD24ABE3A2F5569FF54250B3918@statserversbs.Statkingconsulting.local>
Message-ID: <46F227CF.3060808@geo.uu.nl>

Hi Keith,

Maybe the elsevier journal Computers and Geosciences?

http://www.elsevier.com/wps/find/journaldescription.cws_home/398/description#description

cheers,

Paul

Keith Dunnigan schreef:
> Hi all,
>
>  
>
>   I just wondered if any of you might have a suggestion for a journal to
> submit a environmental/geological paper to.  Something sort of in the
> middle in terms of mathematics.  
>
>  
>
>   This paper does not present any new mathematical formulas or
> derivations.  It takes these results from a publication, and presents a
> tutorial of how to program them into software and then presents a couple
> of worked out examples.  So the journal would need to be comfortable
> enough with mathematics to handle a paper with a lot of linear algebra
> and linear algebraic programming, but not so technical as to want only
> newly derived formula's or statistical methods.
>
>  
>
>   If you have any suggestions, I would very much appreciate it.  Thanks
> so much and have a nice day.
>
>  
>
>   Keith Dunnigan
>
>   Statistician
>
>   Statking Consulting
>
>   Cincinnati Ohio
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>   


-- 
Drs. Paul Hiemstra
Department of Physical Geography
Faculty of Geosciences
University of Utrecht
Heidelberglaan 2
P.O. Box 80.115
3508 TC Utrecht
Phone: 	+31302535773
Fax:	+31302531145
http://intamap.geo.uu.nl/~paul



From zev at zevross.com  Fri Sep 21 22:27:25 2007
From: zev at zevross.com (Zev Ross)
Date: Fri, 21 Sep 2007 16:27:25 -0400
Subject: [R-sig-Geo] Monitoring network design optimization
Message-ID: <46F4292D.5040807@zevross.com>

Hi All,

Just wondering if anyone knows of any good R scripts/methods for 
monitoring network design optimization. I'm interested in methods that 
help calculate the "optimum" number of monitors in terms of cost vs 
coverage as well as methods that calculate the optimum geographic 
locations for a given number of monitors and a given underlying demand 
surface (say population at risk).

In ArcInfo (command line) there are p-median and maximum attendance 
algorithms, is there anything similar in R? Any recommendations?

Thank you!

Zev

-- 
Zev Ross
ZevRoss Spatial Analysis
303 Fairmount Ave
Ithaca, NY 14850
607-277-0004 (phone)
866-877-3690 (fax, toll-free)
zev at zevross.com



From e.pebesma at geo.uu.nl  Fri Sep 21 23:38:33 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Fri, 21 Sep 2007 23:38:33 +0200
Subject: [R-sig-Geo] Monitoring network design optimization
In-Reply-To: <46F4292D.5040807@zevross.com>
References: <46F4292D.5040807@zevross.com>
Message-ID: <46F439D9.4080903@geo.uu.nl>

Zev, package fields contains some optimization codes written by Eric 
Gilleland. I think it's mainly about optimizing an existent network.
--
Edzer

Zev Ross wrote:
> Hi All,
>
> Just wondering if anyone knows of any good R scripts/methods for 
> monitoring network design optimization. I'm interested in methods that 
> help calculate the "optimum" number of monitors in terms of cost vs 
> coverage as well as methods that calculate the optimum geographic 
> locations for a given number of monitors and a given underlying demand 
> surface (say population at risk).
>
> In ArcInfo (command line) there are p-median and maximum attendance 
> algorithms, is there anything similar in R? Any recommendations?
>
> Thank you!
>
> Zev
>
>



From epistat at gmail.com  Sun Sep 23 05:22:40 2007
From: epistat at gmail.com (zhijie zhang)
Date: Sun, 23 Sep 2007 11:22:40 +0800
Subject: [R-sig-Geo] How to change the cooridnates of multiple lines?
Message-ID: <2fc17e30709222022i594711d9u57a02fb3f0d72ab9@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070923/2e4d3b6a/attachment.pl>

From epistat at gmail.com  Sun Sep 23 13:49:55 2007
From: epistat at gmail.com (zhijie zhang)
Date: Sun, 23 Sep 2007 19:49:55 +0800
Subject: [R-sig-Geo] problem on converting the coordinates into interval [0,
	1]?
Message-ID: <2fc17e30709230449g4ef21536x6cb3a4d3d6193c79@mail.gmail.com>

Dear Roger,
  I have a problem on converting the coordinates into interval [0,1], hoping
that you can give me a hand.
  My data consist of points(cases), lines(rivers) and polygon (guichi city),
and i want to change their coordinates into interval [0,1].  I have put the
data in the attachment, so that you can use it.
The following is the programs to read the data and one possible method for
conversion:

library(sp)
library(foreign)
library(mgcv)
library(maptools)

#read the polygon containing the studied points
guichi <- readShapePoly("e:/guichi.shp")  #boundary polygons containing the
points
point_poly <-
getPolygonCoordsSlot(getPolygonsPolygonsSlot(getSpPpolygonsSlot(guichi)[[1]])[[1]])
#get the coordinates of guichi
#plot(point_poly,xlab="x", ylab="y",type="l")

#read the lines of two rivers in the guichi
rivers <- readShapeLines("e:/qiupu.shp")  #change the coordinates??difficult
#plot(rivers,add=T)

#read the points of cases and controls
case_control <- read.csv("e:/casecontrol.csv",sep=",", header=TRUE)
#plot(case_control$x,case_control$y)

*#Plot the whole figure*
plot(point_poly,xlab="x", ylab="y",type="l")
plot(rivers,add=T)
points(case_control$x,case_control$y,add=T)

###################################################################################
#one of the possible methods to convert the x/y coordinates into interval
[0,1]
*# But it seems there is a problem: it convert the x/y coordinates into
interval [0,1], respectively.
# In my opinion, they should be expand or  shrink according to the same
minimum/maximum value*.
st <- function(x)(x-min(x))/(max(x)-min(x))
case_control[,c(8,9)] <- data.frame(lapply(case_control[,c(6,7)],st))
###################################################################################
*Q1. The main problem is on the rivers, which has multiple lines. There are
difficulties for conversion.
Q2. when i convert the x/y coordinates, i'm not very sure whether i should
use the same scale of conversion or not. *
 Could you please help us to solve it?
 Any suggestions/help are greatly appreciated.


-- 
With Kind Regards,

oooO:::::::::
(..):::::::::
:\.(:::Oooo::
::\_)::(..)::
:::::::)./:::
::::::(_/::::
:::::::::::::
[***********************************************************************]
Zhi Jie,Zhang ,PHD
Tel:86-21-54237149
Dept. of Epidemiology,School of Public Health,Fudan University
Address:No. 138 Yi Xue Yuan Road,Shanghai,China
Postcode:200032
Email:epistat at gmail.com
Website: www.statABC.com
[***********************************************************************]
oooO:::::::::
(..):::::::::
:\.(:::Oooo::
::\_)::(..)::
:::::::)./:::
::::::(_/::::
:::::::::::::
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070923/3e55216d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: casecontrol.rar
Type: application/octet-stream
Size: 27619 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070923/3e55216d/attachment.obj>

From Roger.Bivand at nhh.no  Sun Sep 23 21:48:43 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 23 Sep 2007 21:48:43 +0200 (CEST)
Subject: [R-sig-Geo] problem on converting the coordinates into interval
	[0, 1]?
In-Reply-To: <2fc17e30709230449g4ef21536x6cb3a4d3d6193c79@mail.gmail.com>
References: <2fc17e30709230449g4ef21536x6cb3a4d3d6193c79@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0709232134110.18256@reclus.nhh.no>

Dear Zhijie Zhang,

On Sun, 23 Sep 2007, zhijie zhang wrote:

> Dear Roger,
>  I have a problem on converting the coordinates into interval [0,1], hoping
> that you can give me a hand.

I think that you need to explain why. If it is just because the 
coordinates "explode" because they are large, then simply scaling like 
this:

library(rgdal)
guichi <- readOGR(".", "guichi")
plot(guichi, axes=TRUE)
bbox(guichi)
proj4string(guichi)
t1 <- paste("+proj=tmerc +lat_0=0 +lon_0=117 +k=1.000000 +x_0=0",
   "+y_0=-3348000 +a=6378140 +b=6356755.288157528 +units=km")
t2 <- spTransform(guichi, CRS(t1))
plot(t2, axes=TRUE)
bbox(t2)
#          min      max
# r1 10.148541 79.88203
# r2  0.834946 61.90978

gets down to 70 by 80, which is not such a numerical risk (if need be, use 
+units=kmi to get nautical miles, which reduces the numbers even more). It 
has the key advantage that you can always get back to the true coordinates 
by transforming back.

In the internal code, you will see that many packages guard by using [-1, 
+1] on both dimensions. In your case and if you know that you need this, 
you would have to work out the scaling factor on the longer axis, so that 
points on the shorter axis would be scaled correctly. But if you want to 
keep the dimensions' aspect, you could just shift the origin and the units 
as I show above.

Hope this helps,

Roger

>  My data consist of points(cases), lines(rivers) and polygon (guichi city),
> and i want to change their coordinates into interval [0,1].  I have put the
> data in the attachment, so that you can use it.
> The following is the programs to read the data and one possible method for
> conversion:
>
> library(sp)
> library(foreign)
> library(mgcv)
> library(maptools)
>
> #read the polygon containing the studied points
> guichi <- readShapePoly("e:/guichi.shp")  #boundary polygons containing the
> points
> point_poly <-
> getPolygonCoordsSlot(getPolygonsPolygonsSlot(getSpPpolygonsSlot(guichi)[[1]])[[1]])
> #get the coordinates of guichi
> #plot(point_poly,xlab="x", ylab="y",type="l")
>
> #read the lines of two rivers in the guichi
> rivers <- readShapeLines("e:/qiupu.shp")  #change the coordinates??difficult
> #plot(rivers,add=T)
>
> #read the points of cases and controls
> case_control <- read.csv("e:/casecontrol.csv",sep=",", header=TRUE)
> #plot(case_control$x,case_control$y)
>
> *#Plot the whole figure*
> plot(point_poly,xlab="x", ylab="y",type="l")
> plot(rivers,add=T)
> points(case_control$x,case_control$y,add=T)
>
> ###################################################################################
> #one of the possible methods to convert the x/y coordinates into interval
> [0,1]
> *# But it seems there is a problem: it convert the x/y coordinates into
> interval [0,1], respectively.
> # In my opinion, they should be expand or  shrink according to the same
> minimum/maximum value*.
> st <- function(x)(x-min(x))/(max(x)-min(x))
> case_control[,c(8,9)] <- data.frame(lapply(case_control[,c(6,7)],st))
> ###################################################################################
> *Q1. The main problem is on the rivers, which has multiple lines. There are
> difficulties for conversion.
> Q2. when i convert the x/y coordinates, i'm not very sure whether i should
> use the same scale of conversion or not. *
> Could you please help us to solve it?
> Any suggestions/help are greatly appreciated.
>
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From reeves at nceas.ucsb.edu  Mon Sep 24 01:33:31 2007
From: reeves at nceas.ucsb.edu (reeves at nceas.ucsb.edu)
Date: Sun, 23 Sep 2007 17:33:31 -0600 (MDT)
Subject: [R-sig-Geo] using spplot to superimpose points and polygons on
 SpatialGridDataFrame
Message-ID: <1722.71.102.146.37.1190590411.squirrel@webmail.nceas.ucsb.edu>



Hello:

Thanks to the excellent example at:

http://casoilresource.lawr.ucdavis.edu/drupal/node/442

I am close to my goal of creating a map diagram in R
using raster and vector data layers.

I have set up a similar diagram using as inputs a satellite image
and point shapefile:

  cPts = readShapePoints("PSCentroidPointShape")
  ProjString = proj4string(MapPolysDataFrame)
  proj4string(cPts) = CRS(ProjString)
  psImg = readGDAL("PugetSoundSub1.img")
#
# psImg is a SpatialGridDataFrame
#
  image(PsImg) # just display the raster
#
# one way to superimpose vectors over rasters
#
  ctysPS = SpatialPolygons2PolySet(ctys)
  points(cPts,add=TRUE)
  addPolys(ctysPS)
#
# now, see if spplot works as indicated in this case by overlaying
# the points over the grid.
#
  points <- list("sp.points", cPts, pch = 4, col = "black", cex=0.5)
  greys= grey(0:256 / 256)
#
  spplot(psImg,zcol="band1",col.regions=greys,main="my
Plot",sp.layout=list(points))

OK, This generates a diagram, but the points are drawn FIRST, not
second, so they are not visible on top of the image.

This conflicts the Web site example, and the spplot help file:

The order of items in sp.layout matters; objects are drawn in the order
they appear. Plot order and prevalence of sp.layout items: for points and
lines,
sp.layout items are drawn before the points (to allow for grids and
polygons);

** for grids and polygons sp.layout is drawn afterwards (so the item will
not be overdrawn by the grid and/or polygon). **

 Although a matter of taste, transparency may help when combining things.

The only difference that I can see between my attempt and the web site
example is that the example uses a SpatialPixelsDataFrame as a base layer.

Can anyone offer insight?


And a side question: Is there a way to eliminate the grey level scale on
the right side of the spplot-generated diagram?

Version of R used: R version 2.5.0 (2007-04-23) (and also version 2.5.1)

I can provide the data files if necessary.

Thanks, Rick R



From epistat at gmail.com  Mon Sep 24 03:23:27 2007
From: epistat at gmail.com (zhijie zhang)
Date: Mon, 24 Sep 2007 09:23:27 +0800
Subject: [R-sig-Geo] problem on converting the coordinates into interval
	[0, 1]?
In-Reply-To: <Pine.LNX.4.64.0709232134110.18256@reclus.nhh.no>
References: <2fc17e30709230449g4ef21536x6cb3a4d3d6193c79@mail.gmail.com>
	<Pine.LNX.4.64.0709232134110.18256@reclus.nhh.no>
Message-ID: <2fc17e30709231823l63c1c44xad41df9fc8e162a6@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070924/05ddd8c7/attachment.pl>

From Roger.Bivand at nhh.no  Mon Sep 24 07:24:50 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 24 Sep 2007 07:24:50 +0200 (CEST)
Subject: [R-sig-Geo] problem on converting the coordinates into interval
	[0, 1]?
In-Reply-To: <2fc17e30709231823l63c1c44xad41df9fc8e162a6@mail.gmail.com>
References: <2fc17e30709230449g4ef21536x6cb3a4d3d6193c79@mail.gmail.com> 
	<Pine.LNX.4.64.0709232134110.18256@reclus.nhh.no>
	<2fc17e30709231823l63c1c44xad41df9fc8e162a6@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0709240707490.19983@reclus.nhh.no>

On Mon, 24 Sep 2007, zhijie zhang wrote:

> Dear Roger,
>  The reason for doing this is to get more general programs, which can be
> used in the other similar conditions. If we can convert all the related
> objects into internal [0,1], we can more easily use the intermediate results
> calculated from unit square,such as .bandwidth in kernels.

Exactly - with the original data, the units *are* general, because they 
are in meters. Provided the other data is also using the same metric, this
also permits a direct interpretation of bandwidth, possibly with a 
physical or biological basis. In the same sense, you can also get a 
relative bandwidth from (bw / max(diff(bbox(<obj>)))), and multiply it out 
again for a different context.

>  I think the main problem maybe the lines object, that is qiupu.shp. The
> scaling factor on the longer axis is from the guichi polygon.
> #read the lines of two rivers in the guichi
> rivers <- readShapeLines("e:/qiupu.shp")  #change the coordinates??difficult
> #plot(rivers,add=T)
> I'm not very sure if they,especially qiupu lines, can be successfullyy
> done.

Just loops of lapply(), not worse:

lns <- slot(rivers, "lines")

new_lns <- lapply(lns, function(x) {
   Lns <- slot(x, "Lines")
   new_Lns <- lapply(Lns, function(y) {
     new_crds <- mess_up(slot(y, "coords"))
     Line(new_crds)
   })
   Lines(new_Lns, ID=slot(x, "ID")
})
new_rivers <- SpatialLines(new_lns)

for your mess_up() - note, untried. Also note that the projection will not 
be defined.

Roger

> Thanks very much.
>
> On 9/24/07, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>
>> Dear Zhijie Zhang,
>>
>> On Sun, 23 Sep 2007, zhijie zhang wrote:
>>
>>> Dear Roger,
>>>  I have a problem on converting the coordinates into interval [0,1],
>> hoping
>>> that you can give me a hand.
>>
>> I think that you need to explain why. If it is just because the
>> coordinates "explode" because they are large, then simply scaling like
>> this:
>>
>> library(rgdal)
>> guichi <- readOGR(".", "guichi")
>> plot(guichi, axes=TRUE)
>> bbox(guichi)
>> proj4string(guichi)
>> t1 <- paste("+proj=tmerc +lat_0=0 +lon_0=117 +k=1.000000 +x_0=0",
>>   "+y_0=-3348000 +a=6378140 +b=6356755.288157528 +units=km")
>> t2 <- spTransform(guichi, CRS(t1))
>> plot(t2, axes=TRUE)
>> bbox(t2)
>> #          min      max
>> # r1 10.148541 79.88203
>> # r2  0.834946 61.90978
>>
>> gets down to 70 by 80, which is not such a numerical risk (if need be, use
>> +units=kmi to get nautical miles, which reduces the numbers even more). It
>> has the key advantage that you can always get back to the true coordinates
>> by transforming back.
>>
>> In the internal code, you will see that many packages guard by using [-1,
>> +1] on both dimensions. In your case and if you know that you need this,
>> you would have to work out the scaling factor on the longer axis, so that
>> points on the shorter axis would be scaled correctly. But if you want to
>> keep the dimensions' aspect, you could just shift the origin and the units
>> as I show above.
>>
>> Hope this helps,
>>
>> Roger
>>
>>>  My data consist of points(cases), lines(rivers) and polygon (guichi
>> city),
>>> and i want to change their coordinates into interval [0,1].  I have put
>> the
>>> data in the attachment, so that you can use it.
>>> The following is the programs to read the data and one possible method
>> for
>>> conversion:
>>>
>>> library(sp)
>>> library(foreign)
>>> library(mgcv)
>>> library(maptools)
>>>
>>> #read the polygon containing the studied points
>>> guichi <- readShapePoly("e:/guichi.shp")  #boundary polygons containing
>> the
>>> points
>>> point_poly <-
>>>
>> getPolygonCoordsSlot(getPolygonsPolygonsSlot(getSpPpolygonsSlot(guichi)[[1]])[[1]])
>>> #get the coordinates of guichi
>>> #plot(point_poly,xlab="x", ylab="y",type="l")
>>>
>>> #read the lines of two rivers in the guichi
>>> rivers <- readShapeLines("e:/qiupu.shp")  #change the
>> coordinates??difficult
>>> #plot(rivers,add=T)
>>>
>>> #read the points of cases and controls
>>> case_control <- read.csv("e:/casecontrol.csv",sep=",", header=TRUE)
>>> #plot(case_control$x,case_control$y)
>>>
>>> *#Plot the whole figure*
>>> plot(point_poly,xlab="x", ylab="y",type="l")
>>> plot(rivers,add=T)
>>> points(case_control$x,case_control$y,add=T)
>>>
>>>
>> ###################################################################################
>>> #one of the possible methods to convert the x/y coordinates into
>> interval
>>> [0,1]
>>> *# But it seems there is a problem: it convert the x/y coordinates into
>>> interval [0,1], respectively.
>>> # In my opinion, they should be expand or  shrink according to the same
>>> minimum/maximum value*.
>>> st <- function(x)(x-min(x))/(max(x)-min(x))
>>> case_control[,c(8,9)] <- data.frame(lapply(case_control[,c(6,7)],st))
>>>
>> ###################################################################################
>>> *Q1. The main problem is on the rivers, which has multiple lines. There
>> are
>>> difficulties for conversion.
>>> Q2. when i convert the x/y coordinates, i'm not very sure whether i
>> should
>>> use the same scale of conversion or not. *
>>> Could you please help us to solve it?
>>> Any suggestions/help are greatly appreciated.
>>>
>>>
>>>
>>
>> --
>> Roger Bivand
>> Economic Geography Section, Department of Economics, Norwegian School of
>> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
>> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
>> e-mail: Roger.Bivand at nhh.no
>>
>
>
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Mon Sep 24 07:36:53 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 24 Sep 2007 07:36:53 +0200 (CEST)
Subject: [R-sig-Geo] using spplot to superimpose points and polygons on
 SpatialGridDataFrame
In-Reply-To: <1722.71.102.146.37.1190590411.squirrel@webmail.nceas.ucsb.edu>
References: <1722.71.102.146.37.1190590411.squirrel@webmail.nceas.ucsb.edu>
Message-ID: <Pine.LNX.4.64.0709240729570.19983@reclus.nhh.no>

On Sun, 23 Sep 2007, reeves at nceas.ucsb.edu wrote:

>
>
> Hello:
>
> Thanks to the excellent example at:
>
> http://casoilresource.lawr.ucdavis.edu/drupal/node/442
>
> I am close to my goal of creating a map diagram in R
> using raster and vector data layers.
>
> I have set up a similar diagram using as inputs a satellite image
> and point shapefile:
>
>  cPts = readShapePoints("PSCentroidPointShape")
>  ProjString = proj4string(MapPolysDataFrame)
>  proj4string(cPts) = CRS(ProjString)
>  psImg = readGDAL("PugetSoundSub1.img")
> #
> # psImg is a SpatialGridDataFrame
> #
>  image(PsImg) # just display the raster
> #
> # one way to superimpose vectors over rasters
> #
>  ctysPS = SpatialPolygons2PolySet(ctys)
>  points(cPts,add=TRUE)
>  addPolys(ctysPS)

Unless you need to go to PolySets, you could say plot(ctys, add=TRUE).

> #
> # now, see if spplot works as indicated in this case by overlaying
> # the points over the grid.
> #
>  points <- list("sp.points", cPts, pch = 4, col = "black", cex=0.5)
>  greys= grey(0:256 / 256)
> #
>  spplot(psImg,zcol="band1",col.regions=greys,main="my
> Plot",sp.layout=list(points))
>
> OK, This generates a diagram, but the points are drawn FIRST, not
> second, so they are not visible on top of the image.
>

With:

library(sp)
data(meuse)
data(meuse.grid)
coordinates(meuse) <- c("x", "y")
coordinates(meuse.grid) <- c("x", "y")
gridded(meuse.grid) <- TRUE
pts <- list("sp.points", meuse, pch = 4, col = "black", cex=0.5)
greys <- grey(0:256 / 256)
spplot(meuse.grid, "dist", col.regions=greys, sp.layout=list(pts))

though you need to match the # cuts to the col.regions:

spplot(meuse.grid, "dist", col.regions=greys, sp.layout=list(pts),
   cuts=length(greys)-1)

I cannot reproduce your problem. Could you try to demonstrate it with the 
provided data set, so that we can try to isolate the issue?

Roger


> This conflicts the Web site example, and the spplot help file:
>
> The order of items in sp.layout matters; objects are drawn in the order
> they appear. Plot order and prevalence of sp.layout items: for points and
> lines,
> sp.layout items are drawn before the points (to allow for grids and
> polygons);
>
> ** for grids and polygons sp.layout is drawn afterwards (so the item will
> not be overdrawn by the grid and/or polygon). **
>
> Although a matter of taste, transparency may help when combining things.
>
> The only difference that I can see between my attempt and the web site
> example is that the example uses a SpatialPixelsDataFrame as a base layer.
>
> Can anyone offer insight?
>
>
> And a side question: Is there a way to eliminate the grey level scale on
> the right side of the spplot-generated diagram?
>
> Version of R used: R version 2.5.0 (2007-04-23) (and also version 2.5.1)
>
> I can provide the data files if necessary.
>
> Thanks, Rick R
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From epistat at gmail.com  Tue Sep 25 08:36:09 2007
From: epistat at gmail.com (zhijie zhang)
Date: Tue, 25 Sep 2007 14:36:09 +0800
Subject: [R-sig-Geo] problem on converting the coordinates into interval
	[0, 1]?
In-Reply-To: <Pine.LNX.4.64.0709240707490.19983@reclus.nhh.no>
References: <2fc17e30709230449g4ef21536x6cb3a4d3d6193c79@mail.gmail.com>
	<Pine.LNX.4.64.0709232134110.18256@reclus.nhh.no>
	<2fc17e30709231823l63c1c44xad41df9fc8e162a6@mail.gmail.com>
	<Pine.LNX.4.64.0709240707490.19983@reclus.nhh.no>
Message-ID: <2fc17e30709242336u4f286094t9567bc66303b5e70@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070925/7f27d4c0/attachment.pl>

From hag05c at fsu.edu  Wed Sep 26 18:22:57 2007
From: hag05c at fsu.edu (Heather Gamper)
Date: Wed, 26 Sep 2007 12:22:57 -0400
Subject: [R-sig-Geo] point pattern analysis in spatstat
Message-ID: <dacadd417fc1.46fa4f21@fsu.edu>

I have  tree location data (x and y coordinates)  with a corresponding insect density for each tree (marks in spatstat) I would like to know if insect density is a function of the distance to edge of each sampling area.



From Eric.Archer at noaa.gov  Thu Sep 27 17:55:37 2007
From: Eric.Archer at noaa.gov (Eric Archer)
Date: Thu, 27 Sep 2007 08:55:37 -0700
Subject: [R-sig-Geo] LL from bearing and distance
Message-ID: <46FBD279.3040104@noaa.gov>

Is there a package that contains a function that will allow me to 
calculate the ending latitude and longitude given a starting point on 
the earth's surface and bearing and distance?  I have searched the 
archives but have been unable to find one and would like to double check 
before I try to roll my own.  Thanks for any pointers.

Cheers,
eric

-- 

Eric Archer, Ph.D.
NOAA-SWFSC
8604 La Jolla Shores Dr.
La Jolla, CA 92037
858-546-7121,7003(FAX)
eric.archer at noaa.gov
http://swfsc.noaa.gov/prd-etp.aspx

"Innocence about Science is the worst crime today."
   - Sir Charles Percy Snow


"Lighthouses are more helpful than churches."
   - Benjamin Franklin

   "...but I'll take a GPS over either one."
       - John C. "Craig" George 


-------------- next part --------------
A non-text attachment was scrubbed...
Name: eric.archer.vcf
Type: text/x-vcard
Size: 366 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070927/573fed19/attachment.vcf>

From Roger.Bivand at nhh.no  Thu Sep 27 19:40:22 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 27 Sep 2007 19:40:22 +0200 (CEST)
Subject: [R-sig-Geo] LL from bearing and distance
In-Reply-To: <46FBD279.3040104@noaa.gov>
References: <46FBD279.3040104@noaa.gov>
Message-ID: <Pine.LNX.4.64.0709271936160.7163@reclus.nhh.no>

On Thu, 27 Sep 2007, Eric Archer wrote:

> Is there a package that contains a function that will allow me to calculate 
> the ending latitude and longitude given a starting point on the earth's 
> surface and bearing and distance?  I have searched the archives but have been 
> unable to find one and would like to double check before I try to roll my 
> own.  Thanks for any pointers.

Nothing for this case - azimuth for two points is in gzAzimuth() in 
maptools, so that might be a way in. There is C code for the spDistsN1() 
function in the sp package, visible in the CVS repository at the r-spatial 
site at sourceforge, but neither of these meets your need directly. If you 
do "roll your own", please consider contributing to maptools.

Roger

>
> Cheers,
> eric
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From a.lam at geo.uu.nl  Thu Sep 27 23:10:21 2007
From: a.lam at geo.uu.nl (Arien Lam)
Date: Thu, 27 Sep 2007 23:10:21 +0200
Subject: [R-sig-Geo] LL from bearing and distance
In-Reply-To: <Pine.LNX.4.64.0709271936160.7163@reclus.nhh.no>
References: <46FBD279.3040104@noaa.gov>
	<Pine.LNX.4.64.0709271936160.7163@reclus.nhh.no>
Message-ID: <46FC1C3D.4060606@geo.uu.nl>

The following function does what Eric describes, if the earth's surface 
is close enough to a sphere.
Roger, I'll try to document it properly for inclusion in Maptools.

Cheers, Arien




# compute the place where you end up, if you travel
# a certain distance along a great circle,
# which is uniquely defined by a point (your starting point)
# and an angle with the meridian at that point (your direction).
# the travelvector is a dataframe with at least the columns
# magnitude and direction.
# n.b. earth radius is the "ellipsoidal quadratic mean radius"
# of the earth, in m.

vectordestination <- function(lonlatpoint, travelvector) {
      Rearth <- 6372795
      Dd <- travelvector$magnitude / Rearth
      Cc <- travelvector$direction

      if (class(lonlatpoint) == "SpatialPoints") {
          lata <- coordinates(lonlatpoint)[1,2] * (pi/180)
          lona <- coordinates(lonlatpoint)[1,1] * (pi/180)
      }
      else {
          lata <- lonlatpoint[2] * (pi/180)
          lona <- lonlatpoint[1] * (pi/180)
      }
      latb <- asin(cos(Cc) * cos(lata) * sin(Dd) + sin(lata) * cos(Dd))
      dlon <- atan2(cos(Dd) - sin(lata) * sin(latb), sin(Cc) * sin(Dd) * 
cos(lata))
      lonb <- lona - dlon + pi/2

      lonb[lonb >  pi] <- lonb[lonb >  pi] - 2 * pi
      lonb[lonb < -pi] <- lonb[lonb < -pi] + 2 * pi

      latb <- latb * (180 / pi)
      lonb <- lonb * (180 / pi)

      cbind(longitude = lonb, latitude = latb)
}


Roger Bivand wrote:
> On Thu, 27 Sep 2007, Eric Archer wrote:
> 
>> Is there a package that contains a function that will allow me to calculate 
>> the ending latitude and longitude given a starting point on the earth's 
>> surface and bearing and distance?  I have searched the archives but have been 
>> unable to find one and would like to double check before I try to roll my 
>> own.  Thanks for any pointers.
> 
> Nothing for this case - azimuth for two points is in gzAzimuth() in 
> maptools, so that might be a way in. There is C code for the spDistsN1() 
> function in the sp package, visible in the CVS repository at the r-spatial 
> site at sourceforge, but neither of these meets your need directly. If you 
> do "roll your own", please consider contributing to maptools.
> 
> Roger
> 
>> Cheers,
>> eric
>>
>>
>



From Eric.Archer at noaa.gov  Fri Sep 28 00:59:27 2007
From: Eric.Archer at noaa.gov (Eric Archer)
Date: Thu, 27 Sep 2007 15:59:27 -0700
Subject: [R-sig-Geo] LL from bearing and distance
In-Reply-To: <46FC1C3D.4060606@geo.uu.nl>
References: <46FBD279.3040104@noaa.gov>
	<Pine.LNX.4.64.0709271936160.7163@reclus.nhh.no>
	<46FC1C3D.4060606@geo.uu.nl>
Message-ID: <46FC35CF.2010305@noaa.gov>

All,

I have ported JavaScript code from a couple of websites to R for three 
functions that solve my problem.

'calc.destination.sphere'  assumes a perfect spheroid with a given radius
'calc.destination.ellipsoid' assumes an ellipsoid
'calc.destination.Vincenty' uses the Vincenty model which appears to 
have the most accuracy

I'm unfamiliar with the steps involved in including these functions into 
a package, but will be happy to do it if someone can point me in the 
right direction. 

Cheers,
eric.

---

as.radians <- function(degrees) degrees * pi / 180
as.degrees <- function(radians) radians * 180 / pi
as.bearing <- function(radians) (as.degrees(radians) + 360) %% 360

ellipsoid <- function(model = "WGS84") {
  switch(model,
    WGS84 = c(a = 6378137, b = 6356752.3142, f = 1 / 298.257223563),
    GRS80 = c(a = 6378137, b = 6356752.3141, f = 1 / 298.257222101),
    Airy = c(a = 6377563.396, b = 6356256.909, f = 1 / 299.3249646),
    International = c(a = 6378888, b = 6356911.946, f = 1 / 297),
    Clarke = c(a = 6378249.145, b = 6356514.86955, f = 1 / 293.465),
    GRS67 = c(a = 6378160, b = 6356774.719, f = 1 / 298.25),
    c(a = NA, b = NA, f = NA)
  )
}

calc.destination.sphere <- function(lat, lon, brng, dist, dist.units = 
"km", radius = 6371) {
  # lat, lon : lattitude and longitude in decimal degrees
  # brng : bearing from 0 to 360 degrees
  # dist : distance travelled
  # dist.units : units of distance "km" (kilometers), "nm" (nautical 
miles), "mi" (statute miles)
  # radius : radius of sphere in kilometers
  #
  # Code adapted from JavaScript by Chris Veness 
(scripts at movable-type.co.uk) at 
http://www.movable-type.co.uk/scripts/latlong.html#ellipsoid
  #   originally from Ed Williams' Aviation Formulary, 
http://williams.best.vwh.net/avform.htm#LL
  #
  #

  dist <- switch(dist.units,
    km = dist,
    nm = dist * 1.852,
    mi = dist * 1.609344
  )
  lat <- as.radians(lat)
  lon <- as.radians(lon)
  brng <- as.radians(brng)

  psi <- dist / radius
  lat2 <- asin(sin(lat) * cos(psi) +  cos(lat) * sin(psi) * cos(brng))
  lon2 <- lon + atan2(sin(brng) * sin(psi) * cos(lat), cos(psi) - 
sin(lat) * sin(lat2))
  if (is.nan(lat2) || is.nan(lon2)) return(c(lat = NA, lon = NA))
  c(lat = as.degrees(lat2), lon = as.degrees(lon2))
}

calc.destination.ellipsoid <- function(lat, lon, brng, dist, dist.units 
= "km", model = "WGS84") { 
  # lat, lon : lattitude and longitude in decimal degrees
  # brng : bearing from 0 to 360 degrees
  # dist : distance travelled
  # dist.units : units of distance "km" (kilometers), "nm" (nautical 
miles), "mi" (statute miles)
  # model : choice of ellipsoid model ("WGS84", "GRS80", "Airy", 
"International", "Clarke", "GRS67")
  #
  # Code adapted from JavaScript by Larry Bogan (larry at go.ednet.ns.ca) 
at http://www.go.ednet.ns.ca/~larry/bsc/jslatlng.html
  #
  #

  dist <- switch(dist.units,
    km = dist,
    nm = dist * 1.852,
    mi = dist * 1.609344
  )
  lat <- as.radians(lat)
  lon <- as.radians(lon)
  brng <- as.radians(brng)
  e <- 0.08181922
  ellips <- ellipsoid(model)

  radius <- (ellips["a"] / 1000) * (1 - e ^ 2) / ((1 - e ^ 2 * sin(lat) 
^ 2) ^ 1.5)
  psi <- dist / radius
  phi <- pi / 2 - lat
  arc.cos <- cos(psi) * cos(phi) + sin(psi) * sin(phi) * cos(brng)
  lat2 <- as.degrees((pi / 2) - acos(arc.cos))

  arc.sin <- sin(brng) * sin(psi) / sin(phi)
  lon2 <- as.degrees(lon + asin(arc.sin))
 
  c(lat = lat2, lon = lon2)
}


calc.destination.Vincenty <- function(lat, lon, brng, dist, dist.units = 
"km", model = "WGS84") {
  # lat, lon : lattitude and longitude in decimal degrees
  # brng : bearing from 0 to 360 degrees
  # dist : distance travelled
  # dist.units : units of distance "km" (kilometers), "nm" (nautical 
miles), "mi" (statute miles)
  # model : choice of ellipsoid model ("WGS84", "GRS80", "Airy", 
"International", "Clarke", "GRS67")
  #
  # Code adapted from JavaScript by Chris Veness 
(scripts at movable-type.co.uk) at 
http://www.movable-type.co.uk/scripts/latlong-vincenty-direct.html
  # Original reference (http://www.ngs.noaa.gov/PUBS_LIB/inverse.pdf):
  #   Vincenty, T. 1975.  Direct and inverse solutions of geodesics on 
the ellipsoid with application of nested equations.
  #      Survey Review 22(176):88-93
  #
  #

  dist <- switch(dist.units,
    km = dist,
    nm = dist * 1.852,
    mi = dist * 1.609344
  )
  dist <- dist * 1000
  lat <- as.radians(lat)
  lon <- as.radians(lon)
  brng <- as.radians(brng)
  ellips <- ellipsoid(model)

  sin.alpha1 <- sin(brng)
  cos.alpha1 <- cos(brng)
  tan.u1 <- (1 - ellips["f"]) * tan(lat)
  cos.u1 <- 1 / sqrt(1 + (tan.u1 ^ 2))
  sin.u1 <- tan.u1 * cos.u1
  sigma1 <- atan2(tan.u1, cos.alpha1)
  sin.alpha <- cos.u1 * sin.alpha1
  cos.sq.alpha <- 1 - (sin.alpha ^ 2)
  u.sq <- cos.sq.alpha * ((ellips["a"] ^ 2) - (ellips["b"] ^ 2)) / 
(ellips["b"] ^ 2)
  cap.A <- 1 + u.sq / 16384 * (4096 + u.sq * (-768 + u.sq * (320 - 175 * 
u.sq)))
  cap.B <- u.sq / 1024 * (256 + u.sq * (-128 + u.sq * (74 - 47 * u.sq)))
 
  sigma <- dist / (ellips["b"] * cap.A)
  sigma.p <- 2 * pi
  cos.2.sigma.m <- cos(2 * sigma1 + sigma)
  while(abs(sigma - sigma.p) > 1e-12) {
    cos.2.sigma.m <- cos(2 * sigma1 + sigma)
    sin.sigma <- sin(sigma)
    cos.sigma <- cos(sigma)
    delta.sigma <- cap.B * sin.sigma * (cos.2.sigma.m + cap.B / 4 * 
(cos.sigma *
       (-1 + 2 * cos.2.sigma.m ^ 2) - cap.B / 6 * cos.2.sigma.m *
       (-3 + 4 * sin.sigma ^ 2) * (-3 + 4 * cos.2.sigma.m ^ 2)))
    sigma.p <- sigma
    sigma <- dist / (ellips["a"] * cap.A) + delta.sigma
  }

  tmp <- sin.u1 * sin.sigma - cos.u1 * cos.sigma * cos.alpha1
  lat2 <- atan2(sin.u1 * cos.sigma + cos.u1 * sin.sigma * cos.alpha1,
    (1 - ellips["f"]) * sqrt(sin.alpha ^ 2 + tmp ^ 2))
  lambda <- atan2(sin.sigma * sin.alpha1, cos.u1 * cos.sigma - sin.u1 * 
sin.sigma * cos.alpha1)
  cap.C <- ellips["f"] / 16 * cos.sq.alpha * (4 + ellips["f"] * 
(ellips["f"] - 3 * cos.sq.alpha))
  cap.L <- lambda - (1 - cap.C) * ellips["f"] * sin.alpha *
    (sigma + cap.C * sin.sigma * (cos.2.sigma.m + cap.C * cos.sigma * 
(-1 + 2 * cos.2.sigma.m ^ 2)))
 
  c(lat = as.degrees(lat2), lon = as.degrees(lon + cap.L))
}


-- 

Eric Archer, Ph.D.
NOAA-SWFSC
8604 La Jolla Shores Dr.
La Jolla, CA 92037
858-546-7121,7003(FAX)
eric.archer at noaa.gov
http://swfsc.noaa.gov/prd-etp.aspx

"Innocence about Science is the worst crime today."
   - Sir Charles Percy Snow


"Lighthouses are more helpful than churches."
   - Benjamin Franklin

   "...but I'll take a GPS over either one."
       - John C. "Craig" George 


-------------- next part --------------
A non-text attachment was scrubbed...
Name: eric.archer.vcf
Type: text/x-vcard
Size: 366 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070927/bb6727e0/attachment.vcf>

From Roger.Bivand at nhh.no  Fri Sep 28 09:28:51 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 28 Sep 2007 09:28:51 +0200 (CEST)
Subject: [R-sig-Geo] problem on converting the coordinates into interval
	[0, 1]?
In-Reply-To: <2fc17e30709242336u4f286094t9567bc66303b5e70@mail.gmail.com>
References: <2fc17e30709230449g4ef21536x6cb3a4d3d6193c79@mail.gmail.com> 
	<Pine.LNX.4.64.0709232134110.18256@reclus.nhh.no> 
	<2fc17e30709231823l63c1c44xad41df9fc8e162a6@mail.gmail.com> 
	<Pine.LNX.4.64.0709240707490.19983@reclus.nhh.no>
	<2fc17e30709242336u4f286094t9567bc66303b5e70@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.0709280924190.9185@reclus.nhh.no>

Dear Zhijie Zhang,

I'm working on an approach to this. I was asked (also on Tuesday) whether 
it is possible to disguise map position to increase confidentiality for 
patient locations. This is a valid reason to elide spatial position, so 
I'll try to see what can be done, and it will include your case.

I will send a draft version for you to try when I have something that 
seems to work.

Best wishes,

Roger

On Tue, 25 Sep 2007, zhijie zhang wrote:

> Dear Porf. Roger,
>  There are still a little problem with function  mess_up().
> #Programs to read the rivers
> library(sp);library(foreign);library(maptools)
>
> rivers<- readShapeLines("e:/qiupu.shp")
>
> #loops
>
> lns <- slot(rivers, "lines")
>
> new_lns <- lapply(lns, function(x)
> {
>  Lns <- slot(x, "Lines")
>  new_Lns <- lapply(Lns, function(y)
>   {
>    new_crds <-* mess_up*(slot(y, "coords"))
>    Line(new_crds)
>   })
>  Lines(new_Lns, ID=slot(x, "ID"))
> })
> new_rivers <- SpatialLines(new_lns)
>
> 1. #for your mess_up() - note, untried.
> *#Erros: no function mess_up()*
>
> 2. #Also note that the projection will not be defined.
> Actually, i have projected in ARCGIS, so the projection will be unneccessary
> in R.
>
> Thanks.
>
>
>
>
>
> On 9/24/07, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>
>> On Mon, 24 Sep 2007, zhijie zhang wrote:
>>
>>> Dear Roger,
>>>  The reason for doing this is to get more general programs, which can be
>>> used in the other similar conditions. If we can convert all the related
>>> objects into internal [0,1], we can more easily use the intermediate
>> results
>>> calculated from unit square,such as .bandwidth in kernels.
>>
>> Exactly - with the original data, the units *are* general, because they
>> are in meters. Provided the other data is also using the same metric, this
>> also permits a direct interpretation of bandwidth, possibly with a
>> physical or biological basis. In the same sense, you can also get a
>> relative bandwidth from (bw / max(diff(bbox(<obj>)))), and multiply it out
>> again for a different context.
>>
>>>  I think the main problem maybe the lines object, that is qiupu.shp. The
>>> scaling factor on the longer axis is from the guichi polygon.
>>> #read the lines of two rivers in the guichi
>>> rivers <- readShapeLines("e:/qiupu.shp")  #change the
>> coordinates??difficult
>>> #plot(rivers,add=T)
>>> I'm not very sure if they,especially qiupu lines, can be successfullyy
>>> done.
>>
>> Just loops of lapply(), not worse:
>>
>> lns <- slot(rivers, "lines")
>>
>> new_lns <- lapply(lns, function(x) {
>>   Lns <- slot(x, "Lines")
>>   new_Lns <- lapply(Lns, function(y) {
>>     new_crds <- mess_up(slot(y, "coords"))
>>     Line(new_crds)
>>   })
>>   Lines(new_Lns, ID=slot(x, "ID")
>> })
>> new_rivers <- SpatialLines(new_lns)
>>
>> for your mess_up() - note, untried. Also note that the projection will not
>> be defined.
>>
>> Roger
>>
>>> Thanks very much.
>>>
>>> On 9/24/07, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>>>
>>>> Dear Zhijie Zhang,
>>>>
>>>> On Sun, 23 Sep 2007, zhijie zhang wrote:
>>>>
>>>>> Dear Roger,
>>>>>  I have a problem on converting the coordinates into interval [0,1],
>>>> hoping
>>>>> that you can give me a hand.
>>>>
>>>> I think that you need to explain why. If it is just because the
>>>> coordinates "explode" because they are large, then simply scaling like
>>>> this:
>>>>
>>>> library(rgdal)
>>>> guichi <- readOGR(".", "guichi")
>>>> plot(guichi, axes=TRUE)
>>>> bbox(guichi)
>>>> proj4string(guichi)
>>>> t1 <- paste("+proj=tmerc +lat_0=0 +lon_0=117 +k=1.000000 +x_0=0",
>>>>   "+y_0=-3348000 +a=6378140 +b=6356755.288157528 +units=km")
>>>> t2 <- spTransform(guichi, CRS(t1))
>>>> plot(t2, axes=TRUE)
>>>> bbox(t2)
>>>> #          min      max
>>>> # r1 10.148541 79.88203
>>>> # r2  0.834946 61.90978
>>>>
>>>> gets down to 70 by 80, which is not such a numerical risk (if need be,
>> use
>>>> +units=kmi to get nautical miles, which reduces the numbers even more).
>> It
>>>> has the key advantage that you can always get back to the true
>> coordinates
>>>> by transforming back.
>>>>
>>>> In the internal code, you will see that many packages guard by using
>> [-1,
>>>> +1] on both dimensions. In your case and if you know that you need
>> this,
>>>> you would have to work out the scaling factor on the longer axis, so
>> that
>>>> points on the shorter axis would be scaled correctly. But if you want
>> to
>>>> keep the dimensions' aspect, you could just shift the origin and the
>> units
>>>> as I show above.
>>>>
>>>> Hope this helps,
>>>>
>>>> Roger
>>>>
>>>>>  My data consist of points(cases), lines(rivers) and polygon (guichi
>>>> city),
>>>>> and i want to change their coordinates into interval [0,1].  I have
>> put
>>>> the
>>>>> data in the attachment, so that you can use it.
>>>>> The following is the programs to read the data and one possible method
>>>> for
>>>>> conversion:
>>>>>
>>>>> library(sp)
>>>>> library(foreign)
>>>>> library(mgcv)
>>>>> library(maptools)
>>>>>
>>>>> #read the polygon containing the studied points
>>>>> guichi <- readShapePoly("e:/guichi.shp")  #boundary polygons
>> containing
>>>> the
>>>>> points
>>>>> point_poly <-
>>>>>
>>>>
>> getPolygonCoordsSlot(getPolygonsPolygonsSlot(getSpPpolygonsSlot(guichi)[[1]])[[1]])
>>>>> #get the coordinates of guichi
>>>>> #plot(point_poly,xlab="x", ylab="y",type="l")
>>>>>
>>>>> #read the lines of two rivers in the guichi
>>>>> rivers <- readShapeLines("e:/qiupu.shp")  #change the
>>>> coordinates??difficult
>>>>> #plot(rivers,add=T)
>>>>>
>>>>> #read the points of cases and controls
>>>>> case_control <- read.csv("e:/casecontrol.csv",sep=",", header=TRUE)
>>>>> #plot(case_control$x,case_control$y)
>>>>>
>>>>> *#Plot the whole figure*
>>>>> plot(point_poly,xlab="x", ylab="y",type="l")
>>>>> plot(rivers,add=T)
>>>>> points(case_control$x,case_control$y,add=T)
>>>>>
>>>>>
>>>>
>> ###################################################################################
>>>>> #one of the possible methods to convert the x/y coordinates into
>>>> interval
>>>>> [0,1]
>>>>> *# But it seems there is a problem: it convert the x/y coordinates
>> into
>>>>> interval [0,1], respectively.
>>>>> # In my opinion, they should be expand or  shrink according to the
>> same
>>>>> minimum/maximum value*.
>>>>> st <- function(x)(x-min(x))/(max(x)-min(x))
>>>>> case_control[,c(8,9)] <- data.frame(lapply(case_control[,c(6,7)],st))
>>>>>
>>>>
>> ###################################################################################
>>>>> *Q1. The main problem is on the rivers, which has multiple lines.
>> There
>>>> are
>>>>> difficulties for conversion.
>>>>> Q2. when i convert the x/y coordinates, i'm not very sure whether i
>>>> should
>>>>> use the same scale of conversion or not. *
>>>>> Could you please help us to solve it?
>>>>> Any suggestions/help are greatly appreciated.
>>>>>
>>>>>
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Economic Geography Section, Department of Economics, Norwegian School
>> of
>>>> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
>>>> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
>>>> e-mail: Roger.Bivand at nhh.no
>>>>
>>>
>>>
>>>
>>>
>>
>> --
>> Roger Bivand
>> Economic Geography Section, Department of Economics, Norwegian School of
>> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
>> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
>> e-mail: Roger.Bivand at nhh.no
>>
>
>
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From reeves at nceas.ucsb.edu  Sat Sep 29 22:28:43 2007
From: reeves at nceas.ucsb.edu (reeves at nceas.ucsb.edu)
Date: Sat, 29 Sep 2007 13:28:43 -0700 (PDT)
Subject: [R-sig-Geo] Plotting point labels along with points using spplot
	sp.layout list
Message-ID: <1458.71.102.146.37.1191097723.squirrel@webmail.nceas.ucsb.edu>


Hello:

Im working on a method to create multilayer raster/vector plots using
spplot() and supporting functions. The question is: how do I efficiently
add display list instructions for plotting the point labels that works
for different sized point vectors?

Here is a working 'brute force' approach (I have omitted the string parsing
code that produces the point labels) that creates the plot and labels the
first two points only:

library(sp)
library(rgdal)
library(maptools)
#
# read files
#
Counties  <- readShapePoly("PugetSoundCountiesClp2.shp")
psImg     <- readGDAL("PugetSoundSub1.img")
Centroids <- readShapePoints("PSCentroidPointShape")

len = length(Centroids at coords)
LatLongs = Centroids at coords
dim(LatLongs) = c(len/2,2)
#
# spplot 'sp.layout' list entries (demote polygons to spatial lines).
#
points <- list("sp.points", Centroids,  pch = 21,col="green")
Counties_lines <- as(Counties, "SpatialLines")
polys <- list("sp.lines", Counties_lines, col="white")
greys <- grey(0:256 / 256)
#
# Assume: ss[] is a char vector of point labels, one per Centroid element..
# display list entries for first two points
#
pt1 = list("sp.text",c(LatLongs[1,1],LatLongs[1,2]),ss[1],col="white",pos=2)
pt2 = list("sp.text",c(LatLongs[2,1],LatLongs[2,2]),ss[2],col="white",pos=2)
#
# I could create these for all 'n' list elements I suppose, but then the
code is not general.
#
grob2 = spplot(psImg, "band1", col.regions=greys,  
sp.layout=list(points,pl1,pl2,polys),cuts=length(greys),
colorkey=FALSE,scales=list(draw=TRUE))
#
plot(grob2)

Something tells me Im doing this the hard way...
I could construct the arguments to list() dynamically with string building
code, and then call() or eval() the command, but isnt there a much simpler
way to 1) tell sp.layout to add the point labels or 2) add the labels to
the
plot in a separate command? I have tried separate text() and
update(trellis.last.object()) commands to no avail.

Suggestions?

Regards, Rick Reeves / NCEAS



From epistat at gmail.com  Sun Sep 30 17:15:31 2007
From: epistat at gmail.com (zhijie zhang)
Date: Sun, 30 Sep 2007 23:15:31 +0800
Subject: [R-sig-Geo] problem on converting the coordinates into interval
	[0, 1]?
In-Reply-To: <Pine.LNX.4.64.0709280924190.9185@reclus.nhh.no>
References: <2fc17e30709230449g4ef21536x6cb3a4d3d6193c79@mail.gmail.com>
	<Pine.LNX.4.64.0709232134110.18256@reclus.nhh.no>
	<2fc17e30709231823l63c1c44xad41df9fc8e162a6@mail.gmail.com>
	<Pine.LNX.4.64.0709240707490.19983@reclus.nhh.no>
	<2fc17e30709242336u4f286094t9567bc66303b5e70@mail.gmail.com>
	<Pine.LNX.4.64.0709280924190.9185@reclus.nhh.no>
Message-ID: <2fc17e30709300815r30ce1101lb2485ad499f51fda@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070930/0afa968c/attachment.pl>

From SKalenderski at eos.ubc.ca  Sun Sep 30 19:54:42 2007
From: SKalenderski at eos.ubc.ca (Stoitchko Kalenderski)
Date: Sun, 30 Sep 2007 10:54:42 -0700
Subject: [R-sig-Geo] SpatialPoints problem
Message-ID: <79ED20FDBEF84F5FA7C404ED51C67AFF@eos.ubc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070930/e4eeed8b/attachment.pl>

From johan.vandewauw at gmail.com  Sun Sep 30 20:38:34 2007
From: johan.vandewauw at gmail.com (Johan Van de Wauw)
Date: Sun, 30 Sep 2007 20:38:34 +0200
Subject: [R-sig-Geo] SpatialPoints problem
In-Reply-To: <79ED20FDBEF84F5FA7C404ED51C67AFF@eos.ubc.ca>
References: <79ED20FDBEF84F5FA7C404ED51C67AFF@eos.ubc.ca>
Message-ID: <791a12030709301138x89a4eabt55893c22386fcaa7@mail.gmail.com>

On 9/30/07, Stoitchko Kalenderski <SKalenderski at eos.ubc.ca> wrote:
> Hi All,
> I am trying to create a grid but something getting wrong
> here is the code
>
> rm(list=ls())
> library(maptools)
> library(rgdal)
>
> options("digits"=22)
> lat=seq(from= -123.4, to= -121.4, length=20)
> long=seq(from=48.95, to=49.5, length=10)

latitude cannot be <-90, that's why setting the projection string will
fail. I guess you mixed up latitude and longitude!

> #generates the grid
> grd = expand.grid(lat,long)
> sp_grd <- SpatialPoints(cbind(grd[,2],grd[,1]), proj4string=CRS("+proj=longlat +datum=WGS84"))
> #do projection
>
> spTransform(sp_grd,CRS("+proj=merc +datum=WGS84"))
>
>
>
> any hint is appreciated
>
> thx
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



