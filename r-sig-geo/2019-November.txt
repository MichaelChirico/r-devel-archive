From @dr|@n@b@dde|ey @end|ng |rom curt|n@edu@@u  Fri Nov  1 06:16:12 2019
From: @dr|@n@b@dde|ey @end|ng |rom curt|n@edu@@u (Adrian Baddeley)
Date: Fri, 1 Nov 2019 05:16:12 +0000
Subject: [R-sig-Geo] Error in random labelling using the J function
In-Reply-To: <mailman.27727.3.1572519601.50671.r-sig-geo@r-project.org>
References: <mailman.27727.3.1572519601.50671.r-sig-geo@r-project.org>
Message-ID: <ME2PR01MB2244356F1980AEC452158123A4620@ME2PR01MB2244.ausprd01.prod.outlook.com>

<mikel.diaz.rodriguez at hotmail.com> writes:

  > I'm having problems when I run the random labelling using J Function.
> [...]
> library(spatstat)
> [...]
> is.multitype(pm)
> plot(pm)

> Jdif <- function(X, ..., i) {
  >            Jidot <- Jdot(X, ..., i = i)
>              J <- Jest(X, ...)
  >            dif <- eval.fv(Jidot - J)
  >           return(dif)
> }
  > E <- envelope(pm, Jdif, nsim = 39, i = "A", j = "B", simulate = expression(rlabel(pm)))
>  But when I execute E, it shows me the following error:

  > Error: in Fest(X, r) the successive r values must be finely spaced: given spacing = 0.010203; required spacing <=  0.00592

This is a bug in Jdot(). It is not related to the use of random labelling or envelopes.

The bug is fixed in the current development version of spatstat, 1.61-0.042,
which you can install from <https://github.com/spatstat/spatstat>

Note that in the envelope() command above, you don't need the argument 'j="B" and it is ignored.

regards


Prof Adrian Baddeley DSc FAA

John Curtin Distinguished Professor

Department of Mathematics and Statistics

Curtin University, Perth, Western Australia

________________________________


	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Fri Nov  1 11:00:58 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Fri, 1 Nov 2019 11:00:58 +0100
Subject: [R-sig-Geo] About dnearneight and nb2listw function
In-Reply-To: <SC1PR80MB374301E9C0061DEADC7FAEC5A9630@SC1PR80MB3743.lamprd80.prod.outlook.com>
References: <SC1PR80MB374301E9C0061DEADC7FAEC5A9630@SC1PR80MB3743.lamprd80.prod.outlook.com>
Message-ID: <alpine.LFD.2.21.1911011055480.116960@reclus.nhh.no>

On Thu, 31 Oct 2019, Let?cia Dal' Canton wrote:

> In my Ph.D. I am comparing Lee and Anselin's methodology for estimating 
> bivariate spatial autocorrelation. Lee's methodology led me to the spdep 
> package. My questions are as follows:
>
> 1) The weight matrix W, which is part of the "nb2listw" function. First,
>    is it possible to view all elements of this matrix? Because I could
>    only visualize the elements whose autocorrelation is not null. That
>    would be enough, but I need to present this matrix and visualize the
>    autocorrelation between all points. That is, knowing what are the
>    coordinates of the points where the autocorrelation is nonzero and
>    also where it is null.

This is very unclear. Yes, nb2mat() gives a dense matrix, but you do not 
show how you "visualize the elements whose autocorrelation is not null". 
You need to add a reproducible example using a built-in data set. Note 
that the matrix showing the variance-covariance of the observations is 
dense by definition (I - \rho W)^{-1} - see Melanie Wall's 2004 article.

>
> 2) In the "dnearneight" function, is it possible to view the matrix of
>    neighbors? Because getting this matrix, would be able to standardize
>    and get the matrix of weights W without using the nb2list function.

You do not explain why this makes sense. Of course you can use nb2mat(), 
listw2mat() or coerce a listw object to a sparse matrix, but you need to 
motivate this (best with an example).

Roger

>
> Thankfully,
>
> Let???cia Dal' Canton.
>
> Matem???tica.
> Mestre em Engenharia Agr???cola.
> Doutoranda em Engenharia Agr???cola com linha de pesquisa em Estat???stica Espacial.
> Universidade Estadual do Oeste do Paran??? - UNIOESTE.
>
> Curr???culo Lattes: http://lattes.cnpq.br/1085422685501012.
> Contato: (45) 9 9962-7492.
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From d@run@b@@ @end|ng |rom gm@||@com  Fri Nov  1 11:35:00 2019
From: d@run@b@@ @end|ng |rom gm@||@com (Barnabas Daru)
Date: Fri, 1 Nov 2019 05:35:00 -0500
Subject: [R-sig-Geo] =?utf-8?q?Ph=2ED=2E_Assistantship_=E2=80=93_Marine_B?=
 =?utf-8?q?iology_=28Botany=29?=
Message-ID: <DD1B3760-BF33-4AB8-9B8E-C215F87848D5@gmail.com>

Position/title ? Ph.D. Assistantship ? Marine Biology (Botany)

Agency/Location ? Texas A&M University at Corpus Christi, TX

Responsibilities ? A PhD assistantship position is available under the advisement of Dr. Barnabas Daru in the direction of Marine Biology. The successful applicant will be working on projects in the Marine Biology program and has a chance to study the use of herbarium specimens as sources of big data for understanding the distributions and diversity of marine plant microbiomes. The project includes the use of pressed herbarium specimens of marine plants (seagrasses and mangroves) as sources of big data by analyzing the diversity of microbiomes, with the aim of understanding how climate change and urbanization have affected the microbiomes of marine plant species along coasts and estuaries of North America spanning the past 120 years. Student will obtain Ph.D. degree through the Marine Biology Program, an interdisciplinary degree program combining the strengths of three universities within the Texas A&M University (TAMU) System; TAMU-Corpus Christi, TAMU-Galveston, and TAMU-College Station.

Qualifications ? 
(1) B.S. or M.S. in biological sciences, botany, environmental science, marine science, microbiology or related field.
(2) Basic knowledge in plant biology, molecular biology, and microbiology. Experience/knowledge with herbarium specimens, DNA extraction and bioinformatics is a plus.
(3) GPA >=3.0.
(4) >1100 (or >310 in new scoring system) on the verbal and quantitative sections and                 >3.5 in analytical writing of the GRE.

Closing Date ? Applications will be considered until candidate is selected

Contact ? Send cover letter, resume, unofficial transcripts and GRE scores to: Dr. Barnabas Daru, through email: barnabas.daru at tamucc.edu

---
Barnabas Daru (PhD)
Assistant Professor of Biology
Department of Life Sciences
Texas A&M University - Corpus Christi
Phone: +1 361 825 3489
Email: Barnabas.Daru at tamucc.edu <mailto:Barnabas.Daru at tamucc.edu>
Lab Website: https://barnabasdaru.com <https://barnabasdaru.com/>













	[[alternative HTML version deleted]]


From u@erc@tch @end|ng |rom out|ook@com  Sun Nov  3 20:12:17 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Sun, 3 Nov 2019 19:12:17 +0000
Subject: [R-sig-Geo] poly2nb neighbour itself should be considered a
 neighbour
Message-ID: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear All,

I would like to know if the function "poly2nb" ("spdep" package.) let me create a neighborhood of itself, i.e., not only its queen neighbors (queen=TRUE), but a neighbour itself should also be considered a neighbour.

I am looking to create a queen weight neighborhood matrix afterwards using "nb2listw".

Any help would help me a lot.

Many thanks

	[[alternative HTML version deleted]]


From dexter@|ocke @end|ng |rom gm@||@com  Sun Nov  3 21:46:36 2019
From: dexter@|ocke @end|ng |rom gm@||@com (Dexter Locke)
Date: Sun, 3 Nov 2019 15:46:36 -0500
Subject: [R-sig-Geo] poly2nb neighbour itself should be considered a
 neighbour
In-Reply-To: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <D872ED30-5D8D-43E5-A9C7-DB4731F8FC54@gmail.com>

Dear Robert,

It sounds like what you are looking for is typically called a second order neighbor. Higher order neighbors can also included in a weights matrix such as your neighbors?, neighbors?, neighbor which is a third-order neighbor. I think you are seeking second-order neighbors. 

See the spdep vignettes, and the section 5 Higher-Order Neighbors subsection here: https://cran.r-project.org/web/packages/spdep/vignettes/nb.pdf in particular. The spdep::nblag might be what you need, but without additional information it is hard to know. 

Good luck,
Dexter
dexterlocke.com



> On Nov 3, 2019, at 2:12 PM, Robert R <usercatch at outlook.com> wrote:
> 
> ?Dear All,
> 
> I would like to know if the function "poly2nb" ("spdep" package.) let me create a neighborhood of itself, i.e., not only its queen neighbors (queen=TRUE), but a neighbour itself should also be considered a neighbour.
> 
> I am looking to create a queen weight neighborhood matrix afterwards using "nb2listw".
> 
> Any help would help me a lot.
> 
> Many thanks
> 
>    [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From m|ke|@d|@z@rodr|guez @end|ng |rom hotm@||@com  Sun Nov  3 22:53:26 2019
From: m|ke|@d|@z@rodr|guez @end|ng |rom hotm@||@com (=?iso-8859-1?Q?Mikel_D=EDaz_Rodr=EDguez?=)
Date: Sun, 3 Nov 2019 21:53:26 +0000
Subject: [R-sig-Geo] Error in random labelling using the J function
In-Reply-To: <ME2PR01MB2244356F1980AEC452158123A4620@ME2PR01MB2244.ausprd01.prod.outlook.com>
References: <mailman.27727.3.1572519601.50671.r-sig-geo@r-project.org>,
 <ME2PR01MB2244356F1980AEC452158123A4620@ME2PR01MB2244.ausprd01.prod.outlook.com>
Message-ID: <DB7PR10MB201254C3DC2472FABAE316D0D07C0@DB7PR10MB2012.EURPRD10.PROD.OUTLOOK.COM>

I have installed the current development version of spatstat (1.61-0.044) and the code runs correctly without that error appearing. Thank you very much for answering and fixing the bug.

Greetings.

M. D?az.



________________________________
De: R-sig-Geo <r-sig-geo-bounces at r-project.org> en nombre de Adrian Baddeley <adrian.baddeley at curtin.edu.au>
Enviado: viernes, 1 de noviembre de 2019 5:16
Para: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
Asunto: Re: [R-sig-Geo] Error in random labelling using the J function

<mikel.diaz.rodriguez at hotmail.com> writes:

  > I'm having problems when I run the random labelling using J Function.
> [...]
> library(spatstat)
> [...]
> is.multitype(pm)
> plot(pm)

> Jdif <- function(X, ..., i) {
  >            Jidot <- Jdot(X, ..., i = i)
>              J <- Jest(X, ...)
  >            dif <- eval.fv(Jidot - J)
  >           return(dif)
> }
  > E <- envelope(pm, Jdif, nsim = 39, i = "A", j = "B", simulate = expression(rlabel(pm)))
>  But when I execute E, it shows me the following error:

  > Error: in Fest(X, r) the successive r values must be finely spaced: given spacing = 0.010203; required spacing <=  0.00592

This is a bug in Jdot(). It is not related to the use of random labelling or envelopes.

The bug is fixed in the current development version of spatstat, 1.61-0.042,
which you can install from <https://github.com/spatstat/spatstat>

Note that in the envelope() command above, you don't need the argument 'j="B" and it is ignored.

regards


Prof Adrian Baddeley DSc FAA

John Curtin Distinguished Professor

Department of Mathematics and Statistics

Curtin University, Perth, Western Australia

________________________________


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From e||@@kr@|n@k| @end|ng |rom gm@||@com  Sun Nov  3 23:36:41 2019
From: e||@@kr@|n@k| @end|ng |rom gm@||@com (Elias T. Krainski)
Date: Sun, 3 Nov 2019 19:36:41 -0300
Subject: [R-sig-Geo] poly2nb neighbour itself should be considered a
 neighbour
In-Reply-To: <D872ED30-5D8D-43E5-A9C7-DB4731F8FC54@gmail.com>
References: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
 <D872ED30-5D8D-43E5-A9C7-DB4731F8FC54@gmail.com>
Message-ID: <0cb5328c-893b-4c9e-686d-8b3fe99d7c2c@gmail.com>

Hello,

I found this matter easier when working with matrix representations. Set

 ?A^0 = I (identity matrix),

 ?A^1 = A, where A_{ij} = 1 if j is neighbor to j and zero otherwise 
(this consider A_{ii} = 0)

 ?A^2 = A'A

 ?A^3 = A'A^2 and so on

The A^k_{ij} entry gives _how many steps of length k there is between i 
and j_. To me, this definition makes this matter clear. See an example 
considering a 10x10 regular grid:

nb <- grid2nb(d=c(10,10), queen = FALSE)
nn <- card(nb)
A1 <- sparseMatrix(
 ?i = rep(1:length(nn), nn),
 ?j = unlist(nb[nn>0]), x=1)
A2 <- crossprod(A1)
image(A1)
image(A2)

best regards,

Elias

On 03/11/2019 17:46, Dexter Locke wrote:
> Dear Robert,
>
> It sounds like what you are looking for is typically called a second order neighbor. Higher order neighbors can also included in a weights matrix such as your neighbors?, neighbors?, neighbor which is a third-order neighbor. I think you are seeking second-order neighbors.
>
> See the spdep vignettes, and the section 5 Higher-Order Neighbors subsection here: https://cran.r-project.org/web/packages/spdep/vignettes/nb.pdf in particular. The spdep::nblag might be what you need, but without additional information it is hard to know.
>
> Good luck,
> Dexter
> dexterlocke.com
>
>
>
>> On Nov 3, 2019, at 2:12 PM, Robert R <usercatch at outlook.com> wrote:
>>
>> ?Dear All,
>>
>> I would like to know if the function "poly2nb" ("spdep" package.) let me create a neighborhood of itself, i.e., not only its queen neighbors (queen=TRUE), but a neighbour itself should also be considered a neighbour.
>>
>> I am looking to create a queen weight neighborhood matrix afterwards using "nb2listw".
>>
>> Any help would help me a lot.
>>
>> Many thanks
>>
>>     [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From u@erc@tch @end|ng |rom out|ook@com  Mon Nov  4 00:37:01 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Sun, 3 Nov 2019 23:37:01 +0000
Subject: [R-sig-Geo] poly2nb neighbour itself should be considered a
 neighbour
In-Reply-To: <0cb5328c-893b-4c9e-686d-8b3fe99d7c2c@gmail.com>
References: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
 <D872ED30-5D8D-43E5-A9C7-DB4731F8FC54@gmail.com>,
 <0cb5328c-893b-4c9e-686d-8b3fe99d7c2c@gmail.com>
Message-ID: <VI1P190MB0768BDEE2811D715812E2C64B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Dexter, Dear Elias,

Many thanks for your response.

I am looking to create a spatial weight matrix (queen contiguity neighbors), with the only difference that a neighbour itself should also be considered a neighbour.

Below I am sending you the code. Basically I create a polygon for NYC and give that polygon I want to create the spatial weight matrix.

@Elias: I used your answer to generate two matrices A1 and A2. But how to turn them into an nb object so I can use the function nb2listw (with the arguments style="W", zero.policy=TRUE).

Thank you and best regards,
Robert


#####

####################
# ---- packages ----?
####################?
?
packages_install <- function(packages){?
 new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]?
 if (length(new.packages)) ?
 install.packages(new.packages, dependencies = TRUE)?
 sapply(packages, require, character.only = TRUE)?
}?
?
packages_required <- c("data.table", "dplyr", "sf", "spdep", "Matrix")?
packages_install(packages_required)?
?
# Working directory?
setwd("C:/Users/User/Documents/Code")?
?
?
#####################?
# ---- zips_nyc ----?
#####################?
?
zips_nyc_bronx <- c("10451", "10452", "10453", "10454", "10455", "10456", "10457", "10458", "10459", "10460", "10461", "10462", "10463", "10464", "10465", "10466", "10467", "10468", "10469", "10470", "10471", "10472", "10473", "10474", "10475")?
zips_nyc_brooklyn <- c("11201", "11203", "11204", "11205", "11206", "11207", "11208", "11209", "11210", "11211", "11212", "11213", "11214", "11215", "11216", "11217", "11218", "11219", "11220", "11221", "11222", "11223", "11224", "11225", "11226", "11228", "11229", "11230", "11231", "11232", "11233", "11234", "11235", "11236", "11237", "11238", "11239")?
zips_nyc_manhattan <- c("10001", "10002", "10003", "10004", "10005", "10006", "10007", "10009", "10010", "10011", "10012", "10013", "10014", "10016", "10017", "10018", "10019", "10020", "10021", "10022", "10023", "10024", "10025", "10026", "10027", "10028", "10029", "10030", "10031", "10032", "10033", "10034", "10035", "10036", "10037", "10038", "10039", "10040", "10044", "10065", "10075", "10128", "10280")?
zips_nyc_queens <- c("11004", "11005", "11101", "11102", "11103", "11104", "11105", "11106", "11354", "11355", "11356", "11357", "11358", "11359", "11360", "11361", "11362", "11363", "11364", "11365", "11366", "11367", "11368", "11369", "11370", "11372", "11373", "11374", "11375", "11377", "11378", "11379", "11385", "11411", "11412", "11413", "11414", "11415", "11416", "11417", "11418", "11419", "11420", "11421", "11422", "11423", "11426", "11427", "11428", "11429", "11432", "11433", "11434", "11435", "11436", "11691", "11692", "11693", "11694", "11695", "11697")?
zips_nyc_staten_island <- c("10301", "10302", "10303", "10304", "10305", "10306", "10307", "10308", "10309", "10310", "10312", "10314")?
zips_nyc <- sort(c(zips_nyc_bronx, zips_nyc_brooklyn, zips_nyc_manhattan, zips_nyc_queens, zips_nyc_staten_island))?
?
?
#####################?
# ---- shapefile ----?
#####################?
?
## shapefile_us?
?
# Shapefile zips import and Coordinate Reference System (CRS) transformation?
# Download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip?
shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")?
?
# Columns removal?
shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))?
?
# Column rename: ZCTA5CE10?
setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))?
?
# Column class change: zipcode?
shapefile_us$zipcode <- as.character(shapefile_us$zipcode)?
?
?
## polygon_nyc?
polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)?
?
# Variable creation: list of neighbors for each polygon (queen contiguity neighbors)?
nb <- poly2nb(polygon_nyc, queen=FALSE)?
nn <- card(nb)?
A1 <- sparseMatrix(?
  i = rep(1:length(nn), nn),?
  j = unlist(nb[nn>0]), x=1)?
A2 <- crossprod(A1)?
image(A1)?
image(A2)?
?
# next, supplement the neighbor list with spatial weights: "W" row-standardize weights?
W_Matrix <- nb2listw(neighbours = A1, style="W", zero.policy=TRUE)

#####
________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Elias T. Krainski <eliaskrainski at gmail.com>
Sent: Sunday, November 3, 2019 23:36
To: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
Subject: Re: [R-sig-Geo] poly2nb neighbour itself should be considered a neighbour

Hello,

I found this matter easier when working with matrix representations. Set

  A^0 = I (identity matrix),

  A^1 = A, where A_{ij} = 1 if j is neighbor to j and zero otherwise
(this consider A_{ii} = 0)

  A^2 = A'A

  A^3 = A'A^2 and so on

The A^k_{ij} entry gives _how many steps of length k there is between i
and j_. To me, this definition makes this matter clear. See an example
considering a 10x10 regular grid:

nb <- grid2nb(d=c(10,10), queen = FALSE)
nn <- card(nb)
A1 <- sparseMatrix(
  i = rep(1:length(nn), nn),
  j = unlist(nb[nn>0]), x=1)
A2 <- crossprod(A1)
image(A1)
image(A2)

best regards,

Elias

On 03/11/2019 17:46, Dexter Locke wrote:
> Dear Robert,
>
> It sounds like what you are looking for is typically called a second order neighbor. Higher order neighbors can also included in a weights matrix such as your neighbors?, neighbors?, neighbor which is a third-order neighbor. I think you are seeking second-order neighbors.
>
> See the spdep vignettes, and the section 5 Higher-Order Neighbors subsection here: https://cran.r-project.org/web/packages/spdep/vignettes/nb.pdf in particular. The spdep::nblag might be what you need, but without additional information it is hard to know.
>
> Good luck,
> Dexter
> dexterlocke.com
>
>
>
>> On Nov 3, 2019, at 2:12 PM, Robert R <usercatch at outlook.com> wrote:
>>
>> ?Dear All,
>>
>> I would like to know if the function "poly2nb" ("spdep" package.) let me create a neighborhood of itself, i.e., not only its queen neighbors (queen=TRUE), but a neighbour itself should also be considered a neighbour.
>>
>> I am looking to create a queen weight neighborhood matrix afterwards using "nb2listw".
>>
>> Any help would help me a lot.
>>
>> Many thanks
>>
>>     [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From b@row||ng@on @end|ng |rom gm@||@com  Mon Nov  4 11:28:04 2019
From: b@row||ng@on @end|ng |rom gm@||@com (Barry Rowlingson)
Date: Mon, 4 Nov 2019 10:28:04 +0000
Subject: [R-sig-Geo] poly2nb neighbour itself should be considered a
 neighbour
In-Reply-To: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <CANVKczN6xn+USgu_KzhYpcFctnMbQYXkzraEoDQnS6MXpc1_-w@mail.gmail.com>

Can you manipulate the adjacency list structure to add `i` to each list
element vector?

eg using sample data from spdep:

make a neighbour structure:

 > colnn = poly2nb(columbus)

this is a list - so for example polygon 4 is next to:

 > colnn[[4]]
 [1] 2 3 5 8

2, 3, 5, and 8. It seems you want to include `4` in that vector. So run
this loop:

 > for(i in 1:length(colnn)){colnn[[i]]=as.integer(c(i,colnn[[i]]))}

 which produces a nb structure:

 > colnn
Neighbour list object:
Number of regions: 49
Number of nonzero links: 285
Percentage nonzero weights: 11.87005
Average number of links: 5.816327

compared with the original without self-links:

> poly2nb(columbus)
Neighbour list object:
Number of regions: 49
Number of nonzero links: 236
Percentage nonzero weights: 9.829238
Average number of links: 4.816327

If you want to go on to make a weights list object:

> lw = nb2listw(colnn)

the neighbours are preserved:

> str(lw)
List of 3
 $ style     : chr "W"
 $ neighbours:List of 49
  ..$ : int [1:3] 1 2 3
  ..$ : int [1:4] 2 1 3 4
  ..$ : int [1:5] 3 1 2 4 5
  ..$ : int [1:5] 4 2 3 5 8
...
$ weights   :List of 49
  ..$ : num [1:3] 0.333 0.333 0.333
  ..$ : num [1:4] 0.25 0.25 0.25 0.25
  ..$ : num [1:5] 0.2 0.2 0.2 0.2 0.2
  ..$ : num [1:5] 0.2 0.2 0.2 0.2 0.2

So I think that's what you want...

Note carefully the use of `as.integer` here:

  for(i in 1:length(colnn)){colnn[[i]]=as.integer(c(i,colnn[[i]]))}

because there's code in `spdep` that expects these things to be stored as
integer and passes them to C code. Get this wrong when manipulating

 > xx = poly2nb(columbus)
 > xx
Neighbour list object:
Number of regions: 49
Number of nonzero links: 236
Percentage nonzero weights: 9.829238
Average number of links: 4.816327
 > xx[[1]] = c(1, xx[[1]])
 > xx
 Error in card(nb) :
  INTEGER() can only be applied to a 'integer', not a 'double'

so instead:

> xx = poly2nb(columbus)
> xx[[1]] = as.integer(c(1, xx[[1]]))
> xx
Neighbour list object:
Number of regions: 49
Number of nonzero links: 237
Percentage nonzero weights: 9.870887
Average number of links: 4.836735

works - I don't think its strictly necessary in the `for` loop because `i`
is an integer but belt and braces....

Barry



On Sun, Nov 3, 2019 at 7:12 PM Robert R <usercatch at outlook.com> wrote:

> Dear All,
>
> I would like to know if the function "poly2nb" ("spdep" package.) let me
> create a neighborhood of itself, i.e., not only its queen neighbors
> (queen=TRUE), but a neighbour itself should also be considered a neighbour.
>
> I am looking to create a queen weight neighborhood matrix afterwards using
> "nb2listw".
>
> Any help would help me a lot.
>
> Many thanks
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Nov  4 11:32:58 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 4 Nov 2019 11:32:58 +0100
Subject: [R-sig-Geo] poly2nb neighbour itself should be considered a
 neighbour
In-Reply-To: <VI1P190MB0768BDEE2811D715812E2C64B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
 <D872ED30-5D8D-43E5-A9C7-DB4731F8FC54@gmail.com>,
 <0cb5328c-893b-4c9e-686d-8b3fe99d7c2c@gmail.com>
 <VI1P190MB0768BDEE2811D715812E2C64B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1911041126250.2930@reclus.nhh.no>

On Mon, 4 Nov 2019, Robert R wrote:

> Dear Dexter, Dear Elias,
>
> Many thanks for your response.
>
> I am looking to create a spatial weight matrix (queen contiguity 
> neighbors), with the only difference that a neighbour itself should also 
> be considered a neighbour.

I think you are looking for spdep::include.self(), originally written to 
include self-neighbours in nb objects for the local Getis-Ord G_i^* 
measure. Powering the matrix will also give self-neighbours as a 
by-product for observations with neighbours, but not only ones on the 
principal diagonal.

Roger

>
> Below I am sending you the code. Basically I create a polygon for NYC 
> and give that polygon I want to create the spatial weight matrix.
>
> @Elias: I used your answer to generate two matrices A1 and A2. But how to turn them into an nb object so I can use the function nb2listw (with the arguments style="W", zero.policy=TRUE).
>
> Thank you and best regards,
> Robert
>
>
> #####
>
> ####################
> # ---- packages ----?
> ####################?
> ?
> packages_install <- function(packages){?
> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]?
> if (length(new.packages)) ?
> install.packages(new.packages, dependencies = TRUE)?
> sapply(packages, require, character.only = TRUE)?
> }?
> ?
> packages_required <- c("data.table", "dplyr", "sf", "spdep", "Matrix")?
> packages_install(packages_required)?
> ?
> # Working directory?
> setwd("C:/Users/User/Documents/Code")?
> ?
> ?
> #####################?
> # ---- zips_nyc ----?
> #####################?
> ?
> zips_nyc_bronx <- c("10451", "10452", "10453", "10454", "10455", "10456", "10457", "10458", "10459", "10460", "10461", "10462", "10463", "10464", "10465", "10466", "10467", "10468", "10469", "10470", "10471", "10472", "10473", "10474", "10475")?
> zips_nyc_brooklyn <- c("11201", "11203", "11204", "11205", "11206", "11207", "11208", "11209", "11210", "11211", "11212", "11213", "11214", "11215", "11216", "11217", "11218", "11219", "11220", "11221", "11222", "11223", "11224", "11225", "11226", "11228", "11229", "11230", "11231", "11232", "11233", "11234", "11235", "11236", "11237", "11238", "11239")?
> zips_nyc_manhattan <- c("10001", "10002", "10003", "10004", "10005", "10006", "10007", "10009", "10010", "10011", "10012", "10013", "10014", "10016", "10017", "10018", "10019", "10020", "10021", "10022", "10023", "10024", "10025", "10026", "10027", "10028", "10029", "10030", "10031", "10032", "10033", "10034", "10035", "10036", "10037", "10038", "10039", "10040", "10044", "10065", "10075", "10128", "10280")?
> zips_nyc_queens <- c("11004", "11005", "11101", "11102", "11103", "11104", "11105", "11106", "11354", "11355", "11356", "11357", "11358", "11359", "11360", "11361", "11362", "11363", "11364", "11365", "11366", "11367", "11368", "11369", "11370", "11372", "11373", "11374", "11375", "11377", "11378", "11379", "11385", "11411", "11412", "11413", "11414", "11415", "11416", "11417", "11418", "11419", "11420", "11421", "11422", "11423", "11426", "11427", "11428", "11429", "11432", "11433", "11434", "11435", "11436", "11691", "11692", "11693", "11694", "11695", "11697")?
> zips_nyc_staten_island <- c("10301", "10302", "10303", "10304", "10305", "10306", "10307", "10308", "10309", "10310", "10312", "10314")?
> zips_nyc <- sort(c(zips_nyc_bronx, zips_nyc_brooklyn, zips_nyc_manhattan, zips_nyc_queens, zips_nyc_staten_island))?
> ?
> ?
> #####################?
> # ---- shapefile ----?
> #####################?
> ?
> ## shapefile_us?
> ?
> # Shapefile zips import and Coordinate Reference System (CRS) transformation?
> # Download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip?
> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")?
> ?
> # Columns removal?
> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))?
> ?
> # Column rename: ZCTA5CE10?
> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))?
> ?
> # Column class change: zipcode?
> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)?
> ?
> ?
> ## polygon_nyc?
> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)?
> ?
> # Variable creation: list of neighbors for each polygon (queen contiguity neighbors)?
> nb <- poly2nb(polygon_nyc, queen=FALSE)?
> nn <- card(nb)?
> A1 <- sparseMatrix(?
>  i = rep(1:length(nn), nn),?
>  j = unlist(nb[nn>0]), x=1)?
> A2 <- crossprod(A1)?
> image(A1)?
> image(A2)?
> ?
> # next, supplement the neighbor list with spatial weights: "W" row-standardize weights?
> W_Matrix <- nb2listw(neighbours = A1, style="W", zero.policy=TRUE)
>
> #####
> ________________________________
> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Elias T. Krainski <eliaskrainski at gmail.com>
> Sent: Sunday, November 3, 2019 23:36
> To: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
> Subject: Re: [R-sig-Geo] poly2nb neighbour itself should be considered a neighbour
>
> Hello,
>
> I found this matter easier when working with matrix representations. Set
>
>  A^0 = I (identity matrix),
>
>  A^1 = A, where A_{ij} = 1 if j is neighbor to j and zero otherwise
> (this consider A_{ii} = 0)
>
>  A^2 = A'A
>
>  A^3 = A'A^2 and so on
>
> The A^k_{ij} entry gives _how many steps of length k there is between i
> and j_. To me, this definition makes this matter clear. See an example
> considering a 10x10 regular grid:
>
> nb <- grid2nb(d=c(10,10), queen = FALSE)
> nn <- card(nb)
> A1 <- sparseMatrix(
>  i = rep(1:length(nn), nn),
>  j = unlist(nb[nn>0]), x=1)
> A2 <- crossprod(A1)
> image(A1)
> image(A2)
>
> best regards,
>
> Elias
>
> On 03/11/2019 17:46, Dexter Locke wrote:
>> Dear Robert,
>>
>> It sounds like what you are looking for is typically called a second order neighbor. Higher order neighbors can also included in a weights matrix such as your neighbors?, neighbors?, neighbor which is a third-order neighbor. I think you are seeking second-order neighbors.
>>
>> See the spdep vignettes, and the section 5 Higher-Order Neighbors subsection here: https://cran.r-project.org/web/packages/spdep/vignettes/nb.pdf in particular. The spdep::nblag might be what you need, but without additional information it is hard to know.
>>
>> Good luck,
>> Dexter
>> dexterlocke.com
>>
>>
>>
>>> On Nov 3, 2019, at 2:12 PM, Robert R <usercatch at outlook.com> wrote:
>>>
>>> ?Dear All,
>>>
>>> I would like to know if the function "poly2nb" ("spdep" package.) let me create a neighborhood of itself, i.e., not only its queen neighbors (queen=TRUE), but a neighbour itself should also be considered a neighbour.
>>>
>>> I am looking to create a queen weight neighborhood matrix afterwards using "nb2listw".
>>>
>>> Any help would help me a lot.
>>>
>>> Many thanks
>>>
>>>     [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>        [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger@B|v@nd @end|ng |rom nhh@no  Mon Nov  4 11:37:00 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 4 Nov 2019 11:37:00 +0100
Subject: [R-sig-Geo] poly2nb neighbour itself should be considered a
 neighbour
In-Reply-To: <CANVKczN6xn+USgu_KzhYpcFctnMbQYXkzraEoDQnS6MXpc1_-w@mail.gmail.com>
References: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
 <CANVKczN6xn+USgu_KzhYpcFctnMbQYXkzraEoDQnS6MXpc1_-w@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1911041135100.2930@reclus.nhh.no>

On Mon, 4 Nov 2019, Barry Rowlingson wrote:

> Can you manipulate the adjacency list structure to add `i` to each list
> element vector?
>
> eg using sample data from spdep:
>
> make a neighbour structure:
>
> > colnn = poly2nb(columbus)
>
> this is a list - so for example polygon 4 is next to:
>
> > colnn[[4]]
> [1] 2 3 5 8
>
> 2, 3, 5, and 8. It seems you want to include `4` in that vector. So run
> this loop:
>
> > for(i in 1:length(colnn)){colnn[[i]]=as.integer(c(i,colnn[[i]]))}
>
> which produces a nb structure:
>
> > colnn
> Neighbour list object:
> Number of regions: 49
> Number of nonzero links: 285
> Percentage nonzero weights: 11.87005
> Average number of links: 5.816327
>
> compared with the original without self-links:
>
>> poly2nb(columbus)
> Neighbour list object:
> Number of regions: 49
> Number of nonzero links: 236
> Percentage nonzero weights: 9.829238
> Average number of links: 4.816327
>

Barry, thanks: the include.self() function:

include.self <- function(nb) {
 	if (!is.null(attributes(nb)$self.included) &&
 		(as.logical(attributes(nb)$self.included)))
 		stop("Self already included")
 	n <- length(nb)
 	nc <- card(nb)
 	for (i in 1:n) {
 		if (nc[i] > 0) {
 			nb[[i]] <- sort(c(i, nb[[i]]))
 		} else {
 			nb[[i]] <- i
 		}
 	}

 	attr(nb, "self.included") <- TRUE
 	nb
}

does this with sorting, but maybe needs class inheritance checking - it 
was probably written last century.

Roger

> If you want to go on to make a weights list object:
>
>> lw = nb2listw(colnn)
>
> the neighbours are preserved:
>
>> str(lw)
> List of 3
> $ style     : chr "W"
> $ neighbours:List of 49
>  ..$ : int [1:3] 1 2 3
>  ..$ : int [1:4] 2 1 3 4
>  ..$ : int [1:5] 3 1 2 4 5
>  ..$ : int [1:5] 4 2 3 5 8
> ...
> $ weights   :List of 49
>  ..$ : num [1:3] 0.333 0.333 0.333
>  ..$ : num [1:4] 0.25 0.25 0.25 0.25
>  ..$ : num [1:5] 0.2 0.2 0.2 0.2 0.2
>  ..$ : num [1:5] 0.2 0.2 0.2 0.2 0.2
>
> So I think that's what you want...
>
> Note carefully the use of `as.integer` here:
>
>  for(i in 1:length(colnn)){colnn[[i]]=as.integer(c(i,colnn[[i]]))}
>
> because there's code in `spdep` that expects these things to be stored as
> integer and passes them to C code. Get this wrong when manipulating
>
> > xx = poly2nb(columbus)
> > xx
> Neighbour list object:
> Number of regions: 49
> Number of nonzero links: 236
> Percentage nonzero weights: 9.829238
> Average number of links: 4.816327
> > xx[[1]] = c(1, xx[[1]])
> > xx
> Error in card(nb) :
>  INTEGER() can only be applied to a 'integer', not a 'double'
>
> so instead:
>
>> xx = poly2nb(columbus)
>> xx[[1]] = as.integer(c(1, xx[[1]]))
>> xx
> Neighbour list object:
> Number of regions: 49
> Number of nonzero links: 237
> Percentage nonzero weights: 9.870887
> Average number of links: 4.836735
>
> works - I don't think its strictly necessary in the `for` loop because `i`
> is an integer but belt and braces....
>
> Barry
>
>
>
> On Sun, Nov 3, 2019 at 7:12 PM Robert R <usercatch at outlook.com> wrote:
>
>> Dear All,
>>
>> I would like to know if the function "poly2nb" ("spdep" package.) let me
>> create a neighborhood of itself, i.e., not only its queen neighbors
>> (queen=TRUE), but a neighbour itself should also be considered a neighbour.
>>
>> I am looking to create a queen weight neighborhood matrix afterwards using
>> "nb2listw".
>>
>> Any help would help me a lot.
>>
>> Many thanks
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From @r|e|@|uente@d| @end|ng |rom u@@ch@c|  Mon Nov  4 20:25:35 2019
From: @r|e|@|uente@d| @end|ng |rom u@@ch@c| (Ariel Fuentesdi)
Date: Mon, 4 Nov 2019 16:25:35 -0300
Subject: [R-sig-Geo] Spatio-temporal Predictions with CARBayesST
Message-ID: <CAD0a9Kg4HNhfHZn8uDB6X1QvcP0J4ALeQR6Hrwij2XZb9nGFGw@mail.gmail.com>

Hi everyone,

I'm trying to use the CARBayesST package and I need to do Spatio-temporal
predictions. In the vignette of the package on page 27 says " If there had
been say m missing values, then the Y component of the list would have
contained m columns, with each one containing posterior predictive samples
for one of the missing observations."

https://cran.r-project.org/web/packages/CARBayesST/vignettes/CARBayesST.pdf

But I don't understand well how to obtain the posterior predictive values
of Y, let's say I want to predict the value of Y for the next 3 periods for
each zone. How should I do it?

This is the reproducible code (found in the vignette):

library(CARBayesST)
#################################################
#### Run the model on simulated data on a lattice
#################################################
#### set up the regular lattice
x.easting <- 1:10
x.northing <- 1:10
Grid <- expand.grid(x.easting, x.northing)
K <- nrow(Grid)
N <- 10
N.all <- N * K
#### set up spatial neighbourhood matrix W
distance <- as.matrix(dist(Grid))
W <-array(0, c(K,K))
W[distance==1] <-1
#### Simulate the elements in the linear predictor and the data
gamma <- rnorm(n=N.all, mean=0, sd=0.001)
x <- rnorm(n=N.all, mean=0, sd=1)
beta <- 0.1
Q.W <- 0.99 * (diag(apply(W, 2, sum)) - W) + 0.01 * diag(rep(1,K))
Q.W.inv <- solve(Q.W)
phi.temp <- mvrnorm(n=1, mu=rep(0,K), Sigma=(0.1 * Q.W.inv))
phi <- phi.temp
for(i in 2:N)
{
  phi.temp2 <- mvrnorm(n=1, mu=(0.8 * phi.temp), Sigma=(0.1 * Q.W.inv))
  phi.temp <- phi.temp2
  phi <- c(phi, phi.temp)
}
LP <- 3 + x * beta + phi
mean <- exp(LP)
Y <- rpois(n=N.all, lambda=mean)
#### Run the model
model <- ST.CARar(formula=Y~x, family="poisson", W=W, burnin=10,
                  n.sample=50)

Regards,
Ariel

	[[alternative HTML version deleted]]


From u@erc@tch @end|ng |rom out|ook@com  Mon Nov  4 21:48:50 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Mon, 4 Nov 2019 20:48:50 +0000
Subject: [R-sig-Geo] poly2nb neighbour itself should be considered a
 neighbour
In-Reply-To: <alpine.LFD.2.21.1911041135100.2930@reclus.nhh.no>
References: <VI1P190MB07688C3278CB943D1BE4C7E7B07C0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
 <CANVKczN6xn+USgu_KzhYpcFctnMbQYXkzraEoDQnS6MXpc1_-w@mail.gmail.com>,
 <alpine.LFD.2.21.1911041135100.2930@reclus.nhh.no>
Message-ID: <VI1P190MB0768ABA9C6308CB381173DCCB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear All,

Many thanks for the proposed solutions, specially to Roger and Barry.

This was exactly what I was looking for.

I tested with a really simple polygon (see below), and it worked.

##
columbus <- sf::read_sf(system.file("etc/shapes/columbus.shp", package="spdep"))
columbus <- columbus %>% slice(1:2)?
# plot(columbus)?
?
colnn <- poly2nb(columbus)?
colnn[1]?
colnn[2]?
?
# Solution 1?
for(i in 1:length(colnn)){colnn[[i]]=as.integer(c(i,colnn[[i]]))}?
colnn[1]?
colnn[2]?
?
# Solution 2?
colnn <- include.self(colnn)?
colnn[1]?
colnn[2]?
?
lw <- nb2listw(neighbours = colnn, style="W", zero.policy=TRUE)

##

Best regards,
Robert
________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Monday, November 4, 2019 11:37
To: Barry Rowlingson <b.rowlingson at gmail.com>
Cc: Robert R <usercatch at outlook.com>; r-sig-geo at r-project.org <r-sig-geo at r-project.org>
Subject: Re: [R-sig-Geo] poly2nb neighbour itself should be considered a neighbour

On Mon, 4 Nov 2019, Barry Rowlingson wrote:

> Can you manipulate the adjacency list structure to add `i` to each list
> element vector?
>
> eg using sample data from spdep:
>
> make a neighbour structure:
>
> > colnn = poly2nb(columbus)
>
> this is a list - so for example polygon 4 is next to:
>
> > colnn[[4]]
> [1] 2 3 5 8
>
> 2, 3, 5, and 8. It seems you want to include `4` in that vector. So run
> this loop:
>
> > for(i in 1:length(colnn)){colnn[[i]]=as.integer(c(i,colnn[[i]]))}
>
> which produces a nb structure:
>
> > colnn
> Neighbour list object:
> Number of regions: 49
> Number of nonzero links: 285
> Percentage nonzero weights: 11.87005
> Average number of links: 5.816327
>
> compared with the original without self-links:
>
>> poly2nb(columbus)
> Neighbour list object:
> Number of regions: 49
> Number of nonzero links: 236
> Percentage nonzero weights: 9.829238
> Average number of links: 4.816327
>

Barry, thanks: the include.self() function:

include.self <- function(nb) {
         if (!is.null(attributes(nb)$self.included) &&
                 (as.logical(attributes(nb)$self.included)))
                 stop("Self already included")
         n <- length(nb)
         nc <- card(nb)
         for (i in 1:n) {
                 if (nc[i] > 0) {
                         nb[[i]] <- sort(c(i, nb[[i]]))
                 } else {
                         nb[[i]] <- i
                 }
         }

         attr(nb, "self.included") <- TRUE
         nb
}

does this with sorting, but maybe needs class inheritance checking - it
was probably written last century.

Roger

> If you want to go on to make a weights list object:
>
>> lw = nb2listw(colnn)
>
> the neighbours are preserved:
>
>> str(lw)
> List of 3
> $ style     : chr "W"
> $ neighbours:List of 49
>  ..$ : int [1:3] 1 2 3
>  ..$ : int [1:4] 2 1 3 4
>  ..$ : int [1:5] 3 1 2 4 5
>  ..$ : int [1:5] 4 2 3 5 8
> ...
> $ weights   :List of 49
>  ..$ : num [1:3] 0.333 0.333 0.333
>  ..$ : num [1:4] 0.25 0.25 0.25 0.25
>  ..$ : num [1:5] 0.2 0.2 0.2 0.2 0.2
>  ..$ : num [1:5] 0.2 0.2 0.2 0.2 0.2
>
> So I think that's what you want...
>
> Note carefully the use of `as.integer` here:
>
>  for(i in 1:length(colnn)){colnn[[i]]=as.integer(c(i,colnn[[i]]))}
>
> because there's code in `spdep` that expects these things to be stored as
> integer and passes them to C code. Get this wrong when manipulating
>
> > xx = poly2nb(columbus)
> > xx
> Neighbour list object:
> Number of regions: 49
> Number of nonzero links: 236
> Percentage nonzero weights: 9.829238
> Average number of links: 4.816327
> > xx[[1]] = c(1, xx[[1]])
> > xx
> Error in card(nb) :
>  INTEGER() can only be applied to a 'integer', not a 'double'
>
> so instead:
>
>> xx = poly2nb(columbus)
>> xx[[1]] = as.integer(c(1, xx[[1]]))
>> xx
> Neighbour list object:
> Number of regions: 49
> Number of nonzero links: 237
> Percentage nonzero weights: 9.870887
> Average number of links: 4.836735
>
> works - I don't think its strictly necessary in the `for` loop because `i`
> is an integer but belt and braces....
>
> Barry
>
>
>
> On Sun, Nov 3, 2019 at 7:12 PM Robert R <usercatch at outlook.com> wrote:
>
>> Dear All,
>>
>> I would like to know if the function "poly2nb" ("spdep" package.) let me
>> create a neighborhood of itself, i.e., not only its queen neighbors
>> (queen=TRUE), but a neighbour itself should also be considered a neighbour.
>>
>> I am looking to create a queen weight neighborhood matrix afterwards using
>> "nb2listw".
>>
>> Any help would help me a lot.
>>
>> Many thanks
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

	[[alternative HTML version deleted]]


From |et|c|@c@nton @end|ng |rom hotm@||@com  Mon Nov  4 22:02:09 2019
From: |et|c|@c@nton @end|ng |rom hotm@||@com (=?iso-8859-1?Q?Let=EDcia_Dal=27_Canton?=)
Date: Mon, 4 Nov 2019 21:02:09 +0000
Subject: [R-sig-Geo] About dnearneight and nb2listw function
Message-ID: <SC1PR80MB3743BA7C5792AAE806782EEDA97F0@SC1PR80MB3743.lamprd80.prod.outlook.com>

I am forwarding the routine used and the part of the database I am interested in, which uses georeferenced agricultural data.

require(spdep)
require(geoR)

dados = read.geodata("test.txt", header = T)
dados

coord=dados$coord

max(dist(coord))/2

grid = dnearneigh(dados$coord, 0, 880)

lw=nb2listw(grid,zero.policy=TRUE)


>From this routine, I will try to explain more clearly what my doubts are:

1) The "dnearneight" function provides the neighbor list object. Is there any way to numerically visualize who the neighbors are?

2) The "nb2listw" function provides the "Characteristics of weights list object". When requesting the weights (lw$w in the routine), the n points are listed and values are assigned (who are these values?) And why are not the n values associated to the remaining n-1. Because in "attr (," comp ") $ d" it is noticeable that each point is associated to a different amount of points. For example, with the database used, the first point is associated to another 57, the second to another 65. Why does this occur?

Thankfully,


Let?cia Dal' Canton.

Matem?tica.
Mestre em Engenharia Agr?cola.
Doutoranda em Engenharia Agr?cola com linha de pesquisa em Estat?stica Espacial.
Universidade Estadual do Oeste do Paran? - UNIOESTE.

Curr?culo Lattes: http://lattes.cnpq.br/1085422685501012.
Contato: (45) 9 9962-7492.

	[[alternative HTML version deleted]]


From u@erc@tch @end|ng |rom out|ook@com  Tue Nov  5 00:49:52 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Mon, 4 Nov 2019 23:49:52 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
Message-ID: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

I have a large pooled cross-section data set.
?
I would like to estimate/regress using spatial autocorrelation methods. I am assuming for now that spatial dependence is present in both the dependent variable and the error term.?
?
My data set is over a period of 4 years, monthly data (54 periods). For this means, I've created a time dummy variable for each time period.?
?
I also created a weight matrix using the functions "poly2nb" and "nb2listw".?
?
Now I am trying to figure out a way to estimate my model which contains a really big data set.?
?
Basically, my model is as follows: y = ?D + ?W1y + X? + ?W2u + ??
?
My questions are:?
?
1) My spatial weight matrix for the whole data set will be probably a enormous matrix with submatrices for each time period itself. I don't think it would be possible to calculate this.?
What I would like to know is a way to estimate each time dummy/period separately (to compare different periods alone). How to do it??
?
2) Which package to use: spdep or splm??
?
Thank you and best regards,?
Robert?

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Nov  5 15:20:55 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 5 Nov 2019 15:20:55 +0100
Subject: [R-sig-Geo] About dnearneight and nb2listw function
In-Reply-To: <SC1PR80MB3743BA7C5792AAE806782EEDA97F0@SC1PR80MB3743.lamprd80.prod.outlook.com>
References: <SC1PR80MB3743BA7C5792AAE806782EEDA97F0@SC1PR80MB3743.lamprd80.prod.outlook.com>
Message-ID: <alpine.LFD.2.21.1911051457090.22435@reclus.nhh.no>

On Mon, 4 Nov 2019, Let?cia Dal' Canton wrote:

> I am forwarding the routine used and the part of the database I am 
> interested in, which uses georeferenced agricultural data.
>
> require(spdep)
> require(geoR)
>
> dados = read.geodata("test.txt", header = T)
> dados
>
> coord=dados$coord
>
> max(dist(coord))/2
>
> grid = dnearneigh(dados$coord, 0, 880)
>
> lw=nb2listw(grid,zero.policy=TRUE)
>
>
> From this routine, I will try to explain more clearly what my doubts 
> are:
>
> 1) The "dnearneight" function provides the neighbor list object. Is
>    there any way to numerically visualize who the neighbors are?

"numerically visualize" is not clear. Do you mean "show me who is 
neighbour of whom"?

library(spdep)
example(columbus, package="spData")
col.gal.nb # print method for nb objects
print.default(col.gal.nb) # treating nb as a list
col.gal.nb[[1]] # neighbours of observation 1

See also the package vignette vignette("nb_igraph") or: 
https://r-spatial.github.io/spdep/articles/nb_igraph.html
https://cran.r-project.org/web/packages/spdep/vignettes/nb_igraph.html

>
> 2) The "nb2listw" function provides the "Characteristics of weights list
>    object". When requesting the weights (lw$w in the routine), the n
>    points are listed and values are assigned (who are these values?) And
>    why are not the n values associated to the remaining n-1. Because in
>    "attr (," comp ") $ d" it is noticeable that each point is associated
>    to a different amount of points. For example, with the database used,
>    the first point is associated to another 57, the second to another
>    65. Why does this occur?

No idea, your data are not available. If you posted HTML, they were 
certainly discarded as risky. Post plain text only, and provide data on a 
link, not verbatim. If you set the distance threshold to 800, probably 
some observations were further than 800 units apart.

all.equal(attr(nb2listw(col.gal.nb)$weights, "comp")$d, card(col.gal.nb))

shows that in this case (row standardisation), the d vector is the same as 
the neighbour count per observation - it does not have to be so.

Roger

>
> Thankfully,
>
>
> Let???cia Dal' Canton.
>
> Matem???tica.
> Mestre em Engenharia Agr???cola.
> Doutoranda em Engenharia Agr???cola com linha de pesquisa em Estat???stica Espacial.
> Universidade Estadual do Oeste do Paran??? - UNIOESTE.
>
> Curr???culo Lattes: http://lattes.cnpq.br/1085422685501012.
> Contato: (45) 9 9962-7492.
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger@B|v@nd @end|ng |rom nhh@no  Tue Nov  5 15:30:06 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 5 Nov 2019 15:30:06 +0100
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>

On Tue, 5 Nov 2019, Robert R wrote:

> I have a large pooled cross-section data set. ?I would like to 
> estimate/regress using spatial autocorrelation methods. I am assuming 
> for now that spatial dependence is present in both the dependent 
> variable and the error term.? ?My data set is over a period of 4 years, 
> monthly data (54 periods). For this means, I've created a time dummy 
> variable for each time period.? ?I also created a weight matrix using the 
> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way 
> to estimate my model which contains a really big data set.? ?Basically, my 
> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1) 
> My spatial weight matrix for the whole data set will be probably a 
> enormous matrix with submatrices for each time period itself. I don't 
> think it would be possible to calculate this.? What I would like to know 
> is a way to estimate each time dummy/period separately (to compare 
> different periods alone). How to do it?? ?2) Which package to use: spdep 
> or splm?? ?Thank you and best regards,? Robert?

Please do not post HTML, only plain text. Almost certainly your model 
specification is wrong (SARAR/SAC is always a bad idea if alternatives are 
untried). What is your cross-sectional size? Using sparse kronecker 
products, the "enormous" matrix may not be very big. Does it make any 
sense using time dummies (54 x N x T will be mostly zero anyway)? Are most 
of the covariates time-varying? Please provide motivation and use area 
(preferably with affiliation (your email and user name are not 
informative) - this feels like a real estate problem, probably wrongly 
specified. You should use splm if time make sense in your case, but if it 
really doesn't, simplify your approach, as much of the data will be 
subject to very large temporal autocorrelation.

If this is a continuation of your previous question about using 
self-neighbours, be aware that you should not use self-neighbours in 
modelling, they are only useful for the Getis-Ord local G_i^* measure.

Roger

>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From cr|@t@be|dur@n @end|ng |rom gmx@de  Tue Nov  5 17:10:18 2019
From: cr|@t@be|dur@n @end|ng |rom gmx@de (=?UTF-8?Q?=22Cristabel_Dur=C3=A1n_Rangel=22?=)
Date: Tue, 5 Nov 2019 17:10:18 +0100
Subject: [R-sig-Geo] how to get the value of a pixel and its 8 surrounding
 pixels from points?
Message-ID: <trinity-262183c6-1548-42f5-b76a-c40dd66b2579-1572970218056@3c-app-gmx-bs24>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191105/293f65a1/attachment.html>

From u@erc@tch @end|ng |rom out|ook@com  Tue Nov  5 22:40:00 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Tue, 5 Nov 2019 21:40:00 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
Message-ID: <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Roger,

Thank you for your reply. I disabled HTML; my e-mails should be now in plain text.

I will give a better context for my desired outcome.

I am taking Airbnb's listings information for New York City available on: http://insideairbnb.com/get-the-data.html

I save every listings.csv.gz file available for NYC (2015-01 to 2019-09) - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a Listings/ folder. When importing all these 54 files into one single data set, I create a new "date_compiled" variable/column.

In total, after the data cleansing process, I have a little more 2 million observations.

I created 54 timedummy variables for each time period available.

I want to estimate using a hedonic spatial timedummy model the impact of a variety of characteristics which potentially determine the daily rate on Airbnb listings through time in New York City (e.g. characteristics of the listing as number of bedrooms, if the host if professional, proximity to downtown (New York City Hall) and nearest subway station from the listing, income per capita, etc.).

My dependent variable is price (log price, common in the related literature for hedonic prices).

The OLS model is done.

For the spatial model, I am assuming that hosts, when deciding the pricing of their listings, take not only into account its structural and location characteristics, but also the prices charged by near listings with similar characteristics - spatial
autocorrelation is then present, at least spatial dependence is present in the dependent variable.

As I wrote in my previous post, I was willing to consider the neighbor itself as a neighbor.

Parts of my code can be found below:

########

## packages

packages_install <- function(packages){
 new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
 if (length(new.packages))
 install.packages(new.packages, dependencies = TRUE)
 sapply(packages, require, character.only = TRUE)
}

packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
packages_install(packages_required)

# Working directory
setwd("C:/Users/User/R")



## shapefile_us

# Shapefile zips import and Coordinate Reference System (CRS) transformation
# Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")

# Columns removal
shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))

# Column rename: ZCTA5CE10
setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))

# Column class change: zipcode
shapefile_us$zipcode <- as.character(shapefile_us$zipcode)



## polygon_nyc

# Zip code not available in shapefile: 11695
polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)



## weight_matrix

# Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)

# Include neighbour itself as a neighbour
# for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
polygon_nyc_nb <- include.self(polygon_nyc_nb)

# Weights to each neighboring polygon
lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)



## listings

# Data import
files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
listings <- mapply(cbind, listings, date_compiled = names(listings))
listings <- listings %>% bind_rows

# Characters removal
listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
listings$price <- gsub("\\$", "", listings$price)
listings$price <- gsub(",", "", listings$price)



## timedummy

timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
timedummy <- paste(timedummy, sep = "", collapse = " + ")
timedummy <- gsub("-", "_", timedummy)



## OLS regression

# Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")

########

Some of my id's repeat in multiple time periods.

I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.

Now I want to apply the hedonic model with the timedummy variables.

Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?

Again, thank you very much for the help provided until now.

Best regards,
Robert

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Tuesday, November 5, 2019 15:30
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

On Tue, 5 Nov 2019, Robert R wrote:

> I have a large pooled cross-section data set. ?I would like to
> estimate/regress using spatial autocorrelation methods. I am assuming
> for now that spatial dependence is present in both the dependent
> variable and the error term.? ?My data set is over a period of 4 years,
> monthly data (54 periods). For this means, I've created a time dummy
> variable for each time period.? ?I also created a weight matrix using the
> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
> to estimate my model which contains a really big data set.? ?Basically, my
> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
> My spatial weight matrix for the whole data set will be probably a
> enormous matrix with submatrices for each time period itself. I don't
> think it would be possible to calculate this.? What I would like to know
> is a way to estimate each time dummy/period separately (to compare
> different periods alone). How to do it?? ?2) Which package to use: spdep
> or splm?? ?Thank you and best regards,? Robert?

Please do not post HTML, only plain text. Almost certainly your model
specification is wrong (SARAR/SAC is always a bad idea if alternatives are
untried). What is your cross-sectional size? Using sparse kronecker
products, the "enormous" matrix may not be very big. Does it make any
sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
of the covariates time-varying? Please provide motivation and use area
(preferably with affiliation (your email and user name are not
informative) - this feels like a real estate problem, probably wrongly
specified. You should use splm if time make sense in your case, but if it
really doesn't, simplify your approach, as much of the data will be
subject to very large temporal autocorrelation.

If this is a continuation of your previous question about using
self-neighbours, be aware that you should not use self-neighbours in
modelling, they are only useful for the Getis-Ord local G_i^* measure.

Roger

>
>       [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From b|@|ev||@t @end|ng |rom gm@||@com  Wed Nov  6 07:17:30 2019
From: b|@|ev||@t @end|ng |rom gm@||@com (=?UTF-8?Q?Bede-Fazekas_=c3=81kos?=)
Date: Wed, 6 Nov 2019 07:17:30 +0100
Subject: [R-sig-Geo] 
 how to get the value of a pixel and its 8 surrounding
 pixels from points?
In-Reply-To: <trinity-262183c6-1548-42f5-b76a-c40dd66b2579-1572970218056@3c-app-gmx-bs24>
References: <trinity-262183c6-1548-42f5-b76a-c40dd66b2579-1572970218056@3c-app-gmx-bs24>
Message-ID: <93fe4232-38ab-8047-ff20-59f40e2d7dd0@gmail.com>

Dear Cristabel,
function focal() of package raster is what you are searching for.
focal(x, w = matrix(1/9, ncol = 3, nrow = 3), fun = sum)
or
focal(x, w = matrix(1, ncol = 3, nrow = 3), fun = mean)

HTH,
?kos Bede-Fazekas
Hungarian Academy of Sciences

2019.11.05. 17:10 keltez?ssel, "Cristabel Dur?n Rangel" ?rta:
>
> I need to get the value of a pixel (I have coordinates for this 
> pixels) and its 8 surrounding pixel from the pixel/point. I am working 
> with NetCDF files. I am working with R.
>
> This is my code till now:
>
> nc <- nc_open(file, readunlim=FALSE)
>
> mylon <- ncvar_get(nc,"lon")
> mylat <- ncvar_get(nc,"lat")
>
> my coordinates in real-world: lat 52.5935 lon 18.4467
>
> lat.coor <-mylat[which.min(abs(mylat-52.5935))]
> lon.coor <- mylon[which.min(abs(mylon-18.4467))]
>
> var <- nc[lon.coor, lat.coor, ]
>
> In var are the values for my point. But I also need the values of the 
> 8 surrounding pixels to get an average.
>
> Thanks.
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From b@row||ng@on @end|ng |rom gm@||@com  Wed Nov  6 10:19:53 2019
From: b@row||ng@on @end|ng |rom gm@||@com (Barry Rowlingson)
Date: Wed, 6 Nov 2019 09:19:53 +0000
Subject: [R-sig-Geo] 
 how to get the value of a pixel and its 8 surrounding
 pixels from points?
In-Reply-To: <93fe4232-38ab-8047-ff20-59f40e2d7dd0@gmail.com>
References: <trinity-262183c6-1548-42f5-b76a-c40dd66b2579-1572970218056@3c-app-gmx-bs24>
 <93fe4232-38ab-8047-ff20-59f40e2d7dd0@gmail.com>
Message-ID: <CANVKczOOg8AeCcrFYQfFLQ+x4bp9uAOBU4rWYi4qD7jfS-BoVA@mail.gmail.com>

On Wed, Nov 6, 2019 at 6:18 AM Bede-Fazekas ?kos <bfalevlist at gmail.com>
wrote:

> Dear Cristabel,
> function focal() of package raster is what you are searching for.
>

No it isn't. That operates over the whole raster, which is massive overkill
for querying the eight pixels around a single point.


>
> > var <- nc[lon.coor, lat.coor, ]
> >
> > In var are the values for my point. But I also need the values of the
> > 8 surrounding pixels to get an average.
>
>
The 8 pixels around X[i,j] are:

X[i-1,j-1], X[i, j-1], X[i+1,j-1]
X[i-1,j], X[i+1,j]
X[i-1,j+1], X[i,j+1], X[i+1,j+1]

as long as those cells aren't now outside the matrix.

So write a function that generates those offsets given i,j:

 > n8 =
function(i,j){cbind(i+c(-1,0,1,-1,1,-1,0,1),j+c(-1,-1,-1,0,0,1,1,1))}

which returns this:

 > n8(23,12)
     [,1] [,2]
[1,]   22   11
[2,]   23   11
[3,]   24   11
[4,]   22   12
[5,]   24   12
[6,]   22   13
[7,]   23   13
[8,]   24   13

Then for a test matrix:

> m = matrix(1:10000,100,100)

where i,j = 23, 12:

> m[23,12]
[1] 1123

the 8 neighbours are:

> m[n8(23,12)]
[1] 1022 1023 1024 1122 1124 1222 1223 1224
>

Now I'm not sure if you can do exactly that with your netcdf object because
netcdf is a structured flexible format, and you've shown `nc[lon, lat, ]`
where `nc` is your netcdf object rather than a variable got from it and I
can't get that to work for my test NCDF. (I have sea surface temp in an
netcdf which I get with `sst = nc_getvar(nc, "sst")` for example). But the
mechanism is the same - compute the index of the 8 neighbour pixels and
extract.

	[[alternative HTML version deleted]]


From md@umner @end|ng |rom gm@||@com  Wed Nov  6 10:48:24 2019
From: md@umner @end|ng |rom gm@||@com (Michael Sumner)
Date: Wed, 6 Nov 2019 20:48:24 +1100
Subject: [R-sig-Geo] 
 how to get the value of a pixel and its 8 surrounding
 pixels from points?
In-Reply-To: <93fe4232-38ab-8047-ff20-59f40e2d7dd0@gmail.com>
References: <trinity-262183c6-1548-42f5-b76a-c40dd66b2579-1572970218056@3c-app-gmx-bs24>
 <93fe4232-38ab-8047-ff20-59f40e2d7dd0@gmail.com>
Message-ID: <CAAcGz98aaFsnaNx6=vDDmKgj5yK2orFE=Wh8x8geDTAMOzTYow@mail.gmail.com>

In raster try   cellFromXY() and adjacent()  which separates out the two
tasks, find a cell and find cell neighbours

On Wed., 6 Nov. 2019, 17:18 Bede-Fazekas ?kos, <bfalevlist at gmail.com> wrote:

> Dear Cristabel,
> function focal() of package raster is what you are searching for.
> focal(x, w = matrix(1/9, ncol = 3, nrow = 3), fun = sum)
> or
> focal(x, w = matrix(1, ncol = 3, nrow = 3), fun = mean)
>
> HTH,
> ?kos Bede-Fazekas
> Hungarian Academy of Sciences
>
> 2019.11.05. 17:10 keltez?ssel, "Cristabel Dur?n Rangel" ?rta:
> >
> > I need to get the value of a pixel (I have coordinates for this
> > pixels) and its 8 surrounding pixel from the pixel/point. I am working
> > with NetCDF files. I am working with R.
> >
> > This is my code till now:
> >
> > nc <- nc_open(file, readunlim=FALSE)
> >
> > mylon <- ncvar_get(nc,"lon")
> > mylat <- ncvar_get(nc,"lat")
> >
> > my coordinates in real-world: lat 52.5935 lon 18.4467
> >
> > lat.coor <-mylat[which.min(abs(mylat-52.5935))]
> > lon.coor <- mylon[which.min(abs(mylon-18.4467))]
> >
> > var <- nc[lon.coor, lat.coor, ]
> >
> > In var are the values for my point. But I also need the values of the
> > 8 surrounding pixels to get an average.
> >
> > Thanks.
> >
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Nov  6 15:07:59 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 6 Nov 2019 15:07:59 +0100
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>

On Tue, 5 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your reply. I disabled HTML; my e-mails should be now in 
> plain text.
>
> I will give a better context for my desired outcome.
>
> I am taking Airbnb's listings information for New York City available 
> on: http://insideairbnb.com/get-the-data.html
>
> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09) 
> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a 
> Listings/ folder. When importing all these 54 files into one single data 
> set, I create a new "date_compiled" variable/column.
>
> In total, after the data cleansing process, I have a little more 2 
> million observations.

You have repeat lettings for some, but not all properties. So this is at 
best a very unbalanced panel. For those properties with repeats, you may 
see temporal movement (trend/seasonal).

I suggest (strongly) taking a single borough or even zipcode with some 
hindreds of properties, and working from there. Do not include the 
observation as its own neighbour, perhaps identify repeats and handle them 
specially (create or use a property ID). Unbalanced panels may also create 
a selection bias issue (why are some properties only listed sometimes?).

So this although promising isn't simple, and getting to a hedonic model 
may be hard, but not (just) because of spatial autocorrelation. I wouldn't 
necessarily trust OLS output either, partly because of the repeat property 
issue.

Roger

>
> I created 54 timedummy variables for each time period available.
>
> I want to estimate using a hedonic spatial timedummy model the impact of 
> a variety of characteristics which potentially determine the daily rate 
> on Airbnb listings through time in New York City (e.g. characteristics 
> of the listing as number of bedrooms, if the host if professional, 
> proximity to downtown (New York City Hall) and nearest subway station 
> from the listing, income per capita, etc.).
>
> My dependent variable is price (log price, common in the related 
> literature for hedonic prices).
>
> The OLS model is done.
>
> For the spatial model, I am assuming that hosts, when deciding the 
> pricing of their listings, take not only into account its structural and 
> location characteristics, but also the prices charged by near listings 
> with similar characteristics - spatial autocorrelation is then present, 
> at least spatial dependence is present in the dependent variable.
>
> As I wrote in my previous post, I was willing to consider the neighbor 
> itself as a neighbor.
>
> Parts of my code can be found below:
>
> ########
>
> ## packages
>
> packages_install <- function(packages){
> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
> if (length(new.packages))
> install.packages(new.packages, dependencies = TRUE)
> sapply(packages, require, character.only = TRUE)
> }
>
> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
> packages_install(packages_required)
>
> # Working directory
> setwd("C:/Users/User/R")
>
>
>
> ## shapefile_us
>
> # Shapefile zips import and Coordinate Reference System (CRS) transformation
> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>
> # Columns removal
> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>
> # Column rename: ZCTA5CE10
> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>
> # Column class change: zipcode
> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>
>
>
> ## polygon_nyc
>
> # Zip code not available in shapefile: 11695
> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>
>
>
> ## weight_matrix
>
> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>
> # Include neighbour itself as a neighbour
> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>
> # Weights to each neighboring polygon
> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>
>
>
> ## listings
>
> # Data import
> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
> listings <- mapply(cbind, listings, date_compiled = names(listings))
> listings <- listings %>% bind_rows
>
> # Characters removal
> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
> listings$price <- gsub("\\$", "", listings$price)
> listings$price <- gsub(",", "", listings$price)
>
>
>
> ## timedummy
>
> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
> timedummy <- paste(timedummy, sep = "", collapse = " + ")
> timedummy <- gsub("-", "_", timedummy)
>
>
>
> ## OLS regression
>
> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>
> ########
>
> Some of my id's repeat in multiple time periods.
>
> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>
> Now I want to apply the hedonic model with the timedummy variables.
>
> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>
> Again, thank you very much for the help provided until now.
>
> Best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Tuesday, November 5, 2019 15:30
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Tue, 5 Nov 2019, Robert R wrote:
>
>> I have a large pooled cross-section data set. ?I would like to
>> estimate/regress using spatial autocorrelation methods. I am assuming
>> for now that spatial dependence is present in both the dependent
>> variable and the error term.? ?My data set is over a period of 4 years,
>> monthly data (54 periods). For this means, I've created a time dummy
>> variable for each time period.? ?I also created a weight matrix using the
>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>> to estimate my model which contains a really big data set.? ?Basically, my
>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>> My spatial weight matrix for the whole data set will be probably a
>> enormous matrix with submatrices for each time period itself. I don't
>> think it would be possible to calculate this.? What I would like to know
>> is a way to estimate each time dummy/period separately (to compare
>> different periods alone). How to do it?? ?2) Which package to use: spdep
>> or splm?? ?Thank you and best regards,? Robert?
>
> Please do not post HTML, only plain text. Almost certainly your model
> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
> untried). What is your cross-sectional size? Using sparse kronecker
> products, the "enormous" matrix may not be very big. Does it make any
> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
> of the covariates time-varying? Please provide motivation and use area
> (preferably with affiliation (your email and user name are not
> informative) - this feels like a real estate problem, probably wrongly
> specified. You should use splm if time make sense in your case, but if it
> really doesn't, simplify your approach, as much of the data will be
> subject to very large temporal autocorrelation.
>
> If this is a continuation of your previous question about using
> self-neighbours, be aware that you should not use self-neighbours in
> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>
> Roger
>
>>
>>       [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From JOSHUA@FRENCH @end|ng |rom UCDENVER@EDU  Thu Nov  7 00:22:51 2019
From: JOSHUA@FRENCH @end|ng |rom UCDENVER@EDU (French, Joshua)
Date: Wed, 6 Nov 2019 23:22:51 +0000
Subject: [R-sig-Geo] Universal Block Kriging with SpatialPolygons
Message-ID: <CY1PR05MB2636B35011238991AC2BF00488790@CY1PR05MB2636.namprd05.prod.outlook.com>

Is it possible to use the predict or gstat function in the gstat package to do universal block kriging with SpatialPolygons where the universal kriging covariates are spatial coordinates?  The relevant covariates must be available for any point within the SpatialPolygon, which effectively limits to covariates to being the spatial coordinates.  Even with the restriction of coordinate-based covariates, I can't get this to work for *universal* kriging (it works fine for ordinary kriging).

The example below illustrates the problem using a reproducible example.  I think the issue is that the coordinates within each Polygons object inside the SpatialPolygons object do not having coordinate names matching those of the observed data set.  However, I can't seem to figure out a way to give them the desired name.  Is there a simple way to overcome this problem?  I do have a hack to get the predictions I need, but it certainly feels like there is a simpler approach that eludes me.

Thanks!

library(gstat)
library(sp)
data(meuse)
coordinates(meuse) = ~x+y
g = gstat(formula = log(zinc) ~ x + y, data = meuse, model = vgm(1, "Sph", 300, 1))

data(meuse.grid)
gridded(meuse.grid) = ~x+y

# typical prediction on a grid (no problem)
p = predict(g, newdata = meuse.grid)

# create a SpatialPolygons object for prediction
sp_polys = SpatialPolygons
p1 = cbind(c(179000, 179000, 179500, 179600),
           c(330000, 331000, 331000, 330000))
Poly1 = Polygon(p1)
p2 = cbind(c(179600, 179500, 180100, 180000),
           c(330000, 331000, 331000, 330000))
Poly2 = Polygon(p2)
ps1 = Polygons(list(Poly1), 1)
ps2 = Polygons(list(Poly2), 2)
sps = SpatialPolygons(list(ps1, ps2))

predict(g, newdata = sps)
# Error in eval(predvars, data, env) : object 'x' not found
--
Joshua French
Department of Mathematical and Statistical Sciences
University of Colorado Denver


	[[alternative HTML version deleted]]


From u@erc@tch @end|ng |rom out|ook@com  Thu Nov  7 02:13:18 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Thu, 7 Nov 2019 01:13:18 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
Message-ID: <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Roger,

Many thanks for your help.

I have an additional question:

Is it possible to create a "separate" lw (nb2listw) (with different rownumbers) from my data set? For now, I am taking my data set and merging with the sf object polygon_nyc with the function "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create a huge n x n matrix (depending of the size of my data set).

Taking the polygon_nyc alone and turning it to a lw (weights list) object has only n = 177.

Of course running

spatialreg::lagsarlm(formula=model, data = listings_sample, polygon_nyc_lw, tol.solve=1.0e-10)

does not work ("Input data and weights have different dimensions").

The only option is to take my data set, merge it to my polygon_nyc (by zipcode) and then create the weights list lw? Or there another option?

Best regards,
Robert

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Wednesday, November 6, 2019 15:07
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

On Tue, 5 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your reply. I disabled HTML; my e-mails should be now in
> plain text.
>
> I will give a better context for my desired outcome.
>
> I am taking Airbnb's listings information for New York City available
> on: http://insideairbnb.com/get-the-data.html
>
> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
> Listings/ folder. When importing all these 54 files into one single data
> set, I create a new "date_compiled" variable/column.
>
> In total, after the data cleansing process, I have a little more 2
> million observations.

You have repeat lettings for some, but not all properties. So this is at
best a very unbalanced panel. For those properties with repeats, you may
see temporal movement (trend/seasonal).

I suggest (strongly) taking a single borough or even zipcode with some
hindreds of properties, and working from there. Do not include the
observation as its own neighbour, perhaps identify repeats and handle them
specially (create or use a property ID). Unbalanced panels may also create
a selection bias issue (why are some properties only listed sometimes?).

So this although promising isn't simple, and getting to a hedonic model
may be hard, but not (just) because of spatial autocorrelation. I wouldn't
necessarily trust OLS output either, partly because of the repeat property
issue.

Roger

>
> I created 54 timedummy variables for each time period available.
>
> I want to estimate using a hedonic spatial timedummy model the impact of
> a variety of characteristics which potentially determine the daily rate
> on Airbnb listings through time in New York City (e.g. characteristics
> of the listing as number of bedrooms, if the host if professional,
> proximity to downtown (New York City Hall) and nearest subway station
> from the listing, income per capita, etc.).
>
> My dependent variable is price (log price, common in the related
> literature for hedonic prices).
>
> The OLS model is done.
>
> For the spatial model, I am assuming that hosts, when deciding the
> pricing of their listings, take not only into account its structural and
> location characteristics, but also the prices charged by near listings
> with similar characteristics - spatial autocorrelation is then present,
> at least spatial dependence is present in the dependent variable.
>
> As I wrote in my previous post, I was willing to consider the neighbor
> itself as a neighbor.
>
> Parts of my code can be found below:
>
> ########
>
> ## packages
>
> packages_install <- function(packages){
> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
> if (length(new.packages))
> install.packages(new.packages, dependencies = TRUE)
> sapply(packages, require, character.only = TRUE)
> }
>
> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
> packages_install(packages_required)
>
> # Working directory
> setwd("C:/Users/User/R")
>
>
>
> ## shapefile_us
>
> # Shapefile zips import and Coordinate Reference System (CRS) transformation
> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>
> # Columns removal
> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>
> # Column rename: ZCTA5CE10
> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>
> # Column class change: zipcode
> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>
>
>
> ## polygon_nyc
>
> # Zip code not available in shapefile: 11695
> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>
>
>
> ## weight_matrix
>
> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>
> # Include neighbour itself as a neighbour
> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>
> # Weights to each neighboring polygon
> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>
>
>
> ## listings
>
> # Data import
> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
> listings <- mapply(cbind, listings, date_compiled = names(listings))
> listings <- listings %>% bind_rows
>
> # Characters removal
> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
> listings$price <- gsub("\\$", "", listings$price)
> listings$price <- gsub(",", "", listings$price)
>
>
>
> ## timedummy
>
> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
> timedummy <- paste(timedummy, sep = "", collapse = " + ")
> timedummy <- gsub("-", "_", timedummy)
>
>
>
> ## OLS regression
>
> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>
> ########
>
> Some of my id's repeat in multiple time periods.
>
> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>
> Now I want to apply the hedonic model with the timedummy variables.
>
> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>
> Again, thank you very much for the help provided until now.
>
> Best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Tuesday, November 5, 2019 15:30
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Tue, 5 Nov 2019, Robert R wrote:
>
>> I have a large pooled cross-section data set. ?I would like to
>> estimate/regress using spatial autocorrelation methods. I am assuming
>> for now that spatial dependence is present in both the dependent
>> variable and the error term.? ?My data set is over a period of 4 years,
>> monthly data (54 periods). For this means, I've created a time dummy
>> variable for each time period.? ?I also created a weight matrix using the
>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>> to estimate my model which contains a really big data set.? ?Basically, my
>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>> My spatial weight matrix for the whole data set will be probably a
>> enormous matrix with submatrices for each time period itself. I don't
>> think it would be possible to calculate this.? What I would like to know
>> is a way to estimate each time dummy/period separately (to compare
>> different periods alone). How to do it?? ?2) Which package to use: spdep
>> or splm?? ?Thank you and best regards,? Robert?
>
> Please do not post HTML, only plain text. Almost certainly your model
> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
> untried). What is your cross-sectional size? Using sparse kronecker
> products, the "enormous" matrix may not be very big. Does it make any
> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
> of the covariates time-varying? Please provide motivation and use area
> (preferably with affiliation (your email and user name are not
> informative) - this feels like a real estate problem, probably wrongly
> specified. You should use splm if time make sense in your case, but if it
> really doesn't, simplify your approach, as much of the data will be
> subject to very large temporal autocorrelation.
>
> If this is a continuation of your previous question about using
> self-neighbours, be aware that you should not use self-neighbours in
> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>
> Roger
>
>>
>>       [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From r@i@1290 m@iii@g oii @im@com  Thu Nov  7 03:18:31 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Thu, 7 Nov 2019 02:18:31 +0000 (UTC)
Subject: [R-sig-Geo] Isolating only land areas on a global map for computing
 averages
References: <2098265776.786417.1573093111101.ref@mail.yahoo.com>
Message-ID: <2098265776.786417.1573093111101@mail.yahoo.com>

Hi there,
I am interested in calculating precipitation medians globally. However, I only want to isolate land areas to compute the median. I already first created a raster stack, called "RCP1pctCO2median", which contains the median values.?There are 138 layers, with each layer representing one year.? This raster stack has the following attributes:
class?????? : RasterStack 
dimensions? : 64, 128, 8192, 138? (nrow, ncol, ncell, nlayers)
resolution? : 2.8125, 2.789327? (x, y)
extent????? : -181.4062, 178.5938, -89.25846, 89.25846? (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
names?????? :??? layer.1,??? layer.2,??? layer.3,??? layer.4,??? layer.5,??? layer.6,??? layer.7,??? layer.8,??? layer.9,?? layer.10,?? layer.11,?? layer.12,?? layer.13,?? layer.14,?? layer.15, ... 
min values? : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730, 0.53099146, 0.49757186, 0.45697752, 0.41382199, 0.46082401, 0.45516687, 0.51857087, 0.41005131, 0.45956529, 0.47497867, ... 
max values? :?? 96.30350,? 104.08584,?? 88.92751,?? 97.49373,?? 89.57201,?? 90.58570,?? 86.67651,?? 88.33519,?? 96.94720,? 101.58247,?? 96.07792,?? 93.21948,?? 99.59785,?? 94.26218,?? 90.62138, ...??

Previously, I was isolating a specific region by specifying a range of longitudes and latitudes to obtain the medians for that region, like this:
expansion1<-expand.grid(103:120, 3:15)lonlataaa<-extract(RCP1pctCO2Median, expansion1)Columnaaa<-colMeans(lonlataaa)

However, with this approach, too much water can mix with land areas, and if I narrow the latitude/longitude range on land, I might miss too much land to compute the median meaningfully.
Therefore, with this data, would it be possible to use an IF/ELSE statement to tell R that if the "center point" of each grid cell happens to fall on land, then it would be considered as land (i.e. that would be TRUE - if not, then FALSE)? Even if a grid cell happens to have water mixed with land, but the center point of the grid is on land, that would be considered land. But can R even tell what is land or water in this case?
Thank you, and I would greatly appreciate any assistance!

	[[alternative HTML version deleted]]


From teph|||pp| @end|ng |rom gm@||@com  Thu Nov  7 06:44:17 2019
From: teph|||pp| @end|ng |rom gm@||@com (Tom Philippi)
Date: Wed, 6 Nov 2019 21:44:17 -0800
Subject: [R-sig-Geo] 
 Isolating only land areas on a global map for computing averages
In-Reply-To: <2098265776.786417.1573093111101@mail.yahoo.com>
References: <2098265776.786417.1573093111101.ref@mail.yahoo.com>
 <2098265776.786417.1573093111101@mail.yahoo.com>
Message-ID: <CALyPt8zmd78Vfm_Av3mv63z=12s4bs7G53JvfddKZE+4v29GrA@mail.gmail.com>

The easiest approach would be to create a separate aligned raster layer for
land vs water.  There are plenty of coastline polygons available out there
(e.g., maptools, rworldmap, rnaturalearth packages): choose one in your
raster CRS (or choose one and spTransform() it).  Then, use a grid version
of your raster to extract values from that land/water SpatialPolygons
object.

1: Your idea of extracting the land/water value at each grid cell centroid
gives one estimate.  Instead of TRUE/FALSE, think of the numeric
equivalents 1,0,  then using those as weights for averaging across your
grid cells.
2: A "better" estimate would be to compute the fraction of each grid cell
that is land, then use those fractional [0, 1] values as weights to compute
a weighted average of precipitation over land.  At 2.8deg grid cells, a lot
of heavy rainfall coastal areas would have the grid cell centroid offshore
and be omitted by approach #1.
3: I recommend that you think hard about averaging across cells in Lat Lon
to estimate average precipitation over land.  The actual area of a ~2.8 by
2.8 deg grid cell at the equator is much larger than the same at 70 deg N.
I would spend the extra hour computing the actual area (in km^2) of land in
each of your 8192 grid cells, then using those in a raster as weights for
whatever calculations you do on the raster stack.  [Or you can cheat, as
the area of a grid cell in degrees is a function of only the latitudes, and
your required weights are multiplicative.]

Your mileage may vary...

Tom

On Wed, Nov 6, 2019 at 6:18 PM rain1290--- via R-sig-Geo <
r-sig-geo at r-project.org> wrote:

> Hi there,
> I am interested in calculating precipitation medians globally. However, I
> only want to isolate land areas to compute the median. I already first
> created a raster stack, called "RCP1pctCO2median", which contains the
> median values. There are 138 layers, with each layer representing one
> year.  This raster stack has the following attributes:
> class       : RasterStack
> dimensions  : 64, 128, 8192, 138  (nrow, ncol, ncell, nlayers)
> resolution  : 2.8125, 2.789327  (x, y)
> extent      : -181.4062, 178.5938, -89.25846, 89.25846  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> names       :    layer.1,    layer.2,    layer.3,    layer.4,
> layer.5,    layer.6,    layer.7,    layer.8,    layer.9,   layer.10,
> layer.11,   layer.12,   layer.13,   layer.14,   layer.15, ...
> min values  : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730,
> 0.53099146, 0.49757186, 0.45697752, 0.41382199, 0.46082401, 0.45516687,
> 0.51857087, 0.41005131, 0.45956529, 0.47497867, ...
> max values  :   96.30350,  104.08584,   88.92751,   97.49373,
> 89.57201,   90.58570,   86.67651,   88.33519,   96.94720,  101.58247,
> 96.07792,   93.21948,   99.59785,   94.26218,   90.62138, ...
>
> Previously, I was isolating a specific region by specifying a range of
> longitudes and latitudes to obtain the medians for that region, like this:
> expansion1<-expand.grid(103:120, 3:15)lonlataaa<-extract(RCP1pctCO2Median,
> expansion1)Columnaaa<-colMeans(lonlataaa)
>
> However, with this approach, too much water can mix with land areas, and
> if I narrow the latitude/longitude range on land, I might miss too much
> land to compute the median meaningfully.
> Therefore, with this data, would it be possible to use an IF/ELSE
> statement to tell R that if the "center point" of each grid cell happens to
> fall on land, then it would be considered as land (i.e. that would be TRUE
> - if not, then FALSE)? Even if a grid cell happens to have water mixed with
> land, but the center point of the grid is on land, that would be considered
> land. But can R even tell what is land or water in this case?
> Thank you, and I would greatly appreciate any assistance!
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Nov  7 10:13:33 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 7 Nov 2019 10:13:33 +0100
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>

On Thu, 7 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Many thanks for your help.
>
> I have an additional question:
>
> Is it possible to create a "separate" lw (nb2listw) (with different 
> rownumbers) from my data set? For now, I am taking my data set and 
> merging with the sf object polygon_nyc with the function 
> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create 
> a huge n x n matrix (depending of the size of my data set).
>
> Taking the polygon_nyc alone and turning it to a lw (weights list) 
> object has only n = 177.
>
> Of course running
>
> spatialreg::lagsarlm(formula=model, data = listings_sample, 
> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>
> does not work ("Input data and weights have different dimensions").
>
> The only option is to take my data set, merge it to my polygon_nyc (by 
> zipcode) and then create the weights list lw? Or there another option?

I think we are getting more clarity. You do not know the location of the 
lettings beyond their zipcode. You do know the boundaries of the zipcode 
areas, and can create a neighbour object from these boundaries. You then 
want to treat all the lettings in a zipcode area i as neighbours, and 
additionally lettings in zipcode areas neighbouring i as neighbours of 
lettings in i. This is the data structure that motivated the 
spdep::nb2blocknb() function:

https://r-spatial.github.io/spdep/reference/nb2blocknb.html

Try running the examples to get a feel for what is going on.

I feel that most of the variability will vanish in the very large numbers 
of neighbours, over-smoothing the outcomes. If you do not have locations 
for the lettings themselves, I don't think you can make much progress.

You could try a linear mixed model (or gam with a spatially structured 
random effect) with a temporal and a spatial random effect. See the HSAR 
package, articles by Dong et al., and maybe 
https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither 
this nor Dong et al. handle spatio-temporal settings. MRF spatial random 
effects at the zipcode level might be a way forward, together with an IID 
random effect at the same level (equivalent to sef-neighbours).

Hope this helps,

Roger

>
> Best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Wednesday, November 6, 2019 15:07
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Tue, 5 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>> plain text.
>>
>> I will give a better context for my desired outcome.
>>
>> I am taking Airbnb's listings information for New York City available
>> on: http://insideairbnb.com/get-the-data.html
>>
>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>> Listings/ folder. When importing all these 54 files into one single data
>> set, I create a new "date_compiled" variable/column.
>>
>> In total, after the data cleansing process, I have a little more 2
>> million observations.
>
> You have repeat lettings for some, but not all properties. So this is at
> best a very unbalanced panel. For those properties with repeats, you may
> see temporal movement (trend/seasonal).
>
> I suggest (strongly) taking a single borough or even zipcode with some
> hindreds of properties, and working from there. Do not include the
> observation as its own neighbour, perhaps identify repeats and handle them
> specially (create or use a property ID). Unbalanced panels may also create
> a selection bias issue (why are some properties only listed sometimes?).
>
> So this although promising isn't simple, and getting to a hedonic model
> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
> necessarily trust OLS output either, partly because of the repeat property
> issue.
>
> Roger
>
>>
>> I created 54 timedummy variables for each time period available.
>>
>> I want to estimate using a hedonic spatial timedummy model the impact of
>> a variety of characteristics which potentially determine the daily rate
>> on Airbnb listings through time in New York City (e.g. characteristics
>> of the listing as number of bedrooms, if the host if professional,
>> proximity to downtown (New York City Hall) and nearest subway station
>> from the listing, income per capita, etc.).
>>
>> My dependent variable is price (log price, common in the related
>> literature for hedonic prices).
>>
>> The OLS model is done.
>>
>> For the spatial model, I am assuming that hosts, when deciding the
>> pricing of their listings, take not only into account its structural and
>> location characteristics, but also the prices charged by near listings
>> with similar characteristics - spatial autocorrelation is then present,
>> at least spatial dependence is present in the dependent variable.
>>
>> As I wrote in my previous post, I was willing to consider the neighbor
>> itself as a neighbor.
>>
>> Parts of my code can be found below:
>>
>> ########
>>
>> ## packages
>>
>> packages_install <- function(packages){
>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>> if (length(new.packages))
>> install.packages(new.packages, dependencies = TRUE)
>> sapply(packages, require, character.only = TRUE)
>> }
>>
>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>> packages_install(packages_required)
>>
>> # Working directory
>> setwd("C:/Users/User/R")
>>
>>
>>
>> ## shapefile_us
>>
>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>
>> # Columns removal
>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>
>> # Column rename: ZCTA5CE10
>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>
>> # Column class change: zipcode
>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>
>>
>>
>> ## polygon_nyc
>>
>> # Zip code not available in shapefile: 11695
>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>
>>
>>
>> ## weight_matrix
>>
>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>
>> # Include neighbour itself as a neighbour
>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>
>> # Weights to each neighboring polygon
>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>
>>
>>
>> ## listings
>>
>> # Data import
>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>> listings <- listings %>% bind_rows
>>
>> # Characters removal
>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>> listings$price <- gsub("\\$", "", listings$price)
>> listings$price <- gsub(",", "", listings$price)
>>
>>
>>
>> ## timedummy
>>
>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>> timedummy <- gsub("-", "_", timedummy)
>>
>>
>>
>> ## OLS regression
>>
>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>
>> ########
>>
>> Some of my id's repeat in multiple time periods.
>>
>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>
>> Now I want to apply the hedonic model with the timedummy variables.
>>
>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>
>> Again, thank you very much for the help provided until now.
>>
>> Best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Tuesday, November 5, 2019 15:30
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Tue, 5 Nov 2019, Robert R wrote:
>>
>>> I have a large pooled cross-section data set. ?I would like to
>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>> for now that spatial dependence is present in both the dependent
>>> variable and the error term.? ?My data set is over a period of 4 years,
>>> monthly data (54 periods). For this means, I've created a time dummy
>>> variable for each time period.? ?I also created a weight matrix using the
>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>> to estimate my model which contains a really big data set.? ?Basically, my
>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>> My spatial weight matrix for the whole data set will be probably a
>>> enormous matrix with submatrices for each time period itself. I don't
>>> think it would be possible to calculate this.? What I would like to know
>>> is a way to estimate each time dummy/period separately (to compare
>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>> or splm?? ?Thank you and best regards,? Robert?
>>
>> Please do not post HTML, only plain text. Almost certainly your model
>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>> untried). What is your cross-sectional size? Using sparse kronecker
>> products, the "enormous" matrix may not be very big. Does it make any
>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>> of the covariates time-varying? Please provide motivation and use area
>> (preferably with affiliation (your email and user name are not
>> informative) - this feels like a real estate problem, probably wrongly
>> specified. You should use splm if time make sense in your case, but if it
>> really doesn't, simplify your approach, as much of the data will be
>> subject to very large temporal autocorrelation.
>>
>> If this is a continuation of your previous question about using
>> self-neighbours, be aware that you should not use self-neighbours in
>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>
>> Roger
>>
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From per@|e@@geom@r @end|ng |rom gm@||@com  Thu Nov  7 16:55:21 2019
From: per@|e@@geom@r @end|ng |rom gm@||@com (Geomar Perales)
Date: Thu, 7 Nov 2019 10:55:21 -0500
Subject: [R-sig-Geo] It's possible delimit a shapefile very extent using R?
Message-ID: <CAD-TP6khqe--3Y3BA4tpEyEuF+C+gUFOsSs2udwc6B+7Tfc3Bw@mail.gmail.com>

Hi

I need ideas for see if is possible delimit a shapefile very extent using R
or maybe any other language (I add a image of the shape).

thank for your time
Geomar

[image: image.png]

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191107/f3fdc2a5/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 28644 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191107/f3fdc2a5/attachment.png>

From r@i@1290 m@iii@g oii @im@com  Thu Nov  7 17:25:10 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Thu, 7 Nov 2019 16:25:10 +0000 (UTC)
Subject: [R-sig-Geo] 
 Isolating only land areas on a global map for computing averages
In-Reply-To: <CALyPt8zmd78Vfm_Av3mv63z=12s4bs7G53JvfddKZE+4v29GrA@mail.gmail.com>
References: <2098265776.786417.1573093111101.ref@mail.yahoo.com>
 <2098265776.786417.1573093111101@mail.yahoo.com>
 <CALyPt8zmd78Vfm_Av3mv63z=12s4bs7G53JvfddKZE+4v29GrA@mail.gmail.com>
Message-ID: <1931301500.1089634.1573143910861@mail.yahoo.com>

Hi Tom and others,

Thank you for your response and suggestions!?
Yes, I loaded and used the maptools package previously to create continents on my world map, among other things. I do think that the easiest approach would be to create a raster layer for land, and then water, with the values that I have. However, my precipitation values are globally distributed - every grid cell has a precipitation value for each year (i.e. each grid cell has 138 values/layers/years). So, if I were to create a raster of only land areas, how would I have my grid cells coincide with the land areas only on that raster?
Also, would it be possible to accomplish this with the raster stack that I already have? If so, is there a way to separate all land/water areas this way using the maptools package?
Thanks, again, and I really appreciate your help!
-----Original Message-----
From: Tom Philippi <tephilippi at gmail.com>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Thu, Nov 7, 2019 12:44 am
Subject: Re: [R-sig-Geo] Isolating only land areas on a global map for computing averages

The easiest approach would be to create a separate aligned raster layer for land vs water.? There are plenty of coastline polygons available out there (e.g., maptools, rworldmap, rnaturalearth packages): choose one in your raster CRS (or choose one and spTransform() it).? Then, use a grid version of your raster to extract values from that land/water SpatialPolygons object.
1: Your idea of extracting the land/water value at each grid cell centroid gives one estimate.? Instead of TRUE/FALSE, think of the numeric equivalents 1,0,? then using those as weights for averaging across your grid cells.2: A "better" estimate would be to compute the fraction of each grid cell that is land, then use those fractional [0, 1] values as weights to compute a weighted average of precipitation over land.? At 2.8deg grid cells, a lot of heavy rainfall coastal areas would have the grid cell centroid offshore and be omitted by approach #1.3: I recommend that you think hard about averaging across cells in Lat Lon to estimate average precipitation over land.? The actual area of a ~2.8 by 2.8 deg grid cell at the equator is much larger than the same at 70 deg N.? I would spend the extra hour computing the actual area (in km^2) of land in each of your 8192 grid cells, then using those in a raster as weights for whatever calculations you do on the raster stack.? [Or you can cheat, as the area of a grid cell in degrees is a function of only the latitudes, and your required weights are multiplicative.]
Your mileage may vary...
Tom
On Wed, Nov 6, 2019 at 6:18 PM rain1290--- via R-sig-Geo <r-sig-geo at r-project.org> wrote:

Hi there,
I am interested in calculating precipitation medians globally. However, I only want to isolate land areas to compute the median. I already first created a raster stack, called "RCP1pctCO2median", which contains the median values.?There are 138 layers, with each layer representing one year.? This raster stack has the following attributes:
class?????? : RasterStack 
dimensions? : 64, 128, 8192, 138? (nrow, ncol, ncell, nlayers)
resolution? : 2.8125, 2.789327? (x, y)
extent????? : -181.4062, 178.5938, -89.25846, 89.25846? (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
names?????? :??? layer.1,??? layer.2,??? layer.3,??? layer.4,??? layer.5,??? layer.6,??? layer.7,??? layer.8,??? layer.9,?? layer.10,?? layer.11,?? layer.12,?? layer.13,?? layer.14,?? layer.15, ... 
min values? : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730, 0.53099146, 0.49757186, 0.45697752, 0.41382199, 0.46082401, 0.45516687, 0.51857087, 0.41005131, 0.45956529, 0.47497867, ... 
max values? :?? 96.30350,? 104.08584,?? 88.92751,?? 97.49373,?? 89.57201,?? 90.58570,?? 86.67651,?? 88.33519,?? 96.94720,? 101.58247,?? 96.07792,?? 93.21948,?? 99.59785,?? 94.26218,?? 90.62138, ...??

Previously, I was isolating a specific region by specifying a range of longitudes and latitudes to obtain the medians for that region, like this:
expansion1<-expand.grid(103:120, 3:15)lonlataaa<-extract(RCP1pctCO2Median, expansion1)Columnaaa<-colMeans(lonlataaa)

However, with this approach, too much water can mix with land areas, and if I narrow the latitude/longitude range on land, I might miss too much land to compute the median meaningfully.
Therefore, with this data, would it be possible to use an IF/ELSE statement to tell R that if the "center point" of each grid cell happens to fall on land, then it would be considered as land (i.e. that would be TRUE - if not, then FALSE)? Even if a grid cell happens to have water mixed with land, but the center point of the grid is on land, that would be considered land. But can R even tell what is land or water in this case?
Thank you, and I would greatly appreciate any assistance!

? ? ? ? [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From u@erc@tch @end|ng |rom out|ook@com  Fri Nov  8 03:38:55 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Fri, 8 Nov 2019 02:38:55 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
Message-ID: <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Roger,

Thank you for your answer.

I successfully used the function nb2blocknb() for a smaller dataset.

But for a dataset of over 2 million observations, I get the following error: "Error: cannot allocate vector of size 840 Kb".

I am expecting that at least 500.000 observations will be dropped due the lack of values for the chosen variables for the regression model, so probably I will filter and remove the observations/rows that will not be used anyway - do you know if there is any package that does this automatically, given the variables/columns chosed by me?

Or would you recommend me another approach to avoid the above mentioned error?

Thank you and best regards,
Robert

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Thursday, November 7, 2019 10:13
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

On Thu, 7 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Many thanks for your help.
>
> I have an additional question:
>
> Is it possible to create a "separate" lw (nb2listw) (with different
> rownumbers) from my data set? For now, I am taking my data set and
> merging with the sf object polygon_nyc with the function
> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
> a huge n x n matrix (depending of the size of my data set).
>
> Taking the polygon_nyc alone and turning it to a lw (weights list)
> object has only n = 177.
>
> Of course running
>
> spatialreg::lagsarlm(formula=model, data = listings_sample,
> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>
> does not work ("Input data and weights have different dimensions").
>
> The only option is to take my data set, merge it to my polygon_nyc (by
> zipcode) and then create the weights list lw? Or there another option?

I think we are getting more clarity. You do not know the location of the
lettings beyond their zipcode. You do know the boundaries of the zipcode
areas, and can create a neighbour object from these boundaries. You then
want to treat all the lettings in a zipcode area i as neighbours, and
additionally lettings in zipcode areas neighbouring i as neighbours of
lettings in i. This is the data structure that motivated the
spdep::nb2blocknb() function:

https://r-spatial.github.io/spdep/reference/nb2blocknb.html

Try running the examples to get a feel for what is going on.

I feel that most of the variability will vanish in the very large numbers
of neighbours, over-smoothing the outcomes. If you do not have locations
for the lettings themselves, I don't think you can make much progress.

You could try a linear mixed model (or gam with a spatially structured
random effect) with a temporal and a spatial random effect. See the HSAR
package, articles by Dong et al., and maybe
https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
this nor Dong et al. handle spatio-temporal settings. MRF spatial random
effects at the zipcode level might be a way forward, together with an IID
random effect at the same level (equivalent to sef-neighbours).

Hope this helps,

Roger

>
> Best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Wednesday, November 6, 2019 15:07
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Tue, 5 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>> plain text.
>>
>> I will give a better context for my desired outcome.
>>
>> I am taking Airbnb's listings information for New York City available
>> on: http://insideairbnb.com/get-the-data.html
>>
>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>> Listings/ folder. When importing all these 54 files into one single data
>> set, I create a new "date_compiled" variable/column.
>>
>> In total, after the data cleansing process, I have a little more 2
>> million observations.
>
> You have repeat lettings for some, but not all properties. So this is at
> best a very unbalanced panel. For those properties with repeats, you may
> see temporal movement (trend/seasonal).
>
> I suggest (strongly) taking a single borough or even zipcode with some
> hindreds of properties, and working from there. Do not include the
> observation as its own neighbour, perhaps identify repeats and handle them
> specially (create or use a property ID). Unbalanced panels may also create
> a selection bias issue (why are some properties only listed sometimes?).
>
> So this although promising isn't simple, and getting to a hedonic model
> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
> necessarily trust OLS output either, partly because of the repeat property
> issue.
>
> Roger
>
>>
>> I created 54 timedummy variables for each time period available.
>>
>> I want to estimate using a hedonic spatial timedummy model the impact of
>> a variety of characteristics which potentially determine the daily rate
>> on Airbnb listings through time in New York City (e.g. characteristics
>> of the listing as number of bedrooms, if the host if professional,
>> proximity to downtown (New York City Hall) and nearest subway station
>> from the listing, income per capita, etc.).
>>
>> My dependent variable is price (log price, common in the related
>> literature for hedonic prices).
>>
>> The OLS model is done.
>>
>> For the spatial model, I am assuming that hosts, when deciding the
>> pricing of their listings, take not only into account its structural and
>> location characteristics, but also the prices charged by near listings
>> with similar characteristics - spatial autocorrelation is then present,
>> at least spatial dependence is present in the dependent variable.
>>
>> As I wrote in my previous post, I was willing to consider the neighbor
>> itself as a neighbor.
>>
>> Parts of my code can be found below:
>>
>> ########
>>
>> ## packages
>>
>> packages_install <- function(packages){
>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>> if (length(new.packages))
>> install.packages(new.packages, dependencies = TRUE)
>> sapply(packages, require, character.only = TRUE)
>> }
>>
>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>> packages_install(packages_required)
>>
>> # Working directory
>> setwd("C:/Users/User/R")
>>
>>
>>
>> ## shapefile_us
>>
>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>
>> # Columns removal
>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>
>> # Column rename: ZCTA5CE10
>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>
>> # Column class change: zipcode
>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>
>>
>>
>> ## polygon_nyc
>>
>> # Zip code not available in shapefile: 11695
>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>
>>
>>
>> ## weight_matrix
>>
>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>
>> # Include neighbour itself as a neighbour
>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>
>> # Weights to each neighboring polygon
>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>
>>
>>
>> ## listings
>>
>> # Data import
>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>> listings <- listings %>% bind_rows
>>
>> # Characters removal
>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>> listings$price <- gsub("\\$", "", listings$price)
>> listings$price <- gsub(",", "", listings$price)
>>
>>
>>
>> ## timedummy
>>
>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>> timedummy <- gsub("-", "_", timedummy)
>>
>>
>>
>> ## OLS regression
>>
>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>
>> ########
>>
>> Some of my id's repeat in multiple time periods.
>>
>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>
>> Now I want to apply the hedonic model with the timedummy variables.
>>
>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>
>> Again, thank you very much for the help provided until now.
>>
>> Best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Tuesday, November 5, 2019 15:30
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Tue, 5 Nov 2019, Robert R wrote:
>>
>>> I have a large pooled cross-section data set. ?I would like to
>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>> for now that spatial dependence is present in both the dependent
>>> variable and the error term.? ?My data set is over a period of 4 years,
>>> monthly data (54 periods). For this means, I've created a time dummy
>>> variable for each time period.? ?I also created a weight matrix using the
>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>> to estimate my model which contains a really big data set.? ?Basically, my
>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>> My spatial weight matrix for the whole data set will be probably a
>>> enormous matrix with submatrices for each time period itself. I don't
>>> think it would be possible to calculate this.? What I would like to know
>>> is a way to estimate each time dummy/period separately (to compare
>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>> or splm?? ?Thank you and best regards,? Robert?
>>
>> Please do not post HTML, only plain text. Almost certainly your model
>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>> untried). What is your cross-sectional size? Using sparse kronecker
>> products, the "enormous" matrix may not be very big. Does it make any
>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>> of the covariates time-varying? Please provide motivation and use area
>> (preferably with affiliation (your email and user name are not
>> informative) - this feels like a real estate problem, probably wrongly
>> specified. You should use splm if time make sense in your case, but if it
>> really doesn't, simplify your approach, as much of the data will be
>> subject to very large temporal autocorrelation.
>>
>> If this is a continuation of your previous question about using
>> self-neighbours, be aware that you should not use self-neighbours in
>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>
>> Roger
>>
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger@B|v@nd @end|ng |rom nhh@no  Fri Nov  8 13:29:08 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Fri, 8 Nov 2019 13:29:08 +0100
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>

On Fri, 8 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your answer.
>
> I successfully used the function nb2blocknb() for a smaller dataset.
>
> But for a dataset of over 2 million observations, I get the following 
> error: "Error: cannot allocate vector of size 840 Kb".

I don't think the observations are helpful. If you have repeat lets in the 
same property in a given month, you need to handle that anyway. I'd go for 
making the modelling exercise work (we agree that this is not panel data, 
right?) on a small subset first. I would further argue that you need a 
multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID 
RE. You could very well take (stratified) samples per zipcode to represent 
your data. Once that works, introduce an MRF RE at the zipcode level, 
where you do know relative position. Using SARAR is going to be a waste of 
time unless you can geocode the letting addresses. A multi-level approach 
will work. Having big data in your case with no useful location 
information per observation is just adding noise and over-smoothing, I'm 
afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002 
will work, also when you sample the within zipcode lets, given a split 
into training and test sets, and making CV possible.

Roger

>
> I am expecting that at least 500.000 observations will be dropped due 
> the lack of values for the chosen variables for the regression model, so 
> probably I will filter and remove the observations/rows that will not be 
> used anyway - do you know if there is any package that does this 
> automatically, given the variables/columns chosed by me?
>
> Or would you recommend me another approach to avoid the above mentioned 
> error?
>
> Thank you and best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Thursday, November 7, 2019 10:13
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Thu, 7 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Many thanks for your help.
>>
>> I have an additional question:
>>
>> Is it possible to create a "separate" lw (nb2listw) (with different
>> rownumbers) from my data set? For now, I am taking my data set and
>> merging with the sf object polygon_nyc with the function
>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>> a huge n x n matrix (depending of the size of my data set).
>>
>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>> object has only n = 177.
>>
>> Of course running
>>
>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>
>> does not work ("Input data and weights have different dimensions").
>>
>> The only option is to take my data set, merge it to my polygon_nyc (by
>> zipcode) and then create the weights list lw? Or there another option?
>
> I think we are getting more clarity. You do not know the location of the
> lettings beyond their zipcode. You do know the boundaries of the zipcode
> areas, and can create a neighbour object from these boundaries. You then
> want to treat all the lettings in a zipcode area i as neighbours, and
> additionally lettings in zipcode areas neighbouring i as neighbours of
> lettings in i. This is the data structure that motivated the
> spdep::nb2blocknb() function:
>
> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>
> Try running the examples to get a feel for what is going on.
>
> I feel that most of the variability will vanish in the very large numbers
> of neighbours, over-smoothing the outcomes. If you do not have locations
> for the lettings themselves, I don't think you can make much progress.
>
> You could try a linear mixed model (or gam with a spatially structured
> random effect) with a temporal and a spatial random effect. See the HSAR
> package, articles by Dong et al., and maybe
> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
> effects at the zipcode level might be a way forward, together with an IID
> random effect at the same level (equivalent to sef-neighbours).
>
> Hope this helps,
>
> Roger
>
>>
>> Best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Wednesday, November 6, 2019 15:07
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Tue, 5 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>> plain text.
>>>
>>> I will give a better context for my desired outcome.
>>>
>>> I am taking Airbnb's listings information for New York City available
>>> on: http://insideairbnb.com/get-the-data.html
>>>
>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>> Listings/ folder. When importing all these 54 files into one single data
>>> set, I create a new "date_compiled" variable/column.
>>>
>>> In total, after the data cleansing process, I have a little more 2
>>> million observations.
>>
>> You have repeat lettings for some, but not all properties. So this is at
>> best a very unbalanced panel. For those properties with repeats, you may
>> see temporal movement (trend/seasonal).
>>
>> I suggest (strongly) taking a single borough or even zipcode with some
>> hindreds of properties, and working from there. Do not include the
>> observation as its own neighbour, perhaps identify repeats and handle them
>> specially (create or use a property ID). Unbalanced panels may also create
>> a selection bias issue (why are some properties only listed sometimes?).
>>
>> So this although promising isn't simple, and getting to a hedonic model
>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>> necessarily trust OLS output either, partly because of the repeat property
>> issue.
>>
>> Roger
>>
>>>
>>> I created 54 timedummy variables for each time period available.
>>>
>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>> a variety of characteristics which potentially determine the daily rate
>>> on Airbnb listings through time in New York City (e.g. characteristics
>>> of the listing as number of bedrooms, if the host if professional,
>>> proximity to downtown (New York City Hall) and nearest subway station
>>> from the listing, income per capita, etc.).
>>>
>>> My dependent variable is price (log price, common in the related
>>> literature for hedonic prices).
>>>
>>> The OLS model is done.
>>>
>>> For the spatial model, I am assuming that hosts, when deciding the
>>> pricing of their listings, take not only into account its structural and
>>> location characteristics, but also the prices charged by near listings
>>> with similar characteristics - spatial autocorrelation is then present,
>>> at least spatial dependence is present in the dependent variable.
>>>
>>> As I wrote in my previous post, I was willing to consider the neighbor
>>> itself as a neighbor.
>>>
>>> Parts of my code can be found below:
>>>
>>> ########
>>>
>>> ## packages
>>>
>>> packages_install <- function(packages){
>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>> if (length(new.packages))
>>> install.packages(new.packages, dependencies = TRUE)
>>> sapply(packages, require, character.only = TRUE)
>>> }
>>>
>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>> packages_install(packages_required)
>>>
>>> # Working directory
>>> setwd("C:/Users/User/R")
>>>
>>>
>>>
>>> ## shapefile_us
>>>
>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>
>>> # Columns removal
>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>
>>> # Column rename: ZCTA5CE10
>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>
>>> # Column class change: zipcode
>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>
>>>
>>>
>>> ## polygon_nyc
>>>
>>> # Zip code not available in shapefile: 11695
>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>
>>>
>>>
>>> ## weight_matrix
>>>
>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>
>>> # Include neighbour itself as a neighbour
>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>
>>> # Weights to each neighboring polygon
>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>
>>>
>>>
>>> ## listings
>>>
>>> # Data import
>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>> listings <- listings %>% bind_rows
>>>
>>> # Characters removal
>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>> listings$price <- gsub("\\$", "", listings$price)
>>> listings$price <- gsub(",", "", listings$price)
>>>
>>>
>>>
>>> ## timedummy
>>>
>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>> timedummy <- gsub("-", "_", timedummy)
>>>
>>>
>>>
>>> ## OLS regression
>>>
>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>
>>> ########
>>>
>>> Some of my id's repeat in multiple time periods.
>>>
>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>
>>> Now I want to apply the hedonic model with the timedummy variables.
>>>
>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>
>>> Again, thank you very much for the help provided until now.
>>>
>>> Best regards,
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Tuesday, November 5, 2019 15:30
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>
>>>> I have a large pooled cross-section data set. ?I would like to
>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>> for now that spatial dependence is present in both the dependent
>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>> variable for each time period.? ?I also created a weight matrix using the
>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>> My spatial weight matrix for the whole data set will be probably a
>>>> enormous matrix with submatrices for each time period itself. I don't
>>>> think it would be possible to calculate this.? What I would like to know
>>>> is a way to estimate each time dummy/period separately (to compare
>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>> or splm?? ?Thank you and best regards,? Robert?
>>>
>>> Please do not post HTML, only plain text. Almost certainly your model
>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>> untried). What is your cross-sectional size? Using sparse kronecker
>>> products, the "enormous" matrix may not be very big. Does it make any
>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>> of the covariates time-varying? Please provide motivation and use area
>>> (preferably with affiliation (your email and user name are not
>>> informative) - this feels like a real estate problem, probably wrongly
>>> specified. You should use splm if time make sense in your case, but if it
>>> really doesn't, simplify your approach, as much of the data will be
>>> subject to very large temporal autocorrelation.
>>>
>>> If this is a continuation of your previous question about using
>>> self-neighbours, be aware that you should not use self-neighbours in
>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>
>>> Roger
>>>
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From @bdou|@ye@@r @end|ng |rom gm@||@com  Fri Nov  8 17:47:49 2019
From: @bdou|@ye@@r @end|ng |rom gm@||@com (Abdoulaye Sarr)
Date: Fri, 8 Nov 2019 16:47:49 +0000
Subject: [R-sig-Geo] issue with readOGR
Message-ID: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>

I am trying to read a shapefile using readOGR but keep getting error
messages:
map =
readOGR(dsn=path.expand("/Volumes/DS2S/R_QGIS_codes//Data"),"indicator")
Error in ogrInfo(dsn = dsn, layer = layer, encoding = encoding, use_iconv =
use_iconv,  :
  Cannot open layer

What could be causing the problem? sp 1.3-2

Thanks
as

	[[alternative HTML version deleted]]


From cr@be| @end|ng |rom S|gn@|Power@nd||ght@com  Fri Nov  8 23:05:21 2019
From: cr@be| @end|ng |rom S|gn@|Power@nd||ght@com (Cal Abel)
Date: Fri, 8 Nov 2019 22:05:21 +0000
Subject: [R-sig-Geo] CRS in rgdal does not function with PROJ 6.2.0
Message-ID: <B5AE93E5-5EF7-499B-901C-A8E07F373481@signalpowerandlight.com>

I have a Ubuntu 18.0.4 with R 3.6.1 system with the following releases from OSGeo for GEOS 3.8.0, GDAL 3.0.1, PROJ 6.2.0  I have an application that requires using gstat and thus I have to use rgdal as sf objects/classes aren?t supported in gstat, only sp. (This is a bit frustrating because rpostgis uses sf, so when I import my data it is an sf datatype, convert it to sp for gstat, and then back to sf to upload?) I thought the issue originally resided in sp, but after reviewing the sp source, it comes down to the calls that the sp CRS function makes using rgdal::. Below is a summary that shows my problem, sf links with the necessary system files and gets the correct CRS, rgdal does not. I think the issue is in how proj_api.h is included, specifically with not recognizing proj.db and instead relying on proj_def.dat which was removed in PROJ 6.1 as I recall. I can confirm that there is only one set of OSGeo libraries installed and those are those are the ones that I?ve compiled and are in /usr/local.

I can trick rgdal to use the proj_def.dat file by adding gdal/ and proj/ directories from the latest windows binary *.zip to ~/R/x86_64-pc-linux-gnu-library/3.6/rgdal. After doing this, rgdal will execute CRS, but complains that it can?t find proj.db   Once those files are removed it finds proj.db, but then can?t do CRS stuff. Win some, loose some I guess?

After reading through a bit of the source, it seems that rgdal defaults to using the deperecated proj4 strings and does not access the newer formats in proj.db even though during compilation rgdal finds the db with no problem. I am not too familiar with either package, so this is conjecture based on my limited testing/research.

As a note, if $PROJ_LIB is specified as an environment variable the result is Path to PROJ.4 shared files: /usr/local/share/proj.  But, when it is not specified the result is Path to PROJ.4 shared files: (autodetected). The behavior of rgdal remains unchanged in either case.


> library(sf)
Linking to GEOS 3.8.0, GDAL 3.0.1, PROJ 6.2.0
> library(rgdal)
Loading required package: sp
rgdal: version: 1.5-1, (SVN revision 879)
 Geospatial Data Abstraction Library extensions to R successfully loaded
 Loaded GDAL runtime: GDAL 3.0.1, released 2019/06/28
 Path to GDAL shared files:
 GDAL binary built with GEOS: TRUE
 Loaded PROJ.4 runtime: Rel. 6.2.0, September 1st, 2019, [PJ_VERSION: 620]
 Path to PROJ.4 shared files: /usr/local/share/proj
 Linking to sp version: 1.3-1
> CRS("+init=epsg:4269")
Error in CRS("+init=epsg:4269") : no arguments in initialization list
> st_crs(4269)
Coordinate Reference System:
  EPSG: 4269
  proj4string: "+proj=longlat +datum=NAD83 +no_defs?

Here is a coordinate transformation from NAD83 CRS to the UTM16 NAD83(2011) projection to show basic GDAL/PROJ6 functionality:
$ cs2cs EPSG:4269 EPSG:6345
35 -84
773798.10	3877156.69 0.00

I had forgotten that I originally reported the issue here https://github.com/edzer/sp/issues/59 <https://github.com/edzer/sp/issues/59> which I?ve since closed in testing for this email.

Below is an email exchange that drove this current email. I am unconvinced that Roger is correct. GDAL/PROJ work just fine, as does sf. A user error that he is referring to would cause those not to work/complain. The fact that I can make it work and make it not work should actually be grounds for successful bug identification.

Cal

> On Wed, 6 Nov 2019, Cal Abel wrote:
> 
> Dr. Bivand,
> 
> I attempted to report a bug in rgdal but don't really have any place to put it. I reported it in the OSGeo/gdal git page, but the issue was closed:
> 
> https://github.com/OSGeo/gdal/issues/1988 <https://github.com/OSGeo/gdal/issues/1988>
> I posted:
> 
> Almost certainly user blunder - neither **sf** nor **rgdal** install data/share files on source install, always relying on system installs. OP almost certainly neglected to read install notes. If GDAL and PROJ utilities can find their data/share files outside R, not finding them within R could signal R packages built against a different *.so than the one found at runtime.
> 
> 
> 
> One update to the above, I tried creating symbolic links to the compiled gdal and proj directories instead of copying the windows folders and it did not work. I also tried various combinations of links/files and was not able to get it to work. The only method that worked was to copy both windows directories.
> 
> The temporary fix I have, while it does not produce an error it now produces warnings:"
> 
> Do not try any fixes, you are almost certainly in error. How many versions of GDAL and PROJ are on your platform? For source installs, there should be one only, and GDAL should be built with that PROJ.
> 
> 
> Warning messages:
> 1: In CPL_crs_from_epsg(as.integer(x)) :
> GDAL Error 1: PROJ: proj_create_from_database: Cannot find proj.db
> 2: In CPL_crs_from_epsg(as.integer(x)) :
> GDAL Error 1: PROJ: proj_create_from_database: Cannot find proj.db
> 
> "
> 
> 
> Since it cannot find the PROJ proj.db, your installation of PROJ is broken, and copying across from an older version which does not have this key file will obviously fail.
> 
> In the future, what is the best avenue for reporting bugs?
> 
> 
> Use the R-sig-geo list. The package is not on github and will not be migrated - it is where https://cran.r-project.org/package=rgdal <https://cran.r-project.org/package=rgdal> says it is: https://r-forge.r-project.org/projects/rgdal/ <https://r-forge.r-project.org/projects/rgdal/> under SVN. All corresponndence through R-sig-geo, because others can correct your misapprehensions.
> 
> Note that the nabble link is totally different, which you should have known.
> 
> Further be aware that your usage pattern (+init=) is no longer valid in GDAL 3 and PROJ 6, and will stop working shortly. Your working example has GDAL 2 and PROJ 5, although this is not the only cause here, I think. It would be to your benefit to read https://proj.org/ <https://proj.org/> and https://gdal.org/ <https://gdal.org/>, and grasp https://gdal.org/development/rfc/rfc73_proj6_wkt2_srsbarn.html <https://gdal.org/development/rfc/rfc73_proj6_wkt2_srsbarn.html>, since this is a work-related query. How you migrate your corporate workflows to GDAL 3 and PROJ 6//7 may be of considerable interest (blog, mailing list post), and I'm spending all my time trying to adapt rgdal for this major change, and do not have patience with wasted time. See https://github.com/r-spatial/sf/issues/1146 <https://github.com/r-spatial/sf/issues/1146> and https://github.com/r-spatial/discuss/issues/28 <https://github.com/r-spatial/discuss/issues/28>.
> 
> Roger Bivand
> 
Cal Abel, PhD
crabel at signalpowerandlight.com <mailto:crabel at signalpowerandlight.com>

www.signalpowerandlight.com <http://www.signalpowerandlight.com/>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191108/8704eb5c/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 874 bytes
Desc: Message signed with OpenPGP
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191108/8704eb5c/attachment.sig>

From teph|||pp| @end|ng |rom gm@||@com  Sat Nov  9 05:04:41 2019
From: teph|||pp| @end|ng |rom gm@||@com (Tom Philippi)
Date: Fri, 8 Nov 2019 20:04:41 -0800
Subject: [R-sig-Geo] 
 Isolating only land areas on a global map for computing averages
In-Reply-To: <1931301500.1089634.1573143910861@mail.yahoo.com>
References: <2098265776.786417.1573093111101.ref@mail.yahoo.com>
 <2098265776.786417.1573093111101@mail.yahoo.com>
 <CALyPt8zmd78Vfm_Av3mv63z=12s4bs7G53JvfddKZE+4v29GrA@mail.gmail.com>
 <1931301500.1089634.1573143910861@mail.yahoo.com>
Message-ID: <CALyPt8zEeYRTEj_CNRPiMEqyeXhvVadDBX3myOUT7hbsKoSWBg@mail.gmail.com>

Rain--
Yes, it is possible to do it with your extant raster stack.  In fact,
pretty much all reasonable approaches will do that.  Anything you do will
create a raster layer with values at every one of your 8192 raster cells:
some values will be 0 or NA.

The trivial answer if you only had a small range of latitude, and small
raster cell sizes relative to your polygon, would be the
raster::rasterize() function to generate a raster mask.

But, with a global raster from +90 to -90, any interpretable answer
averaging over area needs to take into account the different area of a grid
cell at 70 deg latitude vs 0 deg.  See:
https://gis.stackexchange.com/questions/29734/how-to-calculate-area-of-1-x-1-degree-cells-in-a-raster
At that point, you should go ahead and account for fractions of grid cells
over land so your averaging would be over land area.

I'm reticent to give you a complete code solution because without a name,
you may well be a student with this as an assignment.  But, my approach
would be:

create a SpatialPolygonsDataFrame object "gridPoly" from any of your raster
layers via raster::rasterToPolygons()
generate an id value for each of those polygons as row & column indices
from the raster, or as the cell number.
get land polygon SpatialPolygons object "landPoly" with polygons of land
only, not ocean.
create a new SpatialPolygons object from gIntersect::gridPoly, landPoly,
byid = c(TRUE, FALSE))
calculate an area of each polygon in that object via
geosphere::areaPolygon()  {rgeos::gArea()  only works for projected CRS}
create your mask/weights raster layer with either all NA or all 0.
either:
     parse the id values to row & column values.
     use the triplets of row, column, and area to replace the corresponding
NA or 0 values in that mask
or:
     if you used cell numbers, just use the cell numbers and area values in
replacement in that mask

create a second weight raster via raster::area() on one of your raster
layers.
raster multiply your polygon-area and your raster::area values to give the
actual weighs to use.

This still is an approximation, but likely +/- 1-2%.

If this is still complete gibberish to you, either I need more coffee or
you need to consult a good reference on working with spatial data in
general.

On Thu, Nov 7, 2019 at 8:25 AM <rain1290 at aim.com> wrote:

> Hi Tom and others,
>
> Thank you for your response and suggestions!
>
> Yes, I loaded and used the maptools package previously to create
> continents on my world map, among other things. I do think that the easiest
> approach would be to create a raster layer for land, and then water, with
> the values that I have. However, my precipitation values are globally
> distributed - every grid cell has a precipitation value for each year (i.e.
> each grid cell has 138 values/layers/years). So, if I were to create a
> raster of only land areas, how would I have my grid cells coincide with the
> land areas only on that raster?
>
> Also, would it be possible to accomplish this with the raster stack that I
> already have? If so, is there a way to separate all land/water areas this
> way using the maptools package?
>
> Thanks, again, and I really appreciate your help!
>
> -----Original Message-----
> From: Tom Philippi <tephilippi at gmail.com>
> To: rain1290 <rain1290 at aim.com>
> Cc: r-sig-geo <r-sig-geo at r-project.org>
> Sent: Thu, Nov 7, 2019 12:44 am
> Subject: Re: [R-sig-Geo] Isolating only land areas on a global map for
> computing averages
>
> The easiest approach would be to create a separate aligned raster layer
> for land vs water.  There are plenty of coastline polygons available out
> there (e.g., maptools, rworldmap, rnaturalearth packages): choose one in
> your raster CRS (or choose one and spTransform() it).  Then, use a grid
> version of your raster to extract values from that land/water
> SpatialPolygons object.
>
> 1: Your idea of extracting the land/water value at each grid cell centroid
> gives one estimate.  Instead of TRUE/FALSE, think of the numeric
> equivalents 1,0,  then using those as weights for averaging across your
> grid cells.
> 2: A "better" estimate would be to compute the fraction of each grid cell
> that is land, then use those fractional [0, 1] values as weights to compute
> a weighted average of precipitation over land.  At 2.8deg grid cells, a lot
> of heavy rainfall coastal areas would have the grid cell centroid offshore
> and be omitted by approach #1.
> 3: I recommend that you think hard about averaging across cells in Lat Lon
> to estimate average precipitation over land.  The actual area of a ~2.8 by
> 2.8 deg grid cell at the equator is much larger than the same at 70 deg N.
> I would spend the extra hour computing the actual area (in km^2) of land in
> each of your 8192 grid cells, then using those in a raster as weights for
> whatever calculations you do on the raster stack.  [Or you can cheat, as
> the area of a grid cell in degrees is a function of only the latitudes, and
> your required weights are multiplicative.]
>
> Your mileage may vary...
>
> Tom
>
> On Wed, Nov 6, 2019 at 6:18 PM rain1290--- via R-sig-Geo <
> r-sig-geo at r-project.org> wrote:
>
> Hi there,
> I am interested in calculating precipitation medians globally. However, I
> only want to isolate land areas to compute the median. I already first
> created a raster stack, called "RCP1pctCO2median", which contains the
> median values. There are 138 layers, with each layer representing one
> year.  This raster stack has the following attributes:
> class       : RasterStack
> dimensions  : 64, 128, 8192, 138  (nrow, ncol, ncell, nlayers)
> resolution  : 2.8125, 2.789327  (x, y)
> extent      : -181.4062, 178.5938, -89.25846, 89.25846  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> names       :    layer.1,    layer.2,    layer.3,    layer.4,
> layer.5,    layer.6,    layer.7,    layer.8,    layer.9,   layer.10,
> layer.11,   layer.12,   layer.13,   layer.14,   layer.15, ...
> min values  : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730,
> 0.53099146, 0.49757186, 0.45697752, 0.41382199, 0.46082401, 0.45516687,
> 0.51857087, 0.41005131, 0.45956529, 0.47497867, ...
> max values  :   96.30350,  104.08584,   88.92751,   97.49373,
> 89.57201,   90.58570,   86.67651,   88.33519,   96.94720,  101.58247,
> 96.07792,   93.21948,   99.59785,   94.26218,   90.62138, ...
>
> Previously, I was isolating a specific region by specifying a range of
> longitudes and latitudes to obtain the medians for that region, like this:
> expansion1<-expand.grid(103:120, 3:15)lonlataaa<-extract(RCP1pctCO2Median,
> expansion1)Columnaaa<-colMeans(lonlataaa)
>
> However, with this approach, too much water can mix with land areas, and
> if I narrow the latitude/longitude range on land, I might miss too much
> land to compute the median meaningfully.
> Therefore, with this data, would it be possible to use an IF/ELSE
> statement to tell R that if the "center point" of each grid cell happens to
> fall on land, then it would be considered as land (i.e. that would be TRUE
> - if not, then FALSE)? Even if a grid cell happens to have water mixed with
> land, but the center point of the grid is on land, that would be considered
> land. But can R even tell what is land or water in this case?
> Thank you, and I would greatly appreciate any assistance!
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>

	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Sat Nov  9 06:18:47 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sat, 9 Nov 2019 05:18:47 +0000 (UTC)
Subject: [R-sig-Geo] 
 Isolating only land areas on a global map for computing averages
In-Reply-To: <CALyPt8zEeYRTEj_CNRPiMEqyeXhvVadDBX3myOUT7hbsKoSWBg@mail.gmail.com>
References: <2098265776.786417.1573093111101.ref@mail.yahoo.com>
 <2098265776.786417.1573093111101@mail.yahoo.com>
 <CALyPt8zmd78Vfm_Av3mv63z=12s4bs7G53JvfddKZE+4v29GrA@mail.gmail.com>
 <1931301500.1089634.1573143910861@mail.yahoo.com>
 <CALyPt8zEeYRTEj_CNRPiMEqyeXhvVadDBX3myOUT7hbsKoSWBg@mail.gmail.com>
Message-ID: <1456558013.1932132.1573276727061@mail.yahoo.com>

Hi Tom and others,

Thank you, once again, for your extensive reply and feedback to this! I really appreciate it!!!
I have some idea as to what you suggested to do, but most of this is still fairly new to me in terms of the procedure. What I am trying to do is actually for personal use, so if you choose to share a complete code demonstrating this process of masking, it could definitely be a useful learning means to better visualize and understand how this is done using my existing raster data. If you choose to, would it be possible to provide some brief comments at each step, so that I can understand what is going on? If not, do you know of any useful references that I can refer to that deals with this procedure? I've been looking, but nothing too specific comes up with what I would like to do, unfortunately.
Also, why would the area of the grid cells at 70-90 degrees latitude differ as compared to the area of lower latitude grid cells if each grid cell is 2.8 x 2.8 degrees? Are you suggesting that I need to derive the cosine of latitude, like the following?
Fncfname <- "MaxPrec5.nc"
FModel <- nc_open(Fncfname)
FPrec? <- brick(Fncfname,var="fivedaymax")
Fsm <- mean(FPrec, na.rm=TRUE)
Fw <- init(FPrec, 'y')
Fw <- cos(Fw*(pi/180))Fx <- Fsm*Fw
Thanks, again, for your extensive help!!?
-----Original Message-----
From: Tom Philippi <tephilippi at gmail.com>
To: rain1290 <rain1290 at aim.com>; tephilippi <tephilippi at gmail.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Fri, Nov 8, 2019 11:04 pm
Subject: Re: [R-sig-Geo] Isolating only land areas on a global map for computing averages

Rain--Yes, it is possible to do it with your extant raster stack.? In fact, pretty much all reasonable approaches will do that.? Anything you do will create a raster layer with values at every one of your 8192 raster cells: some values will be 0 or NA.
The trivial answer if you only had a small range of latitude, and small raster cell sizes relative to your polygon, would be the raster::rasterize() function to generate a raster mask.
But, with a global raster from?+90 to -90, any interpretable answer averaging over area needs to take into account the different area of a grid cell at 70 deg latitude vs 0 deg.? See:?https://gis.stackexchange.com/questions/29734/how-to-calculate-area-of-1-x-1-degree-cells-in-a-rasterAt that point, you should go ahead and account for fractions of grid cells over land so your averaging would be over land area.
I'm reticent to give you a complete code solution because without a name, you may well be a student with this as an assignment.? But, my approach would be:
create a SpatialPolygonsDataFrame object "gridPoly" from any of your raster layers via raster::rasterToPolygons()generate an id value for each of those polygons as row & column indices from the raster, or as the cell number.get land polygon SpatialPolygons object "landPoly" with polygons of land only, not ocean.create a new SpatialPolygons object from gIntersect::gridPoly, landPoly, byid = c(TRUE, FALSE))calculate an area of each polygon in that object via geosphere::areaPolygon()? {rgeos::gArea()? only works for projected CRS}create your mask/weights raster layer with either all NA or all 0.either:?? ? ?parse the id values to row & column values.? ? ?use the triplets of row, column, and area to replace the corresponding NA or 0 values in that mask
or:? ? ?if you used cell numbers, just use the cell numbers and area values in replacement in that mask
create a second weight raster via raster::area() on one of your raster layers.
raster multiply your polygon-area and your raster::area values to give the actual weighs to use.
This still is an approximation, but likely?+/- 1-2%.
If this is still complete gibberish to you, either I need more coffee or you need to consult a good reference on working with spatial data in general.
On Thu, Nov 7, 2019 at 8:25 AM <rain1290 at aim.com> wrote:

Hi Tom and others,

Thank you for your response and suggestions!?
Yes, I loaded and used the maptools package previously to create continents on my world map, among other things. I do think that the easiest approach would be to create a raster layer for land, and then water, with the values that I have. However, my precipitation values are globally distributed - every grid cell has a precipitation value for each year (i.e. each grid cell has 138 values/layers/years). So, if I were to create a raster of only land areas, how would I have my grid cells coincide with the land areas only on that raster?
Also, would it be possible to accomplish this with the raster stack that I already have? If so, is there a way to separate all land/water areas this way using the maptools package?
Thanks, again, and I really appreciate your help!
-----Original Message-----
From: Tom Philippi <tephilippi at gmail.com>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Thu, Nov 7, 2019 12:44 am
Subject: Re: [R-sig-Geo] Isolating only land areas on a global map for computing averages

The easiest approach would be to create a separate aligned raster layer for land vs water.? There are plenty of coastline polygons available out there (e.g., maptools, rworldmap, rnaturalearth packages): choose one in your raster CRS (or choose one and spTransform() it).? Then, use a grid version of your raster to extract values from that land/water SpatialPolygons object.
1: Your idea of extracting the land/water value at each grid cell centroid gives one estimate.? Instead of TRUE/FALSE, think of the numeric equivalents 1,0,? then using those as weights for averaging across your grid cells.2: A "better" estimate would be to compute the fraction of each grid cell that is land, then use those fractional [0, 1] values as weights to compute a weighted average of precipitation over land.? At 2.8deg grid cells, a lot of heavy rainfall coastal areas would have the grid cell centroid offshore and be omitted by approach #1.3: I recommend that you think hard about averaging across cells in Lat Lon to estimate average precipitation over land.? The actual area of a ~2.8 by 2.8 deg grid cell at the equator is much larger than the same at 70 deg N.? I would spend the extra hour computing the actual area (in km^2) of land in each of your 8192 grid cells, then using those in a raster as weights for whatever calculations you do on the raster stack.? [Or you can cheat, as the area of a grid cell in degrees is a function of only the latitudes, and your required weights are multiplicative.]
Your mileage may vary...
Tom
On Wed, Nov 6, 2019 at 6:18 PM rain1290--- via R-sig-Geo <r-sig-geo at r-project.org> wrote:

Hi there,
I am interested in calculating precipitation medians globally. However, I only want to isolate land areas to compute the median. I already first created a raster stack, called "RCP1pctCO2median", which contains the median values.?There are 138 layers, with each layer representing one year.? This raster stack has the following attributes:
class?????? : RasterStack 
dimensions? : 64, 128, 8192, 138? (nrow, ncol, ncell, nlayers)
resolution? : 2.8125, 2.789327? (x, y)
extent????? : -181.4062, 178.5938, -89.25846, 89.25846? (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
names?????? :??? layer.1,??? layer.2,??? layer.3,??? layer.4,??? layer.5,??? layer.6,??? layer.7,??? layer.8,??? layer.9,?? layer.10,?? layer.11,?? layer.12,?? layer.13,?? layer.14,?? layer.15, ... 
min values? : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730, 0.53099146, 0.49757186, 0.45697752, 0.41382199, 0.46082401, 0.45516687, 0.51857087, 0.41005131, 0.45956529, 0.47497867, ... 
max values? :?? 96.30350,? 104.08584,?? 88.92751,?? 97.49373,?? 89.57201,?? 90.58570,?? 86.67651,?? 88.33519,?? 96.94720,? 101.58247,?? 96.07792,?? 93.21948,?? 99.59785,?? 94.26218,?? 90.62138, ...??

Previously, I was isolating a specific region by specifying a range of longitudes and latitudes to obtain the medians for that region, like this:
expansion1<-expand.grid(103:120, 3:15)lonlataaa<-extract(RCP1pctCO2Median, expansion1)Columnaaa<-colMeans(lonlataaa)

However, with this approach, too much water can mix with land areas, and if I narrow the latitude/longitude range on land, I might miss too much land to compute the median meaningfully.
Therefore, with this data, would it be possible to use an IF/ELSE statement to tell R that if the "center point" of each grid cell happens to fall on land, then it would be considered as land (i.e. that would be TRUE - if not, then FALSE)? Even if a grid cell happens to have water mixed with land, but the center point of the grid is on land, that would be considered land. But can R even tell what is land or water in this case?
Thank you, and I would greatly appreciate any assistance!

? ? ? ? [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo



	[[alternative HTML version deleted]]


From D@v|d@H|ne @end|ng |rom |@nd@ndw@ter@com@@u  Sat Nov  9 09:58:28 2019
From: D@v|d@H|ne @end|ng |rom |@nd@ndw@ter@com@@u (David Hine)
Date: Sat, 9 Nov 2019 18:58:28 +1000
Subject: [R-sig-Geo] issue with readOGR
In-Reply-To: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>
References: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>
Message-ID: <a50af51b-d6e4-50ec-fb10-2b90885f3eee@landandwater.com.au>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191109/24b9281c/attachment.html>

From bernd@voge|ge@@ng @end|ng |rom gmx@de  Sat Nov  9 10:13:34 2019
From: bernd@voge|ge@@ng @end|ng |rom gmx@de (Bernd Vogelgesang)
Date: Sat, 9 Nov 2019 10:13:34 +0100
Subject: [R-sig-Geo] 
 It's possible delimit a shapefile very extent using R?
In-Reply-To: <CAD-TP6khqe--3Y3BA4tpEyEuF+C+gUFOsSs2udwc6B+7Tfc3Bw@mail.gmail.com>
References: <CAD-TP6khqe--3Y3BA4tpEyEuF+C+gUFOsSs2udwc6B+7Tfc3Bw@mail.gmail.com>
Message-ID: <aa8c0dd4-a5aa-21e0-2e63-294b384cb24d@gmx.de>

Hi Geomar,

it's completely unclear for me from your one-liner what you want to do
and what your problem is.

Cheers,
Bernd

Am 07.11.19 um 16:55 schrieb Geomar Perales:
> Hi
>
> I need ideas for see if is possible delimit?a shapefile very extent
> using R or maybe any other language (I add a image of the shape).
>
> thank for your time
> Geomar
>
> image.png
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191109/9c8e5ad6/attachment.html>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 28644 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191109/9c8e5ad6/attachment.png>

From Roger@B|v@nd @end|ng |rom nhh@no  Sat Nov  9 18:07:49 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sat, 9 Nov 2019 18:07:49 +0100
Subject: [R-sig-Geo] issue with readOGR
In-Reply-To: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>
References: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1911091805080.26030@reclus.nhh.no>

On Fri, 8 Nov 2019, Abdoulaye Sarr wrote:

> I am trying to read a shapefile using readOGR but keep getting error
> messages:
> map =
> readOGR(dsn=path.expand("/Volumes/DS2S/R_QGIS_codes//Data"),"indicator")
> Error in ogrInfo(dsn = dsn, layer = layer, encoding = encoding, use_iconv =
> use_iconv,  :
>  Cannot open layer

What is "/Volumes/DS2S/R_QGIS_codes//Data"? If a network drive, some OGR 
drivers are known not to work with them (or that used to be the case). 
Which OS/Platform is this? Is the repeated "//" correct? Can you see the 
file running list.files("/Volumes/DS2S/R_QGIS_codes//Data") ?

Roger

>
> What could be causing the problem? sp 1.3-2
>
> Thanks
> as
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Sat Nov  9 18:24:01 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sat, 9 Nov 2019 18:24:01 +0100
Subject: [R-sig-Geo] 
 It's possible delimit a shapefile very extent using R?
In-Reply-To: <aa8c0dd4-a5aa-21e0-2e63-294b384cb24d@gmx.de>
References: <CAD-TP6khqe--3Y3BA4tpEyEuF+C+gUFOsSs2udwc6B+7Tfc3Bw@mail.gmail.com>
 <aa8c0dd4-a5aa-21e0-2e63-294b384cb24d@gmx.de>
Message-ID: <alpine.LFD.2.21.1911091809120.26030@reclus.nhh.no>

On Sat, 9 Nov 2019, Bernd Vogelgesang wrote:

> Hi Geomar,
>
> it's completely unclear for me from your one-liner what you want to do
> and what your problem is.
>
> Cheers,
> Bernd

Thanks for trying to understand the post. My best guess was that in the 
external utility, you can use ogr2ogr with an SQL-style select. readOGR() 
cannot do this, but I think sf::st_read() may be able to do this through 
the query= argument. I cannot see any good examples, though.

Roger

>
> Am 07.11.19 um 16:55 schrieb Geomar Perales:
>>  Hi
>>
>>  I need ideas for see if is possible delimit?a shapefile very extent
>>  using R or maybe any other language (I add a image of the shape).
>>
>>  thank for your time
>>  Geomar
>>
>>  image.png
>>
>>  _______________________________________________
>>  R-sig-Geo mailing list
>>  R-sig-Geo at r-project.org
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger@B|v@nd @end|ng |rom nhh@no  Sat Nov  9 18:46:36 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sat, 9 Nov 2019 18:46:36 +0100
Subject: [R-sig-Geo] CRS in rgdal does not function with PROJ 6.2.0
In-Reply-To: <B5AE93E5-5EF7-499B-901C-A8E07F373481@signalpowerandlight.com>
References: <B5AE93E5-5EF7-499B-901C-A8E07F373481@signalpowerandlight.com>
Message-ID: <alpine.LFD.2.21.1911091832580.26030@reclus.nhh.no>

On Fri, 8 Nov 2019, Cal Abel wrote:

> I have a Ubuntu 18.0.4 with R 3.6.1 system with the following releases 
> from OSGeo for GEOS 3.8.0, GDAL 3.0.1, PROJ 6.2.0 I have an application 
> that requires using gstat and thus I have to use rgdal as sf 
> objects/classes aren?t supported in gstat, only sp. (This is a bit 
> frustrating because rpostgis uses sf, so when I import my data it is an 
> sf datatype, convert it to sp for gstat, and then back to sf to upload?)

First, note that gstat does support sf and stars class objects, as shown 
by https://keen-swartz-3146c4.netlify.com/interpolation.html (a draft 
chapter in a WIP-book).

Second, note that unless you are developing software, you should be using 
GDAL 2 and PROJ 5. You should not be using GDAL 3 and PROJ 6. For 
references, please do read the links I sent privately explaining why all 
open source spatial software using GDAL and PROJ is having difficulties 
adapting to the speed of chane in their APIs. Elsewhere I believe you 
encountered this with regard to libspatialite. Unless you really need to 
work on the wild side, stay on the safe side (GDAL2+PROJ5). sf with 
GDAL3/PROJ6 is also broken now.

You are working (day-time job) for a power company, and using R and gstat 
for free. You need to read more and write less before you impose on 
volunteer developers and maintainers whose day jobs do not allocate hours 
to handling your queries.


> I thought the issue originally resided in sp, but after reviewing the sp 
> source, it comes down to the calls that the sp CRS function makes using 
> rgdal::. Below is a summary that shows my problem, sf links with the 
> necessary system files and gets the correct CRS, rgdal does not. I think 
> the issue is in how proj_api.h is included, specifically with not 
> recognizing proj.db and instead relying on proj_def.dat which was 
> removed in PROJ 6.1 as I recall. I can confirm that there is only one 
> set of OSGeo libraries installed and those are those are the ones that 
> I?ve compiled and are in /usr/local.
>
> I can trick rgdal to use the proj_def.dat file by adding gdal/ and proj/ 
> directories from the latest windows binary *.zip to 
> ~/R/x86_64-pc-linux-gnu-library/3.6/rgdal. After doing this, rgdal will 
> execute CRS, but complains that it can?t find proj.db Once those files 
> are removed it finds proj.db, but then can?t do CRS stuff. Win some, 
> loose some I guess?
>

This was an issue faced years ago when PROJ was released without this file 
and is completely misleading here.

> After reading through a bit of the source, it seems that rgdal defaults 
> to using the deperecated proj4 strings and does not access the newer 
> formats in proj.db even though during compilation rgdal finds the db 
> with no problem. I am not too familiar with either package, so this is 
> conjecture based on my limited testing/research.
>
> As a note, if $PROJ_LIB is specified as an environment variable the 
> result is Path to PROJ.4 shared files: /usr/local/share/proj.  But, when 
> it is not specified the result is Path to PROJ.4 shared files: 
> (autodetected). The behavior of rgdal remains unchanged in either case.
>
>
>> library(sf)
> Linking to GEOS 3.8.0, GDAL 3.0.1, PROJ 6.2.0
>> library(rgdal)
> Loading required package: sp
> rgdal: version: 1.5-1, (SVN revision 879)
> Geospatial Data Abstraction Library extensions to R successfully loaded
> Loaded GDAL runtime: GDAL 3.0.1, released 2019/06/28
> Path to GDAL shared files:
> GDAL binary built with GEOS: TRUE
> Loaded PROJ.4 runtime: Rel. 6.2.0, September 1st, 2019, [PJ_VERSION: 620]
> Path to PROJ.4 shared files: /usr/local/share/proj
> Linking to sp version: 1.3-1
>> CRS("+init=epsg:4269")
> Error in CRS("+init=epsg:4269") : no arguments in initialization list
>> st_crs(4269)
> Coordinate Reference System:
>  EPSG: 4269
>  proj4string: "+proj=longlat +datum=NAD83 +no_defs?
>
> Here is a coordinate transformation from NAD83 CRS to the UTM16 NAD83(2011) projection to show basic GDAL/PROJ6 functionality:
> $ cs2cs EPSG:4269 EPSG:6345
> 35 -84
> 773798.10	3877156.69 0.00
>
> I had forgotten that I originally reported the issue here 
> https://github.com/edzer/sp/issues/59 
> <https://github.com/edzer/sp/issues/59> which I?ve since closed in 
> testing for this email.
>
> Below is an email exchange that drove this current email. I am 
> unconvinced that Roger is correct. GDAL/PROJ work just fine, as does sf. 
> A user error that he is referring to would cause those not to 
> work/complain. The fact that I can make it work and make it not work 
> should actually be grounds for successful bug identification.
>

You did not ask my permission to publish a private exchange on a list 
posting. It is normal practice to do so. Another time I might rather 
choose not to reply to any private queries, rather than offer advice, 
which had you heeded it, would have guided you in the right direction.

Check first, read a lot, pester others less, especially when your job 
depends on our offering  goodwill.

Reconfigure your system to use GDAL 2 and PROJ 5, the use whatever you 
need.

Roger

> Cal
>
>> On Wed, 6 Nov 2019, Cal Abel wrote:
>>
>> Dr. Bivand,
>>
>> I attempted to report a bug in rgdal but don't really have any place to 
>> put it. I reported it in the OSGeo/gdal git page, but the issue was 
>> closed:
>>
>> https://github.com/OSGeo/gdal/issues/1988 <https://github.com/OSGeo/gdal/issues/1988>
>> I posted:
>>
>> Almost certainly user blunder - neither **sf** nor **rgdal** install 
>> data/share files on source install, always relying on system installs. 
>> OP almost certainly neglected to read install notes. If GDAL and PROJ 
>> utilities can find their data/share files outside R, not finding them 
>> within R could signal R packages built against a different *.so than 
>> the one found at runtime.
>>
>>
>>
>> One update to the above, I tried creating symbolic links to the 
>> compiled gdal and proj directories instead of copying the windows 
>> folders and it did not work. I also tried various combinations of 
>> links/files and was not able to get it to work. The only method that 
>> worked was to copy both windows directories.
>>
>> The temporary fix I have, while it does not produce an error it now 
>> produces warnings:"
>>
>> Do not try any fixes, you are almost certainly in error. How many 
>> versions of GDAL and PROJ are on your platform? For source installs, 
>> there should be one only, and GDAL should be built with that PROJ.
>>
>>
>> Warning messages:
>> 1: In CPL_crs_from_epsg(as.integer(x)) :
>> GDAL Error 1: PROJ: proj_create_from_database: Cannot find proj.db
>> 2: In CPL_crs_from_epsg(as.integer(x)) :
>> GDAL Error 1: PROJ: proj_create_from_database: Cannot find proj.db
>>
>> "
>>
>>
>> Since it cannot find the PROJ proj.db, your installation of PROJ is 
>> broken, and copying across from an older version which does not have 
>> this key file will obviously fail.
>>
>> In the future, what is the best avenue for reporting bugs?
>>
>>
>> Use the R-sig-geo list. The package is not on github and will not be 
>> migrated - it is where https://cran.r-project.org/package=rgdal 
>> <https://cran.r-project.org/package=rgdal> says it is: 
>> https://r-forge.r-project.org/projects/rgdal/ 
>> <https://r-forge.r-project.org/projects/rgdal/> under SVN. All 
>> corresponndence through R-sig-geo, because others can correct your 
>> misapprehensions.
>>
>> Note that the nabble link is totally different, which you should have 
>> known.
>>
>> Further be aware that your usage pattern (+init=) is no longer valid in 
>> GDAL 3 and PROJ 6, and will stop working shortly. Your working example 
>> has GDAL 2 and PROJ 5, although this is not the only cause here, I 
>> think. It would be to your benefit to read https://proj.org/ 
>> <https://proj.org/> and https://gdal.org/ <https://gdal.org/>, and 
>> grasp https://gdal.org/development/rfc/rfc73_proj6_wkt2_srsbarn.html 
>> <https://gdal.org/development/rfc/rfc73_proj6_wkt2_srsbarn.html>, since 
>> this is a work-related query. How you migrate your corporate workflows 
>> to GDAL 3 and PROJ 6//7 may be of considerable interest (blog, mailing 
>> list post), and I'm spending all my time trying to adapt rgdal for this 
>> major change, and do not have patience with wasted time. See 
>> https://github.com/r-spatial/sf/issues/1146 
>> <https://github.com/r-spatial/sf/issues/1146> and 
>> https://github.com/r-spatial/discuss/issues/28 
>> <https://github.com/r-spatial/discuss/issues/28>.
>>
>> Roger Bivand
>>
> Cal Abel, PhD
> crabel at signalpowerandlight.com <mailto:crabel at signalpowerandlight.com>
>
> www.signalpowerandlight.com <http://www.signalpowerandlight.com/>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From @bdou|@ye@@r @end|ng |rom gm@||@com  Sat Nov  9 19:11:38 2019
From: @bdou|@ye@@r @end|ng |rom gm@||@com (Abdoulaye Sarr)
Date: Sat, 9 Nov 2019 18:11:38 +0000
Subject: [R-sig-Geo] issue with readOGR
In-Reply-To: <alpine.LFD.2.21.1911091805080.26030@reclus.nhh.no>
References: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>
 <alpine.LFD.2.21.1911091805080.26030@reclus.nhh.no>
Message-ID: <CAN=6O0+OTnw=QRmH=mNqH8tMYXPyxnZq4M4+xGeq0MVq+2nh+w@mail.gmail.com>

output from list.files:

     "dhaka_div.shx"        "dhaka_gazipur.cpg"
 "dhaka_gazipur.dbf"    "dhaka_gazipur.prj"    "dhaka_gazipur.qpj"
 "dhaka_gazipur.shp"
 "dhaka_gazipur.shx"    "dhaka.dbf"            "dhaka.prj"
 "dhaka.shp"
"dhaka.shx"            "indicator.csv"        "r_val.csv"

On Sat, Nov 9, 2019 at 5:07 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Fri, 8 Nov 2019, Abdoulaye Sarr wrote:
>
> > I am trying to read a shapefile using readOGR but keep getting error
> > messages:
> > map =
> > readOGR(dsn=path.expand("/Volumes/DS2S/R_QGIS_codes//Data"),"indicator")
> > Error in ogrInfo(dsn = dsn, layer = layer, encoding = encoding,
> use_iconv =
> > use_iconv,  :
> >  Cannot open layer
>
> What is "/Volumes/DS2S/R_QGIS_codes//Data"? If a network drive, some OGR
> drivers are known not to work with them (or that used to be the case).
> Which OS/Platform is this? Is the repeated "//" correct? Can you see the
> file running list.files("/Volumes/DS2S/R_QGIS_codes//Data") ?
>
> Roger
>
> >
> > What could be causing the problem? sp 1.3-2
> >
> > Thanks
> > as
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From @bdou|@ye@@r @end|ng |rom gm@||@com  Sat Nov  9 19:13:08 2019
From: @bdou|@ye@@r @end|ng |rom gm@||@com (Abdoulaye Sarr)
Date: Sat, 9 Nov 2019 18:13:08 +0000
Subject: [R-sig-Geo] issue with readOGR
In-Reply-To: <CAN=6O0+OTnw=QRmH=mNqH8tMYXPyxnZq4M4+xGeq0MVq+2nh+w@mail.gmail.com>
References: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>
 <alpine.LFD.2.21.1911091805080.26030@reclus.nhh.no>
 <CAN=6O0+OTnw=QRmH=mNqH8tMYXPyxnZq4M4+xGeq0MVq+2nh+w@mail.gmail.com>
Message-ID: <CAN=6O0+61UNUTMa9RGCBB6pUpqT1hbXdR5qwgv9h4k4i-3+GVA@mail.gmail.com>

Platform is osx.

Thanks

On Sat, Nov 9, 2019 at 6:11 PM Abdoulaye Sarr <abdoulayesar at gmail.com>
wrote:

> output from list.files:
>
>      "dhaka_div.shx"        "dhaka_gazipur.cpg"
>  "dhaka_gazipur.dbf"    "dhaka_gazipur.prj"    "dhaka_gazipur.qpj"
>  "dhaka_gazipur.shp"
>  "dhaka_gazipur.shx"    "dhaka.dbf"            "dhaka.prj"
>  "dhaka.shp"
> "dhaka.shx"            "indicator.csv"        "r_val.csv"
>
> On Sat, Nov 9, 2019 at 5:07 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Fri, 8 Nov 2019, Abdoulaye Sarr wrote:
>>
>> > I am trying to read a shapefile using readOGR but keep getting error
>> > messages:
>> > map =
>> > readOGR(dsn=path.expand("/Volumes/DS2S/R_QGIS_codes//Data"),"indicator")
>> > Error in ogrInfo(dsn = dsn, layer = layer, encoding = encoding,
>> use_iconv =
>> > use_iconv,  :
>> >  Cannot open layer
>>
>> What is "/Volumes/DS2S/R_QGIS_codes//Data"? If a network drive, some OGR
>> drivers are known not to work with them (or that used to be the case).
>> Which OS/Platform is this? Is the repeated "//" correct? Can you see the
>> file running list.files("/Volumes/DS2S/R_QGIS_codes//Data") ?
>>
>> Roger
>>
>> >
>> > What could be causing the problem? sp 1.3-2
>> >
>> > Thanks
>> > as
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-Geo mailing list
>> > R-sig-Geo at r-project.org
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>

	[[alternative HTML version deleted]]


From bernd@voge|ge@@ng @end|ng |rom gmx@de  Sat Nov  9 20:33:25 2019
From: bernd@voge|ge@@ng @end|ng |rom gmx@de (Bernd Vogelgesang)
Date: Sat, 9 Nov 2019 20:33:25 +0100
Subject: [R-sig-Geo] 
 It's possible delimit a shapefile very extent using R?
In-Reply-To: <alpine.LFD.2.21.1911091809120.26030@reclus.nhh.no>
References: <CAD-TP6khqe--3Y3BA4tpEyEuF+C+gUFOsSs2udwc6B+7Tfc3Bw@mail.gmail.com>
 <aa8c0dd4-a5aa-21e0-2e63-294b384cb24d@gmx.de>
 <alpine.LFD.2.21.1911091809120.26030@reclus.nhh.no>
Message-ID: <60912acb-1afc-365e-8faf-47edfd8dd0f9@gmx.de>

Hi Roger,

... and I also do not understand your answer.
Think I better stay out of this ;)

Bernd


Am 09.11.19 um 18:24 schrieb Roger Bivand:
> On Sat, 9 Nov 2019, Bernd Vogelgesang wrote:
>
>> Hi Geomar,
>>
>> it's completely unclear for me from your one-liner what you want to do
>> and what your problem is.
>>
>> Cheers,
>> Bernd
>
> Thanks for trying to understand the post. My best guess was that in
> the external utility, you can use ogr2ogr with an SQL-style select.
> readOGR() cannot do this, but I think sf::st_read() may be able to do
> this through the query= argument. I cannot see any good examples, though.
>
> Roger
>
>>
>> Am 07.11.19 um 16:55 schrieb Geomar Perales:
>>> ?Hi
>>>
>>> ?I need ideas for see if is possible delimit?a shapefile very extent
>>> ?using R or maybe any other language (I add a image of the shape).
>>>
>>> ?thank for your time
>>> ?Geomar
>>>
>>> ?image.png
>>>
>>> ?_______________________________________________
>>> ?R-sig-Geo mailing list
>>> ?R-sig-Geo at r-project.org
>>> ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>


From Roger@B|v@nd @end|ng |rom nhh@no  Sun Nov 10 14:34:38 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sun, 10 Nov 2019 14:34:38 +0100
Subject: [R-sig-Geo] issue with readOGR
In-Reply-To: <CAN=6O0+61UNUTMa9RGCBB6pUpqT1hbXdR5qwgv9h4k4i-3+GVA@mail.gmail.com>
References: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>
 <alpine.LFD.2.21.1911091805080.26030@reclus.nhh.no>
 <CAN=6O0+OTnw=QRmH=mNqH8tMYXPyxnZq4M4+xGeq0MVq+2nh+w@mail.gmail.com>
 <CAN=6O0+61UNUTMa9RGCBB6pUpqT1hbXdR5qwgv9h4k4i-3+GVA@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1911101427580.57003@reclus.nhh.no>

On Sat, 9 Nov 2019, Abdoulaye Sarr wrote:

> Platform is osx.
>
> Thanks
>
> On Sat, Nov 9, 2019 at 6:11 PM Abdoulaye Sarr <abdoulayesar at gmail.com>
> wrote:
>
>> output from list.files:
>>
>>      "dhaka_div.shx"        "dhaka_gazipur.cpg"
>>  "dhaka_gazipur.dbf"    "dhaka_gazipur.prj"    "dhaka_gazipur.qpj"
>>  "dhaka_gazipur.shp"
>>  "dhaka_gazipur.shx"    "dhaka.dbf"            "dhaka.prj"
>>  "dhaka.shp"
>> "dhaka.shx"            "indicator.csv"        "r_val.csv"
>>

>From this we see that "indicator.csv" is not (immediately) a file that has 
an appropriate driver (it needs a *.vrt to say which columns are which, 
see https://gdal.org/drivers/vector/csv.html). Valid layers should be 
"dhaka" and "dhaka_gazipur", but not "dhaka_div", for which  only the 
*.shx is shown.

Try ogrListLayers(path.expand("/Volumes/DS2S/R_QGIS_codes//Data")) to try 
to detect valid layers. If you really meant "indicators", use 
paste0("CSV:", file.path(path.expand("/Volumes/DS2S/R_QGIS_codes//Data"), 
"indicators.csv")) as per the GDAL vector driver help page.

Roger

>> On Sat, Nov 9, 2019 at 5:07 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>
>>> On Fri, 8 Nov 2019, Abdoulaye Sarr wrote:
>>>
>>>> I am trying to read a shapefile using readOGR but keep getting error
>>>> messages:
>>>> map =
>>>> readOGR(dsn=path.expand("/Volumes/DS2S/R_QGIS_codes//Data"),"indicator")
>>>> Error in ogrInfo(dsn = dsn, layer = layer, encoding = encoding,
>>> use_iconv =
>>>> use_iconv,  :
>>>>  Cannot open layer
>>>
>>> What is "/Volumes/DS2S/R_QGIS_codes//Data"? If a network drive, some OGR
>>> drivers are known not to work with them (or that used to be the case).
>>> Which OS/Platform is this? Is the repeated "//" correct? Can you see the
>>> file running list.files("/Volumes/DS2S/R_QGIS_codes//Data") ?
>>>
>>> Roger
>>>
>>>>
>>>> What could be causing the problem? sp 1.3-2
>>>>
>>>> Thanks
>>>> as
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From u@erc@tch @end|ng |rom out|ook@com  Sun Nov 10 16:04:00 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Sun, 10 Nov 2019 15:04:00 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
Message-ID: <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Roger,

Again, thank you for your answer. I read the material provided and decided that Hierarchical Spatial Autoregressive (HSAR) could be the right model for me.

I indeed have the precise latitude and longitude information for all my listings for NYC.

I created a stratified sample (group = zipcode) with 22172 (1%) of my observations called listings_sample and tried to replicate the hsar model, please see below.

For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.

You recommended then to introduce a Markov random field (MRF) random effect (RE) at the zipcode level, but I did not understand it so well. Could you develop a litte more?

##############
library(spdep)
library(HSAR)
library(dplyr)
library(splitstackshape)


# Stratified sample per zipcode (size = 1%)
listings_sample <- splitstackshape::stratified(indt = listings, group = "zipcode", size = 0.01)

# Removing zipcodes from polygon_nyc which are not observable in listings_sample
polygon_nyc_listings <- polygon_nyc %>% filter(zipcode %in% c(unique(as.character(listings_sample$zipcode))))


## Random effect matrix (N by J)

# N: 22172
# J: 154

# Arrange listings_sample by zipcode (ascending)
listings_sample <- listings_sample %>% arrange(zipcode)

# Count number of listings per zipcode
MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
# sum(MM$count)

# N by J nulled matrix creation
Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])

# The total number of neighbourhood
Uid <- rep(c(1:dim(MM)[1]), MM[,2])

for(i in 1:dim(MM)[1]) {
  Delta[Uid==i,i] <- 1
}
rm(i)

Delta <- as(Delta,"dgCMatrix")


## Higher-level spatial weights matrix or neighbourhood matrix (J by J)

# Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)

# Include neighbour itself as a neighbour
polygon_nyc_nb <- include.self(polygon_nyc_nb)

# Spatial weights matrix for nb
polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
M <- as(polygon_nyc_nb_matrix,"dgCMatrix")


## Fit HSAR SAR upper level random effect
model <- as.formula(log_price ~ guests_included + minimum_nights)

betas = coef(lm(formula = model, data = listings_sample))
pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)

m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)

##############

Thank you and best regards
Robert

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Friday, November 8, 2019 13:29
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

On Fri, 8 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your answer.
>
> I successfully used the function nb2blocknb() for a smaller dataset.
>
> But for a dataset of over 2 million observations, I get the following
> error: "Error: cannot allocate vector of size 840 Kb".

I don't think the observations are helpful. If you have repeat lets in the
same property in a given month, you need to handle that anyway. I'd go for
making the modelling exercise work (we agree that this is not panel data,
right?) on a small subset first. I would further argue that you need a
multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
RE. You could very well take (stratified) samples per zipcode to represent
your data. Once that works, introduce an MRF RE at the zipcode level,
where you do know relative position. Using SARAR is going to be a waste of
time unless you can geocode the letting addresses. A multi-level approach
will work. Having big data in your case with no useful location
information per observation is just adding noise and over-smoothing, I'm
afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
will work, also when you sample the within zipcode lets, given a split
into training and test sets, and making CV possible.

Roger

>
> I am expecting that at least 500.000 observations will be dropped due
> the lack of values for the chosen variables for the regression model, so
> probably I will filter and remove the observations/rows that will not be
> used anyway - do you know if there is any package that does this
> automatically, given the variables/columns chosed by me?
>
> Or would you recommend me another approach to avoid the above mentioned
> error?
>
> Thank you and best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Thursday, November 7, 2019 10:13
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Thu, 7 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Many thanks for your help.
>>
>> I have an additional question:
>>
>> Is it possible to create a "separate" lw (nb2listw) (with different
>> rownumbers) from my data set? For now, I am taking my data set and
>> merging with the sf object polygon_nyc with the function
>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>> a huge n x n matrix (depending of the size of my data set).
>>
>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>> object has only n = 177.
>>
>> Of course running
>>
>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>
>> does not work ("Input data and weights have different dimensions").
>>
>> The only option is to take my data set, merge it to my polygon_nyc (by
>> zipcode) and then create the weights list lw? Or there another option?
>
> I think we are getting more clarity. You do not know the location of the
> lettings beyond their zipcode. You do know the boundaries of the zipcode
> areas, and can create a neighbour object from these boundaries. You then
> want to treat all the lettings in a zipcode area i as neighbours, and
> additionally lettings in zipcode areas neighbouring i as neighbours of
> lettings in i. This is the data structure that motivated the
> spdep::nb2blocknb() function:
>
> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>
> Try running the examples to get a feel for what is going on.
>
> I feel that most of the variability will vanish in the very large numbers
> of neighbours, over-smoothing the outcomes. If you do not have locations
> for the lettings themselves, I don't think you can make much progress.
>
> You could try a linear mixed model (or gam with a spatially structured
> random effect) with a temporal and a spatial random effect. See the HSAR
> package, articles by Dong et al., and maybe
> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
> effects at the zipcode level might be a way forward, together with an IID
> random effect at the same level (equivalent to sef-neighbours).
>
> Hope this helps,
>
> Roger
>
>>
>> Best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Wednesday, November 6, 2019 15:07
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Tue, 5 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>> plain text.
>>>
>>> I will give a better context for my desired outcome.
>>>
>>> I am taking Airbnb's listings information for New York City available
>>> on: http://insideairbnb.com/get-the-data.html
>>>
>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>> Listings/ folder. When importing all these 54 files into one single data
>>> set, I create a new "date_compiled" variable/column.
>>>
>>> In total, after the data cleansing process, I have a little more 2
>>> million observations.
>>
>> You have repeat lettings for some, but not all properties. So this is at
>> best a very unbalanced panel. For those properties with repeats, you may
>> see temporal movement (trend/seasonal).
>>
>> I suggest (strongly) taking a single borough or even zipcode with some
>> hindreds of properties, and working from there. Do not include the
>> observation as its own neighbour, perhaps identify repeats and handle them
>> specially (create or use a property ID). Unbalanced panels may also create
>> a selection bias issue (why are some properties only listed sometimes?).
>>
>> So this although promising isn't simple, and getting to a hedonic model
>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>> necessarily trust OLS output either, partly because of the repeat property
>> issue.
>>
>> Roger
>>
>>>
>>> I created 54 timedummy variables for each time period available.
>>>
>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>> a variety of characteristics which potentially determine the daily rate
>>> on Airbnb listings through time in New York City (e.g. characteristics
>>> of the listing as number of bedrooms, if the host if professional,
>>> proximity to downtown (New York City Hall) and nearest subway station
>>> from the listing, income per capita, etc.).
>>>
>>> My dependent variable is price (log price, common in the related
>>> literature for hedonic prices).
>>>
>>> The OLS model is done.
>>>
>>> For the spatial model, I am assuming that hosts, when deciding the
>>> pricing of their listings, take not only into account its structural and
>>> location characteristics, but also the prices charged by near listings
>>> with similar characteristics - spatial autocorrelation is then present,
>>> at least spatial dependence is present in the dependent variable.
>>>
>>> As I wrote in my previous post, I was willing to consider the neighbor
>>> itself as a neighbor.
>>>
>>> Parts of my code can be found below:
>>>
>>> ########
>>>
>>> ## packages
>>>
>>> packages_install <- function(packages){
>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>> if (length(new.packages))
>>> install.packages(new.packages, dependencies = TRUE)
>>> sapply(packages, require, character.only = TRUE)
>>> }
>>>
>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>> packages_install(packages_required)
>>>
>>> # Working directory
>>> setwd("C:/Users/User/R")
>>>
>>>
>>>
>>> ## shapefile_us
>>>
>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>
>>> # Columns removal
>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>
>>> # Column rename: ZCTA5CE10
>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>
>>> # Column class change: zipcode
>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>
>>>
>>>
>>> ## polygon_nyc
>>>
>>> # Zip code not available in shapefile: 11695
>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>
>>>
>>>
>>> ## weight_matrix
>>>
>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>
>>> # Include neighbour itself as a neighbour
>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>
>>> # Weights to each neighboring polygon
>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>
>>>
>>>
>>> ## listings
>>>
>>> # Data import
>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>> listings <- listings %>% bind_rows
>>>
>>> # Characters removal
>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>> listings$price <- gsub("\\$", "", listings$price)
>>> listings$price <- gsub(",", "", listings$price)
>>>
>>>
>>>
>>> ## timedummy
>>>
>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>> timedummy <- gsub("-", "_", timedummy)
>>>
>>>
>>>
>>> ## OLS regression
>>>
>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>
>>> ########
>>>
>>> Some of my id's repeat in multiple time periods.
>>>
>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>
>>> Now I want to apply the hedonic model with the timedummy variables.
>>>
>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>
>>> Again, thank you very much for the help provided until now.
>>>
>>> Best regards,
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Tuesday, November 5, 2019 15:30
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>
>>>> I have a large pooled cross-section data set. ?I would like to
>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>> for now that spatial dependence is present in both the dependent
>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>> variable for each time period.? ?I also created a weight matrix using the
>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>> My spatial weight matrix for the whole data set will be probably a
>>>> enormous matrix with submatrices for each time period itself. I don't
>>>> think it would be possible to calculate this.? What I would like to know
>>>> is a way to estimate each time dummy/period separately (to compare
>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>> or splm?? ?Thank you and best regards,? Robert?
>>>
>>> Please do not post HTML, only plain text. Almost certainly your model
>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>> untried). What is your cross-sectional size? Using sparse kronecker
>>> products, the "enormous" matrix may not be very big. Does it make any
>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>> of the covariates time-varying? Please provide motivation and use area
>>> (preferably with affiliation (your email and user name are not
>>> informative) - this feels like a real estate problem, probably wrongly
>>> specified. You should use splm if time make sense in your case, but if it
>>> really doesn't, simplify your approach, as much of the data will be
>>> subject to very large temporal autocorrelation.
>>>
>>> If this is a continuation of your previous question about using
>>> self-neighbours, be aware that you should not use self-neighbours in
>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>
>>> Roger
>>>
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From @bdou|@ye@@r @end|ng |rom gm@||@com  Sun Nov 10 17:28:16 2019
From: @bdou|@ye@@r @end|ng |rom gm@||@com (Abdoulaye Sarr)
Date: Sun, 10 Nov 2019 16:28:16 +0000
Subject: [R-sig-Geo] issue with readOGR
In-Reply-To: <alpine.LFD.2.21.1911101427580.57003@reclus.nhh.no>
References: <CAN=6O0L77Q1BoPUcc49Y1hkY2AnBOMw9c4SoXNE2-QOaScV5Qg@mail.gmail.com>
 <alpine.LFD.2.21.1911091805080.26030@reclus.nhh.no>
 <CAN=6O0+OTnw=QRmH=mNqH8tMYXPyxnZq4M4+xGeq0MVq+2nh+w@mail.gmail.com>
 <CAN=6O0+61UNUTMa9RGCBB6pUpqT1hbXdR5qwgv9h4k4i-3+GVA@mail.gmail.com>
 <alpine.LFD.2.21.1911101427580.57003@reclus.nhh.no>
Message-ID: <CAN=6O0J13DsosRt0EierDDvytQzBSd3hZEnPn+O-5QasZ3JOXw@mail.gmail.com>

Thank Roger, that solve the issue.

Best regards,
as

On Sun, Nov 10, 2019 at 1:34 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Sat, 9 Nov 2019, Abdoulaye Sarr wrote:
>
> > Platform is osx.
> >
> > Thanks
> >
> > On Sat, Nov 9, 2019 at 6:11 PM Abdoulaye Sarr <abdoulayesar at gmail.com>
> > wrote:
> >
> >> output from list.files:
> >>
> >>      "dhaka_div.shx"        "dhaka_gazipur.cpg"
> >>  "dhaka_gazipur.dbf"    "dhaka_gazipur.prj"    "dhaka_gazipur.qpj"
> >>  "dhaka_gazipur.shp"
> >>  "dhaka_gazipur.shx"    "dhaka.dbf"            "dhaka.prj"
> >>  "dhaka.shp"
> >> "dhaka.shx"            "indicator.csv"        "r_val.csv"
> >>
>
> From this we see that "indicator.csv" is not (immediately) a file that has
> an appropriate driver (it needs a *.vrt to say which columns are which,
> see https://gdal.org/drivers/vector/csv.html). Valid layers should be
> "dhaka" and "dhaka_gazipur", but not "dhaka_div", for which  only the
> *.shx is shown.
>
> Try ogrListLayers(path.expand("/Volumes/DS2S/R_QGIS_codes//Data")) to try
> to detect valid layers. If you really meant "indicators", use
> paste0("CSV:", file.path(path.expand("/Volumes/DS2S/R_QGIS_codes//Data"),
> "indicators.csv")) as per the GDAL vector driver help page.
>
> Roger
>
> >> On Sat, Nov 9, 2019 at 5:07 PM Roger Bivand <Roger.Bivand at nhh.no>
> wrote:
> >>
> >>> On Fri, 8 Nov 2019, Abdoulaye Sarr wrote:
> >>>
> >>>> I am trying to read a shapefile using readOGR but keep getting error
> >>>> messages:
> >>>> map =
> >>>>
> readOGR(dsn=path.expand("/Volumes/DS2S/R_QGIS_codes//Data"),"indicator")
> >>>> Error in ogrInfo(dsn = dsn, layer = layer, encoding = encoding,
> >>> use_iconv =
> >>>> use_iconv,  :
> >>>>  Cannot open layer
> >>>
> >>> What is "/Volumes/DS2S/R_QGIS_codes//Data"? If a network drive, some
> OGR
> >>> drivers are known not to work with them (or that used to be the case).
> >>> Which OS/Platform is this? Is the repeated "//" correct? Can you see
> the
> >>> file running list.files("/Volumes/DS2S/R_QGIS_codes//Data") ?
> >>>
> >>> Roger
> >>>
> >>>>
> >>>> What could be causing the problem? sp 1.3-2
> >>>>
> >>>> Thanks
> >>>> as
> >>>>
> >>>>       [[alternative HTML version deleted]]
> >>>>
> >>>> _______________________________________________
> >>>> R-sig-Geo mailing list
> >>>> R-sig-Geo at r-project.org
> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>>>
> >>>
> >>> --
> >>> Roger Bivand
> >>> Department of Economics, Norwegian School of Economics,
> >>> Helleveien 30, N-5045 Bergen, Norway.
> >>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> >>> https://orcid.org/0000-0003-2392-6140
> >>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> >>>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From c@o@c@rtogr@||@ @end|ng |rom gm@||@com  Sun Nov 10 19:55:03 2019
From: c@o@c@rtogr@||@ @end|ng |rom gm@||@com (Caridad Serrano Ortega)
Date: Sun, 10 Nov 2019 19:55:03 +0100
Subject: [R-sig-Geo] 
 It's possible delimit a shapefile very extent using R?
 (Roger Bivand)
Message-ID: <CAM98pTxnPQ4_o1yJ_ATPeOeNoC2TmEyFrp5THCFYoUhbU60Gpg@mail.gmail.com>

Hi Rober,

My script with R is intermedi level and the english is basic one. Is this
your answer?

pas_file<- readOGR("D:/DATOS", "CoveredArea")

pas_file at bbox # Extend all
pas_file at polygons[[1]]@Polygons[[1]]@coords # Coordinates from the first
polygon
pas_file at polygons[[1]]@Polygons[[1]]@coords[,1] # All x from the first
polygon
pas_file at polygons[[1]]@Polygons[[1]]@coords[,2] # All y from the first
polygon
max(pas_file at polygons[[1]]@Polygons[[1]]@coords[,1])
min(pas_file at polygons[[1]]@Polygons[[1]]@coords[,1])
max(pas_file at polygons[[1]]@Polygons[[1]]@coords[,2])
min(pas_file at polygons[[1]]@Polygons[[1]]@coords[,2])

The shape file is a spatialPolygonsDataFrame so look at this link:
http://strimas.com/r/tidy-sf/

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Nov 11 11:21:53 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 11 Nov 2019 11:21:53 +0100
Subject: [R-sig-Geo] 
 It's possible delimit a shapefile very extent using R?
 (Roger Bivand)
In-Reply-To: <CAM98pTxnPQ4_o1yJ_ATPeOeNoC2TmEyFrp5THCFYoUhbU60Gpg@mail.gmail.com>
References: <CAM98pTxnPQ4_o1yJ_ATPeOeNoC2TmEyFrp5THCFYoUhbU60Gpg@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1911111119230.2810@reclus.nhh.no>

Hi Caridad,

You may be right, that Geomar actually meant the bounding box or 
coordinates of each polygon - thanks for trying to clarify. I think we'll 
have to Geomar to explain what is needed.

Roger

On Sun, 10 Nov 2019, Caridad Serrano Ortega wrote:

> Hi Rober,
>
> My script with R is intermedi level and the english is basic one. Is this
> your answer?
>
> pas_file<- readOGR("D:/DATOS", "CoveredArea")
>
> pas_file at bbox # Extend all
> pas_file at polygons[[1]]@Polygons[[1]]@coords # Coordinates from the first
> polygon
> pas_file at polygons[[1]]@Polygons[[1]]@coords[,1] # All x from the first
> polygon
> pas_file at polygons[[1]]@Polygons[[1]]@coords[,2] # All y from the first
> polygon
> max(pas_file at polygons[[1]]@Polygons[[1]]@coords[,1])
> min(pas_file at polygons[[1]]@Polygons[[1]]@coords[,1])
> max(pas_file at polygons[[1]]@Polygons[[1]]@coords[,2])
> min(pas_file at polygons[[1]]@Polygons[[1]]@coords[,2])
>
> The shape file is a spatialPolygonsDataFrame so look at this link:
> http://strimas.com/r/tidy-sf/
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Nov 11 11:31:55 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 11 Nov 2019 11:31:55 +0100
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>

On Sun, 10 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Again, thank you for your answer. I read the material provided and 
> decided that Hierarchical Spatial Autoregressive (HSAR) could be the 
> right model for me.
>
> I indeed have the precise latitude and longitude information for all my 
> listings for NYC.
>
> I created a stratified sample (group = zipcode) with 22172 (1%) of my 
> observations called listings_sample and tried to replicate the hsar 
> model, please see below.
>
> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.

Unless you know definitely that you want to relate the response to its 
lagged value, you do not need this. Do note that the matrix is very 
sparse, so could be fitted without difficulty with ML in a cross-sectional 
model.

>
> You recommended then to introduce a Markov random field (MRF) random 
> effect (RE) at the zipcode level, but I did not understand it so well. 
> Could you develop a litte more?
>

Did you read the development in 
https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and 
includes code for fitting the Beijing housing parcels data se from HSAR 
with many other packages (MCMC, INLA, hglm, etc.). I guess that you should 
try to create a model that works on a single borough, sing the zipcodes 
in that borough as a proxy for unobserved neighbourhood effects. Try for 
example using lme4::lmer() with only a zipcode IID random effect, see if 
the hedonic estimates are similar to lm(), and leave adding an MRF RE 
(with for example mgcv::gam() or hglm::hglm()) until you have a working 
testbed. Then advance step-by-step from there.

You still have not said how many repeat lettings you see - it will affect 
the way you specify your model.

Roger

> ##############
> library(spdep)
> library(HSAR)
> library(dplyr)
> library(splitstackshape)
>
>
> # Stratified sample per zipcode (size = 1%) listings_sample <- 
> splitstackshape::stratified(indt = listings, group = "zipcode", size = 
> 0.01)
>
> # Removing zipcodes from polygon_nyc which are not observable in 
> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode 
> %in% c(unique(as.character(listings_sample$zipcode))))
>
>
> ## Random effect matrix (N by J)
>
> # N: 22172
> # J: 154
>
> # Arrange listings_sample by zipcode (ascending)
> listings_sample <- listings_sample %>% arrange(zipcode)
>
> # Count number of listings per zipcode
> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
> # sum(MM$count)
>
> # N by J nulled matrix creation
> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>
> # The total number of neighbourhood
> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>
> for(i in 1:dim(MM)[1]) {
>  Delta[Uid==i,i] <- 1
> }
> rm(i)
>
> Delta <- as(Delta,"dgCMatrix")
>
>
> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>
> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>
> # Include neighbour itself as a neighbour
> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>
> # Spatial weights matrix for nb
> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>
>
> ## Fit HSAR SAR upper level random effect
> model <- as.formula(log_price ~ guests_included + minimum_nights)
>
> betas = coef(lm(formula = model, data = listings_sample))
> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>
> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>
> ##############
>
> Thank you and best regards
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Friday, November 8, 2019 13:29
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Fri, 8 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Thank you for your answer.
>>
>> I successfully used the function nb2blocknb() for a smaller dataset.
>>
>> But for a dataset of over 2 million observations, I get the following
>> error: "Error: cannot allocate vector of size 840 Kb".
>
> I don't think the observations are helpful. If you have repeat lets in the
> same property in a given month, you need to handle that anyway. I'd go for
> making the modelling exercise work (we agree that this is not panel data,
> right?) on a small subset first. I would further argue that you need a
> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
> RE. You could very well take (stratified) samples per zipcode to represent
> your data. Once that works, introduce an MRF RE at the zipcode level,
> where you do know relative position. Using SARAR is going to be a waste of
> time unless you can geocode the letting addresses. A multi-level approach
> will work. Having big data in your case with no useful location
> information per observation is just adding noise and over-smoothing, I'm
> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
> will work, also when you sample the within zipcode lets, given a split
> into training and test sets, and making CV possible.
>
> Roger
>
>>
>> I am expecting that at least 500.000 observations will be dropped due
>> the lack of values for the chosen variables for the regression model, so
>> probably I will filter and remove the observations/rows that will not be
>> used anyway - do you know if there is any package that does this
>> automatically, given the variables/columns chosed by me?
>>
>> Or would you recommend me another approach to avoid the above mentioned
>> error?
>>
>> Thank you and best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Thursday, November 7, 2019 10:13
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Thu, 7 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Many thanks for your help.
>>>
>>> I have an additional question:
>>>
>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>> rownumbers) from my data set? For now, I am taking my data set and
>>> merging with the sf object polygon_nyc with the function
>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>> a huge n x n matrix (depending of the size of my data set).
>>>
>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>> object has only n = 177.
>>>
>>> Of course running
>>>
>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>
>>> does not work ("Input data and weights have different dimensions").
>>>
>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>> zipcode) and then create the weights list lw? Or there another option?
>>
>> I think we are getting more clarity. You do not know the location of the
>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>> areas, and can create a neighbour object from these boundaries. You then
>> want to treat all the lettings in a zipcode area i as neighbours, and
>> additionally lettings in zipcode areas neighbouring i as neighbours of
>> lettings in i. This is the data structure that motivated the
>> spdep::nb2blocknb() function:
>>
>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>
>> Try running the examples to get a feel for what is going on.
>>
>> I feel that most of the variability will vanish in the very large numbers
>> of neighbours, over-smoothing the outcomes. If you do not have locations
>> for the lettings themselves, I don't think you can make much progress.
>>
>> You could try a linear mixed model (or gam with a spatially structured
>> random effect) with a temporal and a spatial random effect. See the HSAR
>> package, articles by Dong et al., and maybe
>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>> effects at the zipcode level might be a way forward, together with an IID
>> random effect at the same level (equivalent to sef-neighbours).
>>
>> Hope this helps,
>>
>> Roger
>>
>>>
>>> Best regards,
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Wednesday, November 6, 2019 15:07
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>> plain text.
>>>>
>>>> I will give a better context for my desired outcome.
>>>>
>>>> I am taking Airbnb's listings information for New York City available
>>>> on: http://insideairbnb.com/get-the-data.html
>>>>
>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>> Listings/ folder. When importing all these 54 files into one single data
>>>> set, I create a new "date_compiled" variable/column.
>>>>
>>>> In total, after the data cleansing process, I have a little more 2
>>>> million observations.
>>>
>>> You have repeat lettings for some, but not all properties. So this is at
>>> best a very unbalanced panel. For those properties with repeats, you may
>>> see temporal movement (trend/seasonal).
>>>
>>> I suggest (strongly) taking a single borough or even zipcode with some
>>> hindreds of properties, and working from there. Do not include the
>>> observation as its own neighbour, perhaps identify repeats and handle them
>>> specially (create or use a property ID). Unbalanced panels may also create
>>> a selection bias issue (why are some properties only listed sometimes?).
>>>
>>> So this although promising isn't simple, and getting to a hedonic model
>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>> necessarily trust OLS output either, partly because of the repeat property
>>> issue.
>>>
>>> Roger
>>>
>>>>
>>>> I created 54 timedummy variables for each time period available.
>>>>
>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>> a variety of characteristics which potentially determine the daily rate
>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>> of the listing as number of bedrooms, if the host if professional,
>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>> from the listing, income per capita, etc.).
>>>>
>>>> My dependent variable is price (log price, common in the related
>>>> literature for hedonic prices).
>>>>
>>>> The OLS model is done.
>>>>
>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>> pricing of their listings, take not only into account its structural and
>>>> location characteristics, but also the prices charged by near listings
>>>> with similar characteristics - spatial autocorrelation is then present,
>>>> at least spatial dependence is present in the dependent variable.
>>>>
>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>> itself as a neighbor.
>>>>
>>>> Parts of my code can be found below:
>>>>
>>>> ########
>>>>
>>>> ## packages
>>>>
>>>> packages_install <- function(packages){
>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>> if (length(new.packages))
>>>> install.packages(new.packages, dependencies = TRUE)
>>>> sapply(packages, require, character.only = TRUE)
>>>> }
>>>>
>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>> packages_install(packages_required)
>>>>
>>>> # Working directory
>>>> setwd("C:/Users/User/R")
>>>>
>>>>
>>>>
>>>> ## shapefile_us
>>>>
>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>
>>>> # Columns removal
>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>
>>>> # Column rename: ZCTA5CE10
>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>
>>>> # Column class change: zipcode
>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>
>>>>
>>>>
>>>> ## polygon_nyc
>>>>
>>>> # Zip code not available in shapefile: 11695
>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>
>>>>
>>>>
>>>> ## weight_matrix
>>>>
>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>
>>>> # Include neighbour itself as a neighbour
>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>
>>>> # Weights to each neighboring polygon
>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>
>>>>
>>>>
>>>> ## listings
>>>>
>>>> # Data import
>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>> listings <- listings %>% bind_rows
>>>>
>>>> # Characters removal
>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>> listings$price <- gsub("\\$", "", listings$price)
>>>> listings$price <- gsub(",", "", listings$price)
>>>>
>>>>
>>>>
>>>> ## timedummy
>>>>
>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>> timedummy <- gsub("-", "_", timedummy)
>>>>
>>>>
>>>>
>>>> ## OLS regression
>>>>
>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>
>>>> ########
>>>>
>>>> Some of my id's repeat in multiple time periods.
>>>>
>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>
>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>
>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>
>>>> Again, thank you very much for the help provided until now.
>>>>
>>>> Best regards,
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Tuesday, November 5, 2019 15:30
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>
>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>> for now that spatial dependence is present in both the dependent
>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>> think it would be possible to calculate this.? What I would like to know
>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>
>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>> of the covariates time-varying? Please provide motivation and use area
>>>> (preferably with affiliation (your email and user name are not
>>>> informative) - this feels like a real estate problem, probably wrongly
>>>> specified. You should use splm if time make sense in your case, but if it
>>>> really doesn't, simplify your approach, as much of the data will be
>>>> subject to very large temporal autocorrelation.
>>>>
>>>> If this is a continuation of your previous question about using
>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>>       [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From r@i@1290 m@iii@g oii @im@com  Mon Nov 11 16:37:43 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Mon, 11 Nov 2019 15:37:43 +0000 (UTC)
Subject: [R-sig-Geo] Calculating weighted values across grid cells annually
References: <327608275.2752322.1573486663885.ref@mail.yahoo.com>
Message-ID: <327608275.2752322.1573486663885@mail.yahoo.com>

Hi there,I am currently trying to calculate "weighted" spatial annual "global" values for precipitation using the object "Prec20". I need to do this for each of the 140 years (i.e. 140 layers) of global precipitation data that I have. The idea would be to somehow apply weights to each grid cell value for each year by using the cosine of its latitude (which means grid cells at the equator would have a weight of 1 (i.e. the cosine of 0 degrees is 1), and the poles would have a value of 1 (as the cosine of 90 is 1)). The idea would also then be to make a time series of these values, from Year 1 to Year 140, after all newly derived grid cell values are averaged for each year, creating 140 (weighted) averages)).?The object "Prec20" looks like this:class       : RasterBrick 
dimensions  : 64, 128, 8192, 140  (nrow, ncol, ncell, nlayers)
resolution  : 2.8125, 2.789327  (x, y)
extent      : -181.4062, 178.5938, -89.25846, 89.25846  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
data source : C:/Users/Travis/Documents/Other documents/All netCDF files/netcdffiles/MaxPrecIPSLIPSL-CM5B-LR1pctCO2.nc 
names       : X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, ... 
z-value     : 1, 140 (min, max)
varname     : onedaymax 
Here is what I have done so far:ncfname20 <- "MaxPrecIPSLIPSL-CM5B-LR1pctCO2.nc"Prec20 <- brick(ncfname20,var="onedaymax")w20 <- cos(Prec20*(pi/180))
Would this approach apply the weights appropriately??
I would greatly appreciate any help with this!!!!Thanks,
	[[alternative HTML version deleted]]


From |red@r@r@mo@ @end|ng |rom gm@||@com  Tue Nov 12 13:33:18 2019
From: |red@r@r@mo@ @end|ng |rom gm@||@com (Fred Ramos)
Date: Tue, 12 Nov 2019 13:33:18 +0100
Subject: [R-sig-Geo] A surface of GWR predicted values
Message-ID: <5dcaa68e.1c69fb81.95d31.de66@mx.google.com>

Dear all,

I`m trying to build a surface with predicted values using gwr.predict. 

When I run the gwr.predict without giving the fitting point it runs without problems. But when I enter with SpatialPointsDataFrame (a point grid) as fitting points the prediction values returns NA. Is there anything that I could do to get these results in the fitting points?


> gwr_test <- gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,kernel="gaussian",bw=21720)

> gwr_test$SDF
class       : SpatialPointsDataFrame 
features    : 423 
extent      : 292782, 414055.3, 7349266, 7424404  (xmin, xmax, ymin, ymax)
crs         : +proj=utm +zone=23 +south +ellps=intl +units=m +no_defs 
variables   : 5
names       :     Intercept_coef,        distances_coef, pop_density_log_coef,        prediction,    prediction_var 
min values  : -0.958964210431336, -0.000128871984509238,    0.287006700717343, -3.26862056578432,  0.48618073025782 
max values  :   4.17712723602899,  -1.8499577687439e-06,    0.979959974621219,  5.64311734285255, 0.693583031013022 

> gwr_out_grid_test <- gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,predictdata=grade_g_DF,kernel="gaussian",bw=21720)


> gwr_out_grid_test$SDF
class       : SpatialPointsDataFrame 
features    : 9075 
extent      : 293282, 413282, 7349904, 7423904  (xmin, xmax, ymin, ymax)
crs         : +proj=utm +zone=23 +south +ellps=intl +units=m +no_defs 
variables   : 5
names       :    Intercept_coef,        distances_coef, pop_density_log_coef, prediction, prediction_var 
min values  : -1.18701861474003, -0.000127242449368378,    0.310710138723114,         NA,             NA 
max values  :  4.04479455484814,  2.02812418949068e-06,    0.995539141570355,         NA,             NA 

Many thanks,
Fred. 




Sent from Mail for Windows 10


	[[alternative HTML version deleted]]


From dechen@|h@m @end|ng |rom |eu@uzh@ch  Tue Nov 12 13:51:51 2019
From: dechen@|h@m @end|ng |rom |eu@uzh@ch (Dechen Lham)
Date: Tue, 12 Nov 2019 13:51:51 +0100
Subject: [R-sig-Geo] Error in mixed model spatial prediction
Message-ID: <EA552AC0-496B-4F7F-B844-AEC4396D95A8@ieu.uzh.ch>

Hi all,

I am trying to do a mixed logistic model prediction on a rasterstack (3 layers) as below and it gives me this error.:

# model response is a integer (0,1)
model1 <- glmer(loss ~ Location + Presence_Absence_SPP + density_am +
                                       (1|grid_id), family = binomial("logit"), data=DATA)

# rs is raster layer composed of 2 layers of factors and 1 layer of numeric variable.
mod_pred <- predict(rs, m1, type="response", index=1:3)

# error
Error in `contrasts<-`(`*tmp*`, value = contrasts.arg[[nn]]) : 
  contrasts apply only to factors

# i tried to do with re.form=~0, based on some suggestions on the net. But think it is not correct to do this, otherwise the model is a glm predictions and not a mixed model predictions. 

With or without re.form=~0, i get the same error. Could anyone suggest one can make such predictions using rasterstack in r? I checked my raster rs structure and the variables have class like the ones in model1. So i am unable to understand what i am doing wrong. Some help is highly appreciated.

Best


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Nov 12 13:59:17 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 12 Nov 2019 13:59:17 +0100
Subject: [R-sig-Geo] A surface of GWR predicted values
In-Reply-To: <5dcaa68e.1c69fb81.95d31.de66@mx.google.com>
References: <5dcaa68e.1c69fb81.95d31.de66@mx.google.com>
Message-ID: <alpine.LFD.2.21.1911121357390.54188@reclus.nhh.no>

Please do not post HTML, only plain text.

You do need to say which package gwr.predict() comes from. Better, provide 
a reproducible example with a built-in data set showing your problem.

Roger

On Tue, 12 Nov 2019, Fred Ramos wrote:

> Dear all,
>
> I`m trying to build a surface with predicted values using gwr.predict.
>
> When I run the gwr.predict without giving the fitting point it runs without problems. But when I enter with SpatialPointsDataFrame (a point grid) as fitting points the prediction values returns NA. Is there anything that I could do to get these results in the fitting points?
>
>
>> gwr_test <- gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,kernel="gaussian",bw=21720)
>
>> gwr_test$SDF
> class       : SpatialPointsDataFrame
> features    : 423
> extent      : 292782, 414055.3, 7349266, 7424404  (xmin, xmax, ymin, ymax)
> crs         : +proj=utm +zone=23 +south +ellps=intl +units=m +no_defs
> variables   : 5
> names       :     Intercept_coef,        distances_coef, pop_density_log_coef,        prediction,    prediction_var
> min values  : -0.958964210431336, -0.000128871984509238,    0.287006700717343, -3.26862056578432,  0.48618073025782
> max values  :   4.17712723602899,  -1.8499577687439e-06,    0.979959974621219,  5.64311734285255, 0.693583031013022
>
>> gwr_out_grid_test <- gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,predictdata=grade_g_DF,kernel="gaussian",bw=21720)
>
>
>> gwr_out_grid_test$SDF
> class       : SpatialPointsDataFrame
> features    : 9075
> extent      : 293282, 413282, 7349904, 7423904  (xmin, xmax, ymin, ymax)
> crs         : +proj=utm +zone=23 +south +ellps=intl +units=m +no_defs
> variables   : 5
> names       :    Intercept_coef,        distances_coef, pop_density_log_coef, prediction, prediction_var
> min values  : -1.18701861474003, -0.000127242449368378,    0.310710138723114,         NA,             NA
> max values  :  4.04479455484814,  2.02812418949068e-06,    0.995539141570355,         NA,             NA
>
> Many thanks,
> Fred.
>
>
>
>
> Sent from Mail for Windows 10
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From bi@bi@iu m@iii@g oii whu@edu@c@  Tue Nov 12 16:18:54 2019
From: bi@bi@iu m@iii@g oii whu@edu@c@ (bi@bi@iu m@iii@g oii whu@edu@c@)
Date: Tue, 12 Nov 2019 23:18:54 +0800 (GMT+08:00)
Subject: [R-sig-Geo] A surface of GWR predicted values
In-Reply-To: <alpine.LFD.2.21.1911121357390.54188@reclus.nhh.no>
References: <5dcaa68e.1c69fb81.95d31.de66@mx.google.com>
 <alpine.LFD.2.21.1911121357390.54188@reclus.nhh.no>
Message-ID: <6cd12576.4342.16e60323903.Coremail.binbinlu@whu.edu.cn>

Dear Fred,

It seems that you were using the gwr.predict from the GWmodel package.




Note that the condition of outputing predictions at specific locations is that observations of the corresponding exploratory variables are available.

The predictions are output when you use the following routine:

gwr_test <- gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,kernel="gaussian",bw=21720)

because you were predicting the observations.

When you specified a point grid, NA were returned. I assumed that you didn't have any observed exploratory variables at them, and that's why. Note that GWR is not an interpolation technique. 




Hope it helps.

Cheers,

Binbin








> -----????-----
> ???: "Roger Bivand" <Roger.Bivand at nhh.no>
> ????: 2019-11-12 20:59:17 (???)
> ???: "Fred Ramos" <fred.r.ramos at gmail.com>
> ??: "r-sig-geo at r-project.org" <r-sig-geo at r-project.org>
> ??: Re: [R-sig-Geo] A surface of GWR predicted values
> 
> Please do not post HTML, only plain text.
> 
> You do need to say which package gwr.predict() comes from. Better, provide 
> a reproducible example with a built-in data set showing your problem.
> 
> Roger
> 
> On Tue, 12 Nov 2019, Fred Ramos wrote:
> 
> > Dear all,
> >
> > I`m trying to build a surface with predicted values using gwr.predict.
> >
> > When I run the gwr.predict without giving the fitting point it runs without problems. But when I enter with SpatialPointsDataFrame (a point grid) as fitting points the prediction values returns NA. Is there anything that I could do to get these results in the fitting points?
> >
> >
> >> gwr_test <- gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,kernel="gaussian",bw=21720)
> >
> >> gwr_test$SDF
> > class       : SpatialPointsDataFrame
> > features    : 423
> > extent      : 292782, 414055.3, 7349266, 7424404  (xmin, xmax, ymin, ymax)
> > crs         : +proj=utm +zone=23 +south +ellps=intl +units=m +no_defs
> > variables   : 5
> > names       :     Intercept_coef,        distances_coef, pop_density_log_coef,        prediction,    prediction_var
> > min values  : -0.958964210431336, -0.000128871984509238,    0.287006700717343, -3.26862056578432,  0.48618073025782
> > max values  :   4.17712723602899,  -1.8499577687439e-06,    0.979959974621219,  5.64311734285255, 0.693583031013022
> >
> >> gwr_out_grid_test <- gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,predictdata=grade_g_DF,kernel="gaussian",bw=21720)
> >
> >
> >> gwr_out_grid_test$SDF
> > class       : SpatialPointsDataFrame
> > features    : 9075
> > extent      : 293282, 413282, 7349904, 7423904  (xmin, xmax, ymin, ymax)
> > crs         : +proj=utm +zone=23 +south +ellps=intl +units=m +no_defs
> > variables   : 5
> > names       :    Intercept_coef,        distances_coef, pop_density_log_coef, prediction, prediction_var
> > min values  : -1.18701861474003, -0.000127242449368378,    0.310710138723114,         NA,             NA
> > max values  :  4.04479455484814,  2.02812418949068e-06,    0.995539141570355,         NA,             NA
> >
> > Many thanks,
> > Fred.
> >
> >
> >
> >
> > Sent from Mail for Windows 10
> >
> >
> >  [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
> 
> -- 
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo



	[[alternative HTML version deleted]]


From |red@r@r@mo@ @end|ng |rom gm@||@com  Wed Nov 13 10:15:38 2019
From: |red@r@r@mo@ @end|ng |rom gm@||@com (Fred Ramos)
Date: Wed, 13 Nov 2019 10:15:38 +0100
Subject: [R-sig-Geo] A surface of GWR predicted values
In-Reply-To: <6cd12576.4342.16e60323903.Coremail.binbinlu@whu.edu.cn>
References: <5dcaa68e.1c69fb81.95d31.de66@mx.google.com>
 <alpine.LFD.2.21.1911121357390.54188@reclus.nhh.no>
 <6cd12576.4342.16e60323903.Coremail.binbinlu@whu.edu.cn>
Message-ID: <5dcbc9ba.1c69fb81.94a41.8604@mx.google.com>

Dear Binbin,

Yes, it helped a lot. Thanks for the clarification. 

Best,
Fred.

Sent from Mail for Windows 10

From: binbinlu at whu.edu.cn
Sent: Tuesday, November 12, 2019 4:19 PM
To: roger.bivand at nhh.no; fred ramos
Cc: r-sig-geo at r-project.org
Subject: Re: Re: [R-sig-Geo] A surface of GWR predicted values

Dear Fred, 
It seems that you were using the gwr.predict from the GWmodel package. 

Note that the condition of outputing predictions at specific locations is that observations of the corresponding?exploratory variables are available. 
The predictions are output when you use the following routine: 
gwr_test?<-?gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,kernel="gaussian",bw=21720) 
because you were predicting the observations. 
When you specified a point grid, NA were returned. I assumed that you didn't have any observed exploratory variables at them, and that's why. Note that GWR is not an interpolation technique.? 

Hope it helps. 
Cheers, 
Binbin 



>?-----????-----
>????:?"Roger?Bivand"?<Roger.Bivand at nhh.no>
>?????:?2019-11-12?20:59:17?(???)
>????:?"Fred?Ramos"?<fred.r.ramos at gmail.com>
>???:?"r-sig-geo at r-project.org"?<r-sig-geo at r-project.org>
>???:?Re:?[R-sig-Geo]?A?surface?of?GWR?predicted?values
>?
>?Please?do?not?post?HTML,?only?plain?text.
>?
>?You?do?need?to?say?which?package?gwr.predict()?comes?from.?Better,?provide?
>?a?reproducible?example?with?a?built-in?data?set?showing?your?problem.
>?
>?Roger
>?
>?On?Tue,?12?Nov?2019,?Fred?Ramos?wrote:
>?
>?>?Dear?all,
>?>
>?>?I`m?trying?to?build?a?surface?with?predicted?values?using?gwr.predict.
>?>
>?>?When?I?run?the?gwr.predict?without?giving?the?fitting?point?it?runs?without?problems.?But?when?I?enter?with?SpatialPointsDataFrame?(a?point?grid)?as?fitting?points?the?prediction?values?returns?NA.?Is?there?anything?that?I?could?do?to?get?these?results?in?the?fitting?points?
>?>
>?>
>?>>?gwr_test?<-?gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,kernel="gaussian",bw=21720)
>?>
>?>>?gwr_test$SDF
>?>?class???????:?SpatialPointsDataFrame
>?>?features????:?423
>?>?extent??????:?292782,?414055.3,?7349266,?7424404??(xmin,?xmax,?ymin,?ymax)
>?>?crs?????????:?+proj=utm?+zone=23?+south?+ellps=intl?+units=m?+no_defs
>?>?variables???:?5
>?>?names???????:?????Intercept_coef,????????distances_coef,?pop_density_log_coef,????????prediction,????prediction_var
>?>?min?values??:?-0.958964210431336,?-0.000128871984509238,????0.287006700717343,?-3.26862056578432,??0.48618073025782
>?>?max?values??:???4.17712723602899,??-1.8499577687439e-06,????0.979959974621219,??5.64311734285255,?0.693583031013022
>?>
>?>>?gwr_out_grid_test?<-?gwr.predict(job_density_log~distances+pop_density_log,data=zonas_OD_P_sp,predictdata=grade_g_DF,kernel="gaussian",bw=21720)
>?>
>?>
>?>>?gwr_out_grid_test$SDF
>?>?class???????:?SpatialPointsDataFrame
>?>?features????:?9075
>?>?extent??????:?293282,?413282,?7349904,?7423904??(xmin,?xmax,?ymin,?ymax)
>?>?crs?????????:?+proj=utm?+zone=23?+south?+ellps=intl?+units=m?+no_defs
>?>?variables???:?5
>?>?names???????:????Intercept_coef,????????distances_coef,?pop_density_log_coef,?prediction,?prediction_var
>?>?min?values??:?-1.18701861474003,?-0.000127242449368378,????0.310710138723114,?????????NA,?????????????NA
>?>?max?values??:??4.04479455484814,??2.02812418949068e-06,????0.995539141570355,?????????NA,?????????????NA
>?>
>?>?Many?thanks,
>?>?Fred.
>?>
>?>
>?>
>?>
>?>?Sent?from?Mail?for?Windows?10
>?>
>?>
>?>? [[alternative?HTML?version?deleted]]
>?>
>?>?_______________________________________________
>?>?R-sig-Geo?mailing?list
>?>?R-sig-Geo at r-project.org
>?>?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>?>
>?
>?--?
>?Roger?Bivand
>?Department?of?Economics,?Norwegian?School?of?Economics,
>?Helleveien?30,?N-5045?Bergen,?Norway.
>?voice:?+47?55?95?93?55;?e-mail:?Roger.Bivand at nhh.no
>?https://orcid.org/0000-0003-2392-6140
>?https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>?
>?_______________________________________________
>?R-sig-Geo?mailing?list
>?R-sig-Geo at r-project.org
>?https://stat.ethz.ch/mailman/listinfo/r-sig-geo



	[[alternative HTML version deleted]]


From denn2duk @end|ng |rom y@hoo@com  Wed Nov 13 10:42:20 2019
From: denn2duk @end|ng |rom y@hoo@com (Denys Dukhovnov)
Date: Wed, 13 Nov 2019 09:42:20 +0000 (UTC)
Subject: [R-sig-Geo] Error in impacts() after lagsarlm (spdep package)
References: <1991552032.2347324.1573638140319.ref@mail.yahoo.com>
Message-ID: <1991552032.2347324.1573638140319@mail.yahoo.com>

I posted this question on StackOverflow a while back but received no answer.?

I have 9,150 polygons in my data set. I was trying to run a spatial autoregressive model (SAR) in?spdep?to test spatial dependence of my outcome variable. After running the model, I wanted to examine the direct/indirect impacts, but encountered an error that seems to have something to do with the length of neighbors in the weights matrix not being equal to?n. I tried running the very same equation as SLX model (Spatial Lag X), and?impacts()?worked fine, even though there were some polygons in my set that had no neighbors.?

> # Defining queen contiguity neighbors for polyset and storing the matrix as list
> q.nbrs <- poly2nb(polyset) 
> listweights <- nb2listw(q.nbrs, zero.policy = TRUE)

> # Defining the model
> model.equation <- TIME ~ A + B + C

> # Run SAR model
> reg <- lagsarlm(model.equation, data = polyset, listw = listweights, zero.policy = TRUE)

> # Run impacts() to show direct/indirect impacts
> impacts(reg, listw = listweights, zero.policy = TRUE)

> Error in intImpacts(rho = rho, beta = beta, P = P, n = n, mu = mu, Sigma = Sigma,??: 
? ? length(listweights$neighbours) == n is not TRUE

What am I doing wrong? I am running Windows 10 machine with R 3.5.3 with the most up-to-date spdep package, if it helps.

Thank you very much.

Regards,
Denys Dukhovnov


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Nov 13 10:54:58 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 13 Nov 2019 10:54:58 +0100
Subject: [R-sig-Geo] Error in impacts() after lagsarlm (spdep package)
In-Reply-To: <1991552032.2347324.1573638140319@mail.yahoo.com>
References: <1991552032.2347324.1573638140319.ref@mail.yahoo.com>
 <1991552032.2347324.1573638140319@mail.yahoo.com>
Message-ID: <alpine.LFD.2.21.1911131044390.99233@reclus.nhh.no>

On Wed, 13 Nov 2019, Denys Dukhovnov via R-sig-Geo wrote:

> I posted this question on StackOverflow a while back but received no answer.?

I maintain the spatialreg package to which I think you are referring. I 
never visit SO, both to save time and because the signal/noise ratio there 
is not encouraging (with exceptions where reproducible examples are used).

While your post was pain text (thanks), what you copied from SO got rather 
mangled. The spdep package does warn you that the functions you are using 
are deprecated and are actually to be called from spatialreg.

>
> I have 9,150 polygons in my data set. I was trying to run a spatial 
> autoregressive model (SAR) in spdep to test spatial dependence of my 
> outcome variable. After running the model, I wanted to examine the 
> direct/indirect impacts, but encountered an error that seems to have 
> something to do with the length of neighbors in the weights matrix not 
> being equal to n. I tried running the very same equation as SLX model 
> (Spatial Lag X), and impacts() worked fine, even though there were some 
> polygons in my set that had no neighbors.
>
>> # Defining queen contiguity neighbors for polyset and storing the 
>> matrix as list
>> q.nbrs <- poly2nb(polyset)
>> listweights <- nb2listw(q.nbrs, zero.policy = TRUE)
>
>> # Defining the model
>> model.equation <- TIME ~ A + B + C
>
>> # Run SAR model reg <- lagsarlm(model.equation, data = polyset, listw = 
>> listweights, zero.policy = TRUE)

You are completely unnecessarily using the "eigen" method. If your weights 
are symmetric, use Cholesky decomposition ("Matrix"), much faster, same 
output.

>
>> # Run impacts() to show direct/indirect impacts
>> impacts(reg, listw = listweights, zero.policy = TRUE)
>
>> Error in intImpacts(rho = rho, beta = beta, P = P, n = n, mu = mu, Sigma = Sigma,??:
> ? ? length(listweights$neighbours) == n is not TRUE
>

Never, ever, do this. Did you read LeSage & Pace? Use traces, not the 
weights themselves. With the listw object, you need to invert an nxn 
matrix once in this case, 1+R times if you run Monte Carlo simulations.

If you provide a reproducible example using built-in data, I can try to 
provide a more informative error message.

> What am I doing wrong? I am running Windows 10 machine with R 3.5.3 with 
> the most up-to-date spdep package, if it helps.
>

R is at 3.6.1.

Hope this clarifies,

Roger

> Thank you very much.
>
> Regards,
> Denys Dukhovnov
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger@B|v@nd @end|ng |rom nhh@no  Wed Nov 13 12:45:45 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 13 Nov 2019 12:45:45 +0100
Subject: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
Message-ID: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>

Because PROJ >= 6 and GDAL >= 3 change the way that PROJ strings 
(representations of coordinate reference systems) are handled, steps are 
being taken to find ways to adapt sp/rgdal workflows. A current proposal 
is to store the WKT2_2018 string as a comment to CRS objects as defined in 
the sp package.

A draft development-in-progress version of rgdal is available at 
https://r-forge.r-project.org/R/?group_id=884, and for sp at 
https://github.com/rsbivand/sp (this version of sp requires rgdal >= 
1.5-1). This adds the WKT comments to CRS objects on reading vector and 
raster data sources, and uses WKT comments if found when writing vector 
and raster objects (or at least does as far as I've checked, possibly 
fragile).

An RFC with tersely worked cases for using CRS object comments to carry 
WKT strings but maintaining full backward compatibility is online at 
http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html.

If you have other ideas or concerns about trying to use this mechanism for 
sp CRS objects, please contribute at your earliest convenience.

http://rgdal.r-forge.r-project.org/reference/list_coordOps.html shows the 
beginning of the next step, to query transformation operations to find 
viable coordinate operation pipelines.

I'm assuming that the previous behaviour (transform without considering 
accuracy with whatever is to hand) is not viable going forward, and that 
we will need two steps: list coordinate operations between source and 
target CRS (using the WKT comments as better specifications than the PROJ 
strings), possibly intervene manually to install missing grids, then 
undertake the coordinate operation.

The fallback may be simply to choose the least inaccurate available 
coordinate operation, but this should be a fallback. This means that all 
uses of spTransform() will require intervention.

Is this OK (it is tiresome but modernises workflows once), or is it not OK 
(no user intervention is crucial)?

These behaviours may be set in an option, so that package maintainers and 
users may delay modernisation, but all are undoubtedly served by rapid 
adaptation (GRASS 7.8.1 released yesterday, libspatialite, pyproj, QGIS 
development versions all state that they list candidate coordinate 
operations).

We cannot ship all the grids, they are very bulky, and probably nobody 
needs sub-metre accuracy world-wide. Work in PROJ is starting to create a 
content delivery network for trusted download and mechanisms for 
registering downloaded grids on user platforms. We would for example not 
want Windows users of rgdal and sf to have to download the same grid 
twice.

Comments welcome here and at 
https://github.com/r-spatial/discuss/issues/28 or 
https://github.com/r-spatial/sf/issues/1187

Roger

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Nov 13 20:22:11 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 13 Nov 2019 20:22:11 +0100
Subject: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
In-Reply-To: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>
References: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>
Message-ID: <alpine.LFD.2.21.1911132021070.105168@reclus.nhh.no>

And this link explains the CDN proposal for grid distribution:

https://www.spatialys.com/en/crowdfunding/

Roger

On Wed, 13 Nov 2019, Roger Bivand wrote:

> Because PROJ >= 6 and GDAL >= 3 change the way that PROJ strings 
> (representations of coordinate reference systems) are handled, steps are 
> being taken to find ways to adapt sp/rgdal workflows. A current proposal is 
> to store the WKT2_2018 string as a comment to CRS objects as defined in the 
> sp package.
>
> A draft development-in-progress version of rgdal is available at 
> https://r-forge.r-project.org/R/?group_id=884, and for sp at 
> https://github.com/rsbivand/sp (this version of sp requires rgdal >= 1.5-1). 
> This adds the WKT comments to CRS objects on reading vector and raster data 
> sources, and uses WKT comments if found when writing vector and raster 
> objects (or at least does as far as I've checked, possibly fragile).
>
> An RFC with tersely worked cases for using CRS object comments to carry WKT 
> strings but maintaining full backward compatibility is online at 
> http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html.
>
> If you have other ideas or concerns about trying to use this mechanism for sp 
> CRS objects, please contribute at your earliest convenience.
>
> http://rgdal.r-forge.r-project.org/reference/list_coordOps.html shows the 
> beginning of the next step, to query transformation operations to find viable 
> coordinate operation pipelines.
>
> I'm assuming that the previous behaviour (transform without considering 
> accuracy with whatever is to hand) is not viable going forward, and that we 
> will need two steps: list coordinate operations between source and target CRS 
> (using the WKT comments as better specifications than the PROJ strings), 
> possibly intervene manually to install missing grids, then undertake the 
> coordinate operation.
>
> The fallback may be simply to choose the least inaccurate available 
> coordinate operation, but this should be a fallback. This means that all uses 
> of spTransform() will require intervention.
>
> Is this OK (it is tiresome but modernises workflows once), or is it not OK 
> (no user intervention is crucial)?
>
> These behaviours may be set in an option, so that package maintainers and 
> users may delay modernisation, but all are undoubtedly served by rapid 
> adaptation (GRASS 7.8.1 released yesterday, libspatialite, pyproj, QGIS 
> development versions all state that they list candidate coordinate 
> operations).
>
> We cannot ship all the grids, they are very bulky, and probably nobody needs 
> sub-metre accuracy world-wide. Work in PROJ is starting to create a content 
> delivery network for trusted download and mechanisms for registering 
> downloaded grids on user platforms. We would for example not want Windows 
> users of rgdal and sf to have to download the same grid twice.
>
> Comments welcome here and at https://github.com/r-spatial/discuss/issues/28 
> or https://github.com/r-spatial/sf/issues/1187
>
> Roger
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From r@i@1290 m@iii@g oii @im@com  Wed Nov 13 21:33:00 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Wed, 13 Nov 2019 20:33:00 +0000 (UTC)
Subject: [R-sig-Geo] Computing the means of gridded data over land only
 using Raster Stack
References: <1954640022.479392.1573677180184.ref@mail.yahoo.com>
Message-ID: <1954640022.479392.1573677180184@mail.yahoo.com>

Greetings,
I am interested in calculating precipitation averages globally. However, I only want to isolate land and/or oceanic areas to compute the mean of those separately. What I would like to do is somehow isolate those grid cells whose centers overlap with either land or ocean and then compute the annual mean. I already first created a raster stack, called "RCP1pctCO2Mean", which contains the mean values of interest. There are 138 layers, with each layer representing one year.? This raster stack has the following attributes:
??? class?????? : RasterStack 
??? dimensions? : 64, 128, 8192, 138? (nrow, ncol, ncell, nlayers)
??? resolution? : 2.8125, 2.789327? (x, y)
??? extent????? : -181.4062, 178.5938, -89.25846, 89.25846? (xmin, xmax, ymin,? 
??? ymax)
??? coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
??? names?????? :??? layer.1,??? layer.2,??? layer.3,??? layer.4,??? layer.5,?????? 
??? layer.6,??? layer.7,??? layer.8,??? layer.9,?? layer.10,?? layer.11,?? 
??? layer.12,?? layer.13,?? layer.14,?? layer.15, ... 
??? min values? : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730,??? 
??? 0.53099146, 0.49757186, 0.45697752, 0.41382199, 0.46082401, 0.45516687, 
??? 0.51857087, 0.41005131, 0.45956529, 0.47497867, ... 
??? max values? :?? 96.30350,? 104.08584,?? 88.92751,?? 97.49373,?? 89.57201,?? 
??? 90.58570,?? 86.67651,?? 88.33519,?? 96.94720,? 101.58247,?? 96.07792,?? 
??? 93.21948,?? 99.59785,?? 94.26218,?? 90.62138, ... ?
Previously, I tried isolating a specific region by specifying a range of longitudes and latitudes to obtain the means and medians for that region, just like this:
? ? >expansion1<-expand.grid(103:120, 3:15) #This is a range of longitudes and then latitudes
? ? >lonlataaa<-extract(RCP1pctCO2Mean, expansion1)
? ? >Columnaaa<-colMeans(lonlataaa)
??? #Packages loaded? ??? ? library(raster)
??? library(maps)
??? library(maptools)
??? library(rasterVis)

However, with this approach, too much water can mix with land areas, and if I narrow the latitude/longitude range on land, I might miss too much land to compute the mean meaningfully.
Therefore, with this RasterStack, would it be possible to create a condition that tells R that if the "center point" or centroid of each grid cell (with each grid cell center representing a specific latitude/longitude coordinate) happens to fall on land, then it would be considered as land (i.e. that would be TRUE - if not, then FALSE, or maybe somehow use 0s or 1s)? Even if a grid cell happens to have water mixed with land, but the center point/centroid of the grid is on land, that would be considered as land. I would like to do this for specific countries, too.
I want the individual 138 layers/years to be retained, so that all the Year 1s can be averaged across all relevant grid cells, then all Year 2s, then all Year 3s, then all Year 4s, etc. (to create a time series later). I'm not sure if this is the correct way to do this, but what I did first was take the "RCP1pctCO2Mean" RasterStack and created a SpatialPolygonsDataframe using:
? ? >trans <- raster::rasterToPolygon(RCP1pctCO2Mean)? ? >trans

class       : SpatialPolygonsDataFrame 
features    : 8192 
extent      : -181.4062, 178.5938, -89.25846, 89.25846  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
variables   : 138
names       :           layer.1,           layer.2,           layer.3,           layer.4,           layer.5,           layer.6,          layer.7,           layer.8,           layer.9,          layer.10,         layer.11,          layer.12,          layer.13,          layer.14,          layer.15, ... 
min values  : 0.429645141288708, 0.433756527757047, 0.517493712584313, 0.508389826053265, 0.453667300300907, 0.530991463885754,  0.4975718597839, 0.456977516231847, 0.413821991744321, 0.460824014230889, 0.45516687008843, 0.518570869929649, 0.410051312472821, 0.459565291388527, 0.474978673081429, ... 
max values  :  96.3034965348338,  104.085840813594,  88.9275127089197,  97.4937326695693,  89.5720053000712,  90.5857030396531, 86.6765123781949,  88.3351859796546,   96.947199473011,  101.582468961459, 96.0779212204739,  93.2194802269814,  99.5978503841538,  94.2621847475029,  90.6213755054263, ... 

Could generating an id value for each of those land (or water) polygons, such that the center of those grid cells (i.e. latitude/longitude coordinates) on land are only counted, be a logical next step to do something like this? Is it possible to somehow just isolate an all-land polygon, and then somehow specify which cells are considered land, and then compute averages for each year across the grid cells? If so, there are a lot of cells to assign a weight individually, so I am not sure if there is a way to do this quickly?
Thank you, and I would greatly appreciate any assistance! I look forward to your response!
	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Thu Nov 14 19:41:43 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Thu, 14 Nov 2019 18:41:43 +0000 (UTC)
Subject: [R-sig-Geo] 
 Isolating only land areas on a global map for computing averages
In-Reply-To: <CALyPt8zEeYRTEj_CNRPiMEqyeXhvVadDBX3myOUT7hbsKoSWBg@mail.gmail.com>
References: <2098265776.786417.1573093111101.ref@mail.yahoo.com>
 <2098265776.786417.1573093111101@mail.yahoo.com>
 <CALyPt8zmd78Vfm_Av3mv63z=12s4bs7G53JvfddKZE+4v29GrA@mail.gmail.com>
 <1931301500.1089634.1573143910861@mail.yahoo.com>
 <CALyPt8zEeYRTEj_CNRPiMEqyeXhvVadDBX3myOUT7hbsKoSWBg@mail.gmail.com>
Message-ID: <89456491.990178.1573756903185@mail.yahoo.com>

Hi Tom and others,

I was able to use rasterToPolygons(RCP1pctCO2Mean) to convert my raster into polygons, which is now an object called "trans". However, does this command retain the original 138 layers/years of data, but just now in polygon form??
Also, I am not too clear as to how to generate id values for polygons as row and column indices from this new object. I was looking online for a procedure, but nothing too specific shows how to do this directly. Would it be possible to provide an example in this context??
Finally, I am unsure of what was meant by "get land polygon SpatialPolygons object "landPoly" with polygons of land only, not ocean." Do you mean the SpatialPolygonsDataframe that was created from rasterToPolygons?
Thank you, and I appreciate your help!
-----Original Message-----
From: Tom Philippi <tephilippi at gmail.com>
To: rain1290 <rain1290 at aim.com>; tephilippi <tephilippi at gmail.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Fri, Nov 8, 2019 11:04 pm
Subject: Re: [R-sig-Geo] Isolating only land areas on a global map for computing averages

Rain--Yes, it is possible to do it with your extant raster stack.? In fact, pretty much all reasonable approaches will do that.? Anything you do will create a raster layer with values at every one of your 8192 raster cells: some values will be 0 or NA.
The trivial answer if you only had a small range of latitude, and small raster cell sizes relative to your polygon, would be the raster::rasterize() function to generate a raster mask.
But, with a global raster from?+90 to -90, any interpretable answer averaging over area needs to take into account the different area of a grid cell at 70 deg latitude vs 0 deg.? See:?https://gis.stackexchange.com/questions/29734/how-to-calculate-area-of-1-x-1-degree-cells-in-a-rasterAt that point, you should go ahead and account for fractions of grid cells over land so your averaging would be over land area.
I'm reticent to give you a complete code solution because without a name, you may well be a student with this as an assignment.? But, my approach would be:
create a SpatialPolygonsDataFrame object "gridPoly" from any of your raster layers via raster::rasterToPolygons()generate an id value for each of those polygons as row & column indices from the raster, or as the cell number.get land polygon SpatialPolygons object "landPoly" with polygons of land only, not ocean.create a new SpatialPolygons object from gIntersect::gridPoly, landPoly, byid = c(TRUE, FALSE))calculate an area of each polygon in that object via geosphere::areaPolygon()? {rgeos::gArea()? only works for projected CRS}create your mask/weights raster layer with either all NA or all 0.either:?? ? ?parse the id values to row & column values.? ? ?use the triplets of row, column, and area to replace the corresponding NA or 0 values in that mask
or:? ? ?if you used cell numbers, just use the cell numbers and area values in replacement in that mask
create a second weight raster via raster::area() on one of your raster layers.
raster multiply your polygon-area and your raster::area values to give the actual weighs to use.
This still is an approximation, but likely?+/- 1-2%.
If this is still complete gibberish to you, either I need more coffee or you need to consult a good reference on working with spatial data in general.
On Thu, Nov 7, 2019 at 8:25 AM <rain1290 at aim.com> wrote:

Hi Tom and others,

Thank you for your response and suggestions!?
Yes, I loaded and used the maptools package previously to create continents on my world map, among other things. I do think that the easiest approach would be to create a raster layer for land, and then water, with the values that I have. However, my precipitation values are globally distributed - every grid cell has a precipitation value for each year (i.e. each grid cell has 138 values/layers/years). So, if I were to create a raster of only land areas, how would I have my grid cells coincide with the land areas only on that raster?
Also, would it be possible to accomplish this with the raster stack that I already have? If so, is there a way to separate all land/water areas this way using the maptools package?
Thanks, again, and I really appreciate your help!
-----Original Message-----
From: Tom Philippi <tephilippi at gmail.com>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Thu, Nov 7, 2019 12:44 am
Subject: Re: [R-sig-Geo] Isolating only land areas on a global map for computing averages

The easiest approach would be to create a separate aligned raster layer for land vs water.? There are plenty of coastline polygons available out there (e.g., maptools, rworldmap, rnaturalearth packages): choose one in your raster CRS (or choose one and spTransform() it).? Then, use a grid version of your raster to extract values from that land/water SpatialPolygons object.
1: Your idea of extracting the land/water value at each grid cell centroid gives one estimate.? Instead of TRUE/FALSE, think of the numeric equivalents 1,0,? then using those as weights for averaging across your grid cells.2: A "better" estimate would be to compute the fraction of each grid cell that is land, then use those fractional [0, 1] values as weights to compute a weighted average of precipitation over land.? At 2.8deg grid cells, a lot of heavy rainfall coastal areas would have the grid cell centroid offshore and be omitted by approach #1.3: I recommend that you think hard about averaging across cells in Lat Lon to estimate average precipitation over land.? The actual area of a ~2.8 by 2.8 deg grid cell at the equator is much larger than the same at 70 deg N.? I would spend the extra hour computing the actual area (in km^2) of land in each of your 8192 grid cells, then using those in a raster as weights for whatever calculations you do on the raster stack.? [Or you can cheat, as the area of a grid cell in degrees is a function of only the latitudes, and your required weights are multiplicative.]
Your mileage may vary...
Tom
On Wed, Nov 6, 2019 at 6:18 PM rain1290--- via R-sig-Geo <r-sig-geo at r-project.org> wrote:

Hi there,
I am interested in calculating precipitation medians globally. However, I only want to isolate land areas to compute the median. I already first created a raster stack, called "RCP1pctCO2median", which contains the median values.?There are 138 layers, with each layer representing one year.? This raster stack has the following attributes:
class?????? : RasterStack 
dimensions? : 64, 128, 8192, 138? (nrow, ncol, ncell, nlayers)
resolution? : 2.8125, 2.789327? (x, y)
extent????? : -181.4062, 178.5938, -89.25846, 89.25846? (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
names?????? :??? layer.1,??? layer.2,??? layer.3,??? layer.4,??? layer.5,??? layer.6,??? layer.7,??? layer.8,??? layer.9,?? layer.10,?? layer.11,?? layer.12,?? layer.13,?? layer.14,?? layer.15, ... 
min values? : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730, 0.53099146, 0.49757186, 0.45697752, 0.41382199, 0.46082401, 0.45516687, 0.51857087, 0.41005131, 0.45956529, 0.47497867, ... 
max values? :?? 96.30350,? 104.08584,?? 88.92751,?? 97.49373,?? 89.57201,?? 90.58570,?? 86.67651,?? 88.33519,?? 96.94720,? 101.58247,?? 96.07792,?? 93.21948,?? 99.59785,?? 94.26218,?? 90.62138, ...??

Previously, I was isolating a specific region by specifying a range of longitudes and latitudes to obtain the medians for that region, like this:
expansion1<-expand.grid(103:120, 3:15)lonlataaa<-extract(RCP1pctCO2Median, expansion1)Columnaaa<-colMeans(lonlataaa)

However, with this approach, too much water can mix with land areas, and if I narrow the latitude/longitude range on land, I might miss too much land to compute the median meaningfully.
Therefore, with this data, would it be possible to use an IF/ELSE statement to tell R that if the "center point" of each grid cell happens to fall on land, then it would be considered as land (i.e. that would be TRUE - if not, then FALSE)? Even if a grid cell happens to have water mixed with land, but the center point of the grid is on land, that would be considered land. But can R even tell what is land or water in this case?
Thank you, and I would greatly appreciate any assistance!

? ? ? ? [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo



	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Fri Nov 15 03:31:04 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Fri, 15 Nov 2019 02:31:04 +0000 (UTC)
Subject: [R-sig-Geo] Averaging values across grid cells for each layer of a
 raster brick object
References: <1406489962.1233729.1573785064160.ref@mail.yahoo.com>
Message-ID: <1406489962.1233729.1573785064160@mail.yahoo.com>

Hi there,
I am trying to average precipitation values across grid cells of a raster (which is masked to only account for land areas). This raster brick object, called "overlap.sub" has 138 layers (years). It was created as follows:

World_land <- readOGR("ne_110m_land.shp")
newprojection1 <- spTransform(World_land, CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))overlap <- crop(RCP1pctCO2Median, extent(newprojection1))
overlap.sub <- mask(overlap, newprojection1) #This isolates grid cells on all land areas
The object "overlap.sub" has the following attributes:
class       : RasterBrick 
dimensions  : 62, 127, 7874, 138  (nrow, ncol, ncell, nlayers)
resolution  : 2.8125, 2.789327  (x, y)
extent      : -178.5938, 178.5938, -89.25846, 83.67981  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
data source : C:/Users/Travis/AppData/Local/Temp/Rtmp0GALJn/raster/r_tmp_2019-11-14_201408_9732_60407.grd 
names       :    layer.1,    layer.2,    layer.3,    layer.4,    layer.5,    layer.6,    layer.7,    layer.8,    layer.9,   layer.10,   layer.11,   layer.12,   layer.13,   layer.14,   layer.15, ... 
min values  : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730, 0.53099146, 0.49757186, 0.45697752, 0.41382199, 0.46082401, 0.45516687, 0.51857087, 0.41005131, 0.45956529, 0.47497867, ... 
max values  :   87.85833,   86.73710,   88.92577,   76.44161,   82.43909,   85.03188,   77.36673,   75.86527,   94.32226,  101.58247,   86.67574,   90.96802,   99.59785,   76.78394,   88.31423, ... 
Now, to obtain the annual averages across the masked grid cells, so that I may plot a time series of the average precipitation across grid cells for Year 1, then for Year 2, then Year 3....all the way to Year 138 (essentially 138 averages):
Averageprec <- colMeans(overlap.sub)
However, this yields this error:
Error in colMeans(overlap.sub) : 
  'x' must be an array of at least two dimensions
I do believe that colMeans is the appropriate function for this, but why am I receiving this error? Unless there is another way to do this? Also, this would only average those grid cells that I masked (i.e. only the land areas), correct??Thanks, and any help/feedback would be greatly appreciated!!
	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Fri Nov 15 17:13:26 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Fri, 15 Nov 2019 16:13:26 +0000 (UTC)
Subject: [R-sig-Geo] Calculating weighted values of precipitation annually
References: <1188623405.1500079.1573834406598.ref@mail.yahoo.com>
Message-ID: <1188623405.1500079.1573834406598@mail.yahoo.com>

Hi there,I am currently trying to calculate "weighted" spatial annual global values for precipitation using the object "RCP1pctCO2Mean", which is a raster brick. I need to do this for each of the 138 years (i.e. 138 layers) of global precipitation data that I have. The idea would be to somehow apply weights to each grid cell value for each year by using the cosine of its latitude (which means grid cells at the equator would have a weight of 1 (i.e. the cosine of 0 degrees is 1), and the poles would have a value of 1 (as the cosine of 90 is 1)). The idea would also then be to make a time series of these values, from Year 1 to Year 138, after all newly derived grid cell values are averaged for each year, creating 138 (weighted) averages)).?The object "RCP1pctCO2Mean" looks like this:
class       : RasterBrick 
dimensions  : 64, 128, 8192, 140  (nrow, ncol, ncell, nlayers)
resolution  : 2.8125, 2.789327  (x, y)
extent      : -181.4062, 178.5938, -89.25846, 89.25846  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
data source : C:/Users/Travis/Documents/Other documents/All netCDF files/netcdffiles/MaxPrecIPSLIPSL-CM5B-LR1pctCO2.nc 
names       : X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, ... 
z-value     : 1, 140 (min, max)
varname     : onedaymax 

Here is what I have done so far:
newraster <- rasterToPoints(RCP1pctCO2Mean) #Not sure if this is necessary?
I then proceeded to do assign weights like this:?
weight <- cos(newraster*(pi/180))
However, this yields strange but identical precipitation values (i.e. all values are 0.97 to 0.99, which is odd) across each grid cell for each layer. I am not sure what I am doing incorrectly (if there is anything incorrect) - could it be that the "pi/180" is not necessary? Also, once this is done, how to revert to a raster stack with the new values? 
I also saw another function, called "getWeights", but I am not sure how relevant this is. I am not sure about its associated package, but I was thinking about using it like this
weight <- getWeights(newraster, f = (newraster) cos(newraster))?
Would this approach apply the weights appropriately??
I would greatly appreciate any help with this!!!!Thanks,
	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Fri Nov 15 21:59:52 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Fri, 15 Nov 2019 21:59:52 +0100
Subject: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
In-Reply-To: <alpine.LFD.2.21.1911132021070.105168@reclus.nhh.no>
References: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>
 <alpine.LFD.2.21.1911132021070.105168@reclus.nhh.no>
Message-ID: <alpine.LFD.2.21.1911152151300.143646@reclus.nhh.no>

The development version of rgdal on R-Forge is now at rev 894, and is now 
ready for trying out with PROJ6/GDAL3 workflows, and workflows that may 
migrate within 6 months to modern CRS representations. The motivating RFC 
is also updated to cover coordinate operations, the use of prepared 
(pre-searched) coordinate operations, and should be read carefully by 
anyone using rgdal::spTransform(). Note further that rgdal::project() will 
not be adapted for PROJ6, and is effectively deprecated.

I'll be running reverse dependency checks, and may be bugging package 
maintainers. I would really prefer that mainainers of packages using 
spTransform() checked themselves and joined this thread or the associated 
twitter thread: https://twitter.com/RogerBivand/status/1194586193108914177

Be ready for modern PROJ and GDAL, they are already being deployed across 
open source geospatial software, like GRASS, QGIS, pyproj, spatialite etc.

Waiting, hopefully not in vain, for contributions.

Roger

On Wed, 13 Nov 2019, Roger Bivand wrote:

> And this link explains the CDN proposal for grid distribution:
>
> https://www.spatialys.com/en/crowdfunding/
>
> Roger
>
> On Wed, 13 Nov 2019, Roger Bivand wrote:
>
>>  Because PROJ >= 6 and GDAL >= 3 change the way that PROJ strings
>>  (representations of coordinate reference systems) are handled, steps are
>>  being taken to find ways to adapt sp/rgdal workflows. A current proposal
>>  is to store the WKT2_2018 string as a comment to CRS objects as defined in
>>  the sp package.
>>
>>  A draft development-in-progress version of rgdal is available at
>>  https://r-forge.r-project.org/R/?group_id=884, and for sp at
>>  https://github.com/rsbivand/sp (this version of sp requires rgdal >=
>>  1.5-1). This adds the WKT comments to CRS objects on reading vector and
>>  raster data sources, and uses WKT comments if found when writing vector
>>  and raster objects (or at least does as far as I've checked, possibly
>>  fragile).
>>
>>  An RFC with tersely worked cases for using CRS object comments to carry
>>  WKT strings but maintaining full backward compatibility is online at
>>  http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html.
>>
>>  If you have other ideas or concerns about trying to use this mechanism for
>>  sp CRS objects, please contribute at your earliest convenience.
>>
>>  http://rgdal.r-forge.r-project.org/reference/list_coordOps.html shows the
>>  beginning of the next step, to query transformation operations to find
>>  viable coordinate operation pipelines.
>>
>>  I'm assuming that the previous behaviour (transform without considering
>>  accuracy with whatever is to hand) is not viable going forward, and that
>>  we will need two steps: list coordinate operations between source and
>>  target CRS (using the WKT comments as better specifications than the PROJ
>>  strings), possibly intervene manually to install missing grids, then
>>  undertake the coordinate operation.
>>
>>  The fallback may be simply to choose the least inaccurate available
>>  coordinate operation, but this should be a fallback. This means that all
>>  uses of spTransform() will require intervention.
>>
>>  Is this OK (it is tiresome but modernises workflows once), or is it not OK
>>  (no user intervention is crucial)?
>>
>>  These behaviours may be set in an option, so that package maintainers and
>>  users may delay modernisation, but all are undoubtedly served by rapid
>>  adaptation (GRASS 7.8.1 released yesterday, libspatialite, pyproj, QGIS
>>  development versions all state that they list candidate coordinate
>>  operations).
>>
>>  We cannot ship all the grids, they are very bulky, and probably nobody
>>  needs sub-metre accuracy world-wide. Work in PROJ is starting to create a
>>  content delivery network for trusted download and mechanisms for
>>  registering downloaded grids on user platforms. We would for example not
>>  want Windows users of rgdal and sf to have to download the same grid
>>  twice.
>>
>>  Comments welcome here and at
>>  https://github.com/r-spatial/discuss/issues/28 or
>>  https://github.com/r-spatial/sf/issues/1187
>>
>>  Roger
>> 
>> 
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From u@erc@tch @end|ng |rom out|ook@com  Sun Nov 17 00:31:53 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Sat, 16 Nov 2019 23:31:53 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
Message-ID: <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Roger,

Thank you for your message and sorry for my late answer.

Regarding the number of listings (lettings) for my data set (2.216.642 observations), each listing contains an individual id:

unique ids: 180.004
time periods: 54 (2015-01 to 2019-09)
number of ids that appear only once: 28.486 (of 180.004 ids) (15,8%)
number of ids that appear/repeat 2-10 times: 82.641 (of 180.004 ids) (45,9%)
number of ids that appear/repeat 11-30 times: 46.465 (of 180.004 ids) (25,8%)
number of ids that appear/repeat 31-54 times: 22.412 (of 180.004 ids) (12,5%)

Important to notice is that hosts can change the room_category (between entire/home apt, private room and shared room) keeping the same listing id number. In my data, the number of unique ids that in some point changed the room_type is of 7.204 ids.

--

For the OLS model, I was using only a fixed effect model, where each time period (date_compiled) (54 in total) is a time dummy.

plm::plm(formula = model, data = listings, model = "pooling", index = c("id", "date_compiled"))


--
Osland et al. (2016) (https://doi.org/10.1111/jors.12281) use a spatial fixed effects (SFE) hedonic model, where each defined neighborhood zone in the study area is represented by dummy variables.

Dong et al. (2015) (https://doi.org/10.1111/gean.12049) outline four model specifications to accommodate geographically hierarchical data structures: (1) groupwise W and fixed regional effects; (2) groupwise W and random regional effects; (3) proximity-based W and fixed regional effects; and (4) proximity-based W and random regional effects.
--

I created a new column/variable containing the borough where the zipcode is found (Manhattan, Brooklyn, Queens, Bronx, Staten Island).

If I understood it right, the (two-level) Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) considers the occurrence of spatial relations at the (lower) individual (geographical coordinates - in my case, the listing location) and (higher) group level (territorial units - in my case, zipcodes).

According to Bivand et al. (2017): "(...) W is a spatial weights matrix. The HSAR model may also be estimated without this component.". So, in this case I only estimate the Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) in a "one-level" basis, i.e., at the higher-level.

HSAR::hsar(model, data = listings, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)

(Where the "model" formula contains the 54 time dummy variables)

Do you think I can proceed with this model? I was able to calculate it.

If I remove all observations/rows with NAs in one of the chosen variables/observations, 884.183 observations remain. If I would create a W matrix for HSAR::hsar, I would have a gigantic 884.183 by 884.183 matrix. This is the reason why I put W = NULL.


Thank you and best regards

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Monday, November 11, 2019 11:31
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

On Sun, 10 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Again, thank you for your answer. I read the material provided and
> decided that Hierarchical Spatial Autoregressive (HSAR) could be the
> right model for me.
>
> I indeed have the precise latitude and longitude information for all my
> listings for NYC.
>
> I created a stratified sample (group = zipcode) with 22172 (1%) of my
> observations called listings_sample and tried to replicate the hsar
> model, please see below.
>
> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.

Unless you know definitely that you want to relate the response to its
lagged value, you do not need this. Do note that the matrix is very
sparse, so could be fitted without difficulty with ML in a cross-sectional
model.

>
> You recommended then to introduce a Markov random field (MRF) random
> effect (RE) at the zipcode level, but I did not understand it so well.
> Could you develop a litte more?
>

Did you read the development in
https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and
includes code for fitting the Beijing housing parcels data se from HSAR
with many other packages (MCMC, INLA, hglm, etc.). I guess that you should
try to create a model that works on a single borough, sing the zipcodes
in that borough as a proxy for unobserved neighbourhood effects. Try for
example using lme4::lmer() with only a zipcode IID random effect, see if
the hedonic estimates are similar to lm(), and leave adding an MRF RE
(with for example mgcv::gam() or hglm::hglm()) until you have a working
testbed. Then advance step-by-step from there.

You still have not said how many repeat lettings you see - it will affect
the way you specify your model.

Roger

> ##############
> library(spdep)
> library(HSAR)
> library(dplyr)
> library(splitstackshape)
>
>
> # Stratified sample per zipcode (size = 1%) listings_sample <-
> splitstackshape::stratified(indt = listings, group = "zipcode", size =
> 0.01)
>
> # Removing zipcodes from polygon_nyc which are not observable in
> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode
> %in% c(unique(as.character(listings_sample$zipcode))))
>
>
> ## Random effect matrix (N by J)
>
> # N: 22172
> # J: 154
>
> # Arrange listings_sample by zipcode (ascending)
> listings_sample <- listings_sample %>% arrange(zipcode)
>
> # Count number of listings per zipcode
> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
> # sum(MM$count)
>
> # N by J nulled matrix creation
> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>
> # The total number of neighbourhood
> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>
> for(i in 1:dim(MM)[1]) {
>  Delta[Uid==i,i] <- 1
> }
> rm(i)
>
> Delta <- as(Delta,"dgCMatrix")
>
>
> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>
> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>
> # Include neighbour itself as a neighbour
> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>
> # Spatial weights matrix for nb
> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>
>
> ## Fit HSAR SAR upper level random effect
> model <- as.formula(log_price ~ guests_included + minimum_nights)
>
> betas = coef(lm(formula = model, data = listings_sample))
> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>
> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>
> ##############
>
> Thank you and best regards
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Friday, November 8, 2019 13:29
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Fri, 8 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Thank you for your answer.
>>
>> I successfully used the function nb2blocknb() for a smaller dataset.
>>
>> But for a dataset of over 2 million observations, I get the following
>> error: "Error: cannot allocate vector of size 840 Kb".
>
> I don't think the observations are helpful. If you have repeat lets in the
> same property in a given month, you need to handle that anyway. I'd go for
> making the modelling exercise work (we agree that this is not panel data,
> right?) on a small subset first. I would further argue that you need a
> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
> RE. You could very well take (stratified) samples per zipcode to represent
> your data. Once that works, introduce an MRF RE at the zipcode level,
> where you do know relative position. Using SARAR is going to be a waste of
> time unless you can geocode the letting addresses. A multi-level approach
> will work. Having big data in your case with no useful location
> information per observation is just adding noise and over-smoothing, I'm
> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
> will work, also when you sample the within zipcode lets, given a split
> into training and test sets, and making CV possible.
>
> Roger
>
>>
>> I am expecting that at least 500.000 observations will be dropped due
>> the lack of values for the chosen variables for the regression model, so
>> probably I will filter and remove the observations/rows that will not be
>> used anyway - do you know if there is any package that does this
>> automatically, given the variables/columns chosed by me?
>>
>> Or would you recommend me another approach to avoid the above mentioned
>> error?
>>
>> Thank you and best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Thursday, November 7, 2019 10:13
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Thu, 7 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Many thanks for your help.
>>>
>>> I have an additional question:
>>>
>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>> rownumbers) from my data set? For now, I am taking my data set and
>>> merging with the sf object polygon_nyc with the function
>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>> a huge n x n matrix (depending of the size of my data set).
>>>
>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>> object has only n = 177.
>>>
>>> Of course running
>>>
>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>
>>> does not work ("Input data and weights have different dimensions").
>>>
>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>> zipcode) and then create the weights list lw? Or there another option?
>>
>> I think we are getting more clarity. You do not know the location of the
>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>> areas, and can create a neighbour object from these boundaries. You then
>> want to treat all the lettings in a zipcode area i as neighbours, and
>> additionally lettings in zipcode areas neighbouring i as neighbours of
>> lettings in i. This is the data structure that motivated the
>> spdep::nb2blocknb() function:
>>
>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>
>> Try running the examples to get a feel for what is going on.
>>
>> I feel that most of the variability will vanish in the very large numbers
>> of neighbours, over-smoothing the outcomes. If you do not have locations
>> for the lettings themselves, I don't think you can make much progress.
>>
>> You could try a linear mixed model (or gam with a spatially structured
>> random effect) with a temporal and a spatial random effect. See the HSAR
>> package, articles by Dong et al., and maybe
>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>> effects at the zipcode level might be a way forward, together with an IID
>> random effect at the same level (equivalent to sef-neighbours).
>>
>> Hope this helps,
>>
>> Roger
>>
>>>
>>> Best regards,
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Wednesday, November 6, 2019 15:07
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>> plain text.
>>>>
>>>> I will give a better context for my desired outcome.
>>>>
>>>> I am taking Airbnb's listings information for New York City available
>>>> on: http://insideairbnb.com/get-the-data.html
>>>>
>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>> Listings/ folder. When importing all these 54 files into one single data
>>>> set, I create a new "date_compiled" variable/column.
>>>>
>>>> In total, after the data cleansing process, I have a little more 2
>>>> million observations.
>>>
>>> You have repeat lettings for some, but not all properties. So this is at
>>> best a very unbalanced panel. For those properties with repeats, you may
>>> see temporal movement (trend/seasonal).
>>>
>>> I suggest (strongly) taking a single borough or even zipcode with some
>>> hindreds of properties, and working from there. Do not include the
>>> observation as its own neighbour, perhaps identify repeats and handle them
>>> specially (create or use a property ID). Unbalanced panels may also create
>>> a selection bias issue (why are some properties only listed sometimes?).
>>>
>>> So this although promising isn't simple, and getting to a hedonic model
>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>> necessarily trust OLS output either, partly because of the repeat property
>>> issue.
>>>
>>> Roger
>>>
>>>>
>>>> I created 54 timedummy variables for each time period available.
>>>>
>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>> a variety of characteristics which potentially determine the daily rate
>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>> of the listing as number of bedrooms, if the host if professional,
>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>> from the listing, income per capita, etc.).
>>>>
>>>> My dependent variable is price (log price, common in the related
>>>> literature for hedonic prices).
>>>>
>>>> The OLS model is done.
>>>>
>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>> pricing of their listings, take not only into account its structural and
>>>> location characteristics, but also the prices charged by near listings
>>>> with similar characteristics - spatial autocorrelation is then present,
>>>> at least spatial dependence is present in the dependent variable.
>>>>
>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>> itself as a neighbor.
>>>>
>>>> Parts of my code can be found below:
>>>>
>>>> ########
>>>>
>>>> ## packages
>>>>
>>>> packages_install <- function(packages){
>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>> if (length(new.packages))
>>>> install.packages(new.packages, dependencies = TRUE)
>>>> sapply(packages, require, character.only = TRUE)
>>>> }
>>>>
>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>> packages_install(packages_required)
>>>>
>>>> # Working directory
>>>> setwd("C:/Users/User/R")
>>>>
>>>>
>>>>
>>>> ## shapefile_us
>>>>
>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>
>>>> # Columns removal
>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>
>>>> # Column rename: ZCTA5CE10
>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>
>>>> # Column class change: zipcode
>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>
>>>>
>>>>
>>>> ## polygon_nyc
>>>>
>>>> # Zip code not available in shapefile: 11695
>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>
>>>>
>>>>
>>>> ## weight_matrix
>>>>
>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>
>>>> # Include neighbour itself as a neighbour
>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>
>>>> # Weights to each neighboring polygon
>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>
>>>>
>>>>
>>>> ## listings
>>>>
>>>> # Data import
>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>> listings <- listings %>% bind_rows
>>>>
>>>> # Characters removal
>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>> listings$price <- gsub("\\$", "", listings$price)
>>>> listings$price <- gsub(",", "", listings$price)
>>>>
>>>>
>>>>
>>>> ## timedummy
>>>>
>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>> timedummy <- gsub("-", "_", timedummy)
>>>>
>>>>
>>>>
>>>> ## OLS regression
>>>>
>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>
>>>> ########
>>>>
>>>> Some of my id's repeat in multiple time periods.
>>>>
>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>
>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>
>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>
>>>> Again, thank you very much for the help provided until now.
>>>>
>>>> Best regards,
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Tuesday, November 5, 2019 15:30
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>
>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>> for now that spatial dependence is present in both the dependent
>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>> think it would be possible to calculate this.? What I would like to know
>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>
>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>> of the covariates time-varying? Please provide motivation and use area
>>>> (preferably with affiliation (your email and user name are not
>>>> informative) - this feels like a real estate problem, probably wrongly
>>>> specified. You should use splm if time make sense in your case, but if it
>>>> really doesn't, simplify your approach, as much of the data will be
>>>> subject to very large temporal autocorrelation.
>>>>
>>>> If this is a continuation of your previous question about using
>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>>       [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From r@m@n@m|@hr@10 @end|ng |rom gm@||@com  Mon Nov 18 07:52:29 2019
From: r@m@n@m|@hr@10 @end|ng |rom gm@||@com (Raman Mishra)
Date: Mon, 18 Nov 2019 12:22:29 +0530
Subject: [R-sig-Geo] Fwd: Highlight an area using symbol
In-Reply-To: <CAGmiv2LkzYQyMvT7TQX_CzDMSK1zTOheEt8Ye=kmKVD3ALQh_g@mail.gmail.com>
References: <CAGmiv2LkzYQyMvT7TQX_CzDMSK1zTOheEt8Ye=kmKVD3ALQh_g@mail.gmail.com>
Message-ID: <CAGmiv2LVrL2bThUtELbPU1pOrfUPxbNiisphAgs+uzhTgi9kaQ@mail.gmail.com>

Raman Mishra
Senior Research Scholar
International Institute for Population Sciences, Mumbai
Mobile: +91-8879317106


---------- Forwarded message ---------
From: Raman Mishra <raman.mishra10 at gmail.com>
Date: Mon, Nov 18, 2019 at 12:19 PM
Subject: Highlight an area using symbol
To: <r-sig-geo at r-project.org>


My data have a missing value for a district, I gave 0 value for the
district and did spatial analysis (Univariate, Regression etc.).

I tried using "NA" instead of 0 value for that district but most of the
codes available online are not working with "NA" and it won't be ethical to
delete that area from my map.

I want to highlight that district using symbols.

Is it possible that after the creation of maps one can highlight that
district using some function?

Please provide a solution as I am new to R.

Thanks
Raman Mishra
Senior Research Scholar
International Institute for Population Sciences, Mumbai
Mobile: +91-8879317106

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Nov 19 14:04:09 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 19 Nov 2019 14:04:09 +0100
Subject: [R-sig-Geo] Fwd: Highlight an area using symbol
In-Reply-To: <CAGmiv2LVrL2bThUtELbPU1pOrfUPxbNiisphAgs+uzhTgi9kaQ@mail.gmail.com>
References: <CAGmiv2LkzYQyMvT7TQX_CzDMSK1zTOheEt8Ye=kmKVD3ALQh_g@mail.gmail.com>
 <CAGmiv2LVrL2bThUtELbPU1pOrfUPxbNiisphAgs+uzhTgi9kaQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1911191400280.483773@reclus.nhh.no>

On Mon, 18 Nov 2019, Raman Mishra wrote:

> Raman Mishra
> Senior Research Scholar
> International Institute for Population Sciences, Mumbai
> Mobile: +91-8879317106
>
>
> ---------- Forwarded message ---------
> From: Raman Mishra <raman.mishra10 at gmail.com>
> Date: Mon, Nov 18, 2019 at 12:19 PM
> Subject: Highlight an area using symbol
> To: <r-sig-geo at r-project.org>
>
>
> My data have a missing value for a district, I gave 0 value for the
> district and did spatial analysis (Univariate, Regression etc.).
>
> I tried using "NA" instead of 0 value for that district but most of the
> codes available online are not working with "NA" and it won't be ethical to
> delete that area from my map.
>
> I want to highlight that district using symbols.
>
> Is it possible that after the creation of maps one can highlight that
> district using some function?
>
> Please provide a solution as I am new to R.

library(sf)
nc <- st_read(system.file("shape/nc.shp", package="sf"))
library(tmap)
tm_shape(nc) + tm_fill("SID74")
set.seed(1)
is.na(nc$SID74) <- sample(1:nrow(nc), 5)
tm_shape(nc) + tm_fill("SID74")

tm_fill() denotes NA values automatically, and provides the easiest 
solution. You could also add a symbol, but I don't think this is 
necessary. tmap handles the older sp objects too, but new vector data work 
should use sf, not sp.

Hope this helps,

Roger

>
> Thanks
> Raman Mishra
> Senior Research Scholar
> International Institute for Population Sciences, Mumbai
> Mobile: +91-8879317106
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Tue Nov 19 20:14:42 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Tue, 19 Nov 2019 21:14:42 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
Message-ID: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>

I run the example with clusterR:

no_cores <- parallel::detectCores() -1
raster::beginCluster(no_cores)
?????? res <- raster::clusterR(inp, raster::stackApply, args =
list(indices=c(2,2,3,3,1,1),fun = mean))
raster::endCluster()

And the result is:

> res
class?????????? : RasterBrick
dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
resolution : 1, 1?? (x, y)
extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
source???????? : memory
names?????????? : layer.1, layer.2, layer.3
min values :???????? 1.5,???????? 3.5,???????? 5.5
max values :???????? 1.5,???????? 3.5,???????? 5.5??


layer.1, layer.2, layer.3 (?)

So what corrensponds to what?


If I run:

res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)

The result is: 

> res2
class      : RasterBrick 
dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
resolution : 1, 1  (x, y)
extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
source     : memory
names      : index_2, index_3, index_1 
min values :     1.5,     3.5,     5.5 
max values :     1.5,     3.5,     5.5 

There is no consistency with the names of the output and obscure correspondence with the indices in the case of clusterR


	[[alternative HTML version deleted]]


From |v|@|e|ro @end|ng |rom gm@||@com  Wed Nov 20 02:30:00 2019
From: |v|@|e|ro @end|ng |rom gm@||@com (Frederico Faleiro)
Date: Tue, 19 Nov 2019 22:30:00 -0300
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
Message-ID: <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>

Hi Leonidas,

both results are in the same order, but the name is different.
You can rename the first as in the second:
names(res) <- names(res2)

I provided an example to help you understand the logic.

library(raster)
beginCluster(2)
r <- raster()
values(r) <- 1
# simple sequential stack from 1 to 6 in all cells
s <- stack(r, r*2, r*3, r*4, r*5, r*6)
s
res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun =
mean))
res
res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
res2
dif <- res - res2
# exatly the same order because the difference is zero for all layers
dif
# rename
names(res) <- names(res2)

Best regards,

Frederico Faleiro

On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo <
r-sig-geo at r-project.org> wrote:

> I run the example with clusterR:
>
> no_cores <- parallel::detectCores() -1
> raster::beginCluster(no_cores)
> ?????? res <- raster::clusterR(inp, raster::stackApply, args =
> list(indices=c(2,2,3,3,1,1),fun = mean))
> raster::endCluster()
>
> And the result is:
>
> > res
> class?????????? : RasterBrick
> dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
> resolution : 1, 1?? (x, y)
> extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
> crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> source???????? : memory
> names?????????? : layer.1, layer.2, layer.3
> min values :???????? 1.5,???????? 3.5,???????? 5.5
> max values :???????? 1.5,???????? 3.5,???????? 5.5??
>
>
> layer.1, layer.2, layer.3 (?)
>
> So what corrensponds to what?
>
>
> If I run:
>
> res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>
> The result is:
>
> > res2
> class      : RasterBrick
> dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
> resolution : 1, 1  (x, y)
> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> source     : memory
> names      : index_2, index_3, index_1
> min values :     1.5,     3.5,     5.5
> max values :     1.5,     3.5,     5.5
>
> There is no consistency with the names of the output and obscure
> correspondence with the indices in the case of clusterR
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Wed Nov 20 07:36:24 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Wed, 20 Nov 2019 08:36:24 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
Message-ID: <035b3e64-34c4-a825-db8f-72a22fbd4cc5@yahoo.gr>

This is not a reasonable solution. It is not efficient to run stackapply
twice to get the right names. Each execution can take hours.


???? 20/11/2019 3:30 ?.?., ? Frederico Faleiro ??????:
> Hi Leonidas,
> 
> both results are in the same order, but the name is different.
> You can rename the first as in the second:
> names(res) <- names(res2)
> 
> I provided an example to help you understand the logic.
> 
> library(raster)
> beginCluster(2)
> r <- raster()
> values(r) <- 1
> # simple sequential stack from 1 to 6 in all cells
> s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> s
> res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun =
> mean))
> res
> res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
> res2
> dif <- res - res2
> # exatly the same order because the difference is zero for all layers
> dif
> # rename
> names(res) <- names(res2)
> 
> Best regards,
> 
> Frederico Faleiro
> 
> On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo <
> r-sig-geo at r-project.org> wrote:
> 
>> I run the example with clusterR:
>>
>> no_cores <- parallel::detectCores() -1
>> raster::beginCluster(no_cores)
>> ?????? res <- raster::clusterR(inp, raster::stackApply, args =
>> list(indices=c(2,2,3,3,1,1),fun = mean))
>> raster::endCluster()
>>
>> And the result is:
>>
>>> res
>> class?????????? : RasterBrick
>> dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>> resolution : 1, 1?? (x, y)
>> extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>> crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>> source???????? : memory
>> names?????????? : layer.1, layer.2, layer.3
>> min values :???????? 1.5,???????? 3.5,???????? 5.5
>> max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>
>>
>> layer.1, layer.2, layer.3 (?)
>>
>> So what corrensponds to what?
>>
>>
>> If I run:
>>
>> res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>
>> The result is:
>>
>>> res2
>> class      : RasterBrick
>> dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
>> resolution : 1, 1  (x, y)
>> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>> crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>> source     : memory
>> names      : index_2, index_3, index_1
>> min values :     1.5,     3.5,     5.5
>> max values :     1.5,     3.5,     5.5
>>
>> There is no consistency with the names of the output and obscure
>> correspondence with the indices in the case of clusterR
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> 


-- 
?????? ????????, ?????????
https://www.geographer.gr
PGP fingerprint: 5237 83F8 E46C D91A 9FBB C7E7 F943 C9B6 8231 0937


From btupper @end|ng |rom b|ge|ow@org  Wed Nov 20 14:05:33 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Wed, 20 Nov 2019 08:05:33 -0500
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <035b3e64-34c4-a825-db8f-72a22fbd4cc5@yahoo.gr>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <035b3e64-34c4-a825-db8f-72a22fbd4cc5@yahoo.gr>
Message-ID: <CALrbzg2QtozmMvZ9-zfEEL1B+LmzQ4Vh0zO8cv7Dc9FpqZibLw@mail.gmail.com>

Hi,

That is certainly is unexpected to have two different naming styles.
It's not really solution to take to the bank, but you could simply
compose your own names assuming that the layer orders are always
returned in ascending index order.
Would that work for you

### start
library(raster)

# Compute layer names for stackApply output
#
# @param index numeric, 1-based layer indices used for stackApply function
# @param prefix character, prefix for names
# @return character layers names in index order
layer_names <- function(index = c(2,2,3,3,1,1), prefix = c("layer.",
"index_")[1]){
  paste0(prefix, sort(unique(index)))
}

indices <- c(2,2,3,3,1,1)

r <- raster()
values(r) <- 1
# simple sequential stack from 1 to 6 in all cells
s <- stack(r, r*2, r*3, r*4, r*5, r*6)
s

beginCluster(2)
res <- clusterR(s, stackApply, args = list(indices=indices, fun = mean))
raster::endCluster()
names(res) <- layer_names(indices, prefix = "foobar.")
res

res2 <- stackApply(s, indices, mean)
names(res2) <- layer_names(indices, prefix = "foobar.")
res2
### end


On Wed, Nov 20, 2019 at 1:36 AM Leonidas Liakos via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
>
> This is not a reasonable solution. It is not efficient to run stackapply
> twice to get the right names. Each execution can take hours.
>
>
> ???? 20/11/2019 3:30 ?.?., ? Frederico Faleiro ??????:
> > Hi Leonidas,
> >
> > both results are in the same order, but the name is different.
> > You can rename the first as in the second:
> > names(res) <- names(res2)
> >
> > I provided an example to help you understand the logic.
> >
> > library(raster)
> > beginCluster(2)
> > r <- raster()
> > values(r) <- 1
> > # simple sequential stack from 1 to 6 in all cells
> > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> > s
> > res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun =
> > mean))
> > res
> > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
> > res2
> > dif <- res - res2
> > # exatly the same order because the difference is zero for all layers
> > dif
> > # rename
> > names(res) <- names(res2)
> >
> > Best regards,
> >
> > Frederico Faleiro
> >
> > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo <
> > r-sig-geo at r-project.org> wrote:
> >
> >> I run the example with clusterR:
> >>
> >> no_cores <- parallel::detectCores() -1
> >> raster::beginCluster(no_cores)
> >> ?????? res <- raster::clusterR(inp, raster::stackApply, args =
> >> list(indices=c(2,2,3,3,1,1),fun = mean))
> >> raster::endCluster()
> >>
> >> And the result is:
> >>
> >>> res
> >> class?????????? : RasterBrick
> >> dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
> >> resolution : 1, 1?? (x, y)
> >> extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
> >> crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> >> source???????? : memory
> >> names?????????? : layer.1, layer.2, layer.3
> >> min values :???????? 1.5,???????? 3.5,???????? 5.5
> >> max values :???????? 1.5,???????? 3.5,???????? 5.5??
> >>
> >>
> >> layer.1, layer.2, layer.3 (?)
> >>
> >> So what corrensponds to what?
> >>
> >>
> >> If I run:
> >>
> >> res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
> >>
> >> The result is:
> >>
> >>> res2
> >> class      : RasterBrick
> >> dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
> >> resolution : 1, 1  (x, y)
> >> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> >> crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> >> source     : memory
> >> names      : index_2, index_3, index_1
> >> min values :     1.5,     3.5,     5.5
> >> max values :     1.5,     3.5,     5.5
> >>
> >> There is no consistency with the names of the output and obscure
> >> correspondence with the indices in the case of clusterR
> >>
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >
>
>
> --
> ?????? ????????, ?????????
> https://www.geographer.gr
> PGP fingerprint: 5237 83F8 E46C D91A 9FBB C7E7 F943 C9B6 8231 0937
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo



-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
West Boothbay Harbor, Maine
http://www.bigelow.org/
https://eco.bigelow.org


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Thu Nov 21 08:52:23 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Thu, 21 Nov 2019 09:52:23 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CALrbzg2QtozmMvZ9-zfEEL1B+LmzQ4Vh0zO8cv7Dc9FpqZibLw@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <035b3e64-34c4-a825-db8f-72a22fbd4cc5@yahoo.gr>
 <CALrbzg2QtozmMvZ9-zfEEL1B+LmzQ4Vh0zO8cv7Dc9FpqZibLw@mail.gmail.com>
Message-ID: <d1249e6a-c8df-0b00-1fdf-094f2b3ad42c@yahoo.gr>

Unfortunately the names are not always in ascending order. This is the
result of my data.

names????? : index_4, index_5, index_6, index_7, index_1, index_2, index_3
min values :?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3
max values :?? 307.0,?? 297.5,?? 311.0,?? 313.0,?? 468.0,?? 290.0,?? 302.0

And worst of all, it is not a proper match with indices.

If I run it with clusterR then the result is different:

names????? : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7
min values :?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3
max values :?? 307.0,?? 297.5,?? 311.0,?? 313.0,?? 468.0,?? 290.0,?? 302.0?


The solution is to reorder the layers of the stack so that the
stackApply indices are in ascending order e.g. 1,1,1,2,2,2,3,3,3 ...

My indices of my data was like that:

4 5 6 7 1 2 3 4 5 6 7 1 2 3 5 6 7

I've reported this behavior here
https://github.com/rspatial/raster/issues/82


On 11/20/19 3:05 PM, Ben Tupper wrote:
> Hi,
>
> That is certainly is unexpected to have two different naming styles.
> It's not really solution to take to the bank, but you could simply
> compose your own names assuming that the layer orders are always
> returned in ascending index order.
> Would that work for you
>
> ### start
> library(raster)
>
> # Compute layer names for stackApply output
> #
> # @param index numeric, 1-based layer indices used for stackApply function
> # @param prefix character, prefix for names
> # @return character layers names in index order
> layer_names <- function(index = c(2,2,3,3,1,1), prefix = c("layer.",
> "index_")[1]){
>   paste0(prefix, sort(unique(index)))
> }
>
> indices <- c(2,2,3,3,1,1)
>
> r <- raster()
> values(r) <- 1
> # simple sequential stack from 1 to 6 in all cells
> s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> s
>
> beginCluster(2)
> res <- clusterR(s, stackApply, args = list(indices=indices, fun = mean))
> raster::endCluster()
> names(res) <- layer_names(indices, prefix = "foobar.")
> res
>
> res2 <- stackApply(s, indices, mean)
> names(res2) <- layer_names(indices, prefix = "foobar.")
> res2
> ### end
>
>
> On Wed, Nov 20, 2019 at 1:36 AM Leonidas Liakos via R-sig-Geo
> <r-sig-geo at r-project.org> wrote:
>> This is not a reasonable solution. It is not efficient to run stackapply
>> twice to get the right names. Each execution can take hours.
>>
>>
>> ???? 20/11/2019 3:30 ?.?., ? Frederico Faleiro ??????:
>>> Hi Leonidas,
>>>
>>> both results are in the same order, but the name is different.
>>> You can rename the first as in the second:
>>> names(res) <- names(res2)
>>>
>>> I provided an example to help you understand the logic.
>>>
>>> library(raster)
>>> beginCluster(2)
>>> r <- raster()
>>> values(r) <- 1
>>> # simple sequential stack from 1 to 6 in all cells
>>> s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>> s
>>> res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun =
>>> mean))
>>> res
>>> res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>> res2
>>> dif <- res - res2
>>> # exatly the same order because the difference is zero for all layers
>>> dif
>>> # rename
>>> names(res) <- names(res2)
>>>
>>> Best regards,
>>>
>>> Frederico Faleiro
>>>
>>> On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo <
>>> r-sig-geo at r-project.org> wrote:
>>>
>>>> I run the example with clusterR:
>>>>
>>>> no_cores <- parallel::detectCores() -1
>>>> raster::beginCluster(no_cores)
>>>> ?????? res <- raster::clusterR(inp, raster::stackApply, args =
>>>> list(indices=c(2,2,3,3,1,1),fun = mean))
>>>> raster::endCluster()
>>>>
>>>> And the result is:
>>>>
>>>>> res
>>>> class?????????? : RasterBrick
>>>> dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>>>> resolution : 1, 1?? (x, y)
>>>> extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>>>> crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>> source???????? : memory
>>>> names?????????? : layer.1, layer.2, layer.3
>>>> min values :???????? 1.5,???????? 3.5,???????? 5.5
>>>> max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>>>
>>>>
>>>> layer.1, layer.2, layer.3 (?)
>>>>
>>>> So what corrensponds to what?
>>>>
>>>>
>>>> If I run:
>>>>
>>>> res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>>>
>>>> The result is:
>>>>
>>>>> res2
>>>> class      : RasterBrick
>>>> dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
>>>> resolution : 1, 1  (x, y)
>>>> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>> crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>> source     : memory
>>>> names      : index_2, index_3, index_1
>>>> min values :     1.5,     3.5,     5.5
>>>> max values :     1.5,     3.5,     5.5
>>>>
>>>> There is no consistency with the names of the output and obscure
>>>> correspondence with the indices in the case of clusterR
>>>>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>
>> --
>> ?????? ????????, ?????????
>> https://www.geographer.gr
>> PGP fingerprint: 5237 83F8 E46C D91A 9FBB C7E7 F943 C9B6 8231 0937
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Fri Nov 22 14:25:32 2019
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Fri, 22 Nov 2019 09:25:32 -0400
Subject: [R-sig-Geo] Comparing distance among point pattern events
References: <14602263-48a5-46a4-26db-096fa3626473.ref@yahoo.com.br>
Message-ID: <14602263-48a5-46a4-26db-096fa3626473@yahoo.com.br>

Dear R-Sig-Geo Members,

I have the hypothetical point process situation:

library(spatstat)
set.seed(2019)
A <- rpoispp(100) ## First event
B <- rpoispp(50) ## Second event
C <- rpoispp(50) ## Third event
plot(A, pch=16)
plot(B, col="red", add=T)
plot(C, col="blue", add=T)

I've like to know an adequate spatial approach for comparing if on 
average the event B or C is more close to A. For this, I try to make:

AB<-superimpose(A,B)
ABd<-pairdist(AB)
AC<-superimpose(A,C)
ACd<-pairdist(A)
mean(ABd)
#[1] 0.5112954
mean(ACd)
#[1] 0.5035042

With this naive approach, I concluded that event C is more close of A 
that B. This sounds enough for a final conclusion or more robust 
analysis is possible?

Thanks in advance,

Alexandre

-- 
Alexandre dos Santos
Geotechnologies and Spatial Statistics applied to Forest Entomology
Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
Caixa Postal 244 (PO Box)
Avenida dos Ramires, s/n - Distrito Industrial
Caceres - MT - CEP 78.200-000 (ZIP code)
Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
Lattes CV: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
ResearchGate: www.researchgate.net/profile/Alexandre_Santos10
Publons: https://publons.com/researcher/3085587/alexandre-dos-santos/
--


From @@r@h@go@|ee @end|ng |rom gm@||@com  Fri Nov 22 15:09:19 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Fri, 22 Nov 2019 09:09:19 -0500
Subject: [R-sig-Geo] Comparing distance among point pattern events
In-Reply-To: <14602263-48a5-46a4-26db-096fa3626473@yahoo.com.br>
References: <14602263-48a5-46a4-26db-096fa3626473.ref@yahoo.com.br>
 <14602263-48a5-46a4-26db-096fa3626473@yahoo.com.br>
Message-ID: <CAM_vjukvthJGT8XBBNOPnaCYVWbk+bZCNdXrdP2ztt94CjJ00w@mail.gmail.com>

Hi,

Great question, and clear example.

The first problem:
ACd<-pairdist(A) instead of ACd <- pairdist(AC)

BUT

pairdist() is the wrong function: that calculates the mean distance
between ALL points, A to A and C to C as well as A to C.

You need crossdist() instead.

The most flexible approach is to roll your own permutation test. That
will work even if B and C are different sizes, etc. If you specify the
problem more exactly, there are probably parametric tests, but I like
permutation tests.


library(spatstat)
set.seed(2019)
A <- rpoispp(100) ## First event
B <- rpoispp(50) ## Second event
C <- rpoispp(50) ## Third event
plot(A, pch=16)
plot(B, col="red", add=T)
plot(C, col="blue", add=T)

ABd<-crossdist(A, B)
ACd<-crossdist(A, C)

mean(ABd)
# 0.5168865
mean(ACd)
# 0.5070118


# test the hypothesis that ABd is equal to ACd

nperm <- 999

permout <- data.frame(ABd = rep(NA, nperm), ACd = rep(NA, nperm))

# create framework for a random assignment of B and C to the existing points

BC <- superimpose(B, C)
B.len <- npoints(B)
C.len <- npoints(C)
B.sampvect <- c(rep(TRUE, B.len), rep(FALSE, C.len))

set.seed(2019)
for(i in seq_len(nperm)) {
    B.sampvect <- sample(B.sampvect)
    B.perm <- BC[B.sampvect]
    C.perm <- BC[!B.sampvect]

    permout[i, ] <- c(mean(crossdist(A, B.perm)), mean(crossdist(A, C.perm)))
}


boxplot(permout$ABd - permout$ACd)
points(1, mean(ABd) - mean(ACd), col="red")

table(abs(mean(ABd) - mean(ACd)) >= abs(permout$ABd - permout$ACd))
# FALSE  TRUE
#  573   426

sum(abs(mean(ABd) - mean(ACd)) >= abs(permout$ABd - permout$ACd)) / nperm
# 0.4264264

The difference between ACd and ABd is indistinguishable from that
obtained by a random resampling of B and C.


Sarah

On Fri, Nov 22, 2019 at 8:26 AM ASANTOS via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
>
> Dear R-Sig-Geo Members,
>
> I have the hypothetical point process situation:
>
> library(spatstat)
> set.seed(2019)
> A <- rpoispp(100) ## First event
> B <- rpoispp(50) ## Second event
> C <- rpoispp(50) ## Third event
> plot(A, pch=16)
> plot(B, col="red", add=T)
> plot(C, col="blue", add=T)
>
> I've like to know an adequate spatial approach for comparing if on
> average the event B or C is more close to A. For this, I try to make:
>
> AB<-superimpose(A,B)
> ABd<-pairdist(AB)
> AC<-superimpose(A,C)
> ACd<-pairdist(A)
> mean(ABd)
> #[1] 0.5112954
> mean(ACd)
> #[1] 0.5035042
>
> With this naive approach, I concluded that event C is more close of A
> that B. This sounds enough for a final conclusion or more robust
> analysis is possible?
>
> Thanks in advance,
>
> Alexandre
>

-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From Jo@@SKOIEN m@iii@g oii ec@europ@@eu  Fri Nov 22 17:31:54 2019
From: Jo@@SKOIEN m@iii@g oii ec@europ@@eu (Jo@@SKOIEN m@iii@g oii ec@europ@@eu)
Date: Fri, 22 Nov 2019 16:31:54 +0000
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <d1249e6a-c8df-0b00-1fdf-094f2b3ad42c@yahoo.gr>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <035b3e64-34c4-a825-db8f-72a22fbd4cc5@yahoo.gr>
 <CALrbzg2QtozmMvZ9-zfEEL1B+LmzQ4Vh0zO8cv7Dc9FpqZibLw@mail.gmail.com>,
 <d1249e6a-c8df-0b00-1fdf-094f2b3ad42c@yahoo.gr>
Message-ID: <4c31e5e072dd43dd9e231f2b45a8dcec@ec.europa.eu>

Leonidas,

I see that you are not happy with the output, but it is not so clear what you actually expect to see.


If you use stackApply directly, the indices are used in the names. Layer 1 and 8 belong to the group with index 4. It is the first group in the list of indexes, so the first layer of the output is then referred to as index_4. Then comes index_5 with layers 2, 10 and 15 of your input. The order of these names will follow the order of the first appearance of your indices. The indices gets lost with the use of clusterR, so it gives you the same output, but with names layer.1 - layer.7.


You could change the names of the result from clusterR with:

names(ResClusterR) = paste0("index_", unique(indices))


If you want your result (from stackApply or clusterR) to follow the order of your indices, you should be able to get this with:


sResClusterR = ResClusterR[[order(names(ResClusterR))]]


Does this help you further?

Jon




--
Jon Olav Sk?ien
European Commission
Joint Research Centre ? JRC.E.1
Disaster Risk Management Unit
Building 26b 1/144 | Via E.Fermi 2749, I-21027 Ispra (VA) Italy, TP 267
jon.skoien at ec.europa.eu<https://remi.webmail.ec.europa.eu/owa/redir.aspx?C=O12RUARdbvGA3WF3zGoSV0j5xMoZlQcIEwiS4Y9G8jzXRqCCC1HUCA..&URL=mailto%3ajon.skoien%40jrc.ec.europa.eu> Tel:  +39 0332 789205 Disclaimer: Views expressed in this email are those of the individual and do not necessarily represent official views of the European Commission.



________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Leonidas Liakos via R-sig-Geo <r-sig-geo at r-project.org>
Sent: 21 November 2019 08:52
To: Ben Tupper; r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] raster: stackApply problems..

Unfortunately the names are not always in ascending order. This is the
result of my data.

names      : index_4, index_5, index_6, index_7, index_1, index_2, index_3
min values :       3,       3,       3,       3,       3,       3,       3
max values :   307.0,   297.5,   311.0,   313.0,   468.0,   290.0,   302.0

And worst of all, it is not a proper match with indices.

If I run it with clusterR then the result is different:

names      : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7
min values :       3,       3,       3,       3,       3,       3,       3
max values :   307.0,   297.5,   311.0,   313.0,   468.0,   290.0,   302.0


The solution is to reorder the layers of the stack so that the
stackApply indices are in ascending order e.g. 1,1,1,2,2,2,3,3,3 ...

My indices of my data was like that:

4 5 6 7 1 2 3 4 5 6 7 1 2 3 5 6 7

I've reported this behavior here
https://urldefense.com/v3/__https://github.com/rspatial/raster/issues/82__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G-KjNZJC$


On 11/20/19 3:05 PM, Ben Tupper wrote:
> Hi,
>
> That is certainly is unexpected to have two different naming styles.
> It's not really solution to take to the bank, but you could simply
> compose your own names assuming that the layer orders are always
> returned in ascending index order.
> Would that work for you
>
> ### start
> library(raster)
>
> # Compute layer names for stackApply output
> #
> # @param index numeric, 1-based layer indices used for stackApply function
> # @param prefix character, prefix for names
> # @return character layers names in index order
> layer_names <- function(index = c(2,2,3,3,1,1), prefix = c("layer.",
> "index_")[1]){
>   paste0(prefix, sort(unique(index)))
> }
>
> indices <- c(2,2,3,3,1,1)
>
> r <- raster()
> values(r) <- 1
> # simple sequential stack from 1 to 6 in all cells
> s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> s
>
> beginCluster(2)
> res <- clusterR(s, stackApply, args = list(indices=indices, fun = mean))
> raster::endCluster()
> names(res) <- layer_names(indices, prefix = "foobar.")
> res
>
> res2 <- stackApply(s, indices, mean)
> names(res2) <- layer_names(indices, prefix = "foobar.")
> res2
> ### end
>
>
> On Wed, Nov 20, 2019 at 1:36 AM Leonidas Liakos via R-sig-Geo
> <r-sig-geo at r-project.org> wrote:
>> This is not a reasonable solution. It is not efficient to run stackapply
>> twice to get the right names. Each execution can take hours.
>>
>>
>> ???? 20/11/2019 3:30 ?.?., ? Frederico Faleiro ??????:
>>> Hi Leonidas,
>>>
>>> both results are in the same order, but the name is different.
>>> You can rename the first as in the second:
>>> names(res) <- names(res2)
>>>
>>> I provided an example to help you understand the logic.
>>>
>>> library(raster)
>>> beginCluster(2)
>>> r <- raster()
>>> values(r) <- 1
>>> # simple sequential stack from 1 to 6 in all cells
>>> s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>> s
>>> res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun =
>>> mean))
>>> res
>>> res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>> res2
>>> dif <- res - res2
>>> # exatly the same order because the difference is zero for all layers
>>> dif
>>> # rename
>>> names(res) <- names(res2)
>>>
>>> Best regards,
>>>
>>> Frederico Faleiro
>>>
>>> On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo <
>>> r-sig-geo at r-project.org> wrote:
>>>
>>>> I run the example with clusterR:
>>>>
>>>> no_cores <- parallel::detectCores() -1
>>>> raster::beginCluster(no_cores)
>>>> ?????? res <- raster::clusterR(inp, raster::stackApply, args =
>>>> list(indices=c(2,2,3,3,1,1),fun = mean))
>>>> raster::endCluster()
>>>>
>>>> And the result is:
>>>>
>>>>> res
>>>> class?????????? : RasterBrick
>>>> dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>>>> resolution : 1, 1?? (x, y)
>>>> extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>>>> crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>> source???????? : memory
>>>> names?????????? : layer.1, layer.2, layer.3
>>>> min values :???????? 1.5,???????? 3.5,???????? 5.5
>>>> max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>>>
>>>>
>>>> layer.1, layer.2, layer.3 (?)
>>>>
>>>> So what corrensponds to what?
>>>>
>>>>
>>>> If I run:
>>>>
>>>> res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>>>
>>>> The result is:
>>>>
>>>>> res2
>>>> class      : RasterBrick
>>>> dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
>>>> resolution : 1, 1  (x, y)
>>>> extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>> crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>> source     : memory
>>>> names      : index_2, index_3, index_1
>>>> min values :     1.5,     3.5,     5.5
>>>> max values :     1.5,     3.5,     5.5
>>>>
>>>> There is no consistency with the names of the output and obscure
>>>> correspondence with the indices in the case of clusterR
>>>>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$
>>>>
>>
>> --
>> ?????? ????????, ?????????
>> https://urldefense.com/v3/__https://www.geographer.gr__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835GzqxUtB7$
>> PGP fingerprint: 5237 83F8 E46C D91A 9FBB C7E7 F943 C9B6 8231 0937
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$
>
>

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$

	[[alternative HTML version deleted]]


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Fri Nov 22 20:53:12 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Fri, 22 Nov 2019 21:53:12 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <4c31e5e072dd43dd9e231f2b45a8dcec@ec.europa.eu>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <035b3e64-34c4-a825-db8f-72a22fbd4cc5@yahoo.gr>
 <CALrbzg2QtozmMvZ9-zfEEL1B+LmzQ4Vh0zO8cv7Dc9FpqZibLw@mail.gmail.com>
 <d1249e6a-c8df-0b00-1fdf-094f2b3ad42c@yahoo.gr>
 <4c31e5e072dd43dd9e231f2b45a8dcec@ec.europa.eu>
Message-ID: <93a8cf9a-006c-2af7-78dc-2a3fded6e8ff@yahoo.gr>

Thank you Jon!
In fact, that's how I thought it worked.
And that's how it worked for me all the time!
But recently, doing some manual checks on some indices I couldn't
confirm it ...
I tried to replicate the problem and my workflow with test data
(https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206) but
stackApply works fine!
However, in my real data/workflow when I try the same procedure, I have
issues with the returned names of stackApply (indexes of names do not
match).

That's the result of the real data to see what I mean, name indices
doesn't match:

> ver_median (my verification with alternative way)
class????? : RasterStack
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 379354, 529354, 4132181, 4282181? (xmin, xmax, ymin, ymax)
crs??????? : +proj=tmerc +lat_0=0 +lon_0=24 +k=0.9996 +x_0=500000 +y_0=0
+ellps=GRS80 +towgs84=-199.87,74.79,246.62,0,0,0,0 +units=m +no_defs
names????? : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7
min values :?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3
max values :?? 297.5,?? 311.0,?? 313.0,?? 468.0,?? 290.0,?? 302.0,?? 307.0


> stackapply_median (stackapply calculations)
class????? : RasterBrick
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 379354, 529354, 4132181, 4282181? (xmin, xmax, ymin, ymax)
crs??????? : +proj=tmerc +lat_0=0 +lon_0=24 +k=0.9996 +x_0=500000 +y_0=0
+ellps=GRS80 +towgs84=-199.87,74.79,246.62,0,0,0,0 +units=m +no_defs
source???? : /home/leonidas/tmp/r_tmp_2019-11-22_185242_4475_86751.grd
names????? : index_4, index_5, index_6, index_7, index_1, index_2, index_3
min values :?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3
max values :?? 307.0,?? 297.5,?? 311.0,?? 313.0,?? 468.0,?? 290.0,?? 302.0

I'll look again at my code...
I apologize for the inconvenience.







On 11/22/19 6:31 PM, Jon.SKOIEN at ec.europa.eu wrote:
>
> Leonidas,
>
> I see that you are not happy with the output, but it is not so clear
> what you actually expect to see.
>
>
> If you use stackApply directly, the indices are used in the names.
> Layer 1 and 8 belong to the group with index 4. It is the first group
> in the list of indexes, so the first layer of the output is then
> referred to as index_4. Then comes index_5 with layers 2, 10 and 15 of
> your input. The order of these names will follow the order of the
> first appearance of your?indices. The indices gets lost with the use
> of clusterR, so it gives you the same output, but with names layer.1 -
> layer.7.
>
>
> You could change the names of the result from clusterR with:
>
> names(ResClusterR) = paste0("index_", unique(indices))
>
>
> If you want your result (from stackApply or clusterR) to follow the
> order of your indices, you should be able to get this with:
>
>
> sResClusterR = ResClusterR[[order(names(ResClusterR))]]
>
>
> Does this help you further?
>
> Jon
>
>
>
>
> --
> Jon Olav Sk?ien
> European Commission
> Joint Research Centre ? JRC.E.1
> Disaster Risk Management Unit
> Building 26b 1/144 | Via E.Fermi 2749, I-21027 Ispra (VA) Italy, TP 267
> jon.skoien at ec.europa.eu Tel: +39 0332 789205 Disclaimer: Views
> expressed in this email are those of the individual and do not
> necessarily represent official views of the European Commission.
>
>
>
> ------------------------------------------------------------------------
> *From:* R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
> Leonidas Liakos via R-sig-Geo <r-sig-geo at r-project.org>
> *Sent:* 21 November 2019 08:52
> *To:* Ben Tupper; r-sig-geo at r-project.org
> *Subject:* Re: [R-sig-Geo] raster: stackApply problems..
> ?
> Unfortunately the names are not always in ascending order. This is the
> result of my data.
>
> names????? : index_4, index_5, index_6, index_7, index_1, index_2, index_3
> min values :?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3
> max values :?? 307.0,?? 297.5,?? 311.0,?? 313.0,?? 468.0,?? 290.0,?? 302.0
>
> And worst of all, it is not a proper match with indices.
>
> If I run it with clusterR then the result is different:
>
> names????? : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7
> min values :?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3
> max values :?? 307.0,?? 297.5,?? 311.0,?? 313.0,?? 468.0,?? 290.0,??
> 302.0?
>
>
> The solution is to reorder the layers of the stack so that the
> stackApply indices are in ascending order e.g. 1,1,1,2,2,2,3,3,3 ...
>
> My indices of my data was like that:
>
> 4 5 6 7 1 2 3 4 5 6 7 1 2 3 5 6 7
>
> I've reported this behavior here
> https://urldefense.com/v3/__https://github.com/rspatial/raster/issues/82__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G-KjNZJC$
>
>
>
> On 11/20/19 3:05 PM, Ben Tupper wrote:
> > Hi,
> >
> > That is certainly is unexpected to have two different naming styles.
> > It's not really solution to take to the bank, but you could simply
> > compose your own names assuming that the layer orders are always
> > returned in ascending index order.
> > Would that work for you
> >
> > ### start
> > library(raster)
> >
> > # Compute layer names for stackApply output
> > #
> > # @param index numeric, 1-based layer indices used for stackApply
> function
> > # @param prefix character, prefix for names
> > # @return character layers names in index order
> > layer_names <- function(index = c(2,2,3,3,1,1), prefix = c("layer.",
> > "index_")[1]){
> >?? paste0(prefix, sort(unique(index)))
> > }
> >
> > indices <- c(2,2,3,3,1,1)
> >
> > r <- raster()
> > values(r) <- 1
> > # simple sequential stack from 1 to 6 in all cells
> > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> > s
> >
> > beginCluster(2)
> > res <- clusterR(s, stackApply, args = list(indices=indices, fun = mean))
> > raster::endCluster()
> > names(res) <- layer_names(indices, prefix = "foobar.")
> > res
> >
> > res2 <- stackApply(s, indices, mean)
> > names(res2) <- layer_names(indices, prefix = "foobar.")
> > res2
> > ### end
> >
> >
> > On Wed, Nov 20, 2019 at 1:36 AM Leonidas Liakos via R-sig-Geo
> > <r-sig-geo at r-project.org> wrote:
> >> This is not a reasonable solution. It is not efficient to run
> stackapply
> >> twice to get the right names. Each execution can take hours.
> >>
> >>
> >> ???? 20/11/2019 3:30 ?.?., ? Frederico Faleiro ??????:
> >>> Hi Leonidas,
> >>>
> >>> both results are in the same order, but the name is different.
> >>> You can rename the first as in the second:
> >>> names(res) <- names(res2)
> >>>
> >>> I provided an example to help you understand the logic.
> >>>
> >>> library(raster)
> >>> beginCluster(2)
> >>> r <- raster()
> >>> values(r) <- 1
> >>> # simple sequential stack from 1 to 6 in all cells
> >>> s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> >>> s
> >>> res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1),
> fun =
> >>> mean))
> >>> res
> >>> res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
> >>> res2
> >>> dif <- res - res2
> >>> # exatly the same order because the difference is zero for all layers
> >>> dif
> >>> # rename
> >>> names(res) <- names(res2)
> >>>
> >>> Best regards,
> >>>
> >>> Frederico Faleiro
> >>>
> >>> On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo <
> >>> r-sig-geo at r-project.org> wrote:
> >>>
> >>>> I run the example with clusterR:
> >>>>
> >>>> no_cores <- parallel::detectCores() -1
> >>>> raster::beginCluster(no_cores)
> >>>> ?????? res <- raster::clusterR(inp, raster::stackApply, args =
> >>>> list(indices=c(2,2,3,3,1,1),fun = mean))
> >>>> raster::endCluster()
> >>>>
> >>>> And the result is:
> >>>>
> >>>>> res
> >>>> class?????????? : RasterBrick
> >>>> dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
> >>>> resolution : 1, 1?? (x, y)
> >>>> extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
> >>>> crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
> +towgs84=0,0,0
> >>>> source???????? : memory
> >>>> names?????????? : layer.1, layer.2, layer.3
> >>>> min values :???????? 1.5,???????? 3.5,???????? 5.5
> >>>> max values :???????? 1.5,???????? 3.5,???????? 5.5??
> >>>>
> >>>>
> >>>> layer.1, layer.2, layer.3 (?)
> >>>>
> >>>> So what corrensponds to what?
> >>>>
> >>>>
> >>>> If I run:
> >>>>
> >>>> res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
> >>>>
> >>>> The result is:
> >>>>
> >>>>> res2
> >>>> class????? : RasterBrick
> >>>> dimensions : 180, 360, 64800, 3? (nrow, ncol, ncell, nlayers)
> >>>> resolution : 1, 1? (x, y)
> >>>> extent???? : -180, 180, -90, 90? (xmin, xmax, ymin, ymax)
> >>>> crs??????? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> >>>> source???? : memory
> >>>> names????? : index_2, index_3, index_1
> >>>> min values :???? 1.5,???? 3.5,???? 5.5
> >>>> max values :???? 1.5,???? 3.5,???? 5.5
> >>>>
> >>>> There is no consistency with the names of the output and obscure
> >>>> correspondence with the indices in the case of clusterR
> >>>>
> >>>>
> >>>>???????? [[alternative HTML version deleted]]
> >>>>
> >>>> _______________________________________________
> >>>> R-sig-Geo mailing list
> >>>> R-sig-Geo at r-project.org
> >>>>
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$
>
> >>>>
> >>
> >> --
> >> ?????? ????????, ?????????
> >>
> https://urldefense.com/v3/__https://www.geographer.gr__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835GzqxUtB7$
>
> >> PGP fingerprint: 5237 83F8 E46C D91A 9FBB C7E7 F943 C9B6 8231 0937
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org
> >>
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$
>
> >
> >
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Sat Nov 23 13:04:17 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sat, 23 Nov 2019 13:04:17 +0100
Subject: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
In-Reply-To: <alpine.LFD.2.21.1911152151300.143646@reclus.nhh.no>
References: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>
 <alpine.LFD.2.21.1911132021070.105168@reclus.nhh.no>
 <alpine.LFD.2.21.1911152151300.143646@reclus.nhh.no>
Message-ID: <alpine.LFD.2.21.1911231259370.538818@reclus.nhh.no>

A description of the status now with regard to a prototype resolution is 
online at:

https://rsbivand.github.io/ECS530_h19/ECS530_III.html

I'm planning to stream a talk about this at 09:15-11:00 CET on Tuesday 3 
December. I need a volunteer to test the streaming link in advance during 
next week. I'm unsure which technology to use for remote participants to 
provide feedback.

Contributions/comments welcome!

Roger


On Fri, 15 Nov 2019, Roger Bivand wrote:

> The development version of rgdal on R-Forge is now at rev 894, and is now 
> ready for trying out with PROJ6/GDAL3 workflows, and workflows that may 
> migrate within 6 months to modern CRS representations. The motivating RFC is 
> also updated to cover coordinate operations, the use of prepared 
> (pre-searched) coordinate operations, and should be read carefully by anyone 
> using rgdal::spTransform(). Note further that rgdal::project() will not be 
> adapted for PROJ6, and is effectively deprecated.
>
> I'll be running reverse dependency checks, and may be bugging package 
> maintainers. I would really prefer that mainainers of packages using 
> spTransform() checked themselves and joined this thread or the associated 
> twitter thread: https://twitter.com/RogerBivand/status/1194586193108914177
>
> Be ready for modern PROJ and GDAL, they are already being deployed across 
> open source geospatial software, like GRASS, QGIS, pyproj, spatialite etc.
>
> Waiting, hopefully not in vain, for contributions.
>
> Roger
>
> On Wed, 13 Nov 2019, Roger Bivand wrote:
>
>>  And this link explains the CDN proposal for grid distribution:
>>
>>  https://www.spatialys.com/en/crowdfunding/
>>
>>  Roger
>>
>>  On Wed, 13 Nov 2019, Roger Bivand wrote:
>>
>>>   Because PROJ >= 6 and GDAL >= 3 change the way that PROJ strings
>>>   (representations of coordinate reference systems) are handled, steps are
>>>   being taken to find ways to adapt sp/rgdal workflows. A current proposal
>>>   is to store the WKT2_2018 string as a comment to CRS objects as defined
>>>   in
>>>   the sp package.
>>>
>>>   A draft development-in-progress version of rgdal is available at
>>>   https://r-forge.r-project.org/R/?group_id=884, and for sp at
>>>   https://github.com/rsbivand/sp (this version of sp requires rgdal >=
>>>   1.5-1). This adds the WKT comments to CRS objects on reading vector and
>>>   raster data sources, and uses WKT comments if found when writing vector
>>>   and raster objects (or at least does as far as I've checked, possibly
>>>   fragile).
>>>
>>>   An RFC with tersely worked cases for using CRS object comments to carry
>>>   WKT strings but maintaining full backward compatibility is online at
>>>   http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html.
>>>
>>>   If you have other ideas or concerns about trying to use this mechanism
>>>   for
>>>   sp CRS objects, please contribute at your earliest convenience.
>>>
>>>   http://rgdal.r-forge.r-project.org/reference/list_coordOps.html shows
>>>   the
>>>   beginning of the next step, to query transformation operations to find
>>>   viable coordinate operation pipelines.
>>>
>>>   I'm assuming that the previous behaviour (transform without considering
>>>   accuracy with whatever is to hand) is not viable going forward, and that
>>>   we will need two steps: list coordinate operations between source and
>>>   target CRS (using the WKT comments as better specifications than the
>>>   PROJ
>>>   strings), possibly intervene manually to install missing grids, then
>>>   undertake the coordinate operation.
>>>
>>>   The fallback may be simply to choose the least inaccurate available
>>>   coordinate operation, but this should be a fallback. This means that all
>>>   uses of spTransform() will require intervention.
>>>
>>>   Is this OK (it is tiresome but modernises workflows once), or is it not
>>>   OK
>>>   (no user intervention is crucial)?
>>>
>>>   These behaviours may be set in an option, so that package maintainers
>>>   and
>>>   users may delay modernisation, but all are undoubtedly served by rapid
>>>   adaptation (GRASS 7.8.1 released yesterday, libspatialite, pyproj, QGIS
>>>   development versions all state that they list candidate coordinate
>>>   operations).
>>>
>>>   We cannot ship all the grids, they are very bulky, and probably nobody
>>>   needs sub-metre accuracy world-wide. Work in PROJ is starting to create
>>>   a
>>>   content delivery network for trusted download and mechanisms for
>>>   registering downloaded grids on user platforms. We would for example not
>>>   want Windows users of rgdal and sf to have to download the same grid
>>>   twice.
>>>
>>>   Comments welcome here and at
>>>   https://github.com/r-spatial/discuss/issues/28 or
>>>   https://github.com/r-spatial/sf/issues/1187
>>>
>>>   Roger
>>>
>>> 
>> 
>> 
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Sat Nov 23 23:04:47 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Sun, 24 Nov 2019 00:04:47 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <4c31e5e072dd43dd9e231f2b45a8dcec@ec.europa.eu>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <035b3e64-34c4-a825-db8f-72a22fbd4cc5@yahoo.gr>
 <CALrbzg2QtozmMvZ9-zfEEL1B+LmzQ4Vh0zO8cv7Dc9FpqZibLw@mail.gmail.com>
 <d1249e6a-c8df-0b00-1fdf-094f2b3ad42c@yahoo.gr>
 <4c31e5e072dd43dd9e231f2b45a8dcec@ec.europa.eu>
Message-ID: <073f2edd-9237-4f6f-59ae-5361af0960e3@yahoo.gr>

Ok, I think it's a bug and I'd like your help to confirm this. When the
x parameter (a raster stack in this case) of stackApply is quite heavy,
then the result of the stackApply is written to the disk and for some
reason the name indexes are shuffled. Conversely, when the provided
raster stack? is lightweight, the result is stored in memory, and the
name indexes match the provided indices correctly. I am posting a gist
again where I create a heavy raster stack to replicate the problem.
Please ensure that the raster stack is heavy enough for your machine
that the result of the stackApply is written to the disk.
Could you please confirm it?
Here is the gist:
https://gist.github.com/kokkytos/76a283a770df10c05b4fb4ce97e2b40e



On 11/22/19 6:31 PM, Jon.SKOIEN at ec.europa.eu wrote:
>
> Leonidas,
>
> I see that you are not happy with the output, but it is not so clear
> what you actually expect to see.
>
>
> If you use stackApply directly, the indices are used in the names.
> Layer 1 and 8 belong to the group with index 4. It is the first group
> in the list of indexes, so the first layer of the output is then
> referred to as index_4. Then comes index_5 with layers 2, 10 and 15 of
> your input. The order of these names will follow the order of the
> first appearance of your?indices. The indices gets lost with the use
> of clusterR, so it gives you the same output, but with names layer.1 -
> layer.7.
>
>
> You could change the names of the result from clusterR with:
>
> names(ResClusterR) = paste0("index_", unique(indices))
>
>
> If you want your result (from stackApply or clusterR) to follow the
> order of your indices, you should be able to get this with:
>
>
> sResClusterR = ResClusterR[[order(names(ResClusterR))]]
>
>
> Does this help you further?
>
> Jon
>
>
>
>
> --
> Jon Olav Sk?ien
> European Commission
> Joint Research Centre ? JRC.E.1
> Disaster Risk Management Unit
> Building 26b 1/144 | Via E.Fermi 2749, I-21027 Ispra (VA) Italy, TP 267
> jon.skoien at ec.europa.eu Tel: +39 0332 789205 Disclaimer: Views
> expressed in this email are those of the individual and do not
> necessarily represent official views of the European Commission.
>
>
>
> ------------------------------------------------------------------------
> *From:* R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
> Leonidas Liakos via R-sig-Geo <r-sig-geo at r-project.org>
> *Sent:* 21 November 2019 08:52
> *To:* Ben Tupper; r-sig-geo at r-project.org
> *Subject:* Re: [R-sig-Geo] raster: stackApply problems..
> ?
> Unfortunately the names are not always in ascending order. This is the
> result of my data.
>
> names????? : index_4, index_5, index_6, index_7, index_1, index_2, index_3
> min values :?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3
> max values :?? 307.0,?? 297.5,?? 311.0,?? 313.0,?? 468.0,?? 290.0,?? 302.0
>
> And worst of all, it is not a proper match with indices.
>
> If I run it with clusterR then the result is different:
>
> names????? : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7
> min values :?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3,?????? 3
> max values :?? 307.0,?? 297.5,?? 311.0,?? 313.0,?? 468.0,?? 290.0,??
> 302.0?
>
>
> The solution is to reorder the layers of the stack so that the
> stackApply indices are in ascending order e.g. 1,1,1,2,2,2,3,3,3 ...
>
> My indices of my data was like that:
>
> 4 5 6 7 1 2 3 4 5 6 7 1 2 3 5 6 7
>
> I've reported this behavior here
> https://urldefense.com/v3/__https://github.com/rspatial/raster/issues/82__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G-KjNZJC$
>
>
>
> On 11/20/19 3:05 PM, Ben Tupper wrote:
> > Hi,
> >
> > That is certainly is unexpected to have two different naming styles.
> > It's not really solution to take to the bank, but you could simply
> > compose your own names assuming that the layer orders are always
> > returned in ascending index order.
> > Would that work for you
> >
> > ### start
> > library(raster)
> >
> > # Compute layer names for stackApply output
> > #
> > # @param index numeric, 1-based layer indices used for stackApply
> function
> > # @param prefix character, prefix for names
> > # @return character layers names in index order
> > layer_names <- function(index = c(2,2,3,3,1,1), prefix = c("layer.",
> > "index_")[1]){
> >?? paste0(prefix, sort(unique(index)))
> > }
> >
> > indices <- c(2,2,3,3,1,1)
> >
> > r <- raster()
> > values(r) <- 1
> > # simple sequential stack from 1 to 6 in all cells
> > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> > s
> >
> > beginCluster(2)
> > res <- clusterR(s, stackApply, args = list(indices=indices, fun = mean))
> > raster::endCluster()
> > names(res) <- layer_names(indices, prefix = "foobar.")
> > res
> >
> > res2 <- stackApply(s, indices, mean)
> > names(res2) <- layer_names(indices, prefix = "foobar.")
> > res2
> > ### end
> >
> >
> > On Wed, Nov 20, 2019 at 1:36 AM Leonidas Liakos via R-sig-Geo
> > <r-sig-geo at r-project.org> wrote:
> >> This is not a reasonable solution. It is not efficient to run
> stackapply
> >> twice to get the right names. Each execution can take hours.
> >>
> >>
> >> ???? 20/11/2019 3:30 ?.?., ? Frederico Faleiro ??????:
> >>> Hi Leonidas,
> >>>
> >>> both results are in the same order, but the name is different.
> >>> You can rename the first as in the second:
> >>> names(res) <- names(res2)
> >>>
> >>> I provided an example to help you understand the logic.
> >>>
> >>> library(raster)
> >>> beginCluster(2)
> >>> r <- raster()
> >>> values(r) <- 1
> >>> # simple sequential stack from 1 to 6 in all cells
> >>> s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> >>> s
> >>> res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1),
> fun =
> >>> mean))
> >>> res
> >>> res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
> >>> res2
> >>> dif <- res - res2
> >>> # exatly the same order because the difference is zero for all layers
> >>> dif
> >>> # rename
> >>> names(res) <- names(res2)
> >>>
> >>> Best regards,
> >>>
> >>> Frederico Faleiro
> >>>
> >>> On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo <
> >>> r-sig-geo at r-project.org> wrote:
> >>>
> >>>> I run the example with clusterR:
> >>>>
> >>>> no_cores <- parallel::detectCores() -1
> >>>> raster::beginCluster(no_cores)
> >>>> ?????? res <- raster::clusterR(inp, raster::stackApply, args =
> >>>> list(indices=c(2,2,3,3,1,1),fun = mean))
> >>>> raster::endCluster()
> >>>>
> >>>> And the result is:
> >>>>
> >>>>> res
> >>>> class?????????? : RasterBrick
> >>>> dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
> >>>> resolution : 1, 1?? (x, y)
> >>>> extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
> >>>> crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
> +towgs84=0,0,0
> >>>> source???????? : memory
> >>>> names?????????? : layer.1, layer.2, layer.3
> >>>> min values :???????? 1.5,???????? 3.5,???????? 5.5
> >>>> max values :???????? 1.5,???????? 3.5,???????? 5.5??
> >>>>
> >>>>
> >>>> layer.1, layer.2, layer.3 (?)
> >>>>
> >>>> So what corrensponds to what?
> >>>>
> >>>>
> >>>> If I run:
> >>>>
> >>>> res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
> >>>>
> >>>> The result is:
> >>>>
> >>>>> res2
> >>>> class????? : RasterBrick
> >>>> dimensions : 180, 360, 64800, 3? (nrow, ncol, ncell, nlayers)
> >>>> resolution : 1, 1? (x, y)
> >>>> extent???? : -180, 180, -90, 90? (xmin, xmax, ymin, ymax)
> >>>> crs??????? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> >>>> source???? : memory
> >>>> names????? : index_2, index_3, index_1
> >>>> min values :???? 1.5,???? 3.5,???? 5.5
> >>>> max values :???? 1.5,???? 3.5,???? 5.5
> >>>>
> >>>> There is no consistency with the names of the output and obscure
> >>>> correspondence with the indices in the case of clusterR
> >>>>
> >>>>
> >>>>???????? [[alternative HTML version deleted]]
> >>>>
> >>>> _______________________________________________
> >>>> R-sig-Geo mailing list
> >>>> R-sig-Geo at r-project.org
> >>>>
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$
>
> >>>>
> >>
> >> --
> >> ?????? ????????, ?????????
> >>
> https://urldefense.com/v3/__https://www.geographer.gr__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835GzqxUtB7$
>
> >> PGP fingerprint: 5237 83F8 E46C D91A 9FBB C7E7 F943 C9B6 8231 0937
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org
> >>
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$
>
> >
> >
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://urldefense.com/v3/__https://stat.ethz.ch/mailman/listinfo/r-sig-geo__;!NW73rmyV52c!SiZfwLn8F-IC_xeeUNNjzf8STJX1LMbYaoJKqfWo5ImGWi_dEhB7ilEG9835G3J81kzN$
>

	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Tue Nov 26 00:29:38 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Mon, 25 Nov 2019 23:29:38 +0000 (UTC)
Subject: [R-sig-Geo] Applying weights using cosine of latitude to Raster
 Stack values
References: <1119526924.6489600.1574724578927.ref@mail.yahoo.com>
Message-ID: <1119526924.6489600.1574724578927@mail.yahoo.com>

Greetings,
I am currently trying to apply weights to the original precipitation values of my Raster Stack object, called "landCO2". These weights are to take into account the differences in area between the equator and poles. However, I am not sure how to approach applying these to the existing values of the Raster Stack. The idea would be to apply the weights to the original values of each of the 138 raster layers, and then eventually plot these values on a global map with wrld_simpl. Here is what was done so far:
? ? library(raster)
??? library(maps)
??? library(maptools)
??? library(rasterVis)
? 
?? 
??? data("wrld_simpl")
??? b <- wrld_simpl??? landCO2 <- mask(RCP1pctCO2Median, b)
??? CO2new <- rasterToPoints(landCO2)
??? weightCO2 <- cos(CO2new[,"y"]*(pi/180))
??? CO2new[,3:ncol(CO2new)] = apply(CO2new[,3:ncol(CO2new)], 2, function(x) x * weightCO2)
??? avgCO2 <- colSums(CO2new[,3:ncol(CO2new)])/sum(weightCO2)
However, this approach obtains an average across all grid cells per layer, effectively creating 138 averages, which is fine. That said, I would like to similarly apply the weights to the values of landCO2 instead, so that the values across the 8192 grid cells for each of the 138 raster layers are appropriately transformed using the approach above. Evidently, this would be done prior to rasterToPoints being applied. 
To do what I would like, I tried the following:? ?? ?? landCO2 <- mask(RCP1pctCO2Median,b)
? ?? weightCO2 <- cos(landCO2[,"y"]*(pi/180)) #Notice that I skipped the "rasterToPoints" stage and replaced CO2new with landCO2 to directly work with landCO2 for this
However, this results in the following error:? ?"Error in landCO2[, "y"] : object of type 'S4' is not subsettable"
Why would the above error emerge? Is this also the correct approach to apply the weights to landCO2?

landCO2 looks like this:
??? class?????? : RasterStack 
??? dimensions? : 64, 128, 8192, 138? (nrow, ncol, ncell, nlayers)
??? resolution? : 2.8125, 2.789327? (x, y)
??? extent????? : -181.4062, 178.5938, -89.25846, 89.25846? (xmin, xmax, ymin, ymax)
??? coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
??? names?????? :??? layer.1,??? layer.2,??? layer.3,??? layer.4,??? layer.5,??? layer.6,??? layer.7,?? 
??? layer.8,??? layer.9,?? layer.10,?? layer.11,?? layer.12,?? layer.13,?? layer.14,?? layer.15, ... 
??? min values? : 0.42964514, 0.43375653, 0.51749371, 0.50838983, 0.45366730, 0.53099146, 0.49757186,
??? 0.45697752, 0.41382199, 0.46082401, 0.45516687, 0.51857087, 0.41005131, 0.45956529, 0.47497867, ... 
??? max values? :?? 96.30350,? 104.08584,?? 88.92751,?? 97.49373,?? 89.57201,?? 90.58570,?? 86.67651,?? 
??? 88.33519,?? 96.94720,? 101.58247,?? 96.07792,?? 93.21948,?? 99.59785,?? 94.26218,?? 90.62138, ... 

Thanks, and any help would be greatly appreciated!
	[[alternative HTML version deleted]]


From tom@heng| @end|ng |rom gm@||@com  Tue Nov 26 10:25:57 2019
From: tom@heng| @end|ng |rom gm@||@com (Tomislav Hengl)
Date: Tue, 26 Nov 2019 10:25:57 +0100
Subject: [R-sig-Geo] =?utf-8?q?OpenGeoHub_Summer_School_2020_=28Wageninge?=
 =?utf-8?q?n_International_Conference_Centre=2C_Aug_16=E2=80=9323=2C_2020?=
 =?utf-8?q?=29?=
Message-ID: <5dfcc038-ce39-980b-68b4-45d2dac7c0cd@gmail.com>


This is the first announcement for the OpenGeoHub Summer School 2020 
that will be held at the Wageningen Int. Conference Centre (Hotel and 
Conference centre) in the period Aug 16?23, 2020. The registrations are 
now open. For more info see:

https://opengeohub.org/summer_school_2020

The special theme of the Summer School 2020 is:

"Developing Machine Learning Algorithms for spatial and spatiotemporal 
data science problems"

Lecturers/topics (sorted alphabetically):
- Christian Knoth: "Introduction to Deep Learning in R for the analysis 
of UAV-based remote sensing data"		
- Dainius Masiliunas: "Global-scale land cover mapping using FOSS4G"
- Dainius Masiliunas: "OpenEO demo (R client)"
- Dainius Masiliunas: "Detection of breaks in time series using the 
'bfast' package in R"
- Dylan Beaudette: "Algorithms for Quantitative Pedology: R packages for 
working with soils data at scale"
- Dylan Beaudette: "High performance vector data analysis delivery with 
PostGIS"
- Dylan Beaudette: "Credible metrics for determining similarity, 
accuracy, and precision in the context of soil type mapping"
- Edzer Pebesma: "Handling and analysing vector and raster data cubes 
with R"		
- Giuseppe Amatulli: "GDAL/OGR and PKTOOLS for massive raster/vector 
operations"		
- Hanna Meyer: "Machine learning in remote sensing applications"		
- John E. Lewis: "Spatial mixed models & semiparametric regression"
- John E. Lewis: "Working with and the modelling of temporal data"
- John E. Lewis: "Using R for machine learning modelling - a coding 
introduction"
- Julia Wagemann: "Analysis of Big Earth Data with Jupyter Notebooks"
- Julia Wagemann: "Dashboarding with Jupyter Notebooks and Voila"	
- Longzhu Shen: "Predictive modeling of nitrogen distributions in US 
streams in a machine learning framework"		
- Madlene Nussbaum: "Mastering machine learning for spatial prediction I 
- overview and introduction in methods"
- Madlene Nussbaum: "Mastering machine learning for spatial prediction 
II - model selection and interpretation, uncertainty"	
- Marius Appel: "Creating and Analyzing Multi-Variable Earth Observation 
Data Cubes in R"		
- Meng Lu: "Assessment of global air pollution exposure"		
- Paula Moraga: "Spatial modeling and interactive visualization with the 
R-INLA package"		
- Richard Barnes: "High-performance geocomputing for hydrological / 
terrain modeling"
- Richard Barnes: "Leveraging Python, clusters, and GPUs for geocomputation"
- Richard Barnes: "Reproducible scientific analysis"
- Tim Appelhans: "mapview package tutorial"		
- Tomislav Hengl: "Automated predictive mapping using Ensemble Machine 
Learning"
- Tomislav Hengl: "A step-by-step tutorial to optimization of 
geocomputing (tiling and parallelization) with R"

Important dates:
- 26th of November 2019 ? registrations open,
- 1st of February 2020 ? registrations close,
- 16th of February 2020 ? selection of candidates and invitation letters 
sent,
- 12th of April 2020 ? course fee payment deadline,
- 1st of May 2020 ? official programme published,
- 16 August to 22 August 2020 ? Summer School,

The Summer School is limited to 70 participants. In the case of higher 
number of applications, candidates will be selected based on a ranking 
system, which is based on: time of registration, solidarity, academic 
output and contributions to the open source projects. The final 
programme of the Summer School will shaped interactively.

Course fees:
The registrations fees for this Summer School full course fee will be in 
the range 400?500 EUR (exact number will be provided in the invitation 
letter). Participants from ODA countries (employed by an organization or 
company in ODA-listed country) and/or full-time students (not under work 
contract as University assistant or similar) have a right on reduced fee 
(usually 40% lower than the full registration fee).

Summer School is organized on a cost-recovery basis. OpenGeoHub 
foundation is a not-for-profit research foundation located in the 
Netherlands. All lecturers are volunteers. None of the lecturers 
receives any honorarium payment or is contracted by the local organizers.

Come to Wageningen the town of Life Sciences and improve your coding / 
geocomputing skills!

Follow us:
https://twitter.com/opengeohub
https://www.youtube.com/c/OpenGeoHubFoundation
https://business.facebook.com/Opengeohub-foundation-274552396602535/

-- 
T. (Tom) Hengl
Technical support / Vice Chair
The OpenGeoHub Foundation
Mail: OpenGeoHub Foundation, Agro Business Park 10, 6708PW Wageningen, NL
Tel: +31 (0)317 427537
Url: http://opengeohub.org/about
skype:tom.hengl?chat


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Tue Nov 26 17:18:56 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Tue, 26 Nov 2019 18:18:56 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
Message-ID: <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>

I added raster::zApply in my tests to validate the results. However, the
indices of the names of the results are different now. Recall that the
goal is to calculate from a raster stack time series the mean per day of
the week. And that problem I have is that stackApply, zApply and
calc/sapply return different indices in the result names. New code is
available here:
https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
I'm really curious about missing something.


On 11/20/19 3:30 AM, Frederico Faleiro wrote:
> Hi Leonidas,
>
> both results are in the same order, but the name is different.
> You can rename the first as in the second:
> names(res) <- names(res2)
>
> I provided an example to help you understand the logic.
>
> library(raster)
> beginCluster(2)
> r <- raster()
> values(r) <- 1
> # simple sequential stack from 1 to 6 in all cells
> s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> s
> res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun
> = mean))
> res
> res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
> res2
> dif <- res - res2
> # exatly the same order because the difference?is zero for all layers
> dif
> # rename
> names(res) <- names(res2)
>
> Best regards,
>
> Frederico Faleiro
>
> On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
> <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
>
>     I run the example with clusterR:
>
>     no_cores <- parallel::detectCores() -1
>     raster::beginCluster(no_cores)
>     ?????? res <- raster::clusterR(inp, raster::stackApply, args =
>     list(indices=c(2,2,3,3,1,1),fun = mean))
>     raster::endCluster()
>
>     And the result is:
>
>     > res
>     class?????????? : RasterBrick
>     dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>     resolution : 1, 1?? (x, y)
>     extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>     crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
>     +towgs84=0,0,0
>     source???????? : memory
>     names?????????? : layer.1, layer.2, layer.3
>     min values :???????? 1.5,???????? 3.5,???????? 5.5
>     max values :???????? 1.5,???????? 3.5,???????? 5.5??
>
>
>     layer.1, layer.2, layer.3 (?)
>
>     So what corrensponds to what?
>
>
>     If I run:
>
>     res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>
>     The result is:
>
>     > res2
>     class? ? ? : RasterBrick
>     dimensions : 180, 360, 64800, 3? (nrow, ncol, ncell, nlayers)
>     resolution : 1, 1? (x, y)
>     extent? ? ?: -180, 180, -90, 90? (xmin, xmax, ymin, ymax)
>     crs? ? ? ? : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>     source? ? ?: memory
>     names? ? ? : index_2, index_3, index_1
>     min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>     max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>
>     There is no consistency with the names of the output and obscure
>     correspondence with the indices in the case of clusterR
>
>
>     ? ? ? ? [[alternative HTML version deleted]]
>
>     _______________________________________________
>     R-sig-Geo mailing list
>     R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>     https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From v|j@y|u||@ @end|ng |rom gm@||@com  Tue Nov 26 18:03:09 2019
From: v|j@y|u||@ @end|ng |rom gm@||@com (Vijay Lulla)
Date: Tue, 26 Nov 2019 12:03:09 -0500
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
Message-ID: <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>

If you read the code/help for `stackApply` and `zApply` you'll see that the
results that you obtain make sense (at least they seem sensible/reasonable
to me).  IMO, if you want to control the ordering of your layers then just
use sapply, like how you've used for ver_mean.  IMO, this is the only
reliable (safe?), and quite a readable, way to accomplish what you're
trying to do.
Just my 2 cents.
-- Vijay.

On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via R-sig-Geo <
r-sig-geo at r-project.org> wrote:

> I added raster::zApply in my tests to validate the results. However, the
> indices of the names of the results are different now. Recall that the
> goal is to calculate from a raster stack time series the mean per day of
> the week. And that problem I have is that stackApply, zApply and
> calc/sapply return different indices in the result names. New code is
> available here:
> https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
> I'm really curious about missing something.
>
>
> On 11/20/19 3:30 AM, Frederico Faleiro wrote:
> > Hi Leonidas,
> >
> > both results are in the same order, but the name is different.
> > You can rename the first as in the second:
> > names(res) <- names(res2)
> >
> > I provided an example to help you understand the logic.
> >
> > library(raster)
> > beginCluster(2)
> > r <- raster()
> > values(r) <- 1
> > # simple sequential stack from 1 to 6 in all cells
> > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
> > s
> > res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun
> > = mean))
> > res
> > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
> > res2
> > dif <- res - res2
> > # exatly the same order because the difference is zero for all layers
> > dif
> > # rename
> > names(res) <- names(res2)
> >
> > Best regards,
> >
> > Frederico Faleiro
> >
> > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
> > <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
> >
> >     I run the example with clusterR:
> >
> >     no_cores <- parallel::detectCores() -1
> >     raster::beginCluster(no_cores)
> >     ?????? res <- raster::clusterR(inp, raster::stackApply, args =
> >     list(indices=c(2,2,3,3,1,1),fun = mean))
> >     raster::endCluster()
> >
> >     And the result is:
> >
> >     > res
> >     class?????????? : RasterBrick
> >     dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
> >     resolution : 1, 1?? (x, y)
> >     extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
> >     crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
> >     +towgs84=0,0,0
> >     source???????? : memory
> >     names?????????? : layer.1, layer.2, layer.3
> >     min values :???????? 1.5,???????? 3.5,???????? 5.5
> >     max values :???????? 1.5,???????? 3.5,???????? 5.5??
> >
> >
> >     layer.1, layer.2, layer.3 (?)
> >
> >     So what corrensponds to what?
> >
> >
> >     If I run:
> >
> >     res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
> >
> >     The result is:
> >
> >     > res2
> >     class      : RasterBrick
> >     dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
> >     resolution : 1, 1  (x, y)
> >     extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> >     crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> >     source     : memory
> >     names      : index_2, index_3, index_1
> >     min values :     1.5,     3.5,     5.5
> >     max values :     1.5,     3.5,     5.5
> >
> >     There is no consistency with the names of the output and obscure
> >     correspondence with the indices in the case of clusterR
> >
> >
> >             [[alternative HTML version deleted]]
> >
> >     _______________________________________________
> >     R-sig-Geo mailing list
> >     R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
> >     https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Tue Nov 26 18:22:19 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Tue, 26 Nov 2019 19:22:19 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
Message-ID: <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>

Why do they seem logical since they do not match?

Check for example index 1 (Sunday). The results are different for the
three processes

> stackapply_mean
class????? : RasterBrick
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
source???? : /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
names????? :? index_5,? index_6,? index_7,? index_1,? index_2,?
index_3,? index_4
min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
442.7436
max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
556.2471


> ver_mean
class????? : RasterStack
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
names????? :? layer.1,? layer.2,? layer.3,? layer.4,? layer.5,?
layer.6,? layer.7
min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
429.6900
max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
561.7972


> z
class????? : RasterBrick
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
source???? : /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
names????? :?????? X1,?????? X2,?????? X3,?????? X4,?????? X5,??????
X6,?????? X7
min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
442.7436
max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
556.2471
?????????? : 1, 2, 3, 4, 5, 6, 7


On 11/26/19 7:03 PM, Vijay Lulla wrote:
> If you read the code/help for `stackApply` and `zApply` you'll see
> that the results that you obtain make sense (at least they seem
> sensible/reasonable to me).? IMO, if you want to control the ordering
> of your layers then just use sapply, like how you've used for
> ver_mean.? IMO, this is the only reliable (safe?), and quite a
> readable, way to accomplish what you're trying to do.
> Just my 2 cents.
> -- Vijay.
>
> On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via R-sig-Geo
> <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
>
>     I added raster::zApply in my tests to validate the results.
>     However, the
>     indices of the names of the results are different now. Recall that the
>     goal is to calculate from a raster stack time series the mean per
>     day of
>     the week. And that problem I have is that stackApply, zApply and
>     calc/sapply return different indices in the result names. New code is
>     available here:
>     https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>     I'm really curious about missing something.
>
>
>     On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>     > Hi Leonidas,
>     >
>     > both results are in the same order, but the name is different.
>     > You can rename the first as in the second:
>     > names(res) <- names(res2)
>     >
>     > I provided an example to help you understand the logic.
>     >
>     > library(raster)
>     > beginCluster(2)
>     > r <- raster()
>     > values(r) <- 1
>     > # simple sequential stack from 1 to 6 in all cells
>     > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>     > s
>     > res <- clusterR(s, stackApply, args =
>     list(indices=c(2,2,3,3,1,1), fun
>     > = mean))
>     > res
>     > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>     > res2
>     > dif <- res - res2
>     > # exatly the same order because the difference?is zero for all
>     layers
>     > dif
>     > # rename
>     > names(res) <- names(res2)
>     >
>     > Best regards,
>     >
>     > Frederico Faleiro
>     >
>     > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
>     > <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>
>     <mailto:r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>>>
>     wrote:
>     >
>     >? ? ?I run the example with clusterR:
>     >
>     >? ? ?no_cores <- parallel::detectCores() -1
>     >? ? ?raster::beginCluster(no_cores)
>     >? ? ??????? res <- raster::clusterR(inp, raster::stackApply, args =
>     >? ? ?list(indices=c(2,2,3,3,1,1),fun = mean))
>     >? ? ?raster::endCluster()
>     >
>     >? ? ?And the result is:
>     >
>     >? ? ?> res
>     >? ? ?class?????????? : RasterBrick
>     >? ? ?dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>     >? ? ?resolution : 1, 1?? (x, y)
>     >? ? ?extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>     >? ? ?crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
>     >? ? ?+towgs84=0,0,0
>     >? ? ?source???????? : memory
>     >? ? ?names?????????? : layer.1, layer.2, layer.3
>     >? ? ?min values :???????? 1.5,???????? 3.5,???????? 5.5
>     >? ? ?max values :???????? 1.5,???????? 3.5,???????? 5.5??
>     >
>     >
>     >? ? ?layer.1, layer.2, layer.3 (?)
>     >
>     >? ? ?So what corrensponds to what?
>     >
>     >
>     >? ? ?If I run:
>     >
>     >? ? ?res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>     >
>     >? ? ?The result is:
>     >
>     >? ? ?> res2
>     >? ? ?class? ? ? : RasterBrick
>     >? ? ?dimensions : 180, 360, 64800, 3? (nrow, ncol, ncell, nlayers)
>     >? ? ?resolution : 1, 1? (x, y)
>     >? ? ?extent? ? ?: -180, 180, -90, 90? (xmin, xmax, ymin, ymax)
>     >? ? ?crs? ? ? ? : +proj=longlat +datum=WGS84 +ellps=WGS84
>     +towgs84=0,0,0
>     >? ? ?source? ? ?: memory
>     >? ? ?names? ? ? : index_2, index_3, index_1
>     >? ? ?min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>     >? ? ?max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>     >
>     >? ? ?There is no consistency with the names of the output and obscure
>     >? ? ?correspondence with the indices in the case of clusterR
>     >
>     >
>     >? ? ?? ? ? ? [[alternative HTML version deleted]]
>     >
>     >? ? ?_______________________________________________
>     >? ? ?R-sig-Geo mailing list
>     >? ? ?R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>     <mailto:R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>>
>     >? ? ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>     >
>
>     ? ? ? ? [[alternative HTML version deleted]]
>
>     _______________________________________________
>     R-sig-Geo mailing list
>     R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>     https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>

	[[alternative HTML version deleted]]


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Tue Nov 26 18:22:19 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Tue, 26 Nov 2019 19:22:19 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
Message-ID: <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>

Why do they seem logical since they do not match?

Check for example index 1 (Sunday). The results are different for the
three processes

> stackapply_mean
class????? : RasterBrick
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
source???? : /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
names????? :? index_5,? index_6,? index_7,? index_1,? index_2,?
index_3,? index_4
min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
442.7436
max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
556.2471


> ver_mean
class????? : RasterStack
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
names????? :? layer.1,? layer.2,? layer.3,? layer.4,? layer.5,?
layer.6,? layer.7
min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
429.6900
max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
561.7972


> z
class????? : RasterBrick
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
source???? : /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
names????? :?????? X1,?????? X2,?????? X3,?????? X4,?????? X5,??????
X6,?????? X7
min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
442.7436
max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
556.2471
?????????? : 1, 2, 3, 4, 5, 6, 7


On 11/26/19 7:03 PM, Vijay Lulla wrote:
> If you read the code/help for `stackApply` and `zApply` you'll see
> that the results that you obtain make sense (at least they seem
> sensible/reasonable to me).? IMO, if you want to control the ordering
> of your layers then just use sapply, like how you've used for
> ver_mean.? IMO, this is the only reliable (safe?), and quite a
> readable, way to accomplish what you're trying to do.
> Just my 2 cents.
> -- Vijay.
>
> On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via R-sig-Geo
> <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
>
>     I added raster::zApply in my tests to validate the results.
>     However, the
>     indices of the names of the results are different now. Recall that the
>     goal is to calculate from a raster stack time series the mean per
>     day of
>     the week. And that problem I have is that stackApply, zApply and
>     calc/sapply return different indices in the result names. New code is
>     available here:
>     https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>     I'm really curious about missing something.
>
>
>     On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>     > Hi Leonidas,
>     >
>     > both results are in the same order, but the name is different.
>     > You can rename the first as in the second:
>     > names(res) <- names(res2)
>     >
>     > I provided an example to help you understand the logic.
>     >
>     > library(raster)
>     > beginCluster(2)
>     > r <- raster()
>     > values(r) <- 1
>     > # simple sequential stack from 1 to 6 in all cells
>     > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>     > s
>     > res <- clusterR(s, stackApply, args =
>     list(indices=c(2,2,3,3,1,1), fun
>     > = mean))
>     > res
>     > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>     > res2
>     > dif <- res - res2
>     > # exatly the same order because the difference?is zero for all
>     layers
>     > dif
>     > # rename
>     > names(res) <- names(res2)
>     >
>     > Best regards,
>     >
>     > Frederico Faleiro
>     >
>     > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
>     > <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>
>     <mailto:r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>>>
>     wrote:
>     >
>     >? ? ?I run the example with clusterR:
>     >
>     >? ? ?no_cores <- parallel::detectCores() -1
>     >? ? ?raster::beginCluster(no_cores)
>     >? ? ??????? res <- raster::clusterR(inp, raster::stackApply, args =
>     >? ? ?list(indices=c(2,2,3,3,1,1),fun = mean))
>     >? ? ?raster::endCluster()
>     >
>     >? ? ?And the result is:
>     >
>     >? ? ?> res
>     >? ? ?class?????????? : RasterBrick
>     >? ? ?dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>     >? ? ?resolution : 1, 1?? (x, y)
>     >? ? ?extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>     >? ? ?crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
>     >? ? ?+towgs84=0,0,0
>     >? ? ?source???????? : memory
>     >? ? ?names?????????? : layer.1, layer.2, layer.3
>     >? ? ?min values :???????? 1.5,???????? 3.5,???????? 5.5
>     >? ? ?max values :???????? 1.5,???????? 3.5,???????? 5.5??
>     >
>     >
>     >? ? ?layer.1, layer.2, layer.3 (?)
>     >
>     >? ? ?So what corrensponds to what?
>     >
>     >
>     >? ? ?If I run:
>     >
>     >? ? ?res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>     >
>     >? ? ?The result is:
>     >
>     >? ? ?> res2
>     >? ? ?class? ? ? : RasterBrick
>     >? ? ?dimensions : 180, 360, 64800, 3? (nrow, ncol, ncell, nlayers)
>     >? ? ?resolution : 1, 1? (x, y)
>     >? ? ?extent? ? ?: -180, 180, -90, 90? (xmin, xmax, ymin, ymax)
>     >? ? ?crs? ? ? ? : +proj=longlat +datum=WGS84 +ellps=WGS84
>     +towgs84=0,0,0
>     >? ? ?source? ? ?: memory
>     >? ? ?names? ? ? : index_2, index_3, index_1
>     >? ? ?min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>     >? ? ?max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>     >
>     >? ? ?There is no consistency with the names of the output and obscure
>     >? ? ?correspondence with the indices in the case of clusterR
>     >
>     >
>     >? ? ?? ? ? ? [[alternative HTML version deleted]]
>     >
>     >? ? ?_______________________________________________
>     >? ? ?R-sig-Geo mailing list
>     >? ? ?R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>     <mailto:R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>>
>     >? ? ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>     >
>
>     ? ? ? ? [[alternative HTML version deleted]]
>
>     _______________________________________________
>     R-sig-Geo mailing list
>     R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>     https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>

	[[alternative HTML version deleted]]


From v|j@y|u||@ @end|ng |rom gm@||@com  Tue Nov 26 20:00:24 2019
From: v|j@y|u||@ @end|ng |rom gm@||@com (Vijay Lulla)
Date: Tue, 26 Nov 2019 14:00:24 -0500
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
Message-ID: <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>

I'm sorry for the miscommunication.  What I meant to say is that the output
from stackApply and zApply are the same (because zApply uses stackApply
internally) except the names.  The behavior of stackApply makes sense
because AFAIUI R doesn't automatically reorder vectors/indices that it
receives.  Your observation about inconsistent result with ver_mean is very
valid though!  And, that's what I meant with my comment that using sapply
with the explicit ordering that you want is the best way to control what
raster package will output.  In R the input order should be maintained
(this is the prime difference between SQL and R) but packages/tools do not
always adhere to this...so it's never clear how the output will be
ordered.  Sorry for the confusion.


On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos <leonidas_liakos at yahoo.gr>
wrote:

> Why do they seem logical since they do not match?
>
> Check for example index 1 (Sunday). The results are different for the
> three processes
>
> > stackapply_mean
> class      : RasterBrick
> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
> resolution : 500, 500  (x, y)
> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
> crs        : NA
> source     : /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
> names      :  index_5,  index_6,  index_7,  index_1,  index_2,  index_3,
> index_4
> min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
> 442.7436
> max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
> 556.2471
>
>
> > ver_mean
> class      : RasterStack
> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
> resolution : 500, 500  (x, y)
> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
> crs        : NA
> names      :  layer.1,  layer.2,  layer.3,  layer.4,  layer.5,  layer.6,
> layer.7
> min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
> 429.6900
> max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
> 561.7972
>
>
> > z
> class      : RasterBrick
> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
> resolution : 500, 500  (x, y)
> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
> crs        : NA
> source     : /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
> names      :       X1,       X2,       X3,       X4,       X5,
> X6,       X7
> min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
> 442.7436
> max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
> 556.2471
>            : 1, 2, 3, 4, 5, 6, 7
>
>
> On 11/26/19 7:03 PM, Vijay Lulla wrote:
>
> If you read the code/help for `stackApply` and `zApply` you'll see that
> the results that you obtain make sense (at least they seem
> sensible/reasonable to me).  IMO, if you want to control the ordering of
> your layers then just use sapply, like how you've used for ver_mean.  IMO,
> this is the only reliable (safe?), and quite a readable, way to accomplish
> what you're trying to do.
> Just my 2 cents.
> -- Vijay.
>
> On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via R-sig-Geo <
> r-sig-geo at r-project.org> wrote:
>
>> I added raster::zApply in my tests to validate the results. However, the
>> indices of the names of the results are different now. Recall that the
>> goal is to calculate from a raster stack time series the mean per day of
>> the week. And that problem I have is that stackApply, zApply and
>> calc/sapply return different indices in the result names. New code is
>> available here:
>> https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>> I'm really curious about missing something.
>>
>>
>> On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>> > Hi Leonidas,
>> >
>> > both results are in the same order, but the name is different.
>> > You can rename the first as in the second:
>> > names(res) <- names(res2)
>> >
>> > I provided an example to help you understand the logic.
>> >
>> > library(raster)
>> > beginCluster(2)
>> > r <- raster()
>> > values(r) <- 1
>> > # simple sequential stack from 1 to 6 in all cells
>> > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>> > s
>> > res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun
>> > = mean))
>> > res
>> > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>> > res2
>> > dif <- res - res2
>> > # exatly the same order because the difference is zero for all layers
>> > dif
>> > # rename
>> > names(res) <- names(res2)
>> >
>> > Best regards,
>> >
>> > Frederico Faleiro
>> >
>> > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
>> > <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
>> >
>> >     I run the example with clusterR:
>> >
>> >     no_cores <- parallel::detectCores() -1
>> >     raster::beginCluster(no_cores)
>> >     ?????? res <- raster::clusterR(inp, raster::stackApply, args =
>> >     list(indices=c(2,2,3,3,1,1),fun = mean))
>> >     raster::endCluster()
>> >
>> >     And the result is:
>> >
>> >     > res
>> >     class?????????? : RasterBrick
>> >     dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>> >     resolution : 1, 1?? (x, y)
>> >     extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>> >     crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
>> >     +towgs84=0,0,0
>> >     source???????? : memory
>> >     names?????????? : layer.1, layer.2, layer.3
>> >     min values :???????? 1.5,???????? 3.5,???????? 5.5
>> >     max values :???????? 1.5,???????? 3.5,???????? 5.5??
>> >
>> >
>> >     layer.1, layer.2, layer.3 (?)
>> >
>> >     So what corrensponds to what?
>> >
>> >
>> >     If I run:
>> >
>> >     res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>> >
>> >     The result is:
>> >
>> >     > res2
>> >     class      : RasterBrick
>> >     dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
>> >     resolution : 1, 1  (x, y)
>> >     extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>> >     crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>> >     source     : memory
>> >     names      : index_2, index_3, index_1
>> >     min values :     1.5,     3.5,     5.5
>> >     max values :     1.5,     3.5,     5.5
>> >
>> >     There is no consistency with the names of the output and obscure
>> >     correspondence with the indices in the case of clusterR
>> >
>> >
>> >             [[alternative HTML version deleted]]
>> >
>> >     _______________________________________________
>> >     R-sig-Geo mailing list
>> >     R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>> >     https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Nov 26 20:48:14 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 26 Nov 2019 20:48:14 +0100
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
 <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1911262040120.583221@reclus.nhh.no>

Sorry for late reply, am indisposed and unable to help further. I feel 
that there is so much noise in your data (differences in offers, rental 
lengths, repeats or not, etc.), that you will certainly have to subset 
vigorously first to isolate response cases that are comparable. What you 
are trying to disentangle are the hedonic components in the bundle where 
you just have price as response, but lots of other bundle characteristics 
on the right hand side (days, etc.). I feel you'd need to try to get to a 
response index of price per day per rental area or some such. I'd 
certainly advise examining responses to a specific driver (major concert 
or sports event) to get a feel for how the market responds, and return to 
spatial hedonic after finding an approach that gives reasonable aspatial 
outcomes.

Roger

On Sun, 17 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your message and sorry for my late answer.
>
> Regarding the number of listings (lettings) for my data set (2.216.642 observations), each listing contains an individual id:
>
> unique ids: 180.004
> time periods: 54 (2015-01 to 2019-09)
> number of ids that appear only once: 28.486 (of 180.004 ids) (15,8%)
> number of ids that appear/repeat 2-10 times: 82.641 (of 180.004 ids) (45,9%)
> number of ids that appear/repeat 11-30 times: 46.465 (of 180.004 ids) (25,8%)
> number of ids that appear/repeat 31-54 times: 22.412 (of 180.004 ids) (12,5%)
>
> Important to notice is that hosts can change the room_category (between entire/home apt, private room and shared room) keeping the same listing id number. In my data, the number of unique ids that in some point changed the room_type is of 7.204 ids.
>
> --
>
> For the OLS model, I was using only a fixed effect model, where each time period (date_compiled) (54 in total) is a time dummy.
>
> plm::plm(formula = model, data = listings, model = "pooling", index = c("id", "date_compiled"))
>
>
> --
> Osland et al. (2016) (https://doi.org/10.1111/jors.12281) use a spatial fixed effects (SFE) hedonic model, where each defined neighborhood zone in the study area is represented by dummy variables.
>
> Dong et al. (2015) (https://doi.org/10.1111/gean.12049) outline four model specifications to accommodate geographically hierarchical data structures: (1) groupwise W and fixed regional effects; (2) groupwise W and random regional effects; (3) proximity-based W and fixed regional effects; and (4) proximity-based W and random regional effects.
> --
>
> I created a new column/variable containing the borough where the zipcode is found (Manhattan, Brooklyn, Queens, Bronx, Staten Island).
>
> If I understood it right, the (two-level) Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) considers the occurrence of spatial relations at the (lower) individual (geographical coordinates - in my case, the listing location) and (higher) group level (territorial units - in my case, zipcodes).
>
> According to Bivand et al. (2017): "(...) W is a spatial weights matrix. The HSAR model may also be estimated without this component.". So, in this case I only estimate the Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) in a "one-level" basis, i.e., at the higher-level.
>
> HSAR::hsar(model, data = listings, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>
> (Where the "model" formula contains the 54 time dummy variables)
>
> Do you think I can proceed with this model? I was able to calculate it.
>
> If I remove all observations/rows with NAs in one of the chosen variables/observations, 884.183 observations remain. If I would create a W matrix for HSAR::hsar, I would have a gigantic 884.183 by 884.183 matrix. This is the reason why I put W = NULL.
>
>
> Thank you and best regards
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Monday, November 11, 2019 11:31
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Sun, 10 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Again, thank you for your answer. I read the material provided and
>> decided that Hierarchical Spatial Autoregressive (HSAR) could be the
>> right model for me.
>>
>> I indeed have the precise latitude and longitude information for all my
>> listings for NYC.
>>
>> I created a stratified sample (group = zipcode) with 22172 (1%) of my
>> observations called listings_sample and tried to replicate the hsar
>> model, please see below.
>>
>> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.
>
> Unless you know definitely that you want to relate the response to its
> lagged value, you do not need this. Do note that the matrix is very
> sparse, so could be fitted without difficulty with ML in a cross-sectional
> model.
>
>>
>> You recommended then to introduce a Markov random field (MRF) random
>> effect (RE) at the zipcode level, but I did not understand it so well.
>> Could you develop a litte more?
>>
>
> Did you read the development in
> https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and
> includes code for fitting the Beijing housing parcels data se from HSAR
> with many other packages (MCMC, INLA, hglm, etc.). I guess that you should
> try to create a model that works on a single borough, sing the zipcodes
> in that borough as a proxy for unobserved neighbourhood effects. Try for
> example using lme4::lmer() with only a zipcode IID random effect, see if
> the hedonic estimates are similar to lm(), and leave adding an MRF RE
> (with for example mgcv::gam() or hglm::hglm()) until you have a working
> testbed. Then advance step-by-step from there.
>
> You still have not said how many repeat lettings you see - it will affect
> the way you specify your model.
>
> Roger
>
>> ##############
>> library(spdep)
>> library(HSAR)
>> library(dplyr)
>> library(splitstackshape)
>>
>>
>> # Stratified sample per zipcode (size = 1%) listings_sample <-
>> splitstackshape::stratified(indt = listings, group = "zipcode", size =
>> 0.01)
>>
>> # Removing zipcodes from polygon_nyc which are not observable in
>> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode
>> %in% c(unique(as.character(listings_sample$zipcode))))
>>
>>
>> ## Random effect matrix (N by J)
>>
>> # N: 22172
>> # J: 154
>>
>> # Arrange listings_sample by zipcode (ascending)
>> listings_sample <- listings_sample %>% arrange(zipcode)
>>
>> # Count number of listings per zipcode
>> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
>> # sum(MM$count)
>>
>> # N by J nulled matrix creation
>> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>>
>> # The total number of neighbourhood
>> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>>
>> for(i in 1:dim(MM)[1]) {
>>  Delta[Uid==i,i] <- 1
>> }
>> rm(i)
>>
>> Delta <- as(Delta,"dgCMatrix")
>>
>>
>> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>>
>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>>
>> # Include neighbour itself as a neighbour
>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>
>> # Spatial weights matrix for nb
>> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
>> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>>
>>
>> ## Fit HSAR SAR upper level random effect
>> model <- as.formula(log_price ~ guests_included + minimum_nights)
>>
>> betas = coef(lm(formula = model, data = listings_sample))
>> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>>
>> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>
>> ##############
>>
>> Thank you and best regards
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Friday, November 8, 2019 13:29
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Fri, 8 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Thank you for your answer.
>>>
>>> I successfully used the function nb2blocknb() for a smaller dataset.
>>>
>>> But for a dataset of over 2 million observations, I get the following
>>> error: "Error: cannot allocate vector of size 840 Kb".
>>
>> I don't think the observations are helpful. If you have repeat lets in the
>> same property in a given month, you need to handle that anyway. I'd go for
>> making the modelling exercise work (we agree that this is not panel data,
>> right?) on a small subset first. I would further argue that you need a
>> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
>> RE. You could very well take (stratified) samples per zipcode to represent
>> your data. Once that works, introduce an MRF RE at the zipcode level,
>> where you do know relative position. Using SARAR is going to be a waste of
>> time unless you can geocode the letting addresses. A multi-level approach
>> will work. Having big data in your case with no useful location
>> information per observation is just adding noise and over-smoothing, I'm
>> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
>> will work, also when you sample the within zipcode lets, given a split
>> into training and test sets, and making CV possible.
>>
>> Roger
>>
>>>
>>> I am expecting that at least 500.000 observations will be dropped due
>>> the lack of values for the chosen variables for the regression model, so
>>> probably I will filter and remove the observations/rows that will not be
>>> used anyway - do you know if there is any package that does this
>>> automatically, given the variables/columns chosed by me?
>>>
>>> Or would you recommend me another approach to avoid the above mentioned
>>> error?
>>>
>>> Thank you and best regards,
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Thursday, November 7, 2019 10:13
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Thu, 7 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Many thanks for your help.
>>>>
>>>> I have an additional question:
>>>>
>>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>>> rownumbers) from my data set? For now, I am taking my data set and
>>>> merging with the sf object polygon_nyc with the function
>>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>>> a huge n x n matrix (depending of the size of my data set).
>>>>
>>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>>> object has only n = 177.
>>>>
>>>> Of course running
>>>>
>>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>>
>>>> does not work ("Input data and weights have different dimensions").
>>>>
>>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>>> zipcode) and then create the weights list lw? Or there another option?
>>>
>>> I think we are getting more clarity. You do not know the location of the
>>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>>> areas, and can create a neighbour object from these boundaries. You then
>>> want to treat all the lettings in a zipcode area i as neighbours, and
>>> additionally lettings in zipcode areas neighbouring i as neighbours of
>>> lettings in i. This is the data structure that motivated the
>>> spdep::nb2blocknb() function:
>>>
>>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>>
>>> Try running the examples to get a feel for what is going on.
>>>
>>> I feel that most of the variability will vanish in the very large numbers
>>> of neighbours, over-smoothing the outcomes. If you do not have locations
>>> for the lettings themselves, I don't think you can make much progress.
>>>
>>> You could try a linear mixed model (or gam with a spatially structured
>>> random effect) with a temporal and a spatial random effect. See the HSAR
>>> package, articles by Dong et al., and maybe
>>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>>> effects at the zipcode level might be a way forward, together with an IID
>>> random effect at the same level (equivalent to sef-neighbours).
>>>
>>> Hope this helps,
>>>
>>> Roger
>>>
>>>>
>>>> Best regards,
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Wednesday, November 6, 2019 15:07
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>
>>>>> Dear Roger,
>>>>>
>>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>>> plain text.
>>>>>
>>>>> I will give a better context for my desired outcome.
>>>>>
>>>>> I am taking Airbnb's listings information for New York City available
>>>>> on: http://insideairbnb.com/get-the-data.html
>>>>>
>>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>>> Listings/ folder. When importing all these 54 files into one single data
>>>>> set, I create a new "date_compiled" variable/column.
>>>>>
>>>>> In total, after the data cleansing process, I have a little more 2
>>>>> million observations.
>>>>
>>>> You have repeat lettings for some, but not all properties. So this is at
>>>> best a very unbalanced panel. For those properties with repeats, you may
>>>> see temporal movement (trend/seasonal).
>>>>
>>>> I suggest (strongly) taking a single borough or even zipcode with some
>>>> hindreds of properties, and working from there. Do not include the
>>>> observation as its own neighbour, perhaps identify repeats and handle them
>>>> specially (create or use a property ID). Unbalanced panels may also create
>>>> a selection bias issue (why are some properties only listed sometimes?).
>>>>
>>>> So this although promising isn't simple, and getting to a hedonic model
>>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>>> necessarily trust OLS output either, partly because of the repeat property
>>>> issue.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> I created 54 timedummy variables for each time period available.
>>>>>
>>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>>> a variety of characteristics which potentially determine the daily rate
>>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>>> of the listing as number of bedrooms, if the host if professional,
>>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>>> from the listing, income per capita, etc.).
>>>>>
>>>>> My dependent variable is price (log price, common in the related
>>>>> literature for hedonic prices).
>>>>>
>>>>> The OLS model is done.
>>>>>
>>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>>> pricing of their listings, take not only into account its structural and
>>>>> location characteristics, but also the prices charged by near listings
>>>>> with similar characteristics - spatial autocorrelation is then present,
>>>>> at least spatial dependence is present in the dependent variable.
>>>>>
>>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>>> itself as a neighbor.
>>>>>
>>>>> Parts of my code can be found below:
>>>>>
>>>>> ########
>>>>>
>>>>> ## packages
>>>>>
>>>>> packages_install <- function(packages){
>>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>>> if (length(new.packages))
>>>>> install.packages(new.packages, dependencies = TRUE)
>>>>> sapply(packages, require, character.only = TRUE)
>>>>> }
>>>>>
>>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>>> packages_install(packages_required)
>>>>>
>>>>> # Working directory
>>>>> setwd("C:/Users/User/R")
>>>>>
>>>>>
>>>>>
>>>>> ## shapefile_us
>>>>>
>>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>>
>>>>> # Columns removal
>>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>>
>>>>> # Column rename: ZCTA5CE10
>>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>>
>>>>> # Column class change: zipcode
>>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>>
>>>>>
>>>>>
>>>>> ## polygon_nyc
>>>>>
>>>>> # Zip code not available in shapefile: 11695
>>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>>
>>>>>
>>>>>
>>>>> ## weight_matrix
>>>>>
>>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>>
>>>>> # Include neighbour itself as a neighbour
>>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>>
>>>>> # Weights to each neighboring polygon
>>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>>
>>>>>
>>>>>
>>>>> ## listings
>>>>>
>>>>> # Data import
>>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>>> listings <- listings %>% bind_rows
>>>>>
>>>>> # Characters removal
>>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>>> listings$price <- gsub("\\$", "", listings$price)
>>>>> listings$price <- gsub(",", "", listings$price)
>>>>>
>>>>>
>>>>>
>>>>> ## timedummy
>>>>>
>>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>>> timedummy <- gsub("-", "_", timedummy)
>>>>>
>>>>>
>>>>>
>>>>> ## OLS regression
>>>>>
>>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>>
>>>>> ########
>>>>>
>>>>> Some of my id's repeat in multiple time periods.
>>>>>
>>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>>
>>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>>
>>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>>
>>>>> Again, thank you very much for the help provided until now.
>>>>>
>>>>> Best regards,
>>>>> Robert
>>>>>
>>>>> ________________________________________
>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>> Sent: Tuesday, November 5, 2019 15:30
>>>>> To: Robert R
>>>>> Cc: r-sig-geo at r-project.org
>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>
>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>
>>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>>> for now that spatial dependence is present in both the dependent
>>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>>> think it would be possible to calculate this.? What I would like to know
>>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>>
>>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>>> of the covariates time-varying? Please provide motivation and use area
>>>>> (preferably with affiliation (your email and user name are not
>>>>> informative) - this feels like a real estate problem, probably wrongly
>>>>> specified. You should use splm if time make sense in your case, but if it
>>>>> really doesn't, simplify your approach, as much of the data will be
>>>>> subject to very large temporal autocorrelation.
>>>>>
>>>>> If this is a continuation of your previous question about using
>>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>>
>>>>> Roger
>>>>>
>>>>>>
>>>>>>       [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-Geo mailing list
>>>>>> R-sig-Geo at r-project.org
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>
>>>>> --
>>>>> Roger Bivand
>>>>> Department of Economics, Norwegian School of Economics,
>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>> https://orcid.org/0000-0003-2392-6140
>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Tue Nov 26 20:53:49 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Tue, 26 Nov 2019 21:53:49 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
 <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
Message-ID: <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>

Thank you!
The problem is not with the resulting values ??but with the index
mapping. Values ??are correct in all three cases.

As I wrote in a previous post in the thread
(https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027821.html) ,
stackApply behaves inconsistently depending on whether the exported
stack will remain in memory or it will be stored, due to its large size,
on the hard disk.

In the first case the indices are identical to my function (ver_mean)
and the lubridate::wday indexing system (and are correct) while in the
second they are shuffled.

So, while Sunday has index 1 and while in the first case (when the
result is in memory) stackApply returns the correct index, in the second
case (when the result is stored on the hard disk) it returns index_4! So
how can one be sure if index_1 corresponds to Sunday or another day
using stackApply since it sometimes enumerates it with index_1 and
sometimes index_4?


Try to run this example (when the resulting stack remains in memory) to
see that the indexes are identical (stackApply = ver_median =
lubridate::wday)
https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206

Thank you again

On 11/26/19 9:00 PM, Vijay Lulla wrote:
> I'm sorry for the miscommunication.? What I meant to say is that the
> output from stackApply and zApply are the same (because zApply uses
> stackApply internally) except the names.? The behavior of stackApply
> makes sense because AFAIUI R doesn't automatically reorder
> vectors/indices that it receives.? Your observation about inconsistent
> result with ver_mean is very valid though!? And, that's what I meant
> with my comment that using sapply with the explicit ordering that you
> want is the best way to control what raster package will output.? In R
> the input order should be maintained (this is the prime difference
> between SQL and R) but packages/tools do not always adhere to
> this...so it's never clear how the output will be ordered.? Sorry for
> the confusion.
>
>
> On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos
> <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>
>     Why do they seem logical since they do not match?
>
>     Check for example index 1 (Sunday). The results are different for
>     the three processes
>
>     > stackapply_mean
>     class????? : RasterBrick
>     dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>     resolution : 500, 500? (x, y)
>     extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>     crs??????? : NA
>     source???? :
>     /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
>     names????? :? index_5,? index_6,? index_7,? index_1,? index_2,?
>     index_3,? index_4
>     min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
>     429.6900, 442.7436
>     max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
>     561.7972, 556.2471
>
>
>     > ver_mean
>     class????? : RasterStack
>     dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>     resolution : 500, 500? (x, y)
>     extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>     crs??????? : NA
>     names????? :? layer.1,? layer.2,? layer.3,? layer.4,? layer.5,?
>     layer.6,? layer.7
>     min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946,
>     440.2028, 429.6900
>     max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671,
>     560.1375, 561.7972
>
>
>     > z
>     class????? : RasterBrick
>     dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>     resolution : 500, 500? (x, y)
>     extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>     crs??????? : NA
>     source???? :
>     /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
>     names????? :?????? X1,?????? X2,?????? X3,?????? X4,??????
>     X5,?????? X6,?????? X7
>     min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
>     429.6900, 442.7436
>     max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
>     561.7972, 556.2471
>     ?????????? : 1, 2, 3, 4, 5, 6, 7
>
>
>     On 11/26/19 7:03 PM, Vijay Lulla wrote:
>>     If you read the code/help for `stackApply` and `zApply` you'll
>>     see that the results that you obtain make sense (at least they
>>     seem sensible/reasonable to me).? IMO, if you want to control the
>>     ordering of your layers then just use sapply, like how you've
>>     used for ver_mean.? IMO, this is the only reliable (safe?), and
>>     quite a readable, way to accomplish what you're trying to do.
>>     Just my 2 cents.
>>     -- Vijay.
>>
>>     On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via R-sig-Geo
>>     <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
>>
>>         I added raster::zApply in my tests to validate the results.
>>         However, the
>>         indices of the names of the results are different now. Recall
>>         that the
>>         goal is to calculate from a raster stack time series the mean
>>         per day of
>>         the week. And that problem I have is that stackApply, zApply and
>>         calc/sapply return different indices in the result names. New
>>         code is
>>         available here:
>>         https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>>         I'm really curious about missing something.
>>
>>
>>         On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>>         > Hi Leonidas,
>>         >
>>         > both results are in the same order, but the name is different.
>>         > You can rename the first as in the second:
>>         > names(res) <- names(res2)
>>         >
>>         > I provided an example to help you understand the logic.
>>         >
>>         > library(raster)
>>         > beginCluster(2)
>>         > r <- raster()
>>         > values(r) <- 1
>>         > # simple sequential stack from 1 to 6 in all cells
>>         > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>         > s
>>         > res <- clusterR(s, stackApply, args =
>>         list(indices=c(2,2,3,3,1,1), fun
>>         > = mean))
>>         > res
>>         > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>         > res2
>>         > dif <- res - res2
>>         > # exatly the same order because the difference?is zero for
>>         all layers
>>         > dif
>>         > # rename
>>         > names(res) <- names(res2)
>>         >
>>         > Best regards,
>>         >
>>         > Frederico Faleiro
>>         >
>>         > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
>>         > <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>
>>         <mailto:r-sig-geo at r-project.org
>>         <mailto:r-sig-geo at r-project.org>>> wrote:
>>         >
>>         >? ? ?I run the example with clusterR:
>>         >
>>         >? ? ?no_cores <- parallel::detectCores() -1
>>         >? ? ?raster::beginCluster(no_cores)
>>         >? ? ??????? res <- raster::clusterR(inp, raster::stackApply,
>>         args =
>>         >? ? ?list(indices=c(2,2,3,3,1,1),fun = mean))
>>         >? ? ?raster::endCluster()
>>         >
>>         >? ? ?And the result is:
>>         >
>>         >? ? ?> res
>>         >? ? ?class?????????? : RasterBrick
>>         >? ? ?dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell,
>>         nlayers)
>>         >? ? ?resolution : 1, 1?? (x, y)
>>         >? ? ?extent???????? : -180, 180, -90, 90?? (xmin, xmax,
>>         ymin, ymax)
>>         >? ? ?crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
>>         >? ? ?+towgs84=0,0,0
>>         >? ? ?source???????? : memory
>>         >? ? ?names?????????? : layer.1, layer.2, layer.3
>>         >? ? ?min values :???????? 1.5,???????? 3.5,???????? 5.5
>>         >? ? ?max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>         >
>>         >
>>         >? ? ?layer.1, layer.2, layer.3 (?)
>>         >
>>         >? ? ?So what corrensponds to what?
>>         >
>>         >
>>         >? ? ?If I run:
>>         >
>>         >? ? ?res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>         >
>>         >? ? ?The result is:
>>         >
>>         >? ? ?> res2
>>         >? ? ?class? ? ? : RasterBrick
>>         >? ? ?dimensions : 180, 360, 64800, 3? (nrow, ncol, ncell,
>>         nlayers)
>>         >? ? ?resolution : 1, 1? (x, y)
>>         >? ? ?extent? ? ?: -180, 180, -90, 90? (xmin, xmax, ymin, ymax)
>>         >? ? ?crs? ? ? ? : +proj=longlat +datum=WGS84 +ellps=WGS84
>>         +towgs84=0,0,0
>>         >? ? ?source? ? ?: memory
>>         >? ? ?names? ? ? : index_2, index_3, index_1
>>         >? ? ?min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>         >? ? ?max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>         >
>>         >? ? ?There is no consistency with the names of the output
>>         and obscure
>>         >? ? ?correspondence with the indices in the case of clusterR
>>         >
>>         >
>>         >? ? ?? ? ? ? [[alternative HTML version deleted]]
>>         >
>>         >? ? ?_______________________________________________
>>         >? ? ?R-sig-Geo mailing list
>>         >? ? ?R-sig-Geo at r-project.org
>>         <mailto:R-sig-Geo at r-project.org>
>>         <mailto:R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>>
>>         >? ? ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>         >
>>
>>         ? ? ? ? [[alternative HTML version deleted]]
>>
>>         _______________________________________________
>>         R-sig-Geo mailing list
>>         R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>         https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
>>
>

	[[alternative HTML version deleted]]


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Tue Nov 26 20:53:49 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Tue, 26 Nov 2019 21:53:49 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
 <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
Message-ID: <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>

Thank you!
The problem is not with the resulting values ??but with the index
mapping. Values ??are correct in all three cases.

As I wrote in a previous post in the thread
(https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027821.html) ,
stackApply behaves inconsistently depending on whether the exported
stack will remain in memory or it will be stored, due to its large size,
on the hard disk.

In the first case the indices are identical to my function (ver_mean)
and the lubridate::wday indexing system (and are correct) while in the
second they are shuffled.

So, while Sunday has index 1 and while in the first case (when the
result is in memory) stackApply returns the correct index, in the second
case (when the result is stored on the hard disk) it returns index_4! So
how can one be sure if index_1 corresponds to Sunday or another day
using stackApply since it sometimes enumerates it with index_1 and
sometimes index_4?


Try to run this example (when the resulting stack remains in memory) to
see that the indexes are identical (stackApply = ver_median =
lubridate::wday)
https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206

Thank you again

On 11/26/19 9:00 PM, Vijay Lulla wrote:
> I'm sorry for the miscommunication.? What I meant to say is that the
> output from stackApply and zApply are the same (because zApply uses
> stackApply internally) except the names.? The behavior of stackApply
> makes sense because AFAIUI R doesn't automatically reorder
> vectors/indices that it receives.? Your observation about inconsistent
> result with ver_mean is very valid though!? And, that's what I meant
> with my comment that using sapply with the explicit ordering that you
> want is the best way to control what raster package will output.? In R
> the input order should be maintained (this is the prime difference
> between SQL and R) but packages/tools do not always adhere to
> this...so it's never clear how the output will be ordered.? Sorry for
> the confusion.
>
>
> On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos
> <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>
>     Why do they seem logical since they do not match?
>
>     Check for example index 1 (Sunday). The results are different for
>     the three processes
>
>     > stackapply_mean
>     class????? : RasterBrick
>     dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>     resolution : 500, 500? (x, y)
>     extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>     crs??????? : NA
>     source???? :
>     /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
>     names????? :? index_5,? index_6,? index_7,? index_1,? index_2,?
>     index_3,? index_4
>     min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
>     429.6900, 442.7436
>     max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
>     561.7972, 556.2471
>
>
>     > ver_mean
>     class????? : RasterStack
>     dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>     resolution : 500, 500? (x, y)
>     extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>     crs??????? : NA
>     names????? :? layer.1,? layer.2,? layer.3,? layer.4,? layer.5,?
>     layer.6,? layer.7
>     min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946,
>     440.2028, 429.6900
>     max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671,
>     560.1375, 561.7972
>
>
>     > z
>     class????? : RasterBrick
>     dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>     resolution : 500, 500? (x, y)
>     extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>     crs??????? : NA
>     source???? :
>     /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
>     names????? :?????? X1,?????? X2,?????? X3,?????? X4,??????
>     X5,?????? X6,?????? X7
>     min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
>     429.6900, 442.7436
>     max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
>     561.7972, 556.2471
>     ?????????? : 1, 2, 3, 4, 5, 6, 7
>
>
>     On 11/26/19 7:03 PM, Vijay Lulla wrote:
>>     If you read the code/help for `stackApply` and `zApply` you'll
>>     see that the results that you obtain make sense (at least they
>>     seem sensible/reasonable to me).? IMO, if you want to control the
>>     ordering of your layers then just use sapply, like how you've
>>     used for ver_mean.? IMO, this is the only reliable (safe?), and
>>     quite a readable, way to accomplish what you're trying to do.
>>     Just my 2 cents.
>>     -- Vijay.
>>
>>     On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via R-sig-Geo
>>     <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
>>
>>         I added raster::zApply in my tests to validate the results.
>>         However, the
>>         indices of the names of the results are different now. Recall
>>         that the
>>         goal is to calculate from a raster stack time series the mean
>>         per day of
>>         the week. And that problem I have is that stackApply, zApply and
>>         calc/sapply return different indices in the result names. New
>>         code is
>>         available here:
>>         https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>>         I'm really curious about missing something.
>>
>>
>>         On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>>         > Hi Leonidas,
>>         >
>>         > both results are in the same order, but the name is different.
>>         > You can rename the first as in the second:
>>         > names(res) <- names(res2)
>>         >
>>         > I provided an example to help you understand the logic.
>>         >
>>         > library(raster)
>>         > beginCluster(2)
>>         > r <- raster()
>>         > values(r) <- 1
>>         > # simple sequential stack from 1 to 6 in all cells
>>         > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>         > s
>>         > res <- clusterR(s, stackApply, args =
>>         list(indices=c(2,2,3,3,1,1), fun
>>         > = mean))
>>         > res
>>         > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>         > res2
>>         > dif <- res - res2
>>         > # exatly the same order because the difference?is zero for
>>         all layers
>>         > dif
>>         > # rename
>>         > names(res) <- names(res2)
>>         >
>>         > Best regards,
>>         >
>>         > Frederico Faleiro
>>         >
>>         > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
>>         > <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>
>>         <mailto:r-sig-geo at r-project.org
>>         <mailto:r-sig-geo at r-project.org>>> wrote:
>>         >
>>         >? ? ?I run the example with clusterR:
>>         >
>>         >? ? ?no_cores <- parallel::detectCores() -1
>>         >? ? ?raster::beginCluster(no_cores)
>>         >? ? ??????? res <- raster::clusterR(inp, raster::stackApply,
>>         args =
>>         >? ? ?list(indices=c(2,2,3,3,1,1),fun = mean))
>>         >? ? ?raster::endCluster()
>>         >
>>         >? ? ?And the result is:
>>         >
>>         >? ? ?> res
>>         >? ? ?class?????????? : RasterBrick
>>         >? ? ?dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell,
>>         nlayers)
>>         >? ? ?resolution : 1, 1?? (x, y)
>>         >? ? ?extent???????? : -180, 180, -90, 90?? (xmin, xmax,
>>         ymin, ymax)
>>         >? ? ?crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
>>         >? ? ?+towgs84=0,0,0
>>         >? ? ?source???????? : memory
>>         >? ? ?names?????????? : layer.1, layer.2, layer.3
>>         >? ? ?min values :???????? 1.5,???????? 3.5,???????? 5.5
>>         >? ? ?max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>         >
>>         >
>>         >? ? ?layer.1, layer.2, layer.3 (?)
>>         >
>>         >? ? ?So what corrensponds to what?
>>         >
>>         >
>>         >? ? ?If I run:
>>         >
>>         >? ? ?res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>         >
>>         >? ? ?The result is:
>>         >
>>         >? ? ?> res2
>>         >? ? ?class? ? ? : RasterBrick
>>         >? ? ?dimensions : 180, 360, 64800, 3? (nrow, ncol, ncell,
>>         nlayers)
>>         >? ? ?resolution : 1, 1? (x, y)
>>         >? ? ?extent? ? ?: -180, 180, -90, 90? (xmin, xmax, ymin, ymax)
>>         >? ? ?crs? ? ? ? : +proj=longlat +datum=WGS84 +ellps=WGS84
>>         +towgs84=0,0,0
>>         >? ? ?source? ? ?: memory
>>         >? ? ?names? ? ? : index_2, index_3, index_1
>>         >? ? ?min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>         >? ? ?max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>         >
>>         >? ? ?There is no consistency with the names of the output
>>         and obscure
>>         >? ? ?correspondence with the indices in the case of clusterR
>>         >
>>         >
>>         >? ? ?? ? ? ? [[alternative HTML version deleted]]
>>         >
>>         >? ? ?_______________________________________________
>>         >? ? ?R-sig-Geo mailing list
>>         >? ? ?R-sig-Geo at r-project.org
>>         <mailto:R-sig-Geo at r-project.org>
>>         <mailto:R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>>
>>         >? ? ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>         >
>>
>>         ? ? ? ? [[alternative HTML version deleted]]
>>
>>         _______________________________________________
>>         R-sig-Geo mailing list
>>         R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>         https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
>>
>

	[[alternative HTML version deleted]]


From v|j@y|u||@ @end|ng |rom gm@||@com  Tue Nov 26 21:58:34 2019
From: v|j@y|u||@ @end|ng |rom gm@||@com (Vijay Lulla)
Date: Tue, 26 Nov 2019 15:58:34 -0500
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
 <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
 <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>
Message-ID: <CAKkiGbvZEtrp-gYApMxPj6eNGd4T-ERb3pvHnFniYaq8a8iNhw@mail.gmail.com>

Hmm...it appears that stackApply is using different conditions which might
be causing this problem. Below is the snippet of the code which I think
might be the problem.

## For canProcessInMemory
if (rowcalc) {
  v <- lapply(uin, function(i) fun(x[, ind == i, drop = FALSE], na.rm =
na.rm))
}
else {
  v <- lapply(uin, function(i, ...) apply(x[, ind == i, drop = FALSE], 1,
fun, na.rm = na.rm))
}


## If canProcessInMemory is not TRUE
if (rowcalc) {
  v <- lapply(uin, function(i) fun(a[, ind == uin[i], drop = FALSE], na.rm
= na.rm))
}
else {
  v <- lapply(uin, function(i, ...) apply(a[, ind == uin[i], drop = FALSE],
1, fun, na.rm = na.rm))
}

I think they should both be same but it appears that they aren't and that's
what you've discovered.  Maybe you can try fix(stackApply) to see if it
really is the problem and can tell us what you find.  Anyways, good
catch...and...sorry for wasting your time.
Cordially,
Vijay.

On Tue, Nov 26, 2019 at 2:53 PM Leonidas Liakos <leonidas_liakos at yahoo.gr>
wrote:

> Thank you!
> The problem is not with the resulting values but with the index mapping.
> Values are correct in all three cases.
>
> As I wrote in a previous post in the thread (
> https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027821.html) ,
> stackApply behaves inconsistently depending on whether the exported stack
> will remain in memory or it will be stored, due to its large size, on the
> hard disk.
>
> In the first case the indices are identical to my function (ver_mean) and
> the lubridate::wday indexing system (and are correct) while in the second
> they are shuffled.
>
> So, while Sunday has index 1 and while in the first case (when the result
> is in memory) stackApply returns the correct index, in the second case
> (when the result is stored on the hard disk) it returns index_4! So how can
> one be sure if index_1 corresponds to Sunday or another day using
> stackApply since it sometimes enumerates it with index_1 and sometimes
> index_4?
>
>
> Try to run this example (when the resulting stack remains in memory) to
> see that the indexes are identical (stackApply = ver_median =
> lubridate::wday)
> https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206
>
> Thank you again
> On 11/26/19 9:00 PM, Vijay Lulla wrote:
>
> I'm sorry for the miscommunication.  What I meant to say is that the
> output from stackApply and zApply are the same (because zApply uses
> stackApply internally) except the names.  The behavior of stackApply makes
> sense because AFAIUI R doesn't automatically reorder vectors/indices that
> it receives.  Your observation about inconsistent result with ver_mean is
> very valid though!  And, that's what I meant with my comment that using
> sapply with the explicit ordering that you want is the best way to control
> what raster package will output.  In R the input order should be maintained
> (this is the prime difference between SQL and R) but packages/tools do not
> always adhere to this...so it's never clear how the output will be
> ordered.  Sorry for the confusion.
>
>
> On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos <leonidas_liakos at yahoo.gr>
> wrote:
>
>> Why do they seem logical since they do not match?
>>
>> Check for example index 1 (Sunday). The results are different for the
>> three processes
>>
>> > stackapply_mean
>> class      : RasterBrick
>> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
>> resolution : 500, 500  (x, y)
>> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
>> crs        : NA
>> source     :
>> /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
>> names      :  index_5,  index_6,  index_7,  index_1,  index_2,  index_3,
>> index_4
>> min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
>> 442.7436
>> max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
>> 556.2471
>>
>>
>> > ver_mean
>> class      : RasterStack
>> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
>> resolution : 500, 500  (x, y)
>> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
>> crs        : NA
>> names      :  layer.1,  layer.2,  layer.3,  layer.4,  layer.5,  layer.6,
>> layer.7
>> min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
>> 429.6900
>> max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
>> 561.7972
>>
>>
>> > z
>> class      : RasterBrick
>> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
>> resolution : 500, 500  (x, y)
>> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
>> crs        : NA
>> source     :
>> /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
>> names      :       X1,       X2,       X3,       X4,       X5,
>> X6,       X7
>> min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
>> 442.7436
>> max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
>> 556.2471
>>            : 1, 2, 3, 4, 5, 6, 7
>>
>>
>> On 11/26/19 7:03 PM, Vijay Lulla wrote:
>>
>> If you read the code/help for `stackApply` and `zApply` you'll see that
>> the results that you obtain make sense (at least they seem
>> sensible/reasonable to me).  IMO, if you want to control the ordering of
>> your layers then just use sapply, like how you've used for ver_mean.  IMO,
>> this is the only reliable (safe?), and quite a readable, way to accomplish
>> what you're trying to do.
>> Just my 2 cents.
>> -- Vijay.
>>
>> On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via R-sig-Geo <
>> r-sig-geo at r-project.org> wrote:
>>
>>> I added raster::zApply in my tests to validate the results. However, the
>>> indices of the names of the results are different now. Recall that the
>>> goal is to calculate from a raster stack time series the mean per day of
>>> the week. And that problem I have is that stackApply, zApply and
>>> calc/sapply return different indices in the result names. New code is
>>> available here:
>>> https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>>> I'm really curious about missing something.
>>>
>>>
>>> On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>>> > Hi Leonidas,
>>> >
>>> > both results are in the same order, but the name is different.
>>> > You can rename the first as in the second:
>>> > names(res) <- names(res2)
>>> >
>>> > I provided an example to help you understand the logic.
>>> >
>>> > library(raster)
>>> > beginCluster(2)
>>> > r <- raster()
>>> > values(r) <- 1
>>> > # simple sequential stack from 1 to 6 in all cells
>>> > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>> > s
>>> > res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun
>>> > = mean))
>>> > res
>>> > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>> > res2
>>> > dif <- res - res2
>>> > # exatly the same order because the difference is zero for all layers
>>> > dif
>>> > # rename
>>> > names(res) <- names(res2)
>>> >
>>> > Best regards,
>>> >
>>> > Frederico Faleiro
>>> >
>>> > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
>>> > <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
>>> >
>>> >     I run the example with clusterR:
>>> >
>>> >     no_cores <- parallel::detectCores() -1
>>> >     raster::beginCluster(no_cores)
>>> >     ?????? res <- raster::clusterR(inp, raster::stackApply, args =
>>> >     list(indices=c(2,2,3,3,1,1),fun = mean))
>>> >     raster::endCluster()
>>> >
>>> >     And the result is:
>>> >
>>> >     > res
>>> >     class?????????? : RasterBrick
>>> >     dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>>> >     resolution : 1, 1?? (x, y)
>>> >     extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>>> >     crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
>>> >     +towgs84=0,0,0
>>> >     source???????? : memory
>>> >     names?????????? : layer.1, layer.2, layer.3
>>> >     min values :???????? 1.5,???????? 3.5,???????? 5.5
>>> >     max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>> >
>>> >
>>> >     layer.1, layer.2, layer.3 (?)
>>> >
>>> >     So what corrensponds to what?
>>> >
>>> >
>>> >     If I run:
>>> >
>>> >     res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>> >
>>> >     The result is:
>>> >
>>> >     > res2
>>> >     class      : RasterBrick
>>> >     dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
>>> >     resolution : 1, 1  (x, y)
>>> >     extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>> >     crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>> >     source     : memory
>>> >     names      : index_2, index_3, index_1
>>> >     min values :     1.5,     3.5,     5.5
>>> >     max values :     1.5,     3.5,     5.5
>>> >
>>> >     There is no consistency with the names of the output and obscure
>>> >     correspondence with the indices in the case of clusterR
>>> >
>>> >
>>> >             [[alternative HTML version deleted]]
>>> >
>>> >     _______________________________________________
>>> >     R-sig-Geo mailing list
>>> >     R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>> >     https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>>
>>
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Nov 27 13:53:33 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 27 Nov 2019 12:53:33 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB07680F075EA1B55A9AE9D56AB0450@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
 <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911262040120.583221@reclus.nhh.no>,
 <VI1P190MB07680F075EA1B55A9AE9D56AB0450@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <3cd7666ef730416c8e7f6f4110315f0a@nhh.no>

Yes this is expected, since the # neighbours in a single zip code block is a dense matrix, and there will be multiple such matrices. (15000^2)*8 is 1.8e+09 so such a dense matrix will max out your RAM. There is no way to look at block neighbours in that format without subsetting your data (think train/test), use a zip code random effect. I would certainly drop all attempts to examine spatial dependency until you get an aspatial multilevel hedonic model working.

Roger 

--
Roger Bivand
Norwegian School of Economics
Helleveien 30, 5045 Bergen, Norway
Roger.Bivand at nhh.no


________________________________________
Fra: Robert R <usercatch at outlook.com>
Sendt: tirsdag 26. november 2019 21.04
Til: Roger Bivand
Kopi: r-sig-geo at r-project.org
Emne: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

Dear Roger,

Thank you for your e-mail. Actually there is less noise that it seems. Rental prices are daily rental prices and I have an extract of all Airbnb listings daily prices once a month for a period of 4 years. Each listings information contains the lat, lon, number of bedrooms, category (entire home/apt, shared room or private room), etc.

One question regarding the spdep::nb2blocknb function: it runs super fast with up to n = 1000, and always crashes my R session with n = 15000 or so. Is there an alternative to solve this problem?

Thank you and best regards,
Robert

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Tuesday, November 26, 2019 20:48
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

Sorry for late reply, am indisposed and unable to help further. I feel
that there is so much noise in your data (differences in offers, rental
lengths, repeats or not, etc.), that you will certainly have to subset
vigorously first to isolate response cases that are comparable. What you
are trying to disentangle are the hedonic components in the bundle where
you just have price as response, but lots of other bundle characteristics
on the right hand side (days, etc.). I feel you'd need to try to get to a
response index of price per day per rental area or some such. I'd
certainly advise examining responses to a specific driver (major concert
or sports event) to get a feel for how the market responds, and return to
spatial hedonic after finding an approach that gives reasonable aspatial
outcomes.

Roger

On Sun, 17 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your message and sorry for my late answer.
>
> Regarding the number of listings (lettings) for my data set (2.216.642 observations), each listing contains an individual id:
>
> unique ids: 180.004
> time periods: 54 (2015-01 to 2019-09)
> number of ids that appear only once: 28.486 (of 180.004 ids) (15,8%)
> number of ids that appear/repeat 2-10 times: 82.641 (of 180.004 ids) (45,9%)
> number of ids that appear/repeat 11-30 times: 46.465 (of 180.004 ids) (25,8%)
> number of ids that appear/repeat 31-54 times: 22.412 (of 180.004 ids) (12,5%)
>
> Important to notice is that hosts can change the room_category (between entire/home apt, private room and shared room) keeping the same listing id number. In my data, the number of unique ids that in some point changed the room_type is of 7.204 ids.
>
> --
>
> For the OLS model, I was using only a fixed effect model, where each time period (date_compiled) (54 in total) is a time dummy.
>
> plm::plm(formula = model, data = listings, model = "pooling", index = c("id", "date_compiled"))
>
>
> --
> Osland et al. (2016) (https://doi.org/10.1111/jors.12281) use a spatial fixed effects (SFE) hedonic model, where each defined neighborhood zone in the study area is represented by dummy variables.
>
> Dong et al. (2015) (https://doi.org/10.1111/gean.12049) outline four model specifications to accommodate geographically hierarchical data structures: (1) groupwise W and fixed regional effects; (2) groupwise W and random regional effects; (3) proximity-based W and fixed regional effects; and (4) proximity-based W and random regional effects.
> --
>
> I created a new column/variable containing the borough where the zipcode is found (Manhattan, Brooklyn, Queens, Bronx, Staten Island).
>
> If I understood it right, the (two-level) Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) considers the occurrence of spatial relations at the (lower) individual (geographical coordinates - in my case, the listing location) and (higher) group level (territorial units - in my case, zipcodes).
>
> According to Bivand et al. (2017): "(...) W is a spatial weights matrix. The HSAR model may also be estimated without this component.". So, in this case I only estimate the Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) in a "one-level" basis, i.e., at the higher-level.
>
> HSAR::hsar(model, data = listings, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>
> (Where the "model" formula contains the 54 time dummy variables)
>
> Do you think I can proceed with this model? I was able to calculate it.
>
> If I remove all observations/rows with NAs in one of the chosen variables/observations, 884.183 observations remain. If I would create a W matrix for HSAR::hsar, I would have a gigantic 884.183 by 884.183 matrix. This is the reason why I put W = NULL.
>
>
> Thank you and best regards
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Monday, November 11, 2019 11:31
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Sun, 10 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Again, thank you for your answer. I read the material provided and
>> decided that Hierarchical Spatial Autoregressive (HSAR) could be the
>> right model for me.
>>
>> I indeed have the precise latitude and longitude information for all my
>> listings for NYC.
>>
>> I created a stratified sample (group = zipcode) with 22172 (1%) of my
>> observations called listings_sample and tried to replicate the hsar
>> model, please see below.
>>
>> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.
>
> Unless you know definitely that you want to relate the response to its
> lagged value, you do not need this. Do note that the matrix is very
> sparse, so could be fitted without difficulty with ML in a cross-sectional
> model.
>
>>
>> You recommended then to introduce a Markov random field (MRF) random
>> effect (RE) at the zipcode level, but I did not understand it so well.
>> Could you develop a litte more?
>>
>
> Did you read the development in
> https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and
> includes code for fitting the Beijing housing parcels data se from HSAR
> with many other packages (MCMC, INLA, hglm, etc.). I guess that you should
> try to create a model that works on a single borough, sing the zipcodes
> in that borough as a proxy for unobserved neighbourhood effects. Try for
> example using lme4::lmer() with only a zipcode IID random effect, see if
> the hedonic estimates are similar to lm(), and leave adding an MRF RE
> (with for example mgcv::gam() or hglm::hglm()) until you have a working
> testbed. Then advance step-by-step from there.
>
> You still have not said how many repeat lettings you see - it will affect
> the way you specify your model.
>
> Roger
>
>> ##############
>> library(spdep)
>> library(HSAR)
>> library(dplyr)
>> library(splitstackshape)
>>
>>
>> # Stratified sample per zipcode (size = 1%) listings_sample <-
>> splitstackshape::stratified(indt = listings, group = "zipcode", size =
>> 0.01)
>>
>> # Removing zipcodes from polygon_nyc which are not observable in
>> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode
>> %in% c(unique(as.character(listings_sample$zipcode))))
>>
>>
>> ## Random effect matrix (N by J)
>>
>> # N: 22172
>> # J: 154
>>
>> # Arrange listings_sample by zipcode (ascending)
>> listings_sample <- listings_sample %>% arrange(zipcode)
>>
>> # Count number of listings per zipcode
>> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
>> # sum(MM$count)
>>
>> # N by J nulled matrix creation
>> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>>
>> # The total number of neighbourhood
>> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>>
>> for(i in 1:dim(MM)[1]) {
>>  Delta[Uid==i,i] <- 1
>> }
>> rm(i)
>>
>> Delta <- as(Delta,"dgCMatrix")
>>
>>
>> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>>
>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>>
>> # Include neighbour itself as a neighbour
>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>
>> # Spatial weights matrix for nb
>> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
>> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>>
>>
>> ## Fit HSAR SAR upper level random effect
>> model <- as.formula(log_price ~ guests_included + minimum_nights)
>>
>> betas = coef(lm(formula = model, data = listings_sample))
>> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>>
>> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>
>> ##############
>>
>> Thank you and best regards
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Friday, November 8, 2019 13:29
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Fri, 8 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Thank you for your answer.
>>>
>>> I successfully used the function nb2blocknb() for a smaller dataset.
>>>
>>> But for a dataset of over 2 million observations, I get the following
>>> error: "Error: cannot allocate vector of size 840 Kb".
>>
>> I don't think the observations are helpful. If you have repeat lets in the
>> same property in a given month, you need to handle that anyway. I'd go for
>> making the modelling exercise work (we agree that this is not panel data,
>> right?) on a small subset first. I would further argue that you need a
>> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
>> RE. You could very well take (stratified) samples per zipcode to represent
>> your data. Once that works, introduce an MRF RE at the zipcode level,
>> where you do know relative position. Using SARAR is going to be a waste of
>> time unless you can geocode the letting addresses. A multi-level approach
>> will work. Having big data in your case with no useful location
>> information per observation is just adding noise and over-smoothing, I'm
>> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
>> will work, also when you sample the within zipcode lets, given a split
>> into training and test sets, and making CV possible.
>>
>> Roger
>>
>>>
>>> I am expecting that at least 500.000 observations will be dropped due
>>> the lack of values for the chosen variables for the regression model, so
>>> probably I will filter and remove the observations/rows that will not be
>>> used anyway - do you know if there is any package that does this
>>> automatically, given the variables/columns chosed by me?
>>>
>>> Or would you recommend me another approach to avoid the above mentioned
>>> error?
>>>
>>> Thank you and best regards,
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Thursday, November 7, 2019 10:13
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Thu, 7 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Many thanks for your help.
>>>>
>>>> I have an additional question:
>>>>
>>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>>> rownumbers) from my data set? For now, I am taking my data set and
>>>> merging with the sf object polygon_nyc with the function
>>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>>> a huge n x n matrix (depending of the size of my data set).
>>>>
>>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>>> object has only n = 177.
>>>>
>>>> Of course running
>>>>
>>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>>
>>>> does not work ("Input data and weights have different dimensions").
>>>>
>>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>>> zipcode) and then create the weights list lw? Or there another option?
>>>
>>> I think we are getting more clarity. You do not know the location of the
>>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>>> areas, and can create a neighbour object from these boundaries. You then
>>> want to treat all the lettings in a zipcode area i as neighbours, and
>>> additionally lettings in zipcode areas neighbouring i as neighbours of
>>> lettings in i. This is the data structure that motivated the
>>> spdep::nb2blocknb() function:
>>>
>>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>>
>>> Try running the examples to get a feel for what is going on.
>>>
>>> I feel that most of the variability will vanish in the very large numbers
>>> of neighbours, over-smoothing the outcomes. If you do not have locations
>>> for the lettings themselves, I don't think you can make much progress.
>>>
>>> You could try a linear mixed model (or gam with a spatially structured
>>> random effect) with a temporal and a spatial random effect. See the HSAR
>>> package, articles by Dong et al., and maybe
>>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>>> effects at the zipcode level might be a way forward, together with an IID
>>> random effect at the same level (equivalent to sef-neighbours).
>>>
>>> Hope this helps,
>>>
>>> Roger
>>>
>>>>
>>>> Best regards,
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Wednesday, November 6, 2019 15:07
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>
>>>>> Dear Roger,
>>>>>
>>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>>> plain text.
>>>>>
>>>>> I will give a better context for my desired outcome.
>>>>>
>>>>> I am taking Airbnb's listings information for New York City available
>>>>> on: http://insideairbnb.com/get-the-data.html
>>>>>
>>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>>> Listings/ folder. When importing all these 54 files into one single data
>>>>> set, I create a new "date_compiled" variable/column.
>>>>>
>>>>> In total, after the data cleansing process, I have a little more 2
>>>>> million observations.
>>>>
>>>> You have repeat lettings for some, but not all properties. So this is at
>>>> best a very unbalanced panel. For those properties with repeats, you may
>>>> see temporal movement (trend/seasonal).
>>>>
>>>> I suggest (strongly) taking a single borough or even zipcode with some
>>>> hindreds of properties, and working from there. Do not include the
>>>> observation as its own neighbour, perhaps identify repeats and handle them
>>>> specially (create or use a property ID). Unbalanced panels may also create
>>>> a selection bias issue (why are some properties only listed sometimes?).
>>>>
>>>> So this although promising isn't simple, and getting to a hedonic model
>>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>>> necessarily trust OLS output either, partly because of the repeat property
>>>> issue.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> I created 54 timedummy variables for each time period available.
>>>>>
>>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>>> a variety of characteristics which potentially determine the daily rate
>>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>>> of the listing as number of bedrooms, if the host if professional,
>>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>>> from the listing, income per capita, etc.).
>>>>>
>>>>> My dependent variable is price (log price, common in the related
>>>>> literature for hedonic prices).
>>>>>
>>>>> The OLS model is done.
>>>>>
>>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>>> pricing of their listings, take not only into account its structural and
>>>>> location characteristics, but also the prices charged by near listings
>>>>> with similar characteristics - spatial autocorrelation is then present,
>>>>> at least spatial dependence is present in the dependent variable.
>>>>>
>>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>>> itself as a neighbor.
>>>>>
>>>>> Parts of my code can be found below:
>>>>>
>>>>> ########
>>>>>
>>>>> ## packages
>>>>>
>>>>> packages_install <- function(packages){
>>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>>> if (length(new.packages))
>>>>> install.packages(new.packages, dependencies = TRUE)
>>>>> sapply(packages, require, character.only = TRUE)
>>>>> }
>>>>>
>>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>>> packages_install(packages_required)
>>>>>
>>>>> # Working directory
>>>>> setwd("C:/Users/User/R")
>>>>>
>>>>>
>>>>>
>>>>> ## shapefile_us
>>>>>
>>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>>
>>>>> # Columns removal
>>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>>
>>>>> # Column rename: ZCTA5CE10
>>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>>
>>>>> # Column class change: zipcode
>>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>>
>>>>>
>>>>>
>>>>> ## polygon_nyc
>>>>>
>>>>> # Zip code not available in shapefile: 11695
>>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>>
>>>>>
>>>>>
>>>>> ## weight_matrix
>>>>>
>>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>>
>>>>> # Include neighbour itself as a neighbour
>>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>>
>>>>> # Weights to each neighboring polygon
>>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>>
>>>>>
>>>>>
>>>>> ## listings
>>>>>
>>>>> # Data import
>>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>>> listings <- listings %>% bind_rows
>>>>>
>>>>> # Characters removal
>>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>>> listings$price <- gsub("\\$", "", listings$price)
>>>>> listings$price <- gsub(",", "", listings$price)
>>>>>
>>>>>
>>>>>
>>>>> ## timedummy
>>>>>
>>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>>> timedummy <- gsub("-", "_", timedummy)
>>>>>
>>>>>
>>>>>
>>>>> ## OLS regression
>>>>>
>>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>>
>>>>> ########
>>>>>
>>>>> Some of my id's repeat in multiple time periods.
>>>>>
>>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>>
>>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>>
>>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>>
>>>>> Again, thank you very much for the help provided until now.
>>>>>
>>>>> Best regards,
>>>>> Robert
>>>>>
>>>>> ________________________________________
>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>> Sent: Tuesday, November 5, 2019 15:30
>>>>> To: Robert R
>>>>> Cc: r-sig-geo at r-project.org
>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>
>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>
>>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>>> for now that spatial dependence is present in both the dependent
>>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>>> think it would be possible to calculate this.? What I would like to know
>>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>>
>>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>>> of the covariates time-varying? Please provide motivation and use area
>>>>> (preferably with affiliation (your email and user name are not
>>>>> informative) - this feels like a real estate problem, probably wrongly
>>>>> specified. You should use splm if time make sense in your case, but if it
>>>>> really doesn't, simplify your approach, as much of the data will be
>>>>> subject to very large temporal autocorrelation.
>>>>>
>>>>> If this is a continuation of your previous question about using
>>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>>
>>>>> Roger
>>>>>
>>>>>>
>>>>>>       [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-Geo mailing list
>>>>>> R-sig-Geo at r-project.org
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>
>>>>> --
>>>>> Roger Bivand
>>>>> Department of Economics, Norwegian School of Economics,
>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>> https://orcid.org/0000-0003-2392-6140
>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Wed Nov 27 18:43:13 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Wed, 27 Nov 2019 19:43:13 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CAKkiGbvZEtrp-gYApMxPj6eNGd4T-ERb3pvHnFniYaq8a8iNhw@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
 <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
 <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>
 <CAKkiGbvZEtrp-gYApMxPj6eNGd4T-ERb3pvHnFniYaq8a8iNhw@mail.gmail.com>
Message-ID: <0f533804-424c-1fbe-30f5-3abc78788b7d@yahoo.gr>

Thank you for your help!

I tried to fix stackApply according to your instructions.

Now the indices of names are the same and consistent with indices
enumeration (gist for validation and tests:
https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170).

I've attached a patch file here:

https://gist.github.com/kokkytos/ca2c319134677b19900579665267a7a7

> stackapply_mean
class????? : RasterBrick
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
source???? : /tmp/Rtmp9W8UNc/raster/r_tmp_2019-11-27_191205_2929_20324.grd
names????? :? index_5,? index_6,? index_7,? index_1,? index_2,?
index_3,? index_4
min values : 444.6946, 440.2028, 429.6900, 442.7436, 440.0467, 444.9182,
437.1589
max values : 565.8671, 560.1375, 561.7972, 556.2471, 563.8341, 561.7687,
560.4509

> ver_mean
class????? : RasterStack
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
names????? :? layer.1,? layer.2,? layer.3,? layer.4,? layer.5,?
layer.6,? layer.7
min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
429.6900
max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
561.7972





On 11/26/19 10:58 PM, Vijay Lulla wrote:
> Hmm...it appears that stackApply is using different conditions which
> might be causing this problem. Below is the snippet of the code which
> I think might be the problem.
>
> ## For canProcessInMemory
> if (rowcalc) {
> ? v <- lapply(uin, function(i) fun(x[, ind == i, drop = FALSE], na.rm
> = na.rm))
> }
> else {
> ? v <- lapply(uin, function(i, ...) apply(x[, ind == i, drop = FALSE],
> 1, fun, na.rm = na.rm))
> }
>
>
> ## If canProcessInMemory is not TRUE
> if (rowcalc) {
> ? v <- lapply(uin, function(i) fun(a[, ind == uin[i], drop = FALSE],
> na.rm = na.rm))
> }
> else {
> ? v <- lapply(uin, function(i, ...) apply(a[, ind == uin[i], drop =
> FALSE], 1, fun, na.rm = na.rm))
> }
>
> I think they should both be same but it appears that they aren't and
> that's what you've discovered.? Maybe you can try fix(stackApply) to
> see if it really is the problem and can tell us what you find.?
> Anyways, good catch...and...sorry for wasting your time.
> Cordially,
> Vijay.
>
> On Tue, Nov 26, 2019 at 2:53 PM Leonidas Liakos
> <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>
>     Thank you!
>     The problem is not with the resulting values but with the index
>     mapping. Values are correct in all three cases.
>
>     As I wrote in a previous post in the thread
>     (https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027821.html)
>     , stackApply behaves inconsistently depending on whether the
>     exported stack will remain in memory or it will be stored, due to
>     its large size, on the hard disk.
>
>     In the first case the indices are identical to my function
>     (ver_mean) and the lubridate::wday indexing system (and are
>     correct) while in the second they are shuffled.
>
>     So, while Sunday has index 1 and while in the first case (when the
>     result is in memory) stackApply returns the correct index, in the
>     second case (when the result is stored on the hard disk) it
>     returns index_4! So how can one be sure if index_1 corresponds to
>     Sunday or another day using stackApply since it sometimes
>     enumerates it with index_1 and sometimes index_4?
>
>
>     Try to run this example (when the resulting stack remains in
>     memory) to see that the indexes are identical (stackApply =
>     ver_median = lubridate::wday)
>     https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206
>
>     Thank you again
>
>     On 11/26/19 9:00 PM, Vijay Lulla wrote:
>>     I'm sorry for the miscommunication.? What I meant to say is that
>>     the output from stackApply and zApply are the same (because
>>     zApply uses stackApply internally) except the names.? The
>>     behavior of stackApply makes sense because AFAIUI R doesn't
>>     automatically reorder vectors/indices that it receives.? Your
>>     observation about inconsistent result with ver_mean is very valid
>>     though!? And, that's what I meant with my comment that using
>>     sapply with the explicit ordering that you want is the best way
>>     to control what raster package will output.? In R the input order
>>     should be maintained (this is the prime difference between SQL
>>     and R) but packages/tools do not always adhere to this...so it's
>>     never clear how the output will be ordered.? Sorry for the confusion.
>>
>>
>>     On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos
>>     <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>>
>>         Why do they seem logical since they do not match?
>>
>>         Check for example index 1 (Sunday). The results are different
>>         for the three processes
>>
>>         > stackapply_mean
>>         class????? : RasterBrick
>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>         resolution : 500, 500? (x, y)
>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>         crs??????? : NA
>>         source???? :
>>         /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
>>         names????? :? index_5,? index_6,? index_7,? index_1,?
>>         index_2,? index_3,? index_4
>>         min values : 440.0467, 444.9182, 437.1589, 444.6946,
>>         440.2028, 429.6900, 442.7436
>>         max values : 563.8341, 561.7687, 560.4509, 565.8671,
>>         560.1375, 561.7972, 556.2471
>>
>>
>>         > ver_mean
>>         class????? : RasterStack
>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>         resolution : 500, 500? (x, y)
>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>         crs??????? : NA
>>         names????? :? layer.1,? layer.2,? layer.3,? layer.4,?
>>         layer.5,? layer.6,? layer.7
>>         min values : 442.7436, 440.0467, 444.9182, 437.1589,
>>         444.6946, 440.2028, 429.6900
>>         max values : 556.2471, 563.8341, 561.7687, 560.4509,
>>         565.8671, 560.1375, 561.7972
>>
>>
>>         > z
>>         class????? : RasterBrick
>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>         resolution : 500, 500? (x, y)
>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>         crs??????? : NA
>>         source???? :
>>         /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
>>         names????? :?????? X1,?????? X2,?????? X3,?????? X4,??????
>>         X5,?????? X6,?????? X7
>>         min values : 440.0467, 444.9182, 437.1589, 444.6946,
>>         440.2028, 429.6900, 442.7436
>>         max values : 563.8341, 561.7687, 560.4509, 565.8671,
>>         560.1375, 561.7972, 556.2471
>>         ?????????? : 1, 2, 3, 4, 5, 6, 7
>>
>>
>>         On 11/26/19 7:03 PM, Vijay Lulla wrote:
>>>         If you read the code/help for `stackApply` and `zApply`
>>>         you'll see that the results that you obtain make sense (at
>>>         least they seem sensible/reasonable to me).? IMO, if you
>>>         want to control the ordering of your layers then just use
>>>         sapply, like how you've used for ver_mean.? IMO, this is the
>>>         only reliable (safe?), and quite a readable, way to
>>>         accomplish what you're trying to do.
>>>         Just my 2 cents.
>>>         -- Vijay.
>>>
>>>         On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via
>>>         R-sig-Geo <r-sig-geo at r-project.org
>>>         <mailto:r-sig-geo at r-project.org>> wrote:
>>>
>>>             I added raster::zApply in my tests to validate the
>>>             results. However, the
>>>             indices of the names of the results are different now.
>>>             Recall that the
>>>             goal is to calculate from a raster stack time series the
>>>             mean per day of
>>>             the week. And that problem I have is that stackApply,
>>>             zApply and
>>>             calc/sapply return different indices in the result
>>>             names. New code is
>>>             available here:
>>>             https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>>>             I'm really curious about missing something.
>>>
>>>
>>>             On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>>>             > Hi Leonidas,
>>>             >
>>>             > both results are in the same order, but the name is
>>>             different.
>>>             > You can rename the first as in the second:
>>>             > names(res) <- names(res2)
>>>             >
>>>             > I provided an example to help you understand the logic.
>>>             >
>>>             > library(raster)
>>>             > beginCluster(2)
>>>             > r <- raster()
>>>             > values(r) <- 1
>>>             > # simple sequential stack from 1 to 6 in all cells
>>>             > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>>             > s
>>>             > res <- clusterR(s, stackApply, args =
>>>             list(indices=c(2,2,3,3,1,1), fun
>>>             > = mean))
>>>             > res
>>>             > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>>             > res2
>>>             > dif <- res - res2
>>>             > # exatly the same order because the difference?is zero
>>>             for all layers
>>>             > dif
>>>             > # rename
>>>             > names(res) <- names(res2)
>>>             >
>>>             > Best regards,
>>>             >
>>>             > Frederico Faleiro
>>>             >
>>>             > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via
>>>             R-sig-Geo
>>>             > <r-sig-geo at r-project.org
>>>             <mailto:r-sig-geo at r-project.org>
>>>             <mailto:r-sig-geo at r-project.org
>>>             <mailto:r-sig-geo at r-project.org>>> wrote:
>>>             >
>>>             >? ? ?I run the example with clusterR:
>>>             >
>>>             >? ? ?no_cores <- parallel::detectCores() -1
>>>             >? ? ?raster::beginCluster(no_cores)
>>>             >? ? ??????? res <- raster::clusterR(inp,
>>>             raster::stackApply, args =
>>>             >? ? ?list(indices=c(2,2,3,3,1,1),fun = mean))
>>>             >? ? ?raster::endCluster()
>>>             >
>>>             >? ? ?And the result is:
>>>             >
>>>             >? ? ?> res
>>>             >? ? ?class?????????? : RasterBrick
>>>             >? ? ?dimensions : 180, 360, 64800, 3?? (nrow, ncol,
>>>             ncell, nlayers)
>>>             >? ? ?resolution : 1, 1?? (x, y)
>>>             >? ? ?extent???????? : -180, 180, -90, 90?? (xmin, xmax,
>>>             ymin, ymax)
>>>             >? ? ?crs?????????????? : +proj=longlat +datum=WGS84
>>>             +ellps=WGS84
>>>             >? ? ?+towgs84=0,0,0
>>>             >? ? ?source???????? : memory
>>>             >? ? ?names?????????? : layer.1, layer.2, layer.3
>>>             >? ? ?min values :???????? 1.5,???????? 3.5,???????? 5.5
>>>             >? ? ?max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>>             >
>>>             >
>>>             >? ? ?layer.1, layer.2, layer.3 (?)
>>>             >
>>>             >? ? ?So what corrensponds to what?
>>>             >
>>>             >
>>>             >? ? ?If I run:
>>>             >
>>>             >? ? ?res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>>             >
>>>             >? ? ?The result is:
>>>             >
>>>             >? ? ?> res2
>>>             >? ? ?class? ? ? : RasterBrick
>>>             >? ? ?dimensions : 180, 360, 64800, 3? (nrow, ncol,
>>>             ncell, nlayers)
>>>             >? ? ?resolution : 1, 1? (x, y)
>>>             >? ? ?extent? ? ?: -180, 180, -90, 90? (xmin, xmax,
>>>             ymin, ymax)
>>>             >? ? ?crs? ? ? ? : +proj=longlat +datum=WGS84
>>>             +ellps=WGS84 +towgs84=0,0,0
>>>             >? ? ?source? ? ?: memory
>>>             >? ? ?names? ? ? : index_2, index_3, index_1
>>>             >? ? ?min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>>             >? ? ?max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>>             >
>>>             >? ? ?There is no consistency with the names of the
>>>             output and obscure
>>>             >? ? ?correspondence with the indices in the case of
>>>             clusterR
>>>             >
>>>             >
>>>             >? ? ?? ? ? ? [[alternative HTML version deleted]]
>>>             >
>>>             >? ? ?_______________________________________________
>>>             >? ? ?R-sig-Geo mailing list
>>>             >? ? ?R-sig-Geo at r-project.org
>>>             <mailto:R-sig-Geo at r-project.org>
>>>             <mailto:R-sig-Geo at r-project.org
>>>             <mailto:R-sig-Geo at r-project.org>>
>>>             >? ? ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>             >
>>>
>>>             ? ? ? ? [[alternative HTML version deleted]]
>>>
>>>             _______________________________________________
>>>             R-sig-Geo mailing list
>>>             R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>>             https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Wed Nov 27 18:43:13 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Wed, 27 Nov 2019 19:43:13 +0200
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <CAKkiGbvZEtrp-gYApMxPj6eNGd4T-ERb3pvHnFniYaq8a8iNhw@mail.gmail.com>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
 <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
 <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>
 <CAKkiGbvZEtrp-gYApMxPj6eNGd4T-ERb3pvHnFniYaq8a8iNhw@mail.gmail.com>
Message-ID: <0f533804-424c-1fbe-30f5-3abc78788b7d@yahoo.gr>

Thank you for your help!

I tried to fix stackApply according to your instructions.

Now the indices of names are the same and consistent with indices
enumeration (gist for validation and tests:
https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170).

I've attached a patch file here:

https://gist.github.com/kokkytos/ca2c319134677b19900579665267a7a7

> stackapply_mean
class????? : RasterBrick
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
source???? : /tmp/Rtmp9W8UNc/raster/r_tmp_2019-11-27_191205_2929_20324.grd
names????? :? index_5,? index_6,? index_7,? index_1,? index_2,?
index_3,? index_4
min values : 444.6946, 440.2028, 429.6900, 442.7436, 440.0467, 444.9182,
437.1589
max values : 565.8671, 560.1375, 561.7972, 556.2471, 563.8341, 561.7687,
560.4509

> ver_mean
class????? : RasterStack
dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
resolution : 500, 500? (x, y)
extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
crs??????? : NA
names????? :? layer.1,? layer.2,? layer.3,? layer.4,? layer.5,?
layer.6,? layer.7
min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
429.6900
max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
561.7972





On 11/26/19 10:58 PM, Vijay Lulla wrote:
> Hmm...it appears that stackApply is using different conditions which
> might be causing this problem. Below is the snippet of the code which
> I think might be the problem.
>
> ## For canProcessInMemory
> if (rowcalc) {
> ? v <- lapply(uin, function(i) fun(x[, ind == i, drop = FALSE], na.rm
> = na.rm))
> }
> else {
> ? v <- lapply(uin, function(i, ...) apply(x[, ind == i, drop = FALSE],
> 1, fun, na.rm = na.rm))
> }
>
>
> ## If canProcessInMemory is not TRUE
> if (rowcalc) {
> ? v <- lapply(uin, function(i) fun(a[, ind == uin[i], drop = FALSE],
> na.rm = na.rm))
> }
> else {
> ? v <- lapply(uin, function(i, ...) apply(a[, ind == uin[i], drop =
> FALSE], 1, fun, na.rm = na.rm))
> }
>
> I think they should both be same but it appears that they aren't and
> that's what you've discovered.? Maybe you can try fix(stackApply) to
> see if it really is the problem and can tell us what you find.?
> Anyways, good catch...and...sorry for wasting your time.
> Cordially,
> Vijay.
>
> On Tue, Nov 26, 2019 at 2:53 PM Leonidas Liakos
> <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>
>     Thank you!
>     The problem is not with the resulting values but with the index
>     mapping. Values are correct in all three cases.
>
>     As I wrote in a previous post in the thread
>     (https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027821.html)
>     , stackApply behaves inconsistently depending on whether the
>     exported stack will remain in memory or it will be stored, due to
>     its large size, on the hard disk.
>
>     In the first case the indices are identical to my function
>     (ver_mean) and the lubridate::wday indexing system (and are
>     correct) while in the second they are shuffled.
>
>     So, while Sunday has index 1 and while in the first case (when the
>     result is in memory) stackApply returns the correct index, in the
>     second case (when the result is stored on the hard disk) it
>     returns index_4! So how can one be sure if index_1 corresponds to
>     Sunday or another day using stackApply since it sometimes
>     enumerates it with index_1 and sometimes index_4?
>
>
>     Try to run this example (when the resulting stack remains in
>     memory) to see that the indexes are identical (stackApply =
>     ver_median = lubridate::wday)
>     https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206
>
>     Thank you again
>
>     On 11/26/19 9:00 PM, Vijay Lulla wrote:
>>     I'm sorry for the miscommunication.? What I meant to say is that
>>     the output from stackApply and zApply are the same (because
>>     zApply uses stackApply internally) except the names.? The
>>     behavior of stackApply makes sense because AFAIUI R doesn't
>>     automatically reorder vectors/indices that it receives.? Your
>>     observation about inconsistent result with ver_mean is very valid
>>     though!? And, that's what I meant with my comment that using
>>     sapply with the explicit ordering that you want is the best way
>>     to control what raster package will output.? In R the input order
>>     should be maintained (this is the prime difference between SQL
>>     and R) but packages/tools do not always adhere to this...so it's
>>     never clear how the output will be ordered.? Sorry for the confusion.
>>
>>
>>     On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos
>>     <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>>
>>         Why do they seem logical since they do not match?
>>
>>         Check for example index 1 (Sunday). The results are different
>>         for the three processes
>>
>>         > stackapply_mean
>>         class????? : RasterBrick
>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>         resolution : 500, 500? (x, y)
>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>         crs??????? : NA
>>         source???? :
>>         /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
>>         names????? :? index_5,? index_6,? index_7,? index_1,?
>>         index_2,? index_3,? index_4
>>         min values : 440.0467, 444.9182, 437.1589, 444.6946,
>>         440.2028, 429.6900, 442.7436
>>         max values : 563.8341, 561.7687, 560.4509, 565.8671,
>>         560.1375, 561.7972, 556.2471
>>
>>
>>         > ver_mean
>>         class????? : RasterStack
>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>         resolution : 500, 500? (x, y)
>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>         crs??????? : NA
>>         names????? :? layer.1,? layer.2,? layer.3,? layer.4,?
>>         layer.5,? layer.6,? layer.7
>>         min values : 442.7436, 440.0467, 444.9182, 437.1589,
>>         444.6946, 440.2028, 429.6900
>>         max values : 556.2471, 563.8341, 561.7687, 560.4509,
>>         565.8671, 560.1375, 561.7972
>>
>>
>>         > z
>>         class????? : RasterBrick
>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>         resolution : 500, 500? (x, y)
>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>         crs??????? : NA
>>         source???? :
>>         /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
>>         names????? :?????? X1,?????? X2,?????? X3,?????? X4,??????
>>         X5,?????? X6,?????? X7
>>         min values : 440.0467, 444.9182, 437.1589, 444.6946,
>>         440.2028, 429.6900, 442.7436
>>         max values : 563.8341, 561.7687, 560.4509, 565.8671,
>>         560.1375, 561.7972, 556.2471
>>         ?????????? : 1, 2, 3, 4, 5, 6, 7
>>
>>
>>         On 11/26/19 7:03 PM, Vijay Lulla wrote:
>>>         If you read the code/help for `stackApply` and `zApply`
>>>         you'll see that the results that you obtain make sense (at
>>>         least they seem sensible/reasonable to me).? IMO, if you
>>>         want to control the ordering of your layers then just use
>>>         sapply, like how you've used for ver_mean.? IMO, this is the
>>>         only reliable (safe?), and quite a readable, way to
>>>         accomplish what you're trying to do.
>>>         Just my 2 cents.
>>>         -- Vijay.
>>>
>>>         On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via
>>>         R-sig-Geo <r-sig-geo at r-project.org
>>>         <mailto:r-sig-geo at r-project.org>> wrote:
>>>
>>>             I added raster::zApply in my tests to validate the
>>>             results. However, the
>>>             indices of the names of the results are different now.
>>>             Recall that the
>>>             goal is to calculate from a raster stack time series the
>>>             mean per day of
>>>             the week. And that problem I have is that stackApply,
>>>             zApply and
>>>             calc/sapply return different indices in the result
>>>             names. New code is
>>>             available here:
>>>             https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>>>             I'm really curious about missing something.
>>>
>>>
>>>             On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>>>             > Hi Leonidas,
>>>             >
>>>             > both results are in the same order, but the name is
>>>             different.
>>>             > You can rename the first as in the second:
>>>             > names(res) <- names(res2)
>>>             >
>>>             > I provided an example to help you understand the logic.
>>>             >
>>>             > library(raster)
>>>             > beginCluster(2)
>>>             > r <- raster()
>>>             > values(r) <- 1
>>>             > # simple sequential stack from 1 to 6 in all cells
>>>             > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>>             > s
>>>             > res <- clusterR(s, stackApply, args =
>>>             list(indices=c(2,2,3,3,1,1), fun
>>>             > = mean))
>>>             > res
>>>             > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>>             > res2
>>>             > dif <- res - res2
>>>             > # exatly the same order because the difference?is zero
>>>             for all layers
>>>             > dif
>>>             > # rename
>>>             > names(res) <- names(res2)
>>>             >
>>>             > Best regards,
>>>             >
>>>             > Frederico Faleiro
>>>             >
>>>             > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via
>>>             R-sig-Geo
>>>             > <r-sig-geo at r-project.org
>>>             <mailto:r-sig-geo at r-project.org>
>>>             <mailto:r-sig-geo at r-project.org
>>>             <mailto:r-sig-geo at r-project.org>>> wrote:
>>>             >
>>>             >? ? ?I run the example with clusterR:
>>>             >
>>>             >? ? ?no_cores <- parallel::detectCores() -1
>>>             >? ? ?raster::beginCluster(no_cores)
>>>             >? ? ??????? res <- raster::clusterR(inp,
>>>             raster::stackApply, args =
>>>             >? ? ?list(indices=c(2,2,3,3,1,1),fun = mean))
>>>             >? ? ?raster::endCluster()
>>>             >
>>>             >? ? ?And the result is:
>>>             >
>>>             >? ? ?> res
>>>             >? ? ?class?????????? : RasterBrick
>>>             >? ? ?dimensions : 180, 360, 64800, 3?? (nrow, ncol,
>>>             ncell, nlayers)
>>>             >? ? ?resolution : 1, 1?? (x, y)
>>>             >? ? ?extent???????? : -180, 180, -90, 90?? (xmin, xmax,
>>>             ymin, ymax)
>>>             >? ? ?crs?????????????? : +proj=longlat +datum=WGS84
>>>             +ellps=WGS84
>>>             >? ? ?+towgs84=0,0,0
>>>             >? ? ?source???????? : memory
>>>             >? ? ?names?????????? : layer.1, layer.2, layer.3
>>>             >? ? ?min values :???????? 1.5,???????? 3.5,???????? 5.5
>>>             >? ? ?max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>>             >
>>>             >
>>>             >? ? ?layer.1, layer.2, layer.3 (?)
>>>             >
>>>             >? ? ?So what corrensponds to what?
>>>             >
>>>             >
>>>             >? ? ?If I run:
>>>             >
>>>             >? ? ?res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>>             >
>>>             >? ? ?The result is:
>>>             >
>>>             >? ? ?> res2
>>>             >? ? ?class? ? ? : RasterBrick
>>>             >? ? ?dimensions : 180, 360, 64800, 3? (nrow, ncol,
>>>             ncell, nlayers)
>>>             >? ? ?resolution : 1, 1? (x, y)
>>>             >? ? ?extent? ? ?: -180, 180, -90, 90? (xmin, xmax,
>>>             ymin, ymax)
>>>             >? ? ?crs? ? ? ? : +proj=longlat +datum=WGS84
>>>             +ellps=WGS84 +towgs84=0,0,0
>>>             >? ? ?source? ? ?: memory
>>>             >? ? ?names? ? ? : index_2, index_3, index_1
>>>             >? ? ?min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>>             >? ? ?max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>>             >
>>>             >? ? ?There is no consistency with the names of the
>>>             output and obscure
>>>             >? ? ?correspondence with the indices in the case of
>>>             clusterR
>>>             >
>>>             >
>>>             >? ? ?? ? ? ? [[alternative HTML version deleted]]
>>>             >
>>>             >? ? ?_______________________________________________
>>>             >? ? ?R-sig-Geo mailing list
>>>             >? ? ?R-sig-Geo at r-project.org
>>>             <mailto:R-sig-Geo at r-project.org>
>>>             <mailto:R-sig-Geo at r-project.org
>>>             <mailto:R-sig-Geo at r-project.org>>
>>>             >? ? ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>             >
>>>
>>>             ? ? ? ? [[alternative HTML version deleted]]
>>>
>>>             _______________________________________________
>>>             R-sig-Geo mailing list
>>>             R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>>             https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From v|j@y|u||@ @end|ng |rom gm@||@com  Wed Nov 27 23:22:19 2019
From: v|j@y|u||@ @end|ng |rom gm@||@com (Vijay Lulla)
Date: Wed, 27 Nov 2019 17:22:19 -0500
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <0f533804-424c-1fbe-30f5-3abc78788b7d@yahoo.gr>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
 <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
 <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>
 <CAKkiGbvZEtrp-gYApMxPj6eNGd4T-ERb3pvHnFniYaq8a8iNhw@mail.gmail.com>
 <0f533804-424c-1fbe-30f5-3abc78788b7d@yahoo.gr>
Message-ID: <CAKkiGbtL1MAWTMerCOvT95-UFKNoL6QThpCPrjP=HAsk1FwMgA@mail.gmail.com>

Very nice!

And, just fyi: if you wish to get stackapply_mean to have layers in same
order as ver_mean then you can use  stackapply_mean <-
stack(stackapply_mean, layers=order(names(stackapply_mean)))

On Wed, Nov 27, 2019 at 12:43 PM Leonidas Liakos <leonidas_liakos at yahoo.gr>
wrote:

> Thank you for your help!
>
> I tried to fix stackApply according to your instructions.
>
> Now the indices of names are the same and consistent with indices
> enumeration (gist for validation and tests:
> https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170).
>
> I've attached a patch file here:
>
> https://gist.github.com/kokkytos/ca2c319134677b19900579665267a7a7
> > stackapply_mean
> class      : RasterBrick
> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
> resolution : 500, 500  (x, y)
> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
> crs        : NA
> source     : /tmp/Rtmp9W8UNc/raster/r_tmp_2019-11-27_191205_2929_20324.grd
> names      :  index_5,  index_6,  index_7,  index_1,  index_2,  index_3,
> index_4
> min values : 444.6946, 440.2028, 429.6900, 442.7436, 440.0467, 444.9182,
> 437.1589
> max values : 565.8671, 560.1375, 561.7972, 556.2471, 563.8341, 561.7687,
> 560.4509
>
> > ver_mean
> class      : RasterStack
> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
> resolution : 500, 500  (x, y)
> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
> crs        : NA
> names      :  layer.1,  layer.2,  layer.3,  layer.4,  layer.5,  layer.6,
> layer.7
> min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
> 429.6900
> max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
> 561.7972
>
>
>
>
>
> On 11/26/19 10:58 PM, Vijay Lulla wrote:
>
> Hmm...it appears that stackApply is using different conditions which might
> be causing this problem. Below is the snippet of the code which I think
> might be the problem.
>
> ## For canProcessInMemory
> if (rowcalc) {
>   v <- lapply(uin, function(i) fun(x[, ind == i, drop = FALSE], na.rm =
> na.rm))
> }
> else {
>   v <- lapply(uin, function(i, ...) apply(x[, ind == i, drop = FALSE], 1,
> fun, na.rm = na.rm))
> }
>
>
> ## If canProcessInMemory is not TRUE
> if (rowcalc) {
>   v <- lapply(uin, function(i) fun(a[, ind == uin[i], drop = FALSE], na.rm
> = na.rm))
> }
> else {
>   v <- lapply(uin, function(i, ...) apply(a[, ind == uin[i], drop =
> FALSE], 1, fun, na.rm = na.rm))
> }
>
> I think they should both be same but it appears that they aren't and
> that's what you've discovered.  Maybe you can try fix(stackApply) to see if
> it really is the problem and can tell us what you find.  Anyways, good
> catch...and...sorry for wasting your time.
> Cordially,
> Vijay.
>
> On Tue, Nov 26, 2019 at 2:53 PM Leonidas Liakos <leonidas_liakos at yahoo.gr>
> wrote:
>
>> Thank you!
>> The problem is not with the resulting values but with the index mapping.
>> Values are correct in all three cases.
>>
>> As I wrote in a previous post in the thread (
>> https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027821.html) ,
>> stackApply behaves inconsistently depending on whether the exported stack
>> will remain in memory or it will be stored, due to its large size, on the
>> hard disk.
>>
>> In the first case the indices are identical to my function (ver_mean) and
>> the lubridate::wday indexing system (and are correct) while in the second
>> they are shuffled.
>>
>> So, while Sunday has index 1 and while in the first case (when the result
>> is in memory) stackApply returns the correct index, in the second case
>> (when the result is stored on the hard disk) it returns index_4! So how can
>> one be sure if index_1 corresponds to Sunday or another day using
>> stackApply since it sometimes enumerates it with index_1 and sometimes
>> index_4?
>>
>>
>> Try to run this example (when the resulting stack remains in memory) to
>> see that the indexes are identical (stackApply = ver_median =
>> lubridate::wday)
>> https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206
>>
>> Thank you again
>> On 11/26/19 9:00 PM, Vijay Lulla wrote:
>>
>> I'm sorry for the miscommunication.  What I meant to say is that the
>> output from stackApply and zApply are the same (because zApply uses
>> stackApply internally) except the names.  The behavior of stackApply makes
>> sense because AFAIUI R doesn't automatically reorder vectors/indices that
>> it receives.  Your observation about inconsistent result with ver_mean is
>> very valid though!  And, that's what I meant with my comment that using
>> sapply with the explicit ordering that you want is the best way to control
>> what raster package will output.  In R the input order should be maintained
>> (this is the prime difference between SQL and R) but packages/tools do not
>> always adhere to this...so it's never clear how the output will be
>> ordered.  Sorry for the confusion.
>>
>>
>> On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos <
>> leonidas_liakos at yahoo.gr> wrote:
>>
>>> Why do they seem logical since they do not match?
>>>
>>> Check for example index 1 (Sunday). The results are different for the
>>> three processes
>>>
>>> > stackapply_mean
>>> class      : RasterBrick
>>> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
>>> resolution : 500, 500  (x, y)
>>> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
>>> crs        : NA
>>> source     :
>>> /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
>>> names      :  index_5,  index_6,  index_7,  index_1,  index_2,
>>> index_3,  index_4
>>> min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
>>> 442.7436
>>> max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
>>> 556.2471
>>>
>>>
>>> > ver_mean
>>> class      : RasterStack
>>> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
>>> resolution : 500, 500  (x, y)
>>> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
>>> crs        : NA
>>> names      :  layer.1,  layer.2,  layer.3,  layer.4,  layer.5,
>>> layer.6,  layer.7
>>> min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
>>> 429.6900
>>> max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
>>> 561.7972
>>>
>>>
>>> > z
>>> class      : RasterBrick
>>> dimensions : 300, 300, 90000, 7  (nrow, ncol, ncell, nlayers)
>>> resolution : 500, 500  (x, y)
>>> extent     : 0, 150000, 0, 150000  (xmin, xmax, ymin, ymax)
>>> crs        : NA
>>> source     :
>>> /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
>>> names      :       X1,       X2,       X3,       X4,       X5,
>>> X6,       X7
>>> min values : 440.0467, 444.9182, 437.1589, 444.6946, 440.2028, 429.6900,
>>> 442.7436
>>> max values : 563.8341, 561.7687, 560.4509, 565.8671, 560.1375, 561.7972,
>>> 556.2471
>>>            : 1, 2, 3, 4, 5, 6, 7
>>>
>>>
>>> On 11/26/19 7:03 PM, Vijay Lulla wrote:
>>>
>>> If you read the code/help for `stackApply` and `zApply` you'll see that
>>> the results that you obtain make sense (at least they seem
>>> sensible/reasonable to me).  IMO, if you want to control the ordering of
>>> your layers then just use sapply, like how you've used for ver_mean.  IMO,
>>> this is the only reliable (safe?), and quite a readable, way to accomplish
>>> what you're trying to do.
>>> Just my 2 cents.
>>> -- Vijay.
>>>
>>> On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via R-sig-Geo <
>>> r-sig-geo at r-project.org> wrote:
>>>
>>>> I added raster::zApply in my tests to validate the results. However, the
>>>> indices of the names of the results are different now. Recall that the
>>>> goal is to calculate from a raster stack time series the mean per day of
>>>> the week. And that problem I have is that stackApply, zApply and
>>>> calc/sapply return different indices in the result names. New code is
>>>> available here:
>>>> https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>>>> I'm really curious about missing something.
>>>>
>>>>
>>>> On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>>>> > Hi Leonidas,
>>>> >
>>>> > both results are in the same order, but the name is different.
>>>> > You can rename the first as in the second:
>>>> > names(res) <- names(res2)
>>>> >
>>>> > I provided an example to help you understand the logic.
>>>> >
>>>> > library(raster)
>>>> > beginCluster(2)
>>>> > r <- raster()
>>>> > values(r) <- 1
>>>> > # simple sequential stack from 1 to 6 in all cells
>>>> > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>>> > s
>>>> > res <- clusterR(s, stackApply, args = list(indices=c(2,2,3,3,1,1), fun
>>>> > = mean))
>>>> > res
>>>> > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>>> > res2
>>>> > dif <- res - res2
>>>> > # exatly the same order because the difference is zero for all layers
>>>> > dif
>>>> > # rename
>>>> > names(res) <- names(res2)
>>>> >
>>>> > Best regards,
>>>> >
>>>> > Frederico Faleiro
>>>> >
>>>> > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via R-sig-Geo
>>>> > <r-sig-geo at r-project.org <mailto:r-sig-geo at r-project.org>> wrote:
>>>> >
>>>> >     I run the example with clusterR:
>>>> >
>>>> >     no_cores <- parallel::detectCores() -1
>>>> >     raster::beginCluster(no_cores)
>>>> >     ?????? res <- raster::clusterR(inp, raster::stackApply, args =
>>>> >     list(indices=c(2,2,3,3,1,1),fun = mean))
>>>> >     raster::endCluster()
>>>> >
>>>> >     And the result is:
>>>> >
>>>> >     > res
>>>> >     class?????????? : RasterBrick
>>>> >     dimensions : 180, 360, 64800, 3?? (nrow, ncol, ncell, nlayers)
>>>> >     resolution : 1, 1?? (x, y)
>>>> >     extent???????? : -180, 180, -90, 90?? (xmin, xmax, ymin, ymax)
>>>> >     crs?????????????? : +proj=longlat +datum=WGS84 +ellps=WGS84
>>>> >     +towgs84=0,0,0
>>>> >     source???????? : memory
>>>> >     names?????????? : layer.1, layer.2, layer.3
>>>> >     min values :???????? 1.5,???????? 3.5,???????? 5.5
>>>> >     max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>>> >
>>>> >
>>>> >     layer.1, layer.2, layer.3 (?)
>>>> >
>>>> >     So what corrensponds to what?
>>>> >
>>>> >
>>>> >     If I run:
>>>> >
>>>> >     res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>>> >
>>>> >     The result is:
>>>> >
>>>> >     > res2
>>>> >     class      : RasterBrick
>>>> >     dimensions : 180, 360, 64800, 3  (nrow, ncol, ncell, nlayers)
>>>> >     resolution : 1, 1  (x, y)
>>>> >     extent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>> >     crs        : +proj=longlat +datum=WGS84 +ellps=WGS84
>>>> +towgs84=0,0,0
>>>> >     source     : memory
>>>> >     names      : index_2, index_3, index_1
>>>> >     min values :     1.5,     3.5,     5.5
>>>> >     max values :     1.5,     3.5,     5.5
>>>> >
>>>> >     There is no consistency with the names of the output and obscure
>>>> >     correspondence with the indices in the case of clusterR
>>>> >
>>>> >
>>>> >             [[alternative HTML version deleted]]
>>>> >
>>>> >     _______________________________________________
>>>> >     R-sig-Geo mailing list
>>>> >     R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>>> >     https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>> >
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>
>>>
>>>
>>
>

-- 
Vijay Lulla

Assistant Professor,
Dept. of Geography, IUPUI
425 University Blvd, CA-207C.
Indianapolis, IN-46202
vlulla at iupui.edu
ORCID: https://orcid.org/0000-0002-0823-2522
Online: http://vijaylulla.com | https://github.com/vlulla

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Nov 28 12:25:20 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 28 Nov 2019 12:25:20 +0100
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <0f533804-424c-1fbe-30f5-3abc78788b7d@yahoo.gr>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
 <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
 <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>
 <CAKkiGbvZEtrp-gYApMxPj6eNGd4T-ERb3pvHnFniYaq8a8iNhw@mail.gmail.com>
 <0f533804-424c-1fbe-30f5-3abc78788b7d@yahoo.gr>
Message-ID: <alpine.LFD.2.21.1911281220530.607873@reclus.nhh.no>

On Wed, 27 Nov 2019, Leonidas Liakos via R-sig-Geo wrote:

> Thank you for your help!
>
> I tried to fix stackApply according to your instructions.
>
> Now the indices of names are the same and consistent with indices
> enumeration (gist for validation and tests:
> https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170).
>
> I've attached a patch file here:
>
> https://gist.github.com/kokkytos/ca2c319134677b19900579665267a7a7

Thanks very much for contributing!

Please consider raising an issue on https://github.com/rspatial/raster 
pointing to this thread and your patch. I had expected response from 
raster developers here, but they may well be on field work, so raising an 
issue on the development site should get their attention when there is 
enough time for them to look. You might even fork raster, apply your patch 
and file a PR in addition to the issue. In that case, a short test would 
be helpful, and maybe edits to the documentation.

Roger


>
>> stackapply_mean
> class????? : RasterBrick
> dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
> resolution : 500, 500? (x, y)
> extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
> crs??????? : NA
> source???? : /tmp/Rtmp9W8UNc/raster/r_tmp_2019-11-27_191205_2929_20324.grd
> names????? :? index_5,? index_6,? index_7,? index_1,? index_2,?
> index_3,? index_4
> min values : 444.6946, 440.2028, 429.6900, 442.7436, 440.0467, 444.9182,
> 437.1589
> max values : 565.8671, 560.1375, 561.7972, 556.2471, 563.8341, 561.7687,
> 560.4509
>
>> ver_mean
> class????? : RasterStack
> dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
> resolution : 500, 500? (x, y)
> extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
> crs??????? : NA
> names????? :? layer.1,? layer.2,? layer.3,? layer.4,? layer.5,?
> layer.6,? layer.7
> min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
> 429.6900
> max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
> 561.7972
>
>
>
>
>
> On 11/26/19 10:58 PM, Vijay Lulla wrote:
>> Hmm...it appears that stackApply is using different conditions which
>> might be causing this problem. Below is the snippet of the code which
>> I think might be the problem.
>>
>> ## For canProcessInMemory
>> if (rowcalc) {
>> ? v <- lapply(uin, function(i) fun(x[, ind == i, drop = FALSE], na.rm
>> = na.rm))
>> }
>> else {
>> ? v <- lapply(uin, function(i, ...) apply(x[, ind == i, drop = FALSE],
>> 1, fun, na.rm = na.rm))
>> }
>>
>>
>> ## If canProcessInMemory is not TRUE
>> if (rowcalc) {
>> ? v <- lapply(uin, function(i) fun(a[, ind == uin[i], drop = FALSE],
>> na.rm = na.rm))
>> }
>> else {
>> ? v <- lapply(uin, function(i, ...) apply(a[, ind == uin[i], drop =
>> FALSE], 1, fun, na.rm = na.rm))
>> }
>>
>> I think they should both be same but it appears that they aren't and
>> that's what you've discovered.? Maybe you can try fix(stackApply) to
>> see if it really is the problem and can tell us what you find.?
>> Anyways, good catch...and...sorry for wasting your time.
>> Cordially,
>> Vijay.
>>
>> On Tue, Nov 26, 2019 at 2:53 PM Leonidas Liakos
>> <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>>
>>     Thank you!
>>     The problem is not with the resulting values but with the index
>>     mapping. Values are correct in all three cases.
>>
>>     As I wrote in a previous post in the thread
>>     (https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027821.html)
>>     , stackApply behaves inconsistently depending on whether the
>>     exported stack will remain in memory or it will be stored, due to
>>     its large size, on the hard disk.
>>
>>     In the first case the indices are identical to my function
>>     (ver_mean) and the lubridate::wday indexing system (and are
>>     correct) while in the second they are shuffled.
>>
>>     So, while Sunday has index 1 and while in the first case (when the
>>     result is in memory) stackApply returns the correct index, in the
>>     second case (when the result is stored on the hard disk) it
>>     returns index_4! So how can one be sure if index_1 corresponds to
>>     Sunday or another day using stackApply since it sometimes
>>     enumerates it with index_1 and sometimes index_4?
>>
>>
>>     Try to run this example (when the resulting stack remains in
>>     memory) to see that the indexes are identical (stackApply =
>>     ver_median = lubridate::wday)
>>     https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206
>>
>>     Thank you again
>>
>>     On 11/26/19 9:00 PM, Vijay Lulla wrote:
>>>     I'm sorry for the miscommunication.? What I meant to say is that
>>>     the output from stackApply and zApply are the same (because
>>>     zApply uses stackApply internally) except the names.? The
>>>     behavior of stackApply makes sense because AFAIUI R doesn't
>>>     automatically reorder vectors/indices that it receives.? Your
>>>     observation about inconsistent result with ver_mean is very valid
>>>     though!? And, that's what I meant with my comment that using
>>>     sapply with the explicit ordering that you want is the best way
>>>     to control what raster package will output.? In R the input order
>>>     should be maintained (this is the prime difference between SQL
>>>     and R) but packages/tools do not always adhere to this...so it's
>>>     never clear how the output will be ordered.? Sorry for the confusion.
>>>
>>>
>>>     On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos
>>>     <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>>>
>>>         Why do they seem logical since they do not match?
>>>
>>>         Check for example index 1 (Sunday). The results are different
>>>         for the three processes
>>>
>>>        > stackapply_mean
>>>         class????? : RasterBrick
>>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>>         resolution : 500, 500? (x, y)
>>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>>         crs??????? : NA
>>>         source???? :
>>>         /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
>>>         names????? :? index_5,? index_6,? index_7,? index_1,?
>>>         index_2,? index_3,? index_4
>>>         min values : 440.0467, 444.9182, 437.1589, 444.6946,
>>>         440.2028, 429.6900, 442.7436
>>>         max values : 563.8341, 561.7687, 560.4509, 565.8671,
>>>         560.1375, 561.7972, 556.2471
>>>
>>>
>>>        > ver_mean
>>>         class????? : RasterStack
>>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>>         resolution : 500, 500? (x, y)
>>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>>         crs??????? : NA
>>>         names????? :? layer.1,? layer.2,? layer.3,? layer.4,?
>>>         layer.5,? layer.6,? layer.7
>>>         min values : 442.7436, 440.0467, 444.9182, 437.1589,
>>>         444.6946, 440.2028, 429.6900
>>>         max values : 556.2471, 563.8341, 561.7687, 560.4509,
>>>         565.8671, 560.1375, 561.7972
>>>
>>>
>>>        > z
>>>         class????? : RasterBrick
>>>         dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>>         resolution : 500, 500? (x, y)
>>>         extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>>         crs??????? : NA
>>>         source???? :
>>>         /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
>>>         names????? :?????? X1,?????? X2,?????? X3,?????? X4,??????
>>>         X5,?????? X6,?????? X7
>>>         min values : 440.0467, 444.9182, 437.1589, 444.6946,
>>>         440.2028, 429.6900, 442.7436
>>>         max values : 563.8341, 561.7687, 560.4509, 565.8671,
>>>         560.1375, 561.7972, 556.2471
>>>         ?????????? : 1, 2, 3, 4, 5, 6, 7
>>>
>>>
>>>         On 11/26/19 7:03 PM, Vijay Lulla wrote:
>>>>         If you read the code/help for `stackApply` and `zApply`
>>>>         you'll see that the results that you obtain make sense (at
>>>>         least they seem sensible/reasonable to me).? IMO, if you
>>>>         want to control the ordering of your layers then just use
>>>>         sapply, like how you've used for ver_mean.? IMO, this is the
>>>>         only reliable (safe?), and quite a readable, way to
>>>>         accomplish what you're trying to do.
>>>>         Just my 2 cents.
>>>>         -- Vijay.
>>>>
>>>>         On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via
>>>>         R-sig-Geo <r-sig-geo at r-project.org
>>>>         <mailto:r-sig-geo at r-project.org>> wrote:
>>>>
>>>>             I added raster::zApply in my tests to validate the
>>>>             results. However, the
>>>>             indices of the names of the results are different now.
>>>>             Recall that the
>>>>             goal is to calculate from a raster stack time series the
>>>>             mean per day of
>>>>             the week. And that problem I have is that stackApply,
>>>>             zApply and
>>>>             calc/sapply return different indices in the result
>>>>             names. New code is
>>>>             available here:
>>>>             https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>>>>             I'm really curious about missing something.
>>>>
>>>>
>>>>             On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>>>>            > Hi Leonidas,
>>>>            >
>>>>            > both results are in the same order, but the name is
>>>>             different.
>>>>            > You can rename the first as in the second:
>>>>            > names(res) <- names(res2)
>>>>            >
>>>>            > I provided an example to help you understand the logic.
>>>>            >
>>>>            > library(raster)
>>>>            > beginCluster(2)
>>>>            > r <- raster()
>>>>            > values(r) <- 1
>>>>            > # simple sequential stack from 1 to 6 in all cells
>>>>            > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>>>            > s
>>>>            > res <- clusterR(s, stackApply, args =
>>>>             list(indices=c(2,2,3,3,1,1), fun
>>>>            > = mean))
>>>>            > res
>>>>            > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>>>            > res2
>>>>            > dif <- res - res2
>>>>            > # exatly the same order because the difference?is zero
>>>>             for all layers
>>>>            > dif
>>>>            > # rename
>>>>            > names(res) <- names(res2)
>>>>            >
>>>>            > Best regards,
>>>>            >
>>>>            > Frederico Faleiro
>>>>            >
>>>>            > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via
>>>>             R-sig-Geo
>>>>            > <r-sig-geo at r-project.org
>>>>             <mailto:r-sig-geo at r-project.org>
>>>>             <mailto:r-sig-geo at r-project.org
>>>>             <mailto:r-sig-geo at r-project.org>>> wrote:
>>>>            >
>>>>            >? ? ?I run the example with clusterR:
>>>>            >
>>>>            >? ? ?no_cores <- parallel::detectCores() -1
>>>>            >? ? ?raster::beginCluster(no_cores)
>>>>            >? ? ??????? res <- raster::clusterR(inp,
>>>>             raster::stackApply, args =
>>>>            >? ? ?list(indices=c(2,2,3,3,1,1),fun = mean))
>>>>            >? ? ?raster::endCluster()
>>>>            >
>>>>            >? ? ?And the result is:
>>>>            >
>>>>            >? ? ?> res
>>>>            >? ? ?class?????????? : RasterBrick
>>>>            >? ? ?dimensions : 180, 360, 64800, 3?? (nrow, ncol,
>>>>             ncell, nlayers)
>>>>            >? ? ?resolution : 1, 1?? (x, y)
>>>>            >? ? ?extent???????? : -180, 180, -90, 90?? (xmin, xmax,
>>>>             ymin, ymax)
>>>>            >? ? ?crs?????????????? : +proj=longlat +datum=WGS84
>>>>             +ellps=WGS84
>>>>            >? ? ?+towgs84=0,0,0
>>>>            >? ? ?source???????? : memory
>>>>            >? ? ?names?????????? : layer.1, layer.2, layer.3
>>>>            >? ? ?min values :???????? 1.5,???????? 3.5,???????? 5.5
>>>>            >? ? ?max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>>>            >
>>>>            >
>>>>            >? ? ?layer.1, layer.2, layer.3 (?)
>>>>            >
>>>>            >? ? ?So what corrensponds to what?
>>>>            >
>>>>            >
>>>>            >? ? ?If I run:
>>>>            >
>>>>            >? ? ?res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>>>            >
>>>>            >? ? ?The result is:
>>>>            >
>>>>            >? ? ?> res2
>>>>            >? ? ?class? ? ? : RasterBrick
>>>>            >? ? ?dimensions : 180, 360, 64800, 3? (nrow, ncol,
>>>>             ncell, nlayers)
>>>>            >? ? ?resolution : 1, 1? (x, y)
>>>>            >? ? ?extent? ? ?: -180, 180, -90, 90? (xmin, xmax,
>>>>             ymin, ymax)
>>>>            >? ? ?crs? ? ? ? : +proj=longlat +datum=WGS84
>>>>             +ellps=WGS84 +towgs84=0,0,0
>>>>            >? ? ?source? ? ?: memory
>>>>            >? ? ?names? ? ? : index_2, index_3, index_1
>>>>            >? ? ?min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>>>            >? ? ?max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>>>            >
>>>>            >? ? ?There is no consistency with the names of the
>>>>             output and obscure
>>>>            >? ? ?correspondence with the indices in the case of
>>>>             clusterR
>>>>            >
>>>>            >
>>>>            >? ? ?? ? ? ? [[alternative HTML version deleted]]
>>>>            >
>>>>            >? ? ?_______________________________________________
>>>>            >? ? ?R-sig-Geo mailing list
>>>>            >? ? ?R-sig-Geo at r-project.org
>>>>             <mailto:R-sig-Geo at r-project.org>
>>>>             <mailto:R-sig-Geo at r-project.org
>>>>             <mailto:R-sig-Geo at r-project.org>>
>>>>            >? ? ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>            >
>>>>
>>>>             ? ? ? ? [[alternative HTML version deleted]]
>>>>
>>>>             _______________________________________________
>>>>             R-sig-Geo mailing list
>>>>             R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>>>             https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>
>>>>
>>>
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger@B|v@nd @end|ng |rom nhh@no  Thu Nov 28 13:32:45 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 28 Nov 2019 13:32:45 +0100
Subject: [R-sig-Geo] To institutional users: heads up! PROJ 6+ is impacting
 your work!
Message-ID: <alpine.LFD.2.21.1911281303390.607873@reclus.nhh.no>

I have received scant input (no input) on my warnings and requests for 
input on:

https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027801.html

Please review that carefully, and in addition please take time now, that 
is best within one week (things are moving very fast), to review and 
comment on the PROJ list on the draft RFC 4:

https://github.com/rouault/PROJ/blob/rfc4_remote_and_geotiff_grid/docs/source/community/rfc/rfc-4.rst

(threads:)
https://lists.osgeo.org/pipermail/proj/2019-November/009021.html
https://lists.osgeo.org/pipermail/proj/2019-November/009024.html
https://lists.osgeo.org/pipermail/proj/2019-November/009030.html

This concerns conditionally permitting PROJ (so here rgdal, sf and lwgeom) 
to download granular subsets of transformation grids behind the scenes to 
a user-writable directory, and then use these in coordinate 
transformation. The download would be from "the cloud", a content 
distribution network.

In practice, this would mean that platforms connected to the internet, and 
after approving access to the CDN, would start "grabbing" grid portions 
from the CDN and using these for transformation, rather than say using 
institutionally sanctioned grids. The CDN grids are expected to be best of 
breed going forward, protecting us forever from horizontal and vertical 
shifts and giving full geodetic accuracy wherever applied (and where 
national mapping agencies agree to publish their best grids).

It would be best if several list members at institutions with R-spatial 
workflow components or with relevant experience discuss this rapidly 
off-list (contact each other on list to volunteer if need be), respond to 
the PROJ list about the RFC draft before it is completed, and report back 
in this thread. Nobody will be able to help later if you sit on your hands 
now, now is when your needs can be protected for the next decade. For an 
individual academic like me to respond on your behalf would be most 
inadequate, as I can only speak from experience in individual research and 
teaching settings.

Please take action urgently, this is an opportunity to contribute to 
shaping critical shared infrastructure with ramifications way beyond 
R-spatial. Please also offer me the comfort (offlist) of knowing that you 
are going to respond to the PROJ list request for comments on the RFC 
draft, it feels very lonely here trying to guess what your needs are.

Roger

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |bu@ett @end|ng |rom gm@||@com  Fri Nov 29 18:30:02 2019
From: |bu@ett @end|ng |rom gm@||@com (Lorenzo Busetto)
Date: Fri, 29 Nov 2019 18:30:02 +0100
Subject: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
In-Reply-To: <alpine.LFD.2.21.1911231259370.538818@reclus.nhh.no>
References: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>
 <alpine.LFD.2.21.1911132021070.105168@reclus.nhh.no>
 <alpine.LFD.2.21.1911152151300.143646@reclus.nhh.no>
 <alpine.LFD.2.21.1911231259370.538818@reclus.nhh.no>
Message-ID: <CAM1G4SRgWX2gBinHfq55CMqUERVzp-OA5O8ZncarDhFOsO8PbA@mail.gmail.com>

Dear Roger,

    I'd be very interested in participating to the stream talk. Is it
confirmed?

Concerning testing the connection, I'd be happy to do it but I do not know
how much I will be available online in the coming days. You can drop me a
mail/DM on twitter, and if I am available I'll gladly help.

Concerning technology, Zoom usually works quite well (https://zoom.us/
<https://zoom.us/ent?zcid=3172>)

   regards,

Lorenzo

On Sat, 23 Nov 2019 at 13:04, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> A description of the status now with regard to a prototype resolution is
> online at:
>
> https://rsbivand.github.io/ECS530_h19/ECS530_III.html
>
> I'm planning to stream a talk about this at 09:15-11:00 CET on Tuesday 3
> December. I need a volunteer to test the streaming link in advance during
> next week. I'm unsure which technology to use for remote participants to
> provide feedback.
>
> Contributions/comments welcome!
>
> Roger
>
>
> On Fri, 15 Nov 2019, Roger Bivand wrote:
>
> > The development version of rgdal on R-Forge is now at rev 894, and is
> now
> > ready for trying out with PROJ6/GDAL3 workflows, and workflows that may
> > migrate within 6 months to modern CRS representations. The motivating
> RFC is
> > also updated to cover coordinate operations, the use of prepared
> > (pre-searched) coordinate operations, and should be read carefully by
> anyone
> > using rgdal::spTransform(). Note further that rgdal::project() will not
> be
> > adapted for PROJ6, and is effectively deprecated.
> >
> > I'll be running reverse dependency checks, and may be bugging package
> > maintainers. I would really prefer that mainainers of packages using
> > spTransform() checked themselves and joined this thread or the
> associated
> > twitter thread:
> https://twitter.com/RogerBivand/status/1194586193108914177
> >
> > Be ready for modern PROJ and GDAL, they are already being deployed
> across
> > open source geospatial software, like GRASS, QGIS, pyproj, spatialite
> etc.
> >
> > Waiting, hopefully not in vain, for contributions.
> >
> > Roger
> >
> > On Wed, 13 Nov 2019, Roger Bivand wrote:
> >
> >>  And this link explains the CDN proposal for grid distribution:
> >>
> >>  https://www.spatialys.com/en/crowdfunding/
> >>
> >>  Roger
> >>
> >>  On Wed, 13 Nov 2019, Roger Bivand wrote:
> >>
> >>>   Because PROJ >= 6 and GDAL >= 3 change the way that PROJ strings
> >>>   (representations of coordinate reference systems) are handled, steps
> are
> >>>   being taken to find ways to adapt sp/rgdal workflows. A current
> proposal
> >>>   is to store the WKT2_2018 string as a comment to CRS objects as
> defined
> >>>   in
> >>>   the sp package.
> >>>
> >>>   A draft development-in-progress version of rgdal is available at
> >>>   https://r-forge.r-project.org/R/?group_id=884, and for sp at
> >>>   https://github.com/rsbivand/sp (this version of sp requires rgdal >=
> >>>   1.5-1). This adds the WKT comments to CRS objects on reading vector
> and
> >>>   raster data sources, and uses WKT comments if found when writing
> vector
> >>>   and raster objects (or at least does as far as I've checked, possibly
> >>>   fragile).
> >>>
> >>>   An RFC with tersely worked cases for using CRS object comments to
> carry
> >>>   WKT strings but maintaining full backward compatibility is online at
> >>>   http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html.
> >>>
> >>>   If you have other ideas or concerns about trying to use this
> mechanism
> >>>   for
> >>>   sp CRS objects, please contribute at your earliest convenience.
> >>>
> >>>   http://rgdal.r-forge.r-project.org/reference/list_coordOps.html
> shows
> >>>   the
> >>>   beginning of the next step, to query transformation operations to
> find
> >>>   viable coordinate operation pipelines.
> >>>
> >>>   I'm assuming that the previous behaviour (transform without
> considering
> >>>   accuracy with whatever is to hand) is not viable going forward, and
> that
> >>>   we will need two steps: list coordinate operations between source and
> >>>   target CRS (using the WKT comments as better specifications than the
> >>>   PROJ
> >>>   strings), possibly intervene manually to install missing grids, then
> >>>   undertake the coordinate operation.
> >>>
> >>>   The fallback may be simply to choose the least inaccurate available
> >>>   coordinate operation, but this should be a fallback. This means that
> all
> >>>   uses of spTransform() will require intervention.
> >>>
> >>>   Is this OK (it is tiresome but modernises workflows once), or is it
> not
> >>>   OK
> >>>   (no user intervention is crucial)?
> >>>
> >>>   These behaviours may be set in an option, so that package maintainers
> >>>   and
> >>>   users may delay modernisation, but all are undoubtedly served by
> rapid
> >>>   adaptation (GRASS 7.8.1 released yesterday, libspatialite, pyproj,
> QGIS
> >>>   development versions all state that they list candidate coordinate
> >>>   operations).
> >>>
> >>>   We cannot ship all the grids, they are very bulky, and probably
> nobody
> >>>   needs sub-metre accuracy world-wide. Work in PROJ is starting to
> create
> >>>   a
> >>>   content delivery network for trusted download and mechanisms for
> >>>   registering downloaded grids on user platforms. We would for example
> not
> >>>   want Windows users of rgdal and sf to have to download the same grid
> >>>   twice.
> >>>
> >>>   Comments welcome here and at
> >>>   https://github.com/r-spatial/discuss/issues/28 or
> >>>   https://github.com/r-spatial/sf/issues/1187
> >>>
> >>>   Roger
> >>>
> >>>
> >>
> >>
> >
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Fri Nov 29 18:56:26 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Fri, 29 Nov 2019 17:56:26 +0000
Subject: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
In-Reply-To: <CAM1G4SRgWX2gBinHfq55CMqUERVzp-OA5O8ZncarDhFOsO8PbA@mail.gmail.com>
References: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>
 <alpine.LFD.2.21.1911132021070.105168@reclus.nhh.no>
 <alpine.LFD.2.21.1911152151300.143646@reclus.nhh.no>
 <alpine.LFD.2.21.1911231259370.538818@reclus.nhh.no>,
 <CAM1G4SRgWX2gBinHfq55CMqUERVzp-OA5O8ZncarDhFOsO8PbA@mail.gmail.com>
Message-ID: <ddae23d15f5748aa897708a059dd195a@nhh.no>

Thanks, we use the local, institutional interface which provides the quality we delivered at the 2014 Geostat summer school. The link will be posted on https://rsbivand.github.io/ECS530_h19/streaming_ecs530_h19.html when available (delayed because I've been off work). We'll just have to use Monday's classes to check that things are working, feedback info in the link above.

Roger

--
Roger Bivand
Norwegian School of Economics
Helleveien 30, 5045 Bergen, Norway
Roger.Bivand at nhh.no


________________________________________
Fra: Lorenzo Busetto <lbusett at gmail.com>
Sendt: fredag 29. november 2019 18.30
Til: Roger Bivand
Kopi: r-sig-geo at r-project.org
Emne: Re: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3

Dear Roger,

    I'd be very interested in participating to the stream talk. Is it confirmed?

Concerning testing the connection, I'd be happy to do it but I do not know how much I will be available online in the coming days. You can drop me a mail/DM on twitter, and if I am available I'll gladly help.

Concerning technology, Zoom usually works quite well (https://zoom.us/<https://zoom.us/ent?zcid=3172>)

   regards,

Lorenzo

On Sat, 23 Nov 2019 at 13:04, Roger Bivand <Roger.Bivand at nhh.no<mailto:Roger.Bivand at nhh.no>> wrote:
A description of the status now with regard to a prototype resolution is
online at:

https://rsbivand.github.io/ECS530_h19/ECS530_III.html

I'm planning to stream a talk about this at 09:15-11:00 CET on Tuesday 3
December. I need a volunteer to test the streaming link in advance during
next week. I'm unsure which technology to use for remote participants to
provide feedback.

Contributions/comments welcome!

Roger


On Fri, 15 Nov 2019, Roger Bivand wrote:

> The development version of rgdal on R-Forge is now at rev 894, and is now
> ready for trying out with PROJ6/GDAL3 workflows, and workflows that may
> migrate within 6 months to modern CRS representations. The motivating RFC is
> also updated to cover coordinate operations, the use of prepared
> (pre-searched) coordinate operations, and should be read carefully by anyone
> using rgdal::spTransform(). Note further that rgdal::project() will not be
> adapted for PROJ6, and is effectively deprecated.
>
> I'll be running reverse dependency checks, and may be bugging package
> maintainers. I would really prefer that mainainers of packages using
> spTransform() checked themselves and joined this thread or the associated
> twitter thread: https://twitter.com/RogerBivand/status/1194586193108914177
>
> Be ready for modern PROJ and GDAL, they are already being deployed across
> open source geospatial software, like GRASS, QGIS, pyproj, spatialite etc.
>
> Waiting, hopefully not in vain, for contributions.
>
> Roger
>
> On Wed, 13 Nov 2019, Roger Bivand wrote:
>
>>  And this link explains the CDN proposal for grid distribution:
>>
>>  https://www.spatialys.com/en/crowdfunding/
>>
>>  Roger
>>
>>  On Wed, 13 Nov 2019, Roger Bivand wrote:
>>
>>>   Because PROJ >= 6 and GDAL >= 3 change the way that PROJ strings
>>>   (representations of coordinate reference systems) are handled, steps are
>>>   being taken to find ways to adapt sp/rgdal workflows. A current proposal
>>>   is to store the WKT2_2018 string as a comment to CRS objects as defined
>>>   in
>>>   the sp package.
>>>
>>>   A draft development-in-progress version of rgdal is available at
>>>   https://r-forge.r-project.org/R/?group_id=884, and for sp at
>>>   https://github.com/rsbivand/sp (this version of sp requires rgdal >=
>>>   1.5-1). This adds the WKT comments to CRS objects on reading vector and
>>>   raster data sources, and uses WKT comments if found when writing vector
>>>   and raster objects (or at least does as far as I've checked, possibly
>>>   fragile).
>>>
>>>   An RFC with tersely worked cases for using CRS object comments to carry
>>>   WKT strings but maintaining full backward compatibility is online at
>>>   http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html.
>>>
>>>   If you have other ideas or concerns about trying to use this mechanism
>>>   for
>>>   sp CRS objects, please contribute at your earliest convenience.
>>>
>>>   http://rgdal.r-forge.r-project.org/reference/list_coordOps.html shows
>>>   the
>>>   beginning of the next step, to query transformation operations to find
>>>   viable coordinate operation pipelines.
>>>
>>>   I'm assuming that the previous behaviour (transform without considering
>>>   accuracy with whatever is to hand) is not viable going forward, and that
>>>   we will need two steps: list coordinate operations between source and
>>>   target CRS (using the WKT comments as better specifications than the
>>>   PROJ
>>>   strings), possibly intervene manually to install missing grids, then
>>>   undertake the coordinate operation.
>>>
>>>   The fallback may be simply to choose the least inaccurate available
>>>   coordinate operation, but this should be a fallback. This means that all
>>>   uses of spTransform() will require intervention.
>>>
>>>   Is this OK (it is tiresome but modernises workflows once), or is it not
>>>   OK
>>>   (no user intervention is crucial)?
>>>
>>>   These behaviours may be set in an option, so that package maintainers
>>>   and
>>>   users may delay modernisation, but all are undoubtedly served by rapid
>>>   adaptation (GRASS 7.8.1 released yesterday, libspatialite, pyproj, QGIS
>>>   development versions all state that they list candidate coordinate
>>>   operations).
>>>
>>>   We cannot ship all the grids, they are very bulky, and probably nobody
>>>   needs sub-metre accuracy world-wide. Work in PROJ is starting to create
>>>   a
>>>   content delivery network for trusted download and mechanisms for
>>>   registering downloaded grids on user platforms. We would for example not
>>>   want Windows users of rgdal and sf to have to download the same grid
>>>   twice.
>>>
>>>   Comments welcome here and at
>>>   https://github.com/r-spatial/discuss/issues/28 or
>>>   https://github.com/r-spatial/sf/issues/1187
>>>
>>>   Roger
>>>
>>>
>>
>>
>
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no<mailto:Roger.Bivand at nhh.no>
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org<mailto:R-sig-Geo at r-project.org>
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From jru85yebe @end|ng |rom hotm@||@com  Sat Nov 30 00:04:33 2019
From: jru85yebe @end|ng |rom hotm@||@com (J Rojo)
Date: Fri, 29 Nov 2019 23:04:33 +0000
Subject: [R-sig-Geo] Combining different RasterBricks maintaining z-values
 (dates)
Message-ID: <AM6PR10MB338474AC35DEB2F6654144BEB4460@AM6PR10MB3384.EURPRD10.PROD.OUTLOOK.COM>

Hi everyone
I am trying to combine different RasterBricks into one and if use the function ?brick? does not work, and with ?stack? remove the z-values as RasterStack does not include it.
The objective is to do a loop for reading and processing one nc file for each iterative step, and include each Brick into a big general Brick
One example with two RasterBricks

> a <- brick("tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2020.nc?); a

class      : RasterBrick 
dimensions : 720, 1440, 1036800, 366  (nrow, ncol, ncell, nlayers)
resolution : 0.25, 0.25  (x, y)
extent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)
crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
source     : /Volumes/Copy/NASA/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2020.nc 
names      : X2020.01.01, X2020.01.02, X2020.01.03, X2020.01.04, X2020.01.05, X2020.01.06, X2020.01.07, X2020.01.08, X2020.01.09, X2020.01.10, X2020.01.11, X2020.01.12, X2020.01.13, X2020.01.14, X2020.01.15, ... 
Date       : 2020-01-01, 2020-12-31 (min, max)
varname    : tasmax 

> b <- brick("tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2021.nc?); b

class      : RasterBrick 
dimensions : 720, 1440, 1036800, 365  (nrow, ncol, ncell, nlayers)
resolution : 0.25, 0.25  (x, y)
extent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)
crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
source     : /Volumes/Copy/NASA/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2021.nc 
names      : X2021.01.01, X2021.01.02, X2021.01.03, X2021.01.04, X2021.01.05, X2021.01.06, X2021.01.07, X2021.01.08, X2021.01.09, X2021.01.10, X2021.01.11, X2021.01.12, X2021.01.13, X2021.01.14, X2021.01.15, ... 
Date       : 2021-01-01, 2021-12-31 (min, max)
varname    : tasmax 

## Now I would like to combine both RasterBrick into one big RasterBrick with 731 layer, maintaining the Date as z-value
> c <- stack(a, b) # If I do a ?stack? z-value is not considered
> c <- brick(a, b) # If I do a ?brick? does not work and the file when saved is very weighted, and any case z-value is not maintained

Any idea about how I can combine in easy way the layers for multiple bricks according to the z-value
I am grateful any help, thank you so much

Jes?s Rojo




From @tephen@@tew@rt85 @end|ng |rom gm@||@com  Sat Nov 30 00:37:46 2019
From: @tephen@@tew@rt85 @end|ng |rom gm@||@com (Stephen Stewart)
Date: Sat, 30 Nov 2019 10:37:46 +1100
Subject: [R-sig-Geo] 
 Combining different RasterBricks maintaining z-values (dates)
In-Reply-To: <AM6PR10MB338474AC35DEB2F6654144BEB4460@AM6PR10MB3384.EURPRD10.PROD.OUTLOOK.COM>
References: <AM6PR10MB338474AC35DEB2F6654144BEB4460@AM6PR10MB3384.EURPRD10.PROD.OUTLOOK.COM>
Message-ID: <CAJefdj6--CDh_yhdWQfBmM2yH4TomPiJpOBftfw1TsY0q8bBnA@mail.gmail.com>

 Hi Jes?s,

You can use the getZ and setZ functions in the raster package to extract
and then reassign Z values.

> rain = brick("C:/Users/sbste/Downloads/2000.monthly_rain.nc")
> rain_z = getZ(rain)
>
> rain_x2 = stack(rain,rain)
> getZ(rain_x2)
NULL
> rain_x2 = setZ(rain_x2,c(rain_z,rain_z),name = "Date")
> rain_x2
class      : RasterStack
dimensions : 681, 841, 572721, 24  (nrow, ncol, ncell, nlayers)
resolution : 0.05, 0.05  (x, y)
extent     : 111.975, 154.025, -44.025, -9.975  (xmin, xmax, ymin, ymax)
crs        : +proj=longlat +datum=WGS84
names      : X2000.01.16.1, X2000.02.15.1, X2000.03.16.1, X2000.04.16.1,
X2000.05.16.1, X2000.06.16.1, X2000.07.16.1, X2000.08.16.1, X2000.09.16.1,
X2000.10.16.1, X2000.11.16.1, X2000.12.16.1, X2000.01.16.2, X2000.02.15.2,
X2000.03.16.2, ...
Date        : 2000-01-16 - 2000-12-16 (range)

Cheers,

Steve

On Sat, 30 Nov 2019 at 10:06, J Rojo <jru85yebe at hotmail.com> wrote:

> Hi everyone
> I am trying to combine different RasterBricks into one and if use the
> function ?brick? does not work, and with ?stack? remove the z-values as
> RasterStack does not include it.
> The objective is to do a loop for reading and processing one nc file for
> each iterative step, and include each Brick into a big general Brick
> One example with two RasterBricks
>
> > a <-
> brick("tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2020.nc?);
> a
>
> class      : RasterBrick
> dimensions : 720, 1440, 1036800, 366  (nrow, ncol, ncell, nlayers)
> resolution : 0.25, 0.25  (x, y)
> extent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)
> crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> source     :
> /Volumes/Copy/NASA/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2020.nc
>
> names      : X2020.01.01, X2020.01.02, X2020.01.03, X2020.01.04,
> X2020.01.05, X2020.01.06, X2020.01.07, X2020.01.08, X2020.01.09,
> X2020.01.10, X2020.01.11, X2020.01.12, X2020.01.13, X2020.01.14,
> X2020.01.15, ...
> Date       : 2020-01-01, 2020-12-31 (min, max)
> varname    : tasmax
>
> > b <-
> brick("tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2021.nc?);
> b
>
> class      : RasterBrick
> dimensions : 720, 1440, 1036800, 365  (nrow, ncol, ncell, nlayers)
> resolution : 0.25, 0.25  (x, y)
> extent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)
> crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> source     :
> /Volumes/Copy/NASA/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2021.nc
>
> names      : X2021.01.01, X2021.01.02, X2021.01.03, X2021.01.04,
> X2021.01.05, X2021.01.06, X2021.01.07, X2021.01.08, X2021.01.09,
> X2021.01.10, X2021.01.11, X2021.01.12, X2021.01.13, X2021.01.14,
> X2021.01.15, ...
> Date       : 2021-01-01, 2021-12-31 (min, max)
> varname    : tasmax
>
> ## Now I would like to combine both RasterBrick into one big RasterBrick
> with 731 layer, maintaining the Date as z-value
> > c <- stack(a, b) # If I do a ?stack? z-value is not considered
> > c <- brick(a, b) # If I do a ?brick? does not work and the file when
> saved is very weighted, and any case z-value is not maintained
>
> Any idea about how I can combine in easy way the layers for multiple
> bricks according to the z-value
> I am grateful any help, thank you so much
>
> Jes?s Rojo
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From jru85yebe @end|ng |rom hotm@||@com  Sat Nov 30 00:52:39 2019
From: jru85yebe @end|ng |rom hotm@||@com (J Rojo)
Date: Fri, 29 Nov 2019 23:52:39 +0000
Subject: [R-sig-Geo] 
 Combining different RasterBricks maintaining z-values (dates)
In-Reply-To: <CAJefdj6--CDh_yhdWQfBmM2yH4TomPiJpOBftfw1TsY0q8bBnA@mail.gmail.com>
References: <AM6PR10MB338474AC35DEB2F6654144BEB4460@AM6PR10MB3384.EURPRD10.PROD.OUTLOOK.COM>
 <CAJefdj6--CDh_yhdWQfBmM2yH4TomPiJpOBftfw1TsY0q8bBnA@mail.gmail.com>
Message-ID: <AM6PR10MB3384A0EB76BF40E5E9E41E6BB4460@AM6PR10MB3384.EURPRD10.PROD.OUTLOOK.COM>

Great!
It works as I needed
Thank you!
Jes?s

El 30 nov 2019, a las 0:37, Stephen Stewart <stephen.stewart85 at gmail.com<mailto:stephen.stewart85 at gmail.com>> escribi?:

Hi Jes?s,

You can use the getZ and setZ functions in the raster package to extract and then reassign Z values.

> rain = brick("C:/Users/sbste/Downloads/2000.monthly_rain.nc<http://2000.monthly_rain.nc/>")
> rain_z = getZ(rain)
>
> rain_x2 = stack(rain,rain)
> getZ(rain_x2)
NULL
> rain_x2 = setZ(rain_x2,c(rain_z,rain_z),name = "Date")
> rain_x2
class      : RasterStack
dimensions : 681, 841, 572721, 24  (nrow, ncol, ncell, nlayers)
resolution : 0.05, 0.05  (x, y)
extent     : 111.975, 154.025, -44.025, -9.975  (xmin, xmax, ymin, ymax)
crs        : +proj=longlat +datum=WGS84
names      : X2000.01.16.1, X2000.02.15.1, X2000.03.16.1, X2000.04.16.1, X2000.05.16.1, X2000.06.16.1, X2000.07.16.1, X2000.08.16.1, X2000.09.16.1, X2000.10.16.1, X2000.11.16.1, X2000.12.16.1, X2000.01.16.2, X2000.02.15.2, X2000.03.16.2, ...
Date        : 2000-01-16 - 2000-12-16 (range)

Cheers,

Steve

On Sat, 30 Nov 2019 at 10:06, J Rojo <jru85yebe at hotmail.com<mailto:jru85yebe at hotmail.com>> wrote:
Hi everyone
I am trying to combine different RasterBricks into one and if use the function ?brick? does not work, and with ?stack? remove the z-values as RasterStack does not include it.
The objective is to do a loop for reading and processing one nc file for each iterative step, and include each Brick into a big general Brick
One example with two RasterBricks

> a <- brick("tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2020.nc?); a

class      : RasterBrick
dimensions : 720, 1440, 1036800, 366  (nrow, ncol, ncell, nlayers)
resolution : 0.25, 0.25  (x, y)
extent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)
crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
source     : /Volumes/Copy/NASA/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2020.nc
names      : X2020.01.01, X2020.01.02, X2020.01.03, X2020.01.04, X2020.01.05, X2020.01.06, X2020.01.07, X2020.01.08, X2020.01.09, X2020.01.10, X2020.01.11, X2020.01.12, X2020.01.13, X2020.01.14, X2020.01.15, ...
Date       : 2020-01-01, 2020-12-31 (min, max)
varname    : tasmax

> b <- brick("tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2021.nc?); b

class      : RasterBrick
dimensions : 720, 1440, 1036800, 365  (nrow, ncol, ncell, nlayers)
resolution : 0.25, 0.25  (x, y)
extent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)
crs        : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
source     : /Volumes/Copy/NASA/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM/tasmax_day_BCSD_rcp45_r1i1p1_MIROC-ESM-CHEM_2021.nc
names      : X2021.01.01, X2021.01.02, X2021.01.03, X2021.01.04, X2021.01.05, X2021.01.06, X2021.01.07, X2021.01.08, X2021.01.09, X2021.01.10, X2021.01.11, X2021.01.12, X2021.01.13, X2021.01.14, X2021.01.15, ...
Date       : 2021-01-01, 2021-12-31 (min, max)
varname    : tasmax

## Now I would like to combine both RasterBrick into one big RasterBrick with 731 layer, maintaining the Date as z-value
> c <- stack(a, b) # If I do a ?stack? z-value is not considered
> c <- brick(a, b) # If I do a ?brick? does not work and the file when saved is very weighted, and any case z-value is not maintained

Any idea about how I can combine in easy way the layers for multiple bricks according to the z-value
I am grateful any help, thank you so much

Jes?s Rojo



_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org<mailto:R-sig-Geo at r-project.org>
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Sat Nov 30 11:53:31 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sat, 30 Nov 2019 11:53:31 +0100
Subject: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
In-Reply-To: <ddae23d15f5748aa897708a059dd195a@nhh.no>
References: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>
 <alpine.LFD.2.21.1911132021070.105168@reclus.nhh.no>
 <alpine.LFD.2.21.1911152151300.143646@reclus.nhh.no>
 <alpine.LFD.2.21.1911231259370.538818@reclus.nhh.no>, 
 <CAM1G4SRgWX2gBinHfq55CMqUERVzp-OA5O8ZncarDhFOsO8PbA@mail.gmail.com>
 <ddae23d15f5748aa897708a059dd195a@nhh.no>
Message-ID: <alpine.LFD.2.21.1911301152070.631238@reclus.nhh.no>

Streaming link now published (thanks to Arild Schanke), only live when 
transmission active.

Roger

On Fri, 29 Nov 2019, Roger Bivand wrote:

> Thanks, we use the local, institutional interface which provides the 
> quality we delivered at the 2014 Geostat summer school. The link will be 
> posted on 
> https://rsbivand.github.io/ECS530_h19/streaming_ecs530_h19.html when 
> available (delayed because I've been off work). We'll just have to use 
> Monday's classes to check that things are working, feedback info in the 
> link above.
>
> Roger
>
> --
> Roger Bivand
> Norwegian School of Economics
> Helleveien 30, 5045 Bergen, Norway
> Roger.Bivand at nhh.no
>
>
> ________________________________________
> Fra: Lorenzo Busetto <lbusett at gmail.com>
> Sendt: fredag 29. november 2019 18.30
> Til: Roger Bivand
> Kopi: r-sig-geo at r-project.org
> Emne: Re: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
>
> Dear Roger,
>
>    I'd be very interested in participating to the stream talk. Is it confirmed?
>
> Concerning testing the connection, I'd be happy to do it but I do not know how much I will be available online in the coming days. You can drop me a mail/DM on twitter, and if I am available I'll gladly help.
>
> Concerning technology, Zoom usually works quite well (https://zoom.us/<https://zoom.us/ent?zcid=3172>)
>
>   regards,
>
> Lorenzo
>
> On Sat, 23 Nov 2019 at 13:04, Roger Bivand <Roger.Bivand at nhh.no<mailto:Roger.Bivand at nhh.no>> wrote:
> A description of the status now with regard to a prototype resolution is
> online at:
>
> https://rsbivand.github.io/ECS530_h19/ECS530_III.html
>
> I'm planning to stream a talk about this at 09:15-11:00 CET on Tuesday 3
> December. I need a volunteer to test the streaming link in advance during
> next week. I'm unsure which technology to use for remote participants to
> provide feedback.
>
> Contributions/comments welcome!
>
> Roger
>
>
> On Fri, 15 Nov 2019, Roger Bivand wrote:
>
>> The development version of rgdal on R-Forge is now at rev 894, and is now
>> ready for trying out with PROJ6/GDAL3 workflows, and workflows that may
>> migrate within 6 months to modern CRS representations. The motivating RFC is
>> also updated to cover coordinate operations, the use of prepared
>> (pre-searched) coordinate operations, and should be read carefully by anyone
>> using rgdal::spTransform(). Note further that rgdal::project() will not be
>> adapted for PROJ6, and is effectively deprecated.
>>
>> I'll be running reverse dependency checks, and may be bugging package
>> maintainers. I would really prefer that mainainers of packages using
>> spTransform() checked themselves and joined this thread or the associated
>> twitter thread: https://twitter.com/RogerBivand/status/1194586193108914177
>>
>> Be ready for modern PROJ and GDAL, they are already being deployed across
>> open source geospatial software, like GRASS, QGIS, pyproj, spatialite etc.
>>
>> Waiting, hopefully not in vain, for contributions.
>>
>> Roger
>>
>> On Wed, 13 Nov 2019, Roger Bivand wrote:
>>
>>>  And this link explains the CDN proposal for grid distribution:
>>>
>>>  https://www.spatialys.com/en/crowdfunding/
>>>
>>>  Roger
>>>
>>>  On Wed, 13 Nov 2019, Roger Bivand wrote:
>>>
>>>>   Because PROJ >= 6 and GDAL >= 3 change the way that PROJ strings
>>>>   (representations of coordinate reference systems) are handled, steps are
>>>>   being taken to find ways to adapt sp/rgdal workflows. A current proposal
>>>>   is to store the WKT2_2018 string as a comment to CRS objects as defined
>>>>   in
>>>>   the sp package.
>>>>
>>>>   A draft development-in-progress version of rgdal is available at
>>>>   https://r-forge.r-project.org/R/?group_id=884, and for sp at
>>>>   https://github.com/rsbivand/sp (this version of sp requires rgdal >=
>>>>   1.5-1). This adds the WKT comments to CRS objects on reading vector and
>>>>   raster data sources, and uses WKT comments if found when writing vector
>>>>   and raster objects (or at least does as far as I've checked, possibly
>>>>   fragile).
>>>>
>>>>   An RFC with tersely worked cases for using CRS object comments to carry
>>>>   WKT strings but maintaining full backward compatibility is online at
>>>>   http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html.
>>>>
>>>>   If you have other ideas or concerns about trying to use this mechanism
>>>>   for
>>>>   sp CRS objects, please contribute at your earliest convenience.
>>>>
>>>>   http://rgdal.r-forge.r-project.org/reference/list_coordOps.html shows
>>>>   the
>>>>   beginning of the next step, to query transformation operations to find
>>>>   viable coordinate operation pipelines.
>>>>
>>>>   I'm assuming that the previous behaviour (transform without considering
>>>>   accuracy with whatever is to hand) is not viable going forward, and that
>>>>   we will need two steps: list coordinate operations between source and
>>>>   target CRS (using the WKT comments as better specifications than the
>>>>   PROJ
>>>>   strings), possibly intervene manually to install missing grids, then
>>>>   undertake the coordinate operation.
>>>>
>>>>   The fallback may be simply to choose the least inaccurate available
>>>>   coordinate operation, but this should be a fallback. This means that all
>>>>   uses of spTransform() will require intervention.
>>>>
>>>>   Is this OK (it is tiresome but modernises workflows once), or is it not
>>>>   OK
>>>>   (no user intervention is crucial)?
>>>>
>>>>   These behaviours may be set in an option, so that package maintainers
>>>>   and
>>>>   users may delay modernisation, but all are undoubtedly served by rapid
>>>>   adaptation (GRASS 7.8.1 released yesterday, libspatialite, pyproj, QGIS
>>>>   development versions all state that they list candidate coordinate
>>>>   operations).
>>>>
>>>>   We cannot ship all the grids, they are very bulky, and probably nobody
>>>>   needs sub-metre accuracy world-wide. Work in PROJ is starting to create
>>>>   a
>>>>   content delivery network for trusted download and mechanisms for
>>>>   registering downloaded grids on user platforms. We would for example not
>>>>   want Windows users of rgdal and sf to have to download the same grid
>>>>   twice.
>>>>
>>>>   Comments welcome here and at
>>>>   https://github.com/r-spatial/discuss/issues/28 or
>>>>   https://github.com/r-spatial/sf/issues/1187
>>>>
>>>>   Roger
>>>>
>>>>
>>>
>>>
>>
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no<mailto:Roger.Bivand at nhh.no>
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org<mailto:R-sig-Geo at r-project.org>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |bu@ett @end|ng |rom gm@||@com  Sat Nov 30 17:26:35 2019
From: |bu@ett @end|ng |rom gm@||@com (Lorenzo Busetto)
Date: Sat, 30 Nov 2019 17:26:35 +0100
Subject: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
In-Reply-To: <alpine.LFD.2.21.1911301152070.631238@reclus.nhh.no>
References: <alpine.LFD.2.21.1911131215370.100009@reclus.nhh.no>
 <alpine.LFD.2.21.1911132021070.105168@reclus.nhh.no>
 <alpine.LFD.2.21.1911152151300.143646@reclus.nhh.no>
 <alpine.LFD.2.21.1911231259370.538818@reclus.nhh.no>
 <CAM1G4SRgWX2gBinHfq55CMqUERVzp-OA5O8ZncarDhFOsO8PbA@mail.gmail.com>
 <ddae23d15f5748aa897708a059dd195a@nhh.no>
 <alpine.LFD.2.21.1911301152070.631238@reclus.nhh.no>
Message-ID: <CAM1G4STYsoRNJHDvxxrgrHEM9Fc9mT+gtPo2hPKfKj7Vd06nLw@mail.gmail.com>

Thanks. I'll check it out on Monday and let you know if there are any
issues.

Lorenzo

On Sat, 30 Nov 2019, 11:53 Roger Bivand, <Roger.Bivand at nhh.no> wrote:

> Streaming link now published (thanks to Arild Schanke), only live when
> transmission active.
>
> Roger
>
> On Fri, 29 Nov 2019, Roger Bivand wrote:
>
> > Thanks, we use the local, institutional interface which provides the
> > quality we delivered at the 2014 Geostat summer school. The link will be
> > posted on
> > https://rsbivand.github.io/ECS530_h19/streaming_ecs530_h19.html when
> > available (delayed because I've been off work). We'll just have to use
> > Monday's classes to check that things are working, feedback info in the
> > link above.
> >
> > Roger
> >
> > --
> > Roger Bivand
> > Norwegian School of Economics
> > Helleveien 30, 5045 Bergen, Norway
> > Roger.Bivand at nhh.no
> >
> >
> > ________________________________________
> > Fra: Lorenzo Busetto <lbusett at gmail.com>
> > Sendt: fredag 29. november 2019 18.30
> > Til: Roger Bivand
> > Kopi: r-sig-geo at r-project.org
> > Emne: Re: [R-sig-Geo] sp/rgdal workflows with PROJ >= 6 and GDAL >= 3
> >
> > Dear Roger,
> >
> >    I'd be very interested in participating to the stream talk. Is it
> confirmed?
> >
> > Concerning testing the connection, I'd be happy to do it but I do not
> know how much I will be available online in the coming days. You can drop
> me a mail/DM on twitter, and if I am available I'll gladly help.
> >
> > Concerning technology, Zoom usually works quite well (https://zoom.us/<
> https://zoom.us/ent?zcid=3172>)
> >
> >   regards,
> >
> > Lorenzo
> >
> > On Sat, 23 Nov 2019 at 13:04, Roger Bivand <Roger.Bivand at nhh.no<mailto:
> Roger.Bivand at nhh.no>> wrote:
> > A description of the status now with regard to a prototype resolution is
> > online at:
> >
> > https://rsbivand.github.io/ECS530_h19/ECS530_III.html
> >
> > I'm planning to stream a talk about this at 09:15-11:00 CET on Tuesday 3
> > December. I need a volunteer to test the streaming link in advance during
> > next week. I'm unsure which technology to use for remote participants to
> > provide feedback.
> >
> > Contributions/comments welcome!
> >
> > Roger
> >
> >
> > On Fri, 15 Nov 2019, Roger Bivand wrote:
> >
> >> The development version of rgdal on R-Forge is now at rev 894, and is
> now
> >> ready for trying out with PROJ6/GDAL3 workflows, and workflows that may
> >> migrate within 6 months to modern CRS representations. The motivating
> RFC is
> >> also updated to cover coordinate operations, the use of prepared
> >> (pre-searched) coordinate operations, and should be read carefully by
> anyone
> >> using rgdal::spTransform(). Note further that rgdal::project() will not
> be
> >> adapted for PROJ6, and is effectively deprecated.
> >>
> >> I'll be running reverse dependency checks, and may be bugging package
> >> maintainers. I would really prefer that mainainers of packages using
> >> spTransform() checked themselves and joined this thread or the
> associated
> >> twitter thread:
> https://twitter.com/RogerBivand/status/1194586193108914177
> >>
> >> Be ready for modern PROJ and GDAL, they are already being deployed
> across
> >> open source geospatial software, like GRASS, QGIS, pyproj, spatialite
> etc.
> >>
> >> Waiting, hopefully not in vain, for contributions.
> >>
> >> Roger
> >>
> >> On Wed, 13 Nov 2019, Roger Bivand wrote:
> >>
> >>>  And this link explains the CDN proposal for grid distribution:
> >>>
> >>>  https://www.spatialys.com/en/crowdfunding/
> >>>
> >>>  Roger
> >>>
> >>>  On Wed, 13 Nov 2019, Roger Bivand wrote:
> >>>
> >>>>   Because PROJ >= 6 and GDAL >= 3 change the way that PROJ strings
> >>>>   (representations of coordinate reference systems) are handled,
> steps are
> >>>>   being taken to find ways to adapt sp/rgdal workflows. A current
> proposal
> >>>>   is to store the WKT2_2018 string as a comment to CRS objects as
> defined
> >>>>   in
> >>>>   the sp package.
> >>>>
> >>>>   A draft development-in-progress version of rgdal is available at
> >>>>   https://r-forge.r-project.org/R/?group_id=884, and for sp at
> >>>>   https://github.com/rsbivand/sp (this version of sp requires rgdal
> >=
> >>>>   1.5-1). This adds the WKT comments to CRS objects on reading vector
> and
> >>>>   raster data sources, and uses WKT comments if found when writing
> vector
> >>>>   and raster objects (or at least does as far as I've checked,
> possibly
> >>>>   fragile).
> >>>>
> >>>>   An RFC with tersely worked cases for using CRS object comments to
> carry
> >>>>   WKT strings but maintaining full backward compatibility is online at
> >>>>   http://rgdal.r-forge.r-project.org/articles/PROJ6_GDAL3.html.
> >>>>
> >>>>   If you have other ideas or concerns about trying to use this
> mechanism
> >>>>   for
> >>>>   sp CRS objects, please contribute at your earliest convenience.
> >>>>
> >>>>   http://rgdal.r-forge.r-project.org/reference/list_coordOps.html
> shows
> >>>>   the
> >>>>   beginning of the next step, to query transformation operations to
> find
> >>>>   viable coordinate operation pipelines.
> >>>>
> >>>>   I'm assuming that the previous behaviour (transform without
> considering
> >>>>   accuracy with whatever is to hand) is not viable going forward, and
> that
> >>>>   we will need two steps: list coordinate operations between source
> and
> >>>>   target CRS (using the WKT comments as better specifications than the
> >>>>   PROJ
> >>>>   strings), possibly intervene manually to install missing grids, then
> >>>>   undertake the coordinate operation.
> >>>>
> >>>>   The fallback may be simply to choose the least inaccurate available
> >>>>   coordinate operation, but this should be a fallback. This means
> that all
> >>>>   uses of spTransform() will require intervention.
> >>>>
> >>>>   Is this OK (it is tiresome but modernises workflows once), or is it
> not
> >>>>   OK
> >>>>   (no user intervention is crucial)?
> >>>>
> >>>>   These behaviours may be set in an option, so that package
> maintainers
> >>>>   and
> >>>>   users may delay modernisation, but all are undoubtedly served by
> rapid
> >>>>   adaptation (GRASS 7.8.1 released yesterday, libspatialite, pyproj,
> QGIS
> >>>>   development versions all state that they list candidate coordinate
> >>>>   operations).
> >>>>
> >>>>   We cannot ship all the grids, they are very bulky, and probably
> nobody
> >>>>   needs sub-metre accuracy world-wide. Work in PROJ is starting to
> create
> >>>>   a
> >>>>   content delivery network for trusted download and mechanisms for
> >>>>   registering downloaded grids on user platforms. We would for
> example not
> >>>>   want Windows users of rgdal and sf to have to download the same grid
> >>>>   twice.
> >>>>
> >>>>   Comments welcome here and at
> >>>>   https://github.com/r-spatial/discuss/issues/28 or
> >>>>   https://github.com/r-spatial/sf/issues/1187
> >>>>
> >>>>   Roger
> >>>>
> >>>>
> >>>
> >>>
> >>
> >>
> >
> > --
> > Roger Bivand
> > Department of Economics, Norwegian School of Economics,
> > Helleveien 30, N-5045 Bergen, Norway.
> > voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no<mailto:
> Roger.Bivand at nhh.no>
> > https://orcid.org/0000-0003-2392-6140
> > https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org<mailto:R-sig-Geo at r-project.org>
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


