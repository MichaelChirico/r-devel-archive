From leonardo.monasterio at gmail.com  Tue Mar  3 15:15:44 2015
From: leonardo.monasterio at gmail.com (Leonardo Monasterio)
Date: Tue, 3 Mar 2015 11:15:44 -0300
Subject: [R-sig-Geo] keep just one polygon for each element
Message-ID: <CAKvLVNKbCHfNMP9AuB4vaZvzM29B=t4JSyDtV6EZFyk6Rh6-xg@mail.gmail.com>

Dear all,

I have a quite detailed SpatialPolygon of thousands of municipalities. Most
elements are formed by just one polygon, but some - thanks to islands or
rivers- are formed by multiple ones. These tiny polygons are useless to me.

Question: Is there a way create a SpatialPolygon formed by the largest
polygon of each element?

I've checked this:
http://www.r-bloggers.com/simplifying-polygon-shapefiles-in-r/

But it is not exactly what I am looking for.

Yours thankfully,

Leo.

	[[alternative HTML version deleted]]


From r.hijmans at gmail.com  Tue Mar  3 23:10:19 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Tue, 3 Mar 2015 14:10:19 -0800
Subject: [R-sig-Geo] keep just one polygon for each element
In-Reply-To: <CAKvLVNKbCHfNMP9AuB4vaZvzM29B=t4JSyDtV6EZFyk6Rh6-xg@mail.gmail.com>
References: <CAKvLVNKbCHfNMP9AuB4vaZvzM29B=t4JSyDtV6EZFyk6Rh6-xg@mail.gmail.com>
Message-ID: <CANtt_hwE==iySy0Mio=p_Jy3=Ef1DOU_rTmdLkGeY__wZteXbA@mail.gmail.com>

Leo, here is an approach that might work for you:

library(raster)
library(rgdal)
library(rgeos)
library(plyr)

# create some example data.
p <- shapefile(system.file("external/lux.shp", package="raster"))
p <- p[, 'NAME_1']
p <- aggregate(p, 'NAME_1', dissolve=FALSE)

# now we have 3 regions, each consisting of multiple polygons.
# let's get the largest polygon in each region

p$id1 <- 1:length(p)
pp <- disaggregate(p)
pp$id2 <- 1:length(pp)
pp$area <- gArea(pp, byid=TRUE)

d <- ddply(data.frame(pp), ~id1, function(x){x[which.max(x$area),]})
r <- pp[d$id2, ]


# show the results
plot(pp, col=rainbow(4)[pp$id1])
plot(r, border='blue', add=TRUE, lwd=3)

On Tue, Mar 3, 2015 at 6:15 AM, Leonardo Monasterio
<leonardo.monasterio at gmail.com> wrote:
> Dear all,
>
> I have a quite detailed SpatialPolygon of thousands of municipalities. Most
> elements are formed by just one polygon, but some - thanks to islands or
> rivers- are formed by multiple ones. These tiny polygons are useless to me.
>
> Question: Is there a way create a SpatialPolygon formed by the largest
> polygon of each element?
>
> I've checked this:
> http://www.r-bloggers.com/simplifying-polygon-shapefiles-in-r/
>
> But it is not exactly what I am looking for.
>
> Yours thankfully,
>
> Leo.
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From antonio.rrz at gmail.com  Wed Mar  4 08:37:43 2015
From: antonio.rrz at gmail.com (Antonio Rodriges)
Date: Wed, 4 Mar 2015 10:37:43 +0300
Subject: [R-sig-Geo] Polygons VS MultiLineString
Message-ID: <CAPrLoNftAT9DWwVdB-jsupHTx_FwkPVe6K0nWHrOSCBk8W4iQA@mail.gmail.com>

Hello,

I use "rasterToContour" to derive isolines for raster. After that I
save them as GeoJSON (or ESRI Shapefile). However, it saves contours
as "MultiLineString"

Is it possible to save "contours" as "Polygons"?

Code:

to_c - is a raster
contours <- rasterToContour(to_c, levels = c(18, 33))
writeOGR(contours, "file.js", layer="", driver="GeoJSON")


From mdsumner at gmail.com  Wed Mar  4 09:10:56 2015
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 04 Mar 2015 08:10:56 +0000
Subject: [R-sig-Geo] Polygons VS MultiLineString
References: <CAPrLoNftAT9DWwVdB-jsupHTx_FwkPVe6K0nWHrOSCBk8W4iQA@mail.gmail.com>
Message-ID: <CAAcGz9-LnYSc_gTxgBkaeBB9Twb-PCNW3paEqHQ767pp_otHgw@mail.gmail.com>

Try gPolygonize from rgeos, but it *really* depends on whether your lines
close cleanly.

On Wed, 4 Mar 2015 18:38 Antonio Rodriges <antonio.rrz at gmail.com> wrote:

> Hello,
>
> I use "rasterToContour" to derive isolines for raster. After that I
> save them as GeoJSON (or ESRI Shapefile). However, it saves contours
> as "MultiLineString"
>
> Is it possible to save "contours" as "Polygons"?
>
> Code:
>
> to_c - is a raster
> contours <- rasterToContour(to_c, levels = c(18, 33))
> writeOGR(contours, "file.js", layer="", driver="GeoJSON")
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From schuetz_daniel at gmx.net  Wed Mar  4 09:18:34 2015
From: schuetz_daniel at gmx.net (=?iso-8859-1?Q?Daniel_Sch=FCtz?=)
Date: Wed, 4 Mar 2015 09:18:34 +0100
Subject: [R-sig-Geo] problems using spml with a listw,
	where not everybody has a neighbour
Message-ID: <001c01d05653$cf10bf80$6d323e80$@net>

Dear Community,



Before explaining my problem I want to apologize for my maybe bad English.



I am currently working at my master exam and estimating a wage curve for
Germany. With the last spatial analysis problems occur I cannot conquer.



I get the following output when using .spml for this last estimation:



> spml(formula=formel_alle_w, data=roh_kfs, model="within",
listw=w_kfs.lwnb, effect="twoways", zero.policy=TRUE, lag=TRUE,
spatial.error="kkp")

Error in lag.listw(listw2, u) : Variable contains non-finite values

In addition: There were 32 warnings (use warnings() to see them)

> warnings()

Warning messages:

1: In lag.listw(listw, u) : NAs in lagged values

?..

32: In lag.listw(listw, u) : NAs in lagged values



I introduced zero.policy to this estimation, because in the .listw object
are elements that have no neighbours.

This is the problem I think which causes the problems. But I have no idea
how I can solve this problem.





I created the listw. object the following way:



> w_kfs.matrix = as.matrix(read.csv(file="W_kfs.csv", header=T, dec=?,? ,
sep=?;? ), nrow=107, ncol=107)

> w_kfs.lwmat = mat2listw(w_kfs.matrix)

> w_kfs.nb = w_kfs.lwmat$neighbours

> w_kfs.lwnb = nb2listw(w_kfs.nb, style="W")



The names of the columns and the rows are the same. That I already checked
like I checked the symmetry what is also given.



So if you have any solution for my problem I would be very grateful.



Yours thankfully,



Daniel





---
Diese E-Mail ist frei von Viren und Malware, denn der avast! Antivirus Schutz ist aktiv.


	[[alternative HTML version deleted]]


From frtog at vestas.com  Wed Mar  4 09:26:33 2015
From: frtog at vestas.com (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Wed, 4 Mar 2015 09:26:33 +0100
Subject: [R-sig-Geo] Polygons VS MultiLineString
In-Reply-To: <CAAcGz9-LnYSc_gTxgBkaeBB9Twb-PCNW3paEqHQ767pp_otHgw@mail.gmail.com>
References: <CAPrLoNftAT9DWwVdB-jsupHTx_FwkPVe6K0nWHrOSCBk8W4iQA@mail.gmail.com>
	<CAAcGz9-LnYSc_gTxgBkaeBB9Twb-PCNW3paEqHQ767pp_otHgw@mail.gmail.com>
Message-ID: <B078CDF40DFE4045AF172A8B4F68FC48587FE4015B@DKRDSEXC016.vestas.net>

Well, since the workhorse behind the rasterToContour() function is contourLines() from the lattice package don't expect the contour lines for a given level to form a single well defined polygon (the definition of which of course needs to be specified).

Try this:

f <- system.file("external/test.grd", package="raster")
r <- raster(f)
x <- rasterToContour(r)
class(x)
str(x)

## the levels
levels(x at data$level)

plot(r)

plot(subset(x, level == 400), add=TRUE)

plot(subset(x, level == 200), add=TRUE)

Yours sincerely / Med venlig hilsen


Frede Aakmann T?gersen
Specialist, M.Sc., Ph.D.
Plant Performance & Modeling

Technology & Service Solutions
T +45 9730 5135
M +45 2547 6050
frtog at vestas.com
http://www.vestas.com

Company reg. name: Vestas Wind Systems A/S
This e-mail is subject to our e-mail disclaimer statement.
Please refer to www.vestas.com/legal/notice
If you have received this e-mail in error please contact the sender. 

> -----Original Message-----
> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
> Michael Sumner
> Sent: 4. marts 2015 09:11
> To: Antonio Rodriges; r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Polygons VS MultiLineString
> 
> Try gPolygonize from rgeos, but it *really* depends on whether your lines
> close cleanly.
> 
> On Wed, 4 Mar 2015 18:38 Antonio Rodriges <antonio.rrz at gmail.com>
> wrote:
> 
> > Hello,
> >
> > I use "rasterToContour" to derive isolines for raster. After that I
> > save them as GeoJSON (or ESRI Shapefile). However, it saves contours
> > as "MultiLineString"
> >
> > Is it possible to save "contours" as "Polygons"?
> >
> > Code:
> >
> > to_c - is a raster
> > contours <- rasterToContour(to_c, levels = c(18, 33))
> > writeOGR(contours, "file.js", layer="", driver="GeoJSON")
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From mdsumner at gmail.com  Wed Mar  4 10:58:10 2015
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 04 Mar 2015 09:58:10 +0000
Subject: [R-sig-Geo] Polygons VS MultiLineString
References: <CAPrLoNftAT9DWwVdB-jsupHTx_FwkPVe6K0nWHrOSCBk8W4iQA@mail.gmail.com>
	<CAAcGz9-LnYSc_gTxgBkaeBB9Twb-PCNW3paEqHQ767pp_otHgw@mail.gmail.com>
	<B078CDF40DFE4045AF172A8B4F68FC48587FE4015B@DKRDSEXC016.vestas.net>
Message-ID: <CAAcGz98Ttdfy+WKWPb7jiNQ+Ov0+a4w1baunt5c-3sAaw=4b_g@mail.gmail.com>

<snip>
> > I use "rasterToContour" to derive isolines for raster. After that I
> > save them as GeoJSON (or ESRI Shapefile). However, it saves contours
> > as "MultiLineString"
> >
> > Is it possible to save "contours" as "Polygons"?
> >


If your data don't build clean lines I would be inclined to classify the
raster data and then turn that into polygons.
But, that means the boundaries are not as smooth.

Using example data:

library(raster)
r <- raster(volcano)
cl <- rasterToContour(r, levels = c(110, 130))

## see how cl$level == 130 will polygonize, but not 110
library(rgeos)
p1 <- gPolygonize(subset(cl, level == 130))
p2 <- gPolygonize(subset(cl, level == 110))
plot(p1)  ## ok
is.null(p2) ## TRUE

And that is even before you decide if you want non-intersecting polygon
donuts etc.

So, forgetting contouring

## cut into our breaks
vals <- c(110, 130)
rc <- cut(r, c(vals, cellStats(r, max)))
plot(rc)
plot(cl, add = TRUE)

## another way to polygonize
rp <- rasterToPolygons(rc, dissolve = TRUE)
plot(rp, col = c("grey", "lightblue"))
rp$layer <- vals

## check all is well
plot(r)
plot(cl, add = TRUE)
contour(r, lev = vals, add = TRUE)
text(rp[2,], lab = rp$layer[2])

plot(rp, add = TRUE, lty = 2)

So there we end up with pretty rough boundaries, since the polygonizing
from cells is following pixel boundaries exactly. Maybe we can
smooth/disaggregate the raster, but it gets pretty expensive.

Another trick might be to buffer the raster with values outside the range,
and do contour individually
for each level.

So,

i <- 1
r1 <- r
r1[is.na(r1) | r1 < vals[i]] <- vals[i] - 10
r2 <- extend(r1, extent(r) + res(r) * 2, value = vals[i] - 10)

plot(r2)
plot(gPolygonize(cl1), add = TRUE, col = grey(0.5, alpha = 0.6))


Is that better, maybe - hard to generalize I imagine, and still you need to
clip your larger boundary with internal ones, and merge them into one data
set.

Thanks for the question though, it's given me some ideas to fix some things
I need to do. (Manifold, and I imagine other, GIS does this seamlessly so
it would be nice to have contour and contourLines upgraded to be able to
trace around the edges.)

Cheers, Mike.


On Wed, 4 Mar 2015 at 19:26 Frede Aakmann T?gersen <frtog at vestas.com> wrote:

> Well, since the workhorse behind the rasterToContour() function is
> contourLines() from the lattice package don't expect the contour lines for
> a given level to form a single well defined polygon (the definition of
> which of course needs to be specified).
>
> Try this:
>
> f <- system.file("external/test.grd", package="raster")
> r <- raster(f)
> x <- rasterToContour(r)
> class(x)
> str(x)
>
> ## the levels
> levels(x at data$level)
>
> plot(r)
>
> plot(subset(x, level == 400), add=TRUE)
>
> plot(subset(x, level == 200), add=TRUE)
>
> Yours sincerely / Med venlig hilsen
>
>
> Frede Aakmann T?gersen
> Specialist, M.Sc., Ph.D.
> Plant Performance & Modeling
>
> Technology & Service Solutions
> T +45 9730 5135
> M +45 2547 6050
> frtog at vestas.com
> http://www.vestas.com
>
> Company reg. name: Vestas Wind Systems A/S
> This e-mail is subject to our e-mail disclaimer statement.
> Please refer to www.vestas.com/legal/notice
> If you have received this e-mail in error please contact the sender.
>
> > -----Original Message-----
> > From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
> > Michael Sumner
> > Sent: 4. marts 2015 09:11
> > To: Antonio Rodriges; r-sig-geo at r-project.org
> > Subject: Re: [R-sig-Geo] Polygons VS MultiLineString
> >
> > Try gPolygonize from rgeos, but it *really* depends on whether your lines
> > close cleanly.
> >
> > On Wed, 4 Mar 2015 18:38 Antonio Rodriges <antonio.rrz at gmail.com>
> > wrote:
> >
> > > Hello,
> > >
> > > I use "rasterToContour" to derive isolines for raster. After that I
> > > save them as GeoJSON (or ESRI Shapefile). However, it saves contours
> > > as "MultiLineString"
> > >
> > > Is it possible to save "contours" as "Polygons"?
> > >
> > > Code:
> > >
> > > to_c - is a raster
> > > contours <- rasterToContour(to_c, levels = c(18, 33))
> > > writeOGR(contours, "file.js", layer="", driver="GeoJSON")
> > >
> > > _______________________________________________
> > > R-sig-Geo mailing list
> > > R-sig-Geo at r-project.org
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> > >
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From richard.redweik at posteo.de  Wed Mar  4 11:45:17 2015
From: richard.redweik at posteo.de (richard.redweik at posteo.de)
Date: Wed, 04 Mar 2015 11:45:17 +0100
Subject: [R-sig-Geo] stplot - legend/classes of categorical vairable
Message-ID: <b20ce355c0118fa346cff2f0b331763c@posteo.de>

Hello,

I have an object of class STFDF named 'fdf'. I want to create a stplot 
in which the categorical variable crop_id resp. crop_short is visualized 
per year.

Currently I am only able to assign a color/legend entry to a range of 
crop_ids, e.g. green --> [1, 2], yellow --> (2, 3]:

> col <- brewer.pal(8, "Accent")
> color <- colorRampPalette(col)(18)
> stplot(fdf[,, "crop_id"], col.regions=color, cuts=18)

However, this does mean, that the crop_ids 1 and 2 are assigned to 
green.
When changing the cuts parameter to cuts=19, I get an unambigious but 
not very readable assignment, e.g. green --> [1, 1.947], yellow --> 
(1.947, 2.895].

Now I am wondering, what is the best way to get one class resp. legend 
entry per crop_id, e.g. green --> 1, yellow --> 2 etc.

Does someone know how to do this?

Thanks in advance,
Richard

PS:
Summary of my fdf object:

> summary(fdf)
Object of class STFDF
  with Dimensions (s, t, attr): (17511, 11, 6)
[[Spatial:]]
Object of class SpatialPoints
Coordinates:
       min     max
x 3464500 3654500
y 2828500 3026500
Is projected: NA
proj4string : [NA]
Number of points: 17511
[[Temporal:]]
      Index                       timeIndex
  Min.   :1995-01-01 00:00:00   Min.   : 1.0
  1st Qu.:1997-07-02 12:00:00   1st Qu.: 3.5
  Median :2000-01-01 00:00:00   Median : 6.0
  Mean   :2000-01-01 08:43:38   Mean   : 6.0
  3rd Qu.:2002-07-02 12:00:00   3rd Qu.: 8.5
  Max.   :2005-01-01 00:00:00   Max.   :11.0
[[Data attributes:]]
     objectid          crop_id        crop_short         year           
prob
  Min.   :3222754   Min.   : 1.00   WSWH   :79136   Min.   :1995   Min.   
:1
  1st Qu.:3255362   1st Qu.: 3.00   MAIF   :62594   1st Qu.:1997   1st 
Qu.:1
  Median :3287732   Median :10.00   RAPE   :13881   Median :2000   Median 
:1
  Mean   :3289614   Mean   :11.56   PULS   :10785   Mean   :2000   Mean   
:1
  3rd Qu.:3322247   3rd Qu.:19.00   WBAR   :10095   3rd Qu.:2003   3rd 
Qu.:1
  Max.   :3367519   Max.   :19.00   MAIZ   : 5069   Max.   :2005   Max.   
:1
                                    (Other):11061
  nuts_code
  FR25:192621


From Valentin.Theubet at hagel.ch  Wed Mar  4 12:35:23 2015
From: Valentin.Theubet at hagel.ch (Valentin Theubet)
Date: Wed, 4 Mar 2015 11:35:23 +0000
Subject: [R-sig-Geo] Polygons holes with plotKML
Message-ID: <9B7A061C500F904A875101FC262CF1CE03D6ED1D@SHEDV-MSX01.hagel.local>

Hi,

I want to creat a KML file with polygons containing holes. The starting data was a raster, then a SpatialGridDataframe. I finally transformed it to SpatialPolygonsDataframe with Grid2Polygons() (with several levels).

Then a creat KML file with plot KML. 
The problem is : the polygons with holes are not correctly shown in GE. There are some small polygons that are hidden under larger polygons and so on..

Thanks for your helps

Valentin Theubet


From adrian.torchiana at gmail.com  Wed Mar  4 16:48:30 2015
From: adrian.torchiana at gmail.com (Adrian Torchiana)
Date: Wed, 4 Mar 2015 16:48:30 +0100
Subject: [R-sig-Geo] R crashes when I run rgeos::gDistance
Message-ID: <CAMBOG9MYN6EPAhWnHQhHM1cC4gmt9RVh3_9ja00T1Gf-LL-mZQ@mail.gmail.com>

Hi,

This is my first post to R-sig-Geo.  I'm having trouble getting rgeos to
work.

Info on the server and packages I'm using:

$ *uname -a*
Linux some-server.somewhere.com 2.6.32-431.11.2.el6.x86_64 #1 SMP Tue Mar
25 19:59:55 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux

$ *R --version*
R version 3.0.2 (2013-09-25) -- "Frisbee Sailing"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-redhat-linux-gnu (64-bit)

$ *lsb_release -a*
LSB Version:
:base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID: CentOS
Description: CentOS release 6.5 (Final)
Release: 6.5
Codename: Final

$ *rpm -qa | grep geos*
geos-devel-3.4.2-1.rhel6.x86_64
geos-3.4.2-1.rhel6.x86_64

$ *rpm -qa | grep gdal*
gdal-1.9.2-6.rhel6.x86_64
gdal-libs-1.9.2-6.rhel6.x86_64
gdal-devel-1.9.2-6.rhel6.x86_64
gdal-java-1.9.2-6.rhel6.x86_64

$ *R -q*
> *library(rgeos)*
rgeos version: 0.3-8, (SVN revision 460)
 GEOS runtime version: 3.4.2-CAPI-1.8.2 r3921
 Polygon checking: TRUE

> *example(gDistance)*

gDstnc> pt1 = readWKT("POINT(0.5 0.5)")

gDstnc> pt2 = readWKT("POINT(2 2)")

gDstnc> p1 = readWKT("POLYGON((0 0,1 0,1 1,0 1,0 0))")

gDstnc> p2 = readWKT("POLYGON((2 0,3 1,4 0,2 0))")

gDstnc> gDistance(pt1,pt2)
R: GeometryComponentFilter.cpp:34: virtual void
geos::geom::GeometryComponentFilter::filter_ro(const
geos::geom::Geometry*): Assertion `0' failed.
Aborted (core dumped)


I'd like to be able to use the gDistance function.  What should I do to fix
this?

Please let me know if any additional information would be helpful.

Thank you for your time,

Adrian

PS  I sent a near-identical email to r-help recently; only one person
responded, and they suggested I post to R-sig-Geo.

	[[alternative HTML version deleted]]


From carolina.lang at ifop.cl  Wed Mar  4 18:19:10 2015
From: carolina.lang at ifop.cl (carolina.lang at ifop.cl)
Date: Wed, 4 Mar 2015 14:19:10 -0300 (CLST)
Subject: [R-sig-Geo] DATA for GLSM/package geoRglm
In-Reply-To: <1983353113.650583.1425485477172.JavaMail.zimbra@ifop.cl>
Message-ID: <94206144.659837.1425489550632.JavaMail.zimbra@ifop.cl>



Dear List, 

I am working with binary spatial data collected on hydroacoustic survey, for this i am trying to run a script using GLSM. But this does not work launch me an ERROR, maybe somebody has had the same ERROR, that i do not understand how to fix (last line ). 

SCRIPT 
bin=as.geodata(data.bin,coords.col=1:2,data.col=3,units.m.col=4) # 


model2 <- list(cov.pars = c(1, 0.11), beta = 1, family = "binomial")# 
mcmc.tune <- mcmc.control(S.scale = 0.43, thin = 5,n.iter=20)# 

test.tune <- glsm.mcmc(bin, model= model2, mcmc.input = mcmc.tune) 
#MODELO ESPACIAL 

bin.tune=test.tune 
bin.pre.lf<-prepare.likfit.glsm(bin.tune); str(bin.pre.lf) 
bin.lf<-likfit.glsm(bin.pre.lf,ini.phi=0.1,hessian=TRUE,cov.model="exp")# here ERROR 

results 
-------------------------------------------------------------------- 
likfit.glsm: likelihood maximisation using the function optim. 
phi = 0.1 tausq.rel = 0 
Min. 1st Qu. Median Mean 3rd Qu. Max. 

0 Inf Inf Inf Inf Inf 
Error in .func.val(SivS, SivD, DivD, beta.hat[ll], sigmasq.hat[ll], log.f.c) : Some function values are not finite 







I really hope somebody can help me 




Cheers, CArolina Lang A. 






	
www.ifop.cl 




------------------------------------------------------------------------------------------------
Certificaci?n ISO 9001/2008: Sistema de Datos Bio-Pesqueros ( Arica, Iquique, Coquimbo, Valpara?so, San Antonio, Talcahuano y Calbuco, pesquer?as industriales y artesanales)


     


	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Wed Mar  4 18:29:14 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 4 Mar 2015 18:29:14 +0100
Subject: [R-sig-Geo] R crashes when I run rgeos::gDistance
In-Reply-To: <CAMBOG9MYN6EPAhWnHQhHM1cC4gmt9RVh3_9ja00T1Gf-LL-mZQ@mail.gmail.com>
References: <CAMBOG9MYN6EPAhWnHQhHM1cC4gmt9RVh3_9ja00T1Gf-LL-mZQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.11.1503041822080.30511@reclus.nhh.no>

On Wed, 4 Mar 2015, Adrian Torchiana wrote:

> Hi,
>
> This is my first post to R-sig-Geo.  I'm having trouble getting rgeos to
> work.

My reply on R-help is at:

https://stat.ethz.ch/pipermail/r-help/2015-March/426477.html

>
> Info on the server and packages I'm using:
>
> $ *uname -a*
> Linux some-server.somewhere.com 2.6.32-431.11.2.el6.x86_64 #1 SMP Tue Mar
> 25 19:59:55 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
>
> $ *R --version*
> R version 3.0.2 (2013-09-25) -- "Frisbee Sailing"
> Copyright (C) 2013 The R Foundation for Statistical Computing
> Platform: x86_64-redhat-linux-gnu (64-bit)
>
> $ *lsb_release -a*
> LSB Version:
> :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
> Distributor ID: CentOS
> Description: CentOS release 6.5 (Final)
> Release: 6.5
> Codename: Final
>
> $ *rpm -qa | grep geos*
> geos-devel-3.4.2-1.rhel6.x86_64
> geos-3.4.2-1.rhel6.x86_64
>
> $ *rpm -qa | grep gdal*
> gdal-1.9.2-6.rhel6.x86_64
> gdal-libs-1.9.2-6.rhel6.x86_64
> gdal-devel-1.9.2-6.rhel6.x86_64
> gdal-java-1.9.2-6.rhel6.x86_64
>
> $ *R -q*
>> *library(rgeos)*
> rgeos version: 0.3-8, (SVN revision 460)
> GEOS runtime version: 3.4.2-CAPI-1.8.2 r3921
> Polygon checking: TRUE
>
>> *example(gDistance)*
>
> gDstnc> pt1 = readWKT("POINT(0.5 0.5)")
>
> gDstnc> pt2 = readWKT("POINT(2 2)")
>
> gDstnc> p1 = readWKT("POLYGON((0 0,1 0,1 1,0 1,0 0))")
>
> gDstnc> p2 = readWKT("POLYGON((2 0,3 1,4 0,2 0))")
>
> gDstnc> gDistance(pt1,pt2)
> R: GeometryComponentFilter.cpp:34: virtual void
> geos::geom::GeometryComponentFilter::filter_ro(const
> geos::geom::Geometry*): Assertion `0' failed.
> Aborted (core dumped)
>
>
> I'd like to be able to use the gDistance function.  What should I do to fix
> this?

Summarising here:

rgeos is tested regularly and frequently, so the issue here is on your 
platform, I'm afraid. How did you install rgeos (from source)? Are other 
versions of GEOS present not installed as RPMs? The simplest way to 
resolve this is to install GEOS from source, because you avoid relying on 
upstream packaging. This will however interfere with other software that 
may depend on your existing GEOS.

We know that rgeos runs its examples across multiple platforms, but we 
don't know where your GEOS came from. You can see that rgeos works on 
Linux platforms from:

http://cran.r-project.org/web/checks/check_results_rgeos.html

>
> Please let me know if any additional information would be helpful.
>
> Thank you for your time,
>
> Adrian
>
> PS  I sent a near-identical email to r-help recently; only one person
> responded, and they suggested I post to R-sig-Geo.
>
> 	[[alternative HTML version deleted]]

Please do not post HTML. Really.

Roger

>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no


From edzer.pebesma at uni-muenster.de  Wed Mar  4 19:12:37 2015
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 04 Mar 2015 19:12:37 +0100
Subject: [R-sig-Geo] stplot - legend/classes of categorical vairable
In-Reply-To: <b20ce355c0118fa346cff2f0b331763c@posteo.de>
References: <b20ce355c0118fa346cff2f0b331763c@posteo.de>
Message-ID: <54F74B15.2060305@uni-muenster.de>

Hi Richard, stplot as it is on CRAN does not support factor variables. I
added this support to the dev version on github, e.g. try this
(reproducible!) example:

# install spacetime from github:
devtools::install_github("edzer/spacetime")

library(spacetime)
example(STFDF)
set.seed(42)
gridded(stfdf at sp) = TRUE
stfdf$f = factor(sample(c("a", "b", "c"), 12, replace = TRUE))
library(RColorBrewer)
stplot(stfdf[,,"f"], col.regions = brewer.pal(3, "Accent"))


Make sure that your variable IS a factor, e.g. by

fdf at crop_id = as.factor(fdf at crop_id)

Your data contains many points. If they are laid out on a grid, the plot
improves if you do

gridded(fdf at sp) = TRUE

so that stplot knows, too.

On 03/04/2015 11:45 AM, richard.redweik at posteo.de wrote:
> Hello,
> 
> I have an object of class STFDF named 'fdf'. I want to create a stplot
> in which the categorical variable crop_id resp. crop_short is visualized
> per year.
> 
> Currently I am only able to assign a color/legend entry to a range of
> crop_ids, e.g. green --> [1, 2], yellow --> (2, 3]:
> 
>> col <- brewer.pal(8, "Accent")
>> color <- colorRampPalette(col)(18)
>> stplot(fdf[,, "crop_id"], col.regions=color, cuts=18)
> 
> However, this does mean, that the crop_ids 1 and 2 are assigned to green.
> When changing the cuts parameter to cuts=19, I get an unambigious but
> not very readable assignment, e.g. green --> [1, 1.947], yellow -->
> (1.947, 2.895].
> 
> Now I am wondering, what is the best way to get one class resp. legend
> entry per crop_id, e.g. green --> 1, yellow --> 2 etc.
> 
> Does someone know how to do this?
> 
> Thanks in advance,
> Richard
> 
> PS:
> Summary of my fdf object:
> 
>> summary(fdf)
> Object of class STFDF
>  with Dimensions (s, t, attr): (17511, 11, 6)
> [[Spatial:]]
> Object of class SpatialPoints
> Coordinates:
>       min     max
> x 3464500 3654500
> y 2828500 3026500
> Is projected: NA
> proj4string : [NA]
> Number of points: 17511
> [[Temporal:]]
>      Index                       timeIndex
>  Min.   :1995-01-01 00:00:00   Min.   : 1.0
>  1st Qu.:1997-07-02 12:00:00   1st Qu.: 3.5
>  Median :2000-01-01 00:00:00   Median : 6.0
>  Mean   :2000-01-01 08:43:38   Mean   : 6.0
>  3rd Qu.:2002-07-02 12:00:00   3rd Qu.: 8.5
>  Max.   :2005-01-01 00:00:00   Max.   :11.0
> [[Data attributes:]]
>     objectid          crop_id        crop_short         year           prob
>  Min.   :3222754   Min.   : 1.00   WSWH   :79136   Min.   :1995   Min.   :1
>  1st Qu.:3255362   1st Qu.: 3.00   MAIF   :62594   1st Qu.:1997   1st Qu.:1
>  Median :3287732   Median :10.00   RAPE   :13881   Median :2000   Median :1
>  Mean   :3289614   Mean   :11.56   PULS   :10785   Mean   :2000   Mean   :1
>  3rd Qu.:3322247   3rd Qu.:19.00   WBAR   :10095   3rd Qu.:2003   3rd Qu.:1
>  Max.   :3367519   Max.   :19.00   MAIZ   : 5069   Max.   :2005   Max.   :1
>                                    (Other):11061
>  nuts_code
>  FR25:192621
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi),  University of M?nster,
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150304/41222c5d/attachment.bin>

From macqueen1 at llnl.gov  Wed Mar  4 20:10:49 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Wed, 4 Mar 2015 19:10:49 +0000
Subject: [R-sig-Geo] still having problems installing rgdal on osx
 mavericks
In-Reply-To: <alpine.LFD.2.11.1502282210220.32564@reclus.nhh.no>
References: <AA6DBEE7-7E43-492E-8094-349CE52973D0@gmail.com>
	<alpine.LFD.2.11.1502282210220.32564@reclus.nhh.no>
Message-ID: <D11C9577.1210A9%macqueen1@llnl.gov>

You didn't say how you installed GDAL, nor did you show any error
messages, so it is difficult to say much. My guess would be that you have
some version incompatibility.

It is not the only way, but over the years, I have had success using
  http://www.kyngchaos.com/software/frameworks
and there is an rgdal there (and I do have a working rgdal on Mavericks).

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 2/28/15, 1:11 PM, "Roger Bivand" <Roger.Bivand at nhh.no> wrote:

>On Sat, 28 Feb 2015, Cyrus Mohammadian wrote:
>
>> Hey Everyone,
>>
>> I?m having problems installing and loading rgal on mavericks. I know
>> there is a lot written on the net about it but I have not been able to
>> successfully install the package. I downloaded the .tgz file and I
>> installed the package from source but every time I attempt to load the
>> library my R workspace crashes. Any help would be fantastic. If
>
>As you should know, files with the *.tgz extension are Mac Binary
>packages. Those with *.tar.gz are source packages. Is this the cause of
>your confusion?
>
>Roger
>
>> upgrading to Yosemite solves the problem I?m willing to upgrade but
>>only 
>> if I can be sure. Thanks everyone!
>>
>> Best,
>>
>> Cyrus Mohammadian
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>-- 
>Roger Bivand
>Department of Economics, Norwegian School of Economics,
>Helleveien 30, N-5045 Bergen, Norway.
>voice: +47 55 95 93 55; fax +47 55 95 91 00
>e-mail: Roger.Bivand at nhh.no


From antonio.rrz at gmail.com  Thu Mar  5 09:49:35 2015
From: antonio.rrz at gmail.com (Antonio Rodriges)
Date: Thu, 5 Mar 2015 11:49:35 +0300
Subject: [R-sig-Geo] Polygons VS MultiLineString
In-Reply-To: <CAAcGz98Ttdfy+WKWPb7jiNQ+Ov0+a4w1baunt5c-3sAaw=4b_g@mail.gmail.com>
References: <CAPrLoNftAT9DWwVdB-jsupHTx_FwkPVe6K0nWHrOSCBk8W4iQA@mail.gmail.com>
	<CAAcGz9-LnYSc_gTxgBkaeBB9Twb-PCNW3paEqHQ767pp_otHgw@mail.gmail.com>
	<B078CDF40DFE4045AF172A8B4F68FC48587FE4015B@DKRDSEXC016.vestas.net>
	<CAAcGz98Ttdfy+WKWPb7jiNQ+Ov0+a4w1baunt5c-3sAaw=4b_g@mail.gmail.com>
Message-ID: <CAPrLoNfHR8ymMRXdMyHb3vmohY-Lnys7N6qo+kU=tAi1rV29CA@mail.gmail.com>

Mike,

Thank you, that was a great example; I reproduced it.

However, if I am satisfied with the rasterToContour output, I think
there should be a way to directly convert it polygons. I am seeking
this method.


From angelini75 at gmail.com  Thu Mar  5 10:50:32 2015
From: angelini75 at gmail.com (angelini75)
Date: Thu, 5 Mar 2015 02:50:32 -0700 (MST)
Subject: [R-sig-Geo] readGDAL no bands in dataset MOD16A2 hdf4
Message-ID: <1425549032211-7587868.post@n2.nabble.com>

Hi,

I'm trying to process MODIS product MOD16A2 (Potential Evapotranspiration).
I need to process 180 images like this:
ftp://ftp.ntsg.umt.edu/pub/MODIS/NTSG_Products/MOD16/MOD16A2_MONTHLY.MERRA_GMAO_1kmALB/Y2014/M12/MOD16A2.A2007M06.h12v13.105.2013120050055.hdf

I downloaded the files at my working directory but I have problem to reed it
with RGDAL

readGDAL("output/h12v12/h12v12_A2000M01.hdf")
output/h12v12/h12v12_A2000M01.hdf has GDAL driver HDF4 
and has 512 rows and 512 columns
Error in getRasterData(x, band = band, offset = offset, region.dim =
region.dim,  : 
  no bands in dataset
In addition: Warning messages:
1: In dim(x) : no bands in dataset
2: In dim(x) : no bands in dataset
3: In readGDAL("output/h12v12/h12v12_A2000M01.hdf") :
  GeoTransform values not available 

When I open the file in QGIS, I can see this
MOD_Grid_MOD16A2:ET_1km
MOD_Grid_MOD16A2:LE_1km
MOD_Grid_MOD16A2:PET_1km
MOD_Grid_MOD16A2:PLE_1km

I have followed this script, which works well for MOD13Q1
(https://everydropr.wordpress.com/2011/11/06/how-to-processing-hdf4-data-using-r/),
but it does not with MOD16A2
 hdfImage <- readGDAL(paste("HDF4_EOS:EOS_GRID:", files[1],
":MODIS_Grid_16DAY_250m_500m_VI:250m 16 days EVI", sep = ""))
 image(hdfImage)
 writeGDAL(hdfImage, "/Users/name/Documents/R_HDF4/output/hdfImage.tif",
drivername = "GTiff", type = "Float32", mvFlag = NA, options=NULL,
copy_drivername = "GTiff", setStatistics=FALSE) 

Thanks in advance for the help.
Marcos






--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/readGDAL-no-bands-in-dataset-MOD16A2-hdf4-tp7587868.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From angelini75 at gmail.com  Thu Mar  5 10:54:23 2015
From: angelini75 at gmail.com (angelini75)
Date: Thu, 5 Mar 2015 02:54:23 -0700 (MST)
Subject: [R-sig-Geo] readGDAL no bands in dataset MOD16A2 hdf4
In-Reply-To: <1425549032211-7587868.post@n2.nabble.com>
References: <1425549032211-7587868.post@n2.nabble.com>
Message-ID: <1425549263007-7587869.post@n2.nabble.com>

Sorry, this was the problem!

readGDAL('HDF4_EOS:EOS_GRID:"output/h12v12/h12v12_A2000M01.hdf":MOD_Grid_MOD16A2:PET_1km')



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/readGDAL-no-bands-in-dataset-MOD16A2-hdf4-tp7587868p7587869.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From gosz at gmx.de  Thu Mar  5 13:09:39 2015
From: gosz at gmx.de (Guido Schulz)
Date: Thu, 5 Mar 2015 13:09:39 +0100
Subject: [R-sig-Geo] Weighting observations in a spdep::lagsarlm Model?
Message-ID: <trinity-527e73b5-562a-480d-9fff-470ee80239d8-1425557379438@3capp-gmx-bs06>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150305/3764d69e/attachment.html>

From Roger.Bivand at nhh.no  Thu Mar  5 13:29:57 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 5 Mar 2015 13:29:57 +0100
Subject: [R-sig-Geo] Weighting observations in a spdep::lagsarlm Model?
In-Reply-To: <trinity-527e73b5-562a-480d-9fff-470ee80239d8-1425557379438@3capp-gmx-bs06>
References: <trinity-527e73b5-562a-480d-9fff-470ee80239d8-1425557379438@3capp-gmx-bs06>
Message-ID: <alpine.LFD.2.11.1503051314200.3075@reclus.nhh.no>

On Thu, 5 Mar 2015, Guido Schulz wrote:

> I would like to estimate a spdep::lagsarlm Model (Spatially 
> Autoregressive Regression) in R. My observations (n=447) are polygons, 
> each representing an administrative region of Berlin and my attributes 
> are aggregated on this regional level (e.g. unemployment rate, median 
> house prices etc.).
> ?However, the problem is that the regions have a highly varying number 
> of inhabitants (between 500 and 32000).
> ?Therefore, I would like to weight each observation with its number of 
> inhabitants. With lm this is easypeasy, because it accepts the optional 
> argument weights=... How can I do something similar with 
> spdep::lagsarlm? Is there a workaround? Or does weighting not make any 
> sense at all in the case of SAR Models?

Good question!

Weighting is potentially important in spatial regression, and is often 
used in epidemiology and criminology, but hasn't been asked for in 
econometrics.

So you can use spautolm() with weights, and as the SAR error model is the 
same thing as errorsarlm(), you can fit weighted SLX (lm), weighted SEM 
and SDEM (spautolm), but not the models with lagged y. Before this can be 
considered, someone has to do the math to work out its consequences for 
estimation and impact methods; I'm not aware of any relevant literature 
for ML estimation. Treated additively, you could look at Bayesian methods, 
either in Matlab where LeSage & Pace fit a heteroskedasticity effect (not 
weights) or maybe using the "slm" model in INLA, but neither of these 
ideas have been thought through. You could also look at GM HAC methods, 
but again, they do not take given weights.

If you can find such literature, or develop the missing reasoning, I'd be 
willing to implement it in ML, as it is an obviously relevant robustness 
check.

Roger

> ?
> 
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no

From richard.redweik at posteo.de  Fri Mar  6 11:18:09 2015
From: richard.redweik at posteo.de (richard.redweik at posteo.de)
Date: Fri, 06 Mar 2015 11:18:09 +0100
Subject: [R-sig-Geo] stplot - legend/classes of categorical vairable
In-Reply-To: <54F74B15.2060305@uni-muenster.de>
References: <b20ce355c0118fa346cff2f0b331763c@posteo.de>
	<54F74B15.2060305@uni-muenster.de>
Message-ID: <42412959f90cd87cfabfdb21579bf51f@posteo.de>

Thank you, Edzer!

That was exactly what I was looking for. It works as expected after 
coercing my variable to a factor!

Best,
Richard

Am 04.03.2015 19:12 schrieb Edzer Pebesma:
> Hi Richard, stplot as it is on CRAN does not support factor variables. 
> I
> added this support to the dev version on github, e.g. try this
> (reproducible!) example:
> 
> # install spacetime from github:
> devtools::install_github("edzer/spacetime")
> 
> library(spacetime)
> example(STFDF)
> set.seed(42)
> gridded(stfdf at sp) = TRUE
> stfdf$f = factor(sample(c("a", "b", "c"), 12, replace = TRUE))
> library(RColorBrewer)
> stplot(stfdf[,,"f"], col.regions = brewer.pal(3, "Accent"))
> 
> 
> Make sure that your variable IS a factor, e.g. by
> 
> fdf at crop_id = as.factor(fdf at crop_id)
> 
> Your data contains many points. If they are laid out on a grid, the 
> plot
> improves if you do
> 
> gridded(fdf at sp) = TRUE
> 
> so that stplot knows, too.
> 
> On 03/04/2015 11:45 AM, richard.redweik at posteo.de wrote:
>> Hello,
>> 
>> I have an object of class STFDF named 'fdf'. I want to create a stplot
>> in which the categorical variable crop_id resp. crop_short is 
>> visualized
>> per year.
>> 
>> Currently I am only able to assign a color/legend entry to a range of
>> crop_ids, e.g. green --> [1, 2], yellow --> (2, 3]:
>> 
>>> col <- brewer.pal(8, "Accent")
>>> color <- colorRampPalette(col)(18)
>>> stplot(fdf[,, "crop_id"], col.regions=color, cuts=18)
>> 
>> However, this does mean, that the crop_ids 1 and 2 are assigned to 
>> green.
>> When changing the cuts parameter to cuts=19, I get an unambigious but
>> not very readable assignment, e.g. green --> [1, 1.947], yellow -->
>> (1.947, 2.895].
>> 
>> Now I am wondering, what is the best way to get one class resp. legend
>> entry per crop_id, e.g. green --> 1, yellow --> 2 etc.
>> 
>> Does someone know how to do this?
>> 
>> Thanks in advance,
>> Richard
>> 
>> PS:
>> Summary of my fdf object:
>> 
>>> summary(fdf)
>> Object of class STFDF
>>  with Dimensions (s, t, attr): (17511, 11, 6)
>> [[Spatial:]]
>> Object of class SpatialPoints
>> Coordinates:
>>       min     max
>> x 3464500 3654500
>> y 2828500 3026500
>> Is projected: NA
>> proj4string : [NA]
>> Number of points: 17511
>> [[Temporal:]]
>>      Index                       timeIndex
>>  Min.   :1995-01-01 00:00:00   Min.   : 1.0
>>  1st Qu.:1997-07-02 12:00:00   1st Qu.: 3.5
>>  Median :2000-01-01 00:00:00   Median : 6.0
>>  Mean   :2000-01-01 08:43:38   Mean   : 6.0
>>  3rd Qu.:2002-07-02 12:00:00   3rd Qu.: 8.5
>>  Max.   :2005-01-01 00:00:00   Max.   :11.0
>> [[Data attributes:]]
>>     objectid          crop_id        crop_short         year           
>> prob
>>  Min.   :3222754   Min.   : 1.00   WSWH   :79136   Min.   :1995   Min. 
>>   :1
>>  1st Qu.:3255362   1st Qu.: 3.00   MAIF   :62594   1st Qu.:1997   1st 
>> Qu.:1
>>  Median :3287732   Median :10.00   RAPE   :13881   Median :2000   
>> Median :1
>>  Mean   :3289614   Mean   :11.56   PULS   :10785   Mean   :2000   Mean 
>>   :1
>>  3rd Qu.:3322247   3rd Qu.:19.00   WBAR   :10095   3rd Qu.:2003   3rd 
>> Qu.:1
>>  Max.   :3367519   Max.   :19.00   MAIZ   : 5069   Max.   :2005   Max. 
>>   :1
>>                                    (Other):11061
>>  nuts_code
>>  FR25:192621
>> 
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From mspinola10 at gmail.com  Fri Mar  6 16:00:05 2015
From: mspinola10 at gmail.com (=?UTF-8?Q?Manuel_Sp=C3=ADnola?=)
Date: Fri, 6 Mar 2015 09:00:05 -0600
Subject: [R-sig-Geo] Creating a raster with an hexagonal grid
Message-ID: <CABkCotSojCSDGMUAxJ7VwzyZ_7AMoAp8yT2ya6MoRxtGz5_nxw@mail.gmail.com>

Dear list members,

I would like to make a raster with hexagonals pixels.

r = raster("raster.tif")

ras = as(r, "SpatialPixelsDataFrame")
image(ras)
HexPts <-spsample(ras,type="hexagonal", cellsize = 1000, offset = c(0,0))
points(HexPts, pch = "+")
HexPols <- HexPoints2SpatialPolygons(HexPts)
row.names(HexPols)<-as.character(HexPols at plotOrder)
HexPolsdf = SpatialPolygonsDataFrame(HexPols, data.frame(hex= c(1:72)))
plot(HexPolsdf, add = T)
text(coordinates(HexPolsdf), labels=HexPolsdf at plotOrder)

# calculate landscape metrics with land.metrics (spatialEco package) for
each  hexagon

lm = land.metrics(x=HexPolsdf, y=rb, bkgd = 0, Trace = F)

I want to make a raster with one of the landscape metrics assigned to each
hexagon (a raster with hexagonal pixels)

HexPolsdf$CantHab = lm$prop.landscape

raster = raster(HexPolsdf)

# rasterize

rastercant = rasterize(HexPolsdf, raster, field = "CantHab")
plot(rastercant)

However the raster doesn?t show hexagonal pixels, it shows square pixels.


How can I make the raster showing hexagonal pixels and make the predictions
to a final raster with hexagonal pixels.

Best,

Manuel






-- 
*Manuel Sp?nola, Ph.D.*
Instituto Internacional en Conservaci?n y Manejo de Vida Silvestre
Universidad Nacional
Apartado 1350-3000
Heredia
COSTA RICA
mspinola at una.ac.cr
mspinola10 at gmail.com
Tel?fono: (506) 2277-3598
Fax: (506) 2237-7036
Personal website: Lobito de r?o <https://sites.google.com/site/lobitoderio/>
Institutional website: ICOMVIS <http://www.icomvis.una.ac.cr/>

	[[alternative HTML version deleted]]


From mtreglia at gmail.com  Fri Mar  6 19:03:48 2015
From: mtreglia at gmail.com (Michael Treglia)
Date: Fri, 6 Mar 2015 12:03:48 -0600
Subject: [R-sig-Geo] Creating a raster with an hexagonal grid
In-Reply-To: <CABkCotSojCSDGMUAxJ7VwzyZ_7AMoAp8yT2ya6MoRxtGz5_nxw@mail.gmail.com>
References: <CABkCotSojCSDGMUAxJ7VwzyZ_7AMoAp8yT2ya6MoRxtGz5_nxw@mail.gmail.com>
Message-ID: <CAPKp32ttU755=12SPHk=meLun2yuEe-94L0pDLTUNLPNsJz_Jg@mail.gmail.com>

Manuel,

I might be mistaken, but as far as I understand, rasters in R use square
pixels... and thats how the raster data are typically stored in raster file
types and used by other GIS packages, with cell size referring to the
square dimensions of pixels.  Thus, all of the ideas are based on that
assumption.  These are just some thoughts of alternative ways to present
and analyze the data in R, without using an explicit 'hexagon-based raster'
type of data format.

(Somebody please chime in and correct me if I'm wrong... if there is some
sort of 'hexagon' type for raster formats in R... or even if there is a
generally appropriate storage type for hexagon-based rasters in general,
I'd love to hear about this)

For visualization, here are a couple of thoughts:

1) Rasterize the hexagonal SpatialPolygonDataFrame to a finer resolution,
with nearest neighbor resampling, such that the resulting raster section
for each polygon would look something like this:
http://en.wikipedia.org/wiki/User:Smcgruer#mediaviewer/File:Hexagonal_hyper_pixel.svg

2) Stick with the hexagons as SpatialPolygonDataFrames, and plot the colors
based on the landscape metric values; there's a package for Chloropleth
maps (I've never used it, but looks handy...):
http://cran.r-project.org/web/packages/choroplethr/;
* Also, this presentation looks useful:
http://www.slideshare.net/tjagger/r-spatial-analysis-using-sp


Re the latter part of your question on Analysis and prediction with
hexagonal pixels, I think it depends on the goals/methods of your analysis.

1)  If your analyses are focused in each pixel individually (e.g., for
distribution modeling, folks typically develop and predict models for a
stack of pixels under a specific point - not the relation between adjacent
pixels and such), then just use an appropriately sized square-pixel based
raster.

2) If your analyses generally rely on the centers of adjacent pixels being
the same distances apart regardless of the direction (e.g., for animal
movement), can you just work with the centroids of the Hexagonal polygons?

Hope that helps.
Cheers,
mike


On Fri, Mar 6, 2015 at 9:00 AM, Manuel Sp?nola <mspinola10 at gmail.com> wrote:

> Dear list members,
>
> I would like to make a raster with hexagonals pixels.
>
> r = raster("raster.tif")
>
> ras = as(r, "SpatialPixelsDataFrame")
> image(ras)
> HexPts <-spsample(ras,type="hexagonal", cellsize = 1000, offset = c(0,0))
> points(HexPts, pch = "+")
> HexPols <- HexPoints2SpatialPolygons(HexPts)
> row.names(HexPols)<-as.character(HexPols at plotOrder)
> HexPolsdf = SpatialPolygonsDataFrame(HexPols, data.frame(hex= c(1:72)))
> plot(HexPolsdf, add = T)
> text(coordinates(HexPolsdf), labels=HexPolsdf at plotOrder)
>
> # calculate landscape metrics with land.metrics (spatialEco package) for
> each  hexagon
>
> lm = land.metrics(x=HexPolsdf, y=rb, bkgd = 0, Trace = F)
>
> I want to make a raster with one of the landscape metrics assigned to each
> hexagon (a raster with hexagonal pixels)
>
> HexPolsdf$CantHab = lm$prop.landscape
>
> raster = raster(HexPolsdf)
>
> # rasterize
>
> rastercant = rasterize(HexPolsdf, raster, field = "CantHab")
> plot(rastercant)
>
> However the raster doesn?t show hexagonal pixels, it shows square pixels.
>
>
> How can I make the raster showing hexagonal pixels and make the predictions
> to a final raster with hexagonal pixels.
>
> Best,
>
> Manuel
>
>
>
>
>
>
> --
> *Manuel Sp?nola, Ph.D.*
> Instituto Internacional en Conservaci?n y Manejo de Vida Silvestre
> Universidad Nacional
> Apartado 1350-3000
> Heredia
> COSTA RICA
> mspinola at una.ac.cr
> mspinola10 at gmail.com
> Tel?fono: (506) 2277-3598
> Fax: (506) 2237-7036
> Personal website: Lobito de r?o <
> https://sites.google.com/site/lobitoderio/>
> Institutional website: ICOMVIS <http://www.icomvis.una.ac.cr/>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From elgabass at gmail.com  Sun Mar  8 19:42:37 2015
From: elgabass at gmail.com (Ahmed El-Gabbas)
Date: Sun, 8 Mar 2015 19:42:37 +0100
Subject: [R-sig-Geo] dismo package : gbif function error
Message-ID: <CAJEGaydtAmnysBuqM8=Xwc-=rvp0w4bR6qLrKvjNPvjEkqzHbg@mail.gmail.com>

Hello all,
I am having an unusual problem using gbif function of the dismo package.

When I am using this code: acaule = gbif("solanum", "acaule*", geo=FALSE)
this error appears: "
http://data.gbif.org/ws/rest/occurrence/count?scientificname=solanum+
Error in gbif("solanum", "", geo = FALSE) : invalid request"

It seems that this is a problem of accessing to the GBIF API not a problem
of dismo package itself. The website used by the gbif function to get the
records from the GBIF website "http://data.gbif.org/" is no longer usable
and gives this error when accessing it using regular browser "data.gbif.org
has now been decommissioned. Please use www.gbif.org for all data access "

Any suggestions,
Regards,
Ahmed

	[[alternative HTML version deleted]]


From antonio.rrz at gmail.com  Mon Mar  9 12:44:51 2015
From: antonio.rrz at gmail.com (Antonio Rodriges)
Date: Mon, 9 Mar 2015 14:44:51 +0300
Subject: [R-sig-Geo] Polygons VS MultiLineString
In-Reply-To: <CAPrLoNfHR8ymMRXdMyHb3vmohY-Lnys7N6qo+kU=tAi1rV29CA@mail.gmail.com>
References: <CAPrLoNftAT9DWwVdB-jsupHTx_FwkPVe6K0nWHrOSCBk8W4iQA@mail.gmail.com>
	<CAAcGz9-LnYSc_gTxgBkaeBB9Twb-PCNW3paEqHQ767pp_otHgw@mail.gmail.com>
	<B078CDF40DFE4045AF172A8B4F68FC48587FE4015B@DKRDSEXC016.vestas.net>
	<CAAcGz98Ttdfy+WKWPb7jiNQ+Ov0+a4w1baunt5c-3sAaw=4b_g@mail.gmail.com>
	<CAPrLoNfHR8ymMRXdMyHb3vmohY-Lnys7N6qo+kU=tAi1rV29CA@mail.gmail.com>
Message-ID: <CAPrLoNe4it5pKTZyZQXQ71EyTEoXZTtjgUDWB_mFNFbhvvO+xw@mail.gmail.com>

The solution I've devised so far

contours <- rasterToContour(to_c, levels = c(18, 33))

res <- lapply(slot(contours, "lines"), function(x) lapply(slot(x,
"Lines"), function(y) slot(y, "coords")))
polies <- lapply(res, function (x) lapply(x, function (y) Polygon(y)))
pp_18 <- Polygons(polies[[1]], ID = "18")
pp_33 <- Polygons(polies[[2]], ID = "33")
all_pp<- SpatialPolygons(c(pp_18, pp_33))
df <- data.frame(value = c(18, 33), row.names = c("18", "33"))
SPDF <- SpatialPolygonsDataFrame(all_pp, df)

writeOGR(SPDF, jsfile, layer="", driver="GeoJSON")

This is in case you have only two values 18 and 33. If you have more,
create list and create Polygons also using lapply or for loop

2015-03-05 11:49 GMT+03:00 Antonio Rodriges <antonio.rrz at gmail.com>:
> Mike,
>
> Thank you, that was a great example; I reproduced it.
>
> However, if I am satisfied with the rasterToContour output, I think
> there should be a way to directly convert it polygons. I am seeking
> this method.


From p.agarwal at duke.edu  Mon Mar  9 17:38:30 2015
From: p.agarwal at duke.edu (Pankaj Agarwal)
Date: Mon, 9 Mar 2015 16:38:30 +0000
Subject: [R-sig-Geo] Spatial interpolation for air pollution data
Message-ID: <BN3PR0501MB11721BDB6164F17758D8E91AA91B0@BN3PR0501MB1172.namprd05.prod.outlook.com>

Dear All,

I am working on figuring out which R package to use for spatial interpolation of air pollutant data.  I am new to the field of geospatial data analysis with a basic background in statistics, so hopefully my questions are not too na?ve.

We have a data set of daily air pollutant levels measured at monitoring stations for the state of North Carolina, USA.  The number of monitoring stations are less than the number of counties in the state.  We have the location of these stations and the location of county centroids in Latitude/Longitude.  We would like to interpolate the air pollutant measurement to the centroid of the counties for which the air pollutant measurement is missing.   Furthermore, we also have the centroid Lat/Long at the zip code level for each county and would also like to interpolate at the zip code level.  Once this is done, we would also like to incorporate meteorological data that is available as daily measurements of wind speeds, humidity, min/max/avg temperature etc. into the interpolation as co-factors.

My understanding is that the kriging method of interpolation should work and I have found three packages - "fields", "geoR" and  "gstat" that provide this function.  I am trying to figure out which one of these would best serve the purpose.  From the documentation I found that only "gstat" offers co-kriging to take into account the meteorological co-factors into account.

1.  Could you please advise which package would provide the capabilities that I need for the spatial interpolation of the air pollutant data at the county and zip code level.
2.  The documentation for "gstat" says that the Lat/Long data needs to be projected and I believe rgdal::spTransform can be used for this purpose.  I have two questions related to this.
2a. Do the other packages also require projected data or can they accept Lat/Long.
2b. When we project Lat/Long data, do we lose any accuracy in the interpolation given that the original air pollutant measurement is at the Lat/Long coordinates.  This may be important because we are trying to study the health effects of the air pollutant levels and the magnitude of this effect may be small so losing accuracy might bias the analysis disproportionately.

Thanks you for any guidance you can provide.

Sincerely,

- Pankaj

----------------------------------------------------------------
Pankaj Agarwal, M.S
Bioinformatician
Database Analyst II
Surgical Sciences Applied Therapeutics Section
Department of Surgery
Duke University
919-244-6389
p.agarwal at duke.edu<mailto:p.agarwal at duke.edu>


	[[alternative HTML version deleted]]


From clint at ecy.wa.gov  Mon Mar  9 18:01:58 2015
From: clint at ecy.wa.gov (Clint Bowman)
Date: Mon, 9 Mar 2015 10:01:58 -0700 (PDT)
Subject: [R-sig-Geo] Spatial interpolation for air pollution data
In-Reply-To: <BN3PR0501MB11721BDB6164F17758D8E91AA91B0@BN3PR0501MB1172.namprd05.prod.outlook.com>
References: <BN3PR0501MB11721BDB6164F17758D8E91AA91B0@BN3PR0501MB1172.namprd05.prod.outlook.com>
Message-ID: <alpine.LRH.2.11.1503090956530.18806@aeolus.ecy.wa.gov>

Pankaj,

Although geospatial techniques should work in the eastern portions of 
North Carolina, I'd doubt they should be applied in the more mountanous 
western portion.  We've used the algorithm in EPA's BenMAP program to fuse 
available CMAQ forecasts with available monitoring to produce a gridded 
map of background concentrations.

<http://lar.wsu.edu/nw-airquest/lookup.html>

Clint

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600

         USPS:           PO Box 47600, Olympia, WA 98504-7600
         Parcels:        300 Desmond Drive, Lacey, WA 98503-1274

On Mon, 9 Mar 2015, Pankaj Agarwal wrote:

> Dear All,
>
> I am working on figuring out which R package to use for spatial interpolation of air pollutant data.  I am new to the field of geospatial data analysis with a basic background in statistics, so hopefully my questions are not too na??e.
>
> We have a data set of daily air pollutant levels measured at monitoring stations for the state of North Carolina, USA.  The number of monitoring stations are less than the number of counties in the state.  We have the location of these stations and the location of county centroids in Latitude/Longitude.  We would like to interpolate the air pollutant measurement to the centroid of the counties for which the air pollutant measurement is missing.   Furthermore, we also have the centroid Lat/Long at the zip code level for each county and would also like to interpolate at the zip code level.  Once this is done, we would also like to incorporate meteorological data that is available as daily measurements of wind speeds, humidity, min/max/avg temperature etc. into the interpolation as co-factors.
>
> My understanding is that the kriging method of interpolation should work and I have found three packages - "fields", "geoR" and  "gstat" that provide this function.  I am trying to figure out which one of these would best serve the purpose.  From the documentation I found that only "gstat" offers co-kriging to take into account the meteorological co-factors into account.
>
> 1.  Could you please advise which package would provide the capabilities that I need for the spatial interpolation of the air pollutant data at the county and zip code level.
> 2.  The documentation for "gstat" says that the Lat/Long data needs to be projected and I believe rgdal::spTransform can be used for this purpose.  I have two questions related to this.
> 2a. Do the other packages also require projected data or can they accept Lat/Long.
> 2b. When we project Lat/Long data, do we lose any accuracy in the interpolation given that the original air pollutant measurement is at the Lat/Long coordinates.  This may be important because we are trying to study the health effects of the air pollutant levels and the magnitude of this effect may be small so losing accuracy might bias the analysis disproportionately.
>
> Thanks you for any guidance you can provide.
>
> Sincerely,
>
> - Pankaj
>
> ----------------------------------------------------------------
> Pankaj Agarwal, M.S
> Bioinformatician
> Database Analyst II
> Surgical Sciences Applied Therapeutics Section
> Department of Surgery
> Duke University
> 919-244-6389
> p.agarwal at duke.edu<mailto:p.agarwal at duke.edu>
>
>
> 	[[alternative HTML version deleted]]
>
>


From r.hijmans at gmail.com  Mon Mar  9 20:57:15 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Mon, 9 Mar 2015 12:57:15 -0700
Subject: [R-sig-Geo] dismo package : gbif function error
In-Reply-To: <CAJEGaydtAmnysBuqM8=Xwc-=rvp0w4bR6qLrKvjNPvjEkqzHbg@mail.gmail.com>
References: <CAJEGaydtAmnysBuqM8=Xwc-=rvp0w4bR6qLrKvjNPvjEkqzHbg@mail.gmail.com>
Message-ID: <CANtt_hywDeNkzWbCQVaDhWMCKbJiAgQ57pHU_ZkS36syMxq+zg@mail.gmail.com>

The web-service that the function depends on has changed. It should
work with the development version of dismo (version 1.0-8). You should
be able to install it from R-Forge like this:

install.packages("dismo", repos="http://R-Forge.R-project.org")

Robert

On Sun, Mar 8, 2015 at 11:42 AM, Ahmed El-Gabbas <elgabass at gmail.com> wrote:
> Hello all,
> I am having an unusual problem using gbif function of the dismo package.
>
> When I am using this code: acaule = gbif("solanum", "acaule*", geo=FALSE)
> this error appears: "
> http://data.gbif.org/ws/rest/occurrence/count?scientificname=solanum+
> Error in gbif("solanum", "", geo = FALSE) : invalid request"
>
> It seems that this is a problem of accessing to the GBIF API not a problem
> of dismo package itself. The website used by the gbif function to get the
> records from the GBIF website "http://data.gbif.org/" is no longer usable
> and gives this error when accessing it using regular browser "data.gbif.org
> has now been decommissioned. Please use www.gbif.org for all data access "
>
> Any suggestions,
> Regards,
> Ahmed
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From corderos at umich.edu  Mon Mar  9 23:02:33 2015
From: corderos at umich.edu (Silvia Cordero-Sancho)
Date: Mon, 9 Mar 2015 18:02:33 -0400
Subject: [R-sig-Geo] categorical values in im-objects (spatstat/ppm)
Message-ID: <CAORmci-vKjTD98vwXdKDAXgVHD-q4AX4s+okc-JdjuOy39=UxA@mail.gmail.com>

Hello,

I will like to fit a point process model (ppm) with several covariates. One
of them is a grid with 15 categorical variables (zones).

To recognized the values in my grid as categorical, I followed the code in
the following link:

http://stackoverflow.com/questions/26162955/r-package-spatstat-how-to-use-point-process-model-covariate-as-factor-when-pixe?answertab=active#tab-top




*zone1<-eval.im <http://eval.im>(as.factor(zone))*

*levels(zone1)<-c("A1","A2","A3","A4","B1","B2","B3","B4",*
*              "C1","C2","C3","C4","C5","C6","D")*

*unitname(zone1)<-c("meter","meters")*

But I am not sure if it really worked. If I run the function
*is.factor(zone1)*, the result is FALSE, but if I run the function
selecting any column or row (e.g. *is.factor(zone1[1,])* or
*is.factor(zone1[,200])*) the results show as TRUE.

However, the function *summary(zone1)* indicates that it is a factor value
pixel image:

factor-valued pixel image
2641 x 680 pixel array (ny, nx)
enclosing rectangle: [992380, 1012780] x [732491, 811721] meters
dimensions of each pixel: 30 x 30 meters
Image is defined on a subset of the rectangular grid
Subset area = 1577529000 square meters
* Pixel values (inside window):

 A1     A2     A3     A4     B1     B2     B3     B4     C1     C2     C3
  C4     C5     C6    D
116928   5670  16614   6823  27917   7547    197   9354 132658 405515
1016 136784 576913 113978 194896

* *The distribution of the number of cells per zone is the same than the
original file *

However, when I used the layer within the ppm function, not all the
categories are included in the analysis:

*m1<-ppm(ag4u,~Z, covariates=list(Z=zone1), AreaInter(200))*

*coef(summary(m1))*

               Estimate
(Intercept) -16.4787854
ZA3           2.6334407
ZA4           1.4900159
ZB1           0.6177496
ZB2           0.3502884
ZB4           1.4179890
ZC1          -2.0643563
ZC2          -0.6806136
ZC4          -0.1897898
ZC5          -2.8285278
ZC6           1.5300109
ZD            2.1210203
Interaction   2.4118998

The zones identified as A1, A2, B3, C3 are excluded from the analysis

Similarly, I get the same results when I used the following expression:

*m2<-ppm(ag4u,~factor(Z),covariates=list(Z=zone),  AreaInter(200)) *

And the following error when I tried to plot the residuals...

*qqplot.ppm(m1,nsim=100,verbose=F)*

Error in model.frame.default(Terms, newdata, na.action = na.action, xlev =
object$xlevels) :
  factor Z has new levels A2, C3

So, I think that the problem could be associated with functions I am
employing to assign the factor-values. Is this is the problem, i*s there an
alternative to define categorical values for im-objects? Or, it could be
other reason for the exclusion of categories?*

I will appreciate any advise.

Silvia Cordero

Only in case, here a link with the data
https://www.dropbox.com/sh/7t9ga3lmsx9ub0y/AACegUGCwXq6F7Gxn3elcBU9a?dl=0

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Tue Mar 10 00:55:04 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 10 Mar 2015 12:55:04 +1300
Subject: [R-sig-Geo] categorical values in im-objects (spatstat/ppm)
In-Reply-To: <CAORmci-vKjTD98vwXdKDAXgVHD-q4AX4s+okc-JdjuOy39=UxA@mail.gmail.com>
References: <CAORmci-vKjTD98vwXdKDAXgVHD-q4AX4s+okc-JdjuOy39=UxA@mail.gmail.com>
Message-ID: <54FE32D8.5020600@auckland.ac.nz>



I cannot make head nor tail of your question.

I don't understand your use of eval.im() --- that function is
intended to do calculations on objects that are (already) of class "im".

E.g. if X is an image with real positive pixel values then

    Y <- eval.im(sqrt(Y))

is an image whose pixel values are the square roots of the corresponding 
pixel values from X.

The help for im() in spatstat explains pretty clearly how to create a 
factor valued image.

I tried to have a look at the data to which you gave a link at the end
of your email.  The "owin.csv" file has 44 lines, all but 6 of which are
missing.  If the missing lines are eliminated the resulting window looks 
like a triangle, despite having 6 vertices.

Of the 224 points in "events.csv", 208 fall outside the window referred 
to above.

We can't really advise you about "zones" unless you provide "zones" 
which you seem not to have done.

I suggest that you back off, get a coherent story together, provide a 
complete and consistent set of data, and then ask again.

cheers,

Rolf Turner

On 10/03/15 11:02, Silvia Cordero-Sancho wrote:
> Hello,
>
> I will like to fit a point process model (ppm) with several covariates. One
> of them is a grid with 15 categorical variables (zones).
>
> To recognized the values in my grid as categorical, I followed the code in
> the following link:
>
> http://stackoverflow.com/questions/26162955/r-package-spatstat-how-to-use-point-process-model-covariate-as-factor-when-pixe?answertab=active#tab-top
>
>
>
>
> *zone1<-eval.im <http://eval.im>(as.factor(zone))*
>
> *levels(zone1)<-c("A1","A2","A3","A4","B1","B2","B3","B4",*
> *              "C1","C2","C3","C4","C5","C6","D")*
>
> *unitname(zone1)<-c("meter","meters")*
>
> But I am not sure if it really worked. If I run the function
> *is.factor(zone1)*, the result is FALSE, but if I run the function
> selecting any column or row (e.g. *is.factor(zone1[1,])* or
> *is.factor(zone1[,200])*) the results show as TRUE.
>
> However, the function *summary(zone1)* indicates that it is a factor value
> pixel image:
>
> factor-valued pixel image
> 2641 x 680 pixel array (ny, nx)
> enclosing rectangle: [992380, 1012780] x [732491, 811721] meters
> dimensions of each pixel: 30 x 30 meters
> Image is defined on a subset of the rectangular grid
> Subset area = 1577529000 square meters
> * Pixel values (inside window):
>
>   A1     A2     A3     A4     B1     B2     B3     B4     C1     C2     C3
>    C4     C5     C6    D
> 116928   5670  16614   6823  27917   7547    197   9354 132658 405515
> 1016 136784 576913 113978 194896
>
> * *The distribution of the number of cells per zone is the same than the
> original file *
>
> However, when I used the layer within the ppm function, not all the
> categories are included in the analysis:
>
> *m1<-ppm(ag4u,~Z, covariates=list(Z=zone1), AreaInter(200))*
>
> *coef(summary(m1))*
>
>                 Estimate
> (Intercept) -16.4787854
> ZA3           2.6334407
> ZA4           1.4900159
> ZB1           0.6177496
> ZB2           0.3502884
> ZB4           1.4179890
> ZC1          -2.0643563
> ZC2          -0.6806136
> ZC4          -0.1897898
> ZC5          -2.8285278
> ZC6           1.5300109
> ZD            2.1210203
> Interaction   2.4118998
>
> The zones identified as A1, A2, B3, C3 are excluded from the analysis
>
> Similarly, I get the same results when I used the following expression:
>
> *m2<-ppm(ag4u,~factor(Z),covariates=list(Z=zone),  AreaInter(200)) *
>
> And the following error when I tried to plot the residuals...
>
> *qqplot.ppm(m1,nsim=100,verbose=F)*
>
> Error in model.frame.default(Terms, newdata, na.action = na.action, xlev =
> object$xlevels) :
>    factor Z has new levels A2, C3
>
> So, I think that the problem could be associated with functions I am
> employing to assign the factor-values. Is this is the problem, i*s there an
> alternative to define categorical values for im-objects? Or, it could be
> other reason for the exclusion of categories?*
>
> I will appreciate any advise.


-- 
Rolf Turner
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276
Home phone: +64-9-480-4619


From hrbuilder at hotmail.com  Tue Mar 10 07:20:56 2015
From: hrbuilder at hotmail.com (Alec Friedhoff)
Date: Tue, 10 Mar 2015 01:20:56 -0500
Subject: [R-sig-Geo] Error importing ESRI Shapefile
Message-ID: <BLU170-W1349A5EECCE22F2FA91D484B4180@phx.gbl>

Hi all,
I'm trying to read in a Shapefile using the rgdal library, and am unable to do so.When using the following code, geo <- readOGR("/path/to/layer","layer"),I'm met with the error "Error in stopifnot(is.list(srl)) : infinite label point"
This one has me stumped. My searches haven't yielded any useful information about this error. I suspect there are some bad geometries in the underlying file. I have access to ArcMap and tried running the RepairGeometry_management method of arcpy, but this has no effect.
How can I go about understanding this error and ultimately fixing it?
Many thanks!
-HR 		 	   		  
	[[alternative HTML version deleted]]


From frtog at vestas.com  Tue Mar 10 09:49:55 2015
From: frtog at vestas.com (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Tue, 10 Mar 2015 09:49:55 +0100
Subject: [R-sig-Geo] Error importing ESRI Shapefile
In-Reply-To: <BLU170-W1349A5EECCE22F2FA91D484B4180@phx.gbl>
References: <BLU170-W1349A5EECCE22F2FA91D484B4180@phx.gbl>
Message-ID: <B078CDF40DFE4045AF172A8B4F68FC48588000A857@DKRDSEXC016.vestas.net>

Hi Alec

It is very impolite to cross post you questions. Besides the r-sig-geo you have probably posted on

1) http://onlinedevs.com/problem-reading-shapefile-using-r/ 
2) http://gis.stackexchange.com/questions/138217/problem-reading-shapefile-using-r 
3) http://stackoverflow.com/questions/28957014/error-reading-in-esri-shapefile-using-r 

Are those sites sharing information? Do people on those sites see my specific answer to your question on r-sig-geo?

Also please do not post in html but in plain text. Set your email client to post in text when posting to r-sig-geo.

Nobody can really help you. You give no information from which we can see why you get this error message.

To make you help us to help you please do something along this route. If you get an error please issue an

> traceback()

at command prompt in the R shell. Now do this at R prompt:

> library(rgdal)

and post the output from that call probably something like this:

" Loading required package: sp
rgdal: version: 0.9-1, (SVN revision 518)
Geospatial Data Abstraction Library extensions to R successfully loaded
Loaded GDAL runtime: GDAL 1.11.1, released 2014/09/24
Path to GDAL shared files: c:/Programmer/R/R-3.1.2/library/rgdal/gdal
GDAL does not use iconv for recoding strings.
Loaded PROJ.4 runtime: Rel. 4.8.0, 6 March 2012, [PJ_VERSION: 480]
Path to PROJ.4 shared files: c:/Programmer/R/R-3.1.2/library/rgdal/proj"

> sessionInfo()

Post the output!

> ogrDrivers()

> dsn <- "/path/to/layer"

> file.choose(dsn)

Can you see any shapefiles?

> ogrListLayers(dsn)

Can you see any layer names?

> ogrInfo(dsn=dsn, layer="layer")

Any information from this call?


Yours sincerely / Med venlig hilsen


Frede Aakmann T?gersen
Specialist, M.Sc., Ph.D.
Plant Performance & Modeling

Technology & Service Solutions
T +45 9730 5135
M +45 2547 6050
frtog at vestas.com
http://www.vestas.com

Company reg. name: Vestas Wind Systems A/S
This e-mail is subject to our e-mail disclaimer statement.
Please refer to www.vestas.com/legal/notice
If you have received this e-mail in error please contact the sender. 

> -----Original Message-----
> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Alec
> Friedhoff
> Sent: 10. marts 2015 07:21
> To: r-sig-geo at r-project.org
> Subject: [R-sig-Geo] Error importing ESRI Shapefile
> 
> Hi all,
> I'm trying to read in a Shapefile using the rgdal library, and am unable to do
> so.When using the following code, geo <-
> readOGR("/path/to/layer","layer"),I'm met with the error "Error in
> stopifnot(is.list(srl)) : infinite label point"
> This one has me stumped. My searches haven't yielded any useful
> information about this error. I suspect there are some bad geometries in the
> underlying file. I have access to ArcMap and tried running the
> RepairGeometry_management method of arcpy, but this has no effect.
> How can I go about understanding this error and ultimately fixing it?
> Many thanks!
> -HR
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From hrbuilder at hotmail.com  Tue Mar 10 15:46:40 2015
From: hrbuilder at hotmail.com (Alec Friedhoff)
Date: Tue, 10 Mar 2015 09:46:40 -0500
Subject: [R-sig-Geo] Error importing ESRI Shapefile
In-Reply-To: <B078CDF40DFE4045AF172A8B4F68FC48588000A857@DKRDSEXC016.vestas.net>
References: <BLU170-W1349A5EECCE22F2FA91D484B4180@phx.gbl>,
	<B078CDF40DFE4045AF172A8B4F68FC48588000A857@DKRDSEXC016.vestas.net>
Message-ID: <BLU170-W19DA0C4436174A61D14343B4180@phx.gbl>

Hi Frede,

My apologies for the incomplete question. I did post on stackoverflow and stackexchange, but have deleted the latter post (I've never visited onlinedevs.com). For what it's worth, I do update questions I've asked on sites like stackoverflow if I find answers elsewhere, but I do understand your general annoyance.

Here's some more information:

Output of traceback():
4: .Call("make_Polygonlist", iG, gComments[[i]], PACKAGE = "rgdal")
3: stopifnot(is.list(srl))
2: Polygons(.Call("make_Polygonlist", iG, gComments[[i]], PACKAGE = "rgdal"), 
       ID = as.character(fids[i]))
1: readOGR("/geo/layers", "tracts")

Message I receive upon loading of rgdal library:
Loading required package: sp
rgdal: version: 0.9-1, (SVN revision 518)
Geospatial Data Abstraction Library extensions to R successfully loaded
Loaded GDAL runtime: GDAL 1.10.1, released 2013/08/26
Path to GDAL shared files: /usr/share/gdal/1.10
Loaded PROJ.4 runtime: Rel. 4.8.0, 6 March 2012, [PJ_VERSION: 480]
Path to PROJ.4 shared files: (autodetected)

ogrDrivers() reveals I do have Shapefile support:
    name write
1      AeronavFAA FALSE
2          ARCGEN FALSE
3          AVCBin FALSE
4          AVCE00 FALSE
5             BNA  TRUE
6         CouchDB  TRUE
7             CSV  TRUE
8             DGN  TRUE
9            DODS FALSE
10            DXF  TRUE
11         EDIGEO FALSE
12  ElasticSearch  TRUE
13 ESRI Shapefile  TRUE
14     Geoconcept  TRUE
...

Output of sessionInfo():
R version 3.1.2 (2014-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] rgdal_0.9-1 sp_1.0-17  

loaded via a namespace (and not attached):
[1] grid_3.1.2      lattice_0.20-30 tools_3.1.2  


Output of ogrListLayers():
[1] "tracts"
attr(,"driver")
[1] "ESRI Shapefile"
attr(,"nlayers")
[1] 1


Output of ogrInfo():
Source: "/geo/layers", layer: "tracts"
Driver: ESRI Shapefile number of rows 73682 
Feature type: wkbPolygon with 2 dimensions
Extent: (-179.1473 17.88124) - (179.7785 71.39048)
CRS: +proj=longlat +datum=WGS84 +no_defs  
LDID: 0 
Number of fields: 34 
         name type length typeName
1        FIPS    4     11   String
2        SQMI    2     16     Real
...

Any help is greatly appreciated.

Best,
Alec

> From: frtog at vestas.com
> To: hrbuilder at hotmail.com; r-sig-geo at r-project.org
> Date: Tue, 10 Mar 2015 09:49:55 +0100
> Subject: RE: [R-sig-Geo] Error importing ESRI Shapefile
> 
> Hi Alec
> 
> It is very impolite to cross post you questions. Besides the r-sig-geo you have probably posted on
> 
> 1) http://onlinedevs.com/problem-reading-shapefile-using-r/ 
> 2) http://gis.stackexchange.com/questions/138217/problem-reading-shapefile-using-r 
> 3) http://stackoverflow.com/questions/28957014/error-reading-in-esri-shapefile-using-r 
> 
> Are those sites sharing information? Do people on those sites see my specific answer to your question on r-sig-geo?
> 
> Also please do not post in html but in plain text. Set your email client to post in text when posting to r-sig-geo.
> 
> Nobody can really help you. You give no information from which we can see why you get this error message.
> 
> To make you help us to help you please do something along this route. If you get an error please issue an
> 
>> traceback()
> 
> at command prompt in the R shell. Now do this at R prompt:
> 
>> library(rgdal)
> 
> and post the output from that call probably something like this:
> 
> " Loading required package: sp
> rgdal: version: 0.9-1, (SVN revision 518)
> Geospatial Data Abstraction Library extensions to R successfully loaded
> Loaded GDAL runtime: GDAL 1.11.1, released 2014/09/24
> Path to GDAL shared files: c:/Programmer/R/R-3.1.2/library/rgdal/gdal
> GDAL does not use iconv for recoding strings.
> Loaded PROJ.4 runtime: Rel. 4.8.0, 6 March 2012, [PJ_VERSION: 480]
> Path to PROJ.4 shared files: c:/Programmer/R/R-3.1.2/library/rgdal/proj"
> 
>> sessionInfo()
> 
> Post the output!
> 
>> ogrDrivers()
> 
>> dsn <- "/path/to/layer"
> 
>> file.choose(dsn)
> 
> Can you see any shapefiles?
> 
>> ogrListLayers(dsn)
> 
> Can you see any layer names?
> 
>> ogrInfo(dsn=dsn, layer="layer")
> 
> Any information from this call?
> 
> 
> Yours sincerely / Med venlig hilsen
> 
> 
> Frede Aakmann T?gersen
> Specialist, M.Sc., Ph.D.
> Plant Performance & Modeling
> 
> Technology & Service Solutions
> T +45 9730 5135
> M +45 2547 6050
> frtog at vestas.com
> http://www.vestas.com
> 
> Company reg. name: Vestas Wind Systems A/S
> This e-mail is subject to our e-mail disclaimer statement.
> Please refer to www.vestas.com/legal/notice
> If you have received this e-mail in error please contact the sender. 
> 
>> -----Original Message-----
>> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Alec
>> Friedhoff
>> Sent: 10. marts 2015 07:21
>> To: r-sig-geo at r-project.org
>> Subject: [R-sig-Geo] Error importing ESRI Shapefile
>> 
>> Hi all,
>> I'm trying to read in a Shapefile using the rgdal library, and am unable to do
>> so.When using the following code, geo <-
>> readOGR("/path/to/layer","layer"),I'm met with the error "Error in
>> stopifnot(is.list(srl)) : infinite label point"
>> This one has me stumped. My searches haven't yielded any useful
>> information about this error. I suspect there are some bad geometries in the
>> underlying file. I have access to ArcMap and tried running the
>> RepairGeometry_management method of arcpy, but this has no effect.
>> How can I go about understanding this error and ultimately fixing it?
>> Many thanks!
>> -HR
>> [[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
 		 	   		  
	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Tue Mar 10 19:06:07 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 10 Mar 2015 19:06:07 +0100
Subject: [R-sig-Geo] Error importing ESRI Shapefile
In-Reply-To: <BLU170-W19DA0C4436174A61D14343B4180@phx.gbl>
References: <BLU170-W1349A5EECCE22F2FA91D484B4180@phx.gbl>,
	<B078CDF40DFE4045AF172A8B4F68FC48588000A857@DKRDSEXC016.vestas.net>
	<BLU170-W19DA0C4436174A61D14343B4180@phx.gbl>
Message-ID: <alpine.LFD.2.11.1503101850560.21388@reclus.nhh.no>

On Tue, 10 Mar 2015, Alec Friedhoff wrote:

> Hi Frede,
>
> My apologies for the incomplete question. I did post on stackoverflow and stackexchange, but have deleted the latter post (I've never visited onlinedevs.com). For what it's worth, I do update questions I've asked on sites like stackoverflow if I find answers elsewhere, but I do understand your general annoyance.
>
> Here's some more information:
>
> Output of traceback():
> 4: .Call("make_Polygonlist", iG, gComments[[i]], PACKAGE = "rgdal")
> 3: stopifnot(is.list(srl))
> 2: Polygons(.Call("make_Polygonlist", iG, gComments[[i]], PACKAGE = "rgdal"),
>       ID = as.character(fids[i]))
> 1: readOGR("/geo/layers", "tracts")

This detail helps, but does not provide a reproducible example. The error 
message said: "infinite label point" which comes from a validity check on 
a Polygon or possibly Polygons object. The C code in the sp package, 
called from rgdal, computes the centroid, but appears here to fail because 
the area of the polygon is not close to zero. Close to zero cases (a 
repeated point trying to pretend to be a polygon, or a short line doing 
the same) are covered. This means that your polygon is the first ever 
encountered in this unusual state. Either make the offending shapefile 
available on a public server and post the link, or contact me off-list 
with that link, or with a zip archive of the shapefile attached, again 
off-list. I'll try to see whether additional logic can be added to 
identify the bad geometries.

Roger

>
> Message I receive upon loading of rgdal library:
> Loading required package: sp
> rgdal: version: 0.9-1, (SVN revision 518)
> Geospatial Data Abstraction Library extensions to R successfully loaded
> Loaded GDAL runtime: GDAL 1.10.1, released 2013/08/26
> Path to GDAL shared files: /usr/share/gdal/1.10
> Loaded PROJ.4 runtime: Rel. 4.8.0, 6 March 2012, [PJ_VERSION: 480]
> Path to PROJ.4 shared files: (autodetected)
>
> ogrDrivers() reveals I do have Shapefile support:
>    name write
> 1      AeronavFAA FALSE
> 2          ARCGEN FALSE
> 3          AVCBin FALSE
> 4          AVCE00 FALSE
> 5             BNA  TRUE
> 6         CouchDB  TRUE
> 7             CSV  TRUE
> 8             DGN  TRUE
> 9            DODS FALSE
> 10            DXF  TRUE
> 11         EDIGEO FALSE
> 12  ElasticSearch  TRUE
> 13 ESRI Shapefile  TRUE
> 14     Geoconcept  TRUE
> ...
>
> Output of sessionInfo():
> R version 3.1.2 (2014-10-31)
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> locale:
> [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
> [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rgdal_0.9-1 sp_1.0-17
>
> loaded via a namespace (and not attached):
> [1] grid_3.1.2      lattice_0.20-30 tools_3.1.2
>
>
> Output of ogrListLayers():
> [1] "tracts"
> attr(,"driver")
> [1] "ESRI Shapefile"
> attr(,"nlayers")
> [1] 1
>
>
> Output of ogrInfo():
> Source: "/geo/layers", layer: "tracts"
> Driver: ESRI Shapefile number of rows 73682
> Feature type: wkbPolygon with 2 dimensions
> Extent: (-179.1473 17.88124) - (179.7785 71.39048)
> CRS: +proj=longlat +datum=WGS84 +no_defs
> LDID: 0
> Number of fields: 34
>         name type length typeName
> 1        FIPS    4     11   String
> 2        SQMI    2     16     Real
> ...
>
> Any help is greatly appreciated.
>
> Best,
> Alec
>
>> From: frtog at vestas.com
>> To: hrbuilder at hotmail.com; r-sig-geo at r-project.org
>> Date: Tue, 10 Mar 2015 09:49:55 +0100
>> Subject: RE: [R-sig-Geo] Error importing ESRI Shapefile
>>
>> Hi Alec
>>
>> It is very impolite to cross post you questions. Besides the r-sig-geo you have probably posted on
>>
>> 1) http://onlinedevs.com/problem-reading-shapefile-using-r/
>> 2) http://gis.stackexchange.com/questions/138217/problem-reading-shapefile-using-r
>> 3) http://stackoverflow.com/questions/28957014/error-reading-in-esri-shapefile-using-r
>>
>> Are those sites sharing information? Do people on those sites see my specific answer to your question on r-sig-geo?
>>
>> Also please do not post in html but in plain text. Set your email client to post in text when posting to r-sig-geo.
>>
>> Nobody can really help you. You give no information from which we can see why you get this error message.
>>
>> To make you help us to help you please do something along this route. If you get an error please issue an
>>
>>> traceback()
>>
>> at command prompt in the R shell. Now do this at R prompt:
>>
>>> library(rgdal)
>>
>> and post the output from that call probably something like this:
>>
>> " Loading required package: sp
>> rgdal: version: 0.9-1, (SVN revision 518)
>> Geospatial Data Abstraction Library extensions to R successfully loaded
>> Loaded GDAL runtime: GDAL 1.11.1, released 2014/09/24
>> Path to GDAL shared files: c:/Programmer/R/R-3.1.2/library/rgdal/gdal
>> GDAL does not use iconv for recoding strings.
>> Loaded PROJ.4 runtime: Rel. 4.8.0, 6 March 2012, [PJ_VERSION: 480]
>> Path to PROJ.4 shared files: c:/Programmer/R/R-3.1.2/library/rgdal/proj"
>>
>>> sessionInfo()
>>
>> Post the output!
>>
>>> ogrDrivers()
>>
>>> dsn <- "/path/to/layer"
>>
>>> file.choose(dsn)
>>
>> Can you see any shapefiles?
>>
>>> ogrListLayers(dsn)
>>
>> Can you see any layer names?
>>
>>> ogrInfo(dsn=dsn, layer="layer")
>>
>> Any information from this call?
>>
>>
>> Yours sincerely / Med venlig hilsen
>>
>>
>> Frede Aakmann T???gersen
>> Specialist, M.Sc., Ph.D.
>> Plant Performance & Modeling
>>
>> Technology & Service Solutions
>> T +45 9730 5135
>> M +45 2547 6050
>> frtog at vestas.com
>> http://www.vestas.com
>>
>> Company reg. name: Vestas Wind Systems A/S
>> This e-mail is subject to our e-mail disclaimer statement.
>> Please refer to www.vestas.com/legal/notice
>> If you have received this e-mail in error please contact the sender.
>>
>>> -----Original Message-----
>>> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Alec
>>> Friedhoff
>>> Sent: 10. marts 2015 07:21
>>> To: r-sig-geo at r-project.org
>>> Subject: [R-sig-Geo] Error importing ESRI Shapefile
>>>
>>> Hi all,
>>> I'm trying to read in a Shapefile using the rgdal library, and am unable to do
>>> so.When using the following code, geo <-
>>> readOGR("/path/to/layer","layer"),I'm met with the error "Error in
>>> stopifnot(is.list(srl)) : infinite label point"
>>> This one has me stumped. My searches haven't yielded any useful
>>> information about this error. I suspect there are some bad geometries in the
>>> underlying file. I have access to ArcMap and tried running the
>>> RepairGeometry_management method of arcpy, but this has no effect.
>>> How can I go about understanding this error and ultimately fixing it?
>>> Many thanks!
>>> -HR
>>> [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no


From elgabass at gmail.com  Wed Mar 11 13:19:55 2015
From: elgabass at gmail.com (Ahmed El-Gabbas)
Date: Wed, 11 Mar 2015 13:19:55 +0100
Subject: [R-sig-Geo] dismo package : gbif function error
In-Reply-To: <CANtt_hywDeNkzWbCQVaDhWMCKbJiAgQ57pHU_ZkS36syMxq+zg@mail.gmail.com>
References: <CAJEGaydtAmnysBuqM8=Xwc-=rvp0w4bR6qLrKvjNPvjEkqzHbg@mail.gmail.com>
	<CANtt_hywDeNkzWbCQVaDhWMCKbJiAgQ57pHU_ZkS36syMxq+zg@mail.gmail.com>
Message-ID: <CAJEGaydbDeb_JsFA0ahMbeFS6L2faYkhhCOGG9nP=xB0NdP4cA@mail.gmail.com>

It works,
Thanks a lot.

Ahmed

On Mon, Mar 9, 2015 at 8:57 PM, Robert J. Hijmans <r.hijmans at gmail.com>
wrote:

> The web-service that the function depends on has changed. It should
> work with the development version of dismo (version 1.0-8). You should
> be able to install it from R-Forge like this:
>
> install.packages("dismo", repos="http://R-Forge.R-project.org")
>
> Robert
>
> On Sun, Mar 8, 2015 at 11:42 AM, Ahmed El-Gabbas <elgabass at gmail.com>
> wrote:
> > Hello all,
> > I am having an unusual problem using gbif function of the dismo package.
> >
> > When I am using this code: acaule = gbif("solanum", "acaule*", geo=FALSE)
> > this error appears: "
> > http://data.gbif.org/ws/rest/occurrence/count?scientificname=solanum+
> > Error in gbif("solanum", "", geo = FALSE) : invalid request"
> >
> > It seems that this is a problem of accessing to the GBIF API not a
> problem
> > of dismo package itself. The website used by the gbif function to get the
> > records from the GBIF website "http://data.gbif.org/" is no longer
> usable
> > and gives this error when accessing it using regular browser "
> data.gbif.org
> > has now been decommissioned. Please use www.gbif.org for all data
> access "
> >
> > Any suggestions,
> > Regards,
> > Ahmed
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From williamcmay at live.com  Wed Mar 11 19:08:00 2015
From: williamcmay at live.com (William May)
Date: Wed, 11 Mar 2015 13:08:00 -0500
Subject: [R-sig-Geo] Local Moran p-values in Geoda, Python, and R
Message-ID: <BAY179-W28333F50E31A452FB4AD8BCE190@phx.gbl>

I sent this to Roger Bivand earlier. Posting it here for
posterity, and in case other people have more to add:

---------------------------------------------------------------------------
Hi Roger,

I'm taking a spatial econometrics class, and we've been making some
LISA maps. We noticed that the p-values from the spdep package seem to
be a lot different than the results from Geoda or from Python's PySAL
package.

I looked through the function definitions and noticed that Python, and
I think Geoda, use simulations to estimate the p-value, while your
localmoran function uses an equation.

I attached the graphs from Geoda and from R, and my R code and the
original dataset. [R code is added below.] Is this kind of
difference normal?

Is there a reason to prefer spdep's results over the Geoda results (or
vice versa)?

Thanks for any help,

Will
---------------------------------------------------------------------------


Here's his response:

---------------------------------------------------------------------------
Please *do* write to the R-sig-geo list rather than to me directly -
others can answer your question as well, perhaps better, and in a more
timely way than I can. In addition, threads in the list can be
searched in the archives, so others can avoid the same problem later.

The only reliable test is localmoran.exact(), followed by
localmoran.sad(), for reasons given in the references in their help
pages.? If you test more than one observation, remember to adjust the
p-value for multiple comparisons. Sampling _i (all but i)
overrepresents non-neighbours of i for a test on i. The formulae used
are from the original paper and most often are similar to the
permutation bootstrap values.

Crucially, almost all use of local Moran's I (and local Geary and
Getis) fall foul of the autocorrelation being related to a
mis-specified mean model (omitted explanatory variables and/or omitted
adjustment for global autocorrelation). Look at the references to
localmoran.sad and localmoran.exact for more details, and at the
relevant parts of Waller & Gotway (2004), Schabenberger & Gotway
(2005), and in Bivand et al. (2008, 2013, ASDAR-book, ch. 9).

LISA maps are extremely misleading when the mean model is
mis-specified, and when the p-values are not adjusted for multiple
comparisons. They look simple, but because adjustment for multiple
comparisons is a subjective choice, you can almost choose the map you
want.

Roger
---------------------------------------------------------------------------


And here's my R code:

---------------------------------------------------------------------------
# make a LISA cluster map

library(maptools)
library(spdep)

setwd('/home/will/classes/polisci_610/1/')

# get the data
nat <- readShapeSpatial("NAT.SHP", ID="FIPSNO")
nat.data<-data.frame(nat)
attach(nat.data)

# nearest neighbors (nnb)
coords<-coordinates(nat)
IDs<-row.names(as(nat, "data.frame"))
nat_10nnb<-knn2nb(knearneigh(coords, k=10), row.names=IDs)

# "W" identifies row standardized weights
nat_10nnb_w <- nb2listw(nat_10nnb, style="W")

# can preset list_w object; if do this, remember to change if needed
list_w <- nat_10nnb_w

# LISA values
lisa <- localmoran(HR60, list_w, zero.policy = T)

# centers the variable of interest around its mean
cDV <- HR60 - mean(HR60) 

# centers the local Moran's around the mean
mI <- lisa[, 1]
C_mI <- mI - mean(mI) # but we don't want to center it! Only the sign
# matters.

quadrant <- vector(mode="numeric",length=nrow(lisa))
quadrant[cDV>0 & mI>0] <- 1 
quadrant[cDV <0 & mI>0] <- 2????? 
quadrant[cDV>0 & mI<0] <- 3
quadrant[cDV <0 & mI<0] <- 4

# set a statistical significance level for the local Moran's
signif <- 0.05

# places non-significant Moran's in the category "5"
quadrant[lisa[, 5]> signif] <- 5 


# map
png(file="Lisa_map_R.png", width = 680, res = 100)
colors <- c("red", "blue", "lightpink", "skyblue2", rgb(.95, .95, .95))
par(mar=c(0,0,1,0)) # sets margin parameters for plot space
plot(nat, border="grey", col=colors[quadrant],
??? ? main = "LISA Cluster Map, 1960 Homicides")
legend("bottomright",legend=c("high-high","low-low","high-low","low-high"),
?????? fill=colors,bty="n",cex=0.7,y.intersp=1,x.intersp=1)
dev.off()
---------------------------------------------------------------------------
 		 	   		  

From Roger.Bivand at nhh.no  Wed Mar 11 19:28:11 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 11 Mar 2015 19:28:11 +0100
Subject: [R-sig-Geo] Local Moran p-values in Geoda, Python, and R
In-Reply-To: <BAY179-W28333F50E31A452FB4AD8BCE190@phx.gbl>
References: <BAY179-W28333F50E31A452FB4AD8BCE190@phx.gbl>
Message-ID: <alpine.LFD.2.11.1503111920420.28385@reclus.nhh.no>

On Wed, 11 Mar 2015, William May wrote:

> I sent this to Roger Bivand earlier. Posting it here for
> posterity, and in case other people have more to add:
>
> ---------------------------------------------------------------------------
> Hi Roger,
>
> I'm taking a spatial econometrics class, and we've been making some
> LISA maps. We noticed that the p-values from the spdep package seem to
> be a lot different than the results from Geoda or from Python's PySAL
> package.
>
> I looked through the function definitions and noticed that Python, and
> I think Geoda, use simulations to estimate the p-value, while your
> localmoran function uses an equation.
>
> I attached the graphs from Geoda and from R, and my R code and the
> original dataset. [R code is added below.] Is this kind of
> difference normal?

If you would put the figures on a website, we could compare them. One 
possible reason for differences is the alternative= argument to 
localmoran(). It's default is "greater", but you could try "two.sided" to 
see whether this gives values closer to GeoDa. You don't say, by the way, 
which version of GeoDa you are using (since you mention Python, probably 
not legacy GeoDa), nor whether you can get the results out in tabular 
form. Are you using the same spatial weights - both can read and write GAL 
files. Are you aware that knearneigh() takes a longlat= argument, which 
must be used if the coordinates are not projected. The p-values in 
localmoran() are analytical ones from: Anselin, L. 1995. Local indicators 
of spatial association, Geographical Analysis, 27, 93--115. As you 
mention, in GeoDa they are by leave-one-out permutation bootstrapping.

Roger

>
> Is there a reason to prefer spdep's results over the Geoda results (or
> vice versa)?
>
> Thanks for any help,
>
> Will
> ---------------------------------------------------------------------------
>
>
> Here's his response:
>
> ---------------------------------------------------------------------------
> Please *do* write to the R-sig-geo list rather than to me directly -
> others can answer your question as well, perhaps better, and in a more
> timely way than I can. In addition, threads in the list can be
> searched in the archives, so others can avoid the same problem later.
>
> The only reliable test is localmoran.exact(), followed by
> localmoran.sad(), for reasons given in the references in their help
> pages.? If you test more than one observation, remember to adjust the
> p-value for multiple comparisons. Sampling _i (all but i)
> overrepresents non-neighbours of i for a test on i. The formulae used
> are from the original paper and most often are similar to the
> permutation bootstrap values.
>
> Crucially, almost all use of local Moran's I (and local Geary and
> Getis) fall foul of the autocorrelation being related to a
> mis-specified mean model (omitted explanatory variables and/or omitted
> adjustment for global autocorrelation). Look at the references to
> localmoran.sad and localmoran.exact for more details, and at the
> relevant parts of Waller & Gotway (2004), Schabenberger & Gotway
> (2005), and in Bivand et al. (2008, 2013, ASDAR-book, ch. 9).
>
> LISA maps are extremely misleading when the mean model is
> mis-specified, and when the p-values are not adjusted for multiple
> comparisons. They look simple, but because adjustment for multiple
> comparisons is a subjective choice, you can almost choose the map you
> want.
>
> Roger
> ---------------------------------------------------------------------------
>
>
> And here's my R code:
>
> ---------------------------------------------------------------------------
> # make a LISA cluster map
>
> library(maptools)
> library(spdep)
>
> setwd('/home/will/classes/polisci_610/1/')
>
> # get the data
> nat <- readShapeSpatial("NAT.SHP", ID="FIPSNO")
> nat.data<-data.frame(nat)
> attach(nat.data)
>
> # nearest neighbors (nnb)
> coords<-coordinates(nat)
> IDs<-row.names(as(nat, "data.frame"))
> nat_10nnb<-knn2nb(knearneigh(coords, k=10), row.names=IDs)
>
> # "W" identifies row standardized weights
> nat_10nnb_w <- nb2listw(nat_10nnb, style="W")
>
> # can preset list_w object; if do this, remember to change if needed
> list_w <- nat_10nnb_w
>
> # LISA values
> lisa <- localmoran(HR60, list_w, zero.policy = T)
>
> # centers the variable of interest around its mean
> cDV <- HR60 - mean(HR60)
>
> # centers the local Moran's around the mean
> mI <- lisa[, 1]
> C_mI <- mI - mean(mI) # but we don't want to center it! Only the sign
> # matters.
>
> quadrant <- vector(mode="numeric",length=nrow(lisa))
> quadrant[cDV>0 & mI>0] <- 1
> quadrant[cDV <0 & mI>0] <- 2?????
> quadrant[cDV>0 & mI<0] <- 3
> quadrant[cDV <0 & mI<0] <- 4
>
> # set a statistical significance level for the local Moran's
> signif <- 0.05
>
> # places non-significant Moran's in the category "5"
> quadrant[lisa[, 5]> signif] <- 5
>
>
> # map
> png(file="Lisa_map_R.png", width = 680, res = 100)
> colors <- c("red", "blue", "lightpink", "skyblue2", rgb(.95, .95, .95))
> par(mar=c(0,0,1,0)) # sets margin parameters for plot space
> plot(nat, border="grey", col=colors[quadrant],
> ??? ? main = "LISA Cluster Map, 1960 Homicides")
> legend("bottomright",legend=c("high-high","low-low","high-low","low-high"),
> ?????? fill=colors,bty="n",cex=0.7,y.intersp=1,x.intersp=1)
> dev.off()
> ---------------------------------------------------------------------------
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no

From williamcmay at live.com  Thu Mar 12 02:51:16 2015
From: williamcmay at live.com (William May)
Date: Wed, 11 Mar 2015 20:51:16 -0500
Subject: [R-sig-Geo] Local Moran p-values in Geoda, Python, and R
In-Reply-To: <alpine.LFD.2.11.1503111920420.28385@reclus.nhh.no>
References: <BAY179-W28333F50E31A452FB4AD8BCE190@phx.gbl>,
	<alpine.LFD.2.11.1503111920420.28385@reclus.nhh.no>
Message-ID: <BAY179-W9183D962097A6770C4DC2CCE060@phx.gbl>

Here's the R LISA map:
http://willonrails.com/images/Lisa_map_R.png

And here's the Geoda version:
http://willonrails.com/images/Lisa_map_geoda.png

I'm using Geoda 1.6.6. Since we were getting inconsistent answers
between R and Geoda, the professor ran the analysis in Python to
get a third opinion. I don't have the Python version of the map
on me, but it's very similar to Geoda's.

I was able to export Geoda's local I and p-values. I put up a zip file
with all the data, R script, the maps, and the spatial weights files
for Geoda:
willonrails.com/images/lisa_spdep.zip

The local Moran's I in R and Geoda are very similar -- on average they
only differ by 0.000290044.

The p-values, on average, differ by 0.1808931.

In both I used 10 nearest neighbors for the weights. I did create the
weights separately in R and Geoda. The Geoda weights are in the zip
file. I'm new to coordinate projections, but here's the relevant part
of the R script:

------------------------------------------------
# nearest neighbors (nnb)
coords <- coordinates(nat)
IDs <- row.names(as(nat, "data.frame"))
nat_10nnb <- knn2nb(knearneigh(coords, k=10), row.names=IDs)
------------------------------------------------

'nat' is an object of class SpatialPolygonsDataFrame, so if I
understand correctly I don't need to set the 'longlat' argument in
knearneigh. I also plotted a random selection of the counties in R,
along with their neighboring counties from the weights matrix, and
everything appeared to be working correctly.

I also tried using Geoda's weights in R. Surprisingly, this map is
even sparser (higher p-values) than the first map I made in R. I'm
tempted to think I mixed up the data, but the map looks correct
when I plot all counties regardless of p-value.








----------------------------------------
> Date: Wed, 11 Mar 2015 19:28:11 +0100
> From: Roger.Bivand at nhh.no
> To: williamcmay at live.com
> CC: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Local Moran p-values in Geoda, Python, and R
>
> On Wed, 11 Mar 2015, William May wrote:
>
>> I sent this to Roger Bivand earlier. Posting it here for
>> posterity, and in case other people have more to add:
>>
>> ---------------------------------------------------------------------------
>> Hi Roger,
>>
>> I'm taking a spatial econometrics class, and we've been making some
>> LISA maps. We noticed that the p-values from the spdep package seem to
>> be a lot different than the results from Geoda or from Python's PySAL
>> package.
>>
>> I looked through the function definitions and noticed that Python, and
>> I think Geoda, use simulations to estimate the p-value, while your
>> localmoran function uses an equation.
>>
>> I attached the graphs from Geoda and from R, and my R code and the
>> original dataset. [R code is added below.] Is this kind of
>> difference normal?
>
> If you would put the figures on a website, we could compare them. One
> possible reason for differences is the alternative= argument to
> localmoran(). It's default is "greater", but you could try "two.sided" to
> see whether this gives values closer to GeoDa. You don't say, by the way,
> which version of GeoDa you are using (since you mention Python, probably
> not legacy GeoDa), nor whether you can get the results out in tabular
> form. Are you using the same spatial weights - both can read and write GAL
> files. Are you aware that knearneigh() takes a longlat= argument, which
> must be used if the coordinates are not projected. The p-values in
> localmoran() are analytical ones from: Anselin, L. 1995. Local indicators
> of spatial association, Geographical Analysis, 27, 93--115. As you
> mention, in GeoDa they are by leave-one-out permutation bootstrapping.
>
> Roger
>
>>
>> Is there a reason to prefer spdep's results over the Geoda results (or
>> vice versa)?
>>
>> Thanks for any help,
>>
>> Will
>> ---------------------------------------------------------------------------
>>
>>
>> Here's his response:
>>
>> ---------------------------------------------------------------------------
>> Please *do* write to the R-sig-geo list rather than to me directly -
>> others can answer your question as well, perhaps better, and in a more
>> timely way than I can. In addition, threads in the list can be
>> searched in the archives, so others can avoid the same problem later.
>>
>> The only reliable test is localmoran.exact(), followed by
>> localmoran.sad(), for reasons given in the references in their help
>> pages.  If you test more than one observation, remember to adjust the
>> p-value for multiple comparisons. Sampling _i (all but i)
>> overrepresents non-neighbours of i for a test on i. The formulae used
>> are from the original paper and most often are similar to the
>> permutation bootstrap values.
>>
>> Crucially, almost all use of local Moran's I (and local Geary and
>> Getis) fall foul of the autocorrelation being related to a
>> mis-specified mean model (omitted explanatory variables and/or omitted
>> adjustment for global autocorrelation). Look at the references to
>> localmoran.sad and localmoran.exact for more details, and at the
>> relevant parts of Waller & Gotway (2004), Schabenberger & Gotway
>> (2005), and in Bivand et al. (2008, 2013, ASDAR-book, ch. 9).
>>
>> LISA maps are extremely misleading when the mean model is
>> mis-specified, and when the p-values are not adjusted for multiple
>> comparisons. They look simple, but because adjustment for multiple
>> comparisons is a subjective choice, you can almost choose the map you
>> want.
>>
>> Roger
>> ---------------------------------------------------------------------------
>>
>>
>> And here's my R code:
>>
>> ---------------------------------------------------------------------------
>> # make a LISA cluster map
>>
>> library(maptools)
>> library(spdep)
>>
>> setwd('/home/will/classes/polisci_610/1/')
>>
>> # get the data
>> nat <- readShapeSpatial("NAT.SHP", ID="FIPSNO")
>> nat.data<-data.frame(nat)
>> attach(nat.data)
>>
>> # nearest neighbors (nnb)
>> coords<-coordinates(nat)
>> IDs<-row.names(as(nat, "data.frame"))
>> nat_10nnb<-knn2nb(knearneigh(coords, k=10), row.names=IDs)
>>
>> # "W" identifies row standardized weights
>> nat_10nnb_w <- nb2listw(nat_10nnb, style="W")
>>
>> # can preset list_w object; if do this, remember to change if needed
>> list_w <- nat_10nnb_w
>>
>> # LISA values
>> lisa <- localmoran(HR60, list_w, zero.policy = T)
>>
>> # centers the variable of interest around its mean
>> cDV <- HR60 - mean(HR60)
>>
>> # centers the local Moran's around the mean
>> mI <- lisa[, 1]
>> C_mI <- mI - mean(mI) # but we don't want to center it! Only the sign
>> # matters.
>>
>> quadrant <- vector(mode="numeric",length=nrow(lisa))
>> quadrant[cDV>0 & mI>0] <- 1
>> quadrant[cDV <0 & mI>0] <- 2
>> quadrant[cDV>0 & mI<0] <- 3
>> quadrant[cDV <0 & mI<0] <- 4
>>
>> # set a statistical significance level for the local Moran's
>> signif <- 0.05
>>
>> # places non-significant Moran's in the category "5"
>> quadrant[lisa[, 5]> signif] <- 5
>>
>>
>> # map
>> png(file="Lisa_map_R.png", width = 680, res = 100)
>> colors <- c("red", "blue", "lightpink", "skyblue2", rgb(.95, .95, .95))
>> par(mar=c(0,0,1,0)) # sets margin parameters for plot space
>> plot(nat, border="grey", col=colors[quadrant],
>>       main = "LISA Cluster Map, 1960 Homicides")
>> legend("bottomright",legend=c("high-high","low-low","high-low","low-high"),
>>        fill=colors,bty="n",cex=0.7,y.intersp=1,x.intersp=1)
>> dev.off()
>> ---------------------------------------------------------------------------
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; fax +47 55 95 91 00
> e-mail: Roger.Bivand at nhh.no
 		 	   		  

From hariom.cs1 at gmail.com  Thu Mar 12 06:37:25 2015
From: hariom.cs1 at gmail.com (hariom)
Date: Wed, 11 Mar 2015 22:37:25 -0700 (MST)
Subject: [R-sig-Geo] customized focal mean
Message-ID: <1426138645618-7587890.post@n2.nabble.com>

Hello list,

I want to customized focal mean in which my concern is to replace mean value
with the matrix such as:

*2 2 2* 2
*2 1 2* 1
*1 2 4* 2
2 1 2 2

first generate the mean of 3*3 matrix and then replace with entire matrix
and it will work throughout the raster data. 
first matrix mean is 2 and  it will replace and continue.....

 2 *2 2 2*
 2 *2 2 1* 
 2 *2 2 2*

 
Please help me i will appreciate for your time and concern

Hariom singh
Research scholar IIT Roorkee




--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/customized-focal-mean-tp7587890.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From karljarvis at gmail.com  Thu Mar 12 19:35:52 2015
From: karljarvis at gmail.com (karljarvis)
Date: Thu, 12 Mar 2015 11:35:52 -0700 (MST)
Subject: [R-sig-Geo] gdistance: costDistance with barriers
Message-ID: <1426185352133-7587891.post@n2.nabble.com>

Hi all,
I am seeing confusing behavior from the costDistance function in gdistance.
In general, when cost is constant, cost distance increases linearly with
actual distance. However, in this example, it is not doing that for the
longest few distances. Am I missing something?
Karl

r <- raster(nrows=18, ncols=36) 
r[] <- 1
t <- transition(r,function(x) 1/mean(x),4)
p <- cbind(seq(-120,120,by=40),rep(0,7))
costDistance(t,p)

> costDistance(t,p)
   1  2  3  4  5  6
2  4               
3  8  4            
4 12  8  4         
5 16 12  8  4      
6 16 16 12  8  4   
7 12 16 16 12  8  4



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/gdistance-costDistance-with-barriers-tp7587891.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From karljarvis at gmail.com  Thu Mar 12 19:41:23 2015
From: karljarvis at gmail.com (karljarvis)
Date: Thu, 12 Mar 2015 11:41:23 -0700 (MST)
Subject: [R-sig-Geo] gdistance: costDistance with barriers
In-Reply-To: <1426185352133-7587891.post@n2.nabble.com>
References: <1426185352133-7587891.post@n2.nabble.com>
Message-ID: <1426185683844-7587892.post@n2.nabble.com>

Apologies for the confusing subject line...



-----
Karl Jarvis
--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/gdistance-costDistance-with-barriers-tp7587891p7587892.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From jstachel at sfwmd.gov  Thu Mar 12 21:12:03 2015
From: jstachel at sfwmd.gov (Stachelek, Joseph)
Date: Thu, 12 Mar 2015 20:12:03 +0000
Subject: [R-sig-Geo] gdistance: costDistance with barriers
In-Reply-To: <1426185352133-7587891.post@n2.nabble.com>
References: <1426185352133-7587891.post@n2.nabble.com>
Message-ID: <D51374C4B889BC47B3C5286047C86DA196C02E89@whqembx03p.ad.sfwmd.gov>

Hi Karl,

I think the underlying shortest.paths algorithm is wrapping around the x dimension of the raster. I was able to fix by remove vertices in the underlying adjacency matrix.

library(gdistance)
r <- raster(nrows=18, ncols=36)
r[] <- 1
t <- transition(r,function(x) 1/mean(x),4)
p <- cbind(seq(-120,120,by=40),rep(0,7))
costDistance(t,p)


#my code
source("gdistance_source/internal-functions.R")
x<-t
fromCoords<-toCoords<-p

fromCoords <- .coordsToMatrix(fromCoords)
toCoords <- .coordsToMatrix(toCoords)

fromCells <- cellFromXY(x, fromCoords)
toCells <- cellFromXY(x, toCoords)

##
costDist <- matrix(NA, nrow=length(fromCoords[,1]),ncol=length(toCoords[,1]))
rownames(costDist) <- rownames(fromCoords)
colnames(costDist) <- rownames(toCoords)
y <- transitionMatrix(x)
#if(isSymmetric(y)) {m <- "undirected"} else{m <- "directed"}
adjacencyGraph <- graph.adjacency(y, mode="directed")
E(adjacencyGraph)$weight <- 1/E(adjacencyGraph)$weight

uniqueFromCells <- unique(fromCells)
uniqueToCells <- unique(toCells)

extent(r)
removepnts <- cbind(rep(180,181),seq(-90,90,by=40))
removecells <- cellFromXY(x, removepnts)
adjacencyGraph<-delete.vertices(adjacencyGraph,removecells)

shortest.paths(adjacencyGraph, v=uniqueFromCells, to=uniqueToCells, mode="out", algorithm="dijkstra")








-----Original Message-----
From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of karljarvis
Sent: Thursday, March 12, 2015 2:36 PM
To: r-sig-geo at r-project.org
Subject: [R-sig-Geo] gdistance: costDistance with barriers

Hi all,
I am seeing confusing behavior from the costDistance function in gdistance.
In general, when cost is constant, cost distance increases linearly with
actual distance. However, in this example, it is not doing that for the
longest few distances. Am I missing something?
Karl

r <- raster(nrows=18, ncols=36)
r[] <- 1
t <- transition(r,function(x) 1/mean(x),4)
p <- cbind(seq(-120,120,by=40),rep(0,7))
costDistance(t,p)

> costDistance(t,p)
   1  2  3  4  5  6
2  4
3  8  4
4 12  8  4
5 16 12  8  4
6 16 16 12  8  4
7 12 16 16 12  8  4



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/gdistance-costDistance-with-barriers-tp7587891.html
Sent from the R-sig-geo mailing list archive at Nabble.com.

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


We value your opinion. Please take a few minutes to share your comments on the service you received from the District by clicking on this link<http://my.sfwmd.gov/portal/page/portal/pg_grp_surveysystem/survey%20ext?pid=1653>.


From Roger.Bivand at nhh.no  Thu Mar 12 21:37:52 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 12 Mar 2015 21:37:52 +0100
Subject: [R-sig-Geo] Local Moran p-values in Geoda, Python, and R
In-Reply-To: <BAY179-W9183D962097A6770C4DC2CCE060@phx.gbl>
References: <BAY179-W28333F50E31A452FB4AD8BCE190@phx.gbl>,
	<alpine.LFD.2.11.1503111920420.28385@reclus.nhh.no>
	<BAY179-W9183D962097A6770C4DC2CCE060@phx.gbl>
Message-ID: <alpine.LFD.2.11.1503122023570.23947@reclus.nhh.no>

On Thu, 12 Mar 2015, William May wrote:

> Here's the R LISA map:
> http://willonrails.com/images/Lisa_map_R.png
>
> And here's the Geoda version:
> http://willonrails.com/images/Lisa_map_geoda.png
>
> I'm using Geoda 1.6.6. Since we were getting inconsistent answers
> between R and Geoda, the professor ran the analysis in Python to
> get a third opinion. I don't have the Python version of the map
> on me, but it's very similar to Geoda's.
>
> I was able to export Geoda's local I and p-values. I put up a zip file
> with all the data, R script, the maps, and the spatial weights files
> for Geoda:
> willonrails.com/images/lisa_spdep.zip
>
> The local Moran's I in R and Geoda are very similar -- on average they
> only differ by 0.000290044.
>
> The p-values, on average, differ by 0.1808931.
>
> In both I used 10 nearest neighbors for the weights. I did create the
> weights separately in R and Geoda. The Geoda weights are in the zip
> file. I'm new to coordinate projections, but here's the relevant part
> of the R script:

OK, so time for Cartography 010: your input data are in unprojected, 
geographical coordinates, so you cannot measure the 10 nearest neighbours 
using Euclidean distance, but rather using Great Circle distance. This 
switches the neighbours of about 2000 of your 3000 counties. This isn't 
the cause of the discrepancy.

Doing hist(nat2$LISA_P) shows that it only extends from 0 to 0.5, so 
obviously is not the same as the values generated by spdep::localmoran (in 
the 0 to 1 range), or even similar. In fact, it is hard to see what has 
been done, and over 1500 counties are Pr < 0.05. Perhaps the upper half 
has been folded down to the lower half?

So for GeoDa, we'd need to establish 1) are the probabilities two-sided or 
greater than (Ii > E(Ii)); 2) have probabilities > 0.5 been folded back?

The functions in spdep use the "greater" alternative by default, and 
"two.sided" is seldom used, but perhaps should be. Using the 
p.adjust.method= argument (for example "bonferroni" or "fdr") and 
accommodating only multiple comparisons among neighbours shifts most 
p-values into insignificant ranges.

I've been looking at:

library(spdep)
library(maptools)
nat <- readShapeSpatial("NAT.SHP", ID="FIPSNO")
coords <- coordinates(nat)
IDs <- row.names(nat)
nat_10nnb_ll <- knn2nb(knearneigh(coords, k=10, longlat=TRUE),
  row.names=IDs)
lisa_10nnb_ll <- localmoran(nat$HR60, nb2listw(nat_10nnb_ll, style="W"),
  alternative="two.sided", p.adjust.method="fdr")
hist(lisa_10nnb_ll[,5])

where only a very few counties have a significant level of local spatial 
autocorrelation at a 5% cutoff (295, still 10% of the total). Your 
GeoDa results gave 1566 of the folded p-values < 5%, half of your 
counties.

nat$PR_Ii <- lisa_10nnb_ll[,5]
spplot(nat, "PR_Ii", at=c(0,0.05,0.95,1+.Machine$double.eps),
   col.regions=cm.colors(3), main="FDR-adjusted two.sided Ii")

shows a simplified map of the probabilities, dominated by counties not 
showing local spatial autocorrelation.

It will also be easier to move the R nb object to GeoDa as a GAL file, the 
ordering of the GWT file seems odd.

There are two main issues. One is that GeoDa uses permutations to generate 
probabilities - there is no agreed position on this, and the references I 
gave earlier suggest that permutating values for counties far from each 
other may create a false impression of local similarity, hence positive 
spatial autocorrelation. The second is the very high count of 
"significant" results with your data - what is a hot spot when half of the 
counties are hot?

One reason why permuting over all but i may be misleading is if many 
values of the variable of interest are in one tail - HR60 is highly 
skewed, with a minimum of 0, a maximum of 92, and an upper hinge of 6. So 
a local clique of neighbours with values in the 20s will seem very unusual 
in the permutation setting.

Finally, the global Moran's I for HR60 is highly significant, and this 
indicates anyway that your mean model is mis-specified.

Hope this helps,

Roger

PS. I regularly respond to overenthusiastic usage of measures of this kind 
when reviewing journal submissions. LISA are useful, but you do need to 
think carefully about what you are actually trying to do with them. 
Careful thematic mapping can be a very effective way of pointing up 
differences in levels of a variable:

library(classInt)
cI <- classIntervals(nat$HR60, n=7, style="fisher")
fcI <- findColours(cI, pal=rev(bpy.colors(7)))
plot(nat, col=fcI, border="transparent")
legend("bottomleft", fill=attr(fcI, "palette"), legend=names(attr(fcI,
  "table")), bty="n")
title(main="HR60")

or using a ColorBrewer palette. Of course, I have no idea what HR60 
represents, and if it is a rate (homicide rate?), it should probably have 
been represented through funnel plot or empirical Bayes methods anyway.

>
> ------------------------------------------------
> # nearest neighbors (nnb)
> coords <- coordinates(nat)
> IDs <- row.names(as(nat, "data.frame"))
> nat_10nnb <- knn2nb(knearneigh(coords, k=10), row.names=IDs)
> ------------------------------------------------
>
> 'nat' is an object of class SpatialPolygonsDataFrame, so if I
> understand correctly I don't need to set the 'longlat' argument in
> knearneigh. I also plotted a random selection of the counties in R,
> along with their neighboring counties from the weights matrix, and
> everything appeared to be working correctly.
>
> I also tried using Geoda's weights in R. Surprisingly, this map is
> even sparser (higher p-values) than the first map I made in R. I'm
> tempted to think I mixed up the data, but the map looks correct
> when I plot all counties regardless of p-value.
>
>
>
>
>
>
>
>
> ----------------------------------------
>> Date: Wed, 11 Mar 2015 19:28:11 +0100
>> From: Roger.Bivand at nhh.no
>> To: williamcmay at live.com
>> CC: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Local Moran p-values in Geoda, Python, and R
>>
>> On Wed, 11 Mar 2015, William May wrote:
>>
>>> I sent this to Roger Bivand earlier. Posting it here for
>>> posterity, and in case other people have more to add:
>>>
>>> ---------------------------------------------------------------------------
>>> Hi Roger,
>>>
>>> I'm taking a spatial econometrics class, and we've been making some
>>> LISA maps. We noticed that the p-values from the spdep package seem to
>>> be a lot different than the results from Geoda or from Python's PySAL
>>> package.
>>>
>>> I looked through the function definitions and noticed that Python, and
>>> I think Geoda, use simulations to estimate the p-value, while your
>>> localmoran function uses an equation.
>>>
>>> I attached the graphs from Geoda and from R, and my R code and the
>>> original dataset. [R code is added below.] Is this kind of
>>> difference normal?
>>
>> If you would put the figures on a website, we could compare them. One
>> possible reason for differences is the alternative= argument to
>> localmoran(). It's default is "greater", but you could try "two.sided" to
>> see whether this gives values closer to GeoDa. You don't say, by the way,
>> which version of GeoDa you are using (since you mention Python, probably
>> not legacy GeoDa), nor whether you can get the results out in tabular
>> form. Are you using the same spatial weights - both can read and write GAL
>> files. Are you aware that knearneigh() takes a longlat= argument, which
>> must be used if the coordinates are not projected. The p-values in
>> localmoran() are analytical ones from: Anselin, L. 1995. Local indicators
>> of spatial association, Geographical Analysis, 27, 93--115. As you
>> mention, in GeoDa they are by leave-one-out permutation bootstrapping.
>>
>> Roger
>>
>>>
>>> Is there a reason to prefer spdep's results over the Geoda results (or
>>> vice versa)?
>>>
>>> Thanks for any help,
>>>
>>> Will
>>> ---------------------------------------------------------------------------
>>>
>>>
>>> Here's his response:
>>>
>>> ---------------------------------------------------------------------------
>>> Please *do* write to the R-sig-geo list rather than to me directly -
>>> others can answer your question as well, perhaps better, and in a more
>>> timely way than I can. In addition, threads in the list can be
>>> searched in the archives, so others can avoid the same problem later.
>>>
>>> The only reliable test is localmoran.exact(), followed by
>>> localmoran.sad(), for reasons given in the references in their help
>>> pages.  If you test more than one observation, remember to adjust the
>>> p-value for multiple comparisons. Sampling _i (all but i)
>>> overrepresents non-neighbours of i for a test on i. The formulae used
>>> are from the original paper and most often are similar to the
>>> permutation bootstrap values.
>>>
>>> Crucially, almost all use of local Moran's I (and local Geary and
>>> Getis) fall foul of the autocorrelation being related to a
>>> mis-specified mean model (omitted explanatory variables and/or omitted
>>> adjustment for global autocorrelation). Look at the references to
>>> localmoran.sad and localmoran.exact for more details, and at the
>>> relevant parts of Waller & Gotway (2004), Schabenberger & Gotway
>>> (2005), and in Bivand et al. (2008, 2013, ASDAR-book, ch. 9).
>>>
>>> LISA maps are extremely misleading when the mean model is
>>> mis-specified, and when the p-values are not adjusted for multiple
>>> comparisons. They look simple, but because adjustment for multiple
>>> comparisons is a subjective choice, you can almost choose the map you
>>> want.
>>>
>>> Roger
>>> ---------------------------------------------------------------------------
>>>
>>>
>>> And here's my R code:
>>>
>>> ---------------------------------------------------------------------------
>>> # make a LISA cluster map
>>>
>>> library(maptools)
>>> library(spdep)
>>>
>>> setwd('/home/will/classes/polisci_610/1/')
>>>
>>> # get the data
>>> nat <- readShapeSpatial("NAT.SHP", ID="FIPSNO")
>>> nat.data<-data.frame(nat)
>>> attach(nat.data)
>>>
>>> # nearest neighbors (nnb)
>>> coords<-coordinates(nat)
>>> IDs<-row.names(as(nat, "data.frame"))
>>> nat_10nnb<-knn2nb(knearneigh(coords, k=10), row.names=IDs)
>>>
>>> # "W" identifies row standardized weights
>>> nat_10nnb_w <- nb2listw(nat_10nnb, style="W")
>>>
>>> # can preset list_w object; if do this, remember to change if needed
>>> list_w <- nat_10nnb_w
>>>
>>> # LISA values
>>> lisa <- localmoran(HR60, list_w, zero.policy = T)
>>>
>>> # centers the variable of interest around its mean
>>> cDV <- HR60 - mean(HR60)
>>>
>>> # centers the local Moran's around the mean
>>> mI <- lisa[, 1]
>>> C_mI <- mI - mean(mI) # but we don't want to center it! Only the sign
>>> # matters.
>>>
>>> quadrant <- vector(mode="numeric",length=nrow(lisa))
>>> quadrant[cDV>0 & mI>0] <- 1
>>> quadrant[cDV <0 & mI>0] <- 2
>>> quadrant[cDV>0 & mI<0] <- 3
>>> quadrant[cDV <0 & mI<0] <- 4
>>>
>>> # set a statistical significance level for the local Moran's
>>> signif <- 0.05
>>>
>>> # places non-significant Moran's in the category "5"
>>> quadrant[lisa[, 5]> signif] <- 5
>>>
>>>
>>> # map
>>> png(file="Lisa_map_R.png", width = 680, res = 100)
>>> colors <- c("red", "blue", "lightpink", "skyblue2", rgb(.95, .95, .95))
>>> par(mar=c(0,0,1,0)) # sets margin parameters for plot space
>>> plot(nat, border="grey", col=colors[quadrant],
>>>       main = "LISA Cluster Map, 1960 Homicides")
>>> legend("bottomright",legend=c("high-high","low-low","high-low","low-high"),
>>>        fill=colors,bty="n",cex=0.7,y.intersp=1,x.intersp=1)
>>> dev.off()
>>> ---------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; fax +47 55 95 91 00
>> e-mail: Roger.Bivand at nhh.no

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no


From r.hijmans at gmail.com  Thu Mar 12 21:50:06 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Thu, 12 Mar 2015 13:50:06 -0700
Subject: [R-sig-Geo] gdistance: costDistance with barriers
In-Reply-To: <1426185352133-7587891.post@n2.nabble.com>
References: <1426185352133-7587891.post@n2.nabble.com>
Message-ID: <CANtt_hzmN4L90UJPHEXtVBaSrfpuWe8q-kDpzYLgLY3kW++h_A@mail.gmail.com>

Karl,

You are using a default RasterLayer which has lon/lat coordinates. In
that case the earth is considered spherical (or similar), not a plane.
The maximum possible distance in longitude is 180 degrees, and the
distance between -120 and 120 is not 240 degrees, but 60+60= 120
degrees, .

To get the results I think you expected, use a planar CRS. For example
r <- raster(nrows=18, ncols=36, crs='+proj=utm +zone=10')

Robert

On Thu, Mar 12, 2015 at 11:35 AM, karljarvis <karljarvis at gmail.com> wrote:
> Hi all,
> I am seeing confusing behavior from the costDistance function in gdistance.
> In general, when cost is constant, cost distance increases linearly with
> actual distance. However, in this example, it is not doing that for the
> longest few distances. Am I missing something?
> Karl
>
> r <- raster(nrows=18, ncols=36)
> r[] <- 1
> t <- transition(r,function(x) 1/mean(x),4)
> p <- cbind(seq(-120,120,by=40),rep(0,7))
> costDistance(t,p)
>
>> costDistance(t,p)
>    1  2  3  4  5  6
> 2  4
> 3  8  4
> 4 12  8  4
> 5 16 12  8  4
> 6 16 16 12  8  4
> 7 12 16 16 12  8  4
>
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/gdistance-costDistance-with-barriers-tp7587891.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From danielvdelatorre at gmail.com  Thu Mar 12 23:05:37 2015
From: danielvdelatorre at gmail.com (=?UTF-8?Q?Daniel_Varaj=C3=A3o_de_Latorre?=)
Date: Thu, 12 Mar 2015 19:05:37 -0300
Subject: [R-sig-Geo] problem in weigths of the extract function
Message-ID: <CAOtDXvAZGehsq4GB2EUPPBOv0ZxnPwPjaUY60Pr0PBqsANkm0g@mail.gmail.com>

Dear list,

I recently upgraded the "raster" library and a script that used to run
perfectly gave me some mistakes. The problem was in the extract function:

r <- raster(nrows=180, ncols=360, xmn=-180, xmx=180, ymn=-90, ymx=90)
r[] <- 1
extract(x = r, y = world_map, weights=T, cellnumber=T)

I've tried different combinations of rasters and polygons and there were
mistakes with the weights output.

The solution I figured was to downgrade the package to the previous version
I was using (2.1-49), and everything is working again

Just thought this was something to share...

Best,
Daniel

	[[alternative HTML version deleted]]


From jacobvanetten at yahoo.com  Fri Mar 13 00:35:24 2015
From: jacobvanetten at yahoo.com (Jacob van Etten)
Date: Thu, 12 Mar 2015 23:35:24 +0000 (UTC)
Subject: [R-sig-Geo] gdistance: costDistance with barriers
In-Reply-To: <CANtt_hzmN4L90UJPHEXtVBaSrfpuWe8q-kDpzYLgLY3kW++h_A@mail.gmail.com>
References: <CANtt_hzmN4L90UJPHEXtVBaSrfpuWe8q-kDpzYLgLY3kW++h_A@mail.gmail.com>
Message-ID: <703768709.1277481.1426203324981.JavaMail.yahoo@mail.yahoo.com>

Robert is right. This example visualizes what is happening.
library(gdistance)r <- raster(nrows=18, ncols=36)?r[] <- 1x <- transition(r,function(x) 1/mean(x),4)origin <- c(-120,0)goal <- c(120,0)sl <- shortestPath(x, origin, goal)plot(raster(sl)) 

     On Thursday, 12 March 2015, 14:50, Robert J. Hijmans <r.hijmans at gmail.com> wrote:
   

 Karl,

You are using a default RasterLayer which has lon/lat coordinates. In
that case the earth is considered spherical (or similar), not a plane.
The maximum possible distance in longitude is 180 degrees, and the
distance between -120 and 120 is not 240 degrees, but 60+60= 120
degrees, .

To get the results I think you expected, use a planar CRS. For example
r <- raster(nrows=18, ncols=36, crs='+proj=utm +zone=10')

Robert

On Thu, Mar 12, 2015 at 11:35 AM, karljarvis <karljarvis at gmail.com> wrote:
> Hi all,
> I am seeing confusing behavior from the costDistance function in gdistance.
> In general, when cost is constant, cost distance increases linearly with
> actual distance. However, in this example, it is not doing that for the
> longest few distances. Am I missing something?
> Karl
>
> r <- raster(nrows=18, ncols=36)
> r[] <- 1
> t <- transition(r,function(x) 1/mean(x),4)
> p <- cbind(seq(-120,120,by=40),rep(0,7))
> costDistance(t,p)
>
>> costDistance(t,p)
>? ? 1? 2? 3? 4? 5? 6
> 2? 4
> 3? 8? 4
> 4 12? 8? 4
> 5 16 12? 8? 4
> 6 16 16 12? 8? 4
> 7 12 16 16 12? 8? 4
>
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/gdistance-costDistance-with-barriers-tp7587891.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


   
	[[alternative HTML version deleted]]


From karljarvis at gmail.com  Fri Mar 13 00:47:32 2015
From: karljarvis at gmail.com (Karl Jarvis)
Date: Thu, 12 Mar 2015 16:47:32 -0700
Subject: [R-sig-Geo] gdistance: costDistance with barriers
In-Reply-To: <703768709.1277481.1426203324981.JavaMail.yahoo@mail.yahoo.com>
References: <CANtt_hzmN4L90UJPHEXtVBaSrfpuWe8q-kDpzYLgLY3kW++h_A@mail.gmail.com>
	<703768709.1277481.1426203324981.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <DB8AC80D-C012-420F-8F1D-0E04186CA6E7@gmail.com>

Great! Thank you all. 

> On Mar 12, 2015, at 4:35 PM, Jacob van Etten <jacobvanetten at yahoo.com> wrote:
> 
> Robert is right. This example visualizes what is happening.
> 
> library(gdistance)
> r <- raster(nrows=18, ncols=36) 
> r[] <- 1
> x <- transition(r,function(x) 1/mean(x),4)
> origin <- c(-120,0)
> goal <- c(120,0)
> sl <- shortestPath(x, origin, goal)
> plot(raster(sl))
> 
> 
> On Thursday, 12 March 2015, 14:50, Robert J. Hijmans <r.hijmans at gmail.com> wrote:
> 
> 
> Karl,
> 
> You are using a default RasterLayer which has lon/lat coordinates. In
> that case the earth is considered spherical (or similar), not a plane.
> The maximum possible distance in longitude is 180 degrees, and the
> distance between -120 and 120 is not 240 degrees, but 60+60= 120
> degrees, .
> 
> To get the results I think you expected, use a planar CRS. For example
> r <- raster(nrows=18, ncols=36, crs='+proj=utm +zone=10')
> 
> Robert
> 
> On Thu, Mar 12, 2015 at 11:35 AM, karljarvis <karljarvis at gmail.com <mailto:karljarvis at gmail.com>> wrote:
> > Hi all,
> > I am seeing confusing behavior from the costDistance function in gdistance.
> > In general, when cost is constant, cost distance increases linearly with
> > actual distance. However, in this example, it is not doing that for the
> > longest few distances. Am I missing something?
> > Karl
> >
> > r <- raster(nrows=18, ncols=36)
> > r[] <- 1
> > t <- transition(r,function(x) 1/mean(x),4)
> > p <- cbind(seq(-120,120,by=40),rep(0,7))
> > costDistance(t,p)
> >
> >> costDistance(t,p)
> >    1  2  3  4  5  6
> > 2  4
> > 3  8  4
> > 4 12  8  4
> > 5 16 12  8  4
> > 6 16 16 12  8  4
> > 7 12 16 16 12  8  4
> >
> >
> >
> > --
> > View this message in context: http://r-sig-geo.2731867.n2.nabble.com/gdistance-costDistance-with-barriers-tp7587891.html <http://r-sig-geo.2731867.n2.nabble.com/gdistance-costDistance-with-barriers-tp7587891.html>
> > Sent from the R-sig-geo mailing list archive at Nabble.com.
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo <https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
> 
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo <https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
> 
> 


	[[alternative HTML version deleted]]


From sean.kearney at alumni.ubc.ca  Mon Mar 16 21:46:28 2015
From: sean.kearney at alumni.ubc.ca (spkearney)
Date: Mon, 16 Mar 2015 13:46:28 -0700 (MST)
Subject: [R-sig-Geo] parallel raster processing with calc and mc2d monte
 carlo simulation
Message-ID: <1426538788500-7587901.post@n2.nabble.com>

Hello all, and thanks in advance for any and all help you can give on this:

I have set up a function to extract the 2.5%, 50% and 97.5% percentiles from
a monte carlo simulation on three rasters that is to be called up using
calc() in the raster package and it works great on a test-sized stack/brick,
thanks to suggestions at this post here:
http://grokbase.com/t/r/r-sig-geo/123cb3daaq/apply-monte-carlo-simulation-for-each-cell-in-a-matrix-originally-raster

My problem, is that I want to run this function on a much larger Raster
Brick that, as written, takes hours to process.  I need to do this multiple
times, so I am trying to speed up the processing using clusterR (or another
option such as rasterEngine with multi-core processing).  However, I can't
get it to work!   Here is the code that works on the test raster brick:

brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one
Raster Brick
testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ## Crop
brick to manageable size

ndunc(101)
fun.CROP_AGWBC <- function(x) {
  dBC_BA <- mcdata(x[[1]], type="0")
  dBC_BA_SE <- mcdata(x[[2]], type = "0")
  SlopePer <-x[[3]]
  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE, 
                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
  BC_AGWBC <- lm.final$coefficients[1] + 
    lm.final$coefficients[2]*stBA + 
    lm.final$coefficients[3]*SlopePer
  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
}
 
CROP_AGWBC <- calc(teststack, fun.CROP_AGWBC) ##Run the calc function
CROP_AGWBC ##Check the result
plot(CROP_AGWBC) ##Plot the three-raster brick result

##Extract the individual raster layers
CROP_AGWBC_PRED <- CROP_AGWBC[[2]]
CROP_AGWBC_LWR <- CROP_AGWBC[[1]]
CROP_AGWBC_UPR <- CROP_AGWBC[[3]]

As I mentioned, the code works great on a small sample.  I tried to speed it
up using clusterR as follows, first testing it on the 'testbrick' Raster
Brick with hopes to use it on the whole Raster Brick:

beginCluster(8)
clusterR(x = testbrick, fun = fun.CROP_AGWBC)

and I get the following error:
[1] "data should be numeric or logical"
attr(,"class")
[1] "snow-try-error" "try-error"     
Error in clusterR(x = testbrick, fun = fun.CROP_AGWBC) : cluster error

It is interesting because, if I try to run it again, I get this error
instead:
Error in as.vector((x[, 1] - 1) * ncol(object) + x[, 2]) : 
  error in evaluating the argument 'x' in selecting a method for function
'as.vector': Error in x[, 2] : subscript out of bounds

I have tried this many different ways, including along the lines of: 
f <- function (x) calc(x, fun.CROP_AGWBC)
y <- clusterR(testbrick, f)

which gives me the same error (more or less) of:
Error in checkForRemoteErrors(lapply(cl, recvResult)) : 
  2 nodes produced errors; first error: data should be numeric or logical

And I have tried using the rasterEngine() function (first without parallel
processing) by changing up the code in two ways, the first being: 
ndunc(101)
fun.CROP_AGWBC <- function(x) {
  dBC_BA <- mcdata(x[[1]], type="0")
  dBC_BA_SE <- mcdata(x[[2]], type = "0")
  SlopePer <-x[[3]]
  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE, 
                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
  BC_AGWBC <- lm.final$coefficients[1] + 
    lm.final$coefficients[2]*stBA + 
    lm.final$coefficients[3]*SlopePer
  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
  output <- quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
  output_array <- array(output,dim=c(dim(x)[1],dim(x)[2],3))
  return(output_array)
}
re <- rasterEngine(x = testbrick, fun = fun.CROP_AGWBC)

which runs but gives me a 3-layer Raster Brick all with NA's or Inf.  The
second thing I tried used the same fun.CROP_AGWBC function as above, but
with the following rasterEngine code to call up the calc formula:
f <- function(x) {reout <- calc(x, fun.CROP_AGWBC)
                  reout_array <- array(getValues(reout),
dim=c(dim(x)[1],dim(x)[2],3))
                  return(reout_array)
}
re <- rasterEngine(x = testbrick, fun = f, chunk_format = "raster")

which gives me the following error, even though I thought I converted the
output to an array:
chunk processing units require array vector outputs.  Please check your
function.
Error in focal_hpc_test(x, fun, window_center, window_dims, args,
layer_names,  :

So, my questions are as follows:
*1) Does anyone know why the clusterR does not work for the calc() function
in my first attempt?  I imagine it has something to do with the conversion
of rasters to mcnodes in the function, but can't figure it out!  Any
suggestions?

2) Any thoughts on why I can't get this to work with the rasterEngine()
function?  I am converting the outputs to arrays with the same dimensions as
the input file, but still no luck.*

Again, any help is much appreciated.  Any suggestions for improving this
question are welcome and I'll do my best to update it - this is my first
post!

Kind Regards,
sean





--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/parallel-raster-processing-with-calc-and-mc2d-monte-carlo-simulation-tp7587901.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From r.hijmans at gmail.com  Tue Mar 17 16:14:41 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Tue, 17 Mar 2015 08:14:41 -0700
Subject: [R-sig-Geo] parallel raster processing with calc and mc2d monte
 carlo simulation
In-Reply-To: <1426538788500-7587901.post@n2.nabble.com>
References: <1426538788500-7587901.post@n2.nabble.com>
Message-ID: <CANtt_hzhAJE2c8JGKncit1_vETu9+knxHD=Sn-5GUv4P11UuWw@mail.gmail.com>

Sean,

fun.CROP_AGWBC  refers to objects that are not defined inside the
function ("lm.final" and "lambda_DV"). I assume that this is
intentional and that these represent constants; and that they are
available in your global environment. If so, you need to export these
objects to the cluster nodes. See the 'export' argument in clusterR.
You also need to load necessary packages before calling beginCluster

Robert

On Mon, Mar 16, 2015 at 1:46 PM, spkearney <sean.kearney at alumni.ubc.ca> wrote:
> Hello all, and thanks in advance for any and all help you can give on this:
>
> I have set up a function to extract the 2.5%, 50% and 97.5% percentiles from
> a monte carlo simulation on three rasters that is to be called up using
> calc() in the raster package and it works great on a test-sized stack/brick,
> thanks to suggestions at this post here:
> http://grokbase.com/t/r/r-sig-geo/123cb3daaq/apply-monte-carlo-simulation-for-each-cell-in-a-matrix-originally-raster
>
> My problem, is that I want to run this function on a much larger Raster
> Brick that, as written, takes hours to process.  I need to do this multiple
> times, so I am trying to speed up the processing using clusterR (or another
> option such as rasterEngine with multi-core processing).  However, I can't
> get it to work!   Here is the code that works on the test raster brick:
>
> brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one
> Raster Brick
> testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ## Crop
> brick to manageable size
>
> ndunc(101)
> fun.CROP_AGWBC <- function(x) {
>   dBC_BA <- mcdata(x[[1]], type="0")
>   dBC_BA_SE <- mcdata(x[[2]], type = "0")
>   SlopePer <-x[[3]]
>   stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>                  mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>   BC_AGWBC <- lm.final$coefficients[1] +
>     lm.final$coefficients[2]*stBA +
>     lm.final$coefficients[3]*SlopePer
>   AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>   quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
> }
>
> CROP_AGWBC <- calc(teststack, fun.CROP_AGWBC) ##Run the calc function
> CROP_AGWBC ##Check the result
> plot(CROP_AGWBC) ##Plot the three-raster brick result
>
> ##Extract the individual raster layers
> CROP_AGWBC_PRED <- CROP_AGWBC[[2]]
> CROP_AGWBC_LWR <- CROP_AGWBC[[1]]
> CROP_AGWBC_UPR <- CROP_AGWBC[[3]]
>
> As I mentioned, the code works great on a small sample.  I tried to speed it
> up using clusterR as follows, first testing it on the 'testbrick' Raster
> Brick with hopes to use it on the whole Raster Brick:
>
> beginCluster(8)
> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
>
> and I get the following error:
> [1] "data should be numeric or logical"
> attr(,"class")
> [1] "snow-try-error" "try-error"
> Error in clusterR(x = testbrick, fun = fun.CROP_AGWBC) : cluster error
>
> It is interesting because, if I try to run it again, I get this error
> instead:
> Error in as.vector((x[, 1] - 1) * ncol(object) + x[, 2]) :
>   error in evaluating the argument 'x' in selecting a method for function
> 'as.vector': Error in x[, 2] : subscript out of bounds
>
> I have tried this many different ways, including along the lines of:
> f <- function (x) calc(x, fun.CROP_AGWBC)
> y <- clusterR(testbrick, f)
>
> which gives me the same error (more or less) of:
> Error in checkForRemoteErrors(lapply(cl, recvResult)) :
>   2 nodes produced errors; first error: data should be numeric or logical
>
> And I have tried using the rasterEngine() function (first without parallel
> processing) by changing up the code in two ways, the first being:
> ndunc(101)
> fun.CROP_AGWBC <- function(x) {
>   dBC_BA <- mcdata(x[[1]], type="0")
>   dBC_BA_SE <- mcdata(x[[2]], type = "0")
>   SlopePer <-x[[3]]
>   stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>                  mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>   BC_AGWBC <- lm.final$coefficients[1] +
>     lm.final$coefficients[2]*stBA +
>     lm.final$coefficients[3]*SlopePer
>   AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>   output <- quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>   output_array <- array(output,dim=c(dim(x)[1],dim(x)[2],3))
>   return(output_array)
> }
> re <- rasterEngine(x = testbrick, fun = fun.CROP_AGWBC)
>
> which runs but gives me a 3-layer Raster Brick all with NA's or Inf.  The
> second thing I tried used the same fun.CROP_AGWBC function as above, but
> with the following rasterEngine code to call up the calc formula:
> f <- function(x) {reout <- calc(x, fun.CROP_AGWBC)
>                   reout_array <- array(getValues(reout),
> dim=c(dim(x)[1],dim(x)[2],3))
>                   return(reout_array)
> }
> re <- rasterEngine(x = testbrick, fun = f, chunk_format = "raster")
>
> which gives me the following error, even though I thought I converted the
> output to an array:
> chunk processing units require array vector outputs.  Please check your
> function.
> Error in focal_hpc_test(x, fun, window_center, window_dims, args,
> layer_names,  :
>
> So, my questions are as follows:
> *1) Does anyone know why the clusterR does not work for the calc() function
> in my first attempt?  I imagine it has something to do with the conversion
> of rasters to mcnodes in the function, but can't figure it out!  Any
> suggestions?
>
> 2) Any thoughts on why I can't get this to work with the rasterEngine()
> function?  I am converting the outputs to arrays with the same dimensions as
> the input file, but still no luck.*
>
> Again, any help is much appreciated.  Any suggestions for improving this
> question are welcome and I'll do my best to update it - this is my first
> post!
>
> Kind Regards,
> sean
>
>
>
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/parallel-raster-processing-with-calc-and-mc2d-monte-carlo-simulation-tp7587901.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From sean.kearney at alumni.ubc.ca  Tue Mar 17 17:21:36 2015
From: sean.kearney at alumni.ubc.ca (Sean Kearney)
Date: Tue, 17 Mar 2015 09:21:36 -0700
Subject: [R-sig-Geo] parallel raster processing with calc and mc2d monte
	carlo simulation
In-Reply-To: <CANtt_hzhAJE2c8JGKncit1_vETu9+knxHD=Sn-5GUv4P11UuWw@mail.gmail.com>
References: <1426538788500-7587901.post@n2.nabble.com>
	<CANtt_hzhAJE2c8JGKncit1_vETu9+knxHD=Sn-5GUv4P11UuWw@mail.gmail.com>
Message-ID: <5ED368A7-996D-46EE-9056-452886082108@alumni.ubc.ca>

Hi Robert,

Thanks for the advice.  So I tried exporting the objects defined in the function (lm.final and lambda_DV) with the following code:

library(raster)
library(rgdal)
library(mc2d)
brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one RasterBrick
testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ##Crop Raster Brick to manageable size

####
#### Predict Biomass using final linear model with Monte Carlo Simulation
ndunc(101)
fun.CROP_AGWBC <- function(x) {
  require(mc2d)
  dBC_BA <- mcdata(x[[1]], type="0")
  dBC_BA_SE <- mcdata(x[[2]], type = "0")
  SlopePer <-x[[3]]
  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE, 
                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
  BC_AGWBC <- lm.final$coefficients[1] + 
    lm.final$coefficients[2]*stBA + 
    lm.final$coefficients[3]*SlopePer
  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
}

####
#### Check function using calc
CROP_AGWBC <- calc(testbrick, fun.CROP_AGWBC)
plot(CROP_AGWBC)
CROP_AGWBC

####
#### Run function using parallel processing 
library(snow)
beginCluster(8)
library(parallel)
cl <- getCluster()
clusterExport(cl, list("lm.final", "lambda_DV"))
clusterR(x = testbrick, fun = fun.CROP_AGWBC)
endCluster()


RESULT: Again, the calc process works fine to produce object CROP_AGWBC but when I try to run the last bit with parallel processing, I still get the same error: 

> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
[1] "data should be numeric or logical"
attr(,"class")
[1] "snow-try-error" "try-error"     
Error in clusterR(x = testbrick, fun = fun.CROP_AGWBC) : cluster error


I was suspicious that maybe the issue was still with the export because while ?lambda_DV" is a constant, lm.final is actually an lm model, so I replaced these objects in the function with constants (see code below) but still get the same error:

REVISED CODE WITH CONSTANTS: 
library(raster)
library(rgdal)
library(mc2d)
brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one RasterBrick
testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ##Crop Raster Brick to manageable size

####
#### Predict Biomass using final linear model with Monte Carlo Simulation
ndunc(101)
fun2.CROP_AGWBC <- function(x) {
  require(mc2d)
  dBC_BA <- mcdata(x[[1]], type="0")
  dBC_BA_SE <- mcdata(x[[2]], type = "0")
  SlopePer <-x[[3]]
  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE, 
                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
  BC_AGWBC <- 0.6419 + 
    0.9307*stBA + 
    (-0.0176)*SlopePer
  AGWBC <- (0.2626263 * BC_AGWBC + 1)^(1/0.2626263)-1
  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
}

####
#### Check function using calc
CROP_AGWBC_2 <- calc(testbrick, fun2.CROP_AGWBC)
plot(CROP_AGWBC_2)
CROP_AGWBC_2

####
#### Run function using parallel processing 
library(snow)
beginCluster(8)
clusterR(x = testbrick, fun = fun2.CROP_AGWBC)
endCluster()

RESULT: same error:
> clusterR(x = testbrick, fun = fun2.CROP_AGWBC)
[1] "data should be numeric or logical"
attr(,"class")
[1] "snow-try-error" "try-error"     
Error in clusterR(x = testbrick, fun = fun2.CROP_AGWBC) : cluster error


Any other thoughts?  Many thanks,

sean

On Mar 17, 2015, at 8:14 AM, Robert J. Hijmans <r.hijmans at gmail.com> wrote:

> Sean,
> 
> fun.CROP_AGWBC  refers to objects that are not defined inside the
> function ("lm.final" and "lambda_DV"). I assume that this is
> intentional and that these represent constants; and that they are
> available in your global environment. If so, you need to export these
> objects to the cluster nodes. See the 'export' argument in clusterR.
> You also need to load necessary packages before calling beginCluster
> 
> Robert
> 
> On Mon, Mar 16, 2015 at 1:46 PM, spkearney <sean.kearney at alumni.ubc.ca> wrote:
>> Hello all, and thanks in advance for any and all help you can give on this:
>> 
>> I have set up a function to extract the 2.5%, 50% and 97.5% percentiles from
>> a monte carlo simulation on three rasters that is to be called up using
>> calc() in the raster package and it works great on a test-sized stack/brick,
>> thanks to suggestions at this post here:
>> http://grokbase.com/t/r/r-sig-geo/123cb3daaq/apply-monte-carlo-simulation-for-each-cell-in-a-matrix-originally-raster
>> 
>> My problem, is that I want to run this function on a much larger Raster
>> Brick that, as written, takes hours to process.  I need to do this multiple
>> times, so I am trying to speed up the processing using clusterR (or another
>> option such as rasterEngine with multi-core processing).  However, I can't
>> get it to work!   Here is the code that works on the test raster brick:
>> 
>> brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one
>> Raster Brick
>> testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ## Crop
>> brick to manageable size
>> 
>> ndunc(101)
>> fun.CROP_AGWBC <- function(x) {
>>  dBC_BA <- mcdata(x[[1]], type="0")
>>  dBC_BA_SE <- mcdata(x[[2]], type = "0")
>>  SlopePer <-x[[3]]
>>  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>>                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>>  BC_AGWBC <- lm.final$coefficients[1] +
>>    lm.final$coefficients[2]*stBA +
>>    lm.final$coefficients[3]*SlopePer
>>  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>>  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>> }
>> 
>> CROP_AGWBC <- calc(teststack, fun.CROP_AGWBC) ##Run the calc function
>> CROP_AGWBC ##Check the result
>> plot(CROP_AGWBC) ##Plot the three-raster brick result
>> 
>> ##Extract the individual raster layers
>> CROP_AGWBC_PRED <- CROP_AGWBC[[2]]
>> CROP_AGWBC_LWR <- CROP_AGWBC[[1]]
>> CROP_AGWBC_UPR <- CROP_AGWBC[[3]]
>> 
>> As I mentioned, the code works great on a small sample.  I tried to speed it
>> up using clusterR as follows, first testing it on the 'testbrick' Raster
>> Brick with hopes to use it on the whole Raster Brick:
>> 
>> beginCluster(8)
>> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
>> 
>> and I get the following error:
>> [1] "data should be numeric or logical"
>> attr(,"class")
>> [1] "snow-try-error" "try-error"
>> Error in clusterR(x = testbrick, fun = fun.CROP_AGWBC) : cluster error
>> 
>> It is interesting because, if I try to run it again, I get this error
>> instead:
>> Error in as.vector((x[, 1] - 1) * ncol(object) + x[, 2]) :
>>  error in evaluating the argument 'x' in selecting a method for function
>> 'as.vector': Error in x[, 2] : subscript out of bounds
>> 
>> I have tried this many different ways, including along the lines of:
>> f <- function (x) calc(x, fun.CROP_AGWBC)
>> y <- clusterR(testbrick, f)
>> 
>> which gives me the same error (more or less) of:
>> Error in checkForRemoteErrors(lapply(cl, recvResult)) :
>>  2 nodes produced errors; first error: data should be numeric or logical
>> 
>> And I have tried using the rasterEngine() function (first without parallel
>> processing) by changing up the code in two ways, the first being:
>> ndunc(101)
>> fun.CROP_AGWBC <- function(x) {
>>  dBC_BA <- mcdata(x[[1]], type="0")
>>  dBC_BA_SE <- mcdata(x[[2]], type = "0")
>>  SlopePer <-x[[3]]
>>  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>>                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>>  BC_AGWBC <- lm.final$coefficients[1] +
>>    lm.final$coefficients[2]*stBA +
>>    lm.final$coefficients[3]*SlopePer
>>  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>>  output <- quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>>  output_array <- array(output,dim=c(dim(x)[1],dim(x)[2],3))
>>  return(output_array)
>> }
>> re <- rasterEngine(x = testbrick, fun = fun.CROP_AGWBC)
>> 
>> which runs but gives me a 3-layer Raster Brick all with NA's or Inf.  The
>> second thing I tried used the same fun.CROP_AGWBC function as above, but
>> with the following rasterEngine code to call up the calc formula:
>> f <- function(x) {reout <- calc(x, fun.CROP_AGWBC)
>>                  reout_array <- array(getValues(reout),
>> dim=c(dim(x)[1],dim(x)[2],3))
>>                  return(reout_array)
>> }
>> re <- rasterEngine(x = testbrick, fun = f, chunk_format = "raster")
>> 
>> which gives me the following error, even though I thought I converted the
>> output to an array:
>> chunk processing units require array vector outputs.  Please check your
>> function.
>> Error in focal_hpc_test(x, fun, window_center, window_dims, args,
>> layer_names,  :
>> 
>> So, my questions are as follows:
>> *1) Does anyone know why the clusterR does not work for the calc() function
>> in my first attempt?  I imagine it has something to do with the conversion
>> of rasters to mcnodes in the function, but can't figure it out!  Any
>> suggestions?
>> 
>> 2) Any thoughts on why I can't get this to work with the rasterEngine()
>> function?  I am converting the outputs to arrays with the same dimensions as
>> the input file, but still no luck.*
>> 
>> Again, any help is much appreciated.  Any suggestions for improving this
>> question are welcome and I'll do my best to update it - this is my first
>> post!
>> 
>> Kind Regards,
>> sean
>> 
>> 
>> 
>> 
>> 
>> --
>> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/parallel-raster-processing-with-calc-and-mc2d-monte-carlo-simulation-tp7587901.html
>> Sent from the R-sig-geo mailing list archive at Nabble.com.
>> 
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From r.hijmans at gmail.com  Tue Mar 17 18:03:22 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Tue, 17 Mar 2015 10:03:22 -0700
Subject: [R-sig-Geo] parallel raster processing with calc and mc2d monte
 carlo simulation
In-Reply-To: <5ED368A7-996D-46EE-9056-452886082108@alumni.ubc.ca>
References: <1426538788500-7587901.post@n2.nabble.com>
	<CANtt_hzhAJE2c8JGKncit1_vETu9+knxHD=Sn-5GUv4P11UuWw@mail.gmail.com>
	<5ED368A7-996D-46EE-9056-452886082108@alumni.ubc.ca>
Message-ID: <CANtt_hxF3-m5JV9P5gZVmiNNDj_RCe2mEeZ8y17BGWZi6eYHXw@mail.gmail.com>

The below works for me. You used function 'f' to clusterR, where it
should have been  'calc'

library(raster)
library(snow)
library(mc2d)

f <- function(x) {
  dBC_BA <- mcdata(x[1], type="0")
  dBC_BA_SE <- mcdata(x[2], type = "0")
  SlopePer <-x[3]
  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE, mean = dBC_BA, sd =
dBC_BA_SE, linf = 0, lhs = FALSE)
  BC_AGWBC <- 0.6419 + 0.9307*stBA + (-0.0176)*SlopePer
  AGWBC <- (0.2626263 * BC_AGWBC + 1)^(1/0.2626263)-1
  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
}


b <- brick(system.file("external/rlogo.grd", package="raster"))
x <- calc(b, f)

# new function for cluster. You could also rewrite f to include calc
ff <- function(x) calc(x, f)

beginCluster()
y <- clusterR(b, fun=ff, export='f')
endCluster()

On Tue, Mar 17, 2015 at 9:21 AM, Sean Kearney
<sean.kearney at alumni.ubc.ca> wrote:
> Hi Robert,
>
> Thanks for the advice.  So I tried exporting the objects defined in the function (lm.final and lambda_DV) with the following code:
>
> library(raster)
> library(rgdal)
> library(mc2d)
> brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one RasterBrick
> testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ##Crop Raster Brick to manageable size
>
> ####
> #### Predict Biomass using final linear model with Monte Carlo Simulation
> ndunc(101)
> fun.CROP_AGWBC <- function(x) {
>   require(mc2d)
>   dBC_BA <- mcdata(x[[1]], type="0")
>   dBC_BA_SE <- mcdata(x[[2]], type = "0")
>   SlopePer <-x[[3]]
>   stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>                  mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>   BC_AGWBC <- lm.final$coefficients[1] +
>     lm.final$coefficients[2]*stBA +
>     lm.final$coefficients[3]*SlopePer
>   AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>   quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
> }
>
> ####
> #### Check function using calc
> CROP_AGWBC <- calc(testbrick, fun.CROP_AGWBC)
> plot(CROP_AGWBC)
> CROP_AGWBC
>
> ####
> #### Run function using parallel processing
> library(snow)
> beginCluster(8)
> library(parallel)
> cl <- getCluster()
> clusterExport(cl, list("lm.final", "lambda_DV"))
> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
> endCluster()
>
>
> RESULT: Again, the calc process works fine to produce object CROP_AGWBC but when I try to run the last bit with parallel processing, I still get the same error:
>
>> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
> [1] "data should be numeric or logical"
> attr(,"class")
> [1] "snow-try-error" "try-error"
> Error in clusterR(x = testbrick, fun = fun.CROP_AGWBC) : cluster error
>
>
> I was suspicious that maybe the issue was still with the export because while ?lambda_DV" is a constant, lm.final is actually an lm model, so I replaced these objects in the function with constants (see code below) but still get the same error:
>
> REVISED CODE WITH CONSTANTS:
> library(raster)
> library(rgdal)
> library(mc2d)
> brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one RasterBrick
> testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ##Crop Raster Brick to manageable size
>
> ####
> #### Predict Biomass using final linear model with Monte Carlo Simulation
> ndunc(101)
> fun2.CROP_AGWBC <- function(x) {
>   require(mc2d)
>   dBC_BA <- mcdata(x[[1]], type="0")
>   dBC_BA_SE <- mcdata(x[[2]], type = "0")
>   SlopePer <-x[[3]]
>   stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>                  mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>   BC_AGWBC <- 0.6419 +
>     0.9307*stBA +
>     (-0.0176)*SlopePer
>   AGWBC <- (0.2626263 * BC_AGWBC + 1)^(1/0.2626263)-1
>   quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
> }
>
> ####
> #### Check function using calc
> CROP_AGWBC_2 <- calc(testbrick, fun2.CROP_AGWBC)
> plot(CROP_AGWBC_2)
> CROP_AGWBC_2
>
> ####
> #### Run function using parallel processing
> library(snow)
> beginCluster(8)
> clusterR(x = testbrick, fun = fun2.CROP_AGWBC)
> endCluster()
>
> RESULT: same error:
>> clusterR(x = testbrick, fun = fun2.CROP_AGWBC)
> [1] "data should be numeric or logical"
> attr(,"class")
> [1] "snow-try-error" "try-error"
> Error in clusterR(x = testbrick, fun = fun2.CROP_AGWBC) : cluster error
>
>
> Any other thoughts?  Many thanks,
>
> sean
>
> On Mar 17, 2015, at 8:14 AM, Robert J. Hijmans <r.hijmans at gmail.com> wrote:
>
>> Sean,
>>
>> fun.CROP_AGWBC  refers to objects that are not defined inside the
>> function ("lm.final" and "lambda_DV"). I assume that this is
>> intentional and that these represent constants; and that they are
>> available in your global environment. If so, you need to export these
>> objects to the cluster nodes. See the 'export' argument in clusterR.
>> You also need to load necessary packages before calling beginCluster
>>
>> Robert
>>
>> On Mon, Mar 16, 2015 at 1:46 PM, spkearney <sean.kearney at alumni.ubc.ca> wrote:
>>> Hello all, and thanks in advance for any and all help you can give on this:
>>>
>>> I have set up a function to extract the 2.5%, 50% and 97.5% percentiles from
>>> a monte carlo simulation on three rasters that is to be called up using
>>> calc() in the raster package and it works great on a test-sized stack/brick,
>>> thanks to suggestions at this post here:
>>> http://grokbase.com/t/r/r-sig-geo/123cb3daaq/apply-monte-carlo-simulation-for-each-cell-in-a-matrix-originally-raster
>>>
>>> My problem, is that I want to run this function on a much larger Raster
>>> Brick that, as written, takes hours to process.  I need to do this multiple
>>> times, so I am trying to speed up the processing using clusterR (or another
>>> option such as rasterEngine with multi-core processing).  However, I can't
>>> get it to work!   Here is the code that works on the test raster brick:
>>>
>>> brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one
>>> Raster Brick
>>> testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ## Crop
>>> brick to manageable size
>>>
>>> ndunc(101)
>>> fun.CROP_AGWBC <- function(x) {
>>>  dBC_BA <- mcdata(x[[1]], type="0")
>>>  dBC_BA_SE <- mcdata(x[[2]], type = "0")
>>>  SlopePer <-x[[3]]
>>>  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>>>                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>>>  BC_AGWBC <- lm.final$coefficients[1] +
>>>    lm.final$coefficients[2]*stBA +
>>>    lm.final$coefficients[3]*SlopePer
>>>  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>>>  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>>> }
>>>
>>> CROP_AGWBC <- calc(teststack, fun.CROP_AGWBC) ##Run the calc function
>>> CROP_AGWBC ##Check the result
>>> plot(CROP_AGWBC) ##Plot the three-raster brick result
>>>
>>> ##Extract the individual raster layers
>>> CROP_AGWBC_PRED <- CROP_AGWBC[[2]]
>>> CROP_AGWBC_LWR <- CROP_AGWBC[[1]]
>>> CROP_AGWBC_UPR <- CROP_AGWBC[[3]]
>>>
>>> As I mentioned, the code works great on a small sample.  I tried to speed it
>>> up using clusterR as follows, first testing it on the 'testbrick' Raster
>>> Brick with hopes to use it on the whole Raster Brick:
>>>
>>> beginCluster(8)
>>> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
>>>
>>> and I get the following error:
>>> [1] "data should be numeric or logical"
>>> attr(,"class")
>>> [1] "snow-try-error" "try-error"
>>> Error in clusterR(x = testbrick, fun = fun.CROP_AGWBC) : cluster error
>>>
>>> It is interesting because, if I try to run it again, I get this error
>>> instead:
>>> Error in as.vector((x[, 1] - 1) * ncol(object) + x[, 2]) :
>>>  error in evaluating the argument 'x' in selecting a method for function
>>> 'as.vector': Error in x[, 2] : subscript out of bounds
>>>
>>> I have tried this many different ways, including along the lines of:
>>> f <- function (x) calc(x, fun.CROP_AGWBC)
>>> y <- clusterR(testbrick, f)
>>>
>>> which gives me the same error (more or less) of:
>>> Error in checkForRemoteErrors(lapply(cl, recvResult)) :
>>>  2 nodes produced errors; first error: data should be numeric or logical
>>>
>>> And I have tried using the rasterEngine() function (first without parallel
>>> processing) by changing up the code in two ways, the first being:
>>> ndunc(101)
>>> fun.CROP_AGWBC <- function(x) {
>>>  dBC_BA <- mcdata(x[[1]], type="0")
>>>  dBC_BA_SE <- mcdata(x[[2]], type = "0")
>>>  SlopePer <-x[[3]]
>>>  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>>>                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>>>  BC_AGWBC <- lm.final$coefficients[1] +
>>>    lm.final$coefficients[2]*stBA +
>>>    lm.final$coefficients[3]*SlopePer
>>>  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>>>  output <- quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>>>  output_array <- array(output,dim=c(dim(x)[1],dim(x)[2],3))
>>>  return(output_array)
>>> }
>>> re <- rasterEngine(x = testbrick, fun = fun.CROP_AGWBC)
>>>
>>> which runs but gives me a 3-layer Raster Brick all with NA's or Inf.  The
>>> second thing I tried used the same fun.CROP_AGWBC function as above, but
>>> with the following rasterEngine code to call up the calc formula:
>>> f <- function(x) {reout <- calc(x, fun.CROP_AGWBC)
>>>                  reout_array <- array(getValues(reout),
>>> dim=c(dim(x)[1],dim(x)[2],3))
>>>                  return(reout_array)
>>> }
>>> re <- rasterEngine(x = testbrick, fun = f, chunk_format = "raster")
>>>
>>> which gives me the following error, even though I thought I converted the
>>> output to an array:
>>> chunk processing units require array vector outputs.  Please check your
>>> function.
>>> Error in focal_hpc_test(x, fun, window_center, window_dims, args,
>>> layer_names,  :
>>>
>>> So, my questions are as follows:
>>> *1) Does anyone know why the clusterR does not work for the calc() function
>>> in my first attempt?  I imagine it has something to do with the conversion
>>> of rasters to mcnodes in the function, but can't figure it out!  Any
>>> suggestions?
>>>
>>> 2) Any thoughts on why I can't get this to work with the rasterEngine()
>>> function?  I am converting the outputs to arrays with the same dimensions as
>>> the input file, but still no luck.*
>>>
>>> Again, any help is much appreciated.  Any suggestions for improving this
>>> question are welcome and I'll do my best to update it - this is my first
>>> post!
>>>
>>> Kind Regards,
>>> sean
>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/parallel-raster-processing-with-calc-and-mc2d-monte-carlo-simulation-tp7587901.html
>>> Sent from the R-sig-geo mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From sean.kearney at alumni.ubc.ca  Tue Mar 17 18:17:15 2015
From: sean.kearney at alumni.ubc.ca (Sean Kearney)
Date: Tue, 17 Mar 2015 10:17:15 -0700
Subject: [R-sig-Geo] parallel raster processing with calc and mc2d monte
	carlo simulation
In-Reply-To: <CANtt_hxF3-m5JV9P5gZVmiNNDj_RCe2mEeZ8y17BGWZi6eYHXw@mail.gmail.com>
References: <1426538788500-7587901.post@n2.nabble.com>
	<CANtt_hzhAJE2c8JGKncit1_vETu9+knxHD=Sn-5GUv4P11UuWw@mail.gmail.com>
	<5ED368A7-996D-46EE-9056-452886082108@alumni.ubc.ca>
	<CANtt_hxF3-m5JV9P5gZVmiNNDj_RCe2mEeZ8y17BGWZi6eYHXw@mail.gmail.com>
Message-ID: <46C85172-40EF-47CA-9733-37F77E281DFB@alumni.ubc.ca>

Hi Robert,

That did the trick!  I didn?t realize you need to put the original function in the ?export=? argument.  The following code now works:

brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one RasterBrick
testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ##Crop Raster Brick to manageable size

####
#### Predict Biomass using final linear model with Monte Carlo Simulation
ndunc(101)
fun.CROP_AGWBC <- function(x) {
  require(mc2d)
  dBC_BA <- mcdata(x[[1]], type="0")
  dBC_BA_SE <- mcdata(x[[2]], type = "0")
  SlopePer <-x[[3]]
  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE, 
                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
  BC_AGWBC <- lm.final$coefficients[1] + 
    lm.final$coefficients[2]*stBA + 
    lm.final$coefficients[3]*SlopePer
  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
}

####
#### Check function using calc
CROP_AGWBC <- calc(testbrick, fun.CROP_AGWBC)
plot(CROP_AGWBC)
CROP_AGWBC

####
#### Run function using parallel processing 
library(snow)
ff <- function(x) calc(x, fun.CROP_AGWBC)
beginCluster(8)
cl <-getCluster()
clusterExport(cl, list("lm.final", "lambda_DV"))
y2 <- clusterR(testbrick, fun = ff, export = "fun.CROP_AGWBC")
endCluster()
plot(y2)
y2

Thanks so much for your help.  I?ll try to do a side-by-side and see how much the clusterR() function actually speeds things up.  Running calc() without parallel processing was taking ~6hrs per run?

Thanks again!

sean
On Mar 17, 2015, at 10:03 AM, Robert J. Hijmans <r.hijmans at gmail.com> wrote:

> The below works for me. You used function 'f' to clusterR, where it
> should have been  'calc'
> 
> library(raster)
> library(snow)
> library(mc2d)
> 
> f <- function(x) {
>  dBC_BA <- mcdata(x[1], type="0")
>  dBC_BA_SE <- mcdata(x[2], type = "0")
>  SlopePer <-x[3]
>  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE, mean = dBC_BA, sd =
> dBC_BA_SE, linf = 0, lhs = FALSE)
>  BC_AGWBC <- 0.6419 + 0.9307*stBA + (-0.0176)*SlopePer
>  AGWBC <- (0.2626263 * BC_AGWBC + 1)^(1/0.2626263)-1
>  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
> }
> 
> 
> b <- brick(system.file("external/rlogo.grd", package="raster"))
> x <- calc(b, f)
> 
> # new function for cluster. You could also rewrite f to include calc
> ff <- function(x) calc(x, f)
> 
> beginCluster()
> y <- clusterR(b, fun=ff, export='f')
> endCluster()
> 
> On Tue, Mar 17, 2015 at 9:21 AM, Sean Kearney
> <sean.kearney at alumni.ubc.ca> wrote:
>> Hi Robert,
>> 
>> Thanks for the advice.  So I tried exporting the objects defined in the function (lm.final and lambda_DV) with the following code:
>> 
>> library(raster)
>> library(rgdal)
>> library(mc2d)
>> brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one RasterBrick
>> testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ##Crop Raster Brick to manageable size
>> 
>> ####
>> #### Predict Biomass using final linear model with Monte Carlo Simulation
>> ndunc(101)
>> fun.CROP_AGWBC <- function(x) {
>>  require(mc2d)
>>  dBC_BA <- mcdata(x[[1]], type="0")
>>  dBC_BA_SE <- mcdata(x[[2]], type = "0")
>>  SlopePer <-x[[3]]
>>  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>>                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>>  BC_AGWBC <- lm.final$coefficients[1] +
>>    lm.final$coefficients[2]*stBA +
>>    lm.final$coefficients[3]*SlopePer
>>  AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>>  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>> }
>> 
>> ####
>> #### Check function using calc
>> CROP_AGWBC <- calc(testbrick, fun.CROP_AGWBC)
>> plot(CROP_AGWBC)
>> CROP_AGWBC
>> 
>> ####
>> #### Run function using parallel processing
>> library(snow)
>> beginCluster(8)
>> library(parallel)
>> cl <- getCluster()
>> clusterExport(cl, list("lm.final", "lambda_DV"))
>> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
>> endCluster()
>> 
>> 
>> RESULT: Again, the calc process works fine to produce object CROP_AGWBC but when I try to run the last bit with parallel processing, I still get the same error:
>> 
>>> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
>> [1] "data should be numeric or logical"
>> attr(,"class")
>> [1] "snow-try-error" "try-error"
>> Error in clusterR(x = testbrick, fun = fun.CROP_AGWBC) : cluster error
>> 
>> 
>> I was suspicious that maybe the issue was still with the export because while ?lambda_DV" is a constant, lm.final is actually an lm model, so I replaced these objects in the function with constants (see code below) but still get the same error:
>> 
>> REVISED CODE WITH CONSTANTS:
>> library(raster)
>> library(rgdal)
>> library(mc2d)
>> brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one RasterBrick
>> testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ##Crop Raster Brick to manageable size
>> 
>> ####
>> #### Predict Biomass using final linear model with Monte Carlo Simulation
>> ndunc(101)
>> fun2.CROP_AGWBC <- function(x) {
>>  require(mc2d)
>>  dBC_BA <- mcdata(x[[1]], type="0")
>>  dBC_BA_SE <- mcdata(x[[2]], type = "0")
>>  SlopePer <-x[[3]]
>>  stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>>                 mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>>  BC_AGWBC <- 0.6419 +
>>    0.9307*stBA +
>>    (-0.0176)*SlopePer
>>  AGWBC <- (0.2626263 * BC_AGWBC + 1)^(1/0.2626263)-1
>>  quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>> }
>> 
>> ####
>> #### Check function using calc
>> CROP_AGWBC_2 <- calc(testbrick, fun2.CROP_AGWBC)
>> plot(CROP_AGWBC_2)
>> CROP_AGWBC_2
>> 
>> ####
>> #### Run function using parallel processing
>> library(snow)
>> beginCluster(8)
>> clusterR(x = testbrick, fun = fun2.CROP_AGWBC)
>> endCluster()
>> 
>> RESULT: same error:
>>> clusterR(x = testbrick, fun = fun2.CROP_AGWBC)
>> [1] "data should be numeric or logical"
>> attr(,"class")
>> [1] "snow-try-error" "try-error"
>> Error in clusterR(x = testbrick, fun = fun2.CROP_AGWBC) : cluster error
>> 
>> 
>> Any other thoughts?  Many thanks,
>> 
>> sean
>> 
>> On Mar 17, 2015, at 8:14 AM, Robert J. Hijmans <r.hijmans at gmail.com> wrote:
>> 
>>> Sean,
>>> 
>>> fun.CROP_AGWBC  refers to objects that are not defined inside the
>>> function ("lm.final" and "lambda_DV"). I assume that this is
>>> intentional and that these represent constants; and that they are
>>> available in your global environment. If so, you need to export these
>>> objects to the cluster nodes. See the 'export' argument in clusterR.
>>> You also need to load necessary packages before calling beginCluster
>>> 
>>> Robert
>>> 
>>> On Mon, Mar 16, 2015 at 1:46 PM, spkearney <sean.kearney at alumni.ubc.ca> wrote:
>>>> Hello all, and thanks in advance for any and all help you can give on this:
>>>> 
>>>> I have set up a function to extract the 2.5%, 50% and 97.5% percentiles from
>>>> a monte carlo simulation on three rasters that is to be called up using
>>>> calc() in the raster package and it works great on a test-sized stack/brick,
>>>> thanks to suggestions at this post here:
>>>> http://grokbase.com/t/r/r-sig-geo/123cb3daaq/apply-monte-carlo-simulation-for-each-cell-in-a-matrix-originally-raster
>>>> 
>>>> My problem, is that I want to run this function on a much larger Raster
>>>> Brick that, as written, takes hours to process.  I need to do this multiple
>>>> times, so I am trying to speed up the processing using clusterR (or another
>>>> option such as rasterEngine with multi-core processing).  However, I can't
>>>> get it to work!   Here is the code that works on the test raster brick:
>>>> 
>>>> brick <- brick(BC_BA, BC_BA_SE, SlopePer)  ## Stack three rasters into one
>>>> Raster Brick
>>>> testbrick <- crop(brick, extent(299700, 300100, 1553550, 1553650)) ## Crop
>>>> brick to manageable size
>>>> 
>>>> ndunc(101)
>>>> fun.CROP_AGWBC <- function(x) {
>>>> dBC_BA <- mcdata(x[[1]], type="0")
>>>> dBC_BA_SE <- mcdata(x[[2]], type = "0")
>>>> SlopePer <-x[[3]]
>>>> stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>>>>                mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>>>> BC_AGWBC <- lm.final$coefficients[1] +
>>>>   lm.final$coefficients[2]*stBA +
>>>>   lm.final$coefficients[3]*SlopePer
>>>> AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>>>> quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>>>> }
>>>> 
>>>> CROP_AGWBC <- calc(teststack, fun.CROP_AGWBC) ##Run the calc function
>>>> CROP_AGWBC ##Check the result
>>>> plot(CROP_AGWBC) ##Plot the three-raster brick result
>>>> 
>>>> ##Extract the individual raster layers
>>>> CROP_AGWBC_PRED <- CROP_AGWBC[[2]]
>>>> CROP_AGWBC_LWR <- CROP_AGWBC[[1]]
>>>> CROP_AGWBC_UPR <- CROP_AGWBC[[3]]
>>>> 
>>>> As I mentioned, the code works great on a small sample.  I tried to speed it
>>>> up using clusterR as follows, first testing it on the 'testbrick' Raster
>>>> Brick with hopes to use it on the whole Raster Brick:
>>>> 
>>>> beginCluster(8)
>>>> clusterR(x = testbrick, fun = fun.CROP_AGWBC)
>>>> 
>>>> and I get the following error:
>>>> [1] "data should be numeric or logical"
>>>> attr(,"class")
>>>> [1] "snow-try-error" "try-error"
>>>> Error in clusterR(x = testbrick, fun = fun.CROP_AGWBC) : cluster error
>>>> 
>>>> It is interesting because, if I try to run it again, I get this error
>>>> instead:
>>>> Error in as.vector((x[, 1] - 1) * ncol(object) + x[, 2]) :
>>>> error in evaluating the argument 'x' in selecting a method for function
>>>> 'as.vector': Error in x[, 2] : subscript out of bounds
>>>> 
>>>> I have tried this many different ways, including along the lines of:
>>>> f <- function (x) calc(x, fun.CROP_AGWBC)
>>>> y <- clusterR(testbrick, f)
>>>> 
>>>> which gives me the same error (more or less) of:
>>>> Error in checkForRemoteErrors(lapply(cl, recvResult)) :
>>>> 2 nodes produced errors; first error: data should be numeric or logical
>>>> 
>>>> And I have tried using the rasterEngine() function (first without parallel
>>>> processing) by changing up the code in two ways, the first being:
>>>> ndunc(101)
>>>> fun.CROP_AGWBC <- function(x) {
>>>> dBC_BA <- mcdata(x[[1]], type="0")
>>>> dBC_BA_SE <- mcdata(x[[2]], type = "0")
>>>> SlopePer <-x[[3]]
>>>> stBA <- mcstoc(rnorm, type = "U", rtrunc = TRUE,
>>>>                mean = dBC_BA, sd = dBC_BA_SE, linf = 0, lhs = FALSE)
>>>> BC_AGWBC <- lm.final$coefficients[1] +
>>>>   lm.final$coefficients[2]*stBA +
>>>>   lm.final$coefficients[3]*SlopePer
>>>> AGWBC <- (lambda_DV * BC_AGWBC + 1)^(1/lambda_DV)-1
>>>> output <- quantile(AGWBC[], c(0.025, 0.5, 0.975), na.rm=TRUE)
>>>> output_array <- array(output,dim=c(dim(x)[1],dim(x)[2],3))
>>>> return(output_array)
>>>> }
>>>> re <- rasterEngine(x = testbrick, fun = fun.CROP_AGWBC)
>>>> 
>>>> which runs but gives me a 3-layer Raster Brick all with NA's or Inf.  The
>>>> second thing I tried used the same fun.CROP_AGWBC function as above, but
>>>> with the following rasterEngine code to call up the calc formula:
>>>> f <- function(x) {reout <- calc(x, fun.CROP_AGWBC)
>>>>                 reout_array <- array(getValues(reout),
>>>> dim=c(dim(x)[1],dim(x)[2],3))
>>>>                 return(reout_array)
>>>> }
>>>> re <- rasterEngine(x = testbrick, fun = f, chunk_format = "raster")
>>>> 
>>>> which gives me the following error, even though I thought I converted the
>>>> output to an array:
>>>> chunk processing units require array vector outputs.  Please check your
>>>> function.
>>>> Error in focal_hpc_test(x, fun, window_center, window_dims, args,
>>>> layer_names,  :
>>>> 
>>>> So, my questions are as follows:
>>>> *1) Does anyone know why the clusterR does not work for the calc() function
>>>> in my first attempt?  I imagine it has something to do with the conversion
>>>> of rasters to mcnodes in the function, but can't figure it out!  Any
>>>> suggestions?
>>>> 
>>>> 2) Any thoughts on why I can't get this to work with the rasterEngine()
>>>> function?  I am converting the outputs to arrays with the same dimensions as
>>>> the input file, but still no luck.*
>>>> 
>>>> Again, any help is much appreciated.  Any suggestions for improving this
>>>> question are welcome and I'll do my best to update it - this is my first
>>>> post!
>>>> 
>>>> Kind Regards,
>>>> sean
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> --
>>>> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/parallel-raster-processing-with-calc-and-mc2d-monte-carlo-simulation-tp7587901.html
>>>> Sent from the R-sig-geo mailing list archive at Nabble.com.
>>>> 
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> 


From strimas at zoology.ubc.ca  Tue Mar 17 19:30:05 2015
From: strimas at zoology.ubc.ca (Matt Strimas-Mackey)
Date: Tue, 17 Mar 2015 11:30:05 -0700
Subject: [R-sig-Geo] Calculating road (i.e. linear feature) density using
	spatstat::density.psp()
Message-ID: <CAHb9yCD6D6uGKApROkChj3e20VKiBorvPj+Nbr2jYZXH-y9p3Q@mail.gmail.com>

Note: I previously posted this question to Stack Exchange, but haven't
receive a response. I will update the SE question with any answers I get on
this listserv:
http://gis.stackexchange.com/questions/138861/calculating-road-density-in-r-using-kernel-density

I have a large (~70MB) shapefile of roads and want to convert this to a
raster with road density/length in each cell. Ideally I'd like to do this
in R.

My initial approach was to directly calculate the lengths of line segments
in each cell. This produces the desired results, but is quite slow even for
shapefiles much smaller than mine. Here's a very simplified example for
which the correct cell values are obvious:

--------------

require(sp)
require(raster)
require(rgeos)
require(RColorBrewer)

# Create some sample lines
l1 <- Lines(Line(cbind(c(0,1),c(.25,0.25))), ID="a")
l2 <- Lines(Line(cbind(c(0.25,0.25),c(0,1))), ID="b")
sl <- SpatialLines(list(l1,l2))

# Function to calculate lengths of lines in given raster cell
lengthInCell <- function(i, r, l) {
    r[i] <- 1
    rpoly <- rasterToPolygons(r, na.rm=T)
    lc <- crop(l, rpoly)
    if (!is.null(lc)) {
        return(gLength(lc))
    } else {
        return(0)
    }
}

# Make template
rLength <- raster(extent(sl), res=0.5)

# Calculate lengths
lengths <- sapply(1:ncell(rLength), lengthInCell, rLength, sl)
rLength[] <- lengths

# Plot results
spplot(rLength, scales = list(draw=TRUE), xlab="x", ylab="y",
       col.regions=colorRampPalette(brewer.pal(9, "YlOrRd")),
       sp.layout=list("sp.lines", sl),
       par.settings=list(fontsize=list(text=15)))
round(as.matrix(rLength),3)

#### Results, road lengths in each cell
     [,1] [,2]
[1,]  0.5  0.0
[2,]  1.0  0.5

--------------

Looks good, but not scaleable! In a previous post on this listserv, the
spatstat::density.psp() function has been recommended for this task. This
function uses a kernel density approach. I am able to implement it and it
seem faster than the above approach, but I'm unclear how to choose the
parameters or interpret the results. Here's the above example using
density.psp():

--------------

require(spatstat)
require(maptools)

# Convert SpatialLines to psp object using maptools library
pspSl <- as.psp(sl)
# Kernel density, sigma chosen more or less arbitrarily
d <- density(pspSl, sigma=0.01, eps=0.5)
# Convert to raster
rKernDensity <- raster(d)
# Values:
round(as.matrix(rKernDensity),3)

#### Results
      [,1] [,2]
[1,] 0.100  0.0
[2,] 0.201  0.1

--------------

I thought it might be the case that the kernel approach calculates density
as opposed to length per cell, so I converted:

--------------

# Convert from density to length per cell for comparison
rKernLength <- rKernDensity * res(rKernDensity)[1] * res(rKernDensity)[2]
round(as.matrix(rKernLength),3)

#### Results
      [,1]  [,2]
[1,] 0.025 0.000
[2,] 0.050 0.025

--------------

But, in neither case, does the kernel approach come close to aligning with
the more direct approach above.

So, my questions are:
1. How can I interpret the output of the density.psp function? What are the
units?
2. How can I choose the sigma parameter in density.psp so the results align
with the more direct, intuitive approach above?
3. Bonus: what is the kernel line density actually doing? I have some sense
for how these approaches work for points, but don't see how that extends to
lines.

Thanks!

	[[alternative HTML version deleted]]


From julleeyaw at yahoo.ca  Tue Mar 17 20:42:01 2015
From: julleeyaw at yahoo.ca (Julie Lee-Yaw)
Date: Tue, 17 Mar 2015 19:42:01 +0000 (UTC)
Subject: [R-sig-Geo] determining scale of autocorrelation in a raster
Message-ID: <137056762.52805.1426621321266.JavaMail.yahoo@mail.yahoo.com>

Hi
I am attempting to explore the scale of spatial autocorrelation in a raster (eventually across a stack of 10 but for now a single layer) and consequently in a potential sample of points across the landscape (ie. if we wanted to know what sampling design in terms of distance would minimize autocorrelation). I?ve spent a couple of days trying to understand the various ways to evaluate spatial autocorrelation for a raster or points dataset but am struggling with a few questions. I hope someone can kindly shed some light on the following (in my example I?m playing with a single WorldClim layer at a resolution of 1 km, cropped to the eastern third of the USA):
1) In spdep, I?ve done the following (with my [potentially erroneous] thinking laid out in the comments and two questions at the end):
### use the raster package to get a regular sample of points across the raster### because using the full set of cells or their centroids on a large raster seems to crash R downstream
y<-sampleRegular(myraster,size=1000,xy=TRUE)
### tidy point dataset### in particular, missing values (e.g. the ocean) in the raster and thus in the points lead to errors later, so remove these
dat<-as.data.frame(y)dd<-y[complete.cases(dat),]dd$ID<-row.names(dd)c<-coordinates(dd[,c("x","y")])
### make nb object (provides list of nearest neighbours for lower lag class)### here I?ve chosen k=8 which I?m assuming given the regular sampling of points is almost akin to the ?queens? design in the raster-specific cell2nb command (except for cells near the ocean)
k1_nb<-knn2nb(knearneigh(c,k=8,longlat=TRUE),row.names=dd$IDs)
### make correlogram
sp.cor<-sp.correlogram(k1_nb,dd$V3,order=15,method="I")plot(sp.cor)
Two questions here:?
a) I?ve been able to successfully set the order to 15 but not 20 before there are empty neighbour sets found for this particular dataset. Is there a way, other than by trial and error to tell the maximum order possible?
b) After plotting the correlogram, I get the Moran?s I as a function of lag distance. I see it crosses the 0 line between lags 13 and 14 ?is there a way to tell what distance this amounts to in kms??
2) Using the pgirmess package (which I understand to be calculating the lags in a fundamentally different way) I can get a correlogram with distances?
### so now I reproject the raster to albers equal area in order to have the units on the x axis be metres (and actually the projection I want to use in the end anyways)?### the rest of the steps to create dd2 are the same as above### use the correlog function to create correlogram
pgi.cor?<- correlog(coords=dd2[,1:2],?z=dd2$V3,?method="Moran",?nbclass=20)plot(pgi.cor)
Questions:
a) In my new plot, the distance class at which Moran?s I is no longer significantly different from zero is around 600 km. That seems really far to me?am I wrong in my interpretation that this distance represents the distance beyond which sample sites would be are relatively free from autocorrelation? or is this truly representative of the scale of autocorrelation that I can expect in climate data over the relatively modest topographic complexity of the eastern USA??
b) In general, when/ for what types of questions or datasets is the approach used by spdep to generate the lag steps more appropriate than the (fixed bins?) method of pgirmess??
3) Finally?
Please forgive me if I?m approaching this problem incorrectly altogether! I?m eventually hoping to say something along the lines of ?if we take sites x distance apart, we can be fairly sure that the amount of spatial autocorrelation in our climate data will be minimal?. But maybe this is completely ridiculous? I?d be really happy to have some suggestions.?(and on a side note, I?m currently looking for a good introduction to spatial statistics course or textbook?something for the truly uninitiated. Any recommendations?)
Many thanks!
PS. Online sources for some of the code above:
http://www.r-bloggers.com/spatial-correlograms-in-r-a-mini-overview/http://www.bias-project.org.uk/ASDARcourse/unit6_slides.pdf

	[[alternative HTML version deleted]]


From Frederic.Pons at cerema.fr  Wed Mar 18 11:31:28 2015
From: Frederic.Pons at cerema.fr (PONS Frederic - CEREMA/DTerMed/DREC/SRILH)
Date: Wed, 18 Mar 2015 11:31:28 +0100
Subject: [R-sig-Geo] How to change coordonnate in a vector spatiladataframe
Message-ID: <55095400.4000202@cerema.fr>

Hi

I try to change some coordinate in a vector read by readOGR
Vecteur = readOGR(dsnlayer,layer=nomlayer)

After some calculation, I try this without success:
Vecteur[nbi,]@coords=cbind(ProfilProj[,1],ProfilProj[,2])

The error is:
Erreur dans as.vector(data) :
   pas de m?thode pour convertir automatiquement cette classe S4 en vecteur

Thanks and best regards
-- 
*Fr?d?ric Pons *
*Expert hydraulique sur les inondations et al?as c?tiers
**DREC/Service Risques Inondations Littoraux et Hydraulique **- T?l.: 
(33)4 42 24 76 68 *
*Direction Territoriale M?diterran?e
*
Centre d'?tudes et d'expertise sur les risques, l'environnement, la 
mobilit? et l'am?nagement
www.cerema.fr <http://www.cerema.fr>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150318/689f715b/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cerema.png
Type: image/png
Size: 6094 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150318/689f715b/attachment.png>

From mathieu.rajerison at gmail.com  Wed Mar 18 11:35:23 2015
From: mathieu.rajerison at gmail.com (Mathieu Rajerison)
Date: Wed, 18 Mar 2015 11:35:23 +0100
Subject: [R-sig-Geo] Polygons VS MultiLineString
In-Reply-To: <CAPrLoNe4it5pKTZyZQXQ71EyTEoXZTtjgUDWB_mFNFbhvvO+xw@mail.gmail.com>
References: <CAPrLoNftAT9DWwVdB-jsupHTx_FwkPVe6K0nWHrOSCBk8W4iQA@mail.gmail.com>
	<CAAcGz9-LnYSc_gTxgBkaeBB9Twb-PCNW3paEqHQ767pp_otHgw@mail.gmail.com>
	<B078CDF40DFE4045AF172A8B4F68FC48587FE4015B@DKRDSEXC016.vestas.net>
	<CAAcGz98Ttdfy+WKWPb7jiNQ+Ov0+a4w1baunt5c-3sAaw=4b_g@mail.gmail.com>
	<CAPrLoNfHR8ymMRXdMyHb3vmohY-Lnys7N6qo+kU=tAi1rV29CA@mail.gmail.com>
	<CAPrLoNe4it5pKTZyZQXQ71EyTEoXZTtjgUDWB_mFNFbhvvO+xw@mail.gmail.com>
Message-ID: <CAGfc75=Z1--0YuuEnt6NR8wPEdyYUbeQq4TLAc8rnCip+xkSqA@mail.gmail.com>

Hello,

My advice is : why don't you reclass your raster in categories according to
raster values intervals, then you polygonize your reclassified raster ?

It would be simpler and you would have a topologically correct layer.

That's the way I do it with GRASS but you all the functions also are
included in the raster package.

Mat



2015-03-09 12:44 GMT+01:00 Antonio Rodriges <antonio.rrz at gmail.com>:

> The solution I've devised so far
>
> contours <- rasterToContour(to_c, levels = c(18, 33))
>
> res <- lapply(slot(contours, "lines"), function(x) lapply(slot(x,
> "Lines"), function(y) slot(y, "coords")))
> polies <- lapply(res, function (x) lapply(x, function (y) Polygon(y)))
> pp_18 <- Polygons(polies[[1]], ID = "18")
> pp_33 <- Polygons(polies[[2]], ID = "33")
> all_pp<- SpatialPolygons(c(pp_18, pp_33))
> df <- data.frame(value = c(18, 33), row.names = c("18", "33"))
> SPDF <- SpatialPolygonsDataFrame(all_pp, df)
>
> writeOGR(SPDF, jsfile, layer="", driver="GeoJSON")
>
> This is in case you have only two values 18 and 33. If you have more,
> create list and create Polygons also using lapply or for loop
>
> 2015-03-05 11:49 GMT+03:00 Antonio Rodriges <antonio.rrz at gmail.com>:
> > Mike,
> >
> > Thank you, that was a great example; I reproduced it.
> >
> > However, if I am satisfied with the rasterToContour output, I think
> > there should be a way to directly convert it polygons. I am seeking
> > this method.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Wed Mar 18 12:05:13 2015
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 18 Mar 2015 12:05:13 +0100
Subject: [R-sig-Geo] How to change coordonnate in a vector
	spatiladataframe
In-Reply-To: <55095400.4000202@cerema.fr>
References: <55095400.4000202@cerema.fr>
Message-ID: <55095BE9.8000002@uni-muenster.de>



On 03/18/2015 11:31 AM, PONS Frederic - CEREMA/DTerMed/DREC/SRILH wrote:
> Hi
> 
> I try to change some coordinate in a vector read by readOGR
> Vecteur = readOGR(dsnlayer,layer=nomlayer)
> 
> After some calculation, I try this without success:
> Vecteur[nbi,]@coords=cbind(ProfilProj[,1],ProfilProj[,2])

try:

Vecteur_b = Vecteur[nbi,]
Vecteur_b at coords=cbind(ProfilProj[,1],ProfilProj[,2])

or:

Vecteur at coords[nbi,]=cbind(ProfilProj[,1],ProfilProj[,2])

> 
> The error is:
> Erreur dans as.vector(data) :
>   pas de m?thode pour convertir automatiquement cette classe S4 en vecteur
> 
> Thanks and best regards
> -- 
> *Fr?d?ric Pons *
> *Expert hydraulique sur les inondations et al?as c?tiers
> **DREC/Service Risques Inondations Littoraux et Hydraulique **- T?l.:
> (33)4 42 24 76 68 *
> *Direction Territoriale M?diterran?e
> *
> Centre d??tudes et d?expertise sur les risques, l?environnement, la
> mobilit? et l?am?nagement
> www.cerema.fr <http://www.cerema.fr>
> 
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi),  University of M?nster,
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150318/0e689297/attachment.bin>

From Roger.Bivand at nhh.no  Wed Mar 18 12:14:33 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 18 Mar 2015 12:14:33 +0100
Subject: [R-sig-Geo] How to change coordonnate in a vector
 spatiladataframe
In-Reply-To: <55095BE9.8000002@uni-muenster.de>
References: <55095400.4000202@cerema.fr> <55095BE9.8000002@uni-muenster.de>
Message-ID: <alpine.LFD.2.11.1503181210460.9411@reclus.nhh.no>

On Wed, 18 Mar 2015, Edzer Pebesma wrote:

>
>
> On 03/18/2015 11:31 AM, PONS Frederic - CEREMA/DTerMed/DREC/SRILH wrote:
>> Hi
>>
>> I try to change some coordinate in a vector read by readOGR
>> Vecteur = readOGR(dsnlayer,layer=nomlayer)
>>
>> After some calculation, I try this without success:
>> Vecteur[nbi,]@coords=cbind(ProfilProj[,1],ProfilProj[,2])
>
> try:
>
> Vecteur_b = Vecteur[nbi,]
> Vecteur_b at coords=cbind(ProfilProj[,1],ProfilProj[,2])
>
> or:
>
> Vecteur at coords[nbi,]=cbind(ProfilProj[,1],ProfilProj[,2])

and remember to update the bbox slot as nothing is automatic when slots 
are modified directly.

I'm also unsure whether Vecteur is a SpatialPointsDataFrame, although it 
looks as though it may be.

>
>>
>> The error is:
>> Erreur dans as.vector(data) :
>>   pas de m?thode pour convertir automatiquement cette classe S4 en vecteur
>>
>> Thanks and best regards
>> --
>> *Fr?d?ric Pons *
>> *Expert hydraulique sur les inondations et al?as c?tiers
>> **DREC/Service Risques Inondations Littoraux et Hydraulique **- T?l.:
>> (33)4 42 24 76 68 *
>> *Direction Territoriale M?diterran?e
>> *
>> Centre d??tudes et d?expertise sur les risques, l?environnement, la
>> mobilit? et l?am?nagement
>> www.cerema.fr <http://www.cerema.fr>
>>
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no

From Frederic.Pons at cerema.fr  Wed Mar 18 13:44:25 2015
From: Frederic.Pons at cerema.fr (PONS Frederic - CEREMA/DTerMed/DREC/SRILH)
Date: Wed, 18 Mar 2015 13:44:25 +0100
Subject: [R-sig-Geo] How to change coordonnate in a vector
	spatiladataframe
In-Reply-To: <alpine.LFD.2.11.1503181210460.9411@reclus.nhh.no>
References: <55095400.4000202@cerema.fr> <55095BE9.8000002@uni-muenster.de>
	<alpine.LFD.2.11.1503181210460.9411@reclus.nhh.no>
Message-ID: <55097329.9080409@cerema.fr>

Thanks
Vecteur at coords[nbi,]=cbind(ProfilProj[,1],ProfilProj[,2]) is what I want
Fr?d?ric

Le 18/03/2015 12:14, "> Roger Bivand (par Internet, d?p?t 
r-sig-geo-bounces at r-project.org)" a ?crit :
> On Wed, 18 Mar 2015, Edzer Pebesma wrote:
>
>>
>>
>> On 03/18/2015 11:31 AM, PONS Frederic - CEREMA/DTerMed/DREC/SRILH wrote:
>>> Hi
>>>
>>> I try to change some coordinate in a vector read by readOGR
>>> Vecteur = readOGR(dsnlayer,layer=nomlayer)
>>>
>>> After some calculation, I try this without success:
>>> Vecteur[nbi,]@coords=cbind(ProfilProj[,1],ProfilProj[,2])
>>
>> try:
>>
>> Vecteur_b = Vecteur[nbi,]
>> Vecteur_b at coords=cbind(ProfilProj[,1],ProfilProj[,2])
>>
>> or:
>>
>> Vecteur at coords[nbi,]=cbind(ProfilProj[,1],ProfilProj[,2])
>
> and remember to update the bbox slot as nothing is automatic when 
> slots are modified directly.
>
> I'm also unsure whether Vecteur is a SpatialPointsDataFrame, although 
> it looks as though it may be.
>
>>
>>>
>>> The error is:
>>> Erreur dans as.vector(data) :
>>>   pas de m?thode pour convertir automatiquement cette classe S4 en 
>>> vecteur
>>>
>>> Thanks and best regards
>>> -- 
>>> *Fr?d?ric Pons *
>>> *Expert hydraulique sur les inondations et al?as c?tiers
>>> **DREC/Service Risques Inondations Littoraux et Hydraulique **- T?l.:
>>> (33)4 42 24 76 68 *
>>> *Direction Territoriale M?diterran?e
>>> *
>>> Centre d'?tudes et d'expertise sur les risques, l'environnement, la
>>> mobilit? et l'am?nagement
>>> www.cerema.fr <http://www.cerema.fr>
>>>
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>>
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From mathieu.rajerison at gmail.com  Wed Mar 18 13:55:14 2015
From: mathieu.rajerison at gmail.com (Mathieu Rajerison)
Date: Wed, 18 Mar 2015 13:55:14 +0100
Subject: [R-sig-Geo] customized focal mean
In-Reply-To: <1426138645618-7587890.post@n2.nabble.com>
References: <1426138645618-7587890.post@n2.nabble.com>
Message-ID: <CAGfc75kG23JnApDd46pUoWQ6jd-T9-MfBDYyqYfag_rxP-OriA@mail.gmail.com>

Hi,

Maybe you could look at raster::focal function and w (matrix of weights
arguments)
http://www.inside-r.org/packages/cran/raster/docs/focal

"w
matrix of weights (the moving window), e.g. a 3 by 3 matrix with values 1;
see Details. The matrix does not need to be square, but the sides must be
odd numbers. If you need even sides, you can add a column or row with
weights of zero"

2015-03-12 6:37 GMT+01:00 hariom <hariom.cs1 at gmail.com>:

> Hello list,
>
> I want to customized focal mean in which my concern is to replace mean
> value
> with the matrix such as:
>
> *2 2 2* 2
> *2 1 2* 1
> *1 2 4* 2
> 2 1 2 2
>
> first generate the mean of 3*3 matrix and then replace with entire matrix
> and it will work throughout the raster data.
> first matrix mean is 2 and  it will replace and continue.....
>
>  2 *2 2 2*
>  2 *2 2 1*
>  2 *2 2 2*
>
>
> Please help me i will appreciate for your time and concern
>
> Hariom singh
> Research scholar IIT Roorkee
>
>
>
>
> --
> View this message in context:
> http://r-sig-geo.2731867.n2.nabble.com/customized-focal-mean-tp7587890.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From adrian.baddeley at uwa.edu.au  Thu Mar 19 01:42:35 2015
From: adrian.baddeley at uwa.edu.au (Adrian Baddeley)
Date: Thu, 19 Mar 2015 08:42:35 +0800
Subject: [R-sig-Geo] Calculating road (i.e. linear feature) density using
 spatstat::density.psp()
Message-ID: <CF5661163F77A44781208D9AC4FDEA72014010A2989A@IS-WIN-376.staffad.uwa.edu.au>

Matt Strimas-Mackey <strimas at zoology.ubc.ca> writes:

> Note: I previously posted this question to Stack Exchange, 
> but haven't receive a response.

Questions about the spatstat package can best be addressed directly to the authors.

> I have a large (~70MB) shapefile of roads and want to convert this to a
> raster with road density/length in each cell. Ideally I'd like to do this in R.

spatstat::pixellate.psp will do this. 
Use the "..." arguments to specify the raster dimensions.
                L <- as.psp(YourData)
                Z <- pixellate(L, dimyx=512)
The resulting image values give the total length of fragments of lines in each pixel. 
To get values which are lengths per unit area, divide by the pixel area:
                Z <- Z/with(Z, xstep * ystep)

> In a previous post on this listserv, the spatstat::density.psp() function 
> has been recommended for this task. 

Uh, no, that function does something else. 

> 1. How can I interpret the output of the density.psp function? 


As the help file states, density.psp computes the convolution of the Gaussian kernel with the lines.
Intuitively, this means that density.psp 'smears' the lines into two-dimensional space.  
So density(L) is like a blurred version of pixellate(L). 
In fact density(L) is very similar to blur(pixellate(L)) where blur is another spatstat function that blurs an image.

> What are the units?

Units are length^(-1), i.e. line length per unit area

> 2. How can I choose the sigma parameter in density.psp so the results align
> with the more direct, intuitive approach above?

They do not agree unless sigma is very small.

'sigma' is the bandwidth of the Gaussian kernel.
The value of density.psp(L) at a given pixel u, is something like the total amount of line length in a circle of radius sigma
around the pixel u, except that it's really a weighted average of such contributions from different circle radii.

> 3. Bonus: what is the kernel line density actually doing? 
> I have some sense for how these approaches work for points, 
> but don't see how that extends to lines.

Imagine replacing each line by a fine grid of points, each point having a 'weight'
equal to the length of line that it replaced. Then apply density.ppp to these points with weights.
The result is the density.psp output.

Adrian Baddeley
spatstat author

Prof Adrian Baddeley FAA
Curtin University

From gosz at gmx.de  Thu Mar 19 18:56:46 2015
From: gosz at gmx.de (Guido Schulz)
Date: Thu, 19 Mar 2015 18:56:46 +0100
Subject: [R-sig-Geo] plotKML::plotKML creates multiple errors in Google
 Earth (Ubuntu 14.10)
Message-ID: <550B0DDE.2060401@gmx.de>

Hi there,

I am trying to use the R package *plotKML* to plot a
sp::SpatialPolygonDataframe in *GoogleEarth*. I am using Ubuntu 14.10.
Unfortunately, I facing various errors here...

Here my reproducable example in R:

############################################
    library("plotKML")
    library("CARBayes")
    library("sp")
    data(spatialhousedata)
    names(spatialhousedata at data)
    proj4string(spatialhousedata) <- CRS("+proj=utm
                +zone=33 +ellps=GRS80 +units=m +no_defs")
    plotKML(spatialhousedata, var.name="price")
############################################

First, R spits out a lot of error lines:

#############################################
    Plotting the first variable on the list
    KML file opened for writing...
    Reprojecting to +proj=longlat +datum=WGS84 ...
    Writing to KML...
    Closing  spatialhousedata.kml
    [0319/101818:ERROR:net_util.cc(2195)] Not implemented reached in
bool net::HaveOnlyLoopbackAddresses()
    [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
handler.
    [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
handler.
    [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
handler.
    [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
handler.
...
...
...
#############################################

Then *GoogleEarth* opens and welcomes me with this error message
(which I translated from German):

----------------------------
*Polygons bound to the ground are not supported*

Polygons bound to the ground can only be displayed as boundaries on
your computer. Your graphics does not support this function.
----------------------------

Then it finally plots only the boundaries of *spatialhousedata* instead
of the coloured polygons (choropleth map) of spatialhousedata at data$price.

Here is my graphics info from Ubuntu:

-----------------------------
    me at me-ThinkPad-T420:~$  sudo lshw -c video

      *-display
           Beschreibung: VGA compatible controller
           Produkt: 2nd Generation Core Processor
                    Family Integrated Graphics Controller
           Hersteller: Intel Corporation
           Physische ID: 2
           Bus-Informationen: pci at 0000:00:02.0
           Version: 09
           Breite: 64 bits
           Takt: 33MHz
           F?higkeiten: msi pm vga_controller bus_master cap_list rom
           Konfiguration: driver=i915 latency=0
           Ressourcen: irq:42 memory:f0000000-f03fffff
                       memory:e0000000-efffffff ioport:5000(Gr??e=64)
------------------------------

And this is the version info from Google Earth:

-----------------------------
    Google Earth: 7.1.2.2041
    Build-Date: 10/7/2013
    Build-Time: 12:17:00 nachmittags
    Renderer: OpenGL
    OS: Linux (3.13.0.0)
    Graphics Driver: Intel Open Source Technology Center
-----------------------------

*Any ideas how to solve this?*

Best,

Guido


From edzer.pebesma at uni-muenster.de  Thu Mar 19 22:38:25 2015
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Thu, 19 Mar 2015 22:38:25 +0100
Subject: [R-sig-Geo] plotKML::plotKML creates multiple errors in Google
 Earth (Ubuntu 14.10)
In-Reply-To: <550B0DDE.2060401@gmx.de>
References: <550B0DDE.2060401@gmx.de>
Message-ID: <550B41D1.3020205@uni-muenster.de>

I can reproduce this under ubuntu 14.04 LTS. In my understanding, the
ubuntu (or linux) google-earth is by far not as good as it is under
windows or Mac OS-X, and this has been so for a long time. The only
solution I can think of is to work towards a solution without
google-earth, or make the switch to windows or mac.

Or is the story different with google-earth pro, which is now free?

On 03/19/2015 06:56 PM, Guido Schulz wrote:
> Hi there,
> 
> I am trying to use the R package *plotKML* to plot a
> sp::SpatialPolygonDataframe in *GoogleEarth*. I am using Ubuntu 14.10.
> Unfortunately, I facing various errors here...
> 
> Here my reproducable example in R:
> 
> ############################################
>     library("plotKML")
>     library("CARBayes")
>     library("sp")
>     data(spatialhousedata)
>     names(spatialhousedata at data)
>     proj4string(spatialhousedata) <- CRS("+proj=utm
>                 +zone=33 +ellps=GRS80 +units=m +no_defs")
>     plotKML(spatialhousedata, var.name="price")
> ############################################
> 
> First, R spits out a lot of error lines:
> 
> #############################################
>     Plotting the first variable on the list
>     KML file opened for writing...
>     Reprojecting to +proj=longlat +datum=WGS84 ...
>     Writing to KML...
>     Closing  spatialhousedata.kml
>     [0319/101818:ERROR:net_util.cc(2195)] Not implemented reached in
> bool net::HaveOnlyLoopbackAddresses()
>     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> handler.
>     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> handler.
>     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> handler.
>     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> handler.
> ...
> ...
> ...
> #############################################
> 
> Then *GoogleEarth* opens and welcomes me with this error message
> (which I translated from German):
> 
> ----------------------------
> *Polygons bound to the ground are not supported*
> 
> Polygons bound to the ground can only be displayed as boundaries on
> your computer. Your graphics does not support this function.
> ----------------------------
> 
> Then it finally plots only the boundaries of *spatialhousedata* instead
> of the coloured polygons (choropleth map) of spatialhousedata at data$price.
> 
> Here is my graphics info from Ubuntu:
> 
> -----------------------------
>     me at me-ThinkPad-T420:~$  sudo lshw -c video
> 
>       *-display
>            Beschreibung: VGA compatible controller
>            Produkt: 2nd Generation Core Processor
>                     Family Integrated Graphics Controller
>            Hersteller: Intel Corporation
>            Physische ID: 2
>            Bus-Informationen: pci at 0000:00:02.0
>            Version: 09
>            Breite: 64 bits
>            Takt: 33MHz
>            F?higkeiten: msi pm vga_controller bus_master cap_list rom
>            Konfiguration: driver=i915 latency=0
>            Ressourcen: irq:42 memory:f0000000-f03fffff
>                        memory:e0000000-efffffff ioport:5000(Gr??e=64)
> ------------------------------
> 
> And this is the version info from Google Earth:
> 
> -----------------------------
>     Google Earth: 7.1.2.2041
>     Build-Date: 10/7/2013
>     Build-Time: 12:17:00 nachmittags
>     Renderer: OpenGL
>     OS: Linux (3.13.0.0)
>     Graphics Driver: Intel Open Source Technology Center
> -----------------------------
> 
> *Any ideas how to solve this?*
> 
> Best,
> 
> Guido
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi),  University of M?nster,
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150319/694642b9/attachment.bin>

From mdsumner at gmail.com  Thu Mar 19 23:04:57 2015
From: mdsumner at gmail.com (Michael Sumner)
Date: Thu, 19 Mar 2015 22:04:57 +0000
Subject: [R-sig-Geo] plotKML::plotKML creates multiple errors in Google
 Earth (Ubuntu 14.10)
References: <550B0DDE.2060401@gmx.de> <550B41D1.3020205@uni-muenster.de>
Message-ID: <CAAcGz99K++R18SiPFx8kA-=t=-qdJNJKZaDA6Lp-XdJR5jJaEA@mail.gmail.com>

Cesium looks like a good alternative to GE, fwiw.

Cheers, Mike

On Fri, 20 Mar 2015 08:39 Edzer Pebesma <edzer.pebesma at uni-muenster.de>
wrote:

> I can reproduce this under ubuntu 14.04 LTS. In my understanding, the
> ubuntu (or linux) google-earth is by far not as good as it is under
> windows or Mac OS-X, and this has been so for a long time. The only
> solution I can think of is to work towards a solution without
> google-earth, or make the switch to windows or mac.
>
> Or is the story different with google-earth pro, which is now free?
>
> On 03/19/2015 06:56 PM, Guido Schulz wrote:
> > Hi there,
> >
> > I am trying to use the R package *plotKML* to plot a
> > sp::SpatialPolygonDataframe in *GoogleEarth*. I am using Ubuntu 14.10.
> > Unfortunately, I facing various errors here...
> >
> > Here my reproducable example in R:
> >
> > ############################################
> >     library("plotKML")
> >     library("CARBayes")
> >     library("sp")
> >     data(spatialhousedata)
> >     names(spatialhousedata at data)
> >     proj4string(spatialhousedata) <- CRS("+proj=utm
> >                 +zone=33 +ellps=GRS80 +units=m +no_defs")
> >     plotKML(spatialhousedata, var.name="price")
> > ############################################
> >
> > First, R spits out a lot of error lines:
> >
> > #############################################
> >     Plotting the first variable on the list
> >     KML file opened for writing...
> >     Reprojecting to +proj=longlat +datum=WGS84 ...
> >     Writing to KML...
> >     Closing  spatialhousedata.kml
> >     [0319/101818:ERROR:net_util.cc(2195)] Not implemented reached in
> > bool net::HaveOnlyLoopbackAddresses()
> >     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> > handler.
> >     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> > handler.
> >     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> > handler.
> >     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> > handler.
> > ...
> > ...
> > ...
> > #############################################
> >
> > Then *GoogleEarth* opens and welcomes me with this error message
> > (which I translated from German):
> >
> > ----------------------------
> > *Polygons bound to the ground are not supported*
> >
> > Polygons bound to the ground can only be displayed as boundaries on
> > your computer. Your graphics does not support this function.
> > ----------------------------
> >
> > Then it finally plots only the boundaries of *spatialhousedata* instead
> > of the coloured polygons (choropleth map) of spatialhousedata at data
> $price.
> >
> > Here is my graphics info from Ubuntu:
> >
> > -----------------------------
> >     me at me-ThinkPad-T420:~$  sudo lshw -c video
> >
> >       *-display
> >            Beschreibung: VGA compatible controller
> >            Produkt: 2nd Generation Core Processor
> >                     Family Integrated Graphics Controller
> >            Hersteller: Intel Corporation
> >            Physische ID: 2
> >            Bus-Informationen: pci at 0000:00:02.0
> >            Version: 09
> >            Breite: 64 bits
> >            Takt: 33MHz
> >            F?higkeiten: msi pm vga_controller bus_master cap_list rom
> >            Konfiguration: driver=i915 latency=0
> >            Ressourcen: irq:42 memory:f0000000-f03fffff
> >                        memory:e0000000-efffffff ioport:5000(Gr??e=64)
> > ------------------------------
> >
> > And this is the version info from Google Earth:
> >
> > -----------------------------
> >     Google Earth: 7.1.2.2041
> >     Build-Date: 10/7/2013
> >     Build-Time: 12:17:00 nachmittags
> >     Renderer: OpenGL
> >     OS: Linux (3.13.0.0)
> >     Graphics Driver: Intel Open Source Technology Center
> > -----------------------------
> >
> > *Any ideas how to solve this?*
> >
> > Best,
> >
> > Guido
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Edzer Pebesma
> Institute for Geoinformatics (ifgi),  University of M?nster,
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
> Spatial Statistics Society http://www.spatialstatistics.info
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From Dominik.Schneider at colorado.edu  Fri Mar 20 00:02:45 2015
From: Dominik.Schneider at colorado.edu (dschneiderch)
Date: Thu, 19 Mar 2015 16:02:45 -0700 (MST)
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
Message-ID: <1426806165111-7587920.post@n2.nabble.com>

I have a spatial points dF that is causing me trouble. I've figured out what
is happening but without a clue why.

at the prompt, I do
> locs
class       : SpatialPointsDataFrame
features    : 10
extent      : -112.0623, -109.0571, 33.65387, 36.32678  (xmin, xmax, ymin,
ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
variables   : 7
names       : network, State, Station_ID, Site_ID,        Site_Name,
Elevation_ft, Elevation_m
min values  :    SNTL,    AZ,     09N05S,     308,      BAKER BUTTE,        
7100,        2164
max values  :    SNTL,    AZ,     12P01S,    1143, HANNAGAN MEADOWS,        
9200,        2804

> head(as.data.frame(locs))
          x        y network State Station_ID Site_ID       Site_Name
1 -111.4064 34.45660    SNTL    AZ     11R06S     308     BAKER BUTTE
2 -111.3827 34.45547    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT
3 -109.5034 33.97883    SNTL    AZ     09S01S     310           BALDY
4 -109.2166 33.69144    SNTL    AZ     09S06S     902     BEAVER HEAD
5 -109.0571 36.32678    SNTL    AZ     09N05S    1143   BEAVER SPRING
6 -112.0623 35.26247    SNTL    AZ     12P01S    1139       CHALENDER
  Elevation_ft Elevation_m
1         7300        2225
2         7700        2347
3         9125        2781
4         7990        2435
5         9200        2804
6         7100        2164

so as expected(?) my coordinate names get converted from Longitude, Latitude
to x, y.

However, when I run my script, the output of head(as.data.frame(locs)) is:
    Longitude   Latitude network State Station_ID Site_ID       Site_Name
1 -111.4064 34.45660    SNTL    AZ     11R06S     308     BAKER BUTTE
2 -111.3827 34.45547    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT
3 -109.5034 33.97883    SNTL    AZ     09S01S     310           BALDY
4 -109.2166 33.69144    SNTL    AZ     09S06S     902     BEAVER HEAD
5 -109.0571 36.32678    SNTL    AZ     09N05S    1143   BEAVER SPRING
6 -112.0623 35.26247    SNTL    AZ     12P01S    1139       CHALENDER
  Elevation_ft Elevation_m
1         7300        2225
2         7700        2347
3         9125        2781
4         7990        2435
5         9200        2804
6         7100        2164


I found out the hard way because i was doing
as.data.frame(locs)[,c('x','y')] to get the coordinates.... I switched this
line to coordinates(locs) but I have other lines in the same code that use
as.data.frame() so I'm wondering if there are designed circumstance for one
behavior compared to the other.  I did notice that
data.frame(locs)[,c('x','y')] seems to always maintain the original
coordinate names but I confirmed that the script uses as.data.frame()

Below is dput of a sample of my data. does anyone else get this behavior?

> dput(locs)
new("SpatialPointsDataFrame"
    , data = structure(list(network = c("SNTL", "SNTL", "SNTL", "SNTL",
"SNTL",
"SNTL", "SNTL", "SNTL", "SNTL", "SNTL"), State = c("AZ", "AZ",
"AZ", "AZ", "AZ", "AZ", "AZ", "AZ", "AZ", "AZ"), Station_ID = c("11R06S",
"11R07S", "09S01S", "09S06S", "09N05S", "12P01S", "09S07S", "11P02S",
"11P13S", "09S11S"), Site_ID = c(308L, 1140L, 310L, 902L, 1143L,
1139L, 416L, 1121L, 488L, 511L), Site_Name = c("BAKER BUTTE",
"BAKER BUTTE SMT", "BALDY", "BEAVER HEAD", "BEAVER SPRING", "CHALENDER",
"CORONADO TRAIL", "FORT VALLEY", "FRY", "HANNAGAN MEADOWS"),
    Elevation_ft = c(7300L, 7700L, 9125L, 7990L, 9200L, 7100L,
    8400L, 7350L, 7200L, 9020L), Elevation_m = c(2225L, 2347L,
    2781L, 2435L, 2804L, 2164L, 2560L, 2240L, 2195L, 2749L)), .Names =
c("network",
"State", "Station_ID", "Site_ID", "Site_Name", "Elevation_ft",
"Elevation_m"), row.names = 63:72, class = "data.frame")
    , coords.nrs = c(7L, 6L)
    , coords = structure(c(-111.40643, -111.38272, -109.50344, -109.21657,
-109.05711,
-112.06231, -109.15282, -111.74486, -111.84374, -109.30952, 34.4566,
34.45547, 33.97883, 33.69144, 36.32678, 35.26247, 33.80392, 35.26806,
35.07297, 33.65387), .Dim = c(10L, 2L), .Dimnames = list(NULL,
    c("Longitude", "Latitude")))
    , bbox = structure(c(-112.06231, 33.65387, -109.05711, 36.32678), .Dim =
c(2L,
2L), .Dimnames = list(c("Longitude", "Latitude"), c("min", "max"
)))
    , proj4string = new("CRS"
    , projargs = "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
)
)


This is running on a cluster at my university, using a SOCK cluster to
parallelize dlply and then an MC backend for a ddply inside the dlply, if
that's important. It seems to produce the expected behavior of my desktop.
> sessionInfo()
R version 3.1.2 (2014-10-31)
Platform: x86_64-unknown-linux-gnu (64-bit)

locale:
[1] C

attached base packages:
[1] grid      parallel  stats     graphics  grDevices utils     datasets
[8] methods   base

other attached packages:
 [1] ncdf4_1.13          smwrBase_1.0.1      lubridate_1.3.3
 [4] digest_0.6.8        memoise_0.2.1       gridExtra_0.9.1
 [7] spdep_0.5-82        Matrix_1.1-4        fields_7.1
[10] maps_2.3-9          spam_1.0-1          doSNOW_1.0.12
[13] snow_0.3-13         doMC_1.3.3          iterators_1.0.7
[16] foreach_1.4.2       ipred_0.9-3         MASS_7.3-37
[19] RColorBrewer_1.1-2  rgdal_0.9-1         stringr_0.6.2
[22] ggplot2_1.0.0       plyr_1.8.1          reshape2_1.4.1
[25] raster_2.3-12       sp_1.0-17           ProjectTemplate_0.6

loaded via a namespace (and not attached):
 [1] LearnBayes_2.15  Rcpp_0.11.3      boot_1.3-13      class_7.3-11
 [5] coda_0.16-1      codetools_0.2-9  colorspace_1.2-4 deldir_0.1-7
 [9] gtable_0.1.2     lattice_0.20-29  lava_1.3         munsell_0.4.2
[13] nlme_3.1-118     nnet_7.3-8       prodlim_1.5.1    proto_0.3-10
[17] rpart_4.1-8      scales_0.2.4     splines_3.1.2    survival_2.37-7



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-tp7587920.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From frtog at vestas.com  Fri Mar 20 07:37:34 2015
From: frtog at vestas.com (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Fri, 20 Mar 2015 07:37:34 +0100
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <1426806165111-7587920.post@n2.nabble.com>
References: <1426806165111-7587920.post@n2.nabble.com>
Message-ID: <B078CDF40DFE4045AF172A8B4F68FC4858805A4C97@DKRDSEXC016.vestas.net>

Hi Dominik

The as.data.frame() function called on an object of class SpatialPointsDataFrame will subsequently call the as.data.frame.SpatialPointsDataFrame() function. That function do not alter any columns names in any circumstances. 

What you have seen (I'm of course guessing here) will probably be because of some previously made objects with columns names x and y for coordinates masking objects that you believe have columns x and y but now in fact have column names Longitude and Latitude instead because you made some changes to your script file. This is what usually can happen when interactively developing a script in an R session where several objects have been created under the way to the final script. When launching the final script within a clean R session (no objects created yet) then it fails because some of parts of your code do not fits other parts of the script. 

So if it is possible for your (meaning all needed R objects are created during execution of the script so you can safely remove all objects) then do a rm(list = ls()), which deletes all objects in current R session and then step interactively through your script file. Whenever meeting an error edit the script accordingly to change the offending part of your script.

Yours sincerely / Med venlig hilsen


Frede Aakmann T?gersen
Specialist, M.Sc., Ph.D.
Plant Performance & Modeling

Technology & Service Solutions
T +45 9730 5135
M +45 2547 6050
frtog at vestas.com
http://www.vestas.com

Company reg. name: Vestas Wind Systems A/S
This e-mail is subject to our e-mail disclaimer statement.
Please refer to www.vestas.com/legal/notice
If you have received this e-mail in error please contact the sender. 


> -----Original Message-----
> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
> dschneiderch
> Sent: 20. marts 2015 00:03
> To: r-sig-geo at r-project.org
> Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
> 
> I have a spatial points dF that is causing me trouble. I've figured out what
> is happening but without a clue why.
> 
> at the prompt, I do
> > locs
> class       : SpatialPointsDataFrame
> features    : 10
> extent      : -112.0623, -109.0571, 33.65387, 36.32678  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> variables   : 7
> names       : network, State, Station_ID, Site_ID,        Site_Name,
> Elevation_ft, Elevation_m
> min values  :    SNTL,    AZ,     09N05S,     308,      BAKER BUTTE,
> 7100,        2164
> max values  :    SNTL,    AZ,     12P01S,    1143, HANNAGAN MEADOWS,
> 9200,        2804
> 
> > head(as.data.frame(locs))
>           x        y network State Station_ID Site_ID       Site_Name
> 1 -111.4064 34.45660    SNTL    AZ     11R06S     308     BAKER BUTTE
> 2 -111.3827 34.45547    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT
> 3 -109.5034 33.97883    SNTL    AZ     09S01S     310           BALDY
> 4 -109.2166 33.69144    SNTL    AZ     09S06S     902     BEAVER HEAD
> 5 -109.0571 36.32678    SNTL    AZ     09N05S    1143   BEAVER SPRING
> 6 -112.0623 35.26247    SNTL    AZ     12P01S    1139       CHALENDER
>   Elevation_ft Elevation_m
> 1         7300        2225
> 2         7700        2347
> 3         9125        2781
> 4         7990        2435
> 5         9200        2804
> 6         7100        2164
> 
> so as expected(?) my coordinate names get converted from Longitude,
> Latitude
> to x, y.
> 
> However, when I run my script, the output of head(as.data.frame(locs)) is:
>     Longitude   Latitude network State Station_ID Site_ID       Site_Name
> 1 -111.4064 34.45660    SNTL    AZ     11R06S     308     BAKER BUTTE
> 2 -111.3827 34.45547    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT
> 3 -109.5034 33.97883    SNTL    AZ     09S01S     310           BALDY
> 4 -109.2166 33.69144    SNTL    AZ     09S06S     902     BEAVER HEAD
> 5 -109.0571 36.32678    SNTL    AZ     09N05S    1143   BEAVER SPRING
> 6 -112.0623 35.26247    SNTL    AZ     12P01S    1139       CHALENDER
>   Elevation_ft Elevation_m
> 1         7300        2225
> 2         7700        2347
> 3         9125        2781
> 4         7990        2435
> 5         9200        2804
> 6         7100        2164
> 
> 
> I found out the hard way because i was doing
> as.data.frame(locs)[,c('x','y')] to get the coordinates.... I switched this
> line to coordinates(locs) but I have other lines in the same code that use
> as.data.frame() so I'm wondering if there are designed circumstance for one
> behavior compared to the other.  I did notice that
> data.frame(locs)[,c('x','y')] seems to always maintain the original
> coordinate names but I confirmed that the script uses as.data.frame()
> 
> Below is dput of a sample of my data. does anyone else get this behavior?
> 
> > dput(locs)
> new("SpatialPointsDataFrame"
>     , data = structure(list(network = c("SNTL", "SNTL", "SNTL", "SNTL",
> "SNTL",
> "SNTL", "SNTL", "SNTL", "SNTL", "SNTL"), State = c("AZ", "AZ",
> "AZ", "AZ", "AZ", "AZ", "AZ", "AZ", "AZ", "AZ"), Station_ID = c("11R06S",
> "11R07S", "09S01S", "09S06S", "09N05S", "12P01S", "09S07S", "11P02S",
> "11P13S", "09S11S"), Site_ID = c(308L, 1140L, 310L, 902L, 1143L,
> 1139L, 416L, 1121L, 488L, 511L), Site_Name = c("BAKER BUTTE",
> "BAKER BUTTE SMT", "BALDY", "BEAVER HEAD", "BEAVER SPRING",
> "CHALENDER",
> "CORONADO TRAIL", "FORT VALLEY", "FRY", "HANNAGAN MEADOWS"),
>     Elevation_ft = c(7300L, 7700L, 9125L, 7990L, 9200L, 7100L,
>     8400L, 7350L, 7200L, 9020L), Elevation_m = c(2225L, 2347L,
>     2781L, 2435L, 2804L, 2164L, 2560L, 2240L, 2195L, 2749L)), .Names =
> c("network",
> "State", "Station_ID", "Site_ID", "Site_Name", "Elevation_ft",
> "Elevation_m"), row.names = 63:72, class = "data.frame")
>     , coords.nrs = c(7L, 6L)
>     , coords = structure(c(-111.40643, -111.38272, -109.50344, -109.21657,
> -109.05711,
> -112.06231, -109.15282, -111.74486, -111.84374, -109.30952, 34.4566,
> 34.45547, 33.97883, 33.69144, 36.32678, 35.26247, 33.80392, 35.26806,
> 35.07297, 33.65387), .Dim = c(10L, 2L), .Dimnames = list(NULL,
>     c("Longitude", "Latitude")))
>     , bbox = structure(c(-112.06231, 33.65387, -109.05711, 36.32678), .Dim =
> c(2L,
> 2L), .Dimnames = list(c("Longitude", "Latitude"), c("min", "max"
> )))
>     , proj4string = new("CRS"
>     , projargs = "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
> )
> )
> 
> 
> This is running on a cluster at my university, using a SOCK cluster to
> parallelize dlply and then an MC backend for a ddply inside the dlply, if
> that's important. It seems to produce the expected behavior of my desktop.
> > sessionInfo()
> R version 3.1.2 (2014-10-31)
> Platform: x86_64-unknown-linux-gnu (64-bit)
> 
> locale:
> [1] C
> 
> attached base packages:
> [1] grid      parallel  stats     graphics  grDevices utils     datasets
> [8] methods   base
> 
> other attached packages:
>  [1] ncdf4_1.13          smwrBase_1.0.1      lubridate_1.3.3
>  [4] digest_0.6.8        memoise_0.2.1       gridExtra_0.9.1
>  [7] spdep_0.5-82        Matrix_1.1-4        fields_7.1
> [10] maps_2.3-9          spam_1.0-1          doSNOW_1.0.12
> [13] snow_0.3-13         doMC_1.3.3          iterators_1.0.7
> [16] foreach_1.4.2       ipred_0.9-3         MASS_7.3-37
> [19] RColorBrewer_1.1-2  rgdal_0.9-1         stringr_0.6.2
> [22] ggplot2_1.0.0       plyr_1.8.1          reshape2_1.4.1
> [25] raster_2.3-12       sp_1.0-17           ProjectTemplate_0.6
> 
> loaded via a namespace (and not attached):
>  [1] LearnBayes_2.15  Rcpp_0.11.3      boot_1.3-13      class_7.3-11
>  [5] coda_0.16-1      codetools_0.2-9  colorspace_1.2-4 deldir_0.1-7
>  [9] gtable_0.1.2     lattice_0.20-29  lava_1.3         munsell_0.4.2
> [13] nlme_3.1-118     nnet_7.3-8       prodlim_1.5.1    proto_0.3-10
> [17] rpart_4.1-8      scales_0.2.4     splines_3.1.2    survival_2.37-7
> 
> 
> 
> --
> View this message in context: http://r-sig-
> geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-
> tp7587920.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From srinivasv at feralindia.org  Fri Mar 20 12:09:14 2015
From: srinivasv at feralindia.org (Srinivas V)
Date: Fri, 20 Mar 2015 16:39:14 +0530
Subject: [R-sig-Geo] higher order neighbours in poly2nb
Message-ID: <550BFFDA.2070508@feralindia.org>

Hi,

Is there a way to create higher order neighbours in R? I'm using the 
poly2nb command to generate the list of neighbours and I don't see an 
option to generate 2nd and 3rd order neighbours.

I would appreciate any advice on dealing with this issue. Thanks!

-- 

Srinivas Vaidyanathan
Senior Research Fellow
Foundation for Ecological Research, Advocacy & Learning

Web: www.feralindia.org


	[[alternative HTML version deleted]]


From hengl at spatial-analyst.net  Fri Mar 20 12:22:41 2015
From: hengl at spatial-analyst.net (Tomislav Hengl)
Date: Fri, 20 Mar 2015 12:22:41 +0100
Subject: [R-sig-Geo] plotKML::plotKML creates multiple errors in Google
 Earth (Ubuntu 14.10)
In-Reply-To: <550B0DDE.2060401@gmx.de>
References: <550B0DDE.2060401@gmx.de>
Message-ID: <550C0301.4050105@spatial-analyst.net>


I've tried this code on my Win7 machine and it works fine:

...
 > plotKML(spatialhousedata["price"])
Plotting the first variable on the list
KML file opened for writing...
Reprojecting to +proj=longlat +datum=WGS84 ...
Writing to KML...
Closing  spatialhousedata__price__.kml

Note that you can best plot the target column by subsetting it e.g. 
'spatialhousedata["price"]' ('var.name' argument is only used for soil 
profiles; see http://plotkml.r-forge.r-project.org/plotKML.html). If you 
need more graphical control, try using:

 > kml(spatialhousedata, colour=price, colour_scale=SAGA_pal[[1]])

To solve the error messages you get from Google Earth under Ubuntu you 
need to write to Google Earth developers (plotKML is completely 
independent from Google Earth of course).

HTH,

T. (Tom) Hengl
Researcher @ ISRIC - World Soil Information
Team member Africa Soil Information Services http://africasoils.net
Url: http://www.wageningenur.nl/en/Persons/dr.-T-Tom-Hengl.htm
Network: http://profiles.google.com/tom.hengl
Publications: http://scholar.google.com/citations?user=2oYU7S8AAAAJ


On 19-3-2015 18:56, Guido Schulz wrote:
> Hi there,
>
> I am trying to use the R package *plotKML* to plot a
> sp::SpatialPolygonDataframe in *GoogleEarth*. I am using Ubuntu 14.10.
> Unfortunately, I facing various errors here...
>
> Here my reproducable example in R:
>
> ############################################
>      library("plotKML")
>      library("CARBayes")
>      library("sp")
>      data(spatialhousedata)
>      names(spatialhousedata at data)
>      proj4string(spatialhousedata) <- CRS("+proj=utm
>                  +zone=33 +ellps=GRS80 +units=m +no_defs")
>      plotKML(spatialhousedata, var.name="price")
> ############################################
>
> First, R spits out a lot of error lines:
>
> #############################################
>      Plotting the first variable on the list
>      KML file opened for writing...
>      Reprojecting to +proj=longlat +datum=WGS84 ...
>      Writing to KML...
>      Closing  spatialhousedata.kml
>      [0319/101818:ERROR:net_util.cc(2195)] Not implemented reached in
> bool net::HaveOnlyLoopbackAddresses()
>      [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> handler.
>      [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> handler.
>      [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> handler.
>      [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
> handler.
> ...
> ...
> ...
> #############################################
>
> Then *GoogleEarth* opens and welcomes me with this error message
> (which I translated from German):
>
> ----------------------------
> *Polygons bound to the ground are not supported*
>
> Polygons bound to the ground can only be displayed as boundaries on
> your computer. Your graphics does not support this function.
> ----------------------------
>
> Then it finally plots only the boundaries of *spatialhousedata* instead
> of the coloured polygons (choropleth map) of spatialhousedata at data$price.
>
> Here is my graphics info from Ubuntu:
>
> -----------------------------
>      me at me-ThinkPad-T420:~$  sudo lshw -c video
>
>        *-display
>             Beschreibung: VGA compatible controller
>             Produkt: 2nd Generation Core Processor
>                      Family Integrated Graphics Controller
>             Hersteller: Intel Corporation
>             Physische ID: 2
>             Bus-Informationen: pci at 0000:00:02.0
>             Version: 09
>             Breite: 64 bits
>             Takt: 33MHz
>             F?higkeiten: msi pm vga_controller bus_master cap_list rom
>             Konfiguration: driver=i915 latency=0
>             Ressourcen: irq:42 memory:f0000000-f03fffff
>                         memory:e0000000-efffffff ioport:5000(Gr??e=64)
> ------------------------------
>
> And this is the version info from Google Earth:
>
> -----------------------------
>      Google Earth: 7.1.2.2041
>      Build-Date: 10/7/2013
>      Build-Time: 12:17:00 nachmittags
>      Renderer: OpenGL
>      OS: Linux (3.13.0.0)
>      Graphics Driver: Intel Open Source Technology Center
> -----------------------------
>
> *Any ideas how to solve this?*
>
> Best,
>
> Guido
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From Roger.Bivand at nhh.no  Fri Mar 20 13:22:05 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 20 Mar 2015 13:22:05 +0100
Subject: [R-sig-Geo] higher order neighbours in poly2nb
In-Reply-To: <550BFFDA.2070508@feralindia.org>
References: <550BFFDA.2070508@feralindia.org>
Message-ID: <alpine.LFD.2.11.1503201321150.3067@reclus.nhh.no>

On Fri, 20 Mar 2015, Srinivas V wrote:

> Hi,
>
> Is there a way to create higher order neighbours in R? I'm using the
> poly2nb command to generate the list of neighbours and I don't see an
> option to generate 2nd and 3rd order neighbours.

?nblag

>
> I would appreciate any advice on dealing with this issue. Thanks!
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no


From gosz at gmx.de  Fri Mar 20 14:52:36 2015
From: gosz at gmx.de (Guido Schulz)
Date: Fri, 20 Mar 2015 14:52:36 +0100
Subject: [R-sig-Geo] plotKML::plotKML creates multiple errors in Google
 Earth (Ubuntu 14.10)
In-Reply-To: <CAAcGz99K++R18SiPFx8kA-=t=-qdJNJKZaDA6Lp-XdJR5jJaEA@mail.gmail.com>
References: <550B0DDE.2060401@gmx.de> <550B41D1.3020205@uni-muenster.de>
	<CAAcGz99K++R18SiPFx8kA-=t=-qdJNJKZaDA6Lp-XdJR5jJaEA@mail.gmail.com>
Message-ID: <550C2624.8070101@gmx.de>

I briefly tried cesium, but I couldn't manage to open a local KML file
with it. But it is supposed to work somehow...

Google Earth Pro has only the Linux binaries for download at the moment,
but I might give it a try...

In the end, I really might have to install Windows 7 for a dual boot...

Best,

G


Am 19.03.2015 um 23:04 schrieb Michael Sumner:
> Cesium looks like a good alternative to GE, fwiw.
> 
> Cheers, Mike
> 
> On Fri, 20 Mar 2015 08:39 Edzer Pebesma <edzer.pebesma at uni-muenster.de>
> wrote:
> 
>> I can reproduce this under ubuntu 14.04 LTS. In my understanding, the
>> ubuntu (or linux) google-earth is by far not as good as it is under
>> windows or Mac OS-X, and this has been so for a long time. The only
>> solution I can think of is to work towards a solution without
>> google-earth, or make the switch to windows or mac.
>>
>> Or is the story different with google-earth pro, which is now free?
>>
>> On 03/19/2015 06:56 PM, Guido Schulz wrote:
>>> Hi there,
>>>
>>> I am trying to use the R package *plotKML* to plot a
>>> sp::SpatialPolygonDataframe in *GoogleEarth*. I am using Ubuntu 14.10.
>>> Unfortunately, I facing various errors here...
>>>
>>> Here my reproducable example in R:
>>>
>>> ############################################
>>>     library("plotKML")
>>>     library("CARBayes")
>>>     library("sp")
>>>     data(spatialhousedata)
>>>     names(spatialhousedata at data)
>>>     proj4string(spatialhousedata) <- CRS("+proj=m
>>>                 +zone3 +ellps=GRS80 +units=m +no_defs")
>>>     plotKML(spatialhousedata, var.name=rice")
>>> ############################################
>>>
>>> First, R spits out a lot of error lines:
>>>
>>> #############################################
>>>     Plotting the first variable on the list
>>>     KML file opened for writing...
>>>     Reprojecting to +proj=nglat +datum=WGS84 ...
>>>     Writing to KML...
>>>     Closing  spatialhousedata.kml
>>>     [0319/101818:ERROR:net_util.cc(2195)] Not implemented reached in
>>> bool net::HaveOnlyLoopbackAddresses()
>>>     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
>>> handler.
>>>     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
>>> handler.
>>>     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
>>> handler.
>>>     [0319/101819:ERROR:nss_ocsp.cc(581)] No URLRequestContext for OCSP
>>> handler.
>>> ...
>>> ...
>>> ...
>>> #############################################
>>>
>>> Then *GoogleEarth* opens and welcomes me with this error message
>>> (which I translated from German):
>>>
>>> ----------------------------
>>> *Polygons bound to the ground are not supported*
>>>
>>> Polygons bound to the ground can only be displayed as boundaries on
>>> your computer. Your graphics does not support this function.
>>> ----------------------------
>>>
>>> Then it finally plots only the boundaries of *spatialhousedata* instead
>>> of the coloured polygons (choropleth map) of spatialhousedata at data
>> $price.
>>>
>>> Here is my graphics info from Ubuntu:
>>>
>>> -----------------------------
>>>     me at me-ThinkPad-T420:~$  sudo lshw -c video
>>>
>>>       *-display
>>>            Beschreibung: VGA compatible controller
>>>            Produkt: 2nd Generation Core Processor
>>>                     Family Integrated Graphics Controller
>>>            Hersteller: Intel Corporation
>>>            Physische ID: 2
>>>            Bus-Informationen: pci at 0000:00:02.0
>>>            Version: 09
>>>            Breite: 64 bits
>>>            Takt: 33MHz
>>>            F?higkeiten: msi pm vga_controller bus_master cap_list rom
>>>            Konfiguration: driver=15 latency=0
>>>            Ressourcen: irq:42 memory:f0000000-f03fffff
>>>                        memory:e0000000-efffffff ioport:5000(Gr??ed)
>>> ------------------------------
>>>
>>> And this is the version info from Google Earth:
>>>
>>> -----------------------------
>>>     Google Earth: 7.1.2.2041
>>>     Build-Date: 10/7/2013
>>>     Build-Time: 12:17:00 nachmittags
>>>     Renderer: OpenGL
>>>     OS: Linux (3.13.0.0)
>>>     Graphics Driver: Intel Open Source Technology Center
>>> -----------------------------
>>>
>>> *Any ideas how to solve this?*
>>>
>>> Best,
>>>
>>> Guido
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Edzer Pebesma
>> Institute for Geoinformatics (ifgi),  University of M?nster,
>> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
>> Journal of Statistical Software:   http://www.jstatsoft.org/
>> Computers & Geosciences:   http://elsevier.com/locate/cageo/
>> Spatial Statistics Society http://www.spatialstatistics.info
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> 
> 	[[alternative HTML version deleted]]
> 
>


From macqueen1 at llnl.gov  Fri Mar 20 17:01:07 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 20 Mar 2015 16:01:07 +0000
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <1426806165111-7587920.post@n2.nabble.com>
References: <1426806165111-7587920.post@n2.nabble.com>
Message-ID: <D1318CA5.122E2E%macqueen1@llnl.gov>

In my experience, relying on column names to extract the coordinates is
not at all a good idea. I would strongly recommend that you take the time
to update all of your scripts to use the coordinates() function. I think
it will be worth it in the long run.

It's not a good idea because the column names of the coordinates depend on
how the SpatialPointsDataFrame was originally created, and in my own
applications that is highly variable.  Sometimes ('x','y'), sometimes
('lon','lat'), or any of several other variations of how to spell or
abbreviate latitude and longitude (with or without capitalization). Or
('easting','northing'). Or, or, or... Trying to carefully control all that
is more trouble than it's worth; I just use, for example,
coordinates(obj)[,1] and coordinates(obj)[,2] if I want to pull them out
as vectors. Ugly, but I can count on it.

That said, if
  as.data.frame(locs)
produces different names for the coordinates when used in different
contexts, then you've got something else going on that should not be going
on. This is where Frede's suggestions might help. You will need to
carefully track the construction of your locs object and see if it is
somehow different in the two situations. I don't know of any "designed
circumstance" that would explain this.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 3/19/15, 4:02 PM, "dschneiderch" <Dominik.Schneider at colorado.edu> wrote:

>I have a spatial points dF that is causing me trouble. I've figured out
>what
>is happening but without a clue why.
>
>at the prompt, I do
>> locs
>class       : SpatialPointsDataFrame
>features    : 10
>extent      : -112.0623, -109.0571, 33.65387, 36.32678  (xmin, xmax, ymin,
>ymax)
>coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>variables   : 7
>names       : network, State, Station_ID, Site_ID,        Site_Name,
>Elevation_ft, Elevation_m
>min values  :    SNTL,    AZ,     09N05S,     308,      BAKER BUTTE,
>  
>7100,        2164
>max values  :    SNTL,    AZ,     12P01S,    1143, HANNAGAN MEADOWS,
>  
>9200,        2804
>
>> head(as.data.frame(locs))
>          x        y network State Station_ID Site_ID       Site_Name
>1 -111.4064 34.45660    SNTL    AZ     11R06S     308     BAKER BUTTE
>2 -111.3827 34.45547    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT
>3 -109.5034 33.97883    SNTL    AZ     09S01S     310           BALDY
>4 -109.2166 33.69144    SNTL    AZ     09S06S     902     BEAVER HEAD
>5 -109.0571 36.32678    SNTL    AZ     09N05S    1143   BEAVER SPRING
>6 -112.0623 35.26247    SNTL    AZ     12P01S    1139       CHALENDER
>  Elevation_ft Elevation_m
>1         7300        2225
>2         7700        2347
>3         9125        2781
>4         7990        2435
>5         9200        2804
>6         7100        2164
>
>so as expected(?) my coordinate names get converted from Longitude,
>Latitude
>to x, y.
>
>However, when I run my script, the output of head(as.data.frame(locs)) is:
>    Longitude   Latitude network State Station_ID Site_ID       Site_Name
>1 -111.4064 34.45660    SNTL    AZ     11R06S     308     BAKER BUTTE
>2 -111.3827 34.45547    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT
>3 -109.5034 33.97883    SNTL    AZ     09S01S     310           BALDY
>4 -109.2166 33.69144    SNTL    AZ     09S06S     902     BEAVER HEAD
>5 -109.0571 36.32678    SNTL    AZ     09N05S    1143   BEAVER SPRING
>6 -112.0623 35.26247    SNTL    AZ     12P01S    1139       CHALENDER
>  Elevation_ft Elevation_m
>1         7300        2225
>2         7700        2347
>3         9125        2781
>4         7990        2435
>5         9200        2804
>6         7100        2164
>
>
>I found out the hard way because i was doing
>as.data.frame(locs)[,c('x','y')] to get the coordinates.... I switched
>this
>line to coordinates(locs) but I have other lines in the same code that use
>as.data.frame() so I'm wondering if there are designed circumstance for
>one
>behavior compared to the other.  I did notice that
>data.frame(locs)[,c('x','y')] seems to always maintain the original
>coordinate names but I confirmed that the script uses as.data.frame()
>
>Below is dput of a sample of my data. does anyone else get this behavior?
>
>> dput(locs)
>new("SpatialPointsDataFrame"
>    , data = structure(list(network = c("SNTL", "SNTL", "SNTL", "SNTL",
>"SNTL",
>"SNTL", "SNTL", "SNTL", "SNTL", "SNTL"), State = c("AZ", "AZ",
>"AZ", "AZ", "AZ", "AZ", "AZ", "AZ", "AZ", "AZ"), Station_ID = c("11R06S",
>"11R07S", "09S01S", "09S06S", "09N05S", "12P01S", "09S07S", "11P02S",
>"11P13S", "09S11S"), Site_ID = c(308L, 1140L, 310L, 902L, 1143L,
>1139L, 416L, 1121L, 488L, 511L), Site_Name = c("BAKER BUTTE",
>"BAKER BUTTE SMT", "BALDY", "BEAVER HEAD", "BEAVER SPRING", "CHALENDER",
>"CORONADO TRAIL", "FORT VALLEY", "FRY", "HANNAGAN MEADOWS"),
>    Elevation_ft = c(7300L, 7700L, 9125L, 7990L, 9200L, 7100L,
>    8400L, 7350L, 7200L, 9020L), Elevation_m = c(2225L, 2347L,
>    2781L, 2435L, 2804L, 2164L, 2560L, 2240L, 2195L, 2749L)), .Names =
>c("network",
>"State", "Station_ID", "Site_ID", "Site_Name", "Elevation_ft",
>"Elevation_m"), row.names = 63:72, class = "data.frame")
>    , coords.nrs = c(7L, 6L)
>    , coords = structure(c(-111.40643, -111.38272, -109.50344, -109.21657,
>-109.05711,
>-112.06231, -109.15282, -111.74486, -111.84374, -109.30952, 34.4566,
>34.45547, 33.97883, 33.69144, 36.32678, 35.26247, 33.80392, 35.26806,
>35.07297, 33.65387), .Dim = c(10L, 2L), .Dimnames = list(NULL,
>    c("Longitude", "Latitude")))
>    , bbox = structure(c(-112.06231, 33.65387, -109.05711, 36.32678),
>.Dim =
>c(2L,
>2L), .Dimnames = list(c("Longitude", "Latitude"), c("min", "max"
>)))
>    , proj4string = new("CRS"
>    , projargs = "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
>)
>)
>
>
>This is running on a cluster at my university, using a SOCK cluster to
>parallelize dlply and then an MC backend for a ddply inside the dlply, if
>that's important. It seems to produce the expected behavior of my desktop.
>> sessionInfo()
>R version 3.1.2 (2014-10-31)
>Platform: x86_64-unknown-linux-gnu (64-bit)
>
>locale:
>[1] C
>
>attached base packages:
>[1] grid      parallel  stats     graphics  grDevices utils     datasets
>[8] methods   base
>
>other attached packages:
> [1] ncdf4_1.13          smwrBase_1.0.1      lubridate_1.3.3
> [4] digest_0.6.8        memoise_0.2.1       gridExtra_0.9.1
> [7] spdep_0.5-82        Matrix_1.1-4        fields_7.1
>[10] maps_2.3-9          spam_1.0-1          doSNOW_1.0.12
>[13] snow_0.3-13         doMC_1.3.3          iterators_1.0.7
>[16] foreach_1.4.2       ipred_0.9-3         MASS_7.3-37
>[19] RColorBrewer_1.1-2  rgdal_0.9-1         stringr_0.6.2
>[22] ggplot2_1.0.0       plyr_1.8.1          reshape2_1.4.1
>[25] raster_2.3-12       sp_1.0-17           ProjectTemplate_0.6
>
>loaded via a namespace (and not attached):
> [1] LearnBayes_2.15  Rcpp_0.11.3      boot_1.3-13      class_7.3-11
> [5] coda_0.16-1      codetools_0.2-9  colorspace_1.2-4 deldir_0.1-7
> [9] gtable_0.1.2     lattice_0.20-29  lava_1.3         munsell_0.4.2
>[13] nlme_3.1-118     nnet_7.3-8       prodlim_1.5.1    proto_0.3-10
>[17] rpart_4.1-8      scales_0.2.4     splines_3.1.2    survival_2.37-7
>
>
>
>--
>View this message in context:
>http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialP
>ointsDF-tp7587920.html
>Sent from the R-sig-geo mailing list archive at Nabble.com.
>
>_______________________________________________
>R-sig-Geo mailing list
>R-sig-Geo at r-project.org
>https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From edzer.pebesma at uni-muenster.de  Fri Mar 20 17:28:25 2015
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Fri, 20 Mar 2015 17:28:25 +0100
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <D1318CA5.122E2E%macqueen1@llnl.gov>
References: <1426806165111-7587920.post@n2.nabble.com>
	<D1318CA5.122E2E%macqueen1@llnl.gov>
Message-ID: <550C4AA9.1050302@uni-muenster.de>

There is some logic: sp tries to track coordinate names when it can, but
if coordinates are set as a nameless matrix, as in the last example
below, it will choose names itself. coordnames() helps you discover how
the coordinates of a SpatialPointsDataFrame are called:

> df = data.frame(x=1:2, y=2:1, z = 3:4)
> df1 = df
> library(sp)
> coordinates(df1) = ~x+y
> as.data.frame(df1)
  x y z
1 1 2 3
2 2 1 4
> coordnames(df1)
[1] "x" "y"
> df1=df
> coordinates(df1) = df[1:2]
> coordnames(df1)
[1] "x" "y"
> df1=df
> coordinates(df1) = cbind(df$x,df$y) # nameless
> df1
  coordinates x y z
1      (1, 2) 1 2 3
2      (2, 1) 2 1 4
> coordnames(df1)
[1] "coords.x1" "coords.x2"


a bit of a semantic trap is this: if your coordinates are longitude and
latitude and carry these names, after you project the object with
rgdal::spTransform they're still called longitude and latitude, although
they are no longer understood as such. You can then solve this yourself by

> coordnames(df1) = c("x", "y")

after which

> coordnames(df1)
[1] "x" "y"


On 03/20/2015 05:01 PM, MacQueen, Don wrote:
> In my experience, relying on column names to extract the coordinates is
> not at all a good idea. I would strongly recommend that you take the time
> to update all of your scripts to use the coordinates() function. I think
> it will be worth it in the long run.
> 
> It's not a good idea because the column names of the coordinates depend on
> how the SpatialPointsDataFrame was originally created, and in my own
> applications that is highly variable.  Sometimes ('x','y'), sometimes
> ('lon','lat'), or any of several other variations of how to spell or
> abbreviate latitude and longitude (with or without capitalization). Or
> ('easting','northing'). Or, or, or... Trying to carefully control all that
> is more trouble than it's worth; I just use, for example,
> coordinates(obj)[,1] and coordinates(obj)[,2] if I want to pull them out
> as vectors. Ugly, but I can count on it.
> 
> That said, if
>   as.data.frame(locs)
> produces different names for the coordinates when used in different
> contexts, then you've got something else going on that should not be going
> on. This is where Frede's suggestions might help. You will need to
> carefully track the construction of your locs object and see if it is
> somehow different in the two situations. I don't know of any "designed
> circumstance" that would explain this.
> 
> -Don
> 

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi),  University of M?nster,
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150320/4b2333c1/attachment.bin>

From Dominik.Schneider at colorado.edu  Fri Mar 20 17:35:43 2015
From: Dominik.Schneider at colorado.edu (dschneiderch)
Date: Fri, 20 Mar 2015 09:35:43 -0700 (MST)
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <D1318CA5.122E2E%macqueen1@llnl.gov>
References: <1426806165111-7587920.post@n2.nabble.com>
	<D1318CA5.122E2E%macqueen1@llnl.gov>
Message-ID: <1426869343600-7587928.post@n2.nabble.com>

Hi -
Thanks for your replies.
I should have mentioned, I made sure to test this with a clean workspace
before posting and confirmed the behavior this morning. it sounds like the
names of the coordinates should never change?
another example of the names being converted at the prompt from Longitude,
Latitude to x,y.
> library('ProjectTemplate')
> load.project() #since i'm loading the data from a cached .rdata file, it
> is always the same to begin. please excuse the slightly different variable
> names. locs = snotellocs[1:10,]
... bunch of packages loading ... and cached data loading ...
> head(str(snotellocs))
Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots
  ..@ data       :'data.frame':	315 obs. of  7 variables:
  .. ..$ network     : chr [1:315] "SNTL" "SNTL" "SNTL" "SNTL" ...
  .. ..$ State       : chr [1:315] "AZ" "AZ" "AZ" "AZ" ...
  .. ..$ Station_ID  : chr [1:315] "11R06S" "11R07S" "09S01S" "09S06S" ...
  .. ..$ Site_ID     : int [1:315] 308 1140 310 902 1143 1139 416 1121 488
511 ...
  .. ..$ Site_Name   : chr [1:315] "BAKER BUTTE" "BAKER BUTTE SMT" "BALDY"
"BEAVER HEAD" ...
  .. ..$ Elevation_ft: int [1:315] 7300 7700 9125 7990 9200 7100 8400 7350
7200 9020 ...
  .. ..$ Elevation_m : int [1:315] 2225 2347 2781 2435 2804 2164 2560 2240
2195 2749 ...
  ..@ coords.nrs : int [1:2] 7 6
  ..@ coords     : num [1:315, 1:2] -111 -111 -110 -109 -109 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:2] "Longitude" "Latitude"
  ..@ bbox       : num [1:2, 1:2] -112.2 33 -105.1 43.7
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2] "Longitude" "Latitude"
  .. .. ..$ : chr [1:2] "min" "max"
  ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
  .. .. ..@ projargs: chr "+proj=longlat +datum=WGS84 +ellps=WGS84
+towgs84=0,0,0"
NULL
> head(as.data.frame(snotellocs))
          x        y network State Station_ID Site_ID       Site_Name
1 -111.4064 34.45660    SNTL    AZ     11R06S     308     BAKER BUTTE
2 -111.3827 34.45547    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT
3 -109.5034 33.97883    SNTL    AZ     09S01S     310           BALDY
4 -109.2166 33.69144    SNTL    AZ     09S06S     902     BEAVER HEAD
5 -109.0571 36.32678    SNTL    AZ     09N05S    1143   BEAVER SPRING
6 -112.0623 35.26247    SNTL    AZ     12P01S    1139       CHALENDER
  Elevation_ft Elevation_m
1         7300        2225
2         7700        2347
3         9125        2781
4         7990        2435
5         9200        2804
6         7100        2164


> sessionInfo()
R version 3.1.2 (2014-10-31)
Platform: x86_64-unknown-linux-gnu (64-bit)

locale:
[1] C

attached base packages:
[1] grid      parallel  stats     graphics  grDevices utils     datasets
[8] methods   base

other attached packages:
 [1] ncdf4_1.13          smwrBase_1.0.1      lubridate_1.3.3
 [4] digest_0.6.8        memoise_0.2.1       gridExtra_0.9.1
 [7] spdep_0.5-82        Matrix_1.1-4        fields_7.1
[10] maps_2.3-9          spam_1.0-1          doSNOW_1.0.12
[13] snow_0.3-13         doMC_1.3.3          iterators_1.0.7
[16] foreach_1.4.2       ipred_0.9-3         MASS_7.3-37
[19] RColorBrewer_1.1-2  rgdal_0.9-1         stringr_0.6.2
[22] ggplot2_1.0.0       plyr_1.8.1          reshape2_1.4.1
[25] raster_2.3-12       sp_1.0-17           ProjectTemplate_0.6

loaded via a namespace (and not attached):
 [1] LearnBayes_2.15  Rcpp_0.11.3      boot_1.3-13      class_7.3-11
 [5] coda_0.16-1      codetools_0.2-9  colorspace_1.2-4 deldir_0.1-7
 [9] gtable_0.1.2     lattice_0.20-29  lava_1.3         munsell_0.4.2
[13] nlme_3.1-118     nnet_7.3-8       prodlim_1.5.1    proto_0.3-10
[17] rpart_4.1-8      scales_0.2.4     splines_3.1.2    survival_2.37-7




--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-tp7587920p7587928.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From Dominik.Schneider at colorado.edu  Fri Mar 20 18:02:27 2015
From: Dominik.Schneider at colorado.edu (dschneiderch)
Date: Fri, 20 Mar 2015 10:02:27 -0700 (MST)
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <550C4AA9.1050302@uni-muenster.de>
References: <1426806165111-7587920.post@n2.nabble.com>
	<D1318CA5.122E2E%macqueen1@llnl.gov>
	<550C4AA9.1050302@uni-muenster.de>
Message-ID: <1426870947485-7587929.post@n2.nabble.com>

Edzer - Look like we posted at the same time.
In my example my coordinates are named, no?

I tried your example in my R environment and can replicate the behavior I
mentioned.

> df = data.frame(x=1:2, y=2:1, z = 3:4)
> df1 = df
> library(sp)
> coordinates(df1) = ~x+y
> as.data.frame(df1)
  x y z
1 1 2 3
2 2 1 4
> coordnames(df1)
[1] "x" "y" 
> coordnames(df1)=c('long','lat')
> str(df1)
Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots
  ..@ data       :'data.frame':	2 obs. of  1 variable:
  .. ..$ z: int [1:2] 3 4
  ..@ coords.nrs : int [1:2] 1 2
  ..@ coords     : num [1:2, 1:2] 1 2 2 1
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:2] "long" "lat"
  ..@ bbox       : num [1:2, 1:2] 1 1 2 2
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2] "long" "lat"
  .. .. ..$ : chr [1:2] "min" "max"
  ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
  .. .. ..@ projargs: chr NA
> as.data.frame(df1)
  x y z
1 1 2 3
2 2 1 4


Interestingly, when I open a new R instance and without loading all the
packages associated with my project, the names are *not* converted.  So it
seems that one of the packages I have loaded is conflicting with sp and
causing the names to change...  my sessionInfo() was attached in the other
post.




--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-tp7587920p7587929.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From macqueen1 at llnl.gov  Fri Mar 20 18:10:38 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 20 Mar 2015 17:10:38 +0000
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <550C4AA9.1050302@uni-muenster.de>
References: <1426806165111-7587920.post@n2.nabble.com>
	<D1318CA5.122E2E%macqueen1@llnl.gov> <550C4AA9.1050302@uni-muenster.de>
Message-ID: <D131A296.122EA1%macqueen1@llnl.gov>

Thanks, Edzer, this is helpful.

-Don


-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 3/20/15, 9:28 AM, "Edzer Pebesma" <edzer.pebesma at uni-muenster.de> wrote:

>There is some logic: sp tries to track coordinate names when it can, but
>if coordinates are set as a nameless matrix, as in the last example
>below, it will choose names itself. coordnames() helps you discover how
>the coordinates of a SpatialPointsDataFrame are called:
>
>> df = data.frame(x=1:2, y=2:1, z = 3:4)
>> df1 = df
>> library(sp)
>> coordinates(df1) = ~x+y
>> as.data.frame(df1)
>  x y z
>1 1 2 3
>2 2 1 4
>> coordnames(df1)
>[1] "x" "y"
>> df1=df
>> coordinates(df1) = df[1:2]
>> coordnames(df1)
>[1] "x" "y"
>> df1=df
>> coordinates(df1) = cbind(df$x,df$y) # nameless
>> df1
>  coordinates x y z
>1      (1, 2) 1 2 3
>2      (2, 1) 2 1 4
>> coordnames(df1)
>[1] "coords.x1" "coords.x2"
>
>
>a bit of a semantic trap is this: if your coordinates are longitude and
>latitude and carry these names, after you project the object with
>rgdal::spTransform they're still called longitude and latitude, although
>they are no longer understood as such. You can then solve this yourself by
>
>> coordnames(df1) = c("x", "y")
>
>after which
>
>> coordnames(df1)
>[1] "x" "y"
>
>
>On 03/20/2015 05:01 PM, MacQueen, Don wrote:
>> In my experience, relying on column names to extract the coordinates is
>> not at all a good idea. I would strongly recommend that you take the
>>time
>> to update all of your scripts to use the coordinates() function. I think
>> it will be worth it in the long run.
>> 
>> It's not a good idea because the column names of the coordinates depend
>>on
>> how the SpatialPointsDataFrame was originally created, and in my own
>> applications that is highly variable.  Sometimes ('x','y'), sometimes
>> ('lon','lat'), or any of several other variations of how to spell or
>> abbreviate latitude and longitude (with or without capitalization). Or
>> ('easting','northing'). Or, or, or... Trying to carefully control all
>>that
>> is more trouble than it's worth; I just use, for example,
>> coordinates(obj)[,1] and coordinates(obj)[,2] if I want to pull them out
>> as vectors. Ugly, but I can count on it.
>> 
>> That said, if
>>   as.data.frame(locs)
>> produces different names for the coordinates when used in different
>> contexts, then you've got something else going on that should not be
>>going
>> on. This is where Frede's suggestions might help. You will need to
>> carefully track the construction of your locs object and see if it is
>> somehow different in the two situations. I don't know of any "designed
>> circumstance" that would explain this.
>> 
>> -Don
>> 
>
>-- 
>Edzer Pebesma
>Institute for Geoinformatics (ifgi),  University of M?nster,
>Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
>Journal of Statistical Software:   http://www.jstatsoft.org/
>Computers & Geosciences:   http://elsevier.com/locate/cageo/
>Spatial Statistics Society http://www.spatialstatistics.info
>


From Dominik.Schneider at colorado.edu  Fri Mar 20 18:18:54 2015
From: Dominik.Schneider at colorado.edu (dschneiderch)
Date: Fri, 20 Mar 2015 10:18:54 -0700 (MST)
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <1426870947485-7587929.post@n2.nabble.com>
References: <1426806165111-7587920.post@n2.nabble.com>
	<D1318CA5.122E2E%macqueen1@llnl.gov>
	<550C4AA9.1050302@uni-muenster.de>
	<1426870947485-7587929.post@n2.nabble.com>
Message-ID: <CAHYDKLYzJUsPsP0c0qBNan2L+DuBhs=HuhWvaRFz4Y+79GHskg@mail.gmail.com>

I just went through my packages, iteratively unloading each one with:
detach('package:raster')
and found that if I unload the raster package then the expected behavior of
keeping the existing coordnames is achieved.

now why that differs in my script I have no idea.

> coordnames(snotellocs)
[1] "Longitude" "Latitude"
> detach('package:raster')
> head(as.data.frame(snotellocs))
  network State Station_ID Site_ID       Site_Name Latitude Longitude
1    SNTL    AZ     11R06S     308     BAKER BUTTE 34.45660 -111.4064
2    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT 34.45547 -111.3827
3    SNTL    AZ     09S01S     310           BALDY 33.97883 -109.5034
4    SNTL    AZ     09S06S     902     BEAVER HEAD 33.69144 -109.2166
5    SNTL    AZ     09N05S    1143   BEAVER SPRING 36.32678 -109.0571
6    SNTL    AZ     12P01S    1139       CHALENDER 35.26247 -112.0623
  Elevation_ft Elevation_m
1         7300        2225
2         7700        2347
3         9125        2781
4         7990        2435
5         9200        2804
6         7100        2164
>

Dominik Schneider
o 303.735.6296 | c 518.956.3978


On Fri, Mar 20, 2015 at 11:02 AM, dschneiderch [via R-sig-geo] <
ml-node+s2731867n7587929h1 at n2.nabble.com> wrote:

> Edzer - Look like we posted at the same time.
> In my example my coordinates are named, no?
>
> I tried your example in my R environment and can replicate the behavior I
> mentioned.
>
> > df = data.frame(x=1:2, y=2:1, z = 3:4)
> > df1 = df
> > library(sp)
> > coordinates(df1) = ~x+y
> > as.data.frame(df1)
>   x y z
> 1 1 2 3
> 2 2 1 4
> > coordnames(df1)
> [1] "x" "y"
> > coordnames(df1)=c('long','lat')
> > str(df1)
> Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots
>   ..@ data       :'data.frame': 2 obs. of  1 variable:
>   .. ..$ z: int [1:2] 3 4
>   ..@ coords.nrs : int [1:2] 1 2
>   ..@ coords     : num [1:2, 1:2] 1 2 2 1
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : NULL
>   .. .. ..$ : chr [1:2] "long" "lat"
>   ..@ bbox       : num [1:2, 1:2] 1 1 2 2
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : chr [1:2] "long" "lat"
>   .. .. ..$ : chr [1:2] "min" "max"
>   ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
>   .. .. ..@ projargs: chr NA
> > as.data.frame(df1)
>   x y z
> 1 1 2 3
> 2 2 1 4
>
>
> Interestingly, when I open a new R instance and without loading all the
> packages associated with my project, the names are *not* converted.  So it
> seems that one of the packages I have loaded is conflicting with sp and
> causing the names to change...  my sessionInfo() was attached in the other
> post.
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-tp7587920p7587929.html
>  To unsubscribe from inconsistent as.data.frame(SpatialPointsDF), click
> here
> <http://r-sig-geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=7587920&code=RG9taW5pay5TY2huZWlkZXJAY29sb3JhZG8uZWR1fDc1ODc5MjB8LTEwMzMyMTA1OQ==>
> .
> NAML
> <http://r-sig-geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-tp7587920p7587931.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From frtog at vestas.com  Fri Mar 20 19:39:29 2015
From: frtog at vestas.com (=?iso-8859-1?Q?Frede_Aakmann_T=F8gersen?=)
Date: Fri, 20 Mar 2015 19:39:29 +0100
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <CAHYDKLYzJUsPsP0c0qBNan2L+DuBhs=HuhWvaRFz4Y+79GHskg@mail.gmail.com>
References: <1426806165111-7587920.post@n2.nabble.com>
	<D1318CA5.122E2E%macqueen1@llnl.gov>	<550C4AA9.1050302@uni-muenster.de>
	<1426870947485-7587929.post@n2.nabble.com>
	<CAHYDKLYzJUsPsP0c0qBNan2L+DuBhs=HuhWvaRFz4Y+79GHskg@mail.gmail.com>
Message-ID: <B078CDF40DFE4045AF172A8B4F68FC4858808A4908@DKRDSEXC016.vestas.net>

Hi

I can reproduce this using Edzer's example. See below for output from R. Also notice the naming of the coordinate columns to "x" and "y" in the S4 method of the raster package's function as.data.frame! Isn't that quite unusual? I have maintainer on CC.

> library(sp)
> df = data.frame(Longitude=1:2, Latitude=2:1, z = 3:4)
> df1 = df
> coordinates(df1) = ~Longitude + Latitude
> as.data.frame(df1)
  Longitude Latitude z
1         1        2 3
2         2        1 4
> library(raster)
> search()
 [1] ".GlobalEnv"        "package:raster"    "package:sp"       
 [4] "ESSR"              "package:stats"     "package:graphics" 
 [7] "package:grDevices" "package:utils"     "package:datasets" 
[10] "package:methods"   "Autoloads"         "package:base"     
> as.data.frame(df1)
  x y z
1 1 2 3
2 2 1 4
> showMethods("as.data.frame")
Function: as.data.frame (package base)
x="ANY"
x="data.frame"
    (inherited from: x="ANY")
x="Raster"
x="SpatialLines"
x="SpatialPoints"
x="SpatialPointsDataFrame"
    (inherited from: x="SpatialPoints")
x="SpatialPolygons"

> selectMethod("as.data.frame", "SpatialPointsDataFrame")
Method Definition:

function (x, row.names = NULL, optional = FALSE, ...) 
{
    .local <- function (x, row.names = NULL, optional = FALSE, 
        xy = TRUE, ...) 
    {
        if (!xy) {
            if (.hasSlot(x, "data")) {
                return(x at data)
            }
            else {
                return(NULL)
            }
        }
        nobj <- length(x)
        d <- coordinates(x)
        if (.hasSlot(x, "data")) {
            d <- cbind(d, x at data)
        }
        colnames(d)[1:2] <- c("x", "y")
        rownames(d) <- NULL
        as.data.frame(d, row.names = row.names, optional = optional, 
            ...)
    }
    .local(x, row.names, optional, ...)
}
<bytecode: 0x000000000d037260>
<environment: namespace:raster>

Signatures:
        x                       
target  "SpatialPointsDataFrame"
defined "SpatialPoints"         
>



Yours sincerely / Med venlig hilsen


Frede Aakmann T?gersen
Specialist, M.Sc., Ph.D.
Plant Performance & Modeling

Technology & Service Solutions
T +45 9730 5135
M +45 2547 6050
frtog at vestas.com
http://www.vestas.com

Company reg. name: Vestas Wind Systems A/S
This e-mail is subject to our e-mail disclaimer statement.
Please refer to www.vestas.com/legal/notice
If you have received this e-mail in error please contact the sender. 

> -----Original Message-----
> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
> dschneiderch
> Sent: 20. marts 2015 18:19
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
> 
> I just went through my packages, iteratively unloading each one with:
> detach('package:raster')
> and found that if I unload the raster package then the expected behavior of
> keeping the existing coordnames is achieved.
> 
> now why that differs in my script I have no idea.
> 
> > coordnames(snotellocs)
> [1] "Longitude" "Latitude"
> > detach('package:raster')
> > head(as.data.frame(snotellocs))
>   network State Station_ID Site_ID       Site_Name Latitude Longitude
> 1    SNTL    AZ     11R06S     308     BAKER BUTTE 34.45660 -111.4064
> 2    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT 34.45547 -111.3827
> 3    SNTL    AZ     09S01S     310           BALDY 33.97883 -109.5034
> 4    SNTL    AZ     09S06S     902     BEAVER HEAD 33.69144 -109.2166
> 5    SNTL    AZ     09N05S    1143   BEAVER SPRING 36.32678 -109.0571
> 6    SNTL    AZ     12P01S    1139       CHALENDER 35.26247 -112.0623
>   Elevation_ft Elevation_m
> 1         7300        2225
> 2         7700        2347
> 3         9125        2781
> 4         7990        2435
> 5         9200        2804
> 6         7100        2164
> >
> 
> Dominik Schneider
> o 303.735.6296 | c 518.956.3978
> 
> 
> On Fri, Mar 20, 2015 at 11:02 AM, dschneiderch [via R-sig-geo] <
> ml-node+s2731867n7587929h1 at n2.nabble.com> wrote:
> 
> > Edzer - Look like we posted at the same time.
> > In my example my coordinates are named, no?
> >
> > I tried your example in my R environment and can replicate the behavior I
> > mentioned.
> >
> > > df = data.frame(x=1:2, y=2:1, z = 3:4)
> > > df1 = df
> > > library(sp)
> > > coordinates(df1) = ~x+y
> > > as.data.frame(df1)
> >   x y z
> > 1 1 2 3
> > 2 2 1 4
> > > coordnames(df1)
> > [1] "x" "y"
> > > coordnames(df1)=c('long','lat')
> > > str(df1)
> > Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots
> >   ..@ data       :'data.frame': 2 obs. of  1 variable:
> >   .. ..$ z: int [1:2] 3 4
> >   ..@ coords.nrs : int [1:2] 1 2
> >   ..@ coords     : num [1:2, 1:2] 1 2 2 1
> >   .. ..- attr(*, "dimnames")=List of 2
> >   .. .. ..$ : NULL
> >   .. .. ..$ : chr [1:2] "long" "lat"
> >   ..@ bbox       : num [1:2, 1:2] 1 1 2 2
> >   .. ..- attr(*, "dimnames")=List of 2
> >   .. .. ..$ : chr [1:2] "long" "lat"
> >   .. .. ..$ : chr [1:2] "min" "max"
> >   ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
> >   .. .. ..@ projargs: chr NA
> > > as.data.frame(df1)
> >   x y z
> > 1 1 2 3
> > 2 2 1 4
> >
> >
> > Interestingly, when I open a new R instance and without loading all the
> > packages associated with my project, the names are *not* converted.  So it
> > seems that one of the packages I have loaded is conflicting with sp and
> > causing the names to change...  my sessionInfo() was attached in the other
> > post.
> >
> >
> > ------------------------------
> >  If you reply to this email, your message will be added to the discussion
> > below:
> >
> > http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-
> SpatialPointsDF-tp7587920p7587929.html
> >  To unsubscribe from inconsistent as.data.frame(SpatialPointsDF), click
> > here
> > <http://r-sig-
> geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_
> by_code&node=7587920&code=RG9taW5pay5TY2huZWlkZXJAY29sb3JhZG8u
> ZWR1fDc1ODc5MjB8LTEwMzMyMTA1OQ==>
> > .
> > NAML
> > <http://r-sig-
> geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=macro_view
> er&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespac
> es.BasicNamespace-nabble.view.web.template.NabbleNamespace-
> nabble.naml.namespaces.BasicNamespace-
> nabble.view.web.template.NabbleNamespace-
> nabble.naml.namespaces.BasicNamespace-
> nabble.view.web.template.NabbleNamespace-
> nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscrib
> ers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> send_instant_email%21nabble%3Aemail.naml>
> >
> 
> 
> 
> 
> --
> View this message in context: http://r-sig-
> geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-
> tp7587920p7587931.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From r.hijmans at gmail.com  Fri Mar 20 20:39:59 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Fri, 20 Mar 2015 12:39:59 -0700
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <B078CDF40DFE4045AF172A8B4F68FC4858808A4908@DKRDSEXC016.vestas.net>
References: <1426806165111-7587920.post@n2.nabble.com>
	<D1318CA5.122E2E%macqueen1@llnl.gov>
	<550C4AA9.1050302@uni-muenster.de>
	<1426870947485-7587929.post@n2.nabble.com>
	<CAHYDKLYzJUsPsP0c0qBNan2L+DuBhs=HuhWvaRFz4Y+79GHskg@mail.gmail.com>
	<B078CDF40DFE4045AF172A8B4F68FC4858808A4908@DKRDSEXC016.vestas.net>
Message-ID: <CANtt_hwEVn-jFABJLJhhJoi6d3Lrb1OCKPQWrJ-pNG5DtHxEUw@mail.gmail.com>

The raster package 'enhances' as.data.frame for SpatialLines* and
SpatialPolygons*. I am not sure why there is also a function for
SpatialPoints* as it does not add anything, except for setting the
names to 'x' and 'y'. Perhaps that is what I did it for, I cannot
remember. But clearly it is not desirable to get different results
depending on when raster is loaded or not, so I have removed it. Sorry
for causing trouble.
Robert

On Fri, Mar 20, 2015 at 11:39 AM, Frede Aakmann T?gersen
<frtog at vestas.com> wrote:
> Hi
>
> I can reproduce this using Edzer's example. See below for output from R. Also notice the naming of the coordinate columns to "x" and "y" in the S4 method of the raster package's function as.data.frame! Isn't that quite unusual? I have maintainer on CC.
>
>> library(sp)
>> df = data.frame(Longitude=1:2, Latitude=2:1, z = 3:4)
>> df1 = df
>> coordinates(df1) = ~Longitude + Latitude
>> as.data.frame(df1)
>   Longitude Latitude z
> 1         1        2 3
> 2         2        1 4
>> library(raster)
>> search()
>  [1] ".GlobalEnv"        "package:raster"    "package:sp"
>  [4] "ESSR"              "package:stats"     "package:graphics"
>  [7] "package:grDevices" "package:utils"     "package:datasets"
> [10] "package:methods"   "Autoloads"         "package:base"
>> as.data.frame(df1)
>   x y z
> 1 1 2 3
> 2 2 1 4
>> showMethods("as.data.frame")
> Function: as.data.frame (package base)
> x="ANY"
> x="data.frame"
>     (inherited from: x="ANY")
> x="Raster"
> x="SpatialLines"
> x="SpatialPoints"
> x="SpatialPointsDataFrame"
>     (inherited from: x="SpatialPoints")
> x="SpatialPolygons"
>
>> selectMethod("as.data.frame", "SpatialPointsDataFrame")
> Method Definition:
>
> function (x, row.names = NULL, optional = FALSE, ...)
> {
>     .local <- function (x, row.names = NULL, optional = FALSE,
>         xy = TRUE, ...)
>     {
>         if (!xy) {
>             if (.hasSlot(x, "data")) {
>                 return(x at data)
>             }
>             else {
>                 return(NULL)
>             }
>         }
>         nobj <- length(x)
>         d <- coordinates(x)
>         if (.hasSlot(x, "data")) {
>             d <- cbind(d, x at data)
>         }
>         colnames(d)[1:2] <- c("x", "y")
>         rownames(d) <- NULL
>         as.data.frame(d, row.names = row.names, optional = optional,
>             ...)
>     }
>     .local(x, row.names, optional, ...)
> }
> <bytecode: 0x000000000d037260>
> <environment: namespace:raster>
>
> Signatures:
>         x
> target  "SpatialPointsDataFrame"
> defined "SpatialPoints"
>>
>
>
>
> Yours sincerely / Med venlig hilsen
>
>
> Frede Aakmann T?gersen
> Specialist, M.Sc., Ph.D.
> Plant Performance & Modeling
>
> Technology & Service Solutions
> T +45 9730 5135
> M +45 2547 6050
> frtog at vestas.com
> http://www.vestas.com
>
> Company reg. name: Vestas Wind Systems A/S
> This e-mail is subject to our e-mail disclaimer statement.
> Please refer to www.vestas.com/legal/notice
> If you have received this e-mail in error please contact the sender.
>
>> -----Original Message-----
>> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
>> dschneiderch
>> Sent: 20. marts 2015 18:19
>> To: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
>>
>> I just went through my packages, iteratively unloading each one with:
>> detach('package:raster')
>> and found that if I unload the raster package then the expected behavior of
>> keeping the existing coordnames is achieved.
>>
>> now why that differs in my script I have no idea.
>>
>> > coordnames(snotellocs)
>> [1] "Longitude" "Latitude"
>> > detach('package:raster')
>> > head(as.data.frame(snotellocs))
>>   network State Station_ID Site_ID       Site_Name Latitude Longitude
>> 1    SNTL    AZ     11R06S     308     BAKER BUTTE 34.45660 -111.4064
>> 2    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT 34.45547 -111.3827
>> 3    SNTL    AZ     09S01S     310           BALDY 33.97883 -109.5034
>> 4    SNTL    AZ     09S06S     902     BEAVER HEAD 33.69144 -109.2166
>> 5    SNTL    AZ     09N05S    1143   BEAVER SPRING 36.32678 -109.0571
>> 6    SNTL    AZ     12P01S    1139       CHALENDER 35.26247 -112.0623
>>   Elevation_ft Elevation_m
>> 1         7300        2225
>> 2         7700        2347
>> 3         9125        2781
>> 4         7990        2435
>> 5         9200        2804
>> 6         7100        2164
>> >
>>
>> Dominik Schneider
>> o 303.735.6296 | c 518.956.3978
>>
>>
>> On Fri, Mar 20, 2015 at 11:02 AM, dschneiderch [via R-sig-geo] <
>> ml-node+s2731867n7587929h1 at n2.nabble.com> wrote:
>>
>> > Edzer - Look like we posted at the same time.
>> > In my example my coordinates are named, no?
>> >
>> > I tried your example in my R environment and can replicate the behavior I
>> > mentioned.
>> >
>> > > df = data.frame(x=1:2, y=2:1, z = 3:4)
>> > > df1 = df
>> > > library(sp)
>> > > coordinates(df1) = ~x+y
>> > > as.data.frame(df1)
>> >   x y z
>> > 1 1 2 3
>> > 2 2 1 4
>> > > coordnames(df1)
>> > [1] "x" "y"
>> > > coordnames(df1)=c('long','lat')
>> > > str(df1)
>> > Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots
>> >   ..@ data       :'data.frame': 2 obs. of  1 variable:
>> >   .. ..$ z: int [1:2] 3 4
>> >   ..@ coords.nrs : int [1:2] 1 2
>> >   ..@ coords     : num [1:2, 1:2] 1 2 2 1
>> >   .. ..- attr(*, "dimnames")=List of 2
>> >   .. .. ..$ : NULL
>> >   .. .. ..$ : chr [1:2] "long" "lat"
>> >   ..@ bbox       : num [1:2, 1:2] 1 1 2 2
>> >   .. ..- attr(*, "dimnames")=List of 2
>> >   .. .. ..$ : chr [1:2] "long" "lat"
>> >   .. .. ..$ : chr [1:2] "min" "max"
>> >   ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
>> >   .. .. ..@ projargs: chr NA
>> > > as.data.frame(df1)
>> >   x y z
>> > 1 1 2 3
>> > 2 2 1 4
>> >
>> >
>> > Interestingly, when I open a new R instance and without loading all the
>> > packages associated with my project, the names are *not* converted.  So it
>> > seems that one of the packages I have loaded is conflicting with sp and
>> > causing the names to change...  my sessionInfo() was attached in the other
>> > post.
>> >
>> >
>> > ------------------------------
>> >  If you reply to this email, your message will be added to the discussion
>> > below:
>> >
>> > http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-
>> SpatialPointsDF-tp7587920p7587929.html
>> >  To unsubscribe from inconsistent as.data.frame(SpatialPointsDF), click
>> > here
>> > <http://r-sig-
>> geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_
>> by_code&node=7587920&code=RG9taW5pay5TY2huZWlkZXJAY29sb3JhZG8u
>> ZWR1fDc1ODc5MjB8LTEwMzMyMTA1OQ==>
>> > .
>> > NAML
>> > <http://r-sig-
>> geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=macro_view
>> er&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespac
>> es.BasicNamespace-nabble.view.web.template.NabbleNamespace-
>> nabble.naml.namespaces.BasicNamespace-
>> nabble.view.web.template.NabbleNamespace-
>> nabble.naml.namespaces.BasicNamespace-
>> nabble.view.web.template.NabbleNamespace-
>> nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscrib
>> ers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
>> send_instant_email%21nabble%3Aemail.naml>
>> >
>>
>>
>>
>>
>> --
>> View this message in context: http://r-sig-
>> geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-
>> tp7587920p7587931.html
>> Sent from the R-sig-geo mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From Dominik.Schneider at colorado.edu  Fri Mar 20 21:09:45 2015
From: Dominik.Schneider at colorado.edu (Dominik Schneider)
Date: Fri, 20 Mar 2015 14:09:45 -0600
Subject: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
In-Reply-To: <CANtt_hwEVn-jFABJLJhhJoi6d3Lrb1OCKPQWrJ-pNG5DtHxEUw@mail.gmail.com>
References: <1426806165111-7587920.post@n2.nabble.com>
	<D1318CA5.122E2E%macqueen1@llnl.gov>
	<550C4AA9.1050302@uni-muenster.de>
	<1426870947485-7587929.post@n2.nabble.com>
	<CAHYDKLYzJUsPsP0c0qBNan2L+DuBhs=HuhWvaRFz4Y+79GHskg@mail.gmail.com>
	<B078CDF40DFE4045AF172A8B4F68FC4858808A4908@DKRDSEXC016.vestas.net>
	<CANtt_hwEVn-jFABJLJhhJoi6d3Lrb1OCKPQWrJ-pNG5DtHxEUw@mail.gmail.com>
Message-ID: <CAHYDKLann_p2=QfszqAHm-ck7seEpMjRv8AX9EVzwqQ2DHd6aw@mail.gmail.com>

Thanks for the update.
FWIW, I actually liked the behavior of changing coordinate names to x and y
when converting out of geographic coordinates to projected coordinates. and
vice versa would be convenient but I will make a habit of using
coordinates() and coordnames() instead.
ds


Dominik Schneider
o 303.735.6296 | c 518.956.3978


On Fri, Mar 20, 2015 at 1:39 PM, Robert J. Hijmans <r.hijmans at gmail.com>
wrote:

> The raster package 'enhances' as.data.frame for SpatialLines* and
> SpatialPolygons*. I am not sure why there is also a function for
> SpatialPoints* as it does not add anything, except for setting the
> names to 'x' and 'y'. Perhaps that is what I did it for, I cannot
> remember. But clearly it is not desirable to get different results
> depending on when raster is loaded or not, so I have removed it. Sorry
> for causing trouble.
> Robert
>
> On Fri, Mar 20, 2015 at 11:39 AM, Frede Aakmann T?gersen
> <frtog at vestas.com> wrote:
> > Hi
> >
> > I can reproduce this using Edzer's example. See below for output from R.
> Also notice the naming of the coordinate columns to "x" and "y" in the S4
> method of the raster package's function as.data.frame! Isn't that quite
> unusual? I have maintainer on CC.
> >
> >> library(sp)
> >> df = data.frame(Longitude=1:2, Latitude=2:1, z = 3:4)
> >> df1 = df
> >> coordinates(df1) = ~Longitude + Latitude
> >> as.data.frame(df1)
> >   Longitude Latitude z
> > 1         1        2 3
> > 2         2        1 4
> >> library(raster)
> >> search()
> >  [1] ".GlobalEnv"        "package:raster"    "package:sp"
> >  [4] "ESSR"              "package:stats"     "package:graphics"
> >  [7] "package:grDevices" "package:utils"     "package:datasets"
> > [10] "package:methods"   "Autoloads"         "package:base"
> >> as.data.frame(df1)
> >   x y z
> > 1 1 2 3
> > 2 2 1 4
> >> showMethods("as.data.frame")
> > Function: as.data.frame (package base)
> > x="ANY"
> > x="data.frame"
> >     (inherited from: x="ANY")
> > x="Raster"
> > x="SpatialLines"
> > x="SpatialPoints"
> > x="SpatialPointsDataFrame"
> >     (inherited from: x="SpatialPoints")
> > x="SpatialPolygons"
> >
> >> selectMethod("as.data.frame", "SpatialPointsDataFrame")
> > Method Definition:
> >
> > function (x, row.names = NULL, optional = FALSE, ...)
> > {
> >     .local <- function (x, row.names = NULL, optional = FALSE,
> >         xy = TRUE, ...)
> >     {
> >         if (!xy) {
> >             if (.hasSlot(x, "data")) {
> >                 return(x at data)
> >             }
> >             else {
> >                 return(NULL)
> >             }
> >         }
> >         nobj <- length(x)
> >         d <- coordinates(x)
> >         if (.hasSlot(x, "data")) {
> >             d <- cbind(d, x at data)
> >         }
> >         colnames(d)[1:2] <- c("x", "y")
> >         rownames(d) <- NULL
> >         as.data.frame(d, row.names = row.names, optional = optional,
> >             ...)
> >     }
> >     .local(x, row.names, optional, ...)
> > }
> > <bytecode: 0x000000000d037260>
> > <environment: namespace:raster>
> >
> > Signatures:
> >         x
> > target  "SpatialPointsDataFrame"
> > defined "SpatialPoints"
> >>
> >
> >
> >
> > Yours sincerely / Med venlig hilsen
> >
> >
> > Frede Aakmann T?gersen
> > Specialist, M.Sc., Ph.D.
> > Plant Performance & Modeling
> >
> > Technology & Service Solutions
> > T +45 9730 5135
> > M +45 2547 6050
> > frtog at vestas.com
> > http://www.vestas.com
> >
> > Company reg. name: Vestas Wind Systems A/S
> > This e-mail is subject to our e-mail disclaimer statement.
> > Please refer to www.vestas.com/legal/notice
> > If you have received this e-mail in error please contact the sender.
> >
> >> -----Original Message-----
> >> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
> >> dschneiderch
> >> Sent: 20. marts 2015 18:19
> >> To: r-sig-geo at r-project.org
> >> Subject: Re: [R-sig-Geo] inconsistent as.data.frame(SpatialPointsDF)
> >>
> >> I just went through my packages, iteratively unloading each one with:
> >> detach('package:raster')
> >> and found that if I unload the raster package then the expected
> behavior of
> >> keeping the existing coordnames is achieved.
> >>
> >> now why that differs in my script I have no idea.
> >>
> >> > coordnames(snotellocs)
> >> [1] "Longitude" "Latitude"
> >> > detach('package:raster')
> >> > head(as.data.frame(snotellocs))
> >>   network State Station_ID Site_ID       Site_Name Latitude Longitude
> >> 1    SNTL    AZ     11R06S     308     BAKER BUTTE 34.45660 -111.4064
> >> 2    SNTL    AZ     11R07S    1140 BAKER BUTTE SMT 34.45547 -111.3827
> >> 3    SNTL    AZ     09S01S     310           BALDY 33.97883 -109.5034
> >> 4    SNTL    AZ     09S06S     902     BEAVER HEAD 33.69144 -109.2166
> >> 5    SNTL    AZ     09N05S    1143   BEAVER SPRING 36.32678 -109.0571
> >> 6    SNTL    AZ     12P01S    1139       CHALENDER 35.26247 -112.0623
> >>   Elevation_ft Elevation_m
> >> 1         7300        2225
> >> 2         7700        2347
> >> 3         9125        2781
> >> 4         7990        2435
> >> 5         9200        2804
> >> 6         7100        2164
> >> >
> >>
> >> Dominik Schneider
> >> o 303.735.6296 | c 518.956.3978
> >>
> >>
> >> On Fri, Mar 20, 2015 at 11:02 AM, dschneiderch [via R-sig-geo] <
> >> ml-node+s2731867n7587929h1 at n2.nabble.com> wrote:
> >>
> >> > Edzer - Look like we posted at the same time.
> >> > In my example my coordinates are named, no?
> >> >
> >> > I tried your example in my R environment and can replicate the
> behavior I
> >> > mentioned.
> >> >
> >> > > df = data.frame(x=1:2, y=2:1, z = 3:4)
> >> > > df1 = df
> >> > > library(sp)
> >> > > coordinates(df1) = ~x+y
> >> > > as.data.frame(df1)
> >> >   x y z
> >> > 1 1 2 3
> >> > 2 2 1 4
> >> > > coordnames(df1)
> >> > [1] "x" "y"
> >> > > coordnames(df1)=c('long','lat')
> >> > > str(df1)
> >> > Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots
> >> >   ..@ data       :'data.frame': 2 obs. of  1 variable:
> >> >   .. ..$ z: int [1:2] 3 4
> >> >   ..@ coords.nrs : int [1:2] 1 2
> >> >   ..@ coords     : num [1:2, 1:2] 1 2 2 1
> >> >   .. ..- attr(*, "dimnames")=List of 2
> >> >   .. .. ..$ : NULL
> >> >   .. .. ..$ : chr [1:2] "long" "lat"
> >> >   ..@ bbox       : num [1:2, 1:2] 1 1 2 2
> >> >   .. ..- attr(*, "dimnames")=List of 2
> >> >   .. .. ..$ : chr [1:2] "long" "lat"
> >> >   .. .. ..$ : chr [1:2] "min" "max"
> >> >   ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
> >> >   .. .. ..@ projargs: chr NA
> >> > > as.data.frame(df1)
> >> >   x y z
> >> > 1 1 2 3
> >> > 2 2 1 4
> >> >
> >> >
> >> > Interestingly, when I open a new R instance and without loading all
> the
> >> > packages associated with my project, the names are *not* converted.
> So it
> >> > seems that one of the packages I have loaded is conflicting with sp
> and
> >> > causing the names to change...  my sessionInfo() was attached in the
> other
> >> > post.
> >> >
> >> >
> >> > ------------------------------
> >> >  If you reply to this email, your message will be added to the
> discussion
> >> > below:
> >> >
> >> > http://r-sig-geo.2731867.n2.nabble.com/inconsistent-as-data-frame-
> >> SpatialPointsDF-tp7587920p7587929.html
> >> >  To unsubscribe from inconsistent as.data.frame(SpatialPointsDF),
> click
> >> > here
> >> > <http://r-sig-
> >> geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_
> >> by_code&node=7587920&code=RG9taW5pay5TY2huZWlkZXJAY29sb3JhZG8u
> >> ZWR1fDc1ODc5MjB8LTEwMzMyMTA1OQ==>
> >> > .
> >> > NAML
> >> > <http://r-sig-
> >> geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=macro_view
> >> er&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespac
> >> es.BasicNamespace-nabble.view.web.template.NabbleNamespace-
> >> nabble.naml.namespaces.BasicNamespace-
> >> nabble.view.web.template.NabbleNamespace-
> >> nabble.naml.namespaces.BasicNamespace-
> >> nabble.view.web.template.NabbleNamespace-
> >> nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscrib
> >> ers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-
> >> send_instant_email%21nabble%3Aemail.naml>
> >> >
> >>
> >>
> >>
> >>
> >> --
> >> View this message in context: http://r-sig-
> >> geo.2731867.n2.nabble.com/inconsistent-as-data-frame-SpatialPointsDF-
> >> tp7587920p7587931.html
> >> Sent from the R-sig-geo mailing list archive at Nabble.com.
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From hengl at spatial-analyst.net  Sun Mar 22 21:46:07 2015
From: hengl at spatial-analyst.net (Tomislav Hengl)
Date: Sun, 22 Mar 2015 21:46:07 +0100
Subject: [R-sig-Geo] Error: 'rawTransform' is not an exported object from
 'namespace:rgdal' in the projectRaster
Message-ID: <550F2A0F.7040906@spatial-analyst.net>


Hi Robert,

I think something happened in the raster package v2.3-33 and your 
function projectRaster reports now an error (which probably affects 
several other packages).

 > # create a new (not projected) RasterLayer with cellnumbers as values
 > r <- raster(xmn=-110, xmx=-90, ymn=40, ymx=60, ncols=40, nrows=40)
 > r <- setValues(r, 1:ncell(r))
 > projection(r)
[1] "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
 > # proj.4 projection description
 > newproj <- "+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84"
 >
 > pr1 <- projectRaster(r, crs=newproj)
Error: 'rawTransform' is not an exported object from 'namespace:rgdal'

Thank you!

T. Hengl


From edzer.pebesma at uni-muenster.de  Sun Mar 22 22:23:48 2015
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Sun, 22 Mar 2015 22:23:48 +0100
Subject: [R-sig-Geo] Error: 'rawTransform' is not an exported object
 from 'namespace:rgdal' in the projectRaster
In-Reply-To: <550F2A0F.7040906@spatial-analyst.net>
References: <550F2A0F.7040906@spatial-analyst.net>
Message-ID: <550F32E4.8080502@uni-muenster.de>

Hi Tom, your example works for me; maybe update rgdal? If this is the
problem, than raster DESCRIPTION needs to update its rgdal dependency.

> sessionInfo()
R version 3.1.3 (2015-03-09)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.2 LTS

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] raster_2.3-33 sp_1.1-0

loaded via a namespace (and not attached):
[1] grid_3.1.3      lattice_0.20-30 rgdal_0.9-2

On 03/22/2015 09:46 PM, Tomislav Hengl wrote:
> 
> Hi Robert,
> 
> I think something happened in the raster package v2.3-33 and your
> function projectRaster reports now an error (which probably affects
> several other packages).
> 
>> # create a new (not projected) RasterLayer with cellnumbers as values
>> r <- raster(xmn=-110, xmx=-90, ymn=40, ymx=60, ncols=40, nrows=40)
>> r <- setValues(r, 1:ncell(r))
>> projection(r)
> [1] "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
>> # proj.4 projection description
>> newproj <- "+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84"
>>
>> pr1 <- projectRaster(r, crs=newproj)
> Error: 'rawTransform' is not an exported object from 'namespace:rgdal'
> 
> Thank you!
> 
> T. Hengl
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi),  University of M?nster,
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150322/f80d2037/attachment.bin>

From villers.alexandre at gmail.com  Sun Mar 22 22:29:48 2015
From: villers.alexandre at gmail.com (Alexandre Villers)
Date: Sun, 22 Mar 2015 22:29:48 +0100
Subject: [R-sig-Geo] Error: 'rawTransform' is not an exported object
 from 'namespace:rgdal' in the projectRaster
In-Reply-To: <550F32E4.8080502@uni-muenster.de>
References: <550F2A0F.7040906@spatial-analyst.net>
	<550F32E4.8080502@uni-muenster.de>
Message-ID: <550F344C.4060404@gmail.com>

Hi Tom,

It also works for me with this

R version 3.1.0 (2014-04-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=French_France.1252  LC_CTYPE=French_France.1252 
LC_MONETARY=French_France.1252
[4] LC_NUMERIC=C                   LC_TIME=French_France.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods base

other attached packages:
[1] rgdal_0.9-1  raster_2.3-0 sp_1.0-15

loaded via a namespace (and not attached):
[1] grid_3.1.0      lattice_0.20-29


HTH

Alex

Le 22/03/2015 22:23, Edzer Pebesma a ?crit :
> Hi Tom, your example works for me; maybe update rgdal? If this is the
> problem, than raster DESCRIPTION needs to update its rgdal dependency.
>
>> sessionInfo()
> R version 3.1.3 (2015-03-09)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 14.04.2 LTS
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] raster_2.3-33 sp_1.1-0
>
> loaded via a namespace (and not attached):
> [1] grid_3.1.3      lattice_0.20-30 rgdal_0.9-2
>
> On 03/22/2015 09:46 PM, Tomislav Hengl wrote:
>> Hi Robert,
>>
>> I think something happened in the raster package v2.3-33 and your
>> function projectRaster reports now an error (which probably affects
>> several other packages).
>>
>>> # create a new (not projected) RasterLayer with cellnumbers as values
>>> r <- raster(xmn=-110, xmx=-90, ymn=40, ymx=60, ncols=40, nrows=40)
>>> r <- setValues(r, 1:ncell(r))
>>> projection(r)
>> [1] "+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"
>>> # proj.4 projection description
>>> newproj <- "+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84"
>>>
>>> pr1 <- projectRaster(r, crs=newproj)
>> Error: 'rawTransform' is not an exported object from 'namespace:rgdal'
>>
>> Thank you!
>>
>> T. Hengl
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From richard.asturia at gmail.com  Mon Mar 23 08:41:52 2015
From: richard.asturia at gmail.com (Richard Asturia)
Date: Mon, 23 Mar 2015 04:41:52 -0300
Subject: [R-sig-Geo] Very dissimilar results with Moran's I and EBI
Message-ID: <CA+xNL7qdHpLd93SWWKVrjrCx7eRj_4u6vgAcnouX1cO=jp6t_w@mail.gmail.com>

Dear list,

I have used a colleague's data to compare geographical clustering using
different ways of detecting clustering. Today, I found very dissimilar
results for the statistic of Moran's I and of the EBI version of Moran's I
(using spdep's EBImoran.mc), what I understand was not supposed to be the
case. In fact, with EBI it was twice as big (1.23 against 0.69).

The issue is that the EBI statistic was supposed to be similar to Moran's I
in what regards the value of Z, while not necessarily in what regards the
p-value.

I would like to ask your advice on what seems to be going wrong. In order
to give a reproducible example without having to make you download my
shapefile, I wrote a code that creates precisely the same neighborhood
structure of the map I am using.

Any advice will be very much appreciated.

Thanks in advance,

RA


#THE CODE:
#obviously loading the package:
require(spdep)

#this big messy piece of code recreates in matrix form the neighborhood
structure:
neighb.structure
<-matrix(c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.142857142857143,0.142857142857143,0,0.142857142857143,0,0,0,0,0,0,0.142857142857143,0,0.142857142857143,0,0.142857142857143,0,0,0.142857142857143,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0.25,0,0.25,0,0,0,0,0,0.25,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0.25,0,0,0,0,0.166666666666667,0,0,0,0,0,0,0,0,0.166666666666667,0,0,0,0,0.166666666666667,0.166666666666667,0,0,0.166666666666667,0.166666666666667,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0,0.125,0.125,0,0,0,0,0.125,0,0,0,0,0,0.125,0.125,0,0,0,0.125,0.125,0,0,0.125,0,0,0,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0.2,0,0.2,0,0.2,0,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0.333333333333333,0,0,0,0,0,0,0,0,0.125,0,0,0,0.125,0.125,0,0,0,0.125,0,0.125,0,0,0.125,0,0,0,0,0,0.125,0,0,0,0,0.125,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0.25,0.25,0,0.25,0,0,0,0.2,0,0,0,0,0,0.2,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0.2,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0.166666666666667,0,0.166666666666667,0,0,0,0,0,0,0,0,0.166666666666667,0.166666666666667,0,0,0,0.166666666666667,0,0,0,0.166666666666667,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0),28,28)

#recreating the absolute value of the cases of interest in each feature of
the map:
cases <-
c(5,55,6,14,22,2,8,159,158,0,15,85,21,8,3,27,1128,18,16,101,72,14,1,55,18,6,0,4)

#recreating  the absolute value of the population in each feature of the
map:
pop <-
c(865,4534,9708,1799,4929,2154,4159,10414,10917,1420,1166,26098,8417,3049,1853,2419,49108,2982,4218,6824,6923,1961,1259,13661,2418,3672,811,1375)

#recreating the rate values in each feature of the map:
rates <- cases/pop

#generating a listW:
neighb.structure <- t(neighb.structure)
row.names(neighb.structure) <-
my.listW <- mat2listw(neighb.structure)

#running the tests:
moran.test(x=rates, listw=my.listW)
moran.mc(x=rates, listw=my.listW, nsim=999)
EBImoran.mc(n=cases, x=pop, listw=my.listW, nsim=999)

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Mon Mar 23 10:29:59 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 23 Mar 2015 10:29:59 +0100
Subject: [R-sig-Geo] Very dissimilar results with Moran's I and EBI
In-Reply-To: <CA+xNL7qdHpLd93SWWKVrjrCx7eRj_4u6vgAcnouX1cO=jp6t_w@mail.gmail.com>
References: <CA+xNL7qdHpLd93SWWKVrjrCx7eRj_4u6vgAcnouX1cO=jp6t_w@mail.gmail.com>
Message-ID: <alpine.LFD.2.11.1503230959230.30005@reclus.nhh.no>

On Mon, 23 Mar 2015, Richard Asturia wrote:

> Dear list,
>
> I have used a colleague's data to compare geographical clustering using
> different ways of detecting clustering. Today, I found very dissimilar
> results for the statistic of Moran's I and of the EBI version of Moran's I
> (using spdep's EBImoran.mc), what I understand was not supposed to be the
> case. In fact, with EBI it was twice as big (1.23 against 0.69).
>
> The issue is that the EBI statistic was supposed to be similar to Moran's I
> in what regards the value of Z, while not necessarily in what regards the
> p-value.
>
> I would like to ask your advice on what seems to be going wrong. In order
> to give a reproducible example without having to make you download my
> shapefile, I wrote a code that creates precisely the same neighborhood
> structure of the map I am using.

Thanks for trying to provide an example. Unfortunately, you posted HTML, 
which makes it very hard to copy and paste from any email client. The 
correct way to move an nb object would be:

nb <- structure(list(21L, c(8L, 9L, 11L, 18L, 20L, 22L, 25L), c(10L,
14L, 26L), c(8L, 25L), c(6L, 12L, 15L, 23L, 26L), c(5L, 12L,
13L, 18L, 27L), c(13L, 16L, 18L, 24L), c(2L, 4L, 20L, 25L), c(2L,
11L, 16L, 17L, 20L, 21L), c(3L, 14L, 28L), c(2L, 9L, 16L, 18L
), c(5L, 6L, 13L, 23L), c(6L, 7L, 12L, 18L, 19L, 23L, 24L, 27L
), c(3L, 10L, 26L, 28L), c(5L, 26L), c(7L, 9L, 11L, 18L, 21L),
     c(9L, 20L, 21L), c(2L, 6L, 7L, 11L, 13L, 16L, 22L, 27L),
     c(13L, 23L, 24L, 26L), c(2L, 8L, 9L, 17L, 21L), c(1L, 9L,
     16L, 17L, 20L), c(2L, 18L, 25L), c(5L, 12L, 13L, 19L, 26L
     ), c(7L, 13L, 19L), c(2L, 4L, 8L, 22L), c(3L, 5L, 14L, 15L,
     19L, 23L), c(6L, 13L, 18L), c(10L, 14L)), class = "nb", region.id = 
c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
"14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
"25", "26", "27", "28"), call = NA, sym = TRUE)

constructed by using dput() - so using spaces permitting line breaks, and 
for the others:

cases <- c(5, 55, 6, 14, 22, 2, 8, 159, 158, 0, 15, 85, 21, 8, 3, 27,
1128, 18, 16, 101, 72, 14, 1, 55, 18, 6, 0, 4)

pop <- c(865, 4534, 9708, 1799, 4929, 2154, 4159, 10414, 10917, 1420,
1166, 26098, 8417, 3049, 1853, 2419, 49108, 2982, 4218, 6824,
6923, 1961, 1259, 13661, 2418, 3672, 811, 1375)

and

my.listW <- nb2listw(nb, style="W")

Could you please say why you expect the values of I to be similar? Is 
this claimed in:

Assun??o RM, Reis EA 1999 A new proposal to adjust Moran's I for
population density. Statistics in Medicine 18, pp. 2147-2162

Running

summary(glm(cases ~ 1 + offset(log(pop)), family=quasipoisson))

suggests that the data are overdispersed, so the assumptions for empirical 
Bayes adjustment may not be met. For Negative Binomial alternatives, see 
the DCluster package.

Hope this helps,

Roger

PS. It would be helpful to know why you are asking, and very helpful to 
have an affiliation in your signature. At least you use a real name!

>
> Any advice will be very much appreciated.
>
> Thanks in advance,
>
> RA
>
>
> #THE CODE:
> #obviously loading the package:
> require(spdep)
>
> #this big messy piece of code recreates in matrix form the neighborhood
> structure:
> neighb.structure
> <-matrix(c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.142857142857143,0.142857142857143,0,0.142857142857143,0,0,0,0,0,0,0.142857142857143,0,0.142857142857143,0,0.142857142857143,0,0,0.142857142857143,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0.25,0,0.25,0,0,0,0,0,0.25,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0.25,0,0,0,0,0.166666666666667,0,0,0,0,0,0,0,0,0.166666666666667,0,0,0,0,0.166666666666667,0.166666666666667,0,0,0.166666666666667,0.166666666666667,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0,0,0,0,0,!
> 0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0,0.125,0.125,0,0,0,0,0.125,0,0,0,0,0,0.125,0.125,0,0,0,0.125,0.125,0,0,0.125,0,0,0,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0.2,0,0.2,0,0.2,0,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0.333333333333333,0,0,0,0,0,0,0,0,0.125,0,0,0,0.125,0.125,0,0,0,0.125,0,0.125,0,0,0.125,0,0,0,0,0,0.125,0,0,0,0,0.125,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0.25,0.25,0,0.25,0,0,0,0.2,0,0,0,0,0,0.2,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0.2,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0.3333!
> 33333333333,0,0,0,0,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0.25,0,0,0,0,0,0,0,0
> ,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0.166666666666667,0,0.166666666666667,0,0,0,0,0,0,0,0,0.166666666666667,0.166666666666667,0,0,0,0.166666666666667,0,0,0,0.166666666666667,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0),28,28)
>
> #recreating the absolute value of the cases of interest in each feature of
> the map:
> cases <-
> c(5,55,6,14,22,2,8,159,158,0,15,85,21,8,3,27,1128,18,16,101,72,14,1,55,18,6,0,4)
>
> #recreating  the absolute value of the population in each feature of the
> map:
> pop <-
> c(865,4534,9708,1799,4929,2154,4159,10414,10917,1420,1166,26098,8417,3049,1853,2419,49108,2982,4218,6824,6923,1961,1259,13661,2418,3672,811,1375)
>
> #recreating the rate values in each feature of the map:
> rates <- cases/pop
>
> #generating a listW:
> neighb.structure <- t(neighb.structure)
> row.names(neighb.structure) <-
> my.listW <- mat2listw(neighb.structure)
>
> #running the tests:
> moran.test(x=rates, listw=my.listW)
> moran.mc(x=rates, listw=my.listW, nsim=999)
> EBImoran.mc(n=cases, x=pop, listw=my.listW, nsim=999)
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no

From Roger.Bivand at nhh.no  Mon Mar 23 11:43:38 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 23 Mar 2015 11:43:38 +0100
Subject: [R-sig-Geo] Very dissimilar results with Moran's I and EBI
In-Reply-To: <alpine.LFD.2.11.1503230959230.30005@reclus.nhh.no>
References: <CA+xNL7qdHpLd93SWWKVrjrCx7eRj_4u6vgAcnouX1cO=jp6t_w@mail.gmail.com>
	<alpine.LFD.2.11.1503230959230.30005@reclus.nhh.no>
Message-ID: <alpine.LFD.2.11.1503231134320.30005@reclus.nhh.no>

On Mon, 23 Mar 2015, Roger Bivand wrote:

Reason for difference inline below:

> On Mon, 23 Mar 2015, Richard Asturia wrote:
>
>>  Dear list,
>>
>>  I have used a colleague's data to compare geographical clustering using
>>  different ways of detecting clustering. Today, I found very dissimilar
>>  results for the statistic of Moran's I and of the EBI version of Moran's I
>>  (using spdep's EBImoran.mc), what I understand was not supposed to be the
>>  case. In fact, with EBI it was twice as big (1.23 against 0.69).
>>
>>  The issue is that the EBI statistic was supposed to be similar to Moran's
>>  I
>>  in what regards the value of Z, while not necessarily in what regards the
>>  p-value.
>>
>>  I would like to ask your advice on what seems to be going wrong. In order
>>  to give a reproducible example without having to make you download my
>>  shapefile, I wrote a code that creates precisely the same neighborhood
>>  structure of the map I am using.
>
> Thanks for trying to provide an example. Unfortunately, you posted HTML, 
> which makes it very hard to copy and paste from any email client. The correct 
> way to move an nb object would be:
>
> nb <- structure(list(21L, c(8L, 9L, 11L, 18L, 20L, 22L, 25L), c(10L,
> 14L, 26L), c(8L, 25L), c(6L, 12L, 15L, 23L, 26L), c(5L, 12L,
> 13L, 18L, 27L), c(13L, 16L, 18L, 24L), c(2L, 4L, 20L, 25L), c(2L,
> 11L, 16L, 17L, 20L, 21L), c(3L, 14L, 28L), c(2L, 9L, 16L, 18L
> ) , c(5L, 6L, 13L, 23L), c(6L, 7L, 12L, 18L, 19L, 23L, 24L, 27L
> ) , c(3L, 10L, 26L, 28L), c(5L, 26L), c(7L, 9L, 11L, 18L, 21L),
>     c(9L, 20L, 21L), c(2L, 6L, 7L, 11L, 13L, 16L, 22L, 27L),
>     c(13L, 23L, 24L, 26L), c(2L, 8L, 9L, 17L, 21L), c(1L, 9L,
>     16L, 17L, 20L), c(2L, 18L, 25L), c(5L, 12L, 13L, 19L, 26L
>    ) , c(7L, 13L, 19L), c(2L, 4L, 8L, 22L), c(3L, 5L, 14L, 15L,
>    19L, 23L), c(6L, 13L, 18L), c(10L, 14L)), class = "nb", region.id = 
> c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13",
> "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
> "25", "26", "27", "28"), call = NA, sym = TRUE)
>
> constructed by using dput() - so using spaces permitting line breaks, and for 
> the others:
>
> cases <- c(5, 55, 6, 14, 22, 2, 8, 159, 158, 0, 15, 85, 21, 8, 3, 27,
> 1128, 18, 16, 101, 72, 14, 1, 55, 18, 6, 0, 4)
>
> pop <- c(865, 4534, 9708, 1799, 4929, 2154, 4159, 10414, 10917, 1420,
> 1166, 26098, 8417, 3049, 1853, 2419, 49108, 2982, 4218, 6824,
> 6923, 1961, 1259, 13661, 2418, 3672, 811, 1375)
>
> and
>
> my.listW <- nb2listw(nb, style="W")
>
> Could you please say why you expect the values of I to be similar? Is this 
> claimed in:
>
> Assun??o RM, Reis EA 1999 A new proposal to adjust Moran's I for
> population density. Statistics in Medicine 18, pp. 2147-2162

On p. 2157, EBI is n/S0 * sum(z_i*lag(z_i))/sum((z_i - \bar(z))^2)

so sum(z_i*lag(z_i)) is implemented as it stands. If we change this to 
differences from the mean of z:

sum((z_i - \bar(z))*(lag(z_i - \bar(z))))

we get the same result as moran() on the EB smoothed rates. So the 
question is whether subtracting the mean of the EB smoothed rates in the 
numerator was intended but that there is a misprint in the paper, or 
whether it was intentionally omitted. The implementation follows the 
printed formula.

I'll add an option to the function to permit the subtraction of the mean 
smoothed rate.

Roger

>
> Running
>
> summary(glm(cases ~ 1 + offset(log(pop)), family=quasipoisson))
>
> suggests that the data are overdispersed, so the assumptions for empirical 
> Bayes adjustment may not be met. For Negative Binomial alternatives, see the 
> DCluster package.
>
> Hope this helps,
>
> Roger
>
> PS. It would be helpful to know why you are asking, and very helpful to have 
> an affiliation in your signature. At least you use a real name!
>
>>
>>  Any advice will be very much appreciated.
>>
>>  Thanks in advance,
>>
>>  RA
>> 
>> 
>> # THE CODE:
>> # obviously loading the package:
>>  require(spdep)
>>
>>  #this big messy piece of code recreates in matrix form the neighborhood
>>  structure:
>>  neighb.structure
>>  <-matrix(c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.142857142857143,0.142857142857143,0,0.142857142857143,0,0,0,0,0,0,0.142857142857143,0,0.142857142857143,0,0.142857142857143,0,0,0.142857142857143,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0.25,0,0.25,0,0,0,0,0,0.25,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0.25,0,0,0,0,0.166666666666667,0,0,0,0,0,0,0,0,0.166666666666667,0,0,0,0,0.166666666666667,0.166666666666667,0,0,0.166666666666667,0.166666666666667,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0,0,0,0
>>  ,0,!
>>  0,0,0,0,0,0.25,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0,0.125,0.125,0,0,0,0,0.125,0,0,0,0,0,0.125,0.125,0,0,0,0.125,0.125,0,0,0.125,0,0,0,0.25,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0,0,0,0,0,0.2,0,0.2,0,0.2,0,0,0,0,0,0,0.2,0,0,0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0.333333333333333,0,0,0,0,0,0,0,0,0.125,0,0,0,0.125,0.125,0,0,0,0.125,0,0.125,0,0,0.125,0,0,0,0,0,0.125,0,0,0,0,0.125,0,0,0,0,0,0,0,0,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0,0.25,0.25,0,0.25,0,0,0,0.2,0,0,0,0,0,0.2,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0.2,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0.2,0,0,0,0,0,0.2,0,0,0,0,0,0,0.2,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0.33
>>  33!
>>  33333333333,0,0,0,0,0,0,0,0,0,0,0.25,0,0.25,0,0,0,0.25,0,0,0,0,0,0,0,0
>>  ,0,0,0,0,0,0.25,0,0,0,0,0,0,0,0,0.166666666666667,0,0.166666666666667,0,0,0,0,0,0,0,0,0.166666666666667,0.166666666666667,0,0,0,0.166666666666667,0,0,0,0.166666666666667,0,0,0,0,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0.333333333333333,0,0,0,0,0.333333333333333,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0,0.5,0,0,0,0,0,0,0,0,0,0,0,0,0,0),28,28)
>>
>>  #recreating the absolute value of the cases of interest in each feature of
>>  the map:
>>  cases <-
>>  c(5,55,6,14,22,2,8,159,158,0,15,85,21,8,3,27,1128,18,16,101,72,14,1,55,18,6,0,4)
>>
>>  #recreating  the absolute value of the population in each feature of the
>>  map:
>>  pop <-
>>  c(865,4534,9708,1799,4929,2154,4159,10414,10917,1420,1166,26098,8417,3049,1853,2419,49108,2982,4218,6824,6923,1961,1259,13661,2418,3672,811,1375)
>>
>>  #recreating the rate values in each feature of the map:
>>  rates <- cases/pop
>>
>>  #generating a listW:
>>  neighb.structure <- t(neighb.structure)
>>  row.names(neighb.structure) <-
>>  my.listW <- mat2listw(neighb.structure)
>>
>>  #running the tests:
>>  moran.test(x=rates, listw=my.listW)
>>  moran.mc(x=rates, listw=my.listW, nsim=999)
>>  EBImoran.mc(n=cases, x=pop, listw=my.listW, nsim=999)
>>
>>   [[alternative HTML version deleted]]
>>
>>  _______________________________________________
>>  R-sig-Geo mailing list
>>  R-sig-Geo at r-project.org
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> 
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no

From mathieu.rajerison at gmail.com  Mon Mar 23 13:07:11 2015
From: mathieu.rajerison at gmail.com (Mathieu Rajerison)
Date: Mon, 23 Mar 2015 13:07:11 +0100
Subject: [R-sig-Geo] points sampled along a Line don't seem to intersect it
Message-ID: <CAGfc75m7MOt9SrMOriEGLDSePM1dmEeSYFKgXCcd9iOVLJkbrQ@mail.gmail.com>

Hi,


I have angle values affected to each segment of a road network and I'd like
to get the angles for sampling points.

The problem is that my points don't seem to intersect with my roads network.

Here is the code and the data is attached to this mail.

> library(rgeos)

> pts <- spsample(rds.un, type="regular", n=100)

> gIntersects(pts, rds.un)
[1] FALSE


Any idea ?

Best,

Mathieu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150323/bc5d4569/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: rds.un.rda
Type: application/octet-stream
Size: 13077 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150323/bc5d4569/attachment.obj>

From b.rowlingson at lancaster.ac.uk  Mon Mar 23 13:30:26 2015
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 23 Mar 2015 12:30:26 +0000
Subject: [R-sig-Geo] points sampled along a Line don't seem to intersect
	it
In-Reply-To: <CAGfc75m7MOt9SrMOriEGLDSePM1dmEeSYFKgXCcd9iOVLJkbrQ@mail.gmail.com>
References: <CAGfc75m7MOt9SrMOriEGLDSePM1dmEeSYFKgXCcd9iOVLJkbrQ@mail.gmail.com>
Message-ID: <CANVKczPDz=F6fBNeybaMjVw1Nh4fPe-jEyXRZJkLkmJ0Hy7igw@mail.gmail.com>

On Mon, Mar 23, 2015 at 12:07 PM, Mathieu Rajerison
<mathieu.rajerison at gmail.com> wrote:
> pts <- spsample(rds.un, type="regular", n=100)

Your points are at least 3.9x10^-17 units from the lines:

 gDistance(pts, rds.un)
[1] 3.908727e-17

I can only think this is due to a difference in how spsample and how
rgeos interpolate lines from points. FAQ 7.31 in disguise?

I suspect you could use gBuffer on your points with a buffer width of
1e-10 to get a reliable intersection, but that would be a line segment
(or segments if the point is close to a junction) but you could
possibly just take a single point from that line geometry and in the
worst case scenario you're only 1e-10 out...



Barry


From Roger.Bivand at nhh.no  Mon Mar 23 20:30:16 2015
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 23 Mar 2015 20:30:16 +0100
Subject: [R-sig-Geo] points sampled along a Line don't seem to intersect
 it
In-Reply-To: <CANVKczPDz=F6fBNeybaMjVw1Nh4fPe-jEyXRZJkLkmJ0Hy7igw@mail.gmail.com>
References: <CAGfc75m7MOt9SrMOriEGLDSePM1dmEeSYFKgXCcd9iOVLJkbrQ@mail.gmail.com>
	<CANVKczPDz=F6fBNeybaMjVw1Nh4fPe-jEyXRZJkLkmJ0Hy7igw@mail.gmail.com>
Message-ID: <alpine.LFD.2.11.1503232008510.554@reclus.nhh.no>

On Mon, 23 Mar 2015, Barry Rowlingson wrote:

> On Mon, Mar 23, 2015 at 12:07 PM, Mathieu Rajerison
> <mathieu.rajerison at gmail.com> wrote:
>> pts <- spsample(rds.un, type="regular", n=100)
>
> Your points are at least 3.9x10^-17 units from the lines:
>
> gDistance(pts, rds.un)
> [1] 3.908727e-17
>
> I can only think this is due to a difference in how spsample and how
> rgeos interpolate lines from points. FAQ 7.31 in disguise?
>
> I suspect you could use gBuffer on your points with a buffer width of
> 1e-10 to get a reliable intersection, but that would be a line segment
> (or segments if the point is close to a junction) but you could
> possibly just take a single point from that line geometry and in the
> worst case scenario you're only 1e-10 out...
>

Had the fuzzyMM package been written in a modularised way (it places 
vehicle GPS positions on OSM road lines, and needs velocity etc.), it 
might have given a starting point. As Barry says, finding which road is 
closest to each point is trivial (I projected too, to get more digits), 
but finding the nearest GEOS point on the road isn't, because GEOS 
discretises using getScale() to go to an integer grid. spsample() is also 
very pushed to get n here, so the # pts is way under.

If anyone would like a brain teaser, this should be attractive!

Roger

>
>
> Barry
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no


From jimmyjmv at hotmail.com  Tue Mar 24 21:45:37 2015
From: jimmyjmv at hotmail.com (=?iso-8859-1?B?SmltbXkgTmV1dHLzbg==?=)
Date: Tue, 24 Mar 2015 13:45:37 -0700
Subject: [R-sig-Geo] Maximizing likelihood function
Message-ID: <BLU182-W804C506D41240414B2CCC5C40A0@phx.gbl>

Dear comRades:
I'm working with the variogram "mattern". Then, I tried to run this function many times to achieve the best parameters for my geodata function (XXSelg) by means of likfit, then I wrote:
likfit(XXSelg, cov.model="matern", ini.cov.pars=c(4.0, 1.8), kappa=3.5, fix.kappa=FALSE, nugget=0.8, lambda=0.01, fix.lambda=FALSE, hessian=TRUE)
however, many times I received the following answer:
likfit: end of numerical maximisation.Error en optim(par = 1, fn = .negloglik.boxcox, method = "L-BFGS-B", lower = limits$lambda["lower"],  : L-BFGS-B needs finite values of 'fn'
One user mentioned that I have to change the method in optim. But likfit only accept RELM or ML.
Then, how can I change the method in likfit???. I wonder if it possible to do it in likfit, because I don't understand the optim script very well (in R-help for package stats).
Does it mean that I can use any of this method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN"), which are mentioned in optim???
Thanks in advance for youR contribution. 		 	   		  
	[[alternative HTML version deleted]]


From jimmyjmv at hotmail.com  Tue Mar 24 21:49:09 2015
From: jimmyjmv at hotmail.com (=?iso-8859-1?B?SmltbXkgTmV1dHLzbg==?=)
Date: Tue, 24 Mar 2015 13:49:09 -0700
Subject: [R-sig-Geo] Maximizing likelihood function
In-Reply-To: <BLU182-W804C506D41240414B2CCC5C40A0@phx.gbl>
References: <BLU182-W804C506D41240414B2CCC5C40A0@phx.gbl>
Message-ID: <BLU182-W43A443DDCBCB00FB593F7EC40A0@phx.gbl>

Dear comRades:
I'm working with the variogram "mattern". Then, I tried to run this function many times to achieve the best parameters for my geodata function (XXSelg) by means of likfit, then I wrote:
likfit(XXSelg, cov.model="matern", ini.cov.pars=c(4.0, 1.8), kappa=3.5, fix.kappa=FALSE, nugget=0.8, lambda=0.01, fix.lambda=FALSE, hessian=TRUE)
however, many times I received the following answer:
likfit: end of numerical maximisation.Error en optim(par = 1, fn = .negloglik.boxcox, method = "L-BFGS-B", lower = limits$lambda["lower"],  : L-BFGS-B needs finite values of 'fn'
One user mentioned that I have to change the method in optim. But likfit only accept RELM or ML.
Then, how can I change the method in likfit???. I wonder if it possible to do it in likfit, because I don't understand the optim script very well (in R-help for package stats).
Does it mean that I can use any of this method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN"), which are mentioned in optim???
Thanks in advance for youR contribution. 		 	   		   		 	   		  
	[[alternative HTML version deleted]]


From jbaldwin at fs.fed.us  Tue Mar 24 22:24:29 2015
From: jbaldwin at fs.fed.us (Baldwin, Jim -FS)
Date: Tue, 24 Mar 2015 21:24:29 +0000
Subject: [R-sig-Geo] Maximizing likelihood function
In-Reply-To: <BLU182-W804C506D41240414B2CCC5C40A0@phx.gbl>
References: <BLU182-W804C506D41240414B2CCC5C40A0@phx.gbl>
Message-ID: <DDC5EC9B78340042B0D5A0C3789D45691A572A9D@001FSN2MPN1-061.001f.mgd2.msft.net>

It sounds like the function .negloglik.boxcox is being fed some parameters it doesn't like (and possibly it doesn't like the starting values). Possibly attempting to divide by zero.   So I would first check .negloglik.boxcox with the starting values to see if something other than infinity comes out.
 
Jim


-----Original Message-----
From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Jimmy Neutr?n
Sent: Tuesday, March 24, 2015 1:46 PM
To: r-sig-geo at r-project.org
Subject: [R-sig-Geo] Maximizing likelihood function

Dear comRades:
I'm working with the variogram "mattern". Then, I tried to run this function many times to achieve the best parameters for my geodata function (XXSelg) by means of likfit, then I wrote:
likfit(XXSelg, cov.model="matern", ini.cov.pars=c(4.0, 1.8), kappa=3.5, fix.kappa=FALSE, nugget=0.8, lambda=0.01, fix.lambda=FALSE, hessian=TRUE) however, many times I received the following answer:
likfit: end of numerical maximisation.Error en optim(par = 1, fn = .negloglik.boxcox, method = "L-BFGS-B", lower = limits$lambda["lower"],  : L-BFGS-B needs finite values of 'fn'
One user mentioned that I have to change the method in optim. But likfit only accept RELM or ML.
Then, how can I change the method in likfit???. I wonder if it possible to do it in likfit, because I don't understand the optim script very well (in R-help for package stats).
Does it mean that I can use any of this method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN"), which are mentioned in optim???
Thanks in advance for youR contribution. 		 	   		  
	[[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From bgr at bgs.ac.uk  Wed Mar 25 17:32:53 2015
From: bgr at bgs.ac.uk (Rawlins, Barry G.)
Date: Wed, 25 Mar 2015 16:32:53 +0000
Subject: [R-sig-Geo] Problem using trend covariates in predict.ppm (spatstat)
Message-ID: <C1D2C279E163C549802E18252065A44109743B8548@nerckwmbc.ad.nerc.ac.uk>

Dear list

I have been successfully forming spatial point pattern models using the function ppm and a series of covariates stored as im objects:

Example here in which I have a spatial point pattern object "CI_pipe_40_spp" and a covariate "cov_CI_len"

mod2<-ppm(CI_pipe40_spp, trend=~cov_CI_len, covariates=list(cov_CI_len)) # this works fine giving me a ppm model
summary(mod2)

Point process model
Fitting method: maximum likelihood (Berman-Turner approximation)
Model was fitted using glm()
Algorithm converged
Call:
ppm.ppp(Q = CI_pipe40_spp, trend = ~cov_CI_len, covariates = list(cov_CI_len))
Edge correction: "border"
        [border correction distance r = 0 ]

I then want to use the predict function I next write in the same workspace:
preds=predict.ppm(mod2,type="trend", window=mask40,ngrid=c(402,402),
                  covariates=list(cov_CI_len))

But I get the following error:
Error in mpl.get.covariates(covariates, list(x = xpredict, y = ypredict),  :
  Each entry in the list 'covariates' should be an image, a function, a window, a tessellation or a single number

But if I check the class of "cov_CI_len":
class(cov_CI_len)
[1] "im"


Which shows that this object is an image. Can someone suggest what is wrong here? The

The help says "If covariates is a list of images, then the names of the entries should correspond to the names of covariates in the model formula trend."
I think I have the code correct - can anyone help with this?

Best wishes, Barry

Dr Barry Rawlins
Soils Team Leader
British Geological Survey
Nottingham
NG12 5GG
Tel  01159363140
Mob 07884235473

www.soil-journal.net




  ________________________________
This message (and any attachments) is for the recipient ...{{dropped:9}}


From jimmyjmv at hotmail.com  Wed Mar 25 18:24:10 2015
From: jimmyjmv at hotmail.com (=?iso-8859-1?B?SmltbXkgTmV1dHLzbg==?=)
Date: Wed, 25 Mar 2015 10:24:10 -0700
Subject: [R-sig-Geo] Variogram with frecuencies
In-Reply-To: <BLU182-W788993B36B85CCBE7A91CC40B0@phx.gbl>
References: <BLU182-W788993B36B85CCBE7A91CC40B0@phx.gbl>
Message-ID: <BLU182-W6216624B7CDE6719F781BBC40B0@phx.gbl>

Dear R-loveRs:
I normally worked with data from fisheries. I only made variograms with density.
I mean, my geodata was with the vectors: X, Y and Density. All of them were numbers about the format:
 x.xx, y.yy, d.dd ; for instance
  xUTMkm   yUTMkm Density450.6532 1008.625  11.5450.5304 1012.401   0.48450.6268 1011.887   0450.9767 1006.326   4.03451.1903 1003.006  3.98450.9832 1007.675   4.28

However, right now I am trying to make variograms with geodata from occurrence frequencies. Then, my data has the following format:
 x.xx, y.yy, d.dd. For instance:
  xUTMkm   yUTMkm frecuency450.6532 1008.625     11450.5304 1012.401      1450.6268 1011.887      4450.9767 1006.326      3451.1903 1003.006     15450.9832 1007.675      4
It is the first time that I see such a cloud variogram. I also show the directional variograms.
What do you think about this graphs? (mainly, about the cloud variogram) . Is it possible to work with such variograms like the ones that we 'normally' see.I realize that there is a kind of two clouds there. But I have decided to ignore the farthest group (because it is too far from the beginning distance). Did I decided correctly?.
Thanks in advance for youR advices.
Jimmy Martina 		 	   		   		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150325/e6ec43cd/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2008nube.gif
Type: image/gif
Size: 12122 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150325/e6ec43cd/attachment.gif>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2008direccional.gif
Type: image/gif
Size: 10663 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150325/e6ec43cd/attachment-0001.gif>

From jimmyjmv at hotmail.com  Wed Mar 25 20:08:51 2015
From: jimmyjmv at hotmail.com (=?iso-8859-1?B?SmltbXkgTmV1dHLzbg==?=)
Date: Wed, 25 Mar 2015 12:08:51 -0700
Subject: [R-sig-Geo] Likfit: Lambda values (Box-Cox's function)
Message-ID: <BLU182-W728F9D5B0A07997D334B5CC40B0@phx.gbl>

Dear comRades:I'm working with the variogram "mattern". Then, I tried to run this function many times to achieve the best parameters for my geodata function (XXSelg) by means of likfit, then I wrote:likfit(XXSelg, cov.model="matern", ini.cov.pars=c(4.0, 1.8), kappa=3.5, fix.kappa=FALSE, nugget=0.8, lambda=0.01, fix.lambda=FALSE, hessian=TRUE)however, many times I received the following answer:likfit: end of numerical maximisation.Error en optim(par = 1, fn = .negloglik.boxcox, method = "L-BFGS-B", lower = limits$lambda["lower"],  : L-BFGS-B needs finite values of 'fn'
Does 'lower = limits$lambda["lower"]' mean that I need lowed lambda values to achieve the best parameter?. I am even trying with negative values.
Thanks in advance for you help. 		 	   		  
	[[alternative HTML version deleted]]


From paulojus at c3sl.ufpr.br  Wed Mar 25 21:38:29 2015
From: paulojus at c3sl.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Wed, 25 Mar 2015 17:38:29 -0300 (BRT)
Subject: [R-sig-Geo] Likfit: Lambda values (Box-Cox's function)
In-Reply-To: <BLU182-W728F9D5B0A07997D334B5CC40B0@phx.gbl>
References: <BLU182-W728F9D5B0A07997D334B5CC40B0@phx.gbl>
Message-ID: <alpine.DEB.2.10.1503251732570.11590@macalan.c3sl.ufpr.br>

For certain data the model may be hard to identify
and objective function can produce odd values.

try to divide and conquer!

lambda = 0.01 is so close to lambda=0 which defines the log-transformation

Have you tried this value?

I would not try to fitt all parameters (at least at first)

the transformation parameter is ussualy almot orthigonal to the others so 
you could try to find the transformation first (use MASS:::boxcox())
and then run likfit fixint the transformation parameters.

For kappa I would also try to fit some values, 0.5, 1, 1.5 .... 5
first and chack the corresponding likelihoods.
In practice you may not need the fine tune on this parameter.

Paulo Justiniano Ribeiro Jr
LEG (Laboratorio de Estatistica e Geoinformacao)
Universidade Federal do Parana
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 3361 3573
VOIP: (+55) (41) (3361 3600) 1053 1066
Fax: (+55) 41 3361 3141
e-mail: paulojus AT  ufpr  br
http://www.leg.ufpr.br/~paulojus

On Wed, 25 Mar 2015, Jimmy Neutr?n wrote:

> Dear comRades:I'm working with the variogram "mattern". Then, I tried to run this function many times to achieve the best parameters for my geodata function (XXSelg) by means of likfit, then I wrote:likfit(XXSelg, cov.model="matern", ini.cov.pars=c(4.0, 1.8), kappa=3.5, fix.kappa=FALSE, nugget=0.8, lambda=0.01, fix.lambda=FALSE, hessian=TRUE)however, many times I received the following answer:likfit: end of numerical maximisation.Error en optim(par = 1, fn = .negloglik.boxcox, method = "L-BFGS-B", lower = limits$lambda["lower"],  : L-BFGS-B needs finite values of 'fn'
> Does 'lower = limits$lambda["lower"]' mean that I need lowed lambda values to achieve the best parameter?. I am even trying with negative values.
> Thanks in advance for you help.
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

From knussear at mac.com  Wed Mar 25 21:55:52 2015
From: knussear at mac.com (Ken Nussear)
Date: Wed, 25 Mar 2015 13:55:52 -0700
Subject: [R-sig-Geo] trouble installing rgeos package in a cluster
	environment
Message-ID: <551320D8.4020800@mac.com>

Hi all - hope you can provide some help

I need to install the rgeos package in a cluster on a x86_64-suse-linux
platform where I do not have admin access, and am unable to get the
admin to install libraries (e.g. libgeos, and libgeos-devel) in standard
locations. I have compiled and installed them into a usr directory under
my home directory using

./configure --prefix=/afs/ipp-garching.mpg.de/home/k/kenu/usr
make
make install

and

rpm2cpio geos-devel-3.4.2-5.3.x86_64.rpm | cpio -idmv

for the devel package as I could not find the source

When Installing the R package I am getting an error:

install.packages("rgeos", repos="http://cran.us.r-project.org",
configure.args="--with-geos-config=/afs/ipp-garching.mpg.de/home/k/kenu/usr/bin/geos-config
--host=x86_64-suse-linux", lib='RPackages')
....
** testing if installed package can be loaded
Error in dyn.load(file, DLLpath = DLLpath, ...) :
  unable to load shared object
'/afs/ipp-garching.mpg.de/home/k/kenu/RPackages/rgeos/libs/rgeos.so':
  libgeos-3.5.0dev.so: cannot open shared object file: No such file or
directory
Error: loading failed
Execution halted
ERROR: loading failed
* removing '/afs/ipp-garching.mpg.de/home/k/kenu/RPackages/rgeos'

Yet this library is in my path

kenu at woese:~> which libgeos-3.5.0dev.so
/afs/ipp-garching.mpg.de/home/k/kenu/usr/lib/libgeos-3.5.0dev.so

Is there a way to point this to the right place?

Thanks

Ken

	[[alternative HTML version deleted]]


From knussear at mac.com  Wed Mar 25 22:44:34 2015
From: knussear at mac.com (knussear)
Date: Wed, 25 Mar 2015 14:44:34 -0700 (MST)
Subject: [R-sig-Geo] trouble installing rgeos package in a cluster
	environment
In-Reply-To: <551320D8.4020800@mac.com>
References: <551320D8.4020800@mac.com>
Message-ID: <1427319874618-7587953.post@n2.nabble.com>

Solved it!! Needed to set my LD_LIBRARY_PATH to see the alternative libraries

export
LD_LIBRARY_PATH=/usr/local/lib:/afs/ipp-garching.mpg.de/home/k/kenu/usr/lib:/afs/ipp-garching.mpg.de/home/k/kenu/usr/lib64

Working now :)



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/trouble-installing-rgeos-package-in-a-cluster-environment-tp7587952p7587953.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From r.turner at auckland.ac.nz  Wed Mar 25 23:14:54 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 26 Mar 2015 11:14:54 +1300
Subject: [R-sig-Geo] Problem using trend covariates in predict.ppm
	(spatstat)
In-Reply-To: <C1D2C279E163C549802E18252065A44109743B8548@nerckwmbc.ad.nerc.ac.uk>
References: <C1D2C279E163C549802E18252065A44109743B8548@nerckwmbc.ad.nerc.ac.uk>
Message-ID: <5513335E.7040000@auckland.ac.nz>

On 26/03/15 05:32, Rawlins, Barry G. wrote:
> Dear list
>

> I have been successfully forming spatial point pattern models using
the function ppm and a series of covariates stored as im objects:
>
> Example here in which I have a spatial point pattern object
"CI_pipe_40_spp" and a covariate "cov_CI_len"
>
> mod2<-ppm(CI_pipe40_spp, trend=~cov_CI_len,
covariates=list(cov_CI_len)) # this works fine giving me a ppm model
> summary(mod2)
>
> Point process model Fitting method: maximum likelihood (Berman-Turner
> approximation) Model was fitted using glm() Algorithm converged
> Call: ppm.ppp(Q = CI_pipe40_spp, trend = ~cov_CI_len, covariates =
list(cov_CI_len))
> Edge correction: "border" [border correction distance r = 0 ]
>
> I then want to use the predict function I next write in the same
workspace:
> preds=predict.ppm(mod2,type="trend", window=mask40,ngrid=c(402,402),
> covariates=list(cov_CI_len))
>
> But I get the following error: Error in
> mpl.get.covariates(covariates, list(x = xpredict, y =
ypredict), :
> Each entry in the list 'covariates' should be an image, a function,
> a
window, a tessellation or a single number
>
> But if I check the class of "cov_CI_len": class(cov_CI_len) [1] "im"
>
>
> Which shows that this object is an image. Can someone suggest what
> is
wrong here? The
>
> The help says "If covariates is a list of images, then the names of
the entries should correspond to the names of covariates in the model
formula trend."
> I think I have the code correct - can anyone help with this?

I think that the problem is that you do not *name* the covariate in the 
list that you supply to predict.  The error message is less informative 
than it might be, I guess.

I believe that

     preds=predict.ppm(mod2,type="trend", window=mask40,ngrid=c(402,402),
                   covariates=list(cov_CI_len=cov_CI_len))

should work.

BTW, what version of spatstat are you using?  The syntax of your calls 
is unnecessarily cumbersome, and many aspects of it could be simplified 
considerably, particularly with recent versions of spatstat:

     mmm <- ppm(CI_pipe_40_spp ~ cov_CI_len)
     prd <- predict(mmm,window=mask40,ngrid=402)

I.e. there is no longer any need to specify "covariates" (the newer 
usage is "data" rather than "covariates", although the older usage still 
works) if the covariate in question is in your global environment/workspace.

Also there is no need to to type "predict.ppm"; the *generic* predict() 
automatically dispatches to the *method* predict.ppm().

Also there is no need to specify 'type="trend"' for a Poisson model.

Also ngrid=c(402,402) is equivalent to ngrid=402.

HTH

cheers,

Rolf Turner

-- 
Rolf Turner
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276
Home phone: +64-9-480-4619


From alexandresantosbr at yahoo.com.br  Thu Mar 26 04:15:01 2015
From: alexandresantosbr at yahoo.com.br (ASANTOS)
Date: Wed, 25 Mar 2015 23:15:01 -0400
Subject: [R-sig-Geo] Simulation of inhomogeneous point process
Message-ID: <551379B5.4090505@yahoo.com.br>

Dear Members,

- I have ant nests position in UTM in:

#Ant nests position-----------------------
a.x<-c(371349.2,371351.4,371277.1,371200.9,371139.9,371435.7,371344,371263.5,
371105.2,371023.2,371262.4,371273.2,371231.8,370959.5,370961.7,371020.3,371048.4,
371039.9,371086.8,371124.5,371034.7,371040,371025.8,370979.7,370954.3,370966.4,
370950.7,370914.1,370828.4,371294.4,371446.4,371468,371294.5,371276.1,371268.5,
371213.2,371055.2,371073.1,371333.6,371347.7,371242.4,371247,371081.9,371054.7,
370866.6,370885.9,370890,370916.2,370956.4,371125.5,371138.9,371094.8,371048.3,
371015.8,370990.2,370926.1,370915.8,370902.2,370882.6,371531.1)
a.y<-c(8247031,8247030,8247100,8247187,8247085,8246710,8246664,8246813,8247012,
8246970,8246750,8246696,8246644,8246803,8246802,8246748,8246711,8246696,8246624,
8246509,8246508,8246500,8246539,8246537,8246546,8246601,8246629,8246652,8246782,
8247238,8246805,8246742,8246811,8246829,8246836,8246903,8247033,8247014,8246730,
8246660,8246575,8246550,8246806,8246802,8246865,8246828,8246794,8246794,8246754,
8246474,8246452,8246460,8246533,8246615,8246615,8246661,8246692,8246710,8246738,
8246867)
ant.pos<-cbind(a.x,a.y)


- This nest are distributed in a stand area:

#Stand contour ----------------------------------
b.x<-c(371299.9,371266.4,371205.6,371111.8,371047.6,371018.2,371014,371009.3,
370983.1,370919.7,370853.6,370785.6,370748.8,370711.8,370687.8,370696.4,370785.9,
370885.5,371035.8,371148.1,371205.2,371231.7,371236.5,371240.3,371285.8,371326.5,
371397.2,371417.1,371432.9,371445,371455.7,371466.4,371476.6,371502.6,371536,371550,
371546.8,371528.3,371470,371393.3,371299.9)
b.y<-c(8246589,8246560,8246508,8246428,8246373,8246349,8246348,8246352,8246385,
8246465,8246551,8246638,8246685,8246732,8246764,8246771,8246846,8246932,8247062,
8247160,8247209,8247230,8247224,8247221,8247160,8247107,8247016,8246991,8246967,
8246939,8246914,8246892,8246875,8246846,8246821,8246809,8246802,8246785,8246735,
8246669,8246589)
bord<-cbind(b.x,b.y)


- I suspect that this ant nests have inhomogeneous Poisson distribution, 
than I make a CSR for inhomogeneous Poisson process using spastat package:

require(spatstat)
require(gpclib)
require(rgeos)
#Spatstat object 
---------------------------------------------------------------------
a <- as(bord, "gpc.poly")
w <- as.owin(a)
formi05.ppp<-ppp(x=coordinates(ant.pos)[,1],y=coordinates(ant.pos)[,2],window=w)
plot(formi05.ppp,main="")

## CSR inhomogeneous 
test-----------------------------------------------------
env.formi<-envelope(formi05.ppp,nsim=999,fun=Kinhom)
par(mfrow=c(1,2))
plot(env.formi,lwd=list(3,1,1,1),xlim=c(0,200), main="Stand 4")
#


- Ok, I have a  inhomogeneous Poisson process. If my process is 
homogeneous Poisson is very simple generate a random point pattern for 
ant nests using rpoispp() function, but now based in my results, what is 
the better approach for find in observed ant nests position the 
retention probability for lambda(x,y)/lmax, when my purposes is create a 
simulation of inhomogeneous point process?


Thanks,

Alexandre

-- 
======================================================================
Alexandre dos Santos
Prote??o Florestal
IFMT - Instituto Federal de Educa??o, Ci?ncia e Tecnologia de Mato Grosso
Campus C?ceres
Caixa Postal 244
Avenida dos Ramires, s/n
Bairro: Distrito Industrial
C?ceres - MT                      CEP: 78.200-000
Fone: (+55) 65 8132-8112 (TIM)   (+55) 65 9686-6970 (VIVO)

         alexandre.santos at cas.ifmt.edu.br
Lattes: http://lattes.cnpq.br/1360403201088680
======================================================================


---
Este email foi escaneado pelo Avast antiv?rus.
http://www.avast.com


From r.turner at auckland.ac.nz  Thu Mar 26 06:50:28 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 26 Mar 2015 18:50:28 +1300
Subject: [R-sig-Geo] Simulation of inhomogeneous point process
In-Reply-To: <551379B5.4090505@yahoo.com.br>
References: <551379B5.4090505@yahoo.com.br>
Message-ID: <55139E24.1040307@auckland.ac.nz>

On 26/03/15 16:15, ASANTOS wrote:
> Dear Members,
>
> - I have ant nests position in UTM in:
>
> #Ant nests position-----------------------
> a.x<-c(371349.2,371351.4,371277.1,371200.9,371139.9,371435.7,371344,371263.5,
>
> 371105.2,371023.2,371262.4,371273.2,371231.8,370959.5,370961.7,371020.3,371048.4,
>
> 371039.9,371086.8,371124.5,371034.7,371040,371025.8,370979.7,370954.3,370966.4,
>
> 370950.7,370914.1,370828.4,371294.4,371446.4,371468,371294.5,371276.1,371268.5,
>
> 371213.2,371055.2,371073.1,371333.6,371347.7,371242.4,371247,371081.9,371054.7,
>
> 370866.6,370885.9,370890,370916.2,370956.4,371125.5,371138.9,371094.8,371048.3,
>
> 371015.8,370990.2,370926.1,370915.8,370902.2,370882.6,371531.1)
> a.y<-c(8247031,8247030,8247100,8247187,8247085,8246710,8246664,8246813,8247012,
>
> 8246970,8246750,8246696,8246644,8246803,8246802,8246748,8246711,8246696,8246624,
>
> 8246509,8246508,8246500,8246539,8246537,8246546,8246601,8246629,8246652,8246782,
>
> 8247238,8246805,8246742,8246811,8246829,8246836,8246903,8247033,8247014,8246730,
>
> 8246660,8246575,8246550,8246806,8246802,8246865,8246828,8246794,8246794,8246754,
>
> 8246474,8246452,8246460,8246533,8246615,8246615,8246661,8246692,8246710,8246738,
>
> 8246867)
> ant.pos<-cbind(a.x,a.y)
>
>
> - This nest are distributed in a stand area:
>
> #Stand contour ----------------------------------
> b.x<-c(371299.9,371266.4,371205.6,371111.8,371047.6,371018.2,371014,371009.3,
>
> 370983.1,370919.7,370853.6,370785.6,370748.8,370711.8,370687.8,370696.4,370785.9,
>
> 370885.5,371035.8,371148.1,371205.2,371231.7,371236.5,371240.3,371285.8,371326.5,
>
> 371397.2,371417.1,371432.9,371445,371455.7,371466.4,371476.6,371502.6,371536,371550,
>
> 371546.8,371528.3,371470,371393.3,371299.9)
> b.y<-c(8246589,8246560,8246508,8246428,8246373,8246349,8246348,8246352,8246385,
>
> 8246465,8246551,8246638,8246685,8246732,8246764,8246771,8246846,8246932,8247062,
>
> 8247160,8247209,8247230,8247224,8247221,8247160,8247107,8247016,8246991,8246967,
>
> 8246939,8246914,8246892,8246875,8246846,8246821,8246809,8246802,8246785,8246735,
>
> 8246669,8246589)
> bord<-cbind(b.x,b.y)
>
>
> - I suspect that this ant nests have inhomogeneous Poisson distribution,
> than I make a CSR for inhomogeneous Poisson process using spastat package:
>
> require(spatstat)
> require(gpclib)
> require(rgeos)
> #Spatstat object
> ---------------------------------------------------------------------
> a <- as(bord, "gpc.poly")
> w <- as.owin(a)
> formi05.ppp<-ppp(x=coordinates(ant.pos)[,1],y=coordinates(ant.pos)[,2],window=w)
>
> plot(formi05.ppp,main="")
>
> ## CSR inhomogeneous
> test-----------------------------------------------------
> env.formi<-envelope(formi05.ppp,nsim=999,fun=Kinhom)
> par(mfrow=c(1,2))
> plot(env.formi,lwd=list(3,1,1,1),xlim=c(0,200), main="Stand 4")
> #
>
>
> - Ok, I have a  inhomogeneous Poisson process. If my process is
> homogeneous Poisson is very simple generate a random point pattern for
> ant nests using rpoispp() function, but now based in my results, what is
> the better approach for find in observed ant nests position the
> retention probability for lambda(x,y)/lmax, when my purposes is create a
> simulation of inhomogeneous point process?

Considerable confusion is indicated by your code and your question.
Note that there is no need for rgeos or gpclib.

(1) You can create your point pattern much more simply by:

W <- owin(poly=list(x=rev(b.x),y=rev(b.y))
X <- ppp(a.x,a.y,window=W)

(2) But ... there is a warning indicating that two points lie outside 
the window; some investigation indicates that these are points 30 and 
60.  You should make sure you understand why these points are present in 
the data.  Something is wrong; it would appear that your specified 
observation window was *not* the actual observation window.

(3) Try again:
a.x <- a.x[-c(30,60)]
a.y <- a.y[-c(30,60)]
X   <- ppp(a.x,a.y,window=W)

(4) Now is there evidence that the pattern is inhomogeneous?  You 
*don't* start with Kinhom()!!! That is for analysing a pattern when you 
have concluded that the pattern is inhomogeneous and you want to decide 
whether there is any interaction between the points.

Testing for inhomogeneity is not all that easy, especially when/if you 
have no particular alternative hypothesis in mind.  The function 
quadrat.test() can be applied, although this test is not particularly 
powerful:

quadrat.test(X,method="MonteCarlo") # The default method produces a
                                     # warning about expected counts
                                     # being too small.

I got a p-value of 0.121 --- no real evidence against a homogeneous 
Poisson process.

Now investigate dependence on the Cartesian coordinates:

cdf.test(X,"x")

gives a p-value of 0.3214, but

cdf.test(X,"y")

gives a p-value of 0.01545 --- here is a *bit* of evidence against 
constant intensity.  However if we try

fit0 <- ppm(X ~ 1)
fit1 <- ppm(X ~ y)
anova(fit0,fit1,test="Chi")

we only get a p-value of 0.05472 --- so "not quite significant" by this 
test.

Finally we might do

E <- envelope(X,savefuns=TRUE)
dclf.test(E)

which gave me p-value of 0.15.  This test is not really appropriate for 
inhomogeneity.  It is essentially testing for the presence of 
interaction *assuming* homogeneity.  However the fact that the null
hypothesis is not rejected says that the data are *consistent* with the 
assumption of a homogeneous Poisson process.

Thus the evidence against a homogeneous Poisson process is meagre at best.

(5) I am not really sure what you are asking in your question at the end 
of the post.  I *think* that you just want to simulate inhomogeneous 
Poisson patterns that "imitate" the behaviour of your observed pattern. 
  In order for that to make much sense you really
should have some model in mind for the intensity of your observed pattern.

One way to proceed without such a model would be to apply smoothing 
methods to get a non-parametric estimate of the intensity of your 
observed pattern:

est.int <- density(X)
Y <- rpoispp(est.int)

This produces a simulated realisation of an inhomogeneous Poisson 
process with intensity equal to the non-parametric estimate of the 
intensity of X.

Another way would be to use the model fitted in terms of the Cartesian
coordinate "y" --- given that there was a little evidence that this
coordinate has an influence on the intensity of X.

Y <- rmh(fit1)

Whether either of these two approaches makes any sense at all depends on 
what you are trying to accomplish --- which is unclear.

cheers,

Rolf Turner

P. S.  Don't send posts like this to r-sig-geo-request at r-project.org; 
that address is for matters pertaining to the administration of the 
r-sig-geo list, e.g. subscribing or unsubscribing.

R. T.

-- 
Rolf Turner
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276
Home phone: +64-9-480-4619


From bgr at bgs.ac.uk  Thu Mar 26 10:34:47 2015
From: bgr at bgs.ac.uk (Rawlins, Barry G.)
Date: Thu, 26 Mar 2015 09:34:47 +0000
Subject: [R-sig-Geo] Problem using trend covariates in predict.ppm
 (spatstat)
In-Reply-To: <5513335E.7040000@auckland.ac.nz>
References: <C1D2C279E163C549802E18252065A44109743B8548@nerckwmbc.ad.nerc.ac.uk>
	<5513335E.7040000@auckland.ac.nz>
Message-ID: <C1D2C279E163C549802E18252065A44109743B85F1@nerckwmbc.ad.nerc.ac.uk>

Dear Rolf

Many thanks for this - your amended code worked fine.

I am using spatstat version 1.41-1 which I believe is the latest.

I suspect you are correct that my syntax is a bit cumbersome. I had been reading your spatstat text "Modelling Spatial Point Patterns in R" which was the most comprehensive description I could find of the package. Maybe this was written a while ago (?) and I think I was using some of the syntax from that which might account for my approach, but I may be wrong here.

I sometimes like to use the full reference (e.g. predict.ppm) to distinguish it from the other calls to predict which I often use in lm or functions in nlme.

Thanks again.
Best wishes, Barry


-----Original Message-----
From: Rolf Turner [mailto:r.turner at auckland.ac.nz]
Sent: 25 March 2015 22:15
To: Rawlins, Barry G.; r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Problem using trend covariates in predict.ppm (spatstat)

On 26/03/15 05:32, Rawlins, Barry G. wrote:
> Dear list
>

> I have been successfully forming spatial point pattern models using
the function ppm and a series of covariates stored as im objects:
>
> Example here in which I have a spatial point pattern object
"CI_pipe_40_spp" and a covariate "cov_CI_len"
>
> mod2<-ppm(CI_pipe40_spp, trend=~cov_CI_len,
covariates=list(cov_CI_len)) # this works fine giving me a ppm model
> summary(mod2)
>
> Point process model Fitting method: maximum likelihood (Berman-Turner
> approximation) Model was fitted using glm() Algorithm converged
> Call: ppm.ppp(Q = CI_pipe40_spp, trend = ~cov_CI_len, covariates =
list(cov_CI_len))
> Edge correction: "border" [border correction distance r = 0 ]
>
> I then want to use the predict function I next write in the same
workspace:
> preds=predict.ppm(mod2,type="trend", window=mask40,ngrid=c(402,402),
> covariates=list(cov_CI_len))
>
> But I get the following error: Error in mpl.get.covariates(covariates,
> list(x = xpredict, y =
ypredict), :
> Each entry in the list 'covariates' should be an image, a function, a
window, a tessellation or a single number
>
> But if I check the class of "cov_CI_len": class(cov_CI_len) [1] "im"
>
>
> Which shows that this object is an image. Can someone suggest what
> is
wrong here? The
>
> The help says "If covariates is a list of images, then the names of
the entries should correspond to the names of covariates in the model
formula trend."
> I think I have the code correct - can anyone help with this?

I think that the problem is that you do not *name* the covariate in the
list that you supply to predict.  The error message is less informative
than it might be, I guess.

I believe that

     preds=predict.ppm(mod2,type="trend", window=mask40,ngrid=c(402,402),
                   covariates=list(cov_CI_len=cov_CI_len))

should work.

BTW, what version of spatstat are you using?  The syntax of your calls
is unnecessarily cumbersome, and many aspects of it could be simplified
considerably, particularly with recent versions of spatstat:

     mmm <- ppm(CI_pipe_40_spp ~ cov_CI_len)
     prd <- predict(mmm,window=mask40,ngrid=402)

I.e. there is no longer any need to specify "covariates" (the newer
usage is "data" rather than "covariates", although the older usage still
works) if the covariate in question is in your global environment/workspace.

Also there is no need to to type "predict.ppm"; the *generic* predict()
automatically dispatches to the *method* predict.ppm().

Also there is no need to specify 'type="trend"' for a Poisson model.

Also ngrid=c(402,402) is equivalent to ngrid=402.

HTH

cheers,

Rolf Turner

--
Rolf Turner
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276
Home phone: +64-9-480-4619

This message (and any attachments) is for the recipient ...{{dropped:6}}


From wolfgang.biener at ise.fraunhofer.de  Thu Mar 26 12:23:57 2015
From: wolfgang.biener at ise.fraunhofer.de (Wolfgang)
Date: Thu, 26 Mar 2015 04:23:57 -0700 (MST)
Subject: [R-sig-Geo] dividing a line by points lying on it
Message-ID: <1427369037585-7587958.post@n2.nabble.com>

Hi there, 

I want to divide a line into n+1 lines. The shall be divided by points lying
on it. 
Since I don't have any idea how to solve can also post the starting point of
the problem. 

Thanks for your Help 


library(sp)
Sl = SpatialLines(list(Lines(list(Line(cbind(c(1,2,2),c(1,1,2)))),
                             ID="a")))

set.seed(123)
sample.points <- spsample(Sl, 10, type="regular")

x11()
plot(Sl)
plot(sample.points, pch=1, col="red", add=T)



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/dividing-a-line-by-points-lying-on-it-tp7587958.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From wolfgang.biener at ise.fraunhofer.de  Thu Mar 26 12:24:49 2015
From: wolfgang.biener at ise.fraunhofer.de (Wolfgang Biener)
Date: Thu, 26 Mar 2015 12:24:49 +0100
Subject: [R-sig-Geo] dividing a line by points lying on it
Message-ID: <5513EC81.9050200@ise.fraunhofer.de>

Hi there,

I want to divide a line into n+1 lines. The shall be divided by points 
lying on it.
Since I don't have any idea how to solve can also post the starting 
point of the problem.

Thanks for your Help


library(sp)
Sl = SpatialLines(list(Lines(list(Line(cbind(c(1,2,2),c(1,1,2)))),
                              ID="a")))

set.seed(123)
sample.points <- spsample(Sl, 10, type="regular")

x11()
plot(Sl)
plot(sample.points, pch=1, col="red", add=T)

-- 
Wolfgang Biener,
Division Electrical Energy Systems
Fraunhofer-Institut f?r Solare Energiesysteme ISE
Heidenhofstrasse 2, 79110 Freiburg, Germany

Phone: +49 (0) 7 61/ 45 88-5893
wolfgang.biener at ise.fraunhofer.de
http://www.ise.fraunhofer.de


From sara_erasmus at yahoo.es  Thu Mar 26 16:58:21 2015
From: sara_erasmus at yahoo.es (Sara Rivero Calle)
Date: Thu, 26 Mar 2015 11:58:21 -0400
Subject: [R-sig-Geo] geom_points adjusting size for multiplot
Message-ID: <E76C8601-5173-4BDF-A0DB-DFB28D5E1D0A@yahoo.es>

Hi, 
 I am using ggplot2 and geometric points. I am trying to create a plot with 3 different panels, one for each time period, I want the size of the points to be a function of the value in column ?CB" and since there is a lot of overlapping I used the alpha=CB so that its slightly transparent. CB goes from 0-420000. I used a subset of the data frame for each time period and the problem is that each time period has a different maximum (subset 1 ranges from 200 000, subset 2 has a max of 420 000 and subset 3 about 60 000. What I have so far works but I want to have all 3 of them sharing the same legend and the size of the points be consistent for all 3 plots. How do I modify the breaks into 20 000, 50 000, 100 000, 200 000, 400 000 ?


library(maps)
library(ggplot2)
library(data frame)
library(spatstat)
library(sp)
library(map tools)

wrld <- map_data("world")
CPR<-read.csv("matchupP605.csv",header=TRUE)


subset1<-CPR[which(CPR$YearCollected>=1960 & CPR$YearCollected<1980& CPR$CB>0),]
 subset2 <-CPR[which(CPR$YearCollected>=1980 & CPR$YearCollected<2000& CPR$CB>0),]
 subset3 <-CPR[which(CPR$YearCollected>=2000 & CPR$YearCollected<2011& CPR$CB>0),]

source("http://peterhaschke.com/Code/multiplot.R") ## for the multiplot function.

	p6<-ggplot(subset1, aes(x = Longitude, y = Latitude))+ geom_point(aes(size=CB, alpha=CB), colour="red")+ coord_equal() + geom_path(data=wrld, aes(x=long, y=lat, group = group))+ xlim(-80,10) + ylim(30,65) 	+ theme_bw(base_size = 12, base_family = "Helvetica")	# alpha modifies transparency

	p7<-ggplot(subset2, aes(x = Longitude, y = Latitude))+ geom_point(aes(size=CB, alpha=CB), colour="red")+ coord_equal() + geom_path(data=wrld, aes(x=long, y=lat, group = group))+ xlim(-80,10) + ylim(30,65) + theme_bw(base_size = 12, base_family = "Helvetica")	

	p8<-ggplot(subset3, aes(x = Longitude, y = Latitude))+ geom_point(aes(size=CB, alpha=CB), colour="red")+ coord_equal() + geom_path(data=wrld, aes(x=long, y=lat, group = group))+ xlim(-80,10) + ylim(30,65)+ theme_bw(base_size = 12, base_family = "Helvetica")

multiplot(p6, p7, p8)

Thanks in advance!

From selebatsom at yahoo.co.uk  Fri Mar 27 08:44:28 2015
From: selebatsom at yahoo.co.uk (moses selebatso)
Date: Fri, 27 Mar 2015 07:44:28 +0000 (UTC)
Subject: [R-sig-Geo] Help with latlong to UTM conversion when UTM zones are
	different
Message-ID: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>

?Hello
I have animal movement data that I have converted from Lat/Long to UTM, unfortunately the data falls in two UTM zones (34S and 35S). For some reason R cannot display both of them in the same window (the 35S data is way off the expected location).
The question is how do I convert the data such that R can correctly read it?
Moses SELEBATSO

(+267) 318 5219 (H) (+267) 716 393 70 (C)
 ?(+267) 738 393 70 (C
	[[alternative HTML version deleted]]


From b.rowlingson at lancaster.ac.uk  Fri Mar 27 11:28:07 2015
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 27 Mar 2015 10:28:07 +0000
Subject: [R-sig-Geo] Help with latlong to UTM conversion when UTM zones
 are different
In-Reply-To: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>
References: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CANVKczPJ72GAZnVKyav-jQN4WUU86FAoxOrxV61WYHV+QjNkig@mail.gmail.com>

If you have lat-long data that crosses two UTM zones then its
generally okay to just pick *one* and transform all the points to
that. Use the one that has the most points in. Basically use the UTM
zones as guidelines to pick one UTM zone coordinate system. Unless
your data spans several zones and you want quite high accuracy of
distance measurements. Some points bleeding over into an adjacent zone
are no problem.

All projections are approximations to the earth's spheroid, so points
that are within a single UTM zone have some distortion in their
distance or angle relationships. Transforming points that are within
an adjacent UTM zone is just an extension of that distortion. You can
compute the precise distance error if you want for the furthest points
by comparing with the geodesic distance.

Alternatively you might find there is a coordinate system that spans
your dataset nicely - often when a country or an island or a region
crosses UTM zones there is an official coordinate system defined that
is used by the authorities there.

Also alternatively, there's nothing to stop you defining a transverse
mercator system based on the centre of your data.

Barry



On Fri, Mar 27, 2015 at 7:44 AM, moses selebatso <selebatsom at yahoo.co.uk> wrote:
>  Hello
> I have animal movement data that I have converted from Lat/Long to UTM, unfortunately the data falls in two UTM zones (34S and 35S). For some reason R cannot display both of them in the same window (the 35S data is way off the expected location).
> The question is how do I convert the data such that R can correctly read it?
> Moses SELEBATSO
>
> (+267) 318 5219 (H) (+267) 716 393 70 (C)
>   (+267) 738 393 70 (C
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From matthias.m.hinz at gmail.com  Fri Mar 27 12:49:13 2015
From: matthias.m.hinz at gmail.com (Matthias Hinz)
Date: Fri, 27 Mar 2015 12:49:13 +0100
Subject: [R-sig-Geo] dividing a line by points lying on it
In-Reply-To: <5513EC81.9050200@ise.fraunhofer.de>
References: <5513EC81.9050200@ise.fraunhofer.de>
Message-ID: <551543B9.7010704@gmail.com>

Hello,

it would have been been easier if you included the link to the related 
thread from 2013, instead of letting me figure out myself ;-)

http://r-sig-geo.2731867.n2.nabble.com/split-divide-SpatialLines-sp-into-n-segments-td7583234.html

So far, my first idea is the following iterative solution:

 1. For the first, figure out on which line segment, i.e. straight line
    it is located (iterate pairwise over all coordinates of the linestring)
      * If you find the intersection, split the SpatialLine at this point
      * Finding a point on a straight line might be implemented in some
        R-function, but actually is elementary mathematics. Mind
        rounding errors during floating point operations.

     2 ...n Repeat for the remaining points, but test against the 
previously splitted SpatialLines, i.e. the     outcome of the preceding step

Regards,
Matthias Hinz

Am 26.03.2015 um 12:24 schrieb Wolfgang Biener:
> Hi there,
>
> I want to divide a line into n+1 lines. The shall be divided by points 
> lying on it.
> Since I don't have any idea how to solve can also post the starting 
> point of the problem.
>
> Thanks for your Help
>
>
> library(sp)
> Sl = SpatialLines(list(Lines(list(Line(cbind(c(1,2,2),c(1,1,2)))),
>                              ID="a")))
>
> set.seed(123)
> sample.points <- spsample(Sl, 10, type="regular")
>
> x11()
> plot(Sl)
> plot(sample.points, pch=1, col="red", add=T)
>


	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Fri Mar 27 18:48:56 2015
From: mdsumner at gmail.com (Michael Sumner)
Date: Fri, 27 Mar 2015 17:48:56 +0000
Subject: [R-sig-Geo] Help with latlong to UTM conversion when UTM zones
 are different
In-Reply-To: <CANVKczPJ72GAZnVKyav-jQN4WUU86FAoxOrxV61WYHV+QjNkig@mail.gmail.com>
References: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>
	<CANVKczPJ72GAZnVKyav-jQN4WUU86FAoxOrxV61WYHV+QjNkig@mail.gmail.com>
Message-ID: <CAAcGz9__G+zfjy_Cp_Jnz0WeJz98DM2XS_7HaRxrjGu9XZkk_g@mail.gmail.com>

There is no good natural reason to use UTM, it mistifies me why our
community tolerates this bizarre default. I always use a local equal-area
projection unless some other compromise dictates a different choice.
Cheers, Mike

On Fri, 27 Mar 2015 21:28 Barry Rowlingson <b.rowlingson at lancaster.ac.uk>
wrote:

> If you have lat-long data that crosses two UTM zones then its
> generally okay to just pick *one* and transform all the points to
> that. Use the one that has the most points in. Basically use the UTM
> zones as guidelines to pick one UTM zone coordinate system. Unless
> your data spans several zones and you want quite high accuracy of
> distance measurements. Some points bleeding over into an adjacent zone
> are no problem.
>
> All projections are approximations to the earth's spheroid, so points
> that are within a single UTM zone have some distortion in their
> distance or angle relationships. Transforming points that are within
> an adjacent UTM zone is just an extension of that distortion. You can
> compute the precise distance error if you want for the furthest points
> by comparing with the geodesic distance.
>
> Alternatively you might find there is a coordinate system that spans
> your dataset nicely - often when a country or an island or a region
> crosses UTM zones there is an official coordinate system defined that
> is used by the authorities there.
>
> Also alternatively, there's nothing to stop you defining a transverse
> mercator system based on the centre of your data.
>
> Barry
>
>
>
> On Fri, Mar 27, 2015 at 7:44 AM, moses selebatso <selebatsom at yahoo.co.uk>
> wrote:
> >  Hello
> > I have animal movement data that I have converted from Lat/Long to UTM,
> unfortunately the data falls in two UTM zones (34S and 35S). For some
> reason R cannot display both of them in the same window (the 35S data is
> way off the expected location).
> > The question is how do I convert the data such that R can correctly read
> it?
> > Moses SELEBATSO
> >
> > (+267) 318 5219 (H) (+267) 716 393 70 (C)
> >   (+267) 738 393 70 (C
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From btupper at bigelow.org  Fri Mar 27 21:20:31 2015
From: btupper at bigelow.org (Ben Tupper)
Date: Fri, 27 Mar 2015 16:20:31 -0400
Subject: [R-sig-Geo] Pole centric map with Raster?
Message-ID: <0DB9402E-968E-4CE8-9448-373184FCEF13@bigelow.org>

Hello,

I'm learning how to use Raster* objects and ultimately hope to draw rasters from a (north) polar perspective.  I can do something like this using tiling in ggplot2:

https://dl.dropboxusercontent.com/u/8433654/polar-map-with-matrix.png


I have hopes of doing similar using raster, sp and lattice.  But I'm drowning in information as neophytes are prone to do.  Below is a self-contained example...  I'm not getting too far.  It creates two plots:

raster without using projectRaster: https://dl.dropboxusercontent.com/u/8433654/raster_no_project.png

raster using projectRaster: https://dl.dropboxusercontent.com/u/8433654/raster_with_project.png

Neither is what I would like.  How does one get the map centered on the pole and draw a (warped) raster using spplot?

Thanks!
Ben


##### START
library(raster)
library(maps)
library(maptools)
library(rgdal)

# create a Raster from 'volcano'
my_proj <- "+proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0" 
r <- raster(volcano,
   xmn = -90, xmx = 0, ymn = 50, ymx = 85, 
   crs = my_proj)
# class       : RasterLayer 
# dimensions  : 87, 61, 5307  (nrow, ncol, ncell)
# resolution  : 1.47541, 0.4022989  (x, y)
# extent      : -90, 0, 50, 85  (xmin, xmax, ymin, ymax)
# coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84 
# data source : in memory
# names       : layer 
# values      : 94, 195  (min, max)   

pr <- projectRaster(r, crs=my_proj)
# class       : RasterLayer 
# dimensions  : 123, 71, 8733  (nrow, ncol, ncell)
# resolution  : 1.48, 0.402  (x, y)
# extent      : -97.4, 7.68, 42.79, 92.236  (xmin, xmax, ymin, ymax)
# coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84 
# data source : in memory
# names       : layer 
# values      : 93.88372, 194.8761  (min, max)

# create a coastline and transform to the projection
mp <- map2SpatialLines(map(plot = FALSE, interior = FALSE), proj4string = CRS(my_proj))
mp <- spTransform(mp, CRSobj = CRS(my_proj))
# class       : SpatialLines 
# features    : 2904 
# extent      : -179.9572, 190.2908, -85.44308, 83.57391  (xmin, xmax, ymin, ymax)
# coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84 

# show the map with the 'unprojected' raster
plot_r <- spplot(r,
   sp.layout = list( sp.lines, mp, first = FALSE)
)

# show the map with the projected raster
plot_pr <- spplot(pr,
   sp.layout = list( sp.lines, mp, first = FALSE)   
)

print(plot_r)
print(plot_pr)
##### END



> sessionInfo()
R version 3.1.2 (2014-10-31)
Platform: x86_64-apple-darwin13.4.0 (64-bit)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] rgdal_0.9-2     maptools_0.8-34 maps_2.3-9      raster_2.3-33   sp_1.0-17      

loaded via a namespace (and not attached):
[1] foreign_0.8-63  grid_3.1.2      lattice_0.20-30 tools_3.1.2 


Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From andrewaduff at gmail.com  Fri Mar 27 21:30:16 2015
From: andrewaduff at gmail.com (Andrew Duff)
Date: Fri, 27 Mar 2015 13:30:16 -0700
Subject: [R-sig-Geo] Help with latlong to UTM conversion when UTM zones
	are different
In-Reply-To: <CAAcGz9__G+zfjy_Cp_Jnz0WeJz98DM2XS_7HaRxrjGu9XZkk_g@mail.gmail.com>
References: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>
	<CANVKczPJ72GAZnVKyav-jQN4WUU86FAoxOrxV61WYHV+QjNkig@mail.gmail.com>
	<CAAcGz9__G+zfjy_Cp_Jnz0WeJz98DM2XS_7HaRxrjGu9XZkk_g@mail.gmail.com>
Message-ID: <0122DA0E-814E-443A-A069-EC51908F5D7F@gmail.com>

A number of field folks prefer UTM because 

-it matches legacy paper USGS quad map series traditionally used for field navigation
-units are in meters and can be used to gauge field distances from a coordinate readout 



> On Mar 27, 2015, at 10:48 AM, Michael Sumner <mdsumner at gmail.com> wrote:
> 
> There is no good natural reason to use UTM, it mistifies me why our
> community tolerates this bizarre default. I always use a local equal-area
> projection unless some other compromise dictates a different choice.
> Cheers, Mike
> 
> On Fri, 27 Mar 2015 21:28 Barry Rowlingson <b.rowlingson at lancaster.ac.uk>
> wrote:
> 
>> If you have lat-long data that crosses two UTM zones then its
>> generally okay to just pick *one* and transform all the points to
>> that. Use the one that has the most points in. Basically use the UTM
>> zones as guidelines to pick one UTM zone coordinate system. Unless
>> your data spans several zones and you want quite high accuracy of
>> distance measurements. Some points bleeding over into an adjacent zone
>> are no problem.
>> 
>> All projections are approximations to the earth's spheroid, so points
>> that are within a single UTM zone have some distortion in their
>> distance or angle relationships. Transforming points that are within
>> an adjacent UTM zone is just an extension of that distortion. You can
>> compute the precise distance error if you want for the furthest points
>> by comparing with the geodesic distance.
>> 
>> Alternatively you might find there is a coordinate system that spans
>> your dataset nicely - often when a country or an island or a region
>> crosses UTM zones there is an official coordinate system defined that
>> is used by the authorities there.
>> 
>> Also alternatively, there's nothing to stop you defining a transverse
>> mercator system based on the centre of your data.
>> 
>> Barry
>> 
>> 
>> 
>> On Fri, Mar 27, 2015 at 7:44 AM, moses selebatso <selebatsom at yahoo.co.uk>
>> wrote:
>>> Hello
>>> I have animal movement data that I have converted from Lat/Long to UTM,
>> unfortunately the data falls in two UTM zones (34S and 35S). For some
>> reason R cannot display both of them in the same window (the 35S data is
>> way off the expected location).
>>> The question is how do I convert the data such that R can correctly read
>> it?
>>> Moses SELEBATSO
>>> 
>>> (+267) 318 5219 (H) (+267) 716 393 70 (C)
>>>  (+267) 738 393 70 (C
>>>        [[alternative HTML version deleted]]
>>> 
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> 
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 
>    [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From tim.appelhans at gmail.com  Fri Mar 27 22:14:34 2015
From: tim.appelhans at gmail.com (Tim Appelhans)
Date: Fri, 27 Mar 2015 22:14:34 +0100
Subject: [R-sig-Geo] Pole centric map with Raster?
In-Reply-To: <0DB9402E-968E-4CE8-9448-373184FCEF13@bigelow.org>
References: <0DB9402E-968E-4CE8-9448-373184FCEF13@bigelow.org>
Message-ID: <5515C83A.8000404@gmail.com>

Ben
here's some code I used for a paper.

Is this producing what you want?



library("remote")

###### EXAMPLE I ##########################################################
###########################################################################
library("rworldmap")
library("rgdal")
library("rgeos")
library("gridExtra")
library("RColorBrewer")

## load data
data("vdendool")
data("coastsCoarse")

## calculate 4 leading modes
nh_modes <- eot(x = vdendool, y = NULL, n = 4, reduce.both = FALSE,
                 standardised = FALSE, verbose = TRUE)

## create coastal outlines
ster <- CRS("+proj=stere +lat_0=90 +lon_0=-45")
xmin <- -180
xmax <- 180
ymin <- 20
ymax <- 90     # Coordinates for bounding box
bb <- cbind(x = c(xmin, xmin, xmax, xmax, xmin),
             y = c(ymin, ymax, ymax, ymin, ymin))    #Create bounding box
SP <- SpatialPolygons(list(Polygons(list(Polygon(bb)), "1")),
                       proj4string = CRS(proj4string(coastsCoarse)))

gI <- gIntersects(coastsCoarse, SP, byid = TRUE)
out <- vector(mode = "list", length = length(which(gI)))
ii <- 1

for (i in seq(along = gI)) if (gI[i]) {
   out[[ii]] <- gIntersection(coastsCoarse[i, ], SP)
   row.names(out[[ii]]) <- row.names(coastsCoarse)[i]
   ii <- ii + 1
}

nh_coasts <- do.call("rbind", out)
nh_coasts_ster <- spTransform(nh_coasts, ster)

lout <- list("sp.lines", nh_coasts_ster,
              col = "grey30", grid = TRUE)

## define colours
clrs <- colorRampPalette(rev(brewer.pal(9, "RdBu")))

## project modes to polar stereographic CRS
mode1 <- projectRaster(nh_modes[[1]]@r_predictor, crs = ster)

spplot(mode1, sp.layout = lout,
        col.regions = clrs(1000), at = seq(-1, 1, 0.2),
        par.settings = list(axis.line = list(col = 0)),
        colorkey = list(height = 0.75, width = 1))


Tim



On 27.03.2015 21:20, Ben Tupper wrote:
> Hello,
>
> I'm learning how to use Raster* objects and ultimately hope to draw rasters from a (north) polar perspective.  I can do something like this using tiling in ggplot2:
>
> https://dl.dropboxusercontent.com/u/8433654/polar-map-with-matrix.png
>
>
> I have hopes of doing similar using raster, sp and lattice.  But I'm drowning in information as neophytes are prone to do.  Below is a self-contained example...  I'm not getting too far.  It creates two plots:
>
> raster without using projectRaster: https://dl.dropboxusercontent.com/u/8433654/raster_no_project.png
>
> raster using projectRaster: https://dl.dropboxusercontent.com/u/8433654/raster_with_project.png
>
> Neither is what I would like.  How does one get the map centered on the pole and draw a (warped) raster using spplot?
>
> Thanks!
> Ben
>
>
> ##### START
> library(raster)
> library(maps)
> library(maptools)
> library(rgdal)
>
> # create a Raster from 'volcano'
> my_proj <- "+proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0"
> r <- raster(volcano,
>     xmn = -90, xmx = 0, ymn = 50, ymx = 85,
>     crs = my_proj)
> # class       : RasterLayer
> # dimensions  : 87, 61, 5307  (nrow, ncol, ncell)
> # resolution  : 1.47541, 0.4022989  (x, y)
> # extent      : -90, 0, 50, 85  (xmin, xmax, ymin, ymax)
> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
> # data source : in memory
> # names       : layer
> # values      : 94, 195  (min, max)
>
> pr <- projectRaster(r, crs=my_proj)
> # class       : RasterLayer
> # dimensions  : 123, 71, 8733  (nrow, ncol, ncell)
> # resolution  : 1.48, 0.402  (x, y)
> # extent      : -97.4, 7.68, 42.79, 92.236  (xmin, xmax, ymin, ymax)
> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
> # data source : in memory
> # names       : layer
> # values      : 93.88372, 194.8761  (min, max)
>
> # create a coastline and transform to the projection
> mp <- map2SpatialLines(map(plot = FALSE, interior = FALSE), proj4string = CRS(my_proj))
> mp <- spTransform(mp, CRSobj = CRS(my_proj))
> # class       : SpatialLines
> # features    : 2904
> # extent      : -179.9572, 190.2908, -85.44308, 83.57391  (xmin, xmax, ymin, ymax)
> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
>
> # show the map with the 'unprojected' raster
> plot_r <- spplot(r,
>     sp.layout = list( sp.lines, mp, first = FALSE)
> )
>
> # show the map with the projected raster
> plot_pr <- spplot(pr,
>     sp.layout = list( sp.lines, mp, first = FALSE)
> )
>
> print(plot_r)
> print(plot_pr)
> ##### END
>
>
>
>> sessionInfo()
> R version 3.1.2 (2014-10-31)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rgdal_0.9-2     maptools_0.8-34 maps_2.3-9      raster_2.3-33   sp_1.0-17
>
> loaded via a namespace (and not attached):
> [1] foreign_0.8-63  grid_3.1.2      lattice_0.20-30 tools_3.1.2
>
>
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
#####################################
Tim Appelhans
Department of Geography
Environmental Informatics
Philipps Universit?t Marburg
Deutschhausstra?e 12
35032 Marburg (Paketpost: 35037 Marburg)
Germany

Tel +49 (0) 6421 28-25957

http://environmentalinformatics-marburg.de/


From mdsumner at gmail.com  Fri Mar 27 22:28:24 2015
From: mdsumner at gmail.com (Michael Sumner)
Date: Fri, 27 Mar 2015 21:28:24 +0000
Subject: [R-sig-Geo] Help with latlong to UTM conversion when UTM zones
 are different
In-Reply-To: <0122DA0E-814E-443A-A069-EC51908F5D7F@gmail.com>
References: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>
	<CANVKczPJ72GAZnVKyav-jQN4WUU86FAoxOrxV61WYHV+QjNkig@mail.gmail.com>
	<CAAcGz9__G+zfjy_Cp_Jnz0WeJz98DM2XS_7HaRxrjGu9XZkk_g@mail.gmail.com>
	<0122DA0E-814E-443A-A069-EC51908F5D7F@gmail.com>
Message-ID: <CAAcGz9-ZuHREFqj4jSpkUgEPGBksci3YBqbbqBEVnAdxAMNZzw@mail.gmail.com>

They are reasonable reasons, but traversing zones is a pain, you should see
if using one or the other is sufficient. I would check carefully the
distances you get against ellipsoidal calculations.

Cheers, Mike

On Sat, 28 Mar 2015 07:30 Andrew Duff <andrewaduff at gmail.com> wrote:

> A number of field folks prefer UTM because
>
> -it matches legacy paper USGS quad map series traditionally used for field
> navigation
> -units are in meters and can be used to gauge field distances from a
> coordinate readout
>
>
>
> > On Mar 27, 2015, at 10:48 AM, Michael Sumner <mdsumner at gmail.com> wrote:
> >
> > There is no good natural reason to use UTM, it mistifies me why our
> > community tolerates this bizarre default. I always use a local equal-area
> > projection unless some other compromise dictates a different choice.
> > Cheers, Mike
> >
> > On Fri, 27 Mar 2015 21:28 Barry Rowlingson <b.rowlingson at lancaster.ac.uk
> >
> > wrote:
> >
> >> If you have lat-long data that crosses two UTM zones then its
> >> generally okay to just pick *one* and transform all the points to
> >> that. Use the one that has the most points in. Basically use the UTM
> >> zones as guidelines to pick one UTM zone coordinate system. Unless
> >> your data spans several zones and you want quite high accuracy of
> >> distance measurements. Some points bleeding over into an adjacent zone
> >> are no problem.
> >>
> >> All projections are approximations to the earth's spheroid, so points
> >> that are within a single UTM zone have some distortion in their
> >> distance or angle relationships. Transforming points that are within
> >> an adjacent UTM zone is just an extension of that distortion. You can
> >> compute the precise distance error if you want for the furthest points
> >> by comparing with the geodesic distance.
> >>
> >> Alternatively you might find there is a coordinate system that spans
> >> your dataset nicely - often when a country or an island or a region
> >> crosses UTM zones there is an official coordinate system defined that
> >> is used by the authorities there.
> >>
> >> Also alternatively, there's nothing to stop you defining a transverse
> >> mercator system based on the centre of your data.
> >>
> >> Barry
> >>
> >>
> >>
> >> On Fri, Mar 27, 2015 at 7:44 AM, moses selebatso <
> selebatsom at yahoo.co.uk>
> >> wrote:
> >>> Hello
> >>> I have animal movement data that I have converted from Lat/Long to UTM,
> >> unfortunately the data falls in two UTM zones (34S and 35S). For some
> >> reason R cannot display both of them in the same window (the 35S data is
> >> way off the expected location).
> >>> The question is how do I convert the data such that R can correctly
> read
> >> it?
> >>> Moses SELEBATSO
> >>>
> >>> (+267) 318 5219 (H) (+267) 716 393 70 (C)
> >>>  (+267) 738 393 70 (C
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-Geo mailing list
> >>> R-sig-Geo at r-project.org
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
> >    [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Fri Mar 27 22:55:58 2015
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Fri, 27 Mar 2015 22:55:58 +0100
Subject: [R-sig-Geo] Pole centric map with Raster?
In-Reply-To: <5515C83A.8000404@gmail.com>
References: <0DB9402E-968E-4CE8-9448-373184FCEF13@bigelow.org>
	<5515C83A.8000404@gmail.com>
Message-ID: <5515D1EE.1080801@uni-muenster.de>



On 03/27/2015 10:14 PM, Tim Appelhans wrote:
> Ben
> here's some code I used for a paper.
> 
> Is this producing what you want?
> 
Cool!

The ggplot2 example of Ben looks as if it plots cells that form a
rectangular grid in long-lat onto a polar stereographic projection. One
could do this by converting the raster to a SpatialPolygonsDataFrame,
rgdal::spTransform this to the new projection, and then plot. It might
be improved by not plotting the cell (polygon) boundaries.

> 
> 
> library("remote")
> 
> ###### EXAMPLE I ##########################################################
> ###########################################################################
> library("rworldmap")
> library("rgdal")
> library("rgeos")
> library("gridExtra")
> library("RColorBrewer")
> 
> ## load data
> data("vdendool")
> data("coastsCoarse")
> 
> ## calculate 4 leading modes
> nh_modes <- eot(x = vdendool, y = NULL, n = 4, reduce.both = FALSE,
>                 standardised = FALSE, verbose = TRUE)
> 
> ## create coastal outlines
> ster <- CRS("+proj=stere +lat_0=90 +lon_0=-45")
> xmin <- -180
> xmax <- 180
> ymin <- 20
> ymax <- 90     # Coordinates for bounding box
> bb <- cbind(x = c(xmin, xmin, xmax, xmax, xmin),
>             y = c(ymin, ymax, ymax, ymin, ymin))    #Create bounding box
> SP <- SpatialPolygons(list(Polygons(list(Polygon(bb)), "1")),
>                       proj4string = CRS(proj4string(coastsCoarse)))
> 
> gI <- gIntersects(coastsCoarse, SP, byid = TRUE)
> out <- vector(mode = "list", length = length(which(gI)))
> ii <- 1
> 
> for (i in seq(along = gI)) if (gI[i]) {
>   out[[ii]] <- gIntersection(coastsCoarse[i, ], SP)
>   row.names(out[[ii]]) <- row.names(coastsCoarse)[i]
>   ii <- ii + 1
> }
> 
> nh_coasts <- do.call("rbind", out)
> nh_coasts_ster <- spTransform(nh_coasts, ster)
> 
> lout <- list("sp.lines", nh_coasts_ster,
>              col = "grey30", grid = TRUE)
> 
> ## define colours
> clrs <- colorRampPalette(rev(brewer.pal(9, "RdBu")))
> 
> ## project modes to polar stereographic CRS
> mode1 <- projectRaster(nh_modes[[1]]@r_predictor, crs = ster)
> 
> spplot(mode1, sp.layout = lout,
>        col.regions = clrs(1000), at = seq(-1, 1, 0.2),
>        par.settings = list(axis.line = list(col = 0)),
>        colorkey = list(height = 0.75, width = 1))
> 
> 
> Tim
> 
> 
> 
> On 27.03.2015 21:20, Ben Tupper wrote:
>> Hello,
>>
>> I'm learning how to use Raster* objects and ultimately hope to draw
>> rasters from a (north) polar perspective.  I can do something like
>> this using tiling in ggplot2:
>>
>> https://dl.dropboxusercontent.com/u/8433654/polar-map-with-matrix.png
>>
>>
>> I have hopes of doing similar using raster, sp and lattice.  But I'm
>> drowning in information as neophytes are prone to do.  Below is a
>> self-contained example...  I'm not getting too far.  It creates two
>> plots:
>>
>> raster without using projectRaster:
>> https://dl.dropboxusercontent.com/u/8433654/raster_no_project.png
>>
>> raster using projectRaster:
>> https://dl.dropboxusercontent.com/u/8433654/raster_with_project.png
>>
>> Neither is what I would like.  How does one get the map centered on
>> the pole and draw a (warped) raster using spplot?
>>
>> Thanks!
>> Ben
>>
>>
>> ##### START
>> library(raster)
>> library(maps)
>> library(maptools)
>> library(rgdal)
>>
>> # create a Raster from 'volcano'
>> my_proj <- "+proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0"
>> r <- raster(volcano,
>>     xmn = -90, xmx = 0, ymn = 50, ymx = 85,
>>     crs = my_proj)
>> # class       : RasterLayer
>> # dimensions  : 87, 61, 5307  (nrow, ncol, ncell)
>> # resolution  : 1.47541, 0.4022989  (x, y)
>> # extent      : -90, 0, 50, 85  (xmin, xmax, ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0
>> +ellps=WGS84
>> # data source : in memory
>> # names       : layer
>> # values      : 94, 195  (min, max)
>>
>> pr <- projectRaster(r, crs=my_proj)
>> # class       : RasterLayer
>> # dimensions  : 123, 71, 8733  (nrow, ncol, ncell)
>> # resolution  : 1.48, 0.402  (x, y)
>> # extent      : -97.4, 7.68, 42.79, 92.236  (xmin, xmax, ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0
>> +ellps=WGS84
>> # data source : in memory
>> # names       : layer
>> # values      : 93.88372, 194.8761  (min, max)
>>
>> # create a coastline and transform to the projection
>> mp <- map2SpatialLines(map(plot = FALSE, interior = FALSE),
>> proj4string = CRS(my_proj))
>> mp <- spTransform(mp, CRSobj = CRS(my_proj))
>> # class       : SpatialLines
>> # features    : 2904
>> # extent      : -179.9572, 190.2908, -85.44308, 83.57391  (xmin, xmax,
>> ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0
>> +ellps=WGS84
>>
>> # show the map with the 'unprojected' raster
>> plot_r <- spplot(r,
>>     sp.layout = list( sp.lines, mp, first = FALSE)
>> )
>>
>> # show the map with the projected raster
>> plot_pr <- spplot(pr,
>>     sp.layout = list( sp.lines, mp, first = FALSE)
>> )
>>
>> print(plot_r)
>> print(plot_pr)
>> ##### END
>>
>>
>>
>>> sessionInfo()
>> R version 3.1.2 (2014-10-31)
>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>>
>> locale:
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] rgdal_0.9-2     maptools_0.8-34 maps_2.3-9      raster_2.3-33  
>> sp_1.0-17
>>
>> loaded via a namespace (and not attached):
>> [1] foreign_0.8-63  grid_3.1.2      lattice_0.20-30 tools_3.1.2
>>
>>
>> Ben Tupper
>> Bigelow Laboratory for Ocean Sciences
>> 60 Bigelow Drive, P.O. Box 380
>> East Boothbay, Maine 04544
>> http://www.bigelow.org
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi),  University of M?nster,
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150327/90bd9802/attachment.bin>

From wandrson01 at gmail.com  Fri Mar 27 23:45:40 2015
From: wandrson01 at gmail.com (Walter Anderson)
Date: Fri, 27 Mar 2015 17:45:40 -0500
Subject: [R-sig-Geo] Calculating area of polygons created from a spatial
	intersect
Message-ID: <5515DD94.5030908@gmail.com>

Hello all,

I am attempting to automate an analysis that I developed with ArcInfo
using R and the gdal and geos packages (or any other) if possible.

Here is the basic process

I have a shape file (lines) that defines the limits of all of the
projects with each project having a unique identifier.

I have another shape file (polys) that contains total population and
low income population and represent Census block groups.  This shape
file has an area field which has the acreage of the total block group.

Process

Step 1.
I then buffer these project lines to create a second shape file that
represents the 'footprint' of the project. (Creates polys).

Step 2.
In ArcInfo, I perform an intersection of the two shape files
(footprint and census blocks) and this creates a third shape file
which has a unique polygon for every project/census block intersection.

Step 3.
I then perform an area calculation (acres) on this new poly shape file
and use this calculated area divided by the original area of the
associated census block group to apportion the two population datum to
this new polygon.

Step 4.
Finally, I sum the two population datums for each of the projects from
the attribute table of this final shape file.

When I try to replicate the above procedure I run into a problem with
Step 2 when I use what I think is the appropriate command:

gIntersects(buffered_projects, census_blocks, byID=TRUE)

This command is producing a matrix of each project/census block
combination and only providing me a true/false indication.  Is there
any way to replicate the process from ArcInfo that I outlined above
within R?

Walter Anderson


From btupper at bigelow.org  Fri Mar 27 23:46:02 2015
From: btupper at bigelow.org (Ben Tupper)
Date: Fri, 27 Mar 2015 18:46:02 -0400
Subject: [R-sig-Geo] Pole centric map with Raster?
In-Reply-To: <5515C83A.8000404@gmail.com>
References: <0DB9402E-968E-4CE8-9448-373184FCEF13@bigelow.org>
	<5515C83A.8000404@gmail.com>
Message-ID: <A8305DD5-D5A2-41F5-A920-8CE94087AF83@bigelow.org>


On Mar 27, 2015, at 5:14 PM, Tim Appelhans <tim.appelhans at gmail.com> wrote:

> Ben
> here's some code I used for a paper.
> 
> Is this producing what you want?


Hi,

Shoot. As soon as I figure out how to get geos and rgeos running on my OSX 10.9.5 I'll try it!  It's like having to your vegetables before eating dessert.

Cheers,
Ben


> 
> 
> 
> library("remote")
> 
> ###### EXAMPLE I ##########################################################
> ###########################################################################
> library("rworldmap")
> library("rgdal")
> library("rgeos")
> library("gridExtra")
> library("RColorBrewer")
> 
> ## load data
> data("vdendool")
> data("coastsCoarse")
> 
> ## calculate 4 leading modes
> nh_modes <- eot(x = vdendool, y = NULL, n = 4, reduce.both = FALSE,
>                standardised = FALSE, verbose = TRUE)
> 
> ## create coastal outlines
> ster <- CRS("+proj=stere +lat_0=90 +lon_0=-45")
> xmin <- -180
> xmax <- 180
> ymin <- 20
> ymax <- 90     # Coordinates for bounding box
> bb <- cbind(x = c(xmin, xmin, xmax, xmax, xmin),
>            y = c(ymin, ymax, ymax, ymin, ymin))    #Create bounding box
> SP <- SpatialPolygons(list(Polygons(list(Polygon(bb)), "1")),
>                      proj4string = CRS(proj4string(coastsCoarse)))
> 
> gI <- gIntersects(coastsCoarse, SP, byid = TRUE)
> out <- vector(mode = "list", length = length(which(gI)))
> ii <- 1
> 
> for (i in seq(along = gI)) if (gI[i]) {
>  out[[ii]] <- gIntersection(coastsCoarse[i, ], SP)
>  row.names(out[[ii]]) <- row.names(coastsCoarse)[i]
>  ii <- ii + 1
> }
> 
> nh_coasts <- do.call("rbind", out)
> nh_coasts_ster <- spTransform(nh_coasts, ster)
> 
> lout <- list("sp.lines", nh_coasts_ster,
>             col = "grey30", grid = TRUE)
> 
> ## define colours
> clrs <- colorRampPalette(rev(brewer.pal(9, "RdBu")))
> 
> ## project modes to polar stereographic CRS
> mode1 <- projectRaster(nh_modes[[1]]@r_predictor, crs = ster)
> 
> spplot(mode1, sp.layout = lout,
>       col.regions = clrs(1000), at = seq(-1, 1, 0.2),
>       par.settings = list(axis.line = list(col = 0)),
>       colorkey = list(height = 0.75, width = 1))
> 
> 
> Tim
> 
> 
> 
> On 27.03.2015 21:20, Ben Tupper wrote:
>> Hello,
>> 
>> I'm learning how to use Raster* objects and ultimately hope to draw rasters from a (north) polar perspective.  I can do something like this using tiling in ggplot2:
>> 
>> https://dl.dropboxusercontent.com/u/8433654/polar-map-with-matrix.png
>> 
>> 
>> I have hopes of doing similar using raster, sp and lattice.  But I'm drowning in information as neophytes are prone to do.  Below is a self-contained example...  I'm not getting too far.  It creates two plots:
>> 
>> raster without using projectRaster: https://dl.dropboxusercontent.com/u/8433654/raster_no_project.png
>> 
>> raster using projectRaster: https://dl.dropboxusercontent.com/u/8433654/raster_with_project.png
>> 
>> Neither is what I would like.  How does one get the map centered on the pole and draw a (warped) raster using spplot?
>> 
>> Thanks!
>> Ben
>> 
>> 
>> ##### START
>> library(raster)
>> library(maps)
>> library(maptools)
>> library(rgdal)
>> 
>> # create a Raster from 'volcano'
>> my_proj <- "+proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0"
>> r <- raster(volcano,
>>    xmn = -90, xmx = 0, ymn = 50, ymx = 85,
>>    crs = my_proj)
>> # class       : RasterLayer
>> # dimensions  : 87, 61, 5307  (nrow, ncol, ncell)
>> # resolution  : 1.47541, 0.4022989  (x, y)
>> # extent      : -90, 0, 50, 85  (xmin, xmax, ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
>> # data source : in memory
>> # names       : layer
>> # values      : 94, 195  (min, max)
>> 
>> pr <- projectRaster(r, crs=my_proj)
>> # class       : RasterLayer
>> # dimensions  : 123, 71, 8733  (nrow, ncol, ncell)
>> # resolution  : 1.48, 0.402  (x, y)
>> # extent      : -97.4, 7.68, 42.79, 92.236  (xmin, xmax, ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
>> # data source : in memory
>> # names       : layer
>> # values      : 93.88372, 194.8761  (min, max)
>> 
>> # create a coastline and transform to the projection
>> mp <- map2SpatialLines(map(plot = FALSE, interior = FALSE), proj4string = CRS(my_proj))
>> mp <- spTransform(mp, CRSobj = CRS(my_proj))
>> # class       : SpatialLines
>> # features    : 2904
>> # extent      : -179.9572, 190.2908, -85.44308, 83.57391  (xmin, xmax, ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
>> 
>> # show the map with the 'unprojected' raster
>> plot_r <- spplot(r,
>>    sp.layout = list( sp.lines, mp, first = FALSE)
>> )
>> 
>> # show the map with the projected raster
>> plot_pr <- spplot(pr,
>>    sp.layout = list( sp.lines, mp, first = FALSE)
>> )
>> 
>> print(plot_r)
>> print(plot_pr)
>> ##### END
>> 
>> 
>> 
>>> sessionInfo()
>> R version 3.1.2 (2014-10-31)
>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>> 
>> locale:
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>> 
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>> 
>> other attached packages:
>> [1] rgdal_0.9-2     maptools_0.8-34 maps_2.3-9      raster_2.3-33   sp_1.0-17
>> 
>> loaded via a namespace (and not attached):
>> [1] foreign_0.8-63  grid_3.1.2      lattice_0.20-30 tools_3.1.2
>> 
>> 
>> Ben Tupper
>> Bigelow Laboratory for Ocean Sciences
>> 60 Bigelow Drive, P.O. Box 380
>> East Boothbay, Maine 04544
>> http://www.bigelow.org
>> 
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 
> -- 
> #####################################
> Tim Appelhans
> Department of Geography
> Environmental Informatics
> Philipps Universit?t Marburg
> Deutschhausstra?e 12
> 35032 Marburg (Paketpost: 35037 Marburg)
> Germany
> 
> Tel +49 (0) 6421 28-25957
> 
> http://environmentalinformatics-marburg.de/
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From btupper at bigelow.org  Sat Mar 28 00:03:49 2015
From: btupper at bigelow.org (Ben Tupper)
Date: Fri, 27 Mar 2015 19:03:49 -0400
Subject: [R-sig-Geo] Pole centric map with Raster?
In-Reply-To: <5515C83A.8000404@gmail.com>
References: <0DB9402E-968E-4CE8-9448-373184FCEF13@bigelow.org>
	<5515C83A.8000404@gmail.com>
Message-ID: <7E20CEC3-C727-4629-9280-72D02122B0E7@bigelow.org>


On Mar 27, 2015, at 5:14 PM, Tim Appelhans <tim.appelhans at gmail.com> wrote:

> Ben
> here's some code I used for a paper.
> 
> Is this producing what you want?
> 

Hello again,

Yes!  That does do what I want.  It shouldn't take me more than three years to figure out how you did that. I'll get started right away!

Thanks for the help.

Cheers,
Ben

P.S. This...

$ R CMD INSTALL /Users/ben/Downloads/rgeos_0.3-8.tar.gz --configure-args='--with-geos-config=/Library/Frameworks/GEOS.framework/unix/bin/geos-config' 

... did the trick with rgeos.



> 
> 
> library("remote")
> 
> ###### EXAMPLE I ##########################################################
> ###########################################################################
> library("rworldmap")
> library("rgdal")
> library("rgeos")
> library("gridExtra")
> library("RColorBrewer")
> 
> ## load data
> data("vdendool")
> data("coastsCoarse")
> 
> ## calculate 4 leading modes
> nh_modes <- eot(x = vdendool, y = NULL, n = 4, reduce.both = FALSE,
>                standardised = FALSE, verbose = TRUE)
> 
> ## create coastal outlines
> ster <- CRS("+proj=stere +lat_0=90 +lon_0=-45")
> xmin <- -180
> xmax <- 180
> ymin <- 20
> ymax <- 90     # Coordinates for bounding box
> bb <- cbind(x = c(xmin, xmin, xmax, xmax, xmin),
>            y = c(ymin, ymax, ymax, ymin, ymin))    #Create bounding box
> SP <- SpatialPolygons(list(Polygons(list(Polygon(bb)), "1")),
>                      proj4string = CRS(proj4string(coastsCoarse)))
> 
> gI <- gIntersects(coastsCoarse, SP, byid = TRUE)
> out <- vector(mode = "list", length = length(which(gI)))
> ii <- 1
> 
> for (i in seq(along = gI)) if (gI[i]) {
>  out[[ii]] <- gIntersection(coastsCoarse[i, ], SP)
>  row.names(out[[ii]]) <- row.names(coastsCoarse)[i]
>  ii <- ii + 1
> }
> 
> nh_coasts <- do.call("rbind", out)
> nh_coasts_ster <- spTransform(nh_coasts, ster)
> 
> lout <- list("sp.lines", nh_coasts_ster,
>             col = "grey30", grid = TRUE)
> 
> ## define colours
> clrs <- colorRampPalette(rev(brewer.pal(9, "RdBu")))
> 
> ## project modes to polar stereographic CRS
> mode1 <- projectRaster(nh_modes[[1]]@r_predictor, crs = ster)
> 
> spplot(mode1, sp.layout = lout,
>       col.regions = clrs(1000), at = seq(-1, 1, 0.2),
>       par.settings = list(axis.line = list(col = 0)),
>       colorkey = list(height = 0.75, width = 1))
> 
> 
> Tim
> 
> 
> 
> On 27.03.2015 21:20, Ben Tupper wrote:
>> Hello,
>> 
>> I'm learning how to use Raster* objects and ultimately hope to draw rasters from a (north) polar perspective.  I can do something like this using tiling in ggplot2:
>> 
>> https://dl.dropboxusercontent.com/u/8433654/polar-map-with-matrix.png
>> 
>> 
>> I have hopes of doing similar using raster, sp and lattice.  But I'm drowning in information as neophytes are prone to do.  Below is a self-contained example...  I'm not getting too far.  It creates two plots:
>> 
>> raster without using projectRaster: https://dl.dropboxusercontent.com/u/8433654/raster_no_project.png
>> 
>> raster using projectRaster: https://dl.dropboxusercontent.com/u/8433654/raster_with_project.png
>> 
>> Neither is what I would like.  How does one get the map centered on the pole and draw a (warped) raster using spplot?
>> 
>> Thanks!
>> Ben
>> 
>> 
>> ##### START
>> library(raster)
>> library(maps)
>> library(maptools)
>> library(rgdal)
>> 
>> # create a Raster from 'volcano'
>> my_proj <- "+proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0"
>> r <- raster(volcano,
>>    xmn = -90, xmx = 0, ymn = 50, ymx = 85,
>>    crs = my_proj)
>> # class       : RasterLayer
>> # dimensions  : 87, 61, 5307  (nrow, ncol, ncell)
>> # resolution  : 1.47541, 0.4022989  (x, y)
>> # extent      : -90, 0, 50, 85  (xmin, xmax, ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
>> # data source : in memory
>> # names       : layer
>> # values      : 94, 195  (min, max)
>> 
>> pr <- projectRaster(r, crs=my_proj)
>> # class       : RasterLayer
>> # dimensions  : 123, 71, 8733  (nrow, ncol, ncell)
>> # resolution  : 1.48, 0.402  (x, y)
>> # extent      : -97.4, 7.68, 42.79, 92.236  (xmin, xmax, ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
>> # data source : in memory
>> # names       : layer
>> # values      : 93.88372, 194.8761  (min, max)
>> 
>> # create a coastline and transform to the projection
>> mp <- map2SpatialLines(map(plot = FALSE, interior = FALSE), proj4string = CRS(my_proj))
>> mp <- spTransform(mp, CRSobj = CRS(my_proj))
>> # class       : SpatialLines
>> # features    : 2904
>> # extent      : -179.9572, 190.2908, -85.44308, 83.57391  (xmin, xmax, ymin, ymax)
>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0 +ellps=WGS84
>> 
>> # show the map with the 'unprojected' raster
>> plot_r <- spplot(r,
>>    sp.layout = list( sp.lines, mp, first = FALSE)
>> )
>> 
>> # show the map with the projected raster
>> plot_pr <- spplot(pr,
>>    sp.layout = list( sp.lines, mp, first = FALSE)
>> )
>> 
>> print(plot_r)
>> print(plot_pr)
>> ##### END
>> 
>> 
>> 
>>> sessionInfo()
>> R version 3.1.2 (2014-10-31)
>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>> 
>> locale:
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>> 
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>> 
>> other attached packages:
>> [1] rgdal_0.9-2     maptools_0.8-34 maps_2.3-9      raster_2.3-33   sp_1.0-17
>> 
>> loaded via a namespace (and not attached):
>> [1] foreign_0.8-63  grid_3.1.2      lattice_0.20-30 tools_3.1.2
>> 
>> 
>> Ben Tupper
>> Bigelow Laboratory for Ocean Sciences
>> 60 Bigelow Drive, P.O. Box 380
>> East Boothbay, Maine 04544
>> http://www.bigelow.org
>> 
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 
> -- 
> #####################################
> Tim Appelhans
> Department of Geography
> Environmental Informatics
> Philipps Universit?t Marburg
> Deutschhausstra?e 12
> 35032 Marburg (Paketpost: 35037 Marburg)
> Germany
> 
> Tel +49 (0) 6421 28-25957
> 
> http://environmentalinformatics-marburg.de/
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From btupper at bigelow.org  Sat Mar 28 00:41:07 2015
From: btupper at bigelow.org (Ben Tupper)
Date: Fri, 27 Mar 2015 19:41:07 -0400
Subject: [R-sig-Geo] Pole centric map with Raster?
In-Reply-To: <5515D1EE.1080801@uni-muenster.de>
References: <0DB9402E-968E-4CE8-9448-373184FCEF13@bigelow.org>
	<5515C83A.8000404@gmail.com> <5515D1EE.1080801@uni-muenster.de>
Message-ID: <B819DFBB-CE0D-4E8C-9A72-4FA934A4FFA8@bigelow.org>

Hi,

On Mar 27, 2015, at 5:55 PM, Edzer Pebesma <edzer.pebesma at uni-muenster.de> wrote:

> 
> 
> On 03/27/2015 10:14 PM, Tim Appelhans wrote:
>> Ben
>> here's some code I used for a paper.
>> 
>> Is this producing what you want?
>> 
> Cool!
> 
> The ggplot2 example of Ben looks as if it plots cells that form a
> rectangular grid in long-lat onto a polar stereographic projection. One
> could do this by converting the raster to a SpatialPolygonsDataFrame,
> rgdal::spTransform this to the new projection, and then plot. It might
> be improved by not plotting the cell (polygon) boundaries.
> 

Yes, in the ggplot2 example the volcano matrix is transformed into a data frame, 'mat', of [lon,lat,z]. I followed this nice post:

http://www.numbertheory.nl/2011/11/08/drawing-polar-centered-spatial-maps-using-ggplot2/

I think the following replicates what I did before (less the coastline). I don't know how to control the boundaries in ggplot2.  I'll try what you suggest with SpatialPolygonsDataFrame - I would prefer to use just one graphics paradigm, such as lattice/spplot, if possible.

Thanks!
Ben

### START
library(ggplot2)

# convert a matrix to data frame
as.tile <- function(x = volcano, 
   bb = c(ll.lat = 45, ll.lon = -90, ur.lat = 75, ur.lon = -40) ){
   if (!is.matrix(x)) stop("input must be a matrix")
   d <- dim(x)
   xx <- rep(seq(from = bb[['ll.lon']], to = bb[['ur.lon']], length = d[2]), each = d[1])
   yy <- rep(seq(from = bb[['ll.lat']], to = bb[['ur.lat']], length = d[1]), d[2])
   data.frame(lon = xx, lat = yy, z = as.vector(x))
}

# custom theme without axes and annotations
theme_polar <- function(){
   list(
      theme_bw(base_size=18),
      theme(
         axis.text = element_blank(),
         axis.ticks = element_blank(),
         legend.position = "none"),
      labs(x='',y=''))
}

ylim <- c(30,90)
xlim <- c(-180,180)
projection <- "azequidist" 
orientation <- c(90, -30, 0)
mat <- as.tile()

ggplot() + 
	geom_tile(data = mat, aes(x = lon, y = lat, fill = z)) +
	coord_map(projection = projection, orientation = orientation, ylim = ylim, xlim = xlim) + 
	scale_x_continuous(breaks=(-6:6) * 30, expand = c(0,0)) +
	scale_y_continuous(breaks = seq(from = ylim[1], to = ylim[2], by = 15), limits = ylim, expand = c(0,0)) +
	theme_polar()

#### END

>> 
>> 
>> library("remote")
>> 
>> ###### EXAMPLE I ##########################################################
>> ###########################################################################
>> library("rworldmap")
>> library("rgdal")
>> library("rgeos")
>> library("gridExtra")
>> library("RColorBrewer")
>> 
>> ## load data
>> data("vdendool")
>> data("coastsCoarse")
>> 
>> ## calculate 4 leading modes
>> nh_modes <- eot(x = vdendool, y = NULL, n = 4, reduce.both = FALSE,
>>                standardised = FALSE, verbose = TRUE)
>> 
>> ## create coastal outlines
>> ster <- CRS("+proj=stere +lat_0=90 +lon_0=-45")
>> xmin <- -180
>> xmax <- 180
>> ymin <- 20
>> ymax <- 90     # Coordinates for bounding box
>> bb <- cbind(x = c(xmin, xmin, xmax, xmax, xmin),
>>            y = c(ymin, ymax, ymax, ymin, ymin))    #Create bounding box
>> SP <- SpatialPolygons(list(Polygons(list(Polygon(bb)), "1")),
>>                      proj4string = CRS(proj4string(coastsCoarse)))
>> 
>> gI <- gIntersects(coastsCoarse, SP, byid = TRUE)
>> out <- vector(mode = "list", length = length(which(gI)))
>> ii <- 1
>> 
>> for (i in seq(along = gI)) if (gI[i]) {
>>  out[[ii]] <- gIntersection(coastsCoarse[i, ], SP)
>>  row.names(out[[ii]]) <- row.names(coastsCoarse)[i]
>>  ii <- ii + 1
>> }
>> 
>> nh_coasts <- do.call("rbind", out)
>> nh_coasts_ster <- spTransform(nh_coasts, ster)
>> 
>> lout <- list("sp.lines", nh_coasts_ster,
>>             col = "grey30", grid = TRUE)
>> 
>> ## define colours
>> clrs <- colorRampPalette(rev(brewer.pal(9, "RdBu")))
>> 
>> ## project modes to polar stereographic CRS
>> mode1 <- projectRaster(nh_modes[[1]]@r_predictor, crs = ster)
>> 
>> spplot(mode1, sp.layout = lout,
>>       col.regions = clrs(1000), at = seq(-1, 1, 0.2),
>>       par.settings = list(axis.line = list(col = 0)),
>>       colorkey = list(height = 0.75, width = 1))
>> 
>> 
>> Tim
>> 
>> 
>> 
>> On 27.03.2015 21:20, Ben Tupper wrote:
>>> Hello,
>>> 
>>> I'm learning how to use Raster* objects and ultimately hope to draw
>>> rasters from a (north) polar perspective.  I can do something like
>>> this using tiling in ggplot2:
>>> 
>>> https://dl.dropboxusercontent.com/u/8433654/polar-map-with-matrix.png
>>> 
>>> 
>>> I have hopes of doing similar using raster, sp and lattice.  But I'm
>>> drowning in information as neophytes are prone to do.  Below is a
>>> self-contained example...  I'm not getting too far.  It creates two
>>> plots:
>>> 
>>> raster without using projectRaster:
>>> https://dl.dropboxusercontent.com/u/8433654/raster_no_project.png
>>> 
>>> raster using projectRaster:
>>> https://dl.dropboxusercontent.com/u/8433654/raster_with_project.png
>>> 
>>> Neither is what I would like.  How does one get the map centered on
>>> the pole and draw a (warped) raster using spplot?
>>> 
>>> Thanks!
>>> Ben
>>> 
>>> 
>>> ##### START
>>> library(raster)
>>> library(maps)
>>> library(maptools)
>>> library(rgdal)
>>> 
>>> # create a Raster from 'volcano'
>>> my_proj <- "+proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0"
>>> r <- raster(volcano,
>>>    xmn = -90, xmx = 0, ymn = 50, ymx = 85,
>>>    crs = my_proj)
>>> # class       : RasterLayer
>>> # dimensions  : 87, 61, 5307  (nrow, ncol, ncell)
>>> # resolution  : 1.47541, 0.4022989  (x, y)
>>> # extent      : -90, 0, 50, 85  (xmin, xmax, ymin, ymax)
>>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0
>>> +ellps=WGS84
>>> # data source : in memory
>>> # names       : layer
>>> # values      : 94, 195  (min, max)
>>> 
>>> pr <- projectRaster(r, crs=my_proj)
>>> # class       : RasterLayer
>>> # dimensions  : 123, 71, 8733  (nrow, ncol, ncell)
>>> # resolution  : 1.48, 0.402  (x, y)
>>> # extent      : -97.4, 7.68, 42.79, 92.236  (xmin, xmax, ymin, ymax)
>>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0
>>> +ellps=WGS84
>>> # data source : in memory
>>> # names       : layer
>>> # values      : 93.88372, 194.8761  (min, max)
>>> 
>>> # create a coastline and transform to the projection
>>> mp <- map2SpatialLines(map(plot = FALSE, interior = FALSE),
>>> proj4string = CRS(my_proj))
>>> mp <- spTransform(mp, CRSobj = CRS(my_proj))
>>> # class       : SpatialLines
>>> # features    : 2904
>>> # extent      : -179.9572, 190.2908, -85.44308, 83.57391  (xmin, xmax,
>>> ymin, ymax)
>>> # coord. ref. : +proj=stere +lat_0=90 +lon_0=-30 +x_0=0 +y_0=0
>>> +ellps=WGS84
>>> 
>>> # show the map with the 'unprojected' raster
>>> plot_r <- spplot(r,
>>>    sp.layout = list( sp.lines, mp, first = FALSE)
>>> )
>>> 
>>> # show the map with the projected raster
>>> plot_pr <- spplot(pr,
>>>    sp.layout = list( sp.lines, mp, first = FALSE)
>>> )
>>> 
>>> print(plot_r)
>>> print(plot_pr)
>>> ##### END
>>> 
>>> 
>>> 
>>>> sessionInfo()
>>> R version 3.1.2 (2014-10-31)
>>> Platform: x86_64-apple-darwin13.4.0 (64-bit)
>>> 
>>> locale:
>>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>> 
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>> 
>>> other attached packages:
>>> [1] rgdal_0.9-2     maptools_0.8-34 maps_2.3-9      raster_2.3-33  
>>> sp_1.0-17
>>> 
>>> loaded via a namespace (and not attached):
>>> [1] foreign_0.8-63  grid_3.1.2      lattice_0.20-30 tools_3.1.2
>>> 
>>> 
>>> Ben Tupper
>>> Bigelow Laboratory for Ocean Sciences
>>> 60 Bigelow Drive, P.O. Box 380
>>> East Boothbay, Maine 04544
>>> http://www.bigelow.org
>>> 
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> 
> 
> -- 
> Edzer Pebesma
> Institute for Geoinformatics (ifgi),  University of M?nster,
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
> Spatial Statistics Society http://www.spatialstatistics.info
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org


From r.hijmans at gmail.com  Sat Mar 28 00:46:54 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Fri, 27 Mar 2015 16:46:54 -0700
Subject: [R-sig-Geo] Calculating area of polygons created from a spatial
	intersect
In-Reply-To: <5515DD94.5030908@gmail.com>
References: <5515DD94.5030908@gmail.com>
Message-ID: <CANtt_hwDW5FAL_YKToWxxAm-jTZqf-SLAO00_pROsCgJmgPvLQ@mail.gmail.com>

Walter,

I think that instead of
  gIntersects(buffered_projects, census_blocks, byID=TRUE)

you want
  library(raster)
  x <- intersect(buffered_projects, census_blocks, byID=TRUE)

and then Step 3
  x$newarea <- gArea(x, byid=TRUE)
  x$relarea <- x$newarea / x$area


Robert


On Fri, Mar 27, 2015 at 3:45 PM, Walter Anderson <wandrson01 at gmail.com> wrote:
> Hello all,
>
> I am attempting to automate an analysis that I developed with ArcInfo
> using R and the gdal and geos packages (or any other) if possible.
>
> Here is the basic process
>
> I have a shape file (lines) that defines the limits of all of the
> projects with each project having a unique identifier.
>
> I have another shape file (polys) that contains total population and
> low income population and represent Census block groups.  This shape
> file has an area field which has the acreage of the total block group.
>
> Process
>
> Step 1.
> I then buffer these project lines to create a second shape file that
> represents the 'footprint' of the project. (Creates polys).
>
> Step 2.
> In ArcInfo, I perform an intersection of the two shape files
> (footprint and census blocks) and this creates a third shape file
> which has a unique polygon for every project/census block intersection.
>
> Step 3.
> I then perform an area calculation (acres) on this new poly shape file
> and use this calculated area divided by the original area of the
> associated census block group to apportion the two population datum to
> this new polygon.
>
> Step 4.
> Finally, I sum the two population datums for each of the projects from
> the attribute table of this final shape file.
>
> When I try to replicate the above procedure I run into a problem with
> Step 2 when I use what I think is the appropriate command:
>
> gIntersects(buffered_projects, census_blocks, byID=TRUE)
>
> This command is producing a matrix of each project/census block
> combination and only providing me a true/false indication.  Is there
> any way to replicate the process from ArcInfo that I outlined above
> within R?
>
> Walter Anderson
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From r.hijmans at gmail.com  Sat Mar 28 00:50:10 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Fri, 27 Mar 2015 16:50:10 -0700
Subject: [R-sig-Geo] Help with latlong to UTM conversion when UTM zones
 are different
In-Reply-To: <CAAcGz9-ZuHREFqj4jSpkUgEPGBksci3YBqbbqBEVnAdxAMNZzw@mail.gmail.com>
References: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>
	<CANVKczPJ72GAZnVKyav-jQN4WUU86FAoxOrxV61WYHV+QjNkig@mail.gmail.com>
	<CAAcGz9__G+zfjy_Cp_Jnz0WeJz98DM2XS_7HaRxrjGu9XZkk_g@mail.gmail.com>
	<0122DA0E-814E-443A-A069-EC51908F5D7F@gmail.com>
	<CAAcGz9-ZuHREFqj4jSpkUgEPGBksci3YBqbbqBEVnAdxAMNZzw@mail.gmail.com>
Message-ID: <CANtt_hxXChRi=fqWNeVzzP5PrCE4fOHTm60RZmLgax25MoPrxA@mail.gmail.com>

Are these really reasonable reasons? I do not think so, given that the
question had nothing to do with map navigation and the person asking
appears to live in the UK. Moreover, other projections have, or can
have, their units in meters as well (or feet or miles or whatever you
might fancy). UTM indeed appears to be an unfortunate default that
deserves some pushback.
Robert

On Fri, Mar 27, 2015 at 2:28 PM, Michael Sumner <mdsumner at gmail.com> wrote:
> They are reasonable reasons, but traversing zones is a pain, you should see
> if using one or the other is sufficient. I would check carefully the
> distances you get against ellipsoidal calculations.
>
> Cheers, Mike
>
> On Sat, 28 Mar 2015 07:30 Andrew Duff <andrewaduff at gmail.com> wrote:
>
>> A number of field folks prefer UTM because
>>
>> -it matches legacy paper USGS quad map series traditionally used for field
>> navigation
>> -units are in meters and can be used to gauge field distances from a
>> coordinate readout
>>
>>
>>
>> > On Mar 27, 2015, at 10:48 AM, Michael Sumner <mdsumner at gmail.com> wrote:
>> >
>> > There is no good natural reason to use UTM, it mistifies me why our
>> > community tolerates this bizarre default. I always use a local equal-area
>> > projection unless some other compromise dictates a different choice.
>> > Cheers, Mike
>> >
>> > On Fri, 27 Mar 2015 21:28 Barry Rowlingson <b.rowlingson at lancaster.ac.uk
>> >
>> > wrote:
>> >
>> >> If you have lat-long data that crosses two UTM zones then its
>> >> generally okay to just pick *one* and transform all the points to
>> >> that. Use the one that has the most points in. Basically use the UTM
>> >> zones as guidelines to pick one UTM zone coordinate system. Unless
>> >> your data spans several zones and you want quite high accuracy of
>> >> distance measurements. Some points bleeding over into an adjacent zone
>> >> are no problem.
>> >>
>> >> All projections are approximations to the earth's spheroid, so points
>> >> that are within a single UTM zone have some distortion in their
>> >> distance or angle relationships. Transforming points that are within
>> >> an adjacent UTM zone is just an extension of that distortion. You can
>> >> compute the precise distance error if you want for the furthest points
>> >> by comparing with the geodesic distance.
>> >>
>> >> Alternatively you might find there is a coordinate system that spans
>> >> your dataset nicely - often when a country or an island or a region
>> >> crosses UTM zones there is an official coordinate system defined that
>> >> is used by the authorities there.
>> >>
>> >> Also alternatively, there's nothing to stop you defining a transverse
>> >> mercator system based on the centre of your data.
>> >>
>> >> Barry
>> >>
>> >>
>> >>
>> >> On Fri, Mar 27, 2015 at 7:44 AM, moses selebatso <
>> selebatsom at yahoo.co.uk>
>> >> wrote:
>> >>> Hello
>> >>> I have animal movement data that I have converted from Lat/Long to UTM,
>> >> unfortunately the data falls in two UTM zones (34S and 35S). For some
>> >> reason R cannot display both of them in the same window (the 35S data is
>> >> way off the expected location).
>> >>> The question is how do I convert the data such that R can correctly
>> read
>> >> it?
>> >>> Moses SELEBATSO
>> >>>
>> >>> (+267) 318 5219 (H) (+267) 716 393 70 (C)
>> >>>  (+267) 738 393 70 (C
>> >>>        [[alternative HTML version deleted]]
>> >>>
>> >>> _______________________________________________
>> >>> R-sig-Geo mailing list
>> >>> R-sig-Geo at r-project.org
>> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >>
>> >> _______________________________________________
>> >> R-sig-Geo mailing list
>> >> R-sig-Geo at r-project.org
>> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >
>> >    [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-Geo mailing list
>> > R-sig-Geo at r-project.org
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From b.rowlingson at lancaster.ac.uk  Sat Mar 28 10:12:53 2015
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Sat, 28 Mar 2015 09:12:53 +0000
Subject: [R-sig-Geo] Help with latlong to UTM conversion when UTM zones
 are different
In-Reply-To: <CANtt_hxXChRi=fqWNeVzzP5PrCE4fOHTm60RZmLgax25MoPrxA@mail.gmail.com>
References: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>
	<CANVKczPJ72GAZnVKyav-jQN4WUU86FAoxOrxV61WYHV+QjNkig@mail.gmail.com>
	<CAAcGz9__G+zfjy_Cp_Jnz0WeJz98DM2XS_7HaRxrjGu9XZkk_g@mail.gmail.com>
	<0122DA0E-814E-443A-A069-EC51908F5D7F@gmail.com>
	<CAAcGz9-ZuHREFqj4jSpkUgEPGBksci3YBqbbqBEVnAdxAMNZzw@mail.gmail.com>
	<CANtt_hxXChRi=fqWNeVzzP5PrCE4fOHTm60RZmLgax25MoPrxA@mail.gmail.com>
Message-ID: <CANVKczM3Rn-vRZf4+J4pcV5=Bi+ft0RVaU6kbHuVVSW0iA9uPw@mail.gmail.com>

On 27 Mar 2015 23:50, "Robert J. Hijmans" <r.hijmans at gmail.com> wrote:
>
> Are these really reasonable reasons? I do not think so, given that the
> question had nothing to do with map navigation and the person asking
> appears to live in the UK.

??? How did you figure that out? He gave some phone numbers that had
Botswana country codes and a southern hemisphere utm zone code! Elementary
my dear Robert!

But yes, as I said, if there's a local standard use it. To find the local
standard, buy a paper map and read the projection info! I would never use
UTM in the uk when epsg:27700 is our standard.

> Moreover, other projections have, or can
> have, their units in meters as well (or feet or miles or whatever you
> might fancy). UTM indeed appears to be an unfortunate default that
> deserves some pushback.

I see utm as a last resort rather than a default. However finding a better
coordinate system in the maze of epsg codes can lead to people using the
wrong thing, and unless you span several zones, utm is never that wrong...

> Robert
>
> On Fri, Mar 27, 2015 at 2:28 PM, Michael Sumner <mdsumner at gmail.com>
wrote:
> > They are reasonable reasons, but traversing zones is a pain, you should
see
> > if using one or the other is sufficient. I would check carefully the
> > distances you get against ellipsoidal calculations.
> >
> > Cheers, Mike
> >
> > On Sat, 28 Mar 2015 07:30 Andrew Duff <andrewaduff at gmail.com> wrote:
> >
> >> A number of field folks prefer UTM because
> >>
> >> -it matches legacy paper USGS quad map series traditionally used for
field
> >> navigation
> >> -units are in meters and can be used to gauge field distances from a
> >> coordinate readout
> >>
> >>
> >>
> >> > On Mar 27, 2015, at 10:48 AM, Michael Sumner <mdsumner at gmail.com>
wrote:
> >> >
> >> > There is no good natural reason to use UTM, it mistifies me why our
> >> > community tolerates this bizarre default. I always use a local
equal-area
> >> > projection unless some other compromise dictates a different choice.
> >> > Cheers, Mike
> >> >
> >> > On Fri, 27 Mar 2015 21:28 Barry Rowlingson <
b.rowlingson at lancaster.ac.uk
> >> >
> >> > wrote:
> >> >
> >> >> If you have lat-long data that crosses two UTM zones then its
> >> >> generally okay to just pick *one* and transform all the points to
> >> >> that. Use the one that has the most points in. Basically use the UTM
> >> >> zones as guidelines to pick one UTM zone coordinate system. Unless
> >> >> your data spans several zones and you want quite high accuracy of
> >> >> distance measurements. Some points bleeding over into an adjacent
zone
> >> >> are no problem.
> >> >>
> >> >> All projections are approximations to the earth's spheroid, so
points
> >> >> that are within a single UTM zone have some distortion in their
> >> >> distance or angle relationships. Transforming points that are within
> >> >> an adjacent UTM zone is just an extension of that distortion. You
can
> >> >> compute the precise distance error if you want for the furthest
points
> >> >> by comparing with the geodesic distance.
> >> >>
> >> >> Alternatively you might find there is a coordinate system that spans
> >> >> your dataset nicely - often when a country or an island or a region
> >> >> crosses UTM zones there is an official coordinate system defined
that
> >> >> is used by the authorities there.
> >> >>
> >> >> Also alternatively, there's nothing to stop you defining a
transverse
> >> >> mercator system based on the centre of your data.
> >> >>
> >> >> Barry
> >> >>
> >> >>
> >> >>
> >> >> On Fri, Mar 27, 2015 at 7:44 AM, moses selebatso <
> >> selebatsom at yahoo.co.uk>
> >> >> wrote:
> >> >>> Hello
> >> >>> I have animal movement data that I have converted from Lat/Long to
UTM,
> >> >> unfortunately the data falls in two UTM zones (34S and 35S). For
some
> >> >> reason R cannot display both of them in the same window (the 35S
data is
> >> >> way off the expected location).
> >> >>> The question is how do I convert the data such that R can correctly
> >> read
> >> >> it?
> >> >>> Moses SELEBATSO
> >> >>>
> >> >>> (+267) 318 5219 (H) (+267) 716 393 70 (C)
> >> >>>  (+267) 738 393 70 (C
> >> >>>        [[alternative HTML version deleted]]
> >> >>>
> >> >>> _______________________________________________
> >> >>> R-sig-Geo mailing list
> >> >>> R-sig-Geo at r-project.org
> >> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >> >>
> >> >> _______________________________________________
> >> >> R-sig-Geo mailing list
> >> >> R-sig-Geo at r-project.org
> >> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >> >
> >> >    [[alternative HTML version deleted]]
> >> >
> >> > _______________________________________________
> >> > R-sig-Geo mailing list
> >> > R-sig-Geo at r-project.org
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >
> >         [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From aspiringbodhisattva at gmail.com  Sat Mar 28 19:01:27 2015
From: aspiringbodhisattva at gmail.com (Mike)
Date: Sat, 28 Mar 2015 14:01:27 -0400
Subject: [R-sig-Geo] Complete traversals: Postman problem with shape files
Message-ID: <CAF97oXsfwSyfRAD5SsD=oH8nZMzaHnbtD49zrOV5ur4yN_euHw@mail.gmail.com>

Hi all,

I'm relatively new to the listserv, but learning a lot.  Please excuse me
if this is a semi-related question.

As part of a public health project to survey an area for tobacco retailers,
we're attempting to investigate a systematic, complete, and reasonably
efficient (not necessarily perfect) traversal of all the roads in a certain
catchment area.  This is an application of the classic "postman" problem -
traversing all the edges of a graph - related to the canonical Traveling
Postman Problems / TSPs.   Specifically, we'd be taking regions of VA and
breaking that up into volunteer catchment areas - a county or two - to
drive the area.
http://en.wikipedia.org/wiki/Route_inspection_problem

I've a proof of concept plot and code below that takes a shape file of
regions (in this case, of VA counties) and traverses them in a reasonably
efficient way... but this uses centroids of shapes and creates a distance
matrix off of that, instead of working with the road shape file itself,
which is currently stumping me.

(Below is me muddling through a test case using the counties as a whole,
code and picture.)

I know this is not at all a trivial problem.  Any guidance or expertise
here?  I could make accessible more useful test cases/datasets (like a
single county's roads) if that were helpful.  Or perhaps there are packages
that do this out of the box besides what I'm using?

Best wishes and much respect,
Mike Fliss
UNC Epidemiology PhD student.


[image: Inline image 1]

library("maptools")
library("spdep")
library("sp")
library("TSP")
#data("USCA312")

VAcounties = readShapeSpatial("E:/Mike/GIS/Base shape files/VA Counties/VA
Counties.shp")
plot(VAcounties, border="grey")
neighbors <-poly2nb(VAcounties)
plot(neighbors, coordinates(VAcounties), line="grey", add=TRUE)

VApoints = read.csv("E:/Dropbox/Community/Driving VA/vapoints.csv")
VA.SPDF = SpatialPointsDataFrame(VApoints[,2:3], VApoints[1], proj4string =
CRS("+proj=longlat"))

vamatrix <- read.csv("E:/Dropbox/Community/Driving VA/vacountymatrix.csv",
stringsAsFactors=FALSE)
va2 = as.matrix(vamatrix)
dimnames(va2) = list(names(vamatrix), names(vamatrix))

vatsp = TSP(va2)
vatsp=insert_dummy(vatsp, label="cut")

#tour = solve_TSP(vatsp, method="repetitive_nn", control=list(start=1))
tour = solve_TSP(vatsp, method="2-opt", control=list(rep=100))
path = cut_tour(tour, "cut")

head(labels(path))

plot(VAcounties, border="grey")
plot(VA.SPDF, pch=16, cex=.4, col="red", add=T)
path_line = SpatialLines(list(Lines(list(Line(VA.SPDF[path,])), ID="1")))
plot(path_line, add=T, col="black")
points(VA.SPDF[c(head(path,1), tail(path,1)),], pch = 19, col = "black")

-- 
---
Mike Dolan Fliss, MSW
mike.dolan.fliss at gmail.com
UNC-CH Epidemiology PhD student
NC public health advocate!
-------------------
"We work on ourselves in order to help others,
but also we help others in order to work on ourselves."
  - Pema Chodron

?Upon this gifted age, in its dark hour
Rains from the sky a meteoric shower
Of facts?they lie, unquestioned, uncombined.
Wisdom enough to leach us of our ill
Is daily spun; but there exists no loom
To weave it into a fabric.?
  - Edna St. Vincent Millay, 1939

There are two kinds of people in the world:
Those who can extrapolate from incomplete data.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150328/cfd8f3df/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 21119 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150328/cfd8f3df/attachment.png>

From g.leask at aston.ac.uk  Sun Mar 29 12:50:47 2015
From: g.leask at aston.ac.uk (Leask, Graham)
Date: Sun, 29 Mar 2015 10:50:47 +0000
Subject: [R-sig-Geo] Heat mapping within shape file
Message-ID: <0FE37739-8797-4D05-A795-82C122288CB5@aston.ac.uk>

What is a straightforward way to produce a heat map with the heat map strictly constrained within but filling shape file borders?

If someone can illustrate an example in R I would be most grateful.

Best wishes


Graham

From b.rowlingson at lancaster.ac.uk  Mon Mar 30 13:00:41 2015
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 30 Mar 2015 12:00:41 +0100
Subject: [R-sig-Geo] Heat mapping within shape file
In-Reply-To: <0FE37739-8797-4D05-A795-82C122288CB5@aston.ac.uk>
References: <0FE37739-8797-4D05-A795-82C122288CB5@aston.ac.uk>
Message-ID: <CANVKczOBk7oaP03hwwF7-Qkh9P_GTXfDXrCTYzXVdqraQX8Ang@mail.gmail.com>

On Sun, Mar 29, 2015 at 11:50 AM, Leask, Graham <g.leask at aston.ac.uk> wrote:
> What is a straightforward way to produce a heat map with the heat map strictly constrained within but filling shape file borders?

 If your heat map is a raster then there's functions in the raster
package to mask rasters to polygons.

> If someone can illustrate an example in R I would be most grateful.

 Where are you starting from? Do you know how to read shapefiles into R?

This question on gis.stackexchange.com might help. It uses "mask" from
the raster package to crop a raster:

http://gis.stackexchange.com/questions/61243/clipping-a-raster-in-r

Barry (aka spacedman on gis.stackexchange.com...)


From mathieu.rajerison at gmail.com  Mon Mar 30 15:25:51 2015
From: mathieu.rajerison at gmail.com (Mathieu Rajerison)
Date: Mon, 30 Mar 2015 15:25:51 +0200
Subject: [R-sig-Geo] points sampled along a Line don't seem to intersect
	it
In-Reply-To: <alpine.LFD.2.11.1503232008510.554@reclus.nhh.no>
References: <CAGfc75m7MOt9SrMOriEGLDSePM1dmEeSYFKgXCcd9iOVLJkbrQ@mail.gmail.com>
	<CANVKczPDz=F6fBNeybaMjVw1Nh4fPe-jEyXRZJkLkmJ0Hy7igw@mail.gmail.com>
	<alpine.LFD.2.11.1503232008510.554@reclus.nhh.no>
Message-ID: <CAGfc75myLhh6yVQbU7nQPJOg4yVeVGpYZ15aohbVCRTfy6V9bA@mail.gmail.com>

Ok thanks for the answer.

Your explanations are very technical as they go deep into the package's
code, and GEOS specifications.

Finally, I used spatstat::project2Segment to get my points on the lines.

Mathieu

2015-03-23 20:30 GMT+01:00 Roger Bivand <Roger.Bivand at nhh.no>:

> On Mon, 23 Mar 2015, Barry Rowlingson wrote:
>
>  On Mon, Mar 23, 2015 at 12:07 PM, Mathieu Rajerison
>> <mathieu.rajerison at gmail.com> wrote:
>>
>>> pts <- spsample(rds.un, type="regular", n=100)
>>>
>>
>> Your points are at least 3.9x10^-17 units from the lines:
>>
>> gDistance(pts, rds.un)
>> [1] 3.908727e-17
>>
>> I can only think this is due to a difference in how spsample and how
>> rgeos interpolate lines from points. FAQ 7.31 in disguise?
>>
>> I suspect you could use gBuffer on your points with a buffer width of
>> 1e-10 to get a reliable intersection, but that would be a line segment
>> (or segments if the point is close to a junction) but you could
>> possibly just take a single point from that line geometry and in the
>> worst case scenario you're only 1e-10 out...
>>
>>
> Had the fuzzyMM package been written in a modularised way (it places
> vehicle GPS positions on OSM road lines, and needs velocity etc.), it might
> have given a starting point. As Barry says, finding which road is closest
> to each point is trivial (I projected too, to get more digits), but finding
> the nearest GEOS point on the road isn't, because GEOS discretises using
> getScale() to go to an integer grid. spsample() is also very pushed to get
> n here, so the # pts is way under.
>
> If anyone would like a brain teaser, this should be attractive!
>
> Roger
>
>
>>
>> Barry
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; fax +47 55 95 91 00
> e-mail: Roger.Bivand at nhh.no
>
>

	[[alternative HTML version deleted]]


From mathieu.rajerison at gmail.com  Mon Mar 30 15:55:14 2015
From: mathieu.rajerison at gmail.com (Mathieu Rajerison)
Date: Mon, 30 Mar 2015 15:55:14 +0200
Subject: [R-sig-Geo] Complete traversals: Postman problem with shape
	files
In-Reply-To: <CAF97oXsfwSyfRAD5SsD=oH8nZMzaHnbtD49zrOV5ur4yN_euHw@mail.gmail.com>
References: <CAF97oXsfwSyfRAD5SsD=oH8nZMzaHnbtD49zrOV5ur4yN_euHw@mail.gmail.com>
Message-ID: <CAGfc75negAsXVd7a+Xyixynm1zHqvhfbADpcDWWzKfX52i-nNw@mail.gmail.com>

Hi,

Maybe you could have a look at v.net.salesman from GRASS.

Please note that GRASS can be reached while using R with spgrass06 package.

Best,

mathieu

2015-03-28 19:01 GMT+01:00 Mike <aspiringbodhisattva at gmail.com>:

> Hi all,
>
> I'm relatively new to the listserv, but learning a lot.  Please excuse me
> if this is a semi-related question.
>
> As part of a public health project to survey an area for tobacco
> retailers, we're attempting to investigate a systematic, complete, and
> reasonably efficient (not necessarily perfect) traversal of all the roads
> in a certain catchment area.  This is an application of the classic
> "postman" problem - traversing all the edges of a graph - related to the
> canonical Traveling Postman Problems / TSPs.   Specifically, we'd be taking
> regions of VA and breaking that up into volunteer catchment areas - a
> county or two - to drive the area.
> http://en.wikipedia.org/wiki/Route_inspection_problem
>
> I've a proof of concept plot and code below that takes a shape file of
> regions (in this case, of VA counties) and traverses them in a reasonably
> efficient way... but this uses centroids of shapes and creates a distance
> matrix off of that, instead of working with the road shape file itself,
> which is currently stumping me.
>
> (Below is me muddling through a test case using the counties as a whole,
> code and picture.)
>
> I know this is not at all a trivial problem.  Any guidance or expertise
> here?  I could make accessible more useful test cases/datasets (like a
> single county's roads) if that were helpful.  Or perhaps there are packages
> that do this out of the box besides what I'm using?
>
> Best wishes and much respect,
> Mike Fliss
> UNC Epidemiology PhD student.
>
>
> [image: Inline image 1]
>
> library("maptools")
> library("spdep")
> library("sp")
> library("TSP")
> #data("USCA312")
>
> VAcounties = readShapeSpatial("E:/Mike/GIS/Base shape files/VA Counties/VA
> Counties.shp")
> plot(VAcounties, border="grey")
> neighbors <-poly2nb(VAcounties)
> plot(neighbors, coordinates(VAcounties), line="grey", add=TRUE)
>
> VApoints = read.csv("E:/Dropbox/Community/Driving VA/vapoints.csv")
> VA.SPDF = SpatialPointsDataFrame(VApoints[,2:3], VApoints[1], proj4string
> = CRS("+proj=longlat"))
>
> vamatrix <- read.csv("E:/Dropbox/Community/Driving VA/vacountymatrix.csv",
> stringsAsFactors=FALSE)
> va2 = as.matrix(vamatrix)
> dimnames(va2) = list(names(vamatrix), names(vamatrix))
>
> vatsp = TSP(va2)
> vatsp=insert_dummy(vatsp, label="cut")
>
> #tour = solve_TSP(vatsp, method="repetitive_nn", control=list(start=1))
> tour = solve_TSP(vatsp, method="2-opt", control=list(rep=100))
> path = cut_tour(tour, "cut")
>
> head(labels(path))
>
> plot(VAcounties, border="grey")
> plot(VA.SPDF, pch=16, cex=.4, col="red", add=T)
> path_line = SpatialLines(list(Lines(list(Line(VA.SPDF[path,])), ID="1")))
> plot(path_line, add=T, col="black")
> points(VA.SPDF[c(head(path,1), tail(path,1)),], pch = 19, col = "black")
>
> --
> ---
> Mike Dolan Fliss, MSW
> mike.dolan.fliss at gmail.com
> UNC-CH Epidemiology PhD student
> NC public health advocate!
> -------------------
> "We work on ourselves in order to help others,
> but also we help others in order to work on ourselves."
>   - Pema Chodron
>
> ?Upon this gifted age, in its dark hour
> Rains from the sky a meteoric shower
> Of facts?they lie, unquestioned, uncombined.
> Wisdom enough to leach us of our ill
> Is daily spun; but there exists no loom
> To weave it into a fabric.?
>   - Edna St. Vincent Millay, 1939
>
> There are two kinds of people in the world:
> Those who can extrapolate from incomplete data.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150330/bc856b59/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image.png
Type: image/png
Size: 21119 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150330/bc856b59/attachment.png>

From doanthanhthuy209 at gmail.com  Mon Mar 30 16:14:06 2015
From: doanthanhthuy209 at gmail.com (Doan Thanh Thuy)
Date: Mon, 30 Mar 2015 16:14:06 +0200
Subject: [R-sig-Geo] Problem giving points back to original location in
	raster in R
Message-ID: <CA+R5XayUxU0zN2mDT_vMsMMA3GNpCLXiCDnPgZdYGMhxaEnGuw@mail.gmail.com>

Dear all,
I am doing Latin Hypercube Sampling (LHS) in R to find the best location to
sample new soil data points. Firstly, I have 5 raster maps of NDVI, Slope,
TWI, Altitude, Geology which were then imported in R. All those data were
collected into a data frame in R to be the input of LHS. The results of LHS
will be the list of points at which I should take soil sample.
However, the problem is that I don't know how to put those resultant points
back to the raster map. Can anyone help me with this kind of stupid
question please? I am at the very early stage in R :)
Thanks in advance!
Best regards,
Thuy

Doan Thanh Thuy
Department of Land Information System
Faculty of Land Management
Vietnam national University of Agriculture
Mobile: +841689686205
Email: doanthanhthuy209 at gmail.com, ThanhThuy.Doan at UGent.be

	[[alternative HTML version deleted]]


From dncgst at gnewarchaeology.it  Mon Mar 30 15:39:56 2015
From: dncgst at gnewarchaeology.it (Domenico Giusti)
Date: Mon, 30 Mar 2015 15:39:56 +0200
Subject: [R-sig-Geo] R 3.1.3 - GNU/Linux - readOGR -- Layer not found
Message-ID: <5519522C.5080404@gnewarchaeology.it>

Dear list,

I cannot read some of the layers (table and view) from a
Postgres/PostGIS DB.

Error in ogrInfo.

> ogrListLayers("PG:dbname=dbname")

doesn't list all the tables and views. e.g. I have two views with both a
3D point geom, but only one of them is in the list.

What could be the problem?

Thanks,

> sessionInfo()
R version 3.1.3 (2015-03-09)
Platform: i486-pc-linux-gnu (32-bit)
Running under: Debian GNU/Linux 7 (wheezy)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
LC_TIME=en_US.UTF-8
 [4] LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8
LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C

[10] LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8
LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] rgdal_0.9-2 sp_1.0-17

loaded via a namespace (and not attached):
[1] grid_3.1.3      lattice_0.20-30 tools_3.1.3

-- 
Domenico Giusti <dncgst at gnewarchaeology.it>
GPG keyID: 2048R/A3AB7054F6E5D778


From jimmyjmv at hotmail.com  Mon Mar 30 19:13:54 2015
From: jimmyjmv at hotmail.com (Jimmy Neutron)
Date: Mon, 30 Mar 2015 10:13:54 -0700
Subject: [R-sig-Geo] How to work with Poisson distribution (count data)?
Message-ID: <BLU182-W5816D072E15B54CCFF5434C4F50@phx.gbl>

Dear comRades:
I realized that I have to work with Poisson, because I have 'count data' with geographic reference. Then, it is 'geodata', such as:
       xUTMkm   yUTMkm DyF1385 450.1202 1011.425   01386 450.4273 1007.219   01387 450.2584 1011.884   01388 450.1696 1010.261   01389 450.1718 1009.887   01390 450.6981 1004.379   0...
I read the geoRglm structure. What does it mean that I have to make a empiric variogram?.
What I used when I worked with Binomial model was as following:a2008.posCEROS.spmod<-list(cov.pars=c(1,20),beta=1.0,cov.model="matern",nugget=0,kappa=0.35,family="binomial",link="logit")
Then, is it right to write my Poisson model as following?:a2008.posCEROS.spmod<-list(cov.pars=c(1,20),beta=1.0,cov.model="matern",nugget=0,kappa=0.35,family="poisson",link="logit")
Do I have to calculate nugget and kappa by some method (like likelihood) from my 'count data' or it is merely a theoretical model?.
After that, I will generate MCMC simulations, such as:
a2008.posCEROS.mcmc<-mcmc.control(S.scale=0.582, thin=10) #mcmc marc of change monte carlo, S.scalea2008.posCEROS.tune<-glsm.mcmc(a2008.posCEROSbin, model=a2008.posCEROS.spmod, mcmc.input=a2008.posCEROS.mcmc)
My goal is to predict how many DyF do I have in the survey.
Thanks in advance for youR help. 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150330/1de2b35c/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2008TrialsSuccesses.gif
Type: image/gif
Size: 15953 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150330/1de2b35c/attachment.gif>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2008ploteo.gif
Type: image/gif
Size: 11859 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150330/1de2b35c/attachment-0001.gif>

From jimmyjmv at hotmail.com  Mon Mar 30 23:13:20 2015
From: jimmyjmv at hotmail.com (Jimmy Neutron)
Date: Mon, 30 Mar 2015 14:13:20 -0700
Subject: [R-sig-Geo] How do I have to work with Poisson distribution with
 geoRglm (count data)?
In-Reply-To: <BLU182-W5816D072E15B54CCFF5434C4F50@phx.gbl>
References: <BLU182-W5816D072E15B54CCFF5434C4F50@phx.gbl>
Message-ID: <BLU182-W2408CB12EA0403AE232543C4F50@phx.gbl>

Dear comRades:
Because of my data, I have just realized that I have to work with Poisson, because I have 'count data' with geographic reference. Then, my my 'geodata' is such as:
       xUTMkm   yUTMkm 'Count'1385 450.1202 1011.425   01386 450.4273 1007.219   01387 450.2584 1011.884   01388 450.1696 1010.261   01389 450.1718 1009.887   01390 450.6981 1004.379   0...
I read the geoRglm structure. What does it mean that I have to make a empiric variogram?.
When I worked with Binomial model (True, False), my script was as following:a2008.posCEROS.spmod<-list(cov.pars=c(1,20),beta=1.0,cov.model="matern",nugget=0,kappa=0.35,family="binomial",link="logit")
Then, I suppose that my Poisson model would be as following:a2008.posCEROS.spmod<-list(cov.pars=c(1,20),beta=1.0,cov.model="matern",nugget=0,kappa=0.35,family="poisson",link="logit")
Do I have to calculate nugget and kappa by some method (like likelihood) from my 'count data' or it is merely a theoretical model?. Is my Poisson model right?.
After that, I will generate MCMC simulations, such as:
a2008.posCEROS.mcmc<-mcmc.control(S.scale=0.582, thin=10) #mcmc marc of change monte carlo, S.scalea2008.posCEROS.tune<-glsm.mcmc(a2008.posCEROSbin, model=a2008.posCEROS.spmod, mcmc.input=a2008.posCEROS.mcmc)
My goal is to predict how many 'Count' do I have in the survey.
Thanks in advance for youR help. 		 	   		  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150330/8b134a4d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2008TrialsSuccesses.gif
Type: image/gif
Size: 15953 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150330/8b134a4d/attachment.gif>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2008ploteo.gif
Type: image/gif
Size: 11859 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150330/8b134a4d/attachment-0001.gif>

From amit.boshale at yahoo.com  Tue Mar 31 11:29:55 2015
From: amit.boshale at yahoo.com (Amit Boshale)
Date: Tue, 31 Mar 2015 09:29:55 +0000 (UTC)
Subject: [R-sig-Geo] MODIS Whittaker Smoothing multicore support
Message-ID: <1300944116.1236406.1427794195897.JavaMail.yahoo@mail.yahoo.com>

Dear Matteo,
Are there any plans to enable multicore in Whittaker smoothing function? I use your MODIS package on regular basis and multicore calculations would make my life easier.

Keep up the great work
Amit
	[[alternative HTML version deleted]]


From forrest at ufl.edu  Tue Mar 31 15:26:41 2015
From: forrest at ufl.edu (Forrest Stevens)
Date: Tue, 31 Mar 2015 09:26:41 -0400
Subject: [R-sig-Geo] MODIS Whittaker Smoothing multicore support
In-Reply-To: <1300944116.1236406.1427794195897.JavaMail.yahoo@mail.yahoo.com>
References: <1300944116.1236406.1427794195897.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAEBQMMnc7ZQ-Aj9mmwDC7O4xcTnpzo56mk_yQL5T8L1UDq=xbQ@mail.gmail.com>

Hi Amit, I can provide you with some help to get smoothing working to
leverage multiple cores.  This is already baked into the
whittaker.raster() function of our MODIS package and it makes sense to
use it since the problem is of the "embarrassingly parallel" variety.
To leverage multiple cores you can do something like the following:


#beginCluster(type="SOCK",exclude="MODIS")
##  Alternatively:
nodes <- 4
beginCluster(nodes)
system.time(whittaker.raster(vi=vi, wt=qa, inT=time,
timeInfo=timeInfo, groupYears=FALSE,
outDirPath=paste(root_path,"output/",sep="")))
endCluster()

##  Where vi, qa, time, timeInfo are all arguments you can learn about
in the whittaker.raster() documentation.


Hope this helps!

Sincerely,
Forrest


On Tue, Mar 31, 2015 at 5:33 AM Amit Boshale via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
>
> Dear Matteo,
> Are there any plans to enable multicore in Whittaker smoothing function? I use your MODIS package on regular basis and multicore calculations would make my life easier.
>
> Keep up the great work
> Amit
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From wandrson01 at gmail.com  Tue Mar 31 19:55:18 2015
From: wandrson01 at gmail.com (Walter Anderson)
Date: Tue, 31 Mar 2015 12:55:18 -0500
Subject: [R-sig-Geo] How to count unique TRUE's
Message-ID: <551ADF86.1070707@gmail.com>

I have a buffered project SpatialPolygonDataFrame and a
SpatialPointDataFrame that I want to intersect (one project at a time)
and count the number of points within each polygon's footprint.

I am using the following code:

for (i in 1:numprjs)
{
	curprj <- as.character(output at PROJECT[i])
	tmp <- subset(buf2640, buf2640$PROJECT == curprj)
	tmp2 <- as.vector(gIntersects(tmp, ac, byid=TRUE))
	output$AC[i] <- length(tmp[tmp2 == TRUE])
}

The problem is that when the project's polygons cover more than one
overlapping shapes (derived from multiple sub-projects that are given
the same project id), the above code can multiple count a single ac
point if it falls within the boundaries of two or more of a single
projects polygons.

As an example project 'B1103' has two points within its two polygons;
however, the above code reports three.  I believe the following extract
of the core gIntersects command shows why

        2     3
122 FALSE  TRUE
313  TRUE  TRUE

So clearly the problem is my use of as.vector, which is taking all of
the columns and combining them into a single vector; however, I am
unsure of the best way to count the number of true in the original data
structure created by the gIntersects command.

Walter Anderson


From andre.stumpf at univ-brest.fr  Tue Mar 31 20:08:48 2015
From: andre.stumpf at univ-brest.fr (=?windows-1252?Q?stumpf_andr=E9?=)
Date: Tue, 31 Mar 2015 20:08:48 +0200
Subject: [R-sig-Geo] Speeding up foreach loop for local neighborhood
	operation on a RasterStack
In-Reply-To: <551AE100.6030102@univ-brest.fr>
References: <2136484184.3691307.1427442268544.JavaMail.yahoo@mail.yahoo.com>	<CANVKczPJ72GAZnVKyav-jQN4WUU86FAoxOrxV61WYHV+QjNkig@mail.gmail.com>	<CAAcGz9__G+zfjy_Cp_Jnz0WeJz98DM2XS_7HaRxrjGu9XZkk_g@mail.gmail.com>	<0122DA0E-814E-443A-A069-EC51908F5D7F@gmail.com>	<CAAcGz9-ZuHREFqj4jSpkUgEPGBksci3YBqbbqBEVnAdxAMNZzw@mail.gmail.com>	<CANtt_hxXChRi=fqWNeVzzP5PrCE4fOHTm60RZmLgax25MoPrxA@mail.gmail.com>
	<CANVKczM3Rn-vRZf4+J4pcV5=Bi+ft0RVaU6kbHuVVSW0iA9uPw@mail.gmail.com>
	<551AE100.6030102@univ-brest.fr>
Message-ID: <551AE2B0.1010907@univ-brest.fr>

Dear list members,

I'm trying to implement an analysis that applies PCA on local
neighborhoods (e.g. 5x5 pixel)
over a large raster stack. To speed up processing I'm using foreach but
run into some performance
issues. Please find below an example that gives the general idea. (Sorry
for the long example...questions at the end)

library(raster)
library(doParallel)

#create a raster
rast <- raster(ncol=100, nrow=100)
rast[] <- runif(100*100)
crs(rast) <- "+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs
+ellps=WGS84 +towgs84=0,0,0"
bb <- extent(310883.5, 310883.5+50, 4918798, 4918798+50)
extent(rast) <- bb

#create two stacks
stack.rast1 <- rast
while (nlayers(stack.rast1)<=20) {
  tmp.rast <- rast-2*runif(1)
  stack.rast1 <- addLayer(stack.rast1,tmp.rast)
}

stack.rast2 <- rast
while (nlayers(stack.rast2)<=20) {
  tmp.rast <- rast-2*runif(1)
  stack.rast2 <- addLayer(stack.rast2,tmp.rast)
}

# only pixel exceeding a certain threshold in the first layer should be
analyzed
thresh <- 0.5
thresh.rast <- rast > thresh
freq(thresh.rast)

# get indices of 1's
cells.ind <- Which(rast > thresh, cells=TRUE)
pixel.ind <- rowColFromCell(thresh.rast, cells.ind)
nrows <- 5
ncols <- 5

# wrap the borders with zeros
stack.rast1 <- extend(stack.rast1, c(nrows,ncols), value=0)
stack.rast2 <- extend(stack.rast2, c(nrows,ncols), value=0)
pixel.ind <- pixel.ind + nrows

# initialize cluster and loop over all indexed pixel
cl <- makeCluster(8)
registerDoParallel(cl)
out.test <- NULL

out.test <-  foreach (i=1:nrow(pixel.ind), .packages='raster',
.combine='c') %dopar% {
  j <- pixel.ind[i,1]
  k <- pixel.ind[i,2]
  tmp.x <- as.vector(getValuesBlock(stack.rast1, row=j, nrows=5, col=k,
ncols=5, lyrs=1:20))
  tmp.y <- as.vector(getValuesBlock(stack.rast2, row=j, nrows=5,
col=k,ncols=5, lyrs=1:20))
  r <- prcomp(cbind(tmp.x, tmp.y))
  (r$sdev[1]^2 / sum(r$sdev^2))
}

stopCluster(cl)

# write
writeRaster(stack.rast1,file="~/RasterStack1.tif",
format="GTiff",overwrite=TRUE)
writeRaster(stack.rast2,file="~/RasterStack2.tif",
format="GTiff",overwrite=TRUE)

# read back and run the loop
stack.rast1 <- stack("~/RasterStack1.tif")
stack.rast2 <- stack("~/RasterStack2.tif")
# the loop now takes forever

# so I ran a simple operation to read the raster back in memory
stack.rast1 <- stack.rast1*1
stack.rast2 <- stack.rast2*1
# now the loops executes as fast as when the stack was original created

Questions:
1. Getting the values out of the raster stack takes most of the time:
Can anyone think of a faster way to access the local values in the stack?

system.time( r <- prcomp(cbind(tmp.x, tmp.y)))
user  system elapsed
0.000   0.000   0.001

system.time(tmp.x <- as.vector(getValuesBlock(stack.rast1,
row=j,nrows=5, col=k, ncols=5, lyrs=1:20)))
user  system elapsed
0.003   0.000   0.002

2. Running an algebraic operation to get the raster back into memory is
maybe a bit clumsy. Is that the right way to do it?

Kind regards, and many thanks in advance for any hints,
Andr?

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20150331/151d689d/attachment.bin>

From tephilippi at gmail.com  Tue Mar 31 20:29:59 2015
From: tephilippi at gmail.com (Tom Philippi)
Date: Tue, 31 Mar 2015 11:29:59 -0700
Subject: [R-sig-Geo] How to count unique TRUE's
In-Reply-To: <551ADF86.1070707@gmail.com>
References: <551ADF86.1070707@gmail.com>
Message-ID: <CALyPt8wEO_N90O2YJn12e_qC_dmqMMFXsyu=wKT0XAq5ByO3ag@mail.gmail.com>

I can't quite make sense of what your objects look like, but I suspect you
need to change byid to a vector, so you are byid for the points but not the
polygons byid=c(FALSE,TRUE) or vice versa.


On Tue, Mar 31, 2015 at 10:55 AM, Walter Anderson <wandrson01 at gmail.com>
wrote:

> I have a buffered project SpatialPolygonDataFrame and a
> SpatialPointDataFrame that I want to intersect (one project at a time)
> and count the number of points within each polygon's footprint.
>
> I am using the following code:
>
> for (i in 1:numprjs)
> {
>         curprj <- as.character(output at PROJECT[i])
>         tmp <- subset(buf2640, buf2640$PROJECT == curprj)
>         tmp2 <- as.vector(gIntersects(tmp, ac, byid=TRUE))
>         output$AC[i] <- length(tmp[tmp2 == TRUE])
> }
>
> The problem is that when the project's polygons cover more than one
> overlapping shapes (derived from multiple sub-projects that are given
> the same project id), the above code can multiple count a single ac
> point if it falls within the boundaries of two or more of a single
> projects polygons.
>
> As an example project 'B1103' has two points within its two polygons;
> however, the above code reports three.  I believe the following extract
> of the core gIntersects command shows why
>
>         2     3
> 122 FALSE  TRUE
> 313  TRUE  TRUE
>
> So clearly the problem is my use of as.vector, which is taking all of
> the columns and combining them into a single vector; however, I am
> unsure of the best way to count the number of true in the original data
> structure created by the gIntersects command.
>
> Walter Anderson
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From forrest at ufl.edu  Tue Mar 31 20:56:13 2015
From: forrest at ufl.edu (Forrest Stevens)
Date: Tue, 31 Mar 2015 14:56:13 -0400
Subject: [R-sig-Geo] How to count unique TRUE's
In-Reply-To: <CALyPt8wEO_N90O2YJn12e_qC_dmqMMFXsyu=wKT0XAq5ByO3ag@mail.gmail.com>
References: <551ADF86.1070707@gmail.com>
	<CALyPt8wEO_N90O2YJn12e_qC_dmqMMFXsyu=wKT0XAq5ByO3ag@mail.gmail.com>
Message-ID: <CAEBQMMk6+zZramhdZDCH1sF1m86rfqu7Obrz2SjQwCXHNHAzHg@mail.gmail.com>

I'm guessing based on the data and situation you describe that some
variant of this would probably get you close:


##  Sample data:
d <- data.frame(A=c(T,F,F,T), B=c(F,T,F,T))

##  Count row-wise trues:
sum( apply(d, MARGIN=1, sum) >= 1 )


Hope that helps,
Forrest
--
Forrest R. Stevens
Ph.D. Candidate, QSE3 IGERT Fellow
Department of Geography
Land Use and Environmental Change Institute
University of Florida
www.clas.ufl.edu/users/forrest


On Tue, Mar 31, 2015 at 2:29 PM, Tom Philippi <tephilippi at gmail.com> wrote:
> I can't quite make sense of what your objects look like, but I suspect you
> need to change byid to a vector, so you are byid for the points but not the
> polygons byid=c(FALSE,TRUE) or vice versa.
>
>
> On Tue, Mar 31, 2015 at 10:55 AM, Walter Anderson <wandrson01 at gmail.com>
> wrote:
>
>> I have a buffered project SpatialPolygonDataFrame and a
>> SpatialPointDataFrame that I want to intersect (one project at a time)
>> and count the number of points within each polygon's footprint.
>>
>> I am using the following code:
>>
>> for (i in 1:numprjs)
>> {
>>         curprj <- as.character(output at PROJECT[i])
>>         tmp <- subset(buf2640, buf2640$PROJECT == curprj)
>>         tmp2 <- as.vector(gIntersects(tmp, ac, byid=TRUE))
>>         output$AC[i] <- length(tmp[tmp2 == TRUE])
>> }
>>
>> The problem is that when the project's polygons cover more than one
>> overlapping shapes (derived from multiple sub-projects that are given
>> the same project id), the above code can multiple count a single ac
>> point if it falls within the boundaries of two or more of a single
>> projects polygons.
>>
>> As an example project 'B1103' has two points within its two polygons;
>> however, the above code reports three.  I believe the following extract
>> of the core gIntersects command shows why
>>
>>         2     3
>> 122 FALSE  TRUE
>> 313  TRUE  TRUE
>>
>> So clearly the problem is my use of as.vector, which is taking all of
>> the columns and combining them into a single vector; however, I am
>> unsure of the best way to count the number of true in the original data
>> structure created by the gIntersects command.
>>
>> Walter Anderson
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From r.hijmans at gmail.com  Tue Mar 31 21:44:56 2015
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Tue, 31 Mar 2015 12:44:56 -0700
Subject: [R-sig-Geo] How to count unique TRUE's
In-Reply-To: <CAEBQMMk6+zZramhdZDCH1sF1m86rfqu7Obrz2SjQwCXHNHAzHg@mail.gmail.com>
References: <551ADF86.1070707@gmail.com>
	<CALyPt8wEO_N90O2YJn12e_qC_dmqMMFXsyu=wKT0XAq5ByO3ag@mail.gmail.com>
	<CAEBQMMk6+zZramhdZDCH1sF1m86rfqu7Obrz2SjQwCXHNHAzHg@mail.gmail.com>
Message-ID: <CANtt_hxRHeDLxfCV99Fd5W-R15hEauZ3BxktrUsCrzR2mzVjKA@mail.gmail.com>

Forrest, it seems to me that you can simplify and do

output$AC[i] <- length(gIntersection(tmp, ac))

instead of

tmp2 <- as.vector(gIntersects(tmp, ac, byid=TRUE))
output$AC[i] <- length(tmp[tmp2 == TRUE])

Robert

On Tue, Mar 31, 2015 at 11:56 AM, Forrest Stevens <forrest at ufl.edu> wrote:
> I'm guessing based on the data and situation you describe that some
> variant of this would probably get you close:
>
>
> ##  Sample data:
> d <- data.frame(A=c(T,F,F,T), B=c(F,T,F,T))
>
> ##  Count row-wise trues:
> sum( apply(d, MARGIN=1, sum) >= 1 )
>
>
> Hope that helps,
> Forrest
> --
> Forrest R. Stevens
> Ph.D. Candidate, QSE3 IGERT Fellow
> Department of Geography
> Land Use and Environmental Change Institute
> University of Florida
> www.clas.ufl.edu/users/forrest
>
>
> On Tue, Mar 31, 2015 at 2:29 PM, Tom Philippi <tephilippi at gmail.com> wrote:
>> I can't quite make sense of what your objects look like, but I suspect you
>> need to change byid to a vector, so you are byid for the points but not the
>> polygons byid=c(FALSE,TRUE) or vice versa.
>>
>>
>> On Tue, Mar 31, 2015 at 10:55 AM, Walter Anderson <wandrson01 at gmail.com>
>> wrote:
>>
>>> I have a buffered project SpatialPolygonDataFrame and a
>>> SpatialPointDataFrame that I want to intersect (one project at a time)
>>> and count the number of points within each polygon's footprint.
>>>
>>> I am using the following code:
>>>
>>> for (i in 1:numprjs)
>>> {
>>>         curprj <- as.character(output at PROJECT[i])
>>>         tmp <- subset(buf2640, buf2640$PROJECT == curprj)
>>>         tmp2 <- as.vector(gIntersects(tmp, ac, byid=TRUE))
>>>         output$AC[i] <- length(tmp[tmp2 == TRUE])
>>> }
>>>
>>> The problem is that when the project's polygons cover more than one
>>> overlapping shapes (derived from multiple sub-projects that are given
>>> the same project id), the above code can multiple count a single ac
>>> point if it falls within the boundaries of two or more of a single
>>> projects polygons.
>>>
>>> As an example project 'B1103' has two points within its two polygons;
>>> however, the above code reports three.  I believe the following extract
>>> of the core gIntersects command shows why
>>>
>>>         2     3
>>> 122 FALSE  TRUE
>>> 313  TRUE  TRUE
>>>
>>> So clearly the problem is my use of as.vector, which is taking all of
>>> the columns and combining them into a single vector; however, I am
>>> unsure of the best way to count the number of true in the original data
>>> structure created by the gIntersects command.
>>>
>>> Walter Anderson
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From strimas at zoology.ubc.ca  Tue Mar 31 22:28:39 2015
From: strimas at zoology.ubc.ca (Matt Strimas-Mackey)
Date: Tue, 31 Mar 2015 13:28:39 -0700
Subject: [R-sig-Geo] Include multiple SpatialPolygons objects in legend of
	spplot
Message-ID: <CAHb9yCA2WCrx8zuFziLF4e06r6u-Oc6Sg55+rYNsqD2H_LgxVw@mail.gmail.com>

Hi,

I have two shapefiles: one with polygons of plantations and another
with polygons of protected areas. There are multiple types of
plantations and multiple types of parks. I am able to plot both of
these shapefiles on one map with spplot by including the second SPDF
object in the sp.layout parameter of spplot. The legend looks great
for the first SPDF object, but the polygons passed to sp.layout are
not included in the legend. Is there some way to do this, i.e. have a
legend that accounts for multiple spatial objects? I know I could
union them together, but I'd like to avoid this workaround if
possible.

In addition, is there a way to alter the legend labels for a
categorical legend? I know how to do this using the colorkey parameter
with continuous legends, but can't figure out how to apply this to
factor variables. For example, if my attributes are codes like 'NP'
and 'SP', could I instead label the legend 'National Park' and 'State
Park'.

Simplified example below. Thanks!

library(sp)
library(raster)
# Make some fake data
# 2 SPDF objects each with one factor attribute
# Plantations
plantation <- union(union(as(extent(0, 0.5, 1, 1.5), 'SpatialPolygons'),
             as(extent(2.5, 3, 0, 1), 'SpatialPolygons')),
             as(extent(1, 1.5, 1, 2), 'SpatialPolygons'))
plantation <- SpatialPolygonsDataFrame(plantation,
                        data.frame(field=c('Timber','Rubber',
'Acacia')), match.ID=F)

# Protected Areas
protected <- union(as(extent(0, 1, 0, 1), 'SpatialPolygons'),
             as(extent(1.5, 2, 0.5, 2), 'SpatialPolygons'))
protected <- SpatialPolygonsDataFrame(protected,
                        data.frame(field=c('NP','SP')), match.ID=F)

# Plot
l <- list('sp.polygons', protected, col='transparent', fill='green')
spplot(plantation, sp.layout=l, scales=list(draw=T))
# I want to add the protected areas to the legend


