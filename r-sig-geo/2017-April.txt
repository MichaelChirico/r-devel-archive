From p.schwager at gmx.at  Mon Apr  3 14:30:35 2017
From: p.schwager at gmx.at (Patrick Schwager)
Date: Mon, 3 Apr 2017 14:30:35 +0200
Subject: [R-sig-Geo] (no subject)
Message-ID: <CAD6JHaaYs1jMHsywQCK6jmYeMS2WSRuaN=JhfgBamK6i7ZAeXg@mail.gmail.com>

Dear list,

I am currently working on modelling species distribution and try to account
spatial autocorrelation. I use R and I successfully managed to incorporate
moran's eigenvectors as predictor to my species distribution model using
GLM (logit). I followed Dormann et al. (2007
<https://www2.unil.ch/biomapper/Download/Dormann-EcoGra-2007.pdf>) and the
appendix
<http://www.ecography.org/sites/ecography.org/files/appendix/e5171.pdf>. I
think I got corrected statistics in the moran.test() of model residuals as
the p-value increases. Also the AIC score indicates a better spatial model.


*Normal model glm:*

Moran I test under randomisation



data:  residuals(model)

weights: priclus.listw



Moran I statistic standard deviate = 7.5632, p-value = 1.966e-14

alternative hypothesis: greater

sample estimates:

Moran I statistic       Expectation          Variance

      0.264335666      -0.007633588       0.001293086



*Spatial model:*

Moran I test under randomisation



data:  residuals(model)

weights: priclus.listw



Moran I statistic standard deviate = 1.5572, p-value = 0.05972

alternative hypothesis: greater

sample estimates:

Moran I statistic       Expectation          Variance

      0.045968614      -0.007633588       0.001184932



*See the summary for both models below:*

> summary(priclus8<- glm(pb_train ~ gesteine + schnee_tag_1 + rs_hospso, family= binomial(link="logit"), data=envtrain))



        Call:

        glm(formula = pb_train ~ gesteine + schnee_tag_1 + rs_hospso,

            family = binomial(link = "logit"), data = envtrain)



        Deviance Residuals:

             Min        1Q    Median        3Q       Max

        -2.76552  -0.18741  -0.00449   0.34032   2.03205



        Coefficients:

                     Estimate Std. Error z value Pr(>|z|)

        (Intercept)  75.89890   20.21051   3.755 0.000173 ***

        gesteine1     2.05287    0.55421   3.704 0.000212 ***

        schnee_tag_1 -0.03328    0.01685  -1.975 0.048223 *

        rs_hospso    -1.56444    0.37856  -4.133 3.59e-05 ***

        ---

        Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1



        (Dispersion parameter for binomial family taken to be 1)



            Null deviance: 387.96  on 284  degrees of freedom

        Residual deviance: 129.56  on 281  degrees of freedom

        AIC: 137.56

        Number of Fisher Scoring iterations: 7



> sevm1 <- fitted(ME(pb_train ~ gesteine + schnee_tag_1 + rs_hospso, data=envtrain, family= binomial(link="logit"),listw=ME.listw))

> summary(priclus8_mem<- glm(pb_train ~ gesteine + schnee_tag_1 + rs_hospso+ I(sevm1), family= binomial(link="logit"), data=envtrain) )



    Call:

    glm(formula = pb_train ~ gesteine + schnee_tag_1 + rs_hospso +

        I(sevm1), family = binomial(link = "logit"), data = envtrain)



    Deviance Residuals:

        Min       1Q   Median       3Q      Max

    -3.6095  -0.1415  -0.0025   0.0784   2.5099



    Coefficients:

                   Estimate Std. Error z value Pr(>|z|)

    (Intercept)   102.50545   28.27283   3.626 0.000288 ***

    gesteine1       0.80346    0.79373   1.012 0.311417

    schnee_tag_1   -0.06435    0.02448  -2.628 0.008586 **

    rs_hospso      -1.99483    0.52879  -3.772 0.000162 ***

    I(sevm1)vec8   35.86461    8.38806   4.276 1.91e-05 ***

    I(sevm1)vec25 -46.33209    8.82448  -5.250 1.52e-07 ***

    ---

    Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1



    (Dispersion parameter for binomial family taken to be 1)



        Null deviance: 387.959  on 284  degrees of freedom

    Residual deviance:  69.198  on 279  degrees of freedom

    AIC: 81.198



    Number of Fisher Scoring iterations: 8



Now I would like to compare the results of evaluate() and the
predictions between the normal model and the spatial model.
For the spatial model I get an error: "there are different length for
the variables".
I also want to plot the SAC-corrected predicted model to visualize the
distribution.

>e.priclus8_mem<-evaluate(test_pres_val, test_abs_val, priclus8_mem)



#Error in model.frame.default(Terms, newdata, na.action = na.action,

#xlev = object$xlevels) :  Variablenl?ngen sind unterschiedlich

#(gefunden f?r 'I(sevm1)') In addition: Warning message:'newdata' had

#120 rows but variables found have 285 rows



> plot(pclus8_mem<-predict(env_data, priclus8_mem, type="response"), main="GML priclu8_mem")



#Error in model.frame.default(Terms, newdata, na.action = na.action,

#xlev = object$xlevels):Variablenl?ngen sind unterschiedlich (gefunden

#f?r 'I(sevm1)')



Is this error caused by points with no neighbour? I used zero.polycy=TRUE
to accept no neighbours in the nb object. What else could be the problem?

I also read Bivand et al. (2008): Applied Spatial Data Analysis with R and
Borcard et al. (2011): Numerical Ecology with R. The given examples always
refer to vector data but I am working with raster data?

I am new in the matter and cannot get any further here. I have certainly
overlooked something or misunderstood. Any help and further reading is
appreciated!

Many thanks,

Patrick

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Mon Apr  3 17:31:50 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 3 Apr 2017 17:31:50 +0200
Subject: [R-sig-Geo] (no subject)
In-Reply-To: <CAD6JHaaYs1jMHsywQCK6jmYeMS2WSRuaN=JhfgBamK6i7ZAeXg@mail.gmail.com>
References: <CAD6JHaaYs1jMHsywQCK6jmYeMS2WSRuaN=JhfgBamK6i7ZAeXg@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1704031724250.32604@reclus.nhh.no>

On Mon, 3 Apr 2017, Patrick Schwager wrote:

> Dear list,
>
> I am currently working on modelling species distribution and try to account
> spatial autocorrelation. I use R and I successfully managed to incorporate
> moran's eigenvectors as predictor to my species distribution model using
> GLM (logit). I followed Dormann et al. (2007
> <https://www2.unil.ch/biomapper/Download/Dormann-EcoGra-2007.pdf>) and the
> appendix
> <http://www.ecography.org/sites/ecography.org/files/appendix/e5171.pdf>. I
> think I got corrected statistics in the moran.test() of model residuals as
> the p-value increases. Also the AIC score indicates a better spatial model.
>

Please post text-only messages: posting HTML has severely modified your 
code and output.

Please provide a reproducible example, preferably from built-in data.

Please state clearly where evaluate() and predict() are coming from. I 
think predict is stats::predict, but evaluate is from which package? 
dismo?

Most likely they expect the ME fitted matrix is not being found in the 
scope of newdata. Note that out of sample prediction for SF/ME models is 
not defined, as the weights are not the same, so the eigenvectors will not 
be either.

Hope this clarifies,

Roger

>
> *Normal model glm:*
>
> Moran I test under randomisation
>
>
>
> data:  residuals(model)
>
> weights: priclus.listw
>
>
>
> Moran I statistic standard deviate = 7.5632, p-value = 1.966e-14
>
> alternative hypothesis: greater
>
> sample estimates:
>
> Moran I statistic       Expectation          Variance
>
>      0.264335666      -0.007633588       0.001293086
>
>
>
> *Spatial model:*
>
> Moran I test under randomisation
>
>
>
> data:  residuals(model)
>
> weights: priclus.listw
>
>
>
> Moran I statistic standard deviate = 1.5572, p-value = 0.05972
>
> alternative hypothesis: greater
>
> sample estimates:
>
> Moran I statistic       Expectation          Variance
>
>      0.045968614      -0.007633588       0.001184932
>
>
>
> *See the summary for both models below:*
>
>> summary(priclus8<- glm(pb_train ~ gesteine + schnee_tag_1 + rs_hospso, family= binomial(link="logit"), data=envtrain))
>
>
>
>        Call:
>
>        glm(formula = pb_train ~ gesteine + schnee_tag_1 + rs_hospso,
>
>            family = binomial(link = "logit"), data = envtrain)
>
>
>
>        Deviance Residuals:
>
>             Min        1Q    Median        3Q       Max
>
>        -2.76552  -0.18741  -0.00449   0.34032   2.03205
>
>
>
>        Coefficients:
>
>                     Estimate Std. Error z value Pr(>|z|)
>
>        (Intercept)  75.89890   20.21051   3.755 0.000173 ***
>
>        gesteine1     2.05287    0.55421   3.704 0.000212 ***
>
>        schnee_tag_1 -0.03328    0.01685  -1.975 0.048223 *
>
>        rs_hospso    -1.56444    0.37856  -4.133 3.59e-05 ***
>
>        ---
>
>        Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>
>
>        (Dispersion parameter for binomial family taken to be 1)
>
>
>
>            Null deviance: 387.96  on 284  degrees of freedom
>
>        Residual deviance: 129.56  on 281  degrees of freedom
>
>        AIC: 137.56
>
>        Number of Fisher Scoring iterations: 7
>
>
>
>> sevm1 <- fitted(ME(pb_train ~ gesteine + schnee_tag_1 + rs_hospso, data=envtrain, family= binomial(link="logit"),listw=ME.listw))
>
>> summary(priclus8_mem<- glm(pb_train ~ gesteine + schnee_tag_1 + rs_hospso+ I(sevm1), family= binomial(link="logit"), data=envtrain) )
>
>
>
>    Call:
>
>    glm(formula = pb_train ~ gesteine + schnee_tag_1 + rs_hospso +
>
>        I(sevm1), family = binomial(link = "logit"), data = envtrain)
>
>
>
>    Deviance Residuals:
>
>        Min       1Q   Median       3Q      Max
>
>    -3.6095  -0.1415  -0.0025   0.0784   2.5099
>
>
>
>    Coefficients:
>
>                   Estimate Std. Error z value Pr(>|z|)
>
>    (Intercept)   102.50545   28.27283   3.626 0.000288 ***
>
>    gesteine1       0.80346    0.79373   1.012 0.311417
>
>    schnee_tag_1   -0.06435    0.02448  -2.628 0.008586 **
>
>    rs_hospso      -1.99483    0.52879  -3.772 0.000162 ***
>
>    I(sevm1)vec8   35.86461    8.38806   4.276 1.91e-05 ***
>
>    I(sevm1)vec25 -46.33209    8.82448  -5.250 1.52e-07 ***
>
>    ---
>
>    Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
>
>
>    (Dispersion parameter for binomial family taken to be 1)
>
>
>
>        Null deviance: 387.959  on 284  degrees of freedom
>
>    Residual deviance:  69.198  on 279  degrees of freedom
>
>    AIC: 81.198
>
>
>
>    Number of Fisher Scoring iterations: 8
>
>
>
> Now I would like to compare the results of evaluate() and the
> predictions between the normal model and the spatial model.
> For the spatial model I get an error: "there are different length for
> the variables".
> I also want to plot the SAC-corrected predicted model to visualize the
> distribution.
>
>> e.priclus8_mem<-evaluate(test_pres_val, test_abs_val, priclus8_mem)
>
>
>
> #Error in model.frame.default(Terms, newdata, na.action = na.action,
>
> #xlev = object$xlevels) :  Variablenl?ngen sind unterschiedlich
>
> #(gefunden f?r 'I(sevm1)') In addition: Warning message:'newdata' had
>
> #120 rows but variables found have 285 rows
>
>
>
>> plot(pclus8_mem<-predict(env_data, priclus8_mem, type="response"), main="GML priclu8_mem")
>
>
>
> #Error in model.frame.default(Terms, newdata, na.action = na.action,
>
> #xlev = object$xlevels):Variablenl?ngen sind unterschiedlich (gefunden
>
> #f?r 'I(sevm1)')
>
>
>
> Is this error caused by points with no neighbour? I used zero.polycy=TRUE
> to accept no neighbours in the nb object. What else could be the problem?
>
> I also read Bivand et al. (2008): Applied Spatial Data Analysis with R and
> Borcard et al. (2011): Numerical Ecology with R. The given examples always
> refer to vector data but I am working with raster data?
>
> I am new in the matter and cannot get any further here. I have certainly
> overlooked something or misunderstood. Any help and further reading is
> appreciated!
>
> Many thanks,
>
> Patrick
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From k.musa at lancaster.ac.uk  Wed Apr  5 09:27:20 2017
From: k.musa at lancaster.ac.uk (Musa, Kamarul Imran)
Date: Wed, 5 Apr 2017 07:27:20 +0000
Subject: [R-sig-Geo] after running as.ppp(),
	all points lying outside the window
Message-ID: <4AF347BCEED71C418A39B9DE1995E196818142D9@EX-0-MB1.lancs.local>

Dear all,

I was trying to plot 1135 points using spatstat package. The data points were in the CSV file, and I would like to convert them to a ppp object so I can run relrisk().  Unfortunately, after running as.ppp(), I received a warning message and when plotted, all the points were lying outside the rectangle window. I have been looking for answers elsewhere but could not find the right one for me, so I hope if anyone can help, then I would be grateful. 

The file is in the dropbox folder (https://www.dropbox.com/s/90yxvd5cwnvbty8/stroke_typhoid_RSO2.csv?dl=0) for those interested to download. But the codes and results that I have run are listed below:

1) When I run str(), it gives these:

> str(str_typ)
'data.frame':	1135 obs. of  3 variables:
 $ mark: int  1 2 2 2 2 2 2 1 2 2 ...
 $ x   : int  388992 422451 429127 430811 432153 432153 432306 432862 432887 433456 ...
 $ y   : int  516003 575800 627691 629927 624186 624186 617470 638829 615294 607246 ...

2) The summary returns these:

> summary(str_typ)
      mark             x                          y         
 Min.   :1.000   Min.   :388992      Min.   :516003  
 1st Qu.:1.000   1st Qu.:467048   1st Qu.:660183  
 Median :1.000   Median :474740   Median :669192  
 Mean   :1.303   Mean   :473189   Mean   :664687  
 3rd Qu.:2.000   3rd Qu.:481698   3rd Qu.:676389  
 Max.   :2.000   Max.   :503274   Max.   :689283  

3) To create the Window, I used this:

Win <- owin(c(388992,503274), c(516002,689283))

4) Then, I run as.ppp()
 
> str_typ2 <- as.ppp(str_typ, W = Win)
Warning message:
1135 points were rejected as lying outside the specified window 

5) When I run plot()
> plot(str_typ2)
Warning message:
In plot.ppp(str_typ2) : 1135 illegal points also plotted

6) What I could see is that all points were lying outside the rectangle box.

Best regards,
Kamarul
USM, Malaysia 



         

From b.rowlingson at lancaster.ac.uk  Wed Apr  5 11:46:44 2017
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 5 Apr 2017 10:46:44 +0100
Subject: [R-sig-Geo] after running as.ppp(),
	all points lying outside the window
In-Reply-To: <4AF347BCEED71C418A39B9DE1995E196818142D9@EX-0-MB1.lancs.local>
References: <4AF347BCEED71C418A39B9DE1995E196818142D9@EX-0-MB1.lancs.local>
Message-ID: <CANVKczPVZ-OC993D9C-K_Y1b51EBUmp3GMmy+RfySpMTR_cfXw@mail.gmail.com>

On Wed, Apr 5, 2017 at 8:27 AM, Musa, Kamarul Imran
<k.musa at lancaster.ac.uk> wrote:
> str_typ2 <- as.ppp(str_typ, W = Win)

as.ppp is using the first two columns of your data frame for
coordinates, the first one in yours is the mark!

So you have:

 > str_typ = data.frame(mark=sample(2,1135,TRUE), x=runif(1135,388992,
503274), y=runif(1135, 516003, 689283))
 > Win <- owin(c(388992,503274), c(516002,689283))
 > str_typ2 <- as.ppp(str_typ, W = Win)
Warning message:
1135 points were rejected as lying outside the specified window

But do the data frame with x,y,mark and it works:

 > str_typ = data.frame(x=runif(1135,388992, 503274), y=runif(1135,
516003, 689283), mark=sample(2,1135,TRUE))
 > str_typ2 <- as.ppp(str_typ, W = Win)

Probably better to use `ppp()` to construct a marked `ppp` object:

  str_typ2 <- ppp(x=str_typ$x, y=str_typ$y, marks=str_typ$mark, window=Win)

Note the window argument is `window=` here, in other places in
`spatstat` its `W` or `Win` or other things!

Barry


From k.musa at lancaster.ac.uk  Wed Apr  5 14:56:49 2017
From: k.musa at lancaster.ac.uk (Musa, Kamarul Imran)
Date: Wed, 5 Apr 2017 12:56:49 +0000
Subject: [R-sig-Geo] after running as.ppp(),
 all points lying outside the window
In-Reply-To: <CANVKczPVZ-OC993D9C-K_Y1b51EBUmp3GMmy+RfySpMTR_cfXw@mail.gmail.com>
References: <4AF347BCEED71C418A39B9DE1995E196818142D9@EX-0-MB1.lancs.local>,
 <CANVKczPVZ-OC993D9C-K_Y1b51EBUmp3GMmy+RfySpMTR_cfXw@mail.gmail.com>
Message-ID: <4AF347BCEED71C418A39B9DE1995E196818142F5@EX-0-MB1.lancs.local>

Dear Barry,

It works! I must make sure that the coordinates are correctly specified and positioned. And that I need to use 'window' and not 'W' in the arguments. Really appreciate your help.

Regards,
Kamarul

________________________________________
From: b.rowlingson at gmail.com [b.rowlingson at gmail.com] on behalf of Barry Rowlingson [b.rowlingson at lancaster.ac.uk]
Sent: Wednesday, April 05, 2017 5:46 PM
To: Musa, Kamarul Imran
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] after running as.ppp(), all points lying outside the window

On Wed, Apr 5, 2017 at 8:27 AM, Musa, Kamarul Imran
<k.musa at lancaster.ac.uk> wrote:
> str_typ2 <- as.ppp(str_typ, W = Win)

as.ppp is using the first two columns of your data frame for
coordinates, the first one in yours is the mark!

So you have:

 > str_typ = data.frame(mark=sample(2,1135,TRUE), x=runif(1135,388992,
503274), y=runif(1135, 516003, 689283))
 > Win <- owin(c(388992,503274), c(516002,689283))
 > str_typ2 <- as.ppp(str_typ, W = Win)
Warning message:
1135 points were rejected as lying outside the specified window

But do the data frame with x,y,mark and it works:

 > str_typ = data.frame(x=runif(1135,388992, 503274), y=runif(1135,
516003, 689283), mark=sample(2,1135,TRUE))
 > str_typ2 <- as.ppp(str_typ, W = Win)

Probably better to use `ppp()` to construct a marked `ppp` object:

  str_typ2 <- ppp(x=str_typ$x, y=str_typ$y, marks=str_typ$mark, window=Win)

Note the window argument is `window=` here, in other places in
`spatstat` its `W` or `Win` or other things!

Barry


From giuseppe.amatulli at gmail.com  Thu Apr  6 05:14:34 2017
From: giuseppe.amatulli at gmail.com (Giuseppe Amatulli)
Date: Wed, 5 Apr 2017 23:14:34 -0400
Subject: [R-sig-Geo] Summer School in Geocomputation - Italy - June 2017
Message-ID: <CAKoiDHKHD-yYKxTZGOCVR9fA1SYd+s2AKLJBiK6JdsN-+oZwxA@mail.gmail.com>

Dear colleagues,
There are still a limited number of seats available for the* International
Summer School* organized by Spatial Ecology (www.spatial-ecology.net)
<http://www.spatial-ecology.net/> held at the Univ. of Basilicata, in the
magnificent town Matera, Italy.

*Geocomputation using free and Open Source Software** (19th-23th June 2017)*

A 5 days intense experience opening new horizons on the use of the vast
potentials of Linux environment and the command line approach for *geo-data*
 processing using Bash, AWK, Python, GRASS, QGIS, GDAL/OGR, R, PKtools,
OpenForis. We will guide newbies and experienced GIS users who have never
used a command line terminal to a stage which will allow them to understand
and apply very advanced open source data processing routines. Our focus is
to enhance a self-learning approach. This allows participants to keep on
progressing and improving their skills in a continuously evolving
technological environment.

More information and registration:

www.spatial-ecology.net <http://www.spatial-ecology.net/upcoming-events>
www.spatial-ecology.net/upcoming-events
www.facebook.com/spatialecology > see events
twitter: @BigDataEcology

Best regards
Spatial Ecology ? Team

-- 
Giuseppe Amatulli, Ph.D.

Research scientist at
Yale School of Forestry & Environmental Studies
Yale Center for Research Computing
Center for Science and Social Science Information
New Haven, 06511
Teaching: http://spatial-ecology.org
Work:  https://environment.yale.edu/profile/giuseppe-amatulli/

	[[alternative HTML version deleted]]


From giuseppe.amatulli at gmail.com  Thu Apr  6 05:14:34 2017
From: giuseppe.amatulli at gmail.com (Giuseppe Amatulli)
Date: Wed, 5 Apr 2017 23:14:34 -0400
Subject: [R-sig-Geo] Summer School in Geocomputation - Italy - June 2017
Message-ID: <CAKoiDHKHD-yYKxTZGOCVR9fA1SYd+s2AKLJBiK6JdsN-+oZwxA@mail.gmail.com>

Dear colleagues,
There are still a limited number of seats available for the* International
Summer School* organized by Spatial Ecology (www.spatial-ecology.net)
<http://www.spatial-ecology.net/> held at the Univ. of Basilicata, in the
magnificent town Matera, Italy.

*Geocomputation using free and Open Source Software** (19th-23th June 2017)*

A 5 days intense experience opening new horizons on the use of the vast
potentials of Linux environment and the command line approach for *geo-data*
 processing using Bash, AWK, Python, GRASS, QGIS, GDAL/OGR, R, PKtools,
OpenForis. We will guide newbies and experienced GIS users who have never
used a command line terminal to a stage which will allow them to understand
and apply very advanced open source data processing routines. Our focus is
to enhance a self-learning approach. This allows participants to keep on
progressing and improving their skills in a continuously evolving
technological environment.

More information and registration:

www.spatial-ecology.net <http://www.spatial-ecology.net/upcoming-events>
www.spatial-ecology.net/upcoming-events
www.facebook.com/spatialecology > see events
twitter: @BigDataEcology

Best regards
Spatial Ecology ? Team

-- 
Giuseppe Amatulli, Ph.D.

Research scientist at
Yale School of Forestry & Environmental Studies
Yale Center for Research Computing
Center for Science and Social Science Information
New Haven, 06511
Teaching: http://spatial-ecology.org
Work:  https://environment.yale.edu/profile/giuseppe-amatulli/

	[[alternative HTML version deleted]]


From Virgilio.Gomez at uclm.es  Thu Apr  6 11:55:19 2017
From: Virgilio.Gomez at uclm.es (VIRGILIO GOMEZ RUBIO)
Date: Thu, 6 Apr 2017 09:55:19 +0000
Subject: [R-sig-Geo] RFC SEjags: Bayesian spatial econometrics models with
	JAGS
Message-ID: <AE670303-F764-48AA-9B93-5CB949B72F0E@uclm.es>

Hi,

I am developing a new package on Bayesian spatial econometrics models that implements the main models using JAGS.  Right now, it fits SEM, SLM, SDM, SDEM, SLX and SAC models (using the terminology in the works by LeSage and Page). It also supports a model with a proper CAR distribution for the error term. In addition, probit and logit links (for binary data) are also supported. Computation of the impacts is available through the impacts() function.

You can install it from github and run some examples on the Columbus dataset with:

devtools::install_github("becarioprecario/SEjags")
library(SEjags)
example(SEjags)

Other models could be easily implemented and the bugs models included in the package can serve as a starting point. I believe that this implementation will be slow for medium or large datasets because of the way it handles inversion of matrices.

I?ve noticed that some people on the list work with these models and I would love to hear from them. If you are able to try and have some comments, please, do not hesitate to contact me off list.

Best wishes,

Virgilio



From dave.gregovich at alaska.gov  Thu Apr  6 19:27:18 2017
From: dave.gregovich at alaska.gov (Gregovich, Dave P (DFG))
Date: Thu, 6 Apr 2017 17:27:18 +0000
Subject: [R-sig-Geo] Raster math on a stack of large rasters
Message-ID: <174524AB15B8E6478D6F927B2131B09B0134C83F63@SOAJNUEXMB4.soa.alaska.gov>

Hi,
I am performing a math operation on a stack of large rasters. The code below uses smaller files for illustration and reproducibility.
Any alternative way of performing this task that does not create huge temporary files, and perhaps cuts down on processing time, would be greatly appreciated. The 'calc'  process creates a couple of temporary raster files that are huge. The first one is 142 GB, and I don't have hard drive space for that one and the second one that begins writing during the process.
Thanks kindly for any advice!
Dave.

#create raster stack and coefficients...
library(raster)
mod.coefs <- rnorm(10)
s <- stack()
r <- raster(nrow = 100, ncol = 100)
#actual rasters I am working with are about 40000, pixels square, with each GeoTiff raster in the stack taking about 2.5 GB on disk

for(i in 1:10){
  r[] <- rnorm(10000)
  s <- addLayer(s, r)
}

#attempt to perform raster math...
out.file <- 'C:/ out.rast.tif'
out.rast <- calc(s, function(x){exp(sum(mod.coefs * x))}, filename = out.file)

#at this point, the temporary files in the \AppData\Local\Temp\RtmpqQYzfS\raster folder eventually become quite large,
#with one .GRI file reaching 142 GB, and another now growing to 8 GB before I ended the process
#the file 'out.file' has not been created at that point.
#_____________end____________________________________________________________________

	[[alternative HTML version deleted]]


From dave.gregovich at alaska.gov  Thu Apr  6 19:39:16 2017
From: dave.gregovich at alaska.gov (Gregovich, Dave P (DFG))
Date: Thu, 6 Apr 2017 17:39:16 +0000
Subject: [R-sig-Geo] Raster math on a stack of large rasters
Message-ID: <174524AB15B8E6478D6F927B2131B09B0134C83F6F@SOAJNUEXMB4.soa.alaska.gov>

I apologize for not including my sessionInfo() previously:
> sessionInfo()
R version 3.3.3 (2017-03-06)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] raster_2.5-8 sp_1.2-4

loaded via a namespace (and not attached):
[1] rgdal_1.2-5     tools_3.3.3     Rcpp_0.12.9     grid_3.3.3      lattice_0.20-34







From: Gregovich, Dave P (DFG)
Sent: Thursday, April 06, 2017 9:27 AM
To: 'r-sig-geo at r-project.org'
Subject: Raster math on a stack of large rasters

Hi,
I am performing a math operation on a stack of large rasters. The code below uses smaller files for illustration and reproducibility.
Any alternative way of performing this task that does not create huge temporary files, and perhaps cuts down on processing time, would be greatly appreciated. The 'calc'  process creates a couple of temporary raster files that are huge. The first one is 142 GB, and I don't have hard drive space for that one and the second one that begins writing during the process.
Thanks kindly for any advice!
Dave.

#create raster stack and coefficients...
library(raster)
mod.coefs <- rnorm(10)
s <- stack()
r <- raster(nrow = 100, ncol = 100)
#actual rasters I am working with are about 40000, pixels square, with each GeoTiff raster in the stack taking about 2.5 GB on disk

for(i in 1:10){
  r[] <- rnorm(10000)
  s <- addLayer(s, r)
}

#attempt to perform raster math...
out.file <- 'C:/ out.rast.tif'
out.rast <- calc(s, function(x){exp(sum(mod.coefs * x))}, filename = out.file)

#at this point, the temporary files in the \AppData\Local\Temp\RtmpqQYzfS\raster folder eventually become quite large,
#with one .GRI file reaching 142 GB, and another now growing to 8 GB before I ended the process
#the file 'out.file' has not been created at that point.
#_____________end____________________________________________________________________

	[[alternative HTML version deleted]]


From benjamin.leutner at uni-wuerzburg.de  Fri Apr  7 15:50:06 2017
From: benjamin.leutner at uni-wuerzburg.de (Benjamin Leutner)
Date: Fri, 7 Apr 2017 15:50:06 +0200
Subject: [R-sig-Geo] Raster math on a stack of large rasters
In-Reply-To: <174524AB15B8E6478D6F927B2131B09B0134C83F63@SOAJNUEXMB4.soa.alaska.gov>
References: <174524AB15B8E6478D6F927B2131B09B0134C83F63@SOAJNUEXMB4.soa.alaska.gov>
Message-ID: <205e8b66-4307-5dea-73ea-63bcd464241d@uni-wuerzburg.de>

You could stick to the native raster format (grd), in which case calc() 
writes to your final file directly.
Of course that doesn't change the file size issue, but saves you the 
translation step to geotiff.

For the file size you could consider restricting the datatype to integer 
(see ?dataType).
If I have reflectance data [0,1] for example; I scale them by a factor 
10000 and then save as "INT4U" or "INT2U" (depending on your maximally 
expected value range), or "INT4S" or "INT2S" if you have negative 
values. That can bring down the filesize quite a bit, while retaining 
most of the relevant precission (beware: remaining decimal places will 
be cut-off,)

e.g. calc(..., function(x) { yourcalculations(x) * 10000 }, datatype = 
"INT4S")

pro tip: the argument in calc() (or more precisely in writeRaster()) is 
called datatype, not to be confused with the stand-alone function 
dataType() with a capital T. That one has bitten me many times, because 
due to the "..." argument there will be no warning if you mistype it ;-)



On 06.04.2017 19:27, Gregovich, Dave P (DFG) wrote:
> Hi,
> I am performing a math operation on a stack of large rasters. The code below uses smaller files for illustration and reproducibility.
> Any alternative way of performing this task that does not create huge temporary files, and perhaps cuts down on processing time, would be greatly appreciated. The 'calc'  process creates a couple of temporary raster files that are huge. The first one is 142 GB, and I don't have hard drive space for that one and the second one that begins writing during the process.
> Thanks kindly for any advice!
> Dave.
>
> #create raster stack and coefficients...
> library(raster)
> mod.coefs <- rnorm(10)
> s <- stack()
> r <- raster(nrow = 100, ncol = 100)
> #actual rasters I am working with are about 40000, pixels square, with each GeoTiff raster in the stack taking about 2.5 GB on disk
>
> for(i in 1:10){
>    r[] <- rnorm(10000)
>    s <- addLayer(s, r)
> }
>
> #attempt to perform raster math...
> out.file <- 'C:/ out.rast.tif'
> out.rast <- calc(s, function(x){exp(sum(mod.coefs * x))}, filename = out.file)
>
> #at this point, the temporary files in the \AppData\Local\Temp\RtmpqQYzfS\raster folder eventually become quite large,
> #with one .GRI file reaching 142 GB, and another now growing to 8 GB before I ended the process
> #the file 'out.file' has not been created at that point.
> #_____________end____________________________________________________________________
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Benjamin Leutner M.Sc.
PhD candidate

Department of Remote Sensing
University of Wuerzburg
Campus Hubland Nord 86
97074 Wuerzburg, Germany

Tel: +49-(0)931-31 89594
Email: benjamin.leutner at uni-wuerzburg.de
Web: http://www.fernerkundung.uni-wuerzburg.de


From mel at mbacou.com  Fri Apr  7 22:05:11 2017
From: mel at mbacou.com (Melanie Bacou)
Date: Fri, 7 Apr 2017 16:05:11 -0400
Subject: [R-sig-Geo] Raster math on a stack of large rasters
In-Reply-To: <205e8b66-4307-5dea-73ea-63bcd464241d@uni-wuerzburg.de>
References: <174524AB15B8E6478D6F927B2131B09B0134C83F63@SOAJNUEXMB4.soa.alaska.gov>
 <205e8b66-4307-5dea-73ea-63bcd464241d@uni-wuerzburg.de>
Message-ID: <01e5562d-827c-bcc8-5c12-0578b02611cb@mbacou.com>

Also take a look at using raster:: clusterR() in combination with calc() 
to use multiple cores. This works well if your your calculation function 
does not require neighboring cells.

--Mel.


On 04/07/2017 09:50 AM, Benjamin Leutner wrote:
> You could stick to the native raster format (grd), in which case 
> calc() writes to your final file directly.
> Of course that doesn't change the file size issue, but saves you the 
> translation step to geotiff.
>
> For the file size you could consider restricting the datatype to 
> integer (see ?dataType).
> If I have reflectance data [0,1] for example; I scale them by a factor 
> 10000 and then save as "INT4U" or "INT2U" (depending on your maximally 
> expected value range), or "INT4S" or "INT2S" if you have negative 
> values. That can bring down the filesize quite a bit, while retaining 
> most of the relevant precission (beware: remaining decimal places will 
> be cut-off,)
>
> e.g. calc(..., function(x) { yourcalculations(x) * 10000 }, datatype = 
> "INT4S")
>
> pro tip: the argument in calc() (or more precisely in writeRaster()) 
> is called datatype, not to be confused with the stand-alone function 
> dataType() with a capital T. That one has bitten me many times, 
> because due to the "..." argument there will be no warning if you 
> mistype it ;-)
>
>
>
> On 06.04.2017 19:27, Gregovich, Dave P (DFG) wrote:
>> Hi,
>> I am performing a math operation on a stack of large rasters. The 
>> code below uses smaller files for illustration and reproducibility.
>> Any alternative way of performing this task that does not create huge 
>> temporary files, and perhaps cuts down on processing time, would be 
>> greatly appreciated. The 'calc'  process creates a couple of 
>> temporary raster files that are huge. The first one is 142 GB, and I 
>> don't have hard drive space for that one and the second one that 
>> begins writing during the process.
>> Thanks kindly for any advice!
>> Dave.
>>
>> #create raster stack and coefficients...
>> library(raster)
>> mod.coefs <- rnorm(10)
>> s <- stack()
>> r <- raster(nrow = 100, ncol = 100)
>> #actual rasters I am working with are about 40000, pixels square, 
>> with each GeoTiff raster in the stack taking about 2.5 GB on disk
>>
>> for(i in 1:10){
>>    r[] <- rnorm(10000)
>>    s <- addLayer(s, r)
>> }
>>
>> #attempt to perform raster math...
>> out.file <- 'C:/ out.rast.tif'
>> out.rast <- calc(s, function(x){exp(sum(mod.coefs * x))}, filename = 
>> out.file)
>>
>> #at this point, the temporary files in the 
>> \AppData\Local\Temp\RtmpqQYzfS\raster folder eventually become quite 
>> large,
>> #with one .GRI file reaching 142 GB, and another now growing to 8 GB 
>> before I ended the process
>> #the file 'out.file' has not been created at that point.
>> #_____________end____________________________________________________________________ 
>>
>>
>>     [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>


From Scott.Waichler at pnnl.gov  Sat Apr  8 00:01:26 2017
From: Scott.Waichler at pnnl.gov (Waichler, Scott R)
Date: Fri, 7 Apr 2017 22:01:26 +0000
Subject: [R-sig-Geo] how to get weights for inverse distance weighting
Message-ID: <074C83DAD4825242A20B2D83FDBCB8881BA1257A@EX10MBOX03.pnnl.gov>

Hello, I am trying to get inverse distance weights to estimate values on a regular grid from a set of data points, over a sequence of times.  The locations of the data points don't vary with time, but their values will with each instance, and in general some are NA.  I want to determine the number of unique weight matrices needed to do the IDW estimation across time.  I'm looking at gstat and spdep but am still having trouble with some of the basics . . .I should be more fluent with sp, I know.  The weight matrix has grid points for rows and data points for columns.

x.grid <- y.grid <- seq(0, 100, by=10)  # regular grid, locations I want to estimate z at
N <- 10  # number of data points
x.data <- runif(0, 100, n=N)  # set locations of the data points
y.data <- runif(0, 100, n=N)

w <- list()  # 
# Loop thru 20 instances (a sequence of months in my real problem)
for(i in 1:20) {
  # set the z.data values to something between 0 and 1
  z.data <- runif(0, 1, n=N)
  # set some of the z.data values to NA
  ind.na <- sample.int(N, size=round(runif(1, 0, N)))
  z.data[ind.na] <- NA

  # For this time, I want to get the matrix of weights (>= 0) that would be used in an inverse
  # distance weighting interpolation.  I would like to specify parameters like nmax, nmin, omax, maxdist, and force
  # as used in gstat().   Rows are grid points, columns are data points.
  #w[[i]] <- matrix(data= ????, ncol=N, byrow=T)
}

Finally, how can I determine the number of unique w[[i]] matrices?  I know for my toy problem it will
in general be 20, but in my real problem it will be less than the number of times.

Thank you,
Scott Waichler
Pacific Northwest National Laboratory
Richland, WA    USA


From edzer.pebesma at uni-muenster.de  Sat Apr  8 00:35:08 2017
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Sat, 8 Apr 2017 00:35:08 +0200
Subject: [R-sig-Geo] how to get weights for inverse distance weighting
In-Reply-To: <074C83DAD4825242A20B2D83FDBCB8881BA1257A@EX10MBOX03.pnnl.gov>
References: <074C83DAD4825242A20B2D83FDBCB8881BA1257A@EX10MBOX03.pnnl.gov>
Message-ID: <53fce53b-b46b-cbba-b5a9-c71bf3afd250@uni-muenster.de>

Scott,

you may want to have a look at gstat::idw0 , which expresses the idw
problem in a hand full of expressions, essentially

idw0 = function (data, newdata, y, idp = 2) {
    s = coordinates(data)
    s0 = coordinates(newdata)
    D = 1/(spDists(s0, s)^idp)
    sumD = apply(D, 1, sum)
    D %*% y/sumD
}


On 08/04/17 00:01, Waichler, Scott R wrote:
> Hello, I am trying to get inverse distance weights to estimate values on a regular grid from a set of data points, over a sequence of times.  The locations of the data points don't vary with time, but their values will with each instance, and in general some are NA.  I want to determine the number of unique weight matrices needed to do the IDW estimation across time.  I'm looking at gstat and spdep but am still having trouble with some of the basics . . .I should be more fluent with sp, I know.  The weight matrix has grid points for rows and data points for columns.
> 
> x.grid <- y.grid <- seq(0, 100, by=10)  # regular grid, locations I want to estimate z at
> N <- 10  # number of data points
> x.data <- runif(0, 100, n=N)  # set locations of the data points
> y.data <- runif(0, 100, n=N)
> 
> w <- list()  # 
> # Loop thru 20 instances (a sequence of months in my real problem)
> for(i in 1:20) {
>   # set the z.data values to something between 0 and 1
>   z.data <- runif(0, 1, n=N)
>   # set some of the z.data values to NA
>   ind.na <- sample.int(N, size=round(runif(1, 0, N)))
>   z.data[ind.na] <- NA
> 
>   # For this time, I want to get the matrix of weights (>= 0) that would be used in an inverse
>   # distance weighting interpolation.  I would like to specify parameters like nmax, nmin, omax, maxdist, and force
>   # as used in gstat().   Rows are grid points, columns are data points.
>   #w[[i]] <- matrix(data= ????, ncol=N, byrow=T)
> }
> 
> Finally, how can I determine the number of unique w[[i]] matrices?  I know for my toy problem it will
> in general be 20, but in my real problem it will be less than the number of times.
> 
> Thank you,
> Scott Waichler
> Pacific Northwest National Laboratory
> Richland, WA    USA
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/38bb272e/attachment.sig>

From sarosama at feps.edu.eg  Sat Apr  8 12:47:44 2017
From: sarosama at feps.edu.eg (sara osama)
Date: Sat, 8 Apr 2017 13:47:44 +0300
Subject: [R-sig-Geo] questions about gstat krigeST
Message-ID: <CAJsLiQRtyPgUhZKpWLo7K+XOjuzntSvog62qMN7zCff1KqZ91Q@mail.gmail.com>

I have used the gstat package for analyzing air pollution data from a
network of monitoring statins using *krigeSTTg* and I have two main
question that i really need their answers for completting my work:

first: Can we use the spatiotemporal kriging krigSTTg for making forecast
for future values and if yes how the spatiotemporal kriging do this.
second how the krigeSTTg deals with missing data, I defined my data as
STSDF and it worked out but I need to know how krigeSTTg deal with missing
data

Thanks alot, I will be waiting for your valauble comments that I realy
need.

-- 
Best Regards
Sara
Faculty of Economics & Political Science
Cairo University
Tel:(202)35728055-(202)35728116-(202)35736608-(202)35736605
Fax:(202)35711020
Follow us on twitter:https://twitter.com/fepsnews

	[[alternative HTML version deleted]]


From craab at uni-goettingen.de  Sat Apr  8 14:33:28 2017
From: craab at uni-goettingen.de (Christoph Raab)
Date: Sat, 8 Apr 2017 14:33:28 +0200
Subject: [R-sig-Geo] Raster math on a stack of large rasters
In-Reply-To: <mailman.7.1491645601.33086.r-sig-geo@r-project.org>
References: <mailman.7.1491645601.33086.r-sig-geo@r-project.org>
Message-ID: <0d97e642-34cc-0d50-fe5b-65ceb1c5f8d9@uni-goettingen.de>

You can try to run the calc process based on tiles in parallel (pblapply 
package ) and delete all files in the tempdir of the raster package 
after each iteration. In the end you need to mosaic the tiles back together.


library(rgdal)
library(raster)
library(pbapply)
library(GSIF)

#set the tempdir for the raster package
#the tempdir can be cleared after a tile is processed
tempdir <- 'path_to_your_desired_tempdir'
rasterOptions(tmpdir=tempdir)

#detect number of cores
#you should adjust this according to your memory
n.cores <- detectCores()

out.file <- 'path_to_result_folder_where_the_tiles_will_be_saved'
fn <- "path_to_your_in_raster.tif"
obj <- GDALinfo(fn)

#create tiles with e.g. 2000*2000 pixels and no overlap
#you can adjust the block size in x and y direction
ras.lst <- GSIF::getSpatialTiles(obj, block.x=2000, overlap.percent = 0)

#use pbapply in order to run the processing in parallel
out.list <- pblapply(1:length(ras.lst[,1]),function(i){
   #credit goes to stackoverflow or so, but I couldn't find the source I 
got it from anymore
   offset <- c(ras.lst$offset.y[i], ras.lst$offset.x[i])
   region.dim <- c(ras.lst$region.dim.y[i],
                   ras.lst$region.dim.x[i])
   ## read a tile
   s <- readGDAL(fn, offset=offset,
                   region.dim=region.dim)

   #perfome calc
   out.rast <- calc(s, function(x){exp(sum(mod.coefs * x))}, filename = 
paste0(out.file,toString(i),".tif"))
   #clear the tempdir
   do.call(file.remove, list(list.files(tempdir, full.names = TRUE)))

} , cl = n.cores)

#settings for the mosaic process
out.list$fun <- mean
out.list$filename <- 'final_result.tif'
#mosaic each tile in the list from pblapply
mos <- do.call(mosaic, out.list)


On 08.04.2017 12:00, r-sig-geo-request at r-project.org wrote:
> Send R-sig-Geo mailing list submissions to
> 	r-sig-geo at r-project.org
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> or, via email, send a message with subject or body 'help' to
> 	r-sig-geo-request at r-project.org
>
> You can reach the person managing the list at
> 	r-sig-geo-owner at r-project.org
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-Geo digest..."
>
>
> Today's Topics:
>
>     1. Re: Raster math on a stack of large rasters (Benjamin Leutner)
>     2. Re: Raster math on a stack of large rasters (Melanie Bacou)
>     3. how to get weights for inverse distance weighting
>        (Waichler, Scott R)
>     4. Re: how to get weights for inverse distance weighting
>        (Edzer Pebesma)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Fri, 7 Apr 2017 15:50:06 +0200
> From: Benjamin Leutner <benjamin.leutner at uni-wuerzburg.de>
> To: R-mailing list <r-sig-geo at r-project.org>
> Subject: Re: [R-sig-Geo] Raster math on a stack of large rasters
> Message-ID: <205e8b66-4307-5dea-73ea-63bcd464241d at uni-wuerzburg.de>
> Content-Type: text/plain; charset=windows-1252; format=flowed
>
> You could stick to the native raster format (grd), in which case calc()
> writes to your final file directly.
> Of course that doesn't change the file size issue, but saves you the
> translation step to geotiff.
>
> For the file size you could consider restricting the datatype to integer
> (see ?dataType).
> If I have reflectance data [0,1] for example; I scale them by a factor
> 10000 and then save as "INT4U" or "INT2U" (depending on your maximally
> expected value range), or "INT4S" or "INT2S" if you have negative
> values. That can bring down the filesize quite a bit, while retaining
> most of the relevant precission (beware: remaining decimal places will
> be cut-off,)
>
> e.g. calc(..., function(x) { yourcalculations(x) * 10000 }, datatype =
> "INT4S")
>
> pro tip: the argument in calc() (or more precisely in writeRaster()) is
> called datatype, not to be confused with the stand-alone function
> dataType() with a capital T. That one has bitten me many times, because
> due to the "..." argument there will be no warning if you mistype it ;-)
>
>
>
> On 06.04.2017 19:27, Gregovich, Dave P (DFG) wrote:
>> Hi,
>> I am performing a math operation on a stack of large rasters. The code below uses smaller files for illustration and reproducibility.
>> Any alternative way of performing this task that does not create huge temporary files, and perhaps cuts down on processing time, would be greatly appreciated. The 'calc'  process creates a couple of temporary raster files that are huge. The first one is 142 GB, and I don't have hard drive space for that one and the second one that begins writing during the process.
>> Thanks kindly for any advice!
>> Dave.
>>
>> #create raster stack and coefficients...
>> library(raster)
>> mod.coefs <- rnorm(10)
>> s <- stack()
>> r <- raster(nrow = 100, ncol = 100)
>> #actual rasters I am working with are about 40000, pixels square, with each GeoTiff raster in the stack taking about 2.5 GB on disk
>>
>> for(i in 1:10){
>>     r[] <- rnorm(10000)
>>     s <- addLayer(s, r)
>> }
>>
>> #attempt to perform raster math...
>> out.file <- 'C:/ out.rast.tif'
>> out.rast <- calc(s, function(x){exp(sum(mod.coefs * x))}, filename = out.file)
>>
>> #at this point, the temporary files in the \AppData\Local\Temp\RtmpqQYzfS\raster folder eventually become quite large,
>> #with one .GRI file reaching 142 GB, and another now growing to 8 GB before I ended the process
>> #the file 'out.file' has not been created at that point.
>> #_____________end____________________________________________________________________
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>


From carlaoliveir at gmail.com  Sat Apr  8 19:37:23 2017
From: carlaoliveir at gmail.com (Carla Oliveira)
Date: Sat, 8 Apr 2017 18:37:23 +0100
Subject: [R-sig-Geo] Fwd: GEOMED2017 - CALL FOR ABSTRACTS CLOSES ON 15th OF
	APRIL
References: <f4ce56751813aefc6eafc8a70.7228bb7854.20170408080552.b1571ced76.99487a53@mail206.atl81.rsgsv.net>
Message-ID: <ADE1CC8F-FC31-47B6-93D8-7488FB94B2DA@gmail.com>


 

> 
> 
> 
> Instituto de Investiga??o e Inova??o em Sa?de - i3S
> View this email in your browser
>                                
>            
>  
> CALL FOR ABSTRACTS CLOSES ON 15th OF APRIL
> The Institute for Research and Innovation in Health (Instituto de Investiga??o e Inova??o em Sa?de) (i3S), is pleased to announce GEOMED 2017 ? ?Deeper insight from big data and small areas?.
> 
> If you have not submited your abstract yet, we invite you to do it in any one of the topic areas  planned for GEOMED2017 which include:
> 
> GIS in Public Health;
> Spatial Health Surveillance;
> Spatial survival and registry data analysis;
> Clustering of temporal trends in space-time disease mapping;
> Agent-based modeling;
> Modifiable areal unit issues and methods;
> Social networks and spatial epidemiology: tools, opportunities and challenges;
> Modelling climate-sensitive disease;
> Modelling and inference in infectious disease epidemiology;
> The use of linked data in spatial epidemiology;
> Joint Modelling with Spatial Variation;
> Health Applications of the Google Earth Engine;
> Challenges and Advances in Spatio-Temporal Disease Modelling.
> Remote sensing applications in health
> SUBMIT YOUR ABSTRACT NOW
>                         
> Porto is Portugal's second largest city and the capital of the Northern region.
> Icons such as "Serralves" or "Casa da M?sica" and a regular agenda of music, art and sports events stand witness to a urban space, in constant movement. 
> NEED ACCOMMODATION?
> VIEW OUR WEBSTIE
> 
> 
> 
> 
> 
> 
> This email was sent to carlaoliveir at gmail.com 
> why did I get this?    unsubscribe from this list    update subscription preferences 
> Geomed 2017 ? RUA ALFREDO ALLEN, 208 ? Porto 4200-135 ? Portugal 
> 
> 

	[[alternative HTML version deleted]]


From arudolph at bu.edu  Sat Apr  8 20:26:33 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: Sat, 8 Apr 2017 18:26:33 +0000
Subject: [R-sig-Geo] CALL
Message-ID: <CY1PR03MB13438317942B058228B96DC9AE0F0@CY1PR03MB1343.namprd03.prod.outlook.com>

> CALL FOR ABSTRACTS CLOSES ON 15th OF APRIL
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 1498 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/330be222/attachment.ics>

From arudolph at bu.edu  Sat Apr  8 20:26:59 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: Sat, 8 Apr 2017 18:26:59 +0000
Subject: [R-sig-Geo] Canceled: CALL
Message-ID: <CY1PR03MB1343A45F454B37C44CDBA4D2AE0F0@CY1PR03MB1343.namprd03.prod.outlook.com>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/7b22b73d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 1366 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/7b22b73d/attachment.ics>

From arudolph at bu.edu  Sat Apr  8 20:27:15 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:27:15 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.0.1491676042.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ac13db9d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5853 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ac13db9d/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5853 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ac13db9d/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:27:16 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:27:16 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.1.1491676048.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/1bbaedc3/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5868 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/1bbaedc3/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5868 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/1bbaedc3/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:27:19 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:27:19 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.2.1491676048.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/7013be9f/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5856 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/7013be9f/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5856 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/7013be9f/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:28:01 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:28:01 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.3.1491676089.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/f9676300/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5863 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/f9676300/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5863 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/f9676300/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:28:18 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:28:18 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.4.1491676103.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/38d6a75b/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5849 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/38d6a75b/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5849 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/38d6a75b/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:28:23 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:28:23 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.5.1491676110.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/96ed10a5/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5853 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/96ed10a5/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5853 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/96ed10a5/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:28:34 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:28:34 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.6.1491676121.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/13a0c492/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5853 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/13a0c492/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5853 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/13a0c492/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:28:36 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:28:36 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.7.1491676123.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ad14bbe2/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5849 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ad14bbe2/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5849 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ad14bbe2/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:28:45 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:28:45 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.8.1491676134.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/34c6cc9d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5857 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/34c6cc9d/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5857 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/34c6cc9d/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:28:48 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:28:48 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.9.1491676135.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/d3b4f487/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5858 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/d3b4f487/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5858 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/d3b4f487/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:30:44 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:30:44 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.10.1491676250.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/e09f9eec/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5858 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/e09f9eec/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5858 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/e09f9eec/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:31:32 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:31:32 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.11.1491676300.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ae9fdbab/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5863 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ae9fdbab/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5863 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/ae9fdbab/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:31:53 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:31:53 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.12.1491676320.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/58bcfda8/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5845 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/58bcfda8/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5845 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/58bcfda8/attachment.bin>

From arudolph at bu.edu  Sat Apr  8 20:36:24 2017
From: arudolph at bu.edu (Rudolph, Abby E)
Date: 08 Apr 2017 18:36:24 +0000
Subject: [R-sig-Geo] Cancelled: CALL
Message-ID: <mailman.13.1491676591.12620.r-sig-geo@r-project.org>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/541fd671/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: text/calendar
Size: 5856 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/541fd671/attachment.ics>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: invite.ics
Type: application/ics
Size: 5856 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170408/541fd671/attachment.bin>

From a.soufianou at yahoo.fr  Mon Apr 10 11:12:54 2017
From: a.soufianou at yahoo.fr (Soufianou Abou)
Date: Mon, 10 Apr 2017 09:12:54 +0000 (UTC)
Subject: [R-sig-Geo] Question  niche based model whith R
References: <626667739.279201.1491815574118.ref@mail.yahoo.com>
Message-ID: <626667739.279201.1491815574118@mail.yahoo.com>

?Hello dears,I work on the modeling of the ecological niche of striga, I have presence data (data) and bioclimatic variables and other variables around.Indeed, my question how should I proceed to model the ecological niche with software R??Pending receipt of my request, please accept my best regards.SADDA Abou-Soufianou -------------------------------------- DoctorantUniversit? Dan Dicko Dankoulodo de Maradi,120 avenue Maman Koraou,ADS, MaradiBP 465 Maradi/Niger E-mail:?a.soufianou at yahoo.frGSM?: (+227)96269987
	[[alternative HTML version deleted]]


From mauriziomarchi85 at gmail.com  Mon Apr 10 12:29:33 2017
From: mauriziomarchi85 at gmail.com (Maurizio Marchi)
Date: Mon, 10 Apr 2017 12:29:33 +0200
Subject: [R-sig-Geo] Question niche based model with R
Message-ID: <CANJhsN3yhHFMr11rT80+ugPP83jC0h4bHQYQguWqJo-Bdq23Xw@mail.gmail.com>

Hi Abou,
it is a quite broad question...a simple and easy answer would be "you
can just model yous data.frame with ?lm function or ?glm but you would
lose many of the iteresting stuff!! Then, in my opinion, first of all
you must decide between "presence-only" and "presence-absence" models
(my suggestion is for the second and in your case you must learn how
to generate pseudo-absences or background points. So, in my
experience, there are two main packages which are the most interesting
and suited for this purpose and which may help you: dismo and biomod2
(https://cran.r-project.org/web/packages/dismo/dismo.pdf -
https://cran.r-project.org/web/packages/biomod2/biomod2.pdf). There
are also many tutorials which can help you, for instance
https://cran.r-project.org/web/packages/dismo/vignettes/sdm.pdf
Then once you will handle the subject you will also be able to create
your own SDM using the algorithm you prefer (e.g. RandomForest,
MaxEnt, GLM, MARS, GAM and so on...) and without any additional
package. Anyway, my advice is to use one of the two above cited
packages because they are object-oriented and optimized for Ecological
Modelling.

Just to cover possible next question (which model is the best
solution) you will see that it is impossible to define the BEST method
which can perform correctly in all the circumstances. Many Researchers
are used to work with a single-algorithm (for example Generalized
Linear Models, Maximum Entropy, Generalized Additive Models, Neural
network, Random Forest etc.) while others are more willing to work
with ensemble projections and consensus models (i.e. mean or weighted
mean of more than one algorithm). Some examples of single-model and
ensemble model are here:

http://dx.doi.org/10.5424/fs/2016253-09476
http://dx.doi.org/10.1111/gcb.12604
http://dx.doi.org/10.1007/s13595-014-0439-4
http://dx.doi.org/10.1111/gcb.12476

Cheers,

-- 
Maurizio Marchi
Skype ID: maurizioxyz
linux user 552742


From naimi.b at gmail.com  Mon Apr 10 13:34:39 2017
From: naimi.b at gmail.com (Babak Naimi)
Date: Mon, 10 Apr 2017 11:34:39 +0000
Subject: [R-sig-Geo] Question niche based model whith R
In-Reply-To: <mailman.5.1491818401.32106.r-sig-geo@r-project.org>
References: <mailman.5.1491818401.32106.r-sig-geo@r-project.org>
Message-ID: <CAGL-CGDk+LXKOtq+GE74sXGkqVZ55PX=Fuk_OW0o2rZEuy0v1g@mail.gmail.com>

There are several packages available in R to do ecological niche modelling.
One easy-to-use option would be the sdm package. It is a comprehensive
modelling and simulation framework to fit species distribution models
supports several algorithms, and also is user-friendly. You can find a
tutorial for a quick start at: http://biogeoinformatics.org

Good luck,
Babak


>
> Date: Mon, 10 Apr 2017 09:12:54 +0000 (UTC)
> From: Soufianou Abou <a.soufianou at yahoo.fr>
> To: "r-sig-geo at r-project.org" <r-sig-geo at r-project.org>
> Subject: [R-sig-Geo] Question  niche based model whith R
> Message-ID: <626667739.279201.1491815574118 at mail.yahoo.com>
> Content-Type: text/plain; charset="UTF-8"
>
> ?Hello dears,I work on the modeling of the ecological niche of striga, I
> have presence data (data) and bioclimatic variables and other variables
> around.Indeed, my question how should I proceed to model the ecological
> niche with software R??Pending receipt of my request, please accept my best
> regards.SADDA Abou-Soufianou --------------------------------------
> DoctorantUniversit? Dan Dicko Dankoulodo de Maradi,120 avenue Maman
> Koraou,ADS, MaradiBP 465 Maradi/Niger E-mail:?a.soufianou at yahoo.frGSM?:
> (+227)96269987 <+227%2096%2026%2099%2087>
>         [[alternative HTML version deleted]]
>
>
>
>
>

	[[alternative HTML version deleted]]


From naimi.b at gmail.com  Mon Apr 10 13:37:11 2017
From: naimi.b at gmail.com (Babak Naimi)
Date: Mon, 10 Apr 2017 11:37:11 +0000
Subject: [R-sig-Geo] SDM course in the beautiful city of Evora, Portugal
Message-ID: <CAGL-CGCDYkomKjFbRcN6EcPGetR+peuFy4nC0wHbcORqjp6r_g@mail.gmail.com>

Dear List,

This is to inform you that the 4th edition of the PhD/Postdoc course
?Species distributions models: concepts, methods, applications, and
challenges? is organised by Prof. Miguel Ara?jo and Dr. Babak Naimi in
University of Evora, Portugal between 22th and 28th of May, 2017.

The course aims to introduce the fundamental concepts underpinning
ecological niche models (ENM), describing some of the most prominent
methods currently in use, and discussing the strengths and limitations of
these models for different applications. The course gives equal weight to
theory and application. The students will have the opportunity to learn how
to run ENM with the new R package, sdm, that is an object-oriented,
reproducible and extensible R platform for species distribution modelling (
http://www.biogeoinformatics.org/) that has been developed by the
organisers of the course.

For more details about the course and the registration, check the following
website:

http://www.maraujolab.com/2017-species-distributions-modelling-course/

You can also download the paper introduces the sdm R package from the
following link:

http://onlinelibrary.wiley.com/doi/10.1111/ecog.01881/full

Please feel free to forward this message to anyone who may be interested.

Best regards,

Babak Naimi

	[[alternative HTML version deleted]]


From tim.howard at dec.ny.gov  Mon Apr 10 14:16:42 2017
From: tim.howard at dec.ny.gov (Howard, Tim G (DEC))
Date: Mon, 10 Apr 2017 12:16:42 +0000
Subject: [R-sig-Geo]  Question  niche based model whith R
In-Reply-To: <mailman.5.1491818401.32106.r-sig-geo@r-project.org>
References: <mailman.5.1491818401.32106.r-sig-geo@r-project.org>
Message-ID: <SN1PR09MB1071E0969B37DA0B476C484FA8010@SN1PR09MB1071.namprd09.prod.outlook.com>


You might check out the dismo package and the vignette it has on species distribution modeling. 

https://cran.r-project.org/web/packages/dismo/index.html

Tim


   
Date: Mon, 10 Apr 2017 09:12:54 +0000 (UTC)
From: Soufianou Abou <a.soufianou at yahoo.fr>
To: "r-sig-geo at r-project.org" <r-sig-geo at r-project.org>
Subject: [R-sig-Geo] Question? niche based model whith R
Message-ID: <626667739.279201.1491815574118 at mail.yahoo.com>
Content-Type: text/plain; charset="UTF-8"

?Hello dears,I work on the modeling of the ecological niche of striga, I have presence data (data) and bioclimatic variables and other variables around.Indeed, my question how should I proceed to model the ecological niche with software R??Pending receipt of  my request, please accept my best regards.SADDA Abou-Soufianou -------------------------------------- DoctorantUniversit? Dan Dicko Dankoulodo de Maradi,120 avenue Maman Koraou,ADS, MaradiBP 465 Maradi/Niger E-mail:?a.soufianou at yahoo.frGSM?: (+227)96269987
??????? [[alternative HTML version deleted]]


From mercetadzio at gmail.com  Mon Apr 10 18:16:47 2017
From: mercetadzio at gmail.com (=?UTF-8?Q?Mercedes_Rom=C3=A1n?=)
Date: Mon, 10 Apr 2017 18:16:47 +0200
Subject: [R-sig-Geo] Ordinary co-kriging with gstat - computing time/syntax
	error?
Message-ID: <CAAOWriMQ0QZLTTK=P57UBnPPvejRmwewvW-_NauW0-++YdZ=cg@mail.gmail.com>

Dear R-sig-Geo members,

I want to perform co-kriging on the residuals on two regression models with
gstat. I am following some tutorials (Rossiter, 2012; Malone et al., 2017),
but I may have a mistake in the script. The computing time seems too long
(I know it takes a lot of calculation time, but in four days it never
finished), even when I set nmax=20. Originally, I wanted to perform a test
on ~ 11000 points, but I am trying now with a subset of 200 points. My
training dataset has ~ 27000 observations. I would like to produce a
reproducible example, but perhaps just the sintaxis of the script may be
enough to have a clue of my problem. My computer has an Intel?Xeon? CPU
E5-2609 v2 processor, and 32Go RAM memory.

Let?s say my two response variables are X1 and X2, and the dataframes with
the coordinates and covariates are ?training? and ?test?.

### Calculate the predictions on the training dataset (predictors is a
dataframe with the covariates)
training$pred.X1 <- predict(modelX1, newdata = predictors)
training$pred.X2 <- predict(modelX2, newdata = predictors)
### Calculate residuals
training$res.X1<- training$X1 - training$ pred.X1
training$res.X2 <- training$X2 - training$pred.X2
### Transform to spatial
training.sp <- training
coordinates(training.sp) <- ~ x + y
proj4string(training.sp) <- CRS("+init=epsg:2154")

### Define gstat object and compute experimental semivariograms for X1 and
X2 (for visual examination)
X1.g <- gstat(formula = res.X1~1, data = training.sp)
X1.svar <- variogram(X1.g, width = 5000, cutoff=250000)
X1.ivgm <- vgm(nugget=0.56, range=14229, psill=0.65, kappa=10, model="Mat")
## initial variogram model
X1.vgm <- fit.variogram(X1.svar, model=X1.ivgm, fit.method=7)
plot(X1.svar, X1.vgm)

X2.g <- gstat(formula = res.X2~1, data = training.sp)
X2.svar <- variogram(X2.g, width = 5000, cutoff=250000)
X2.ivgm <- vgm(nugget=0.19, range=13954, psill=0.21,kappa=10,   model="Mat")
X2.vgm <- fit.variogram(X2.svar, model=X2.ivgm, fit.method=7)
plot(X2.svar, X2.vgm)

### Define gstat object to compute the direct variograms and covariogram
g <- gstat( NULL, id="res.X1",  formula = res.X1~1, data = training.sp)
g <- gstat( g, id="res.X2",  formula = res.X2~1, data = training.sp)
v.cross <- variogram(g, width=5000, cutoff = 250000)
#### Fill parameters
g <- gstat(g, id = "res.X2", model = X2.vgm,  fill.all=T)
### fit LMCR
g <- fit.lmc(v.cross, g)
### Predict at the test locations (test.sp only indicates the coordinates)
test.sp <- test
coordinates(ensemble.sp) <- ~ x + y
proj4string(ensemble.sp) <- CRS("+init=epsg:2154")
k.c_residuals <- predict(g, test.sp, nmax=20)

I am wondering if there is something missing in the predict command? I do
not need to include the training data in it?
I have been reading previous posts from R-sig-Geo, but I still did not find
the solution. I would appreciate any advice you could give me.

Thanks,

Mercedes Roman

INRA

Unit? de Service InfoSol

2163, avenue de la Pomme de Pin

CS 40001 Ardon

45075 ORLEANS cedex 2

France

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Mon Apr 10 19:03:35 2017
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Mon, 10 Apr 2017 19:03:35 +0200
Subject: [R-sig-Geo] Ordinary co-kriging with gstat - computing
 time/syntax error?
In-Reply-To: <CAAOWriMQ0QZLTTK=P57UBnPPvejRmwewvW-_NauW0-++YdZ=cg@mail.gmail.com>
References: <CAAOWriMQ0QZLTTK=P57UBnPPvejRmwewvW-_NauW0-++YdZ=cg@mail.gmail.com>
Message-ID: <eb3f08f1-01bd-24f7-2dc7-d49cd14b6052@uni-muenster.de>

You need to set nmax for each variable, as a parameter of the gstat()
function call. I don't see nmax mentioned anywhere in the documentation
of gstat::predict.

On 10/04/17 18:16, Mercedes Rom?n wrote:
> Dear R-sig-Geo members,
> 
> I want to perform co-kriging on the residuals on two regression models with
> gstat. I am following some tutorials (Rossiter, 2012; Malone et al., 2017),
> but I may have a mistake in the script. The computing time seems too long
> (I know it takes a lot of calculation time, but in four days it never
> finished), even when I set nmax=20. Originally, I wanted to perform a test
> on ~ 11000 points, but I am trying now with a subset of 200 points. My
> training dataset has ~ 27000 observations. I would like to produce a
> reproducible example, but perhaps just the sintaxis of the script may be
> enough to have a clue of my problem. My computer has an Intel?Xeon? CPU
> E5-2609 v2 processor, and 32Go RAM memory.
> 
> Let?s say my two response variables are X1 and X2, and the dataframes with
> the coordinates and covariates are ?training? and ?test?.
> 
> ### Calculate the predictions on the training dataset (predictors is a
> dataframe with the covariates)
> training$pred.X1 <- predict(modelX1, newdata = predictors)
> training$pred.X2 <- predict(modelX2, newdata = predictors)
> ### Calculate residuals
> training$res.X1<- training$X1 - training$ pred.X1
> training$res.X2 <- training$X2 - training$pred.X2
> ### Transform to spatial
> training.sp <- training
> coordinates(training.sp) <- ~ x + y
> proj4string(training.sp) <- CRS("+init=epsg:2154")
> 
> ### Define gstat object and compute experimental semivariograms for X1 and
> X2 (for visual examination)
> X1.g <- gstat(formula = res.X1~1, data = training.sp)
> X1.svar <- variogram(X1.g, width = 5000, cutoff=250000)
> X1.ivgm <- vgm(nugget=0.56, range=14229, psill=0.65, kappa=10, model="Mat")
> ## initial variogram model
> X1.vgm <- fit.variogram(X1.svar, model=X1.ivgm, fit.method=7)
> plot(X1.svar, X1.vgm)
> 
> X2.g <- gstat(formula = res.X2~1, data = training.sp)
> X2.svar <- variogram(X2.g, width = 5000, cutoff=250000)
> X2.ivgm <- vgm(nugget=0.19, range=13954, psill=0.21,kappa=10,   model="Mat")
> X2.vgm <- fit.variogram(X2.svar, model=X2.ivgm, fit.method=7)
> plot(X2.svar, X2.vgm)
> 
> ### Define gstat object to compute the direct variograms and covariogram
> g <- gstat( NULL, id="res.X1",  formula = res.X1~1, data = training.sp)
> g <- gstat( g, id="res.X2",  formula = res.X2~1, data = training.sp)
> v.cross <- variogram(g, width=5000, cutoff = 250000)
> #### Fill parameters
> g <- gstat(g, id = "res.X2", model = X2.vgm,  fill.all=T)
> ### fit LMCR
> g <- fit.lmc(v.cross, g)
> ### Predict at the test locations (test.sp only indicates the coordinates)
> test.sp <- test
> coordinates(ensemble.sp) <- ~ x + y
> proj4string(ensemble.sp) <- CRS("+init=epsg:2154")
> k.c_residuals <- predict(g, test.sp, nmax=20)
> 
> I am wondering if there is something missing in the predict command? I do
> not need to include the training data in it?
> I have been reading previous posts from R-sig-Geo, but I still did not find
> the solution. I would appreciate any advice you could give me.
> 
> Thanks,
> 
> Mercedes Roman
> 
> INRA
> 
> Unit? de Service InfoSol
> 
> 2163, avenue de la Pomme de Pin
> 
> CS 40001 Ardon
> 
> 45075 ORLEANS cedex 2
> 
> France
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170410/28442d14/attachment.sig>

From qiuhuanihao at gmail.com  Tue Apr 11 01:27:42 2017
From: qiuhuanihao at gmail.com (Qiuhua Ma)
Date: Mon, 10 Apr 2017 17:27:42 -0600
Subject: [R-sig-Geo] variance-covariance matrix for GMerrorsar
Message-ID: <CACc7PKqU6Pv_Z8r-MGNT4ryw4W50AzJesaVscBV15aBTK14ZJQ@mail.gmail.com>

Dear list,

I want to use bootstrapping to derive confidence intervals for marginal
wtp after GMerrorsar command.

It works for stsls since covariance matrix is directly available. However,
I cannot find covariance matrix for GMerrorsar.

For example, the following code works for stsls:

model1.beta <- coef(model1)

model1.vcov <- summary(model1)$var

model1.sim <- rmultnorm(10000, mu = model1.beta, vmat = model1.vcov)

model1.mwtp <- model1.sim * Pbar

model1.ci <- apply(model1.mwtp, 2, quantile, c(0.025, 0.975))


when I apply the same code for GMerrorsar:


model2.beta <- coef(model2)

model2.vcov <- summary(model2)$var


> model2.vcov

NULL


How can I obtain covariance matrix for GMerrorsar?


Chelsea

	[[alternative HTML version deleted]]


From tom.hengl at gmail.com  Tue Apr 11 15:08:53 2017
From: tom.hengl at gmail.com (Tomislav Hengl)
Date: Tue, 11 Apr 2017 23:08:53 +1000
Subject: [R-sig-Geo] GEOSTAT Summer School for PhD students | 13-19
 August 2017 | MEDILS Split
In-Reply-To: <dcc8495e-7c34-7687-f758-2b965f130916@gmail.com>
References: <9aaeb793-4db7-5669-2079-5edee312428d@spatial-analyst.net>
 <dcc8495e-7c34-7687-f758-2b965f130916@gmail.com>
Message-ID: <b7c0ab95-a5a6-b440-5f69-d4cd916ec8d1@gmail.com>

MEMO: This is to remind you that the registrations for GEOSTAT Summer 
School 13-19 August 2017 | MEDILS Split, will close on:

- 15th of April 2017

To register for GEOSTAT visit:

https://geostat-course.org/2017

PS: Current number of registrations: 44 (this summer school is limited 
to 55 participants).

T. Hengl
http://www.wageningenur.nl/en/Persons/dr.-T-Tom-Hengl.htm


On 06/03/17 19:58, Tomislav Hengl wrote:
>
> The GEOSTAT Split 2017 Summer School will be held this year at the 
> Mediterranean Institute for Life Sciences (MEDILS) Split, Croatia in 
> the period 13-19 August 2017. MEDILS is an international centre of 
> excellence for molecular biology and is located in a nature park in a 
> near proximity of the historic town Split. It has in-house 
> accommodation and numerous facilities including its own park and sport 
> facilities.
>
> The registrations are now open. To register for this Summer School 
> please visit:
>
> https://geostat-course.org/2017
>
> This summer school is limited to 55 participants. In the case of 
> higher number of registrations, applicants will be ranked based on (1) 
> their contribution to the FOSS, (2) academic output, (3) distance to 
> the venue and (4) time of registration. The registrations fees are 
> usually around 550 EUR (invoice will be sent after registrations). 
> Registration fees cover costs of using facilities, lunch and coffee 
> breaks and costs of travel and accommodation for lecturers. 
> Participants from ODA countries (employed by an organization or 
> company in ODA-listed country) typically receive a subsidized price 
> for the accommodation and registration costs. GEOSTAT makes no profit. 
> All lecturers are volunteers. None of the lecturers receives any 
> honorarium payment or is contracted by the local organizers.
>
> This years topics include:
> - Application of R+OSGeo tools in: spatio-temporal monitoring, 
> geostatistical mapping, point pattern analysis, epidemiology...
> - Machine Learning methods for spatial and spatio-temporal data,
> - QGIS and RQGIS tutorials,
> - RMarkdown/Sweave tutorials,
> - Visualization of spatio-temporal data,
>
> Lecturers:
> - Barry Rowlingson (Lancaster Medical School, Lancaster University),
> - Virgilio Gomez Rubio (Deparment of Mathematics, Universidad de 
> Castilla-La Mancha),
> - Jannes Muenchow (Department of GIScience, University of Jena),
> - Daniel N?st (Institute for Geoinformatics, University of M?nster)
> - Tomislav Hengl (ISRIC - World Soil Information),
>
> Registration deadline is:
>
> - April 15th 2017
>
> hope to see you in Split,
>
> T. Hengl
> http://www.wageningenur.nl/en/Persons/dr.-T-Tom-Hengl.htm


From Roger.Bivand at nhh.no  Tue Apr 11 15:54:19 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 11 Apr 2017 15:54:19 +0200
Subject: [R-sig-Geo] variance-covariance matrix for GMerrorsar
In-Reply-To: <CACc7PKqU6Pv_Z8r-MGNT4ryw4W50AzJesaVscBV15aBTK14ZJQ@mail.gmail.com>
References: <CACc7PKqU6Pv_Z8r-MGNT4ryw4W50AzJesaVscBV15aBTK14ZJQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1704111525550.20756@reclus.nhh.no>

On Tue, 11 Apr 2017, Qiuhua Ma wrote:

> Dear list,
>
> I want to use bootstrapping to derive confidence intervals for marginal
> wtp after GMerrorsar command.
>
> It works for stsls since covariance matrix is directly available. However,
> I cannot find covariance matrix for GMerrorsar.
>
> For example, the following code works for stsls:
>
> model1.beta <- coef(model1)
>
> model1.vcov <- summary(model1)$var
>
> model1.sim <- rmultnorm(10000, mu = model1.beta, vmat = model1.vcov)
>
> model1.mwtp <- model1.sim * Pbar
>
> model1.ci <- apply(model1.mwtp, 2, quantile, c(0.025, 0.975))

The DGP for this model is (I - \rho W)^{-1} (X \beta + e), so I'm in geat 
doubt about whether your proposal is correct (model1.vcov is has one more 
row and column than X has columns, so including \rho); the first element 
of model1.beta is \rho.

>
>
> when I apply the same code for GMerrorsar:
>
>
> model2.beta <- coef(model2)
>
> model2.vcov <- summary(model2)$var
>
>
>> model2.vcov
>
> NULL
>
>
> How can I obtain covariance matrix for GMerrorsar?
>
Reading the code, you'll see where the matrices occur. Running under 
debug, you can assign the outside the environment of the function if you 
like (use <<- ). I've added a vcov component in the returned object 
(source on R-Forge, I can send a source package or a Windows binary 
package).

You should also look at sphet::spreg, which does return a var component. 
Please note that you should think of the DGP first and foremost, the coef 
and var may return the values for what you are treating as nuisance parts 
of the model. Getting the distribution of the willingess to pay also 
probably involves them and their variability.

Have you considered getting the WTP marginal from a Bayesian approach?

Hope this helps,

Roger


>
> Chelsea
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Scott.Waichler at pnnl.gov  Tue Apr 11 16:40:52 2017
From: Scott.Waichler at pnnl.gov (Waichler, Scott R)
Date: Tue, 11 Apr 2017 14:40:52 +0000
Subject: [R-sig-Geo] best way to find contiguous chunks in a sparse
	space-time matrix
Message-ID: <074C83DAD4825242A20B2D83FDBCB8881BA1280E@EX10MBOX03.pnnl.gov>

Hi, I have a space-time sparse grid layout where columns are locations and rows are time.  In general, a location is missing data for some of its times.  I need to subset the layout such that I can represent contiguous blocks of time, wherein for each block of time (rows), all of the selected locations have data.  The tradeoff is that I would like to minimize the number of blocks needed to span the total range in time, while also keeping  as many locations as possible for any given block.  When done, the resulting selection might look like this, where X are non-selected space-time, and the numbers denote selected space-time, 1 . . . k where k is the number of blocks to span the total time range of interest.  

1 1 1 X 1 X 1 1 X  
1 1 1 X 1 X 1 1 X  
1 1 1 X 1 X 1 1 X  
X 2 2 X X 2 2 2 2
X 2 2 X X 2 2 2 2
X 2 2 X X 2 2 2 2
X 2 2 X X 2 2 2 2
X 2 2 X X 2 2 2 2
3 3 X 3 3 3 3 X 3  
3 3 X 3 3 3 3 X 3  
3 3 X 3 3 3 3 X 3  
3 3 X 3 3 3 3 X 3 
. . . 

For starters, I am content to ignore spatial relationships between the locations represented in the columns.  Later, it would be wonderful if I could consider spatial proximity such that a location that has missing values is more likely to be left out of selection if there are nearby locations that have data and can "cover" for the problematic location.  

If anyone has some suggestions on approaches to solve this problem, I would greatly appreciate it.

Thank you,
Scott Waichler
Pacific Northwest National Laboratory
Richland, WA    USA


From mael.lenoc at laposte.net  Wed Apr 12 03:14:03 2017
From: mael.lenoc at laposte.net (=?UTF-8?Q?Ma=c3=abl_Le_Noc?=)
Date: Tue, 11 Apr 2017 20:14:03 -0500
Subject: [R-sig-Geo] dnearneigh() from spdep: Points with the exact same
 location are not considered neighbours.
Message-ID: <bf737ebe-cf93-ada7-2ab2-ed52f48b38b5@laposte.net>

Dear List

As I was working on a project, I realized that when I use dnearneigh
from spdep, two (or more) points that have the exact same coordinates
are not considered neighbours and thus are not linked (even when the
lower bound is put to 0 or even to -1). See below for an example.
(However this does not happen if the parameter longlat is set to false)

 Does the function behave the same way for you? Am I missing something?
Is this an expected behavior? And if so, if there a way to change that ?

In the example below, points 1 and 2 are not connected to each other/are
not neighbours (as you can see since the both have only one link, to 3),
even though they have the exact same coordinates (and are thus less than
25km apart), while point 3 is connected to both point 1 and 2.
If I want to assess autocorrelation using, for instance joincount.test,
this is then an issue...

>/library(data.table) />/library(spdep) />/pointstable <- data.table(XCoord=c(13.667029,13.667029,13.667028), /YCoord=c(42.772396,42.772396,42.772396))
>/print(pointstable) /     XCoord  YCoord
1: 13.667029 42.772396
2: 13.667029 42.772396
3: 13.667028 42.772396
>/coords <-cbind(pointstable$XCoord, pointstable$YCoord) />/nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = TRUE) />/nbLocal<- dnearneigh(coords, d1=-1, d2=25, longlat = TRUE) #both lines /produce the same output
>/summary(nbLocal) /Neighbour list object:
Number of regions: 3
Number of nonzero links: 4
Percentage nonzero weights: 44.44444
Average number of links: 1.333333
Link number distribution:

1 2
2 1
2 least connected regions:
1 2 with 1 link
1 most connected region:
3 with 2 links
>//
Thanks
Ma?l


	[[alternative HTML version deleted]]


From qiuhuanihao at gmail.com  Wed Apr 12 06:59:04 2017
From: qiuhuanihao at gmail.com (Qiuhua Ma)
Date: Tue, 11 Apr 2017 22:59:04 -0600
Subject: [R-sig-Geo] variance-covariance matrix for GMerrorsar
In-Reply-To: <alpine.LFD.2.20.1704111525550.20756@reclus.nhh.no>
References: <CACc7PKqU6Pv_Z8r-MGNT4ryw4W50AzJesaVscBV15aBTK14ZJQ@mail.gmail.com>
 <alpine.LFD.2.20.1704111525550.20756@reclus.nhh.no>
Message-ID: <CACc7PKpmFnRguuR=P0y2J+_SndgJp=SJz=sxJ-_DvyneMQCr_w@mail.gmail.com>

Thanks for your quick reply. You are right. Marginal wtp should take into
account rho for spatial lag model.

I still would like to use GMerrorsar. Can you please send me the source
package?

Best,

Chelsea

On Tue, Apr 11, 2017 at 7:54 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Tue, 11 Apr 2017, Qiuhua Ma wrote:
>
> Dear list,
>>
>> I want to use bootstrapping to derive confidence intervals for marginal
>> wtp after GMerrorsar command.
>>
>> It works for stsls since covariance matrix is directly available. However,
>> I cannot find covariance matrix for GMerrorsar.
>>
>> For example, the following code works for stsls:
>>
>> model1.beta <- coef(model1)
>>
>> model1.vcov <- summary(model1)$var
>>
>> model1.sim <- rmultnorm(10000, mu = model1.beta, vmat = model1.vcov)
>>
>> model1.mwtp <- model1.sim * Pbar
>>
>> model1.ci <- apply(model1.mwtp, 2, quantile, c(0.025, 0.975))
>>
>
> The DGP for this model is (I - \rho W)^{-1} (X \beta + e), so I'm in geat
> doubt about whether your proposal is correct (model1.vcov is has one more
> row and column than X has columns, so including \rho); the first element of
> model1.beta is \rho.
>
>
>>
>> when I apply the same code for GMerrorsar:
>>
>>
>> model2.beta <- coef(model2)
>>
>> model2.vcov <- summary(model2)$var
>>
>>
>> model2.vcov
>>>
>>
>> NULL
>>
>>
>> How can I obtain covariance matrix for GMerrorsar?
>>
>> Reading the code, you'll see where the matrices occur. Running under
> debug, you can assign the outside the environment of the function if you
> like (use <<- ). I've added a vcov component in the returned object (source
> on R-Forge, I can send a source package or a Windows binary package).
>
> You should also look at sphet::spreg, which does return a var component.
> Please note that you should think of the DGP first and foremost, the coef
> and var may return the values for what you are treating as nuisance parts
> of the model. Getting the distribution of the willingess to pay also
> probably involves them and their variability.
>
> Have you considered getting the WTP marginal from a Bayesian approach?
>
> Hope this helps,
>
> Roger
>
>
>
>> Chelsea
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From qiuhuanihao at gmail.com  Wed Apr 12 07:26:28 2017
From: qiuhuanihao at gmail.com (Qiuhua Ma)
Date: Tue, 11 Apr 2017 23:26:28 -0600
Subject: [R-sig-Geo] variance-covariance matrix for GMerrorsar
In-Reply-To: <CACc7PKpmFnRguuR=P0y2J+_SndgJp=SJz=sxJ-_DvyneMQCr_w@mail.gmail.com>
References: <CACc7PKqU6Pv_Z8r-MGNT4ryw4W50AzJesaVscBV15aBTK14ZJQ@mail.gmail.com>
 <alpine.LFD.2.20.1704111525550.20756@reclus.nhh.no>
 <CACc7PKpmFnRguuR=P0y2J+_SndgJp=SJz=sxJ-_DvyneMQCr_w@mail.gmail.com>
Message-ID: <CACc7PKpVLwHVXhM4uhe3c2vodCJBe1KqXK4dKiPqoTRbeQAHUQ@mail.gmail.com>

Is this the link to the package?

https://r-forge.r-project.org/R/?group_id=182

Chelsea

On Tue, Apr 11, 2017 at 10:59 PM, Qiuhua Ma <qiuhuanihao at gmail.com> wrote:

> Thanks for your quick reply. You are right. Marginal wtp should take into
> account rho for spatial lag model.
>
> I still would like to use GMerrorsar. Can you please send me the source
> package?
>
> Best,
>
> Chelsea
>
> On Tue, Apr 11, 2017 at 7:54 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Tue, 11 Apr 2017, Qiuhua Ma wrote:
>>
>> Dear list,
>>>
>>> I want to use bootstrapping to derive confidence intervals for marginal
>>> wtp after GMerrorsar command.
>>>
>>> It works for stsls since covariance matrix is directly available.
>>> However,
>>> I cannot find covariance matrix for GMerrorsar.
>>>
>>> For example, the following code works for stsls:
>>>
>>> model1.beta <- coef(model1)
>>>
>>> model1.vcov <- summary(model1)$var
>>>
>>> model1.sim <- rmultnorm(10000, mu = model1.beta, vmat = model1.vcov)
>>>
>>> model1.mwtp <- model1.sim * Pbar
>>>
>>> model1.ci <- apply(model1.mwtp, 2, quantile, c(0.025, 0.975))
>>>
>>
>> The DGP for this model is (I - \rho W)^{-1} (X \beta + e), so I'm in geat
>> doubt about whether your proposal is correct (model1.vcov is has one more
>> row and column than X has columns, so including \rho); the first element of
>> model1.beta is \rho.
>>
>>
>>>
>>> when I apply the same code for GMerrorsar:
>>>
>>>
>>> model2.beta <- coef(model2)
>>>
>>> model2.vcov <- summary(model2)$var
>>>
>>>
>>> model2.vcov
>>>>
>>>
>>> NULL
>>>
>>>
>>> How can I obtain covariance matrix for GMerrorsar?
>>>
>>> Reading the code, you'll see where the matrices occur. Running under
>> debug, you can assign the outside the environment of the function if you
>> like (use <<- ). I've added a vcov component in the returned object (source
>> on R-Forge, I can send a source package or a Windows binary package).
>>
>> You should also look at sphet::spreg, which does return a var component.
>> Please note that you should think of the DGP first and foremost, the coef
>> and var may return the values for what you are treating as nuisance parts
>> of the model. Getting the distribution of the willingess to pay also
>> probably involves them and their variability.
>>
>> Have you considered getting the WTP marginal from a Bayesian approach?
>>
>> Hope this helps,
>>
>> Roger
>>
>>
>>
>>> Chelsea
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> Editor-in-Chief of The R Journal, https://journal.r-project.org/
>> index.html
>> http://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
>

	[[alternative HTML version deleted]]


From ptitle at umich.edu  Tue Apr 11 19:33:00 2017
From: ptitle at umich.edu (Pascal Title)
Date: Tue, 11 Apr 2017 13:33:00 -0400
Subject: [R-sig-Geo] bug in plotting rasterLayer?
Message-ID: <CAPSA=ychwQJQu7NWBqEWUP=DsRKeRMXeD+_zNnbatrEwfG2VBA@mail.gmail.com>

Hi,

I found that when I try to plot 3 rasters on top of each other, the third
raster plot causes the plot device coordinates to shift to the right.

I can reproduce this error easily:

library(raster)
f <- system.file("external/test.grd", package="raster")
r <- raster(f)

plot(r)

plot(r, add=TRUE)
plot(r, add=TRUE)
plot(r, add=TRUE)

For me, the 3rd plot with add=TRUE causes a coordinate system shift.

I am running R v3.3.3 with raster package v2.5-8.

Are others finding the same thing?

-- 
Pascal Title
PhD candidate | Rabosky Lab <http://www.raboskylab.org/>
Dept of Ecology and Evolutionary Biology
University of Michigan
ptitle at umich.edu

pascaltitle.weebly.com

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Wed Apr 12 09:27:57 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 12 Apr 2017 09:27:57 +0200
Subject: [R-sig-Geo] dnearneigh() from spdep: Points with the exact same
 location are not considered neighbours.
In-Reply-To: <bf737ebe-cf93-ada7-2ab2-ed52f48b38b5@laposte.net>
References: <bf737ebe-cf93-ada7-2ab2-ed52f48b38b5@laposte.net>
Message-ID: <alpine.LFD.2.20.1704120919520.29243@reclus.nhh.no>

Do not post HTML-mail, only plain text. Your example is not reproducible 
because you used HTML-mail.

Please read the help file, the bounds are described as being between lower 
(greater than) and upper (less than or equal to) bounds. Since the 
distance between identical points is strictly zero, they are not 
neighbours because the distance must be > d1 and <= d2. If d1 is < 0, it 
is reset to 0, as it is assumed that a negative lower bound is a user 
error (and it would break the underlying compiled code).

In any case, no reasonable cross-sectional spatial process has duplicated 
point (nugget) observations in situations in which spatial weights would 
be used (spatio-temporal panels will have, but then time differs).

Hope this clarifies,

Roger

On Wed, 12 Apr 2017, Ma?l Le Noc via R-sig-Geo wrote:

> Dear List
>
> As I was working on a project, I realized that when I use dnearneigh
> from spdep, two (or more) points that have the exact same coordinates
> are not considered neighbours and thus are not linked (even when the
> lower bound is put to 0 or even to -1). See below for an example.
> (However this does not happen if the parameter longlat is set to false)
>
> Does the function behave the same way for you? Am I missing something?
> Is this an expected behavior? And if so, if there a way to change that ?
>
> In the example below, points 1 and 2 are not connected to each other/are
> not neighbours (as you can see since the both have only one link, to 3),
> even though they have the exact same coordinates (and are thus less than
> 25km apart), while point 3 is connected to both point 1 and 2.
> If I want to assess autocorrelation using, for instance joincount.test,
> this is then an issue...
>
>> /library(data.table) />/library(spdep) />/pointstable <- data.table(XCoord=c(13.667029,13.667029,13.667028), /YCoord=c(42.772396,42.772396,42.772396))
>> /print(pointstable) /     XCoord  YCoord
> 1: 13.667029 42.772396
> 2: 13.667029 42.772396
> 3: 13.667028 42.772396
>> /coords <-cbind(pointstable$XCoord, pointstable$YCoord) />/nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = TRUE) />/nbLocal<- dnearneigh(coords, d1=-1, d2=25, longlat = TRUE) #both lines /produce the same output
>> /summary(nbLocal) /Neighbour list object:
> Number of regions: 3
> Number of nonzero links: 4
> Percentage nonzero weights: 44.44444
> Average number of links: 1.333333
> Link number distribution:
>
> 1 2
> 2 1
> 2 least connected regions:
> 1 2 with 1 link
> 1 most connected region:
> 3 with 2 links
>> //
> Thanks
> Ma?l
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger.Bivand at nhh.no  Wed Apr 12 09:29:41 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 12 Apr 2017 09:29:41 +0200
Subject: [R-sig-Geo] variance-covariance matrix for GMerrorsar
In-Reply-To: <CACc7PKpVLwHVXhM4uhe3c2vodCJBe1KqXK4dKiPqoTRbeQAHUQ@mail.gmail.com>
References: <CACc7PKqU6Pv_Z8r-MGNT4ryw4W50AzJesaVscBV15aBTK14ZJQ@mail.gmail.com>
 <alpine.LFD.2.20.1704111525550.20756@reclus.nhh.no>
 <CACc7PKpmFnRguuR=P0y2J+_SndgJp=SJz=sxJ-_DvyneMQCr_w@mail.gmail.com>
 <CACc7PKpVLwHVXhM4uhe3c2vodCJBe1KqXK4dKiPqoTRbeQAHUQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1704120928320.29243@reclus.nhh.no>

On Wed, 12 Apr 2017, Qiuhua Ma wrote:

> Is this the link to the package?
>
> https://r-forge.r-project.org/R/?group_id=182

Yes, and:

install.packages("spdep", repos="http://R-Forge.R-project.org")

should work. But do try sphet::spreg() too, it is more flexible than the 
older spdep GM functions.

Roger

>
> Chelsea
>
> On Tue, Apr 11, 2017 at 10:59 PM, Qiuhua Ma <qiuhuanihao at gmail.com> wrote:
>
>> Thanks for your quick reply. You are right. Marginal wtp should take into
>> account rho for spatial lag model.
>>
>> I still would like to use GMerrorsar. Can you please send me the source
>> package?
>>
>> Best,
>>
>> Chelsea
>>
>> On Tue, Apr 11, 2017 at 7:54 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>
>>> On Tue, 11 Apr 2017, Qiuhua Ma wrote:
>>>
>>> Dear list,
>>>>
>>>> I want to use bootstrapping to derive confidence intervals for marginal
>>>> wtp after GMerrorsar command.
>>>>
>>>> It works for stsls since covariance matrix is directly available.
>>>> However,
>>>> I cannot find covariance matrix for GMerrorsar.
>>>>
>>>> For example, the following code works for stsls:
>>>>
>>>> model1.beta <- coef(model1)
>>>>
>>>> model1.vcov <- summary(model1)$var
>>>>
>>>> model1.sim <- rmultnorm(10000, mu = model1.beta, vmat = model1.vcov)
>>>>
>>>> model1.mwtp <- model1.sim * Pbar
>>>>
>>>> model1.ci <- apply(model1.mwtp, 2, quantile, c(0.025, 0.975))
>>>>
>>>
>>> The DGP for this model is (I - \rho W)^{-1} (X \beta + e), so I'm in geat
>>> doubt about whether your proposal is correct (model1.vcov is has one more
>>> row and column than X has columns, so including \rho); the first element of
>>> model1.beta is \rho.
>>>
>>>
>>>>
>>>> when I apply the same code for GMerrorsar:
>>>>
>>>>
>>>> model2.beta <- coef(model2)
>>>>
>>>> model2.vcov <- summary(model2)$var
>>>>
>>>>
>>>> model2.vcov
>>>>>
>>>>
>>>> NULL
>>>>
>>>>
>>>> How can I obtain covariance matrix for GMerrorsar?
>>>>
>>>> Reading the code, you'll see where the matrices occur. Running under
>>> debug, you can assign the outside the environment of the function if you
>>> like (use <<- ). I've added a vcov component in the returned object (source
>>> on R-Forge, I can send a source package or a Windows binary package).
>>>
>>> You should also look at sphet::spreg, which does return a var component.
>>> Please note that you should think of the DGP first and foremost, the coef
>>> and var may return the values for what you are treating as nuisance parts
>>> of the model. Getting the distribution of the willingess to pay also
>>> probably involves them and their variability.
>>>
>>> Have you considered getting the WTP marginal from a Bayesian approach?
>>>
>>> Hope this helps,
>>>
>>> Roger
>>>
>>>
>>>
>>>> Chelsea
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> Editor-in-Chief of The R Journal, https://journal.r-project.org/
>>> index.html
>>> http://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From dmwarner at usgs.gov  Wed Apr 12 12:05:56 2017
From: dmwarner at usgs.gov (Warner, David)
Date: Wed, 12 Apr 2017 06:05:56 -0400
Subject: [R-sig-Geo] raster and oceancolor L2 netcdf data
Message-ID: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>

Greetings all

I am trying to develop R code for processing L2 data (netcdf v4 files) from
the Ocean Biology Processing Group.

The data file I am working with to develop the code is at
https://github.com/dmwarn/Tethys/blob/master/A2016244185500.L2_LAC_OC.x.nc
and represents primarily Lake Michigan in the United States.  The data were
extracted from a global dataset by the oceancolor L1 and L2 data server,
not by me.

I have been using the code below to try to get the data into R and ready
for processing but am having problems with dimensions and/or
orthorectification.  The

#extent of the scene obtained using nc_open and ncvar_get
nc <- nc_open('A2016214184500.L2_LAC_OC.x.nc')
lon <- ncvar_get(nc, "navigation_data/longitude")
lat <- ncvar_get(nc, "navigation_data/latitude")
minx <- min(lon)
maxx <- max(lon)
miny <- min(lat)
maxy <- max(lat)

#create extent object
myext <- extent(-90.817, -81.92438, 40.46493, 47.14244)

#create raster
rrs.412 <- raster('A2016214184500.L2_LAC_OC.x.nc', var
="geophysical_data/Rrs_412" ,
                  ext=myext)
rrs.412
> rrs.412
class       : RasterLayer
dimensions  : 644, 528, 340032  (nrow, ncol, ncell)
resolution  : 1, 1  (x, y)
extent      : 0.5, 528.5, 0.5, 644.5  (xmin, xmax, ymin, ymax)
coord. ref. : NA
data source : /Users/dmwarner/Documents/MODIS/OC/
A2016214184500.L2_LAC_OC.x.nc
names       : Remote.sensing.reflectance.at.412.nm
zvar        : geophysical_data/Rrs_412

In spite of having tried to assign an extent, the raster extent is in rows
and columns.

Further, plotting the raster reveals that it is flipped on x axis and
somewhat rotated relative to what it should look like.  Even when flipped,
it is still not orthorectified.

How do I get the raster to have the correct extent and also get it
orthorectified?
Thanks,
Dave Warner

-- 
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Wed Apr 12 13:01:03 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 12 Apr 2017 11:01:03 +0000
Subject: [R-sig-Geo] raster and oceancolor L2 netcdf data
In-Reply-To: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>
References: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>
Message-ID: <CAAcGz98xcapeUajSHzsi=C2ZXbthYSZ=_=MDQMPZumwzKXD60w@mail.gmail.com>

You can't georeference these data without remapping the data, essentially
treating the pixels as points. They have no natural regular grid form,
except possibly a unique satellite-perspective one. The data are in 2D
array form, but they have explicit "geolocation arrays", i.e. a longitude
and latitude for every cell and not based on a regular mapping.

R does not have tools for this directly from these data, though it can be
treated as a resampling or modelling problem.
You can use raster to get at the values of the locations easily enough,
here's a couple of plotting options in case it's useful:

u <- "
https://github.com/dmwarn/Tethys/blob/master/A2016244185500.L2_LAC_OC.x.nc?raw=true
"
f <- basename(f)
download.file(u, f, mode = "wb")

library(raster)
## use raster to treat as raw point data, on long-lat locations
rrs <- raster(f, varname = "geophysical_data/Rrs_412")
longitude <- raster(f, varname = "navigation_data/longitude")
latitude <- raster(f, varname = "navigation_data/latitude")

## plot in grid space, and add the geolocation space as a graticule
plot(rrs)
contour(longitude, add = TRUE)
contour(latitude, add = TRUE)

## raw scaling against rrs values
scl <- function(x) (x - min(x, na.rm = TRUE))/diff(range(x, na.rm = TRUE))
plot(values(longitude), values(latitude), pch = ".", col =
topo.colors(56)[scl(values(rrs)) * 55 + 1])

## ggplot
library(ggplot2)
d <- data.frame(x = values(longitude), y = values(latitude), rrs =
values(rrs))
ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = ".")

## might as well discard the missing values (depends on the other vars in
the file)
d <- d[!is.na(d$rrs), ]
ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = 19, cex = 0.1)

There are some MODIS and GDAL based packages that might be of use, but I
haven't yet seen any R tool that does this remapping task at scale. (I
believe the MODIS tools and the best warping tools in GDAL use thin-plate
spline techniques).

 Some applications would use the observations as points (i.e. the ocean
colour L3 bins as a daily aggregate from L2) and others use a direct
remapping of the data as an array, for say high-resolution sea ice imagery.

You might not need to do anything particularly complicated though, what's
the goal?

Cheers, Mike.

On Wed, Apr 12, 2017, 20:06 Warner, David <dmwarner at usgs.gov> wrote:

Greetings all

I am trying to develop R code for processing L2 data (netcdf v4 files) from
the Ocean Biology Processing Group.

The data file I am working with to develop the code is at
https://github.com/dmwarn/Tethys/blob/master/A2016244185500.L2_LAC_OC.x.nc
and represents primarily Lake Michigan in the United States.  The data were
extracted from a global dataset by the oceancolor L1 and L2 data server,
not by me.

I have been using the code below to try to get the data into R and ready
for processing but am having problems with dimensions and/or
orthorectification.  The

#extent of the scene obtained using nc_open and ncvar_get
nc <- nc_open('A2016214184500.L2_LAC_OC.x.nc')
lon <- ncvar_get(nc, "navigation_data/longitude")
lat <- ncvar_get(nc, "navigation_data/latitude")
minx <- min(lon)
maxx <- max(lon)
miny <- min(lat)
maxy <- max(lat)

#create extent object
myext <- extent(-90.817, -81.92438, 40.46493, 47.14244)

#create raster
rrs.412 <- raster('A2016214184500.L2_LAC_OC.x.nc', var
="geophysical_data/Rrs_412" ,
                  ext=myext)
rrs.412
> rrs.412
class       : RasterLayer
dimensions  : 644, 528, 340032  (nrow, ncol, ncell)
resolution  : 1, 1  (x, y)
extent      : 0.5, 528.5, 0.5, 644.5  (xmin, xmax, ymin, ymax)
coord. ref. : NA
data source : /Users/dmwarner/Documents/MODIS/OC/
A2016214184500.L2_LAC_OC.x.nc
names       : Remote.sensing.reflectance.at.412.nm
zvar        : geophysical_data/Rrs_412

In spite of having tried to assign an extent, the raster extent is in rows
and columns.

Further, plotting the raster reveals that it is flipped on x axis and
somewhat rotated relative to what it should look like.  Even when flipped,
it is still not orthorectified.

How do I get the raster to have the correct extent and also get it
orthorectified?
Thanks,
Dave Warner

--
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392 <(734)%20214-9392>

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From dmwarner at usgs.gov  Wed Apr 12 14:35:16 2017
From: dmwarner at usgs.gov (Warner, David)
Date: Wed, 12 Apr 2017 08:35:16 -0400
Subject: [R-sig-Geo] raster and oceancolor L2 netcdf data
In-Reply-To: <CAAcGz98xcapeUajSHzsi=C2ZXbthYSZ=_=MDQMPZumwzKXD60w@mail.gmail.com>
References: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>
 <CAAcGz98xcapeUajSHzsi=C2ZXbthYSZ=_=MDQMPZumwzKXD60w@mail.gmail.com>
Message-ID: <CA+Y23taQCGwVD4_QYFNP6+tODFw5FS44zn_Qw3W_nF7GUVxBgQ@mail.gmail.com>

Thanks Mike!

The goal is to estimate daily chlorophyll via band ratio polynomial
equation for hundreds of days of data (hundreds of data files).  Sounds
like rather than finding a way to orthorectify in R I should learn to batch
reproject using SeaDAS, which does produce a product that is in geotiff
format, is orthorectified, and has readily mappable.  I was trying to avoid
that as the help and documentation available for doing that seems much less
abundant.  One file at a time is easy using the SeaDAS gui.

Thanks very, very much for the other tricks.  Not surprisingly, ggplot2
comes through again with plots that look right!
Cheers,
Dave



On Wed, Apr 12, 2017 at 7:01 AM, Michael Sumner <mdsumner at gmail.com> wrote:

> You can't georeference these data without remapping the data, essentially
> treating the pixels as points. They have no natural regular grid form,
> except possibly a unique satellite-perspective one. The data are in 2D
> array form, but they have explicit "geolocation arrays", i.e. a longitude
> and latitude for every cell and not based on a regular mapping.
>
> R does not have tools for this directly from these data, though it can be
> treated as a resampling or modelling problem.
> You can use raster to get at the values of the locations easily enough,
> here's a couple of plotting options in case it's useful:
>
> u <- "https://github.com/dmwarn/Tethys/blob/master/
> A2016244185500.L2_LAC_OC.x.nc?raw=true"
> f <- basename(f)
> download.file(u, f, mode = "wb")
>
> library(raster)
> ## use raster to treat as raw point data, on long-lat locations
> rrs <- raster(f, varname = "geophysical_data/Rrs_412")
> longitude <- raster(f, varname = "navigation_data/longitude")
> latitude <- raster(f, varname = "navigation_data/latitude")
>
> ## plot in grid space, and add the geolocation space as a graticule
> plot(rrs)
> contour(longitude, add = TRUE)
> contour(latitude, add = TRUE)
>
> ## raw scaling against rrs values
> scl <- function(x) (x - min(x, na.rm = TRUE))/diff(range(x, na.rm = TRUE))
> plot(values(longitude), values(latitude), pch = ".", col =
> topo.colors(56)[scl(values(rrs)) * 55 + 1])
>
> ## ggplot
> library(ggplot2)
> d <- data.frame(x = values(longitude), y = values(latitude), rrs =
> values(rrs))
> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = ".")
>
> ## might as well discard the missing values (depends on the other vars in
> the file)
> d <- d[!is.na(d$rrs), ]
> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = 19, cex =
> 0.1)
>
> There are some MODIS and GDAL based packages that might be of use, but I
> haven't yet seen any R tool that does this remapping task at scale. (I
> believe the MODIS tools and the best warping tools in GDAL use thin-plate
> spline techniques).
>
>  Some applications would use the observations as points (i.e. the ocean
> colour L3 bins as a daily aggregate from L2) and others use a direct
> remapping of the data as an array, for say high-resolution sea ice imagery.
>
> You might not need to do anything particularly complicated though, what's
> the goal?
>
> Cheers, Mike.
>
> On Wed, Apr 12, 2017, 20:06 Warner, David <dmwarner at usgs.gov> wrote:
>
> Greetings all
>
> I am trying to develop R code for processing L2 data (netcdf v4 files) from
> the Ocean Biology Processing Group.
>
> The data file I am working with to develop the code is at
> https://github.com/dmwarn/Tethys/blob/master/A2016244185500.L2_LAC_OC.x.nc
> and represents primarily Lake Michigan in the United States.  The data were
> extracted from a global dataset by the oceancolor L1 and L2 data server,
> not by me.
>
> I have been using the code below to try to get the data into R and ready
> for processing but am having problems with dimensions and/or
> orthorectification.  The
>
> #extent of the scene obtained using nc_open and ncvar_get
> nc <- nc_open('A2016214184500.L2_LAC_OC.x.nc')
> lon <- ncvar_get(nc, "navigation_data/longitude")
> lat <- ncvar_get(nc, "navigation_data/latitude")
> minx <- min(lon)
> maxx <- max(lon)
> miny <- min(lat)
> maxy <- max(lat)
>
> #create extent object
> myext <- extent(-90.817, -81.92438, 40.46493, 47.14244)
>
> #create raster
> rrs.412 <- raster('A2016214184500.L2_LAC_OC.x.nc', var
> ="geophysical_data/Rrs_412" ,
>                   ext=myext)
> rrs.412
> > rrs.412
> class       : RasterLayer
> dimensions  : 644, 528, 340032  (nrow, ncol, ncell)
> resolution  : 1, 1  (x, y)
> extent      : 0.5, 528.5, 0.5, 644.5  (xmin, xmax, ymin, ymax)
> coord. ref. : NA
> data source : /Users/dmwarner/Documents/MODIS/OC/
> A2016214184500.L2_LAC_OC.x.nc
> names       : Remote.sensing.reflectance.at.412.nm
> zvar        : geophysical_data/Rrs_412
>
> In spite of having tried to assign an extent, the raster extent is in rows
> and columns.
>
> Further, plotting the raster reveals that it is flipped on x axis and
> somewhat rotated relative to what it should look like.  Even when flipped,
> it is still not orthorectified.
>
> How do I get the raster to have the correct extent and also get it
> orthorectified?
> Thanks,
> Dave Warner
>
> --
> David Warner
> Research Fisheries Biologist
> U.S.G.S. Great Lakes Science Center
> 1451 Green Road
> Ann Arbor, MI 48105
> 734-214-9392 <(734)%20214-9392>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>


-- 
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Wed Apr 12 14:57:42 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 12 Apr 2017 12:57:42 +0000
Subject: [R-sig-Geo] raster and oceancolor L2 netcdf data
In-Reply-To: <CA+Y23taQCGwVD4_QYFNP6+tODFw5FS44zn_Qw3W_nF7GUVxBgQ@mail.gmail.com>
References: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>
 <CAAcGz98xcapeUajSHzsi=C2ZXbthYSZ=_=MDQMPZumwzKXD60w@mail.gmail.com>
 <CA+Y23taQCGwVD4_QYFNP6+tODFw5FS44zn_Qw3W_nF7GUVxBgQ@mail.gmail.com>
Message-ID: <CAAcGz98Camhhuzmra5uhn-oBX4-_o8Y=Mk+wuJmvqMK0ntsQng@mail.gmail.com>

Glad it's some help, but it sounds like you intend to calculate after
mapping (?) which is definitely not the right way to go. Calculate
chlorophyll and then map, that's how Seadas does it, even though the
remapping is the hard part. And apologies if I misread,  just checking.

I have two algorithms in my roc package on GitHub in case they help
understanding how the calcs get done. Presumably you'll have locally
calibrated parameters for a local algo?

If you want to aggregate into a local map I think it's fair to group-by
directly on L2 pixels coords and then sum into a geographic map, without
worrying about swath-as-image at all. We've road tested doing this but want
the entire southern ocean eventually so it needs a bit of unrelated
preparation for the raw files.

I'd be happy to explore an R solution off list if you're interested. L2 is
surprisingly easy and efficient in R via GDAL.

(This is also a good example for future workflows for the planned stars
package imo.)

Cheers, Mike

On Wed, Apr 12, 2017, 22:35 Warner, David <dmwarner at usgs.gov> wrote:

> Thanks Mike!
>
> The goal is to estimate daily chlorophyll via band ratio polynomial
> equation for hundreds of days of data (hundreds of data files).  Sounds
> like rather than finding a way to orthorectify in R I should learn to batch
> reproject using SeaDAS, which does produce a product that is in geotiff
> format, is orthorectified, and has readily mappable.  I was trying to avoid
> that as the help and documentation available for doing that seems much less
> abundant.  One file at a time is easy using the SeaDAS gui.
>
> Thanks very, very much for the other tricks.  Not surprisingly, ggplot2
> comes through again with plots that look right!
> Cheers,
> Dave
>
>
>
> On Wed, Apr 12, 2017 at 7:01 AM, Michael Sumner <mdsumner at gmail.com>
> wrote:
>
> You can't georeference these data without remapping the data, essentially
> treating the pixels as points. They have no natural regular grid form,
> except possibly a unique satellite-perspective one. The data are in 2D
> array form, but they have explicit "geolocation arrays", i.e. a longitude
> and latitude for every cell and not based on a regular mapping.
>
> R does not have tools for this directly from these data, though it can be
> treated as a resampling or modelling problem.
> You can use raster to get at the values of the locations easily enough,
> here's a couple of plotting options in case it's useful:
>
> u <- "
> https://github.com/dmwarn/Tethys/blob/master/A2016244185500.L2_LAC_OC.x.nc?raw=true
> "
> f <- basename(f)
> download.file(u, f, mode = "wb")
>
> library(raster)
> ## use raster to treat as raw point data, on long-lat locations
> rrs <- raster(f, varname = "geophysical_data/Rrs_412")
> longitude <- raster(f, varname = "navigation_data/longitude")
> latitude <- raster(f, varname = "navigation_data/latitude")
>
> ## plot in grid space, and add the geolocation space as a graticule
> plot(rrs)
> contour(longitude, add = TRUE)
> contour(latitude, add = TRUE)
>
> ## raw scaling against rrs values
> scl <- function(x) (x - min(x, na.rm = TRUE))/diff(range(x, na.rm = TRUE))
> plot(values(longitude), values(latitude), pch = ".", col =
> topo.colors(56)[scl(values(rrs)) * 55 + 1])
>
> ## ggplot
> library(ggplot2)
> d <- data.frame(x = values(longitude), y = values(latitude), rrs =
> values(rrs))
> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = ".")
>
> ## might as well discard the missing values (depends on the other vars in
> the file)
> d <- d[!is.na(d$rrs), ]
> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = 19, cex =
> 0.1)
>
> There are some MODIS and GDAL based packages that might be of use, but I
> haven't yet seen any R tool that does this remapping task at scale. (I
> believe the MODIS tools and the best warping tools in GDAL use thin-plate
> spline techniques).
>
>  Some applications would use the observations as points (i.e. the ocean
> colour L3 bins as a daily aggregate from L2) and others use a direct
> remapping of the data as an array, for say high-resolution sea ice imagery.
>
> You might not need to do anything particularly complicated though, what's
> the goal?
>
> Cheers, Mike.
>
> On Wed, Apr 12, 2017, 20:06 Warner, David <dmwarner at usgs.gov> wrote:
>
> Greetings all
>
> I am trying to develop R code for processing L2 data (netcdf v4 files) from
> the Ocean Biology Processing Group.
>
> The data file I am working with to develop the code is at
> https://github.com/dmwarn/Tethys/blob/master/A2016244185500.L2_LAC_OC.x.nc
> and represents primarily Lake Michigan in the United States.  The data were
> extracted from a global dataset by the oceancolor L1 and L2 data server,
> not by me.
>
> I have been using the code below to try to get the data into R and ready
> for processing but am having problems with dimensions and/or
> orthorectification.  The
>
> #extent of the scene obtained using nc_open and ncvar_get
> nc <- nc_open('A2016214184500.L2_LAC_OC.x.nc')
> lon <- ncvar_get(nc, "navigation_data/longitude")
> lat <- ncvar_get(nc, "navigation_data/latitude")
> minx <- min(lon)
> maxx <- max(lon)
> miny <- min(lat)
> maxy <- max(lat)
>
> #create extent object
> myext <- extent(-90.817, -81.92438, 40.46493, 47.14244)
>
> #create raster
> rrs.412 <- raster('A2016214184500.L2_LAC_OC.x.nc', var
> ="geophysical_data/Rrs_412" ,
>                   ext=myext)
> rrs.412
> > rrs.412
> class       : RasterLayer
> dimensions  : 644, 528, 340032  (nrow, ncol, ncell)
> resolution  : 1, 1  (x, y)
> extent      : 0.5, 528.5, 0.5, 644.5  (xmin, xmax, ymin, ymax)
> coord. ref. : NA
> data source : /Users/dmwarner/Documents/MODIS/OC/
> A2016214184500.L2_LAC_OC.x.nc
> names       : Remote.sensing.reflectance.at.412.nm
> zvar        : geophysical_data/Rrs_412
>
> In spite of having tried to assign an extent, the raster extent is in rows
> and columns.
>
> Further, plotting the raster reveals that it is flipped on x axis and
> somewhat rotated relative to what it should look like.  Even when flipped,
> it is still not orthorectified.
>
> How do I get the raster to have the correct extent and also get it
> orthorectified?
> Thanks,
> Dave Warner
>
> --
> David Warner
> Research Fisheries Biologist
> U.S.G.S. Great Lakes Science Center
> 1451 Green Road
> Ann Arbor, MI 48105
> 734-214-9392 <(734)%20214-9392>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>
>
>
> --
> David Warner
> Research Fisheries Biologist
> U.S.G.S. Great Lakes Science Center
> 1451 Green Road
> Ann Arbor, MI 48105
> 734-214-9392
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From mael.lenoc at laposte.net  Wed Apr 12 15:03:33 2017
From: mael.lenoc at laposte.net (=?UTF-8?Q?Ma=c3=abl_Le_Noc?=)
Date: Wed, 12 Apr 2017 08:03:33 -0500
Subject: [R-sig-Geo] dnearneigh() from spdep: Points with the exact same
 location are not considered neighbours.
In-Reply-To: <alpine.LFD.2.20.1704120919520.29243@reclus.nhh.no>
References: <bf737ebe-cf93-ada7-2ab2-ed52f48b38b5@laposte.net>
 <alpine.LFD.2.20.1704120919520.29243@reclus.nhh.no>
Message-ID: <e7ec3908-ccdd-1248-a2a0-87b41f43c464@laposte.net>

Dear Roger,
Thank you for your answer, (And sorry for the HTML posting).

The issue persists if I specify "GE" for the lower bound, but only when
the parameter latlong is set to TRUE (see example below).


Regarding the nature of my data, it is a series of record of Jews
arrested during the Holocaust in Italy. Those are point data, and some
people have been arrested at the same place and at the same time (hence
my problem). I am trying to assess spatial autocorrelation for a binary
attribute (whether they survived the Holocaust or not), and I plan to
use a Join-count method, for which I need a spatial weight matrix. Is
using Join-count on such a dataset wrong ?

Best



Code:

library(data.table)
library(spdep)
pointstable <- data.table(XCoord=c(13.667029,13.667029,13.667028),
YCoord=c(42.772396,42.772396,42.772396))
print(pointstable)
coords <-cbind(pointstable$XCoord, pointstable$YCoord)
nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = TRUE, bound =
c("GE", "LE"))
summary(nbLocal)
nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = FALSE, bound =
c("GE", "LE"))
summary(nbLocal)


Output:
> print(pointstable)
     XCoord  YCoord
1: 13.66703 42.7724
2: 13.66703 42.7724
3: 13.66703 42.7724

> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = TRUE, bound =
c("GE", "LE"))
> summary(nbLocal)
Neighbour list object:
Number of regions: 3
Number of nonzero links: 4
Percentage nonzero weights: 44.44444
Average number of links: 1.333333
Link number distribution:

1 2
2 1
2 least connected regions:
1 2 with 1 link
1 most connected region:
3 with 2 links

> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = FALSE, bound =
c("GE", "LE"))
> summary(nbLocal)
Neighbour list object:
Number of regions: 3
Number of nonzero links: 6
Percentage nonzero weights: 66.66667
Average number of links: 2
Link number distribution:

2
3
3 least connected regions:
1 2 3 with 2 links
3 most connected regions:
1 2 3 with 2 links



On 12/04/2017 02:27, Roger Bivand wrote:
> Do not post HTML-mail, only plain text. Your example is not reproducible
> because you used HTML-mail.
> 
> Please read the help file, the bounds are described as being between
> lower (greater than) and upper (less than or equal to) bounds. Since the
> distance between identical points is strictly zero, they are not
> neighbours because the distance must be > d1 and <= d2. If d1 is < 0, it
> is reset to 0, as it is assumed that a negative lower bound is a user
> error (and it would break the underlying compiled code).
> 
> In any case, no reasonable cross-sectional spatial process has
> duplicated point (nugget) observations in situations in which spatial
> weights would be used (spatio-temporal panels will have, but then time
> differs).
> 
> Hope this clarifies,
> 
> Roger
> 
> On Wed, 12 Apr 2017, Ma?l Le Noc via R-sig-Geo wrote:
> 
>> Dear List
>>
>> As I was working on a project, I realized that when I use dnearneigh
>> from spdep, two (or more) points that have the exact same coordinates
>> are not considered neighbours and thus are not linked (even when the
>> lower bound is put to 0 or even to -1). See below for an example.
>> (However this does not happen if the parameter longlat is set to false)
>>
>> Does the function behave the same way for you? Am I missing something?
>> Is this an expected behavior? And if so, if there a way to change that ?
>>
>> In the example below, points 1 and 2 are not connected to each other/are
>> not neighbours (as you can see since the both have only one link, to 3),
>> even though they have the exact same coordinates (and are thus less than
>> 25km apart), while point 3 is connected to both point 1 and 2.
>> If I want to assess autocorrelation using, for instance joincount.test,
>> this is then an issue...
>>
>>> /library(data.table) />/library(spdep) />/pointstable <-
>>> data.table(XCoord=c(13.667029,13.667029,13.667028),
>>> /YCoord=c(42.772396,42.772396,42.772396))
>>> /print(pointstable) /     XCoord  YCoord
>> 1: 13.667029 42.772396
>> 2: 13.667029 42.772396
>> 3: 13.667028 42.772396
>>> /coords <-cbind(pointstable$XCoord, pointstable$YCoord) />/nbLocal<-
>>> dnearneigh(coords, d1=0, d2=25, longlat = TRUE) />/nbLocal<-
>>> dnearneigh(coords, d1=-1, d2=25, longlat = TRUE) #both lines /produce
>>> the same output
>>> /summary(nbLocal) /Neighbour list object:
>> Number of regions: 3
>> Number of nonzero links: 4
>> Percentage nonzero weights: 44.44444
>> Average number of links: 1.333333
>> Link number distribution:
>>
>> 1 2
>> 2 1
>> 2 least connected regions:
>> 1 2 with 1 link
>> 1 most connected region:
>> 3 with 2 links
>>> //
>> Thanks
>> Ma?l
>>
>>
>>     [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From Roger.Bivand at nhh.no  Wed Apr 12 16:00:34 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 12 Apr 2017 16:00:34 +0200
Subject: [R-sig-Geo] dnearneigh() from spdep: Points with the exact same
 location are not considered neighbours.
In-Reply-To: <e7ec3908-ccdd-1248-a2a0-87b41f43c464@laposte.net>
References: <bf737ebe-cf93-ada7-2ab2-ed52f48b38b5@laposte.net>
 <alpine.LFD.2.20.1704120919520.29243@reclus.nhh.no>
 <e7ec3908-ccdd-1248-a2a0-87b41f43c464@laposte.net>
Message-ID: <alpine.LFD.2.20.1704121540280.11251@reclus.nhh.no>

On Wed, 12 Apr 2017, Ma?l Le Noc wrote:

> Dear Roger,
> Thank you for your answer, (And sorry for the HTML posting).
>
> The issue persists if I specify "GE" for the lower bound, but only when
> the parameter latlong is set to TRUE (see example below).

Thanks, very useful. The Great Circle distance measure returned NotANumber 
for zero distance, because of an unprotected division by zero. I've 
committed a patched source version to R-Forge. Look on

https://r-forge.r-project.org/R/?group_id=182

later today for a version with today's date and Rev: 693 - should show up 
mid to late evening CEST. Please say whether this performs as expected.

>
>
> Regarding the nature of my data, it is a series of record of Jews
> arrested during the Holocaust in Italy. Those are point data, and some
> people have been arrested at the same place and at the same time (hence
> my problem). I am trying to assess spatial autocorrelation for a binary
> attribute (whether they survived the Holocaust or not), and I plan to
> use a Join-count method, for which I need a spatial weight matrix. Is
> using Join-count on such a dataset wrong ?

Join-count should be OK, but if you have covariates you could try to 
remove the mean model first and only then see whether there is a spatially 
structured random effect, for example with hglm, R2BayesX, INLA, or 
similar. For hglm see for example:

https://journal.r-project.org/archive/2015/RJ-2015-017/index.html

The data you most likely do not have (addresses with residents at 
risk of arrest but not arrested) would also help, giving you a risk of 
arrest measure by address. There is also a spatial probit literature that 
might be relevant; if you have timestamps, you will likely find that 
operational factors play in, with arrests in a small area at the same 
time.

Hope this helps,

Roger

>
> Best
>
>
>
> Code:
>
> library(data.table)
> library(spdep)
> pointstable <- data.table(XCoord=c(13.667029,13.667029,13.667028),
> YCoord=c(42.772396,42.772396,42.772396))
> print(pointstable)
> coords <-cbind(pointstable$XCoord, pointstable$YCoord)
> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = TRUE, bound =
> c("GE", "LE"))
> summary(nbLocal)
> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = FALSE, bound =
> c("GE", "LE"))
> summary(nbLocal)
>
>
> Output:
>> print(pointstable)
>     XCoord  YCoord
> 1: 13.66703 42.7724
> 2: 13.66703 42.7724
> 3: 13.66703 42.7724
>
>> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = TRUE, bound =
> c("GE", "LE"))
>> summary(nbLocal)
> Neighbour list object:
> Number of regions: 3
> Number of nonzero links: 4
> Percentage nonzero weights: 44.44444
> Average number of links: 1.333333
> Link number distribution:
>
> 1 2
> 2 1
> 2 least connected regions:
> 1 2 with 1 link
> 1 most connected region:
> 3 with 2 links
>
>> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = FALSE, bound =
> c("GE", "LE"))
>> summary(nbLocal)
> Neighbour list object:
> Number of regions: 3
> Number of nonzero links: 6
> Percentage nonzero weights: 66.66667
> Average number of links: 2
> Link number distribution:
>
> 2
> 3
> 3 least connected regions:
> 1 2 3 with 2 links
> 3 most connected regions:
> 1 2 3 with 2 links
>
>
>
> On 12/04/2017 02:27, Roger Bivand wrote:
>> Do not post HTML-mail, only plain text. Your example is not reproducible
>> because you used HTML-mail.
>>
>> Please read the help file, the bounds are described as being between
>> lower (greater than) and upper (less than or equal to) bounds. Since the
>> distance between identical points is strictly zero, they are not
>> neighbours because the distance must be > d1 and <= d2. If d1 is < 0, it
>> is reset to 0, as it is assumed that a negative lower bound is a user
>> error (and it would break the underlying compiled code).
>>
>> In any case, no reasonable cross-sectional spatial process has
>> duplicated point (nugget) observations in situations in which spatial
>> weights would be used (spatio-temporal panels will have, but then time
>> differs).
>>
>> Hope this clarifies,
>>
>> Roger
>>
>> On Wed, 12 Apr 2017, Ma?l Le Noc via R-sig-Geo wrote:
>>
>>> Dear List
>>>
>>> As I was working on a project, I realized that when I use dnearneigh
>>> from spdep, two (or more) points that have the exact same coordinates
>>> are not considered neighbours and thus are not linked (even when the
>>> lower bound is put to 0 or even to -1). See below for an example.
>>> (However this does not happen if the parameter longlat is set to false)
>>>
>>> Does the function behave the same way for you? Am I missing something?
>>> Is this an expected behavior? And if so, if there a way to change that ?
>>>
>>> In the example below, points 1 and 2 are not connected to each other/are
>>> not neighbours (as you can see since the both have only one link, to 3),
>>> even though they have the exact same coordinates (and are thus less than
>>> 25km apart), while point 3 is connected to both point 1 and 2.
>>> If I want to assess autocorrelation using, for instance joincount.test,
>>> this is then an issue...
>>>
>>>> /library(data.table) />/library(spdep) />/pointstable <-
>>>> data.table(XCoord=c(13.667029,13.667029,13.667028),
>>>> /YCoord=c(42.772396,42.772396,42.772396))
>>>> /print(pointstable) /     XCoord  YCoord
>>> 1: 13.667029 42.772396
>>> 2: 13.667029 42.772396
>>> 3: 13.667028 42.772396
>>>> /coords <-cbind(pointstable$XCoord, pointstable$YCoord) />/nbLocal<-
>>>> dnearneigh(coords, d1=0, d2=25, longlat = TRUE) />/nbLocal<-
>>>> dnearneigh(coords, d1=-1, d2=25, longlat = TRUE) #both lines /produce
>>>> the same output
>>>> /summary(nbLocal) /Neighbour list object:
>>> Number of regions: 3
>>> Number of nonzero links: 4
>>> Percentage nonzero weights: 44.44444
>>> Average number of links: 1.333333
>>> Link number distribution:
>>>
>>> 1 2
>>> 2 1
>>> 2 least connected regions:
>>> 1 2 with 1 link
>>> 1 most connected region:
>>> 3 with 2 links
>>>> //
>>> Thanks
>>> Ma?l
>>>
>>>
>>>     [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From dmwarner at usgs.gov  Wed Apr 12 19:05:37 2017
From: dmwarner at usgs.gov (Warner, David)
Date: Wed, 12 Apr 2017 13:05:37 -0400
Subject: [R-sig-Geo] raster and oceancolor L2 netcdf data
In-Reply-To: <CAAcGz98Camhhuzmra5uhn-oBX4-_o8Y=Mk+wuJmvqMK0ntsQng@mail.gmail.com>
References: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>
 <CAAcGz98xcapeUajSHzsi=C2ZXbthYSZ=_=MDQMPZumwzKXD60w@mail.gmail.com>
 <CA+Y23taQCGwVD4_QYFNP6+tODFw5FS44zn_Qw3W_nF7GUVxBgQ@mail.gmail.com>
 <CAAcGz98Camhhuzmra5uhn-oBX4-_o8Y=Mk+wuJmvqMK0ntsQng@mail.gmail.com>
Message-ID: <CA+Y23tYQNUO_ggX5-FDdGJX7NBJZc4U-=pDJY6Dn+_cscuaFUg@mail.gmail.com>

Mike
I had not really thought about order of operations to be honest.  I just
noticed early on when I was attempting to use raster approach that the data
were not mapped as hoped or orthorectified.  I certainly don't need to
remap before calculating chlor-a on a daily basis as all the bands I need
for a single day are aligned (if not mapped the way I wish).  In the end I
do need the data correctly mapped as I want to do matchups with data
collected with an LRAUV.

I am planning on using locally calibrated coefficients.  I will check out
your package!  I initially wanted to use L3 data but I and a colleague
determined that there was for some reason poor agreement between the L3
data and our in situ matchup data even though at L2 there is good
agreement.  This colleague has typically done the heavy lifting using ENVI,
which I don't have and would rather not learn if what I want to do can be
done in R.

It looks like I can create a raster with vect2rast.SpatialPoints() from the
plotKML package quite easily but the default settings for cell.size lead to
loss of data (I think).  You can set a cell.size but I am not sure if it
works correctly without having multiple values per cell or not.  Or what it
does if you have multiple values per cell.  There is some functionality
that allows you to pick the first, last, the min, the max, or the mean if
there are multiple  values for the same grid cell but I can't get that to
work without Saga GIS.

Cheers and thanks,
Dave

On Wed, Apr 12, 2017 at 8:57 AM, Michael Sumner <mdsumner at gmail.com> wrote:

> Glad it's some help, but it sounds like you intend to calculate after
> mapping (?) which is definitely not the right way to go. Calculate
> chlorophyll and then map, that's how Seadas does it, even though the
> remapping is the hard part. And apologies if I misread,  just checking.
>
> I have two algorithms in my roc package on GitHub in case they help
> understanding how the calcs get done. Presumably you'll have locally
> calibrated parameters for a local algo?
>
> If you want to aggregate into a local map I think it's fair to group-by
> directly on L2 pixels coords and then sum into a geographic map, without
> worrying about swath-as-image at all. We've road tested doing this but want
> the entire southern ocean eventually so it needs a bit of unrelated
> preparation for the raw files.
>
> I'd be happy to explore an R solution off list if you're interested. L2 is
> surprisingly easy and efficient in R via GDAL.
>
> (This is also a good example for future workflows for the planned stars
> package imo.)
>
> Cheers, Mike
>
> On Wed, Apr 12, 2017, 22:35 Warner, David <dmwarner at usgs.gov> wrote:
>
>> Thanks Mike!
>>
>> The goal is to estimate daily chlorophyll via band ratio polynomial
>> equation for hundreds of days of data (hundreds of data files).  Sounds
>> like rather than finding a way to orthorectify in R I should learn to batch
>> reproject using SeaDAS, which does produce a product that is in geotiff
>> format, is orthorectified, and has readily mappable.  I was trying to avoid
>> that as the help and documentation available for doing that seems much less
>> abundant.  One file at a time is easy using the SeaDAS gui.
>>
>> Thanks very, very much for the other tricks.  Not surprisingly, ggplot2
>> comes through again with plots that look right!
>> Cheers,
>> Dave
>>
>>
>>
>> On Wed, Apr 12, 2017 at 7:01 AM, Michael Sumner <mdsumner at gmail.com>
>> wrote:
>>
>> You can't georeference these data without remapping the data, essentially
>> treating the pixels as points. They have no natural regular grid form,
>> except possibly a unique satellite-perspective one. The data are in 2D
>> array form, but they have explicit "geolocation arrays", i.e. a longitude
>> and latitude for every cell and not based on a regular mapping.
>>
>> R does not have tools for this directly from these data, though it can be
>> treated as a resampling or modelling problem.
>> You can use raster to get at the values of the locations easily enough,
>> here's a couple of plotting options in case it's useful:
>>
>> u <- "https://github.com/dmwarn/Tethys/blob/master/
>> A2016244185500.L2_LAC_OC.x.nc?raw=true"
>> f <- basename(f)
>> download.file(u, f, mode = "wb")
>>
>> library(raster)
>> ## use raster to treat as raw point data, on long-lat locations
>> rrs <- raster(f, varname = "geophysical_data/Rrs_412")
>> longitude <- raster(f, varname = "navigation_data/longitude")
>> latitude <- raster(f, varname = "navigation_data/latitude")
>>
>> ## plot in grid space, and add the geolocation space as a graticule
>> plot(rrs)
>> contour(longitude, add = TRUE)
>> contour(latitude, add = TRUE)
>>
>> ## raw scaling against rrs values
>> scl <- function(x) (x - min(x, na.rm = TRUE))/diff(range(x, na.rm = TRUE))
>> plot(values(longitude), values(latitude), pch = ".", col =
>> topo.colors(56)[scl(values(rrs)) * 55 + 1])
>>
>> ## ggplot
>> library(ggplot2)
>> d <- data.frame(x = values(longitude), y = values(latitude), rrs =
>> values(rrs))
>> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = ".")
>>
>> ## might as well discard the missing values (depends on the other vars in
>> the file)
>> d <- d[!is.na(d$rrs), ]
>> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = 19, cex =
>> 0.1)
>>
>> There are some MODIS and GDAL based packages that might be of use, but I
>> haven't yet seen any R tool that does this remapping task at scale. (I
>> believe the MODIS tools and the best warping tools in GDAL use thin-plate
>> spline techniques).
>>
>>  Some applications would use the observations as points (i.e. the ocean
>> colour L3 bins as a daily aggregate from L2) and others use a direct
>> remapping of the data as an array, for say high-resolution sea ice imagery.
>>
>> You might not need to do anything particularly complicated though, what's
>> the goal?
>>
>> Cheers, Mike.
>>
>> On Wed, Apr 12, 2017, 20:06 Warner, David <dmwarner at usgs.gov> wrote:
>>
>> Greetings all
>>
>> I am trying to develop R code for processing L2 data (netcdf v4 files)
>> from
>> the Ocean Biology Processing Group.
>>
>> The data file I am working with to develop the code is at
>> https://github.com/dmwarn/Tethys/blob/master/
>> A2016244185500.L2_LAC_OC.x.nc
>> and represents primarily Lake Michigan in the United States.  The data
>> were
>> extracted from a global dataset by the oceancolor L1 and L2 data server,
>> not by me.
>>
>> I have been using the code below to try to get the data into R and ready
>> for processing but am having problems with dimensions and/or
>> orthorectification.  The
>>
>> #extent of the scene obtained using nc_open and ncvar_get
>> nc <- nc_open('A2016214184500.L2_LAC_OC.x.nc')
>> lon <- ncvar_get(nc, "navigation_data/longitude")
>> lat <- ncvar_get(nc, "navigation_data/latitude")
>> minx <- min(lon)
>> maxx <- max(lon)
>> miny <- min(lat)
>> maxy <- max(lat)
>>
>> #create extent object
>> myext <- extent(-90.817, -81.92438, 40.46493, 47.14244)
>>
>> #create raster
>> rrs.412 <- raster('A2016214184500.L2_LAC_OC.x.nc', var
>> ="geophysical_data/Rrs_412" ,
>>                   ext=myext)
>> rrs.412
>> > rrs.412
>> class       : RasterLayer
>> dimensions  : 644, 528, 340032  (nrow, ncol, ncell)
>> resolution  : 1, 1  (x, y)
>> extent      : 0.5, 528.5, 0.5, 644.5  (xmin, xmax, ymin, ymax)
>> coord. ref. : NA
>> data source : /Users/dmwarner/Documents/MODIS/OC/
>> A2016214184500.L2_LAC_OC.x.nc
>> names       : Remote.sensing.reflectance.at.412.nm
>> zvar        : geophysical_data/Rrs_412
>>
>> In spite of having tried to assign an extent, the raster extent is in rows
>> and columns.
>>
>> Further, plotting the raster reveals that it is flipped on x axis and
>> somewhat rotated relative to what it should look like.  Even when flipped,
>> it is still not orthorectified.
>>
>> How do I get the raster to have the correct extent and also get it
>> orthorectified?
>> Thanks,
>> Dave Warner
>>
>> --
>> David Warner
>> Research Fisheries Biologist
>> U.S.G.S. Great Lakes Science Center
>> 1451 Green Road
>> Ann Arbor, MI 48105
>> 734-214-9392 <(734)%20214-9392>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> --
>> Dr. Michael Sumner
>> Software and Database Engineer
>> Australian Antarctic Division
>> 203 Channel Highway
>> Kingston Tasmania 7050 Australia
>>
>>
>>
>>
>> --
>> David Warner
>> Research Fisheries Biologist
>> U.S.G.S. Great Lakes Science Center
>> 1451 Green Road
>> Ann Arbor, MI 48105
>> 734-214-9392
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>


-- 
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392

	[[alternative HTML version deleted]]


From mael.lenoc at laposte.net  Wed Apr 12 21:15:59 2017
From: mael.lenoc at laposte.net (=?UTF-8?Q?Ma=c3=abl_Le_Noc?=)
Date: Wed, 12 Apr 2017 14:15:59 -0500
Subject: [R-sig-Geo] dnearneigh() from spdep: Points with the exact same
 location are not considered neighbours.
In-Reply-To: <alpine.LFD.2.20.1704121540280.11251@reclus.nhh.no>
References: <bf737ebe-cf93-ada7-2ab2-ed52f48b38b5@laposte.net>
 <alpine.LFD.2.20.1704120919520.29243@reclus.nhh.no>
 <e7ec3908-ccdd-1248-a2a0-87b41f43c464@laposte.net>
 <alpine.LFD.2.20.1704121540280.11251@reclus.nhh.no>
Message-ID: <9cde9f0d-73aa-64ed-dd6f-fab1cfa434bd@laposte.net>

Thank you Roger,
Your patched version now works as expected!

And thank you for your suggestions, I am currently looking into all of that.
Best
Ma?l



On 12/04/2017 09:00, Roger Bivand wrote:
> On Wed, 12 Apr 2017, Ma?l Le Noc wrote:
>
>> Dear Roger,
>> Thank you for your answer, (And sorry for the HTML posting).
>>
>> The issue persists if I specify "GE" for the lower bound, but only when
>> the parameter latlong is set to TRUE (see example below).
>
> Thanks, very useful. The Great Circle distance measure returned
> NotANumber for zero distance, because of an unprotected division by
> zero. I've committed a patched source version to R-Forge. Look on
>
> https://r-forge.r-project.org/R/?group_id=182
>
> later today for a version with today's date and Rev: 693 - should show
> up mid to late evening CEST. Please say whether this performs as
> expected.
>
>>
>>
>> Regarding the nature of my data, it is a series of record of Jews
>> arrested during the Holocaust in Italy. Those are point data, and some
>> people have been arrested at the same place and at the same time (hence
>> my problem). I am trying to assess spatial autocorrelation for a binary
>> attribute (whether they survived the Holocaust or not), and I plan to
>> use a Join-count method, for which I need a spatial weight matrix. Is
>> using Join-count on such a dataset wrong ?
>
> Join-count should be OK, but if you have covariates you could try to
> remove the mean model first and only then see whether there is a
> spatially structured random effect, for example with hglm, R2BayesX,
> INLA, or similar. For hglm see for example:
>
> https://journal.r-project.org/archive/2015/RJ-2015-017/index.html
>
> The data you most likely do not have (addresses with residents at risk
> of arrest but not arrested) would also help, giving you a risk of
> arrest measure by address. There is also a spatial probit literature
> that might be relevant; if you have timestamps, you will likely find
> that operational factors play in, with arrests in a small area at the
> same time.
>
> Hope this helps,
>
> Roger
>
>>
>> Best
>>
>>
>>
>> Code:
>>
>> library(data.table)
>> library(spdep)
>> pointstable <- data.table(XCoord=c(13.667029,13.667029,13.667028),
>> YCoord=c(42.772396,42.772396,42.772396))
>> print(pointstable)
>> coords <-cbind(pointstable$XCoord, pointstable$YCoord)
>> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = TRUE, bound =
>> c("GE", "LE"))
>> summary(nbLocal)
>> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = FALSE, bound =
>> c("GE", "LE"))
>> summary(nbLocal)
>>
>>
>> Output:
>>> print(pointstable)
>>     XCoord  YCoord
>> 1: 13.66703 42.7724
>> 2: 13.66703 42.7724
>> 3: 13.66703 42.7724
>>
>>> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = TRUE, bound =
>> c("GE", "LE"))
>>> summary(nbLocal)
>> Neighbour list object:
>> Number of regions: 3
>> Number of nonzero links: 4
>> Percentage nonzero weights: 44.44444
>> Average number of links: 1.333333
>> Link number distribution:
>>
>> 1 2
>> 2 1
>> 2 least connected regions:
>> 1 2 with 1 link
>> 1 most connected region:
>> 3 with 2 links
>>
>>> nbLocal<- dnearneigh(coords, d1=0, d2=25, longlat = FALSE, bound =
>> c("GE", "LE"))
>>> summary(nbLocal)
>> Neighbour list object:
>> Number of regions: 3
>> Number of nonzero links: 6
>> Percentage nonzero weights: 66.66667
>> Average number of links: 2
>> Link number distribution:
>>
>> 2
>> 3
>> 3 least connected regions:
>> 1 2 3 with 2 links
>> 3 most connected regions:
>> 1 2 3 with 2 links
>>
>>
>>
>> On 12/04/2017 02:27, Roger Bivand wrote:
>>> Do not post HTML-mail, only plain text. Your example is not
>>> reproducible
>>> because you used HTML-mail.
>>>
>>> Please read the help file, the bounds are described as being between
>>> lower (greater than) and upper (less than or equal to) bounds. Since
>>> the
>>> distance between identical points is strictly zero, they are not
>>> neighbours because the distance must be > d1 and <= d2. If d1 is <
>>> 0, it
>>> is reset to 0, as it is assumed that a negative lower bound is a user
>>> error (and it would break the underlying compiled code).
>>>
>>> In any case, no reasonable cross-sectional spatial process has
>>> duplicated point (nugget) observations in situations in which spatial
>>> weights would be used (spatio-temporal panels will have, but then time
>>> differs).
>>>
>>> Hope this clarifies,
>>>
>>> Roger
>>>
>>> On Wed, 12 Apr 2017, Ma?l Le Noc via R-sig-Geo wrote:
>>>
>>>> Dear List
>>>>
>>>> As I was working on a project, I realized that when I use dnearneigh
>>>> from spdep, two (or more) points that have the exact same coordinates
>>>> are not considered neighbours and thus are not linked (even when the
>>>> lower bound is put to 0 or even to -1). See below for an example.
>>>> (However this does not happen if the parameter longlat is set to
>>>> false)
>>>>
>>>> Does the function behave the same way for you? Am I missing something?
>>>> Is this an expected behavior? And if so, if there a way to change
>>>> that ?
>>>>
>>>> In the example below, points 1 and 2 are not connected to each
>>>> other/are
>>>> not neighbours (as you can see since the both have only one link,
>>>> to 3),
>>>> even though they have the exact same coordinates (and are thus less
>>>> than
>>>> 25km apart), while point 3 is connected to both point 1 and 2.
>>>> If I want to assess autocorrelation using, for instance
>>>> joincount.test,
>>>> this is then an issue...
>>>>
>>>>> /library(data.table) />/library(spdep) />/pointstable <-
>>>>> data.table(XCoord=c(13.667029,13.667029,13.667028),
>>>>> /YCoord=c(42.772396,42.772396,42.772396))
>>>>> /print(pointstable) /     XCoord  YCoord
>>>> 1: 13.667029 42.772396
>>>> 2: 13.667029 42.772396
>>>> 3: 13.667028 42.772396
>>>>> /coords <-cbind(pointstable$XCoord, pointstable$YCoord) />/nbLocal<-
>>>>> dnearneigh(coords, d1=0, d2=25, longlat = TRUE) />/nbLocal<-
>>>>> dnearneigh(coords, d1=-1, d2=25, longlat = TRUE) #both lines /produce
>>>>> the same output
>>>>> /summary(nbLocal) /Neighbour list object:
>>>> Number of regions: 3
>>>> Number of nonzero links: 4
>>>> Percentage nonzero weights: 44.44444
>>>> Average number of links: 1.333333
>>>> Link number distribution:
>>>>
>>>> 1 2
>>>> 2 1
>>>> 2 least connected regions:
>>>> 1 2 with 1 link
>>>> 1 most connected region:
>>>> 3 with 2 links
>>>>> //
>>>> Thanks
>>>> Ma?l
>>>>
>>>>
>>>>     [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>>
>


From loic.dutrieux at conabio.gob.mx  Wed Apr 12 22:12:47 2017
From: loic.dutrieux at conabio.gob.mx (=?UTF-8?Q?Lo=c3=afc_Dutrieux?=)
Date: Wed, 12 Apr 2017 15:12:47 -0500
Subject: [R-sig-Geo] raster and oceancolor L2 netcdf data
In-Reply-To: <CA+Y23tYQNUO_ggX5-FDdGJX7NBJZc4U-=pDJY6Dn+_cscuaFUg@mail.gmail.com>
References: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>
 <CAAcGz98xcapeUajSHzsi=C2ZXbthYSZ=_=MDQMPZumwzKXD60w@mail.gmail.com>
 <CA+Y23taQCGwVD4_QYFNP6+tODFw5FS44zn_Qw3W_nF7GUVxBgQ@mail.gmail.com>
 <CAAcGz98Camhhuzmra5uhn-oBX4-_o8Y=Mk+wuJmvqMK0ntsQng@mail.gmail.com>
 <CA+Y23tYQNUO_ggX5-FDdGJX7NBJZc4U-=pDJY6Dn+_cscuaFUg@mail.gmail.com>
Message-ID: <786d12ea-c907-d680-7b95-bcac354cdc87@conabio.gob.mx>

I've done some work recently on ocean color L2 binning/mapping, see the 
discussion on the ocean color forum 
https://oceancolor.gsfc.nasa.gov/forum/oceancolor/topic_show.pl?tid=6550 
and the code in the gist (which I'll update). It's not a R solution, but 
could be useful still.

Cheers,
Lo?c

On 12/04/17 12:05, Warner, David wrote:
> Mike
> I had not really thought about order of operations to be honest.  I just
> noticed early on when I was attempting to use raster approach that the data
> were not mapped as hoped or orthorectified.  I certainly don't need to
> remap before calculating chlor-a on a daily basis as all the bands I need
> for a single day are aligned (if not mapped the way I wish).  In the end I
> do need the data correctly mapped as I want to do matchups with data
> collected with an LRAUV.
>
> I am planning on using locally calibrated coefficients.  I will check out
> your package!  I initially wanted to use L3 data but I and a colleague
> determined that there was for some reason poor agreement between the L3
> data and our in situ matchup data even though at L2 there is good
> agreement.  This colleague has typically done the heavy lifting using ENVI,
> which I don't have and would rather not learn if what I want to do can be
> done in R.
>
> It looks like I can create a raster with vect2rast.SpatialPoints() from the
> plotKML package quite easily but the default settings for cell.size lead to
> loss of data (I think).  You can set a cell.size but I am not sure if it
> works correctly without having multiple values per cell or not.  Or what it
> does if you have multiple values per cell.  There is some functionality
> that allows you to pick the first, last, the min, the max, or the mean if
> there are multiple  values for the same grid cell but I can't get that to
> work without Saga GIS.
>
> Cheers and thanks,
> Dave
>
> On Wed, Apr 12, 2017 at 8:57 AM, Michael Sumner <mdsumner at gmail.com> wrote:
>
>> Glad it's some help, but it sounds like you intend to calculate after
>> mapping (?) which is definitely not the right way to go. Calculate
>> chlorophyll and then map, that's how Seadas does it, even though the
>> remapping is the hard part. And apologies if I misread,  just checking.
>>
>> I have two algorithms in my roc package on GitHub in case they help
>> understanding how the calcs get done. Presumably you'll have locally
>> calibrated parameters for a local algo?
>>
>> If you want to aggregate into a local map I think it's fair to group-by
>> directly on L2 pixels coords and then sum into a geographic map, without
>> worrying about swath-as-image at all. We've road tested doing this but want
>> the entire southern ocean eventually so it needs a bit of unrelated
>> preparation for the raw files.
>>
>> I'd be happy to explore an R solution off list if you're interested. L2 is
>> surprisingly easy and efficient in R via GDAL.
>>
>> (This is also a good example for future workflows for the planned stars
>> package imo.)
>>
>> Cheers, Mike
>>
>> On Wed, Apr 12, 2017, 22:35 Warner, David <dmwarner at usgs.gov> wrote:
>>
>>> Thanks Mike!
>>>
>>> The goal is to estimate daily chlorophyll via band ratio polynomial
>>> equation for hundreds of days of data (hundreds of data files).  Sounds
>>> like rather than finding a way to orthorectify in R I should learn to batch
>>> reproject using SeaDAS, which does produce a product that is in geotiff
>>> format, is orthorectified, and has readily mappable.  I was trying to avoid
>>> that as the help and documentation available for doing that seems much less
>>> abundant.  One file at a time is easy using the SeaDAS gui.
>>>
>>> Thanks very, very much for the other tricks.  Not surprisingly, ggplot2
>>> comes through again with plots that look right!
>>> Cheers,
>>> Dave
>>>
>>>
>>>
>>> On Wed, Apr 12, 2017 at 7:01 AM, Michael Sumner <mdsumner at gmail.com>
>>> wrote:
>>>
>>> You can't georeference these data without remapping the data, essentially
>>> treating the pixels as points. They have no natural regular grid form,
>>> except possibly a unique satellite-perspective one. The data are in 2D
>>> array form, but they have explicit "geolocation arrays", i.e. a longitude
>>> and latitude for every cell and not based on a regular mapping.
>>>
>>> R does not have tools for this directly from these data, though it can be
>>> treated as a resampling or modelling problem.
>>> You can use raster to get at the values of the locations easily enough,
>>> here's a couple of plotting options in case it's useful:
>>>
>>> u <- "https://github.com/dmwarn/Tethys/blob/master/
>>> A2016244185500.L2_LAC_OC.x.nc?raw=true"
>>> f <- basename(f)
>>> download.file(u, f, mode = "wb")
>>>
>>> library(raster)
>>> ## use raster to treat as raw point data, on long-lat locations
>>> rrs <- raster(f, varname = "geophysical_data/Rrs_412")
>>> longitude <- raster(f, varname = "navigation_data/longitude")
>>> latitude <- raster(f, varname = "navigation_data/latitude")
>>>
>>> ## plot in grid space, and add the geolocation space as a graticule
>>> plot(rrs)
>>> contour(longitude, add = TRUE)
>>> contour(latitude, add = TRUE)
>>>
>>> ## raw scaling against rrs values
>>> scl <- function(x) (x - min(x, na.rm = TRUE))/diff(range(x, na.rm = TRUE))
>>> plot(values(longitude), values(latitude), pch = ".", col =
>>> topo.colors(56)[scl(values(rrs)) * 55 + 1])
>>>
>>> ## ggplot
>>> library(ggplot2)
>>> d <- data.frame(x = values(longitude), y = values(latitude), rrs =
>>> values(rrs))
>>> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = ".")
>>>
>>> ## might as well discard the missing values (depends on the other vars in
>>> the file)
>>> d <- d[!is.na(d$rrs), ]
>>> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = 19, cex =
>>> 0.1)
>>>
>>> There are some MODIS and GDAL based packages that might be of use, but I
>>> haven't yet seen any R tool that does this remapping task at scale. (I
>>> believe the MODIS tools and the best warping tools in GDAL use thin-plate
>>> spline techniques).
>>>
>>>  Some applications would use the observations as points (i.e. the ocean
>>> colour L3 bins as a daily aggregate from L2) and others use a direct
>>> remapping of the data as an array, for say high-resolution sea ice imagery.
>>>
>>> You might not need to do anything particularly complicated though, what's
>>> the goal?
>>>
>>> Cheers, Mike.
>>>
>>> On Wed, Apr 12, 2017, 20:06 Warner, David <dmwarner at usgs.gov> wrote:
>>>
>>> Greetings all
>>>
>>> I am trying to develop R code for processing L2 data (netcdf v4 files)
>>> from
>>> the Ocean Biology Processing Group.
>>>
>>> The data file I am working with to develop the code is at
>>> https://github.com/dmwarn/Tethys/blob/master/
>>> A2016244185500.L2_LAC_OC.x.nc
>>> and represents primarily Lake Michigan in the United States.  The data
>>> were
>>> extracted from a global dataset by the oceancolor L1 and L2 data server,
>>> not by me.
>>>
>>> I have been using the code below to try to get the data into R and ready
>>> for processing but am having problems with dimensions and/or
>>> orthorectification.  The
>>>
>>> #extent of the scene obtained using nc_open and ncvar_get
>>> nc <- nc_open('A2016214184500.L2_LAC_OC.x.nc')
>>> lon <- ncvar_get(nc, "navigation_data/longitude")
>>> lat <- ncvar_get(nc, "navigation_data/latitude")
>>> minx <- min(lon)
>>> maxx <- max(lon)
>>> miny <- min(lat)
>>> maxy <- max(lat)
>>>
>>> #create extent object
>>> myext <- extent(-90.817, -81.92438, 40.46493, 47.14244)
>>>
>>> #create raster
>>> rrs.412 <- raster('A2016214184500.L2_LAC_OC.x.nc', var
>>> ="geophysical_data/Rrs_412" ,
>>>                   ext=myext)
>>> rrs.412
>>>> rrs.412
>>> class       : RasterLayer
>>> dimensions  : 644, 528, 340032  (nrow, ncol, ncell)
>>> resolution  : 1, 1  (x, y)
>>> extent      : 0.5, 528.5, 0.5, 644.5  (xmin, xmax, ymin, ymax)
>>> coord. ref. : NA
>>> data source : /Users/dmwarner/Documents/MODIS/OC/
>>> A2016214184500.L2_LAC_OC.x.nc
>>> names       : Remote.sensing.reflectance.at.412.nm
>>> zvar        : geophysical_data/Rrs_412
>>>
>>> In spite of having tried to assign an extent, the raster extent is in rows
>>> and columns.
>>>
>>> Further, plotting the raster reveals that it is flipped on x axis and
>>> somewhat rotated relative to what it should look like.  Even when flipped,
>>> it is still not orthorectified.
>>>
>>> How do I get the raster to have the correct extent and also get it
>>> orthorectified?
>>> Thanks,
>>> Dave Warner
>>>
>>> --
>>> David Warner
>>> Research Fisheries Biologist
>>> U.S.G.S. Great Lakes Science Center
>>> 1451 Green Road
>>> Ann Arbor, MI 48105
>>> 734-214-9392 <(734)%20214-9392>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>> --
>>> Dr. Michael Sumner
>>> Software and Database Engineer
>>> Australian Antarctic Division
>>> 203 Channel Highway
>>> Kingston Tasmania 7050 Australia
>>>
>>>
>>>
>>>
>>> --
>>> David Warner
>>> Research Fisheries Biologist
>>> U.S.G.S. Great Lakes Science Center
>>> 1451 Green Road
>>> Ann Arbor, MI 48105
>>> 734-214-9392
>>>
>> --
>> Dr. Michael Sumner
>> Software and Database Engineer
>> Australian Antarctic Division
>> 203 Channel Highway
>> Kingston Tasmania 7050 Australia
>>
>>
>
>


From dmwarner at usgs.gov  Thu Apr 13 00:02:58 2017
From: dmwarner at usgs.gov (Warner, David)
Date: Wed, 12 Apr 2017 18:02:58 -0400
Subject: [R-sig-Geo] raster and oceancolor L2 netcdf data
In-Reply-To: <786d12ea-c907-d680-7b95-bcac354cdc87@conabio.gob.mx>
References: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>
 <CAAcGz98xcapeUajSHzsi=C2ZXbthYSZ=_=MDQMPZumwzKXD60w@mail.gmail.com>
 <CA+Y23taQCGwVD4_QYFNP6+tODFw5FS44zn_Qw3W_nF7GUVxBgQ@mail.gmail.com>
 <CAAcGz98Camhhuzmra5uhn-oBX4-_o8Y=Mk+wuJmvqMK0ntsQng@mail.gmail.com>
 <CA+Y23tYQNUO_ggX5-FDdGJX7NBJZc4U-=pDJY6Dn+_cscuaFUg@mail.gmail.com>
 <786d12ea-c907-d680-7b95-bcac354cdc87@conabio.gob.mx>
Message-ID: <CA+Y23tb9DNMM2tw8g+91kUpMhOo25+eN4MkVP-9YP+MJz+S4aQ@mail.gmail.com>

Thanks Lo?c!  I will read this.  I am not completely wedded to doing what I
want in R either.  Python would be ok but I have to learn more about
programming with Python.

Cheers,
Dave



On Wed, Apr 12, 2017 at 4:12 PM, Lo?c Dutrieux <loic.dutrieux at conabio.gob.mx
> wrote:

> I've done some work recently on ocean color L2 binning/mapping, see the
> discussion on the ocean color forum https://oceancolor.gsfc.nasa.g
> ov/forum/oceancolor/topic_show.pl?tid=6550 and the code in the gist
> (which I'll update). It's not a R solution, but could be useful still.
>
> Cheers,
> Lo?c
>
> On 12/04/17 12:05, Warner, David wrote:
>
>> Mike
>> I had not really thought about order of operations to be honest.  I just
>> noticed early on when I was attempting to use raster approach that the
>> data
>> were not mapped as hoped or orthorectified.  I certainly don't need to
>> remap before calculating chlor-a on a daily basis as all the bands I need
>> for a single day are aligned (if not mapped the way I wish).  In the end I
>> do need the data correctly mapped as I want to do matchups with data
>> collected with an LRAUV.
>>
>> I am planning on using locally calibrated coefficients.  I will check out
>> your package!  I initially wanted to use L3 data but I and a colleague
>> determined that there was for some reason poor agreement between the L3
>> data and our in situ matchup data even though at L2 there is good
>> agreement.  This colleague has typically done the heavy lifting using
>> ENVI,
>> which I don't have and would rather not learn if what I want to do can be
>> done in R.
>>
>> It looks like I can create a raster with vect2rast.SpatialPoints() from
>> the
>> plotKML package quite easily but the default settings for cell.size lead
>> to
>> loss of data (I think).  You can set a cell.size but I am not sure if it
>> works correctly without having multiple values per cell or not.  Or what
>> it
>> does if you have multiple values per cell.  There is some functionality
>> that allows you to pick the first, last, the min, the max, or the mean if
>> there are multiple  values for the same grid cell but I can't get that to
>> work without Saga GIS.
>>
>> Cheers and thanks,
>> Dave
>>
>> On Wed, Apr 12, 2017 at 8:57 AM, Michael Sumner <mdsumner at gmail.com>
>> wrote:
>>
>> Glad it's some help, but it sounds like you intend to calculate after
>>> mapping (?) which is definitely not the right way to go. Calculate
>>> chlorophyll and then map, that's how Seadas does it, even though the
>>> remapping is the hard part. And apologies if I misread,  just checking.
>>>
>>> I have two algorithms in my roc package on GitHub in case they help
>>> understanding how the calcs get done. Presumably you'll have locally
>>> calibrated parameters for a local algo?
>>>
>>> If you want to aggregate into a local map I think it's fair to group-by
>>> directly on L2 pixels coords and then sum into a geographic map, without
>>> worrying about swath-as-image at all. We've road tested doing this but
>>> want
>>> the entire southern ocean eventually so it needs a bit of unrelated
>>> preparation for the raw files.
>>>
>>> I'd be happy to explore an R solution off list if you're interested. L2
>>> is
>>> surprisingly easy and efficient in R via GDAL.
>>>
>>> (This is also a good example for future workflows for the planned stars
>>> package imo.)
>>>
>>> Cheers, Mike
>>>
>>> On Wed, Apr 12, 2017, 22:35 Warner, David <dmwarner at usgs.gov> wrote:
>>>
>>> Thanks Mike!
>>>>
>>>> The goal is to estimate daily chlorophyll via band ratio polynomial
>>>> equation for hundreds of days of data (hundreds of data files).  Sounds
>>>> like rather than finding a way to orthorectify in R I should learn to
>>>> batch
>>>> reproject using SeaDAS, which does produce a product that is in geotiff
>>>> format, is orthorectified, and has readily mappable.  I was trying to
>>>> avoid
>>>> that as the help and documentation available for doing that seems much
>>>> less
>>>> abundant.  One file at a time is easy using the SeaDAS gui.
>>>>
>>>> Thanks very, very much for the other tricks.  Not surprisingly, ggplot2
>>>> comes through again with plots that look right!
>>>> Cheers,
>>>> Dave
>>>>
>>>>
>>>>
>>>> On Wed, Apr 12, 2017 at 7:01 AM, Michael Sumner <mdsumner at gmail.com>
>>>> wrote:
>>>>
>>>> You can't georeference these data without remapping the data,
>>>> essentially
>>>> treating the pixels as points. They have no natural regular grid form,
>>>> except possibly a unique satellite-perspective one. The data are in 2D
>>>> array form, but they have explicit "geolocation arrays", i.e. a
>>>> longitude
>>>> and latitude for every cell and not based on a regular mapping.
>>>>
>>>> R does not have tools for this directly from these data, though it can
>>>> be
>>>> treated as a resampling or modelling problem.
>>>> You can use raster to get at the values of the locations easily enough,
>>>> here's a couple of plotting options in case it's useful:
>>>>
>>>> u <- "https://github.com/dmwarn/Tethys/blob/master/
>>>> A2016244185500.L2_LAC_OC.x.nc?raw=true"
>>>> f <- basename(f)
>>>> download.file(u, f, mode = "wb")
>>>>
>>>> library(raster)
>>>> ## use raster to treat as raw point data, on long-lat locations
>>>> rrs <- raster(f, varname = "geophysical_data/Rrs_412")
>>>> longitude <- raster(f, varname = "navigation_data/longitude")
>>>> latitude <- raster(f, varname = "navigation_data/latitude")
>>>>
>>>> ## plot in grid space, and add the geolocation space as a graticule
>>>> plot(rrs)
>>>> contour(longitude, add = TRUE)
>>>> contour(latitude, add = TRUE)
>>>>
>>>> ## raw scaling against rrs values
>>>> scl <- function(x) (x - min(x, na.rm = TRUE))/diff(range(x, na.rm =
>>>> TRUE))
>>>> plot(values(longitude), values(latitude), pch = ".", col =
>>>> topo.colors(56)[scl(values(rrs)) * 55 + 1])
>>>>
>>>> ## ggplot
>>>> library(ggplot2)
>>>> d <- data.frame(x = values(longitude), y = values(latitude), rrs =
>>>> values(rrs))
>>>> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = ".")
>>>>
>>>> ## might as well discard the missing values (depends on the other vars
>>>> in
>>>> the file)
>>>> d <- d[!is.na(d$rrs), ]
>>>> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = 19, cex =
>>>> 0.1)
>>>>
>>>> There are some MODIS and GDAL based packages that might be of use, but I
>>>> haven't yet seen any R tool that does this remapping task at scale. (I
>>>> believe the MODIS tools and the best warping tools in GDAL use
>>>> thin-plate
>>>> spline techniques).
>>>>
>>>>  Some applications would use the observations as points (i.e. the ocean
>>>> colour L3 bins as a daily aggregate from L2) and others use a direct
>>>> remapping of the data as an array, for say high-resolution sea ice
>>>> imagery.
>>>>
>>>> You might not need to do anything particularly complicated though,
>>>> what's
>>>> the goal?
>>>>
>>>> Cheers, Mike.
>>>>
>>>> On Wed, Apr 12, 2017, 20:06 Warner, David <dmwarner at usgs.gov> wrote:
>>>>
>>>> Greetings all
>>>>
>>>> I am trying to develop R code for processing L2 data (netcdf v4 files)
>>>> from
>>>> the Ocean Biology Processing Group.
>>>>
>>>> The data file I am working with to develop the code is at
>>>> https://github.com/dmwarn/Tethys/blob/master/
>>>> A2016244185500.L2_LAC_OC.x.nc
>>>> and represents primarily Lake Michigan in the United States.  The data
>>>> were
>>>> extracted from a global dataset by the oceancolor L1 and L2 data server,
>>>> not by me.
>>>>
>>>> I have been using the code below to try to get the data into R and ready
>>>> for processing but am having problems with dimensions and/or
>>>> orthorectification.  The
>>>>
>>>> #extent of the scene obtained using nc_open and ncvar_get
>>>> nc <- nc_open('A2016214184500.L2_LAC_OC.x.nc')
>>>> lon <- ncvar_get(nc, "navigation_data/longitude")
>>>> lat <- ncvar_get(nc, "navigation_data/latitude")
>>>> minx <- min(lon)
>>>> maxx <- max(lon)
>>>> miny <- min(lat)
>>>> maxy <- max(lat)
>>>>
>>>> #create extent object
>>>> myext <- extent(-90.817, -81.92438, 40.46493, 47.14244)
>>>>
>>>> #create raster
>>>> rrs.412 <- raster('A2016214184500.L2_LAC_OC.x.nc', var
>>>> ="geophysical_data/Rrs_412" ,
>>>>                   ext=myext)
>>>> rrs.412
>>>>
>>>>> rrs.412
>>>>>
>>>> class       : RasterLayer
>>>> dimensions  : 644, 528, 340032  (nrow, ncol, ncell)
>>>> resolution  : 1, 1  (x, y)
>>>> extent      : 0.5, 528.5, 0.5, 644.5  (xmin, xmax, ymin, ymax)
>>>> coord. ref. : NA
>>>> data source : /Users/dmwarner/Documents/MODIS/OC/
>>>> A2016214184500.L2_LAC_OC.x.nc
>>>> names       : Remote.sensing.reflectance.at.412.nm
>>>> zvar        : geophysical_data/Rrs_412
>>>>
>>>> In spite of having tried to assign an extent, the raster extent is in
>>>> rows
>>>> and columns.
>>>>
>>>> Further, plotting the raster reveals that it is flipped on x axis and
>>>> somewhat rotated relative to what it should look like.  Even when
>>>> flipped,
>>>> it is still not orthorectified.
>>>>
>>>> How do I get the raster to have the correct extent and also get it
>>>> orthorectified?
>>>> Thanks,
>>>> Dave Warner
>>>>
>>>> --
>>>> David Warner
>>>> Research Fisheries Biologist
>>>> U.S.G.S. Great Lakes Science Center
>>>> 1451 Green Road
>>>> Ann Arbor, MI 48105
>>>> 734-214-9392 <(734)%20214-9392>
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>> --
>>>> Dr. Michael Sumner
>>>> Software and Database Engineer
>>>> Australian Antarctic Division
>>>> 203 Channel Highway
>>>> Kingston Tasmania 7050 Australia
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> David Warner
>>>> Research Fisheries Biologist
>>>> U.S.G.S. Great Lakes Science Center
>>>> 1451 Green Road
>>>> Ann Arbor, MI 48105
>>>> 734-214-9392
>>>>
>>>> --
>>> Dr. Michael Sumner
>>> Software and Database Engineer
>>> Australian Antarctic Division
>>> 203 Channel Highway
>>> Kingston Tasmania 7050 Australia
>>>
>>>
>>>
>>
>>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>


-- 
David Warner
Research Fisheries Biologist
U.S.G.S. Great Lakes Science Center
1451 Green Road
Ann Arbor, MI 48105
734-214-9392

	[[alternative HTML version deleted]]


From mtreglia at gmail.com  Thu Apr 13 00:45:09 2017
From: mtreglia at gmail.com (Michael Treglia)
Date: Wed, 12 Apr 2017 18:45:09 -0400
Subject: [R-sig-Geo] raster and oceancolor L2 netcdf data
In-Reply-To: <CA+Y23tb9DNMM2tw8g+91kUpMhOo25+eN4MkVP-9YP+MJz+S4aQ@mail.gmail.com>
References: <CA+Y23tYfcWc-u-tTrDLB8hfO6xyrwsvkaHaMdB2ZoZ0LyJZXFA@mail.gmail.com>
 <CAAcGz98xcapeUajSHzsi=C2ZXbthYSZ=_=MDQMPZumwzKXD60w@mail.gmail.com>
 <CA+Y23taQCGwVD4_QYFNP6+tODFw5FS44zn_Qw3W_nF7GUVxBgQ@mail.gmail.com>
 <CAAcGz98Camhhuzmra5uhn-oBX4-_o8Y=Mk+wuJmvqMK0ntsQng@mail.gmail.com>
 <CA+Y23tYQNUO_ggX5-FDdGJX7NBJZc4U-=pDJY6Dn+_cscuaFUg@mail.gmail.com>
 <786d12ea-c907-d680-7b95-bcac354cdc87@conabio.gob.mx>
 <CA+Y23tb9DNMM2tw8g+91kUpMhOo25+eN4MkVP-9YP+MJz+S4aQ@mail.gmail.com>
Message-ID: <CAPKp32uabTu8K1VC6JXpHwVnRUSwEmDFePLL=9K=wEr-CwKpsg@mail.gmail.com>

Hi All,

Just saw this thread, and wanted to share some code I've got, in case it's
useful, though I'll give the disclaimer I haven't looked carefully at the
focal dataset here, though it sounds somewhat similar.

Basically, I'm working with modeled data for NY Harbor, across multiple
time points, with multiple depth horizons.  It's also not a regular grid...
so I need look at the data as points or polygons. (it looks like a
spiderweb across the area...)

I've got code for pulling the data from a remote NetCDF source, assigning
values to x/y points, and plotting it with base R here:
https://github.com/mltConsEcol/R_NYHOPS_NetCDF_Access/blob/master/R/Working_RCode_MLT.R


You can disregard Ln 91 onward for now, but the rest might be of some use.
You can assign the data, with the x/y coords to a spatial (or simple
features) object - just havent included the code here yet (and sorry, in
the midst of travel so I'm appending this right now).

The next step will involve pulling x coords and y coords for polygons
associated with the points, which are represented in arrays (see lines
~66-71 of the code to get a sense of the structure), and then formatting
those such that they can be convert to spatial polygons/simple features
objects...  In the mean-time though, I've just used Voronoi polygons to
roughly approximate represent the areas associated with the points for
visualization (not included in the code, just done quickly in QGIS).

Again - hope this is of some use and not just noise... I'm all ears for
better ways to deal w/ the data too.

Best,
Mike




On Wed, Apr 12, 2017 at 6:02 PM, Warner, David <dmwarner at usgs.gov> wrote:

> Thanks Lo?c!  I will read this.  I am not completely wedded to doing what I
> want in R either.  Python would be ok but I have to learn more about
> programming with Python.
>
> Cheers,
> Dave
>
>
>
> On Wed, Apr 12, 2017 at 4:12 PM, Lo?c Dutrieux <
> loic.dutrieux at conabio.gob.mx
> > wrote:
>
> > I've done some work recently on ocean color L2 binning/mapping, see the
> > discussion on the ocean color forum https://oceancolor.gsfc.nasa.g
> > ov/forum/oceancolor/topic_show.pl?tid=6550 and the code in the gist
> > (which I'll update). It's not a R solution, but could be useful still.
> >
> > Cheers,
> > Lo?c
> >
> > On 12/04/17 12:05, Warner, David wrote:
> >
> >> Mike
> >> I had not really thought about order of operations to be honest.  I just
> >> noticed early on when I was attempting to use raster approach that the
> >> data
> >> were not mapped as hoped or orthorectified.  I certainly don't need to
> >> remap before calculating chlor-a on a daily basis as all the bands I
> need
> >> for a single day are aligned (if not mapped the way I wish).  In the
> end I
> >> do need the data correctly mapped as I want to do matchups with data
> >> collected with an LRAUV.
> >>
> >> I am planning on using locally calibrated coefficients.  I will check
> out
> >> your package!  I initially wanted to use L3 data but I and a colleague
> >> determined that there was for some reason poor agreement between the L3
> >> data and our in situ matchup data even though at L2 there is good
> >> agreement.  This colleague has typically done the heavy lifting using
> >> ENVI,
> >> which I don't have and would rather not learn if what I want to do can
> be
> >> done in R.
> >>
> >> It looks like I can create a raster with vect2rast.SpatialPoints() from
> >> the
> >> plotKML package quite easily but the default settings for cell.size lead
> >> to
> >> loss of data (I think).  You can set a cell.size but I am not sure if it
> >> works correctly without having multiple values per cell or not.  Or what
> >> it
> >> does if you have multiple values per cell.  There is some functionality
> >> that allows you to pick the first, last, the min, the max, or the mean
> if
> >> there are multiple  values for the same grid cell but I can't get that
> to
> >> work without Saga GIS.
> >>
> >> Cheers and thanks,
> >> Dave
> >>
> >> On Wed, Apr 12, 2017 at 8:57 AM, Michael Sumner <mdsumner at gmail.com>
> >> wrote:
> >>
> >> Glad it's some help, but it sounds like you intend to calculate after
> >>> mapping (?) which is definitely not the right way to go. Calculate
> >>> chlorophyll and then map, that's how Seadas does it, even though the
> >>> remapping is the hard part. And apologies if I misread,  just checking.
> >>>
> >>> I have two algorithms in my roc package on GitHub in case they help
> >>> understanding how the calcs get done. Presumably you'll have locally
> >>> calibrated parameters for a local algo?
> >>>
> >>> If you want to aggregate into a local map I think it's fair to group-by
> >>> directly on L2 pixels coords and then sum into a geographic map,
> without
> >>> worrying about swath-as-image at all. We've road tested doing this but
> >>> want
> >>> the entire southern ocean eventually so it needs a bit of unrelated
> >>> preparation for the raw files.
> >>>
> >>> I'd be happy to explore an R solution off list if you're interested. L2
> >>> is
> >>> surprisingly easy and efficient in R via GDAL.
> >>>
> >>> (This is also a good example for future workflows for the planned stars
> >>> package imo.)
> >>>
> >>> Cheers, Mike
> >>>
> >>> On Wed, Apr 12, 2017, 22:35 Warner, David <dmwarner at usgs.gov> wrote:
> >>>
> >>> Thanks Mike!
> >>>>
> >>>> The goal is to estimate daily chlorophyll via band ratio polynomial
> >>>> equation for hundreds of days of data (hundreds of data files).
> Sounds
> >>>> like rather than finding a way to orthorectify in R I should learn to
> >>>> batch
> >>>> reproject using SeaDAS, which does produce a product that is in
> geotiff
> >>>> format, is orthorectified, and has readily mappable.  I was trying to
> >>>> avoid
> >>>> that as the help and documentation available for doing that seems much
> >>>> less
> >>>> abundant.  One file at a time is easy using the SeaDAS gui.
> >>>>
> >>>> Thanks very, very much for the other tricks.  Not surprisingly,
> ggplot2
> >>>> comes through again with plots that look right!
> >>>> Cheers,
> >>>> Dave
> >>>>
> >>>>
> >>>>
> >>>> On Wed, Apr 12, 2017 at 7:01 AM, Michael Sumner <mdsumner at gmail.com>
> >>>> wrote:
> >>>>
> >>>> You can't georeference these data without remapping the data,
> >>>> essentially
> >>>> treating the pixels as points. They have no natural regular grid form,
> >>>> except possibly a unique satellite-perspective one. The data are in 2D
> >>>> array form, but they have explicit "geolocation arrays", i.e. a
> >>>> longitude
> >>>> and latitude for every cell and not based on a regular mapping.
> >>>>
> >>>> R does not have tools for this directly from these data, though it can
> >>>> be
> >>>> treated as a resampling or modelling problem.
> >>>> You can use raster to get at the values of the locations easily
> enough,
> >>>> here's a couple of plotting options in case it's useful:
> >>>>
> >>>> u <- "https://github.com/dmwarn/Tethys/blob/master/
> >>>> A2016244185500.L2_LAC_OC.x.nc?raw=true"
> >>>> f <- basename(f)
> >>>> download.file(u, f, mode = "wb")
> >>>>
> >>>> library(raster)
> >>>> ## use raster to treat as raw point data, on long-lat locations
> >>>> rrs <- raster(f, varname = "geophysical_data/Rrs_412")
> >>>> longitude <- raster(f, varname = "navigation_data/longitude")
> >>>> latitude <- raster(f, varname = "navigation_data/latitude")
> >>>>
> >>>> ## plot in grid space, and add the geolocation space as a graticule
> >>>> plot(rrs)
> >>>> contour(longitude, add = TRUE)
> >>>> contour(latitude, add = TRUE)
> >>>>
> >>>> ## raw scaling against rrs values
> >>>> scl <- function(x) (x - min(x, na.rm = TRUE))/diff(range(x, na.rm =
> >>>> TRUE))
> >>>> plot(values(longitude), values(latitude), pch = ".", col =
> >>>> topo.colors(56)[scl(values(rrs)) * 55 + 1])
> >>>>
> >>>> ## ggplot
> >>>> library(ggplot2)
> >>>> d <- data.frame(x = values(longitude), y = values(latitude), rrs =
> >>>> values(rrs))
> >>>> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = ".")
> >>>>
> >>>> ## might as well discard the missing values (depends on the other vars
> >>>> in
> >>>> the file)
> >>>> d <- d[!is.na(d$rrs), ]
> >>>> ggplot(d, aes(x = x, y = y, colour = rrs)) + geom_point(pch = 19, cex
> =
> >>>> 0.1)
> >>>>
> >>>> There are some MODIS and GDAL based packages that might be of use,
> but I
> >>>> haven't yet seen any R tool that does this remapping task at scale. (I
> >>>> believe the MODIS tools and the best warping tools in GDAL use
> >>>> thin-plate
> >>>> spline techniques).
> >>>>
> >>>>  Some applications would use the observations as points (i.e. the
> ocean
> >>>> colour L3 bins as a daily aggregate from L2) and others use a direct
> >>>> remapping of the data as an array, for say high-resolution sea ice
> >>>> imagery.
> >>>>
> >>>> You might not need to do anything particularly complicated though,
> >>>> what's
> >>>> the goal?
> >>>>
> >>>> Cheers, Mike.
> >>>>
> >>>> On Wed, Apr 12, 2017, 20:06 Warner, David <dmwarner at usgs.gov> wrote:
> >>>>
> >>>> Greetings all
> >>>>
> >>>> I am trying to develop R code for processing L2 data (netcdf v4 files)
> >>>> from
> >>>> the Ocean Biology Processing Group.
> >>>>
> >>>> The data file I am working with to develop the code is at
> >>>> https://github.com/dmwarn/Tethys/blob/master/
> >>>> A2016244185500.L2_LAC_OC.x.nc
> >>>> and represents primarily Lake Michigan in the United States.  The data
> >>>> were
> >>>> extracted from a global dataset by the oceancolor L1 and L2 data
> server,
> >>>> not by me.
> >>>>
> >>>> I have been using the code below to try to get the data into R and
> ready
> >>>> for processing but am having problems with dimensions and/or
> >>>> orthorectification.  The
> >>>>
> >>>> #extent of the scene obtained using nc_open and ncvar_get
> >>>> nc <- nc_open('A2016214184500.L2_LAC_OC.x.nc')
> >>>> lon <- ncvar_get(nc, "navigation_data/longitude")
> >>>> lat <- ncvar_get(nc, "navigation_data/latitude")
> >>>> minx <- min(lon)
> >>>> maxx <- max(lon)
> >>>> miny <- min(lat)
> >>>> maxy <- max(lat)
> >>>>
> >>>> #create extent object
> >>>> myext <- extent(-90.817, -81.92438, 40.46493, 47.14244)
> >>>>
> >>>> #create raster
> >>>> rrs.412 <- raster('A2016214184500.L2_LAC_OC.x.nc', var
> >>>> ="geophysical_data/Rrs_412" ,
> >>>>                   ext=myext)
> >>>> rrs.412
> >>>>
> >>>>> rrs.412
> >>>>>
> >>>> class       : RasterLayer
> >>>> dimensions  : 644, 528, 340032  (nrow, ncol, ncell)
> >>>> resolution  : 1, 1  (x, y)
> >>>> extent      : 0.5, 528.5, 0.5, 644.5  (xmin, xmax, ymin, ymax)
> >>>> coord. ref. : NA
> >>>> data source : /Users/dmwarner/Documents/MODIS/OC/
> >>>> A2016214184500.L2_LAC_OC.x.nc
> >>>> names       : Remote.sensing.reflectance.at.412.nm
> >>>> zvar        : geophysical_data/Rrs_412
> >>>>
> >>>> In spite of having tried to assign an extent, the raster extent is in
> >>>> rows
> >>>> and columns.
> >>>>
> >>>> Further, plotting the raster reveals that it is flipped on x axis and
> >>>> somewhat rotated relative to what it should look like.  Even when
> >>>> flipped,
> >>>> it is still not orthorectified.
> >>>>
> >>>> How do I get the raster to have the correct extent and also get it
> >>>> orthorectified?
> >>>> Thanks,
> >>>> Dave Warner
> >>>>
> >>>> --
> >>>> David Warner
> >>>> Research Fisheries Biologist
> >>>> U.S.G.S. Great Lakes Science Center
> >>>> 1451 Green Road
> >>>> Ann Arbor, MI 48105
> >>>> 734-214-9392 <(734)%20214-9392>
> >>>>
> >>>>         [[alternative HTML version deleted]]
> >>>>
> >>>> _______________________________________________
> >>>> R-sig-Geo mailing list
> >>>> R-sig-Geo at r-project.org
> >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>>>
> >>>> --
> >>>> Dr. Michael Sumner
> >>>> Software and Database Engineer
> >>>> Australian Antarctic Division
> >>>> 203 Channel Highway
> >>>> Kingston Tasmania 7050 Australia
> >>>>
> >>>>
> >>>>
> >>>>
> >>>> --
> >>>> David Warner
> >>>> Research Fisheries Biologist
> >>>> U.S.G.S. Great Lakes Science Center
> >>>> 1451 Green Road
> >>>> Ann Arbor, MI 48105
> >>>> 734-214-9392
> >>>>
> >>>> --
> >>> Dr. Michael Sumner
> >>> Software and Database Engineer
> >>> Australian Antarctic Division
> >>> 203 Channel Highway
> >>> Kingston Tasmania 7050 Australia
> >>>
> >>>
> >>>
> >>
> >>
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
> >
>
>
> --
> David Warner
> Research Fisheries Biologist
> U.S.G.S. Great Lakes Science Center
> 1451 Green Road
> Ann Arbor, MI 48105
> 734-214-9392
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From vladimir.wingate at unibas.ch  Thu Apr 13 15:23:54 2017
From: vladimir.wingate at unibas.ch (Vladimir Ruslan Wingate)
Date: Thu, 13 Apr 2017 13:23:54 +0000
Subject: [R-sig-Geo] MODIS package and gdalUtils
Message-ID: <2C2A465108AF8348915C9F73CCAFC8146F63079F@urz-mbx-2.urz.unibas.ch>

Hi all,
I get the following error from running to following code using the MODIS package and gdalUtils. Thanks.
names(s) <- as.Date(modis.hdf[[1]], "A%
Error in `names<-`(`*tmp*`, value = c(NA_real_, NA_real_)) :
  incorrect number of layer names

Here is the whole code:

library(MODIS)
library(gdalUtils)
MODISoptions(gdalPath="C:/OSGeo4W64/bin")
#install.packages("MODIS", repos ="http://R-Forge.R-project.org")
setwd("J:/chapter 11")
getwd()

gdal_setInstallation(search_path = "C:/OSGeo4W64/bin", rescan = TRUE,ignore.full_scan = TRUE, verbose = FALSE)

MODISoptions(localArcPath = getwd(), outDirPath =getwd())

MODIS:::checkTools('GDAL')
MODIS:::checkTools('MRT')

viname <- "ndvi"
product <- "MOD13Q1"
ofilename <- paste0(product, "_", viname, "_brick.grd")
ofilename

pth <- paste0(getwd(), "/raster_data/", product)
pth

fileout <- paste(pth, "/", ofilename, sep="")
fileout

if (!file.exists(fileout)) {
  if (!file.exists(pth)) {
    print("the outfolder does not exist and will be created")
    print(pth)
    dir.create(pth, recursive = TRUE)
  }
}

tileH <- 19
tileV <- 10

begin <- "2015.01.01"
end <- "2015.01.20"

modis.hdf <- getHdf(product = product, begin=begin, end=end, tileH=tileH, tileV=tileV, checkIntegrity=TRUE)
modis.hdf



for (i in 1:length(modis.hdf[[1]])) {

  ifl <- unlist(strsplit(unlist(strsplit(modis.hdf[[1]][i], c("[/]"))) [6], "[.]")) [1]#5
  ifl
  print(ifl)

  fn <- paste("cvi_", ifl, ".tif", sep="")
  print(fn)

  if (is.na(ifl) | file.exists(fn)) {

    print("file exists or is not available on the server")

  } else {

    sds <- get_subdatasets(modis.hdf[[1]][i])
  }
}


tmp <- rasterTmpFile()
extension(tmp) <- "tif"
library(gdalUtils)
gdal_translate(sds[1], dst_dataset=tmp)

ndvi <- raster(tmp)/10000

writeRaster(ndvi, filename=paste0("ndvi_", ifl, ".tif", sep=""), dataType="INT2U", format="GTiff", overwrite=TRUE)

tmp2 <- rasterTmpFile()
extension(tmp2) <- "tif"
gdal_translate(sds[12], dst_dataset=tmp2)

rel <- crop(x=raster(tmp2), y=raster(tmp2),
            filename=paste("rel_", ifl, ".tif", sep=""),
            dataType="INT2U", format="GTiff")


f_cleanvi <- function(vi, rel) {

  i <- (rel <= 1)

  res <- matrix(NA, length(i), 1)

  if (sum(i, na.rm=TRUE) > 0) {

    i <- which(i)

    res[i] <- vi[i]
  }
  res
}


clvi <- overlay(ndvi, rel, fun=f_cleanvi, filename=fn, dataType="INT2U", overwrite=TRUE)
clvi

rm(ndvi, rel, clvi, tmp, tmp2)

ofl <- list.files(pattern=glob2rx("rel*.tif"))
class(ofl)
ofl

modis.hdf[[1]]


s <- do.call("brick", lapply(ofl, raster))


names(s) <- as.Date(modis.hdf[[1]], "A%Y%j")

	[[alternative HTML version deleted]]


From asharma at unbc.ca  Thu Apr 13 21:11:38 2017
From: asharma at unbc.ca (Aseem Sharma)
Date: Thu, 13 Apr 2017 19:11:38 +0000
Subject: [R-sig-Geo] How to perform integration of .nc file in Raster R
	Package
Message-ID: <5ba16925cd9a4addb571c3266ee3e64f@pg-adr-exch-03.adr.unbc.ca>

Hi,
I have a .nc<https://www.dropbox.com/s/9x951b2qwjwfktf/qu.nc?dl=0> file with 2 variables q an u and 4 dimensions namely lat, lon, pressure level and time. This is a sample data of specific humidity and wind direction from ERA_interim.
How can i calculate the integration of product of variables q and u at each grid cell ( ie the vapour flux)  for all time steps i e how to solve following equation in R to get int.

[cid:image002.jpg at 01D2B44F.1A97D5A0]
I have tried to make stacks of q and u for 3 levels as qs and us using raster package but got lost on calculating the integration value.
The sample data file qu.nc (529 KB) is here<https://www.dropbox.com/s/9x951b2qwjwfktf/qu.nc?dl=0>.
 Any help to write a function to solve this integration would be greatly appreciated.
required (raster)
    ncfl<-"qu.nc" #.nc datafile
    ncf<-stack(ncfl)
    print(ncf)
n <- 3 # Pressure levels 1000,950,900
for (i in 1:n) {
  names <- paste("u", i, sep=".")
  assign(names, brick(ncfl, varname ="u", lvar = 3, level
                      = i) )
}
for (i in 1:n) {
  names <- paste("q", i, sep=".")
  assign(names, brick(ncfl, varname ="q", lvar = 3, level
                      = i) )
}

qs<-stack(q.1,q.2,q.3)
us<-stack(u.1,u.2,u.3)

Thank you,
as

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170413/22bd72de/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image002.jpg
Type: image/jpeg
Size: 1676 bytes
Desc: image002.jpg
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170413/22bd72de/attachment.jpg>

From basille.web at ase-research.org  Sat Apr 15 06:14:50 2017
From: basille.web at ase-research.org (Mathieu Basille)
Date: Sat, 15 Apr 2017 00:14:50 -0400
Subject: [R-sig-Geo] Question niche based model whith R
In-Reply-To: <626667739.279201.1491815574118@mail.yahoo.com>
References: <626667739.279201.1491815574118.ref@mail.yahoo.com>
 <626667739.279201.1491815574118@mail.yahoo.com>
Message-ID: <2930a7b2-237a-f336-672c-96ddc68158d8@ase-research.org>

You can also have a look at exploratory niche analyses, such as the
Ecological niche factor analysis. A few of them are available in the
'adehabitatHS' package:

https://cran.r-project.org/package=adehabitatHS

Detailed explanations are available in the vignette (section 3: Design I
studies):

https://cran.r-project.org/web/packages/adehabitatHS/vignettes/adehabitatHS.pdf

Hope this helps,
Mathieu.


On 04/10/2017 05:12 AM, Soufianou Abou via R-sig-Geo wrote:
>  Hello dears,I work on the modeling of the ecological niche of striga, I have presence data (data) and bioclimatic variables and other variables around.Indeed, my question how should I proceed to model the ecological niche with software R? Pending receipt of my request, please accept my best regards.SADDA Abou-Soufianou -------------------------------------- DoctorantUniversit? Dan Dicko Dankoulodo de Maradi,120 avenue Maman Koraou,ADS, MaradiBP 465 Maradi/Niger E-mail: a.soufianou at yahoo.frGSM : (+227)96269987
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 

Mathieu Basille

basille at ufl.edu | http://ase-research.org/basille
+1 954-577-6314 | University of Florida FLREC

  ? Le tout est de tout dire, et je manque de mots
  Et je manque de temps, et je manque d'audace. ?
  ? Paul ?luard


From rbarnes at umn.edu  Tue Apr 18 03:50:02 2017
From: rbarnes at umn.edu (Richard Barnes)
Date: Mon, 17 Apr 2017 18:50:02 -0700
Subject: [R-sig-Geo] WebGL Earth
Message-ID: <d0d3ba1f-bed1-2a94-2cd8-b80ddfed5abb@umn.edu>

I'm thinking about cobbling together a little package to show data in
WebGL Earth (http://www.webglearth.org/), similar to Leaflet for R
(https://rstudio.github.io/leaflet/).

Just wanted to inquire as to whether anyone's already built such a thing
or at work on it?

Best regards,
Richard


From tim.appelhans at gmail.com  Tue Apr 18 07:40:51 2017
From: tim.appelhans at gmail.com (Tim Appelhans)
Date: Tue, 18 Apr 2017 07:40:51 +0200
Subject: [R-sig-Geo] WebGL Earth
In-Reply-To: <d0d3ba1f-bed1-2a94-2cd8-b80ddfed5abb@umn.edu>
References: <d0d3ba1f-bed1-2a94-2cd8-b80ddfed5abb@umn.edu>
Message-ID: <5fa6dfd2-78a4-d0b9-796b-e5a9a5102789@gmail.com>

Richard,

great idea! We have a 'Hello World' Cesium htmlwidget which has never 
seen any further progress.

https://github.com/environmentalinformatics-marburg/cesium

May I ask what the added value of webglearth over leaflet is (apart from 
a fancy spinning globe)? Cesium has native space-time support so would 
really add something that isn't (yet) accessible from R. Would you 
consider taking on a grander challenge working on cesium implementation? 
If so, we could move the above repo to r-spatial at github and trun it 
into a group effort.

Anyway, just some thoughts...

Tim


On 18.04.2017 03:50, Richard Barnes wrote:
> I'm thinking about cobbling together a little package to show data in
> WebGL Earth (http://www.webglearth.org/), similar to Leaflet for R
> (https://rstudio.github.io/leaflet/).
>
> Just wanted to inquire as to whether anyone's already built such a thing
> or at work on it?
>
> Best regards,
> Richard
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Tim Appelhans
Research Specialist, Geomarketing
GfK | Bamberger Str. 6, 90425 N?rnberg | Germany
Postal address: Nordwestring 101 | 90419 N?rnberg


From mdsumner at gmail.com  Tue Apr 18 08:54:44 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Tue, 18 Apr 2017 06:54:44 +0000
Subject: [R-sig-Geo] WebGL Earth
In-Reply-To: <d0d3ba1f-bed1-2a94-2cd8-b80ddfed5abb@umn.edu>
References: <d0d3ba1f-bed1-2a94-2cd8-b80ddfed5abb@umn.edu>
Message-ID: <CAAcGz98S35U-GxMW5+-6VgaE1ceNAAGwU7rpN4-+6eyN=1KfeA@mail.gmail.com>

HI Richard,


On Tue, 18 Apr 2017 at 11:50 Richard Barnes <rbarnes at umn.edu> wrote:

> I'm thinking about cobbling together a little package to show data in
> WebGL Earth (http://www.webglearth.org/), similar to Leaflet for R
> (https://rstudio.github.io/leaflet/).
>
>
I put this together to put various R data structures into forms ready for
rgl or WebGL:

https://github.com/r-gris/rangl

The pattern is

## convert to table entities
x <- rangl(object)

## plot in rgl form and return the rgl structure (list of arrays)
rg <- plot(x)

## optionally convert to geocentric coordinates rather than the native CRS
rg <- plot(globe(x))

 The geocentric step is merely transformation on the unique vertices after
decomposition to primitiveslike, so it can be done independent of any other
structure.

This is how I would transfer data to WebGL, with optimizations as needed. I
wish we had a general central core with primitives like this but ultimately
it's just tables and indexing so pretty easy to do.

Cheers, Mike.


Just wanted to inquire as to whether anyone's already built such a thing
> or at work on it?
>
> Best regards,
> Richard
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From rbarnes at umn.edu  Fri Apr 21 21:28:43 2017
From: rbarnes at umn.edu (Richard Barnes)
Date: Fri, 21 Apr 2017 12:28:43 -0700
Subject: [R-sig-Geo] 3D interactive maps with "webglobe"
Message-ID: <6f0d8e84-0e0c-0181-4d77-af6c37645d6c@umn.edu>

I've just released a beta version of my package "webglobe".

It lets you plot map-related data in on a 3D, interactive globe using
your web browser.

The package is available at: https://github.com/r-barnes/webglobe

Cheers,
Richard


From carlaoliveir at gmail.com  Sun Apr 23 01:43:02 2017
From: carlaoliveir at gmail.com (Carla Oliveira)
Date: Sun, 23 Apr 2017 00:43:02 +0100
Subject: [R-sig-Geo] GEOMED 2017 - One Week To Abstract Submission
Message-ID: <CAOoNHqjE1Mitn7WP+wm3C17Vuekd+zJ2aH4Bk5V9LgXxWDpp2w@mail.gmail.com>

GEOMED 2017 - Porto, Portugal
View this email in your browser
<http://mailchi.mp/6d0deac3357d/geomed-2017-one-week-to-abstract-submission?e=7228bb7854>
DEADLINE EXTENTION:
30th OF APRIL
*ONE WEEK MORE TO ABSTRACT SUBMISSION*

The *Institute for Research and Innovation in Health **(Instituto de
Investiga??o e Inova??o em Sa?de) *(i3S), is pleased to
announce GEOMED 2017 ? ?Deeper insight from big data and small areas?.

If you have not submited your abstract yet, we invite you to do it in any
one of the topic areas  planned for GEOMED2017 which include:

   - *Data Science applied to Health: Strategies and tools for big data,
   machine learning and data mining.*
   - *Agent-based modeling;*
   - *Remote sensing applications in health.*
   - *Spatial Health Surveillance;*
   - *Spatial survival and registry data analysis;*
   - *Clustering of temporal trends in space-time disease mapping;*
   - *Modifiable areal unit issues and methods;*
   - *Social networks and spatial epidemiology: tools, opportunities and
   challenges;*
   - *Modelling climate-sensitive disease;*
   - *Modelling and inference in infectious disease epidemiology;*
   - *The use of linked data in spatial epidemiology;*
   - *Joint Modelling with Spatial Variation;*
   - *Health Applications of the Google Earth Engine;*
   - *Challenges and Advances in Spatio-Temporal Disease Modelling;*
   - *GIS in Public Health;*

SUBMIT YOUR ABSTRACT NOW
<http://i3s.us15.list-manage1.com/track/click?u=f4ce56751813aefc6eafc8a70&id=6ce2defaec&e=7228bb7854>
<http://i3s.us15.list-manage.com/track/click?u=f4ce56751813aefc6eafc8a70&id=59b26307e7&e=7228bb7854>

Porto is Portugal's second largest city and the capital of the Northern
region.
Icons such as "Serralves" or "Casa da M?sica" and a regular agenda of
music, art and sports events stand witness to a urban space, in constant
movement.
NEED ACCOMMODATION?
<http://i3s.us15.list-manage1.com/track/click?u=f4ce56751813aefc6eafc8a70&id=d47f629673&e=7228bb7854>
VIEW OUR WEBSTIE
<http://i3s.us15.list-manage.com/track/click?u=f4ce56751813aefc6eafc8a70&id=dd11baa0d1&e=7228bb7854>





This email was sent to carlaoliveir at gmail.com
*why did I get this?*
<http://i3s.us15.list-manage1.com/about?u=f4ce56751813aefc6eafc8a70&id=71cbd92552&e=7228bb7854&c=478bb4df52>
    unsubscribe from this list
<http://i3s.us15.list-manage1.com/unsubscribe?u=f4ce56751813aefc6eafc8a70&id=71cbd92552&e=7228bb7854&c=478bb4df52>
    update subscription preferences
<http://i3s.us15.list-manage2.com/profile?u=f4ce56751813aefc6eafc8a70&id=71cbd92552&e=7228bb7854>
Geomed 2017 ? RUA ALFREDO ALLEN, 208 ? Porto 4200-135 ? Portugal

[image: Email Marketing Powered by MailChimp]
<http://www.mailchimp.com/monkey-rewards/?utm_source=freemium_newsletter&utm_medium=email&utm_campaign=monkey_rewards&aid=f4ce56751813aefc6eafc8a70&afl=1>



-- 

*Carla Oliveira*


*Associate Research Statistician*

Blueclinical ? Investiga??o e Desenvolvimento em Sa?de, Ltd.

coliveira at blueclinical.pt <clopes at blueclinical.pt> / www.blueclinical.pt



*Invited Assistant Professor *

ESTSP ? Escola Superior Tecnol?gica da Sa?de do Porto, IPP

Departamento de Biomatem?tica, Bioestat?stica e Bioinform?tica

*cto at estsp.ipp.pt <cto at estsp.ipp.pt>* / *www.estsp.ipp.pt
<http://www.estsp.ipp.pt/>*



*Researcher*

i3S - Instituto de Investiga??o e Inova??o em Sa?de, UP

INEB ? Instituto de Engenharia Biom?dica, UP

*coliveir at ineb.up.pt <coliveir at ineb.up.pt>* / *www.i3s.up.pt
<http://www.i3s.up.pt/>* / *www.ineb.up.pt <http://www.ineb.up.pt/>*

	[[alternative HTML version deleted]]


From ankur.sum13 at yahoo.com  Sun Apr 23 06:02:10 2017
From: ankur.sum13 at yahoo.com (Ankur Sarker)
Date: Sun, 23 Apr 2017 04:02:10 +0000 (UTC)
Subject: [R-sig-Geo] Help for STFDF Object Creation
References: <505334731.7947457.1492920130921.ref@mail.yahoo.com>
Message-ID: <505334731.7947457.1492920130921@mail.yahoo.com>

Hi,
I am trying to create a STFDF object and draw variogram. However, I am getting several errors. Here is my r code:
#create count data
c0157ts <- as.numeric(c(t(c0157)))
c0001ts <- as.numeric(c(t(c0001)))

#create coordinates
  lat <- 0
  lon <- 0
  lat[1] <- 34.0793684
  lon[1] <- -81.1301722
  lat[2] <- 33.979127
  lon[2] <- -81.321166398
  sp <- data.frame(lat,lon)
  coordinates(sp)=~lon+lat
  projection(sp)=CRS("+init=epsg:4326")

#create time object
time_index <- seq(from = as.POSIXct("2017-01-02 01:00", tz = 'UTC'), to = as.POSIXct("2017-03-22 00:00", tz = 'UTC'), by = "hour")

#combine the data
mydata <- data.frame("count"=c(c0157ts,c0001ts)) 

#create STFDF object
stfdf = STFDF(ozoneSP, time_index, mydata)
#trying to create variogram
    var <- variogramST(count~1,data=stfdf,tunit="hours",assumeRegular=F,na.omit=T)which gives me following error:0%Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  : 
      dim(X) must have a positive length
I guess my STFDF object creation is not correct. Can anyone give us any hint? Which elements in STFDF object is not correct.Here is the description of STFDF object:> str(stfdf)
Formal class 'STFDF' [package "spacetime"] with 4 slots
  ..@ data   :'data.frame':     3792 obs. of  1 variable:
  .. ..$ count: num [1:3792] 40 32 64 40 55 89 71 43 69 44 ...
  ..@ sp     :Formal class 'SpatialPoints' [package "sp"] with 3 slots
  .. .. ..@ coords     : num [1:2, 1:2] -9031369 -9052631 4015522 4002120
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : NULL
  .. .. .. .. ..$ : chr [1:2] "lon" "lat"
  .. .. ..@ bbox       : num [1:2, 1:2] -9052631 4002120 -9031369 4015522
  .. .. .. ..- attr(*, "dimnames")=List of 2
  .. .. .. .. ..$ : chr [1:2] "lon" "lat"
  .. .. .. .. ..$ : chr [1:2] "min" "max"
  .. .. ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
  .. .. .. .. ..@ projargs: chr "+init=epsg:3395 +proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
  ..@ time   :An ?xts? object on 2017-01-02 01:00:00/2017-03-22 containing:
  Data: int [1:1896, 1] 1 2 3 4 5 6 7 8 9 10 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr "timeIndex"
  Indexed by objects of class: [POSIXct,POSIXt] TZ: UTC
  xts Attributes:  
 NULL
Thanks for your help!
With Regards,Rukna
	[[alternative HTML version deleted]]


From Brenovic8 at hotmail.com  Sun Apr 23 22:35:58 2017
From: Brenovic8 at hotmail.com (Breno Oliveira)
Date: Sun, 23 Apr 2017 20:35:58 +0000
Subject: [R-sig-Geo] Pycnophylactic Interpolation
Message-ID: <BL2PR20MB0770AC257FED85E884DE0715FF1C0@BL2PR20MB0770.namprd20.prod.outlook.com>

Hi,

I'm a student of Information Systems in Brazil and a junior software archtect. To my final work on the undergraduate program I'm implementing a software with the Potential model. After calculate the potential values to each point of analysis I would like to be able to create a map to represent the result. The map should looks like this one: https://www.r-bloggers.com/contour-and-density-layers-with-ggmap/. I'm new in R so I have no a big knowhow and a lot of doubts. I would like ask for community's help. My problem is that I have no to many points like the example so the map looks incorrect, actualy I have a few points (x,y and z). I think I must Interpolate these points to get a good result, I would like to use the Pycnophylactic Interpolation avaliable in pycno package.

Here is a example of data I obtain (no interpolation applied):


City    Area    Score   latitudes       longitudes      Potentials
Belo Horizone   330,95  2513451 -19,91663263    -43,93337198    164874,1295
Betim   346     412003  -19,96730327    -44,20119724    82629,32514
Contagem        195,268 648766  -19,91615326    -44,08087722    140851,9626
Sarzedo 61,892  25798   -20,06250329    -44,11504723    79307,30623
Rio Acima       50      90      -20,08830329    -43,79104717    55226,91779


Please let me know if you need any extra information.
Thank you for your patience and cooperation.
Breno Oliveira


	[[alternative HTML version deleted]]


From ankur.sum13 at yahoo.com  Sun Apr 23 23:00:44 2017
From: ankur.sum13 at yahoo.com (Ankur Sarker)
Date: Sun, 23 Apr 2017 21:00:44 +0000 (UTC)
Subject: [R-sig-Geo] STFDF Object Creation and Variogram
References: <174974993.8355909.1492981244159.ref@mail.yahoo.com>
Message-ID: <174974993.8355909.1492981244159@mail.yahoo.com>

Hi,I am trying to create a STFDF object and draw variogram. However, I am getting several errors.?
Here is my r code:
? ? #create count data? ? c0157ts <- as.numeric(c(t(c0157)))? ? c0001ts <- as.numeric(c(t(c0001)))
? ? #create coordinates? ? ? ?lat <- 0? ? ? lon <- 0? ? ? lat[1] <- 34.0793684? ? ? lon[1] <- -81.1301722? ? ? lat[2] <- 33.979127? ? ? lon[2] <- -81.321166398? ? ? sp <- data.frame(lat,lon)? ? ? coordinates(sp)=~lon+lat? ? ? projection(sp)=CRS("+init=epsg:4326")
? ? #create time object? ? ? ?time_index <- seq(from = as.POSIXct("2017-01-02 01:00", tz = 'UTC'), to = as.POSIXct("2017-03-22 00:00", tz = 'UTC'), by = "hour")
? ? #combine the data? ? ? ?mydata <- data.frame("count"=c(c0157ts,c0001ts))?
? ? #create STFDF object? ? stfdf = STFDF(ozoneSP, time_index, mydata)
? ? #trying to create variogram? ? var <- variogramST(count~1,data=stfdf,tunit="hours",assumeRegular=F,na.omit=T)

which gives me following error:? ? Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum, ?: dim(X) must have a positive length

I guess my STFDF object creation is not correct. Can anyone give me any hint? Which elements in STFDF object is not correct. Is it because of my time object?

Here is the description of STFDF object:str(stfdf)
Output:
? ? Formal class 'STFDF' [package "spacetime"] with 4 slots? ? ? ..@ data ?:'data.frame': ? ?3792 obs. of ?1 variable:? ? ? .. ..$ count: num [1:3792] 40 32 64 40 55 89 71 43 69 44 ...? ? ? ..@ sp ? ?:Formal class 'SpatialPoints' [package "sp"] with 3 slots? ? ? .. .. ..@ coords ? ?: num [1:2, 1:2] -9031369 -9052631 4015522 4002120? ? ? .. .. .. ..- attr(*, "dimnames")=List of 2? ? ? .. .. .. .. ..$ : NULL? ? ? .. .. .. .. ..$ : chr [1:2] "lon" "lat"? ? ? .. .. ..@ bbox ? ? ?: num [1:2, 1:2] -9052631 4002120 -9031369 4015522? ? ? .. .. .. ..- attr(*, "dimnames")=List of 2? ? ? .. .. .. .. ..$ : chr [1:2] "lon" "lat"? ? ? .. .. .. .. ..$ : chr [1:2] "min" "max"? ? ? .. .. ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot? ? ? .. .. .. .. ..@ projargs: chr "+init=epsg:3395 +proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"? ? ? ..@ time ?:An ?xts? object on 2017-01-02 01:00:00/2017-03-22 containing:? ? ? Data: int [1:1896, 1] 1 2 3 4 5 6 7 8 9 10 ...? ? - attr(*, "dimnames")=List of 2? ? ? ..$ : NULL? ? ? ..$ : chr "timeIndex"? ? ? Indexed by objects of class: [POSIXct,POSIXt] TZ: UTC? ? ? xts Attributes: ?? ? NULL


Here, I have attached the link of Rdat for your better understanding.?Thanks for your help!
STFDF.RData

  
|  
|    |  
STFDF.RData
   |  |

  |

 

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Sun Apr 23 23:24:59 2017
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Sun, 23 Apr 2017 23:24:59 +0200
Subject: [R-sig-Geo] Help for STFDF Object Creation
In-Reply-To: <505334731.7947457.1492920130921@mail.yahoo.com>
References: <505334731.7947457.1492920130921.ref@mail.yahoo.com>
 <505334731.7947457.1492920130921@mail.yahoo.com>
Message-ID: <5d78be19-6059-c3c3-6d53-c2f0ad49717e@uni-muenster.de>

Please provide a script of an example that we can reproduce, preferably
using data that is loaded from a package (to safe other people's time),
otherwise it is impossible for anyone to help you, or unlikely somebody
will. Your script does not create count data but assumes it is there;
you don't show which package you loaded; it is unclear what your problem is.

Your second post on this list came through completely scrambled, and
within 24 hours. Don't expect that reposting, or scrambling, helps.

On 23/04/17 06:02, Ankur Sarker via R-sig-Geo wrote:
> Hi,
> I am trying to create a STFDF object and draw variogram. However, I am getting several errors. Here is my r code:
> #create count data
> c0157ts <- as.numeric(c(t(c0157)))
> c0001ts <- as.numeric(c(t(c0001)))
> 
> #create coordinates
>   lat <- 0
>   lon <- 0
>   lat[1] <- 34.0793684
>   lon[1] <- -81.1301722
>   lat[2] <- 33.979127
>   lon[2] <- -81.321166398
>   sp <- data.frame(lat,lon)
>   coordinates(sp)=~lon+lat
>   projection(sp)=CRS("+init=epsg:4326")
> 
> #create time object
> time_index <- seq(from = as.POSIXct("2017-01-02 01:00", tz = 'UTC'), to = as.POSIXct("2017-03-22 00:00", tz = 'UTC'), by = "hour")
> 
> #combine the data
> mydata <- data.frame("count"=c(c0157ts,c0001ts)) 
> 
> #create STFDF object
> stfdf = STFDF(ozoneSP, time_index, mydata)
> #trying to create variogram
>     var <- variogramST(count~1,data=stfdf,tunit="hours",assumeRegular=F,na.omit=T)which gives me following error:0%Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  : 
>       dim(X) must have a positive length
> I guess my STFDF object creation is not correct. Can anyone give us any hint? Which elements in STFDF object is not correct.Here is the description of STFDF object:> str(stfdf)
> Formal class 'STFDF' [package "spacetime"] with 4 slots
>   ..@ data   :'data.frame':     3792 obs. of  1 variable:
>   .. ..$ count: num [1:3792] 40 32 64 40 55 89 71 43 69 44 ...
>   ..@ sp     :Formal class 'SpatialPoints' [package "sp"] with 3 slots
>   .. .. ..@ coords     : num [1:2, 1:2] -9031369 -9052631 4015522 4002120
>   .. .. .. ..- attr(*, "dimnames")=List of 2
>   .. .. .. .. ..$ : NULL
>   .. .. .. .. ..$ : chr [1:2] "lon" "lat"
>   .. .. ..@ bbox       : num [1:2, 1:2] -9052631 4002120 -9031369 4015522
>   .. .. .. ..- attr(*, "dimnames")=List of 2
>   .. .. .. .. ..$ : chr [1:2] "lon" "lat"
>   .. .. .. .. ..$ : chr [1:2] "min" "max"
>   .. .. ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
>   .. .. .. .. ..@ projargs: chr "+init=epsg:3395 +proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
>   ..@ time   :An ?xts? object on 2017-01-02 01:00:00/2017-03-22 containing:
>   Data: int [1:1896, 1] 1 2 3 4 5 6 7 8 9 10 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : NULL
>   ..$ : chr "timeIndex"
>   Indexed by objects of class: [POSIXct,POSIXt] TZ: UTC
>   xts Attributes:  
>  NULL
> Thanks for your help!
> With Regards,Rukna
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170423/6762ae88/attachment.sig>

From joelarson452452 at gmail.com  Mon Apr 24 00:19:59 2017
From: joelarson452452 at gmail.com (Joe Larson)
Date: Sun, 23 Apr 2017 18:19:59 -0400
Subject: [R-sig-Geo] R-help - Shiny and Datatable
Message-ID: <CAHnLHfs_OY0KTSHySjk6Kh+=KrVQt156+fztt2Lu5F+Jn_wiWg@mail.gmail.com>

Hello,
 I am getting wrapped around the axial on a shiny project, being a new R
learner I am asking for some help. I am trying to combine CDC data, Shiny,
and leaflet together to produce a map that has the top 500 USA cities with
adverse medical outcomes.  The goal is to allow the user to select either
the range of outcome, state or adverse outcome.  Through Shiny examples, I
am able to get the range function to work, but not the "state" or "health
outcomes" to function correctly. My issue is in using input type selectable
 "selectInput("state", "State Abbr", data_c$state, selected = NULL,
multiple = FALSE, selectize = TRUE, width = NULL, size = NULL), "  The
challenge I am facing is selecting the correct "Render reactive output"
 and how to mutate the datatable so the users inputs are correctly
displayed.  If you want the full code, to let me know.
I have been reading shiny.rstudios.com as well the shiny-cheatsheet.pdf
with the solution not coming to mind.
Thanks for any suggestion you can offer.

The datatable has this format (first 2 cities and allThe isisue is I can
not find out which type of 13 outcomens):
X year state cityname crudeprevalence populationcount measureid lat long
healthoutcome metropop
1 2014 AL Birmingham 32.6 212237 ARTHRITIS 33.52756638 -86.79881747
0.0153 Birmingham(city),
212237(popluation), 0.0153(Arthritis %)
2 2013 AL Birmingham 45.9 212237 BPHIGH 33.52756638 -86.79881747
0.0216 Birmingham(city),
212237(popluation), 0.0216(High blood pressure %)
3 2014 AL Birmingham 6.1 212237 CANCER 33.52756638 -86.79881747 0.0028
Birmingham(city),
212237(popluation), 0.0028(Cancer (excluding skin cancer)  %)
4 2014 AL Birmingham 11.4 212237 CASTHMA 33.52756638 -86.79881747
0.0053 Birmingham(city),
212237(popluation), 0.0053(Asthma %)
5 2014 AL Birmingham 7.6 212237 CHD 33.52756638 -86.79881747 0.0035
Birmingham(city),
212237(popluation), 0.0035(Coronary heart disease %)
6 2014 AL Birmingham 9.4 212237 COPD 33.52756638 -86.79881747 0.0044
Birmingham(city),
212237(popluation), 0.0044(Chronic obstructive pulmonary disease %)
7 2014 AL Birmingham 16.1 212237 DIABETES 33.52756638 -86.79881747
0.0075 Birmingham(city),
212237(popluation), 0.0075(Diabetes %)
8 2013 AL Birmingham 35.4 212237 HIGHCHOL 33.52756638 -86.79881747
0.0166 Birmingham(city),
212237(popluation), 0.0166(High cholesterol %)
9 2014 AL Birmingham 3.3 212237 KIDNEY 33.52756638 -86.79881747 0.0015
Birmingham(city),
212237(popluation), 0.0015(Kidney disease %)
10 2014 AL Birmingham 17 212237 MHLTH 33.52756638 -86.79881747 0.008
Birmingham(city),
212237(popluation), 0.008(Mental health %)
11 2014 AL Birmingham 18.3 212237 PHLTH 33.52756638 -86.79881747
0.0086 Birmingham(city),
212237(popluation), 0.0086(Physical health %)
12 2014 AL Birmingham 5 212237 STROKE 33.52756638 -86.79881747 0.0023
Birmingham(city),
212237(popluation), 0.0023(Stroke %)
13 2014 AL Birmingham 25.9 212237 TEETHLOST 33.52756638 -86.79881747
0.0122 Birmingham(city),
212237(popluation), 0.0122(All teeth lost %)
14 2014 AL Hoover 25.3 81619 ARTHRITIS 33.37676027 -86.80519376 0.0309
Hoover(city),
81619(popluation), 0.0309(Arthritis %)
Thanks for you help and insight in advance
Joe

	[[alternative HTML version deleted]]


From ankur.sum13 at yahoo.com  Mon Apr 24 01:20:27 2017
From: ankur.sum13 at yahoo.com (Ankur Sarker)
Date: Sun, 23 Apr 2017 23:20:27 +0000 (UTC)
Subject: [R-sig-Geo] Help for STFDF Object Creation
In-Reply-To: <5d78be19-6059-c3c3-6d53-c2f0ad49717e@uni-muenster.de>
References: <505334731.7947457.1492920130921.ref@mail.yahoo.com>
 <505334731.7947457.1492920130921@mail.yahoo.com>
 <5d78be19-6059-c3c3-6d53-c2f0ad49717e@uni-muenster.de>
Message-ID: <419403032.8378550.1492989627208@mail.yahoo.com>

Thanks for your reply. Sorry for my second post, I tried to be more clear.

Here is link of my complete source code, data and R instance:https://drive.google.com/drive/folders/0B3tdiSZ9hBj4dVZXOXItUjdrRlE?usp=sharing


Hope, it would be easier to understand my problem. 

    On Sunday, April 23, 2017 5:25 PM, Edzer Pebesma <edzer.pebesma at uni-muenster.de> wrote:
 

 Please provide a script of an example that we can reproduce, preferably
using data that is loaded from a package (to safe other people's time),
otherwise it is impossible for anyone to help you, or unlikely somebody
will. Your script does not create count data but assumes it is there;
you don't show which package you loaded; it is unclear what your problem is.

Your second post on this list came through completely scrambled, and
within 24 hours. Don't expect that reposting, or scrambling, helps.

On 23/04/17 06:02, Ankur Sarker via R-sig-Geo wrote:
> Hi,
> I am trying to create a STFDF object and draw variogram. However, I am getting several errors. Here is my r code:
> #create count data
> c0157ts <- as.numeric(c(t(c0157)))
> c0001ts <- as.numeric(c(t(c0001)))
> 
> #create coordinates
>? lat <- 0
>? lon <- 0
>? lat[1] <- 34.0793684
>? lon[1] <- -81.1301722
>? lat[2] <- 33.979127
>? lon[2] <- -81.321166398
>? sp <- data.frame(lat,lon)
>? coordinates(sp)=~lon+lat
>? projection(sp)=CRS("+init=epsg:4326")
> 
> #create time object
> time_index <- seq(from = as.POSIXct("2017-01-02 01:00", tz = 'UTC'), to = as.POSIXct("2017-03-22 00:00", tz = 'UTC'), by = "hour")
> 
> #combine the data
> mydata <- data.frame("count"=c(c0157ts,c0001ts)) 
> 
> #create STFDF object
> stfdf = STFDF(ozoneSP, time_index, mydata)
> #trying to create variogram
>? ? var <- variogramST(count~1,data=stfdf,tunit="hours",assumeRegular=F,na.omit=T)which gives me following error:0%Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,? : 
>? ? ? dim(X) must have a positive length
> I guess my STFDF object creation is not correct. Can anyone give us any hint? Which elements in STFDF object is not correct.Here is the description of STFDF object:> str(stfdf)
> Formal class 'STFDF' [package "spacetime"] with 4 slots
>? ..@ data? :'data.frame':? ? 3792 obs. of? 1 variable:
>? .. ..$ count: num [1:3792] 40 32 64 40 55 89 71 43 69 44 ...
>? ..@ sp? ? :Formal class 'SpatialPoints' [package "sp"] with 3 slots
>? .. .. ..@ coords? ? : num [1:2, 1:2] -9031369 -9052631 4015522 4002120
>? .. .. .. ..- attr(*, "dimnames")=List of 2
>? .. .. .. .. ..$ : NULL
>? .. .. .. .. ..$ : chr [1:2] "lon" "lat"
>? .. .. ..@ bbox? ? ? : num [1:2, 1:2] -9052631 4002120 -9031369 4015522
>? .. .. .. ..- attr(*, "dimnames")=List of 2
>? .. .. .. .. ..$ : chr [1:2] "lon" "lat"
>? .. .. .. .. ..$ : chr [1:2] "min" "max"
>? .. .. ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slot
>? .. .. .. .. ..@ projargs: chr "+init=epsg:3395 +proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
>? ..@ time? :An ?xts? object on 2017-01-02 01:00:00/2017-03-22 containing:
>? Data: int [1:1896, 1] 1 2 3 4 5 6 7 8 9 10 ...
>? - attr(*, "dimnames")=List of 2
>? ..$ : NULL
>? ..$ : chr "timeIndex"
>? Indexed by objects of class: [POSIXct,POSIXt] TZ: UTC
>? xts Attributes:? 
>? NULL
> Thanks for your help!
> With Regards,Rukna
> ??? [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics? (ifgi),? University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:? http://www.jstatsoft.org/
Computers & Geosciences:? http://elsevier.com/locate/cageo/
_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

   
	[[alternative HTML version deleted]]


From englishchristophera at gmail.com  Mon Apr 24 02:16:52 2017
From: englishchristophera at gmail.com (chris english)
Date: Sun, 23 Apr 2017 20:16:52 -0400
Subject: [R-sig-Geo] R-help - Shiny and Datatable
In-Reply-To: <CAHnLHfs_OY0KTSHySjk6Kh+=KrVQt156+fztt2Lu5F+Jn_wiWg@mail.gmail.com>
References: <CAHnLHfs_OY0KTSHySjk6Kh+=KrVQt156+fztt2Lu5F+Jn_wiWg@mail.gmail.com>
Message-ID: <CAASFQpRRYMWtgy2seoHcWVzg-C6QY6DK-_ss9N65iM_xjs0cAA@mail.gmail.com>

Joe,

If you look again at the CDC, USA 500 ,(
https://chronicdata.cdc.gov/500-Cities/500-Cities-Local-Data-for-Better-Health/6vp6-wxuq
),
the top 26 rows are USA crude prevalence  then age adjusted prevalence by
disease (for 2014), or the 13 (either crude prevalence, or age adjusted)
that you are looking for.

Judging by what you were describing above, it doesn't sound like you're
doing a 'things got better or things got worse,
year on year', so perhaps filtering the data to a given year (2014) and
given  health outcome (age adjusted) prior to
downloading might simplify things.
HTH
Chris

On Sun, Apr 23, 2017 at 6:19 PM, Joe Larson <joelarson452452 at gmail.com>
wrote:

> Hello,
>  I am getting wrapped around the axial on a shiny project, being a new R
> learner I am asking for some help. I am trying to combine CDC data, Shiny,
> and leaflet together to produce a map that has the top 500 USA cities with
> adverse medical outcomes.  The goal is to allow the user to select either
> the range of outcome, state or adverse outcome.  Through Shiny examples, I
> am able to get the range function to work, but not the "state" or "health
> outcomes" to function correctly. My issue is in using input type selectable
>  "selectInput("state", "State Abbr", data_c$state, selected = NULL,
> multiple = FALSE, selectize = TRUE, width = NULL, size = NULL), "  The
> challenge I am facing is selecting the correct "Render reactive output"
>  and how to mutate the datatable so the users inputs are correctly
> displayed.  If you want the full code, to let me know.
> I have been reading shiny.rstudios.com as well the shiny-cheatsheet.pdf
> with the solution not coming to mind.
> Thanks for any suggestion you can offer.
>
> The datatable has this format (first 2 cities and allThe isisue is I can
> not find out which type of 13 outcomens):
> X year state cityname crudeprevalence populationcount measureid lat long
> healthoutcome metropop
> 1 2014 AL Birmingham 32.6 212237 ARTHRITIS 33.52756638 -86.79881747
> 0.0153 Birmingham(city),
> 212237(popluation), 0.0153(Arthritis %)
> 2 2013 AL Birmingham 45.9 212237 BPHIGH 33.52756638 -86.79881747
> 0.0216 Birmingham(city),
> 212237(popluation), 0.0216(High blood pressure %)
> 3 2014 AL Birmingham 6.1 212237 CANCER 33.52756638 -86.79881747 0.0028
> Birmingham(city),
> 212237(popluation), 0.0028(Cancer (excluding skin cancer)  %)
> 4 2014 AL Birmingham 11.4 212237 CASTHMA 33.52756638 -86.79881747
> 0.0053 Birmingham(city),
> 212237(popluation), 0.0053(Asthma %)
> 5 2014 AL Birmingham 7.6 212237 CHD 33.52756638 -86.79881747 0.0035
> Birmingham(city),
> 212237(popluation), 0.0035(Coronary heart disease %)
> 6 2014 AL Birmingham 9.4 212237 COPD 33.52756638 -86.79881747 0.0044
> Birmingham(city),
> 212237(popluation), 0.0044(Chronic obstructive pulmonary disease %)
> 7 2014 AL Birmingham 16.1 212237 DIABETES 33.52756638 -86.79881747
> 0.0075 Birmingham(city),
> 212237(popluation), 0.0075(Diabetes %)
> 8 2013 AL Birmingham 35.4 212237 HIGHCHOL 33.52756638 -86.79881747
> 0.0166 Birmingham(city),
> 212237(popluation), 0.0166(High cholesterol %)
> 9 2014 AL Birmingham 3.3 212237 KIDNEY 33.52756638 -86.79881747 0.0015
> Birmingham(city),
> 212237(popluation), 0.0015(Kidney disease %)
> 10 2014 AL Birmingham 17 212237 MHLTH 33.52756638 -86.79881747 0.008
> Birmingham(city),
> 212237(popluation), 0.008(Mental health %)
> 11 2014 AL Birmingham 18.3 212237 PHLTH 33.52756638 -86.79881747
> 0.0086 Birmingham(city),
> 212237(popluation), 0.0086(Physical health %)
> 12 2014 AL Birmingham 5 212237 STROKE 33.52756638 -86.79881747 0.0023
> Birmingham(city),
> 212237(popluation), 0.0023(Stroke %)
> 13 2014 AL Birmingham 25.9 212237 TEETHLOST 33.52756638 -86.79881747
> 0.0122 Birmingham(city),
> 212237(popluation), 0.0122(All teeth lost %)
> 14 2014 AL Hoover 25.3 81619 ARTHRITIS 33.37676027 -86.80519376 0.0309
> Hoover(city),
> 81619(popluation), 0.0309(Arthritis %)
> Thanks for you help and insight in advance
> Joe
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From kent3737 at gmail.com  Mon Apr 24 16:01:34 2017
From: kent3737 at gmail.com (Kent Johnson)
Date: Mon, 24 Apr 2017 10:01:34 -0400
Subject: [R-sig-Geo] R-help - Shiny and Datatable
Message-ID: <CAPP0wygJXeCCLcoZfiifhFS545Kcqg47ajkbY2SdA6suCY5GWw@mail.gmail.com>

On Mon, Apr 24, 2017 at 6:00 AM, <r-sig-geo-request at r-project.org> wrote:
>
> Message: 4
> Date: Sun, 23 Apr 2017 18:19:59 -0400
> From: Joe Larson <joelarson452452 at gmail.com>
>
> Hello,
>  I am getting wrapped around the axial on a shiny project, being a new R
> learner I am asking for some help.... If you want the full code, to let me
> know.


Yes, if you can post your code and data somewhere that would make it easier
to help.

Kent

	[[alternative HTML version deleted]]


From englishchristophera at gmail.com  Mon Apr 24 16:44:42 2017
From: englishchristophera at gmail.com (chris english)
Date: Mon, 24 Apr 2017 10:44:42 -0400
Subject: [R-sig-Geo] Pycnophylactic Interpolation
In-Reply-To: <BL2PR20MB0770AC257FED85E884DE0715FF1C0@BL2PR20MB0770.namprd20.prod.outlook.com>
References: <BL2PR20MB0770AC257FED85E884DE0715FF1C0@BL2PR20MB0770.namprd20.prod.outlook.com>
Message-ID: <CAASFQpRSmadigPjbDd1CuBuqGzjWwDx2SwmcF2Ok_gG1oFToow@mail.gmail.com>

Breno,

Just an article comparing pycnophylatic interpolation to area-to-point
kriging
https://www.researchgate.net/publication/227727493_Reconstructing_Population_Density_Surfaces_from_Areal_Data_A_Comparison_of_Tobler's_Pycnophylactic_Interpolation_Method_and_Area-to-Point_Kriging
Chris

On Sun, Apr 23, 2017 at 4:35 PM, Breno Oliveira <Brenovic8 at hotmail.com>
wrote:

> Hi,
>
> I'm a student of Information Systems in Brazil and a junior software
> archtect. To my final work on the undergraduate program I'm implementing a
> software with the Potential model. After calculate the potential values to
> each point of analysis I would like to be able to create a map to represent
> the result. The map should looks like this one:
> https://www.r-bloggers.com/contour-and-density-layers-with-ggmap/. I'm
> new in R so I have no a big knowhow and a lot of doubts. I would like ask
> for community's help. My problem is that I have no to many points like the
> example so the map looks incorrect, actualy I have a few points (x,y and
> z). I think I must Interpolate these points to get a good result, I would
> like to use the Pycnophylactic Interpolation avaliable in pycno package.
>
> Here is a example of data I obtain (no interpolation applied):
>
>
> City    Area    Score   latitudes       longitudes      Potentials
> Belo Horizone   330,95  2513451 -19,91663263    -43,93337198    164874,1295
> Betim   346     412003  -19,96730327    -44,20119724    82629,32514
> Contagem        195,268 648766  -19,91615326    -44,08087722    140851,9626
> Sarzedo 61,892  25798   -20,06250329    -44,11504723    79307,30623
> Rio Acima       50      90      -20,08830329    -43,79104717    55226,91779
>
>
> Please let me know if you need any extra information.
> Thank you for your patience and cooperation.
> Breno Oliveira
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From englishchristophera at gmail.com  Tue Apr 25 02:19:21 2017
From: englishchristophera at gmail.com (chris english)
Date: Mon, 24 Apr 2017 20:19:21 -0400
Subject: [R-sig-Geo] alpha hulls (always complete) and incomplete envelops
	(confidence intervals)
Message-ID: <CAASFQpSyeF_MFFd_Tqkhd703i22fXRbV=43QsmH19=5xLM7kPA@mail.gmail.com>

I have a series of noodles, which is to say line shapes, that I am trying
to come to grips.

If I generalize these lines away to point clouds in time order, then an
alpha hull will contain all points, as would convex hull, but potentially
with less wasted space. The thing about a convex hull or alpha hull is that
all samples are 'within'.  This is to say that it is converged as sample =
infinity, or, more normally, sample = total number of samples available
which is generally less than infinity.

In a very general way, I am wondering if there is an accepted method to
allow, say 30 percent of the 'alpha hulled' samples (which clearly are not
designed to allow such) to reside outside the alpha hull (essentially
creating confidence intervals upon the alpha hull (or perhaps I haven't
read enough)) . Secondarily, is there a way to compare 'confidence
interval' alpha hulls, where 70 percent of the sample points reside within,
and the rest exogenous.

I ask as my noodles may share, to an unknown amount, but perhaps
extensively, commonalities of co-existance to as much as 70 percent (it is
speculated) of an alpha hull space, and the variance that I am trying to
account for is the 30 percent that I would guess and perhaps to define as a
separate alpha hull, or some sort space, outside the alpha hull space.

Having it both ways: *if *100% of points are within the alpha hull, how
might one reduce this to 70% or some such, because hulls are always sample
complete.

I realize that this probably sounds inchoate, or am I fantabulizing, but I
think I am asking about comparing the shapes of constrained (incomplete)
alpha hulls in the context of Parzen windows (whose shape I wonder about,
boxes?).

Another way of looking at this a point process is that given a 1280x1280
grid, there are an enormous number of cells that will always be NA, a
smaller number that will be visited once. So how to proceed to compare the
noodles.

While I sense I will be killed on this question: Any thoughts or suggested
reading appreciated so I might address more intelligently anon.
Chris

	[[alternative HTML version deleted]]


From rameshv at caryinstitute.org  Tue Apr 25 01:02:55 2017
From: rameshv at caryinstitute.org (Vijay Ramesh)
Date: Mon, 24 Apr 2017 19:02:55 -0400
Subject: [R-sig-Geo] Error in GDAL for library MODIS
Message-ID: <CAOGv6M-nCXsMGuJzCEhS3Csf5R-BxxZbT9c+9BfLSJVOJzdtNw@mail.gmail.com>

I keep getting a GDAL error when I try to use the MODIS library to download
and process MODIS data.

PROCESSING:
_______________
GDAL           : Not available. Use 'MODIS:::checkTools('GDAL')' for more
information!

Note: I have rgdal installed, along with GDAL separately installed from
osgeo4w.

Any suggestions?


-- 
Data Manager,
Barbara Han Lab,
Cary Institute of Ecosystem Studies,
2801 Sharon Turnpike, Millbrook, NY 12545
Lab Site : http://www.hanlab.science/
Personal Site : http://evolecol.weebly.com/
Phone : (845)-677-7600 Ext: 241

	[[alternative HTML version deleted]]


From orion at ofb.net  Fri Apr 28 10:09:11 2017
From: orion at ofb.net (Anne C. Hanna)
Date: Fri, 28 Apr 2017 04:09:11 -0400
Subject: [R-sig-Geo] sp::disaggregate() does not seem to add the necessary
 "comment" attribute to identify holes if Polygons does not contain exactly
 1 polygon with 0 or 1 holes
Message-ID: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>

Hello.  I first posted this issue report on the sp GitHub repo
(https://github.com/edzer/sp/issues/27) and it was suggested that I redirect
it to here.

I am working with a geographic dataset with complex borders. The data is
stored as a SpatialPolygonsDataFrame.  Each Polygons object in the data frame
may be composed of multiple disjoint polygons, and each polygon may have
multiple holes.  I want to disaggregate each of the Polygons objects into its
individual disjoint polygons and construct an adjacency matrix for all the
disjoint components, and I was using disaggregate() to do this.  However, when
I run gTouches() on the disaggregated data, in order to compute the
adjacencies, I get a number of warnings like this:

Error in RGEOSBinPredFunc(spgeom1, spgeom2, byid, "rgeos_touches") : Polygons
object missing comment attribute ignoring hole(s).  See function createSPComment.

Looking at the Polygons "comment" attributes in the SpatialPolygonsDataFrame
output by disaggregate(), I see that the only comment values are "0"
(indicating a single polygon with no holes), "0 1" (indicating a single
polygon with a single hole), and NULL (apparently no comment was written).
Since I know my dataset contains several Polygons objects which are composed
of multiple disjoint regions, and also several Polygons which contain more
than one hole, this is not the expected result.  In reading the disaggregate()
code in the sp GitHub repository (specifically, explodePolygons()), I also
can't see anywhere the comment is being added for the cases where a Polygons
object has more than two parts or more than two holes.  It actually seems like
it's getting carried along almost accidentally in the few cases that do get
comments, and neglected otherwise.

Assuming I'm not failing to understand the code and the desired behavior
(entirely possible, as I am new at working with this software!), this seems
suboptimal to me.  My dataset is pretty well-behaved (despite its complexity),
so I should be able to fix my issues with judicious application of
createPolygonsComment.  But I had a heck of a time figuring out what was going
wrong with gTouches, since Polygons comment management appears to be a pretty
obscure field (and createSPComment wasn't working for me, for whatever
reason).  So it seems like it might be better if disaggregate() just parses
and passes along the comments from its input correctly, or, if it's absolutely
necessary to not create comments, passes nothing and warns clearly in the
manual that comments and associated hole information are being lost.  Passing
along comments in some cases while silently dropping them in others seems like
kind of the worst of both worlds.

I've attached a set of tests I wrote to demonstrate desired/undesired
behavior: disaggregate_comment_tests.R.  My R version is 3.4.0, my sp version
is 1.2-4, my rgeos version is 0.3-23 (SVN revision 546), and my GEOS runtime
version is 3.5.1-CAPI-1.9.1 r4246.  I am using Debian Release 9.0 with kernel
version 4.9.0-2-amd64.  I hope this is useful; please let me know if you need
more info or if there is a better place to post this issue.

 - Anne
-------------- next part --------------
require(sp);
require(rgeos);

# Make two disjoint Polygon objects (p1 and p2) and two holes in p1 (p3 and p4), and one hole in p2 (p5)
p1 <- Polygon(cbind(c(0, 1, 0, 0), c(0, 0, 1, 0)), hole = FALSE);
p2 <- Polygon(cbind(c(1, 2, 1, 1), c(1, 1, 2, 1)), hole = FALSE);
p3 <- Polygon(cbind(c(0.25, 0.5, 0.25, 0.25), c(0.25, 0.25, 0.5, 0.25)), hole = TRUE);
p4 <- Polygon(cbind(c(0.0625, 0.125, 0.0625, 0.0625), c(0.0625, 0.0625, 0.125, 0.0625)), hole = TRUE);
p5 <- Polygon(cbind(c(1.25, 1.5, 1.25, 1.25), c(1.25, 1.25, 1.5, 1.25)), hole = TRUE);

# Combine Polygon objects in a few different ways 
P123 <- Polygons(list(p1, p2, p3), 1);
P12 <- Polygons(list(p1, p2), 1);
P13 <- Polygons(list(p1, p3), 1);
P134 <- Polygons(list(p1, p3, p4), 1);
P1 <- Polygons(list(p1), 1);
P1235 <- Polygons(list(p1, p2, p3, p5), 1);

sp123 <- SpatialPolygons(list(P123));
sp12 <- SpatialPolygons(list(P12));
sp13 <- SpatialPolygons(list(P13));
sp134 <- SpatialPolygons(list(P134));
sp1 <- SpatialPolygons(list(P1));
sp1235 <- SpatialPolygons(list(P1235));

# Add hole comments to the SpatialPolygons, so that everything is correct before disaggregation
attr(sp123 at polygons[[1]], "comment") <- createPolygonsComment(sp123 at polygons[[1]]);
attr(sp12 at polygons[[1]], "comment") <- createPolygonsComment(sp12 at polygons[[1]]);
attr(sp13 at polygons[[1]], "comment") <- createPolygonsComment(sp13 at polygons[[1]]);
attr(sp134 at polygons[[1]], "comment") <- createPolygonsComment(sp134 at polygons[[1]]);
attr(sp1 at polygons[[1]], "comment") <- createPolygonsComment(sp1 at polygons[[1]]);
attr(sp1235 at polygons[[1]], "comment") <- createPolygonsComment(sp1235 at polygons[[1]]);

# Disaggregate the SpatialPolygons
spd123 <- disaggregate(sp123, ignoreholes = FALSE);
spd12 <- disaggregate(sp12, ignoreholes = FALSE);
spd13 <- disaggregate(sp13, ignoreholes = FALSE);
spd134 <- disaggregate(sp134, ignoreholes = FALSE);
spd1 <- disaggregate(sp1, ignoreholes = FALSE);
spd1235 <- disaggregate(sp1235, ignoreholes = FALSE);

# Extract the comments from each result
ca123 <- unlist(lapply(lapply(spd123 at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));
ca12 <- unlist(lapply(lapply(spd12 at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));
ca13 <- unlist(lapply(lapply(spd13 at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));
ca134 <- unlist(lapply(lapply(spd134 at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));
ca1 <- unlist(lapply(lapply(spd1 at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));
ca1235 <- unlist(lapply(lapply(spd1235 at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));

# Compute what the comments *should* be for each case
cd123 <- unlist(lapply(spd123 at polygons, function(p) {createPolygonsComment(p)}));
cd12 <- unlist(lapply(spd12 at polygons, function(p) {createPolygonsComment(p)}));
cd13 <- unlist(lapply(spd13 at polygons, function(p) {createPolygonsComment(p)}));
cd134 <- unlist(lapply(spd134 at polygons, function(p) {createPolygonsComment(p)}));
cd1 <- unlist(lapply(spd1 at polygons, function(p) {createPolygonsComment(p)}));
cd1235 <- unlist(lapply(spd1235 at polygons, function(p) {createPolygonsComment(p)}));

# Shape consists of one polygon with a hole and one without
print(ca123);  # This is NA NA (wrong).
print(cd123);  # This is "0 1" "0" (correct).

# Shape consists of two polygons without holes
print(ca12);  # This is NA NA (wrong, but less important).
print(cd12);  # This is "0" "0" (correct).

# Shape consists of one polygon with one hole
print(ca13);  # This is "0 1" (correct).
print(cd13);  # This is "0 1" (correct).

# Shape consists of one polygon with two holes
print(ca134);  # This is NA (wrong).
print(cd134);  # This is "0 1 1" (correct).

# Shape consists of one polygon with no holes
print(ca1);  # This is "0" (correct).
print(cd1);  # This is "0" (correct).

# Shape consists of two polygons, each with one hole
print(ca1235);  # This is NA NA (wrong).
print(cd1235);  # This is "0 1" "0 1" (correct).
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170428/842f5e93/attachment.sig>

From tramni at abv.bg  Fri Apr 28 10:09:41 2017
From: tramni at abv.bg (Martin Ivanov)
Date: Fri, 28 Apr 2017 11:09:41 +0300 (EEST)
Subject: [R-sig-Geo] Extracting a single grid cell from a SpatialGrid
	keeping the grid structure
Message-ID: <820912789.1226842.1493366986082.JavaMail.apache@nm31.abv.bg>

Hello,
I would like to extract a single grid cell from a SpatialGrid or a SpatialGridDataFrame object without destroying the grid structure, that is, the resulting object must be?
a SpatialGrid or a SpatialGridDataFrame consisting of a single grid cell. Here is a minimum working example:

library(sp);
data(meuse.grid);
coordinates(meuse.grid) = c("x", "y") # promote to SpatialPointsDataFrame
gridded(meuse.grid) <- TRUE # promote to SpatialPixelsDataFrame
x <- as(meuse.grid, "SpatialGridDataFrame") # creates the full grid
fullgrid(meuse.grid) <- TRUE
image(meuse.grid);

# The following works:
latInd <- 20:70; lonInd <- 10:70; # the grid cell indices along latitude and longitude, respectively
a_df <- meuse.grid[latInd, lonInd, "dist"]; # the example SpatialGridDataFrame?object
?a_grid <- as(meuse.grid, "SpatialGrid"); # the example SpatialGrid object
image(a_df, add = TRUE, col = bpy.colors());
# the following prints a warning and assumes the grid cellsize of the single-cell dimension is equal to the cellsize of the?
# other dimension, which is not always appropriate:
lonInd <- 50;
a1_df <- meuse.grid[latInd, lonInd, "dist"]; image(a1_df);
# the following returns a SpatialPointsDataFrame object, which is not what is desired:
latInd <- 50;
a11_df <- meuse.grid[latInd, lonInd, "dist"];
# The following simply fails:
a11_grid <- a_grid[latInd, lonInd];

# a way round is to go through a RasterLayer object:
library(raster);
latInd <- a_grid at grid@cells.dim[2] - latInd + 1; a11_grid <- raster(x=a_grid);
# this is for the SpatialGrid case:
a11_grid <- crop(x=a11_grid, y=extent(x=a11_grid, r1=tail(x=latInd, n=1), r2=latInd[1], c1=lonInd[1], c2=tail(x=lonInd, n=1)));
a11_grid <- as(object=a11_grid, Class="SpatialGrid");
# and this is for the SpatialGridDataFrame case:
a11_df <- raster(x=meuse.grid["dist"]);
a11_df <- crop(x=a11_df, y=extent(x=a11_df, r1=tail(x=latInd, n=1), r2=latInd[1], c1=lonInd[1], c2=tail(x=lonInd, n=1)));
a11_df <- as(object=a11_df, Class="SpatialGridDataFrame");
# This approach also works if I only want to extract a single spatial column or a single spatial row and still keep the original grid structure

I would like to ask if there is a simpler and more elegant way to avoid the RasterLayer object? Certainly, this can be achieved directly by hacking the code of the?
"[" operator for the SpatialGrid and the SpatialGridDataFrame classes. However, I do not know how to access that code ...?

Any suggestions will be appreciated.

Thank you very much for your attention and your time.

Best regards,
Martin


From a.soufianou at yahoo.fr  Fri Apr 28 15:38:43 2017
From: a.soufianou at yahoo.fr (Soufianou Abou)
Date: Fri, 28 Apr 2017 13:38:43 +0000 (UTC)
Subject: [R-sig-Geo] =?utf-8?q?mod=C3=A9lisation_de_la_distribution_spatia?=
 =?utf-8?b?bGUgZHUgbmnDqWLDqQ==?=
References: <655226278.6371764.1493386723852.ref@mail.yahoo.com>
Message-ID: <655226278.6371764.1493386723852@mail.yahoo.com>

Bonsoir chers,?j'aimerai mod?liser la distribution spatiale de la niche ?cologique du ni?b? , je dispose d'un jeu de donn?es (variables environnementales et bioclimatiques) et des donn?es de pr?sence des vari?t?s du ni?b?.J'ai voulu utilis? le logiciel Maxent pour le faire; mais j'ai compris ? travers la litt?rature que le logiciel R est de nos jours largement utilis?.?C'est dans cette optique que j'aimerai m'y plancher l? dessus.?Au fait, j'ai besoin de l'aide pour comprendre les proc?dures pour faire cette mod?lisation ? l'aide de R. ??1. J'aimerai faire Un test de normalit? de Shapiro-Wilks, dont l?hypoth?se nulle consiste ? affirmer que les donn?es suivent une distribution normale et o? l'hypoth?se alternative implique une distribution asym?trique ou anormale, permet de conclure que les observations sont de distribution tr?s asym?trique.?2. J'aimerai faire une transformation logarithmique des donn?es r?ponses est donc effectu?e afin de r?duire l?asym?trie des donn?es et, ainsi, d?am?liorer la puissance des tests statistiques.
3.?J'aimerai faire une?analyse de redondance canonique ?sur l?ensemble des tableaux de donn?es?.4.J'aimerai faire??une s?lection progressive bidirectionnelle ?afin ?de choir les variables pertinentes en fonction du crit?re d?information d?Akaike, ou?AIC?et de la repr?sentativit? (p-value) .5. J'aimerai faire??une s?lection?progressive par ajout, en fonction du R-carr? ajust?, ?afin de comparer le r?sultat de la s?lection
6. En fin les proc?dure ? suivre pour mod?liser la distribution des vari?t?s.
merci cordialement.
SADDA Abou-Soufianou -------------------------------------- DoctorantUniversit? Dan Dicko Dankoulodo de Maradi,120 avenue Maman Koraou,ADS, MaradiBP 465 Maradi/Niger E-mail:?a.soufianou at yahoo.frGSM?: (+227)96269987
	[[alternative HTML version deleted]]


From bfalevlist at gmail.com  Fri Apr 28 16:12:49 2017
From: bfalevlist at gmail.com (=?UTF-8?Q?Bede-Fazekas_=c3=81kos?=)
Date: Fri, 28 Apr 2017 16:12:49 +0200
Subject: [R-sig-Geo]
 =?utf-8?q?mod=C3=A9lisation_de_la_distribution_spatia?=
 =?utf-8?b?bGUgZHUgbmnDqWLDqQ==?=
In-Reply-To: <655226278.6371764.1493386723852@mail.yahoo.com>
References: <655226278.6371764.1493386723852.ref@mail.yahoo.com>
 <655226278.6371764.1493386723852@mail.yahoo.com>
Message-ID: <1e3e2afb-5262-8693-1072-117981662100@gmail.com>

Cher Sadda,

vous avez ?crit que "J'ai voulu utilis? le logiciel Maxent pour le 
faire; mais j'ai compris ? travers la litt?rature que le logiciel R est 
de nos jours largement utilis?.", mais vous pouvez utiliser MaxEnt via 
le logiciel R:
install.packages("dismo")
library(dismo)
m <- maxent(x = tableau_des_pr?dicteurs_environnementaux, p = 
points_de_pr?sence)

Vous devez d'abord t?l?charger le programme ? partir d'ici:
http://biodiversityinformatics.amnh.org/open_source/maxent/

J'esp?re que cela t'aides,
?kos Bede-Fazekas
Acad?mie hongroise des sciences


2017.04.28. 15:38 keltez?ssel, Soufianou Abou via R-sig-Geo ?rta:
> Bonsoir chers, j'aimerai mod?liser la distribution spatiale de la niche ?cologique du ni?b? , je dispose d'un jeu de donn?es (variables environnementales et bioclimatiques) et des donn?es de pr?sence des vari?t?s du ni?b?.J'ai voulu utilis? le logiciel Maxent pour le faire; mais j'ai compris ? travers la litt?rature que le logiciel R est de nos jours largement utilis?. C'est dans cette optique que j'aimerai m'y plancher l? dessus. Au fait, j'ai besoin de l'aide pour comprendre les proc?dures pour faire cette mod?lisation ? l'aide de R.   1. J'aimerai faire Un test de normalit? de Shapiro-Wilks, dont l?hypoth?se nulle consiste ? affirmer que les donn?es suivent une distribution normale et o? l'hypoth?se alternative implique une distribution asym?trique ou anormale, permet de conclure que les observations sont de distribution tr?s asym?trique. 2. J'aimerai faire une transformation logarithmique des donn?es r?ponses est donc effectu?e afin de r?duire l?asym?trie des donn?es et, ainsi, d?am?liorer la puissance des tests statistiques.
> 3. J'aimerai faire une analyse de redondance canonique  sur l?ensemble des tableaux de donn?es .4.J'aimerai faire  une s?lection progressive bidirectionnelle  afin  de choir les variables pertinentes en fonction du crit?re d?information d?Akaike, ou AIC et de la repr?sentativit? (p-value) .5. J'aimerai faire  une s?lection progressive par ajout, en fonction du R-carr? ajust?,  afin de comparer le r?sultat de la s?lection
> 6. En fin les proc?dure ? suivre pour mod?liser la distribution des vari?t?s.
> merci cordialement.
> SADDA Abou-Soufianou -------------------------------------- DoctorantUniversit? Dan Dicko Dankoulodo de Maradi,120 avenue Maman Koraou,ADS, MaradiBP 465 Maradi/Niger E-mail: a.soufianou at yahoo.frGSM : (+227)96269987
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From a.soufianou at yahoo.fr  Fri Apr 28 16:17:09 2017
From: a.soufianou at yahoo.fr (Soufianou Abou)
Date: Fri, 28 Apr 2017 14:17:09 +0000 (UTC)
Subject: [R-sig-Geo] patial distribution of the ecological niche of cowpea, 
References: <1610626693.15124898.1493389029945.ref@mail.yahoo.com>
Message-ID: <1610626693.15124898.1493389029945@mail.yahoo.com>

I want to model the spatial distribution of the ecological niche of cowpea, I have a set of data (environmental variables and bioclimatic) and data of presence of the varieties of cowpea.I wanted to use the software Maxent for do it; But I understood through the literature that the R software is now widely used. It is with this in mind that I would like to focus on this. By the way, I need help to understand the procedures for doing this modeling using R. 1. I would like to do a Shapiro-Wilks normality test, the null hypothesis of which is to assert that The data follow a normal distribution and where the alternative hypothesis implies an asymmetric or abnormal distribution, makes it possible to conclude that the observations are of very asymmetric distribution. 2. I would like to do a logarithmic transformation of the response data is therefore performed in order to reduce the asymmetry of the data and thus improve the power of the statistical tests.3. I would like to do a canonical redundancy analysis on all the data tables. 4. I would like to make a bidirectional progressive selection in order to select the relevant variables according to Akaike's information criterion or AIC and Of the representativeness (p-value) .5. I would like to make a progressive selection by adding, according to the adjusted R-square, in order to compare the result of the selection 6. Finally, the procedures to be followed to model the distribution of the varieties.

Best regards?
?SADDA Abou-Soufianou -------------------------------------- DoctorantUniversit? Dan Dicko Dankoulodo de Maradi,120 avenue Maman Koraou,ADS, MaradiBP 465 Maradi/Niger E-mail:?a.soufianou at yahoo.frGSM?: (+227)96269987
	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Fri Apr 28 18:11:02 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 28 Apr 2017 18:11:02 +0200
Subject: [R-sig-Geo] sp::disaggregate() does not seem to add the
 necessary "comment" attribute to identify holes if Polygons does not
 contain exactly 1 polygon with 0 or 1 holes
In-Reply-To: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>
References: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>
Message-ID: <alpine.LFD.2.20.1704281804080.27020@reclus.nhh.no>

I've pushed the fix to my fork:

https://github.com/rsbivand/sp

and created a pull request:

https://github.com/edzer/sp/pull/28

Only one part of a complicated set if nested if() in disaggregate() was 
adding comments, but in some settings the existing comments survived the 
disaggregation. Now the Polygons object comment attribute is re-created 
for all Polygons objects. This is version 1.2-6, also including code 
changes that internally affect rgdal and rgeos - you may like to 
re-install them from source after installing this sp version (shouldn't 
matter).

Roger

On Fri, 28 Apr 2017, Anne C. Hanna wrote:

> Hello.  I first posted this issue report on the sp GitHub repo
> (https://github.com/edzer/sp/issues/27) and it was suggested that I redirect
> it to here.
>
> I am working with a geographic dataset with complex borders. The data is
> stored as a SpatialPolygonsDataFrame.  Each Polygons object in the data frame
> may be composed of multiple disjoint polygons, and each polygon may have
> multiple holes.  I want to disaggregate each of the Polygons objects into its
> individual disjoint polygons and construct an adjacency matrix for all the
> disjoint components, and I was using disaggregate() to do this.  However, when
> I run gTouches() on the disaggregated data, in order to compute the
> adjacencies, I get a number of warnings like this:
>
> Error in RGEOSBinPredFunc(spgeom1, spgeom2, byid, "rgeos_touches") : Polygons
> object missing comment attribute ignoring hole(s).  See function createSPComment.
>
> Looking at the Polygons "comment" attributes in the SpatialPolygonsDataFrame
> output by disaggregate(), I see that the only comment values are "0"
> (indicating a single polygon with no holes), "0 1" (indicating a single
> polygon with a single hole), and NULL (apparently no comment was written).
> Since I know my dataset contains several Polygons objects which are composed
> of multiple disjoint regions, and also several Polygons which contain more
> than one hole, this is not the expected result.  In reading the disaggregate()
> code in the sp GitHub repository (specifically, explodePolygons()), I also
> can't see anywhere the comment is being added for the cases where a Polygons
> object has more than two parts or more than two holes.  It actually seems like
> it's getting carried along almost accidentally in the few cases that do get
> comments, and neglected otherwise.
>
> Assuming I'm not failing to understand the code and the desired behavior
> (entirely possible, as I am new at working with this software!), this seems
> suboptimal to me.  My dataset is pretty well-behaved (despite its complexity),
> so I should be able to fix my issues with judicious application of
> createPolygonsComment.  But I had a heck of a time figuring out what was going
> wrong with gTouches, since Polygons comment management appears to be a pretty
> obscure field (and createSPComment wasn't working for me, for whatever
> reason).  So it seems like it might be better if disaggregate() just parses
> and passes along the comments from its input correctly, or, if it's absolutely
> necessary to not create comments, passes nothing and warns clearly in the
> manual that comments and associated hole information are being lost.  Passing
> along comments in some cases while silently dropping them in others seems like
> kind of the worst of both worlds.
>
> I've attached a set of tests I wrote to demonstrate desired/undesired
> behavior: disaggregate_comment_tests.R.  My R version is 3.4.0, my sp version
> is 1.2-4, my rgeos version is 0.3-23 (SVN revision 546), and my GEOS runtime
> version is 3.5.1-CAPI-1.9.1 r4246.  I am using Debian Release 9.0 with kernel
> version 4.9.0-2-amd64.  I hope this is useful; please let me know if you need
> more info or if there is a better place to post this issue.
>
> - Anne
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From orion at ofb.net  Sat Apr 29 01:25:24 2017
From: orion at ofb.net (Anne C. Hanna)
Date: Fri, 28 Apr 2017 19:25:24 -0400
Subject: [R-sig-Geo] sp::disaggregate() does not seem to add the
 necessary "comment" attribute to identify holes if Polygons does not
 contain exactly 1 polygon with 0 or 1 holes
In-Reply-To: <alpine.LFD.2.20.1704281804080.27020@reclus.nhh.no>
References: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>
 <alpine.LFD.2.20.1704281804080.27020@reclus.nhh.no>
Message-ID: <78988b99-67ef-8129-6622-aad0a4946920@ofb.net>

Roger,

This looks great, and I will try it out ASAP.  I do have one reservation
though --- it seems you are using createSPComment() to reconstruct the
comments, and I have seen some discussion that that may not be reliable in all
cases (e.g. if the initial polygons are wonky in some way).  I don't know a
lot about this, but is it possible that it would be preferable to parse and
appropriately disaggregate the original comments strings (if they exist), so
as to deal slightly more smoothly with such cases (i.e., the polygons would
still be wonky, but at least the hole/polygon matching would track with
whatever was in the original data)?

I also had no success using createSPComment to fix disaggregate()'s output
previously, even though my polygons are perfectly non-wonky, so I am perhaps a
little more untrusting of it than I should be.  But I'll let you know how this
version works with my data.  Thanks for addressing this so quickly!

 - Anne


On 04/28/2017 12:11 PM, Roger Bivand wrote:
> I've pushed the fix to my fork:
> 
> https://github.com/rsbivand/sp
> 
> and created a pull request:
> 
> https://github.com/edzer/sp/pull/28
> 
> Only one part of a complicated set if nested if() in disaggregate() was adding
> comments, but in some settings the existing comments survived the
> disaggregation. Now the Polygons object comment attribute is re-created for
> all Polygons objects. This is version 1.2-6, also including code changes that
> internally affect rgdal and rgeos - you may like to re-install them from
> source after installing this sp version (shouldn't matter).
> 
> Roger
> 
> On Fri, 28 Apr 2017, Anne C. Hanna wrote:
> 
>> Hello.  I first posted this issue report on the sp GitHub repo
>> (https://github.com/edzer/sp/issues/27) and it was suggested that I redirect
>> it to here.
>>
>> I am working with a geographic dataset with complex borders. The data is
>> stored as a SpatialPolygonsDataFrame.  Each Polygons object in the data frame
>> may be composed of multiple disjoint polygons, and each polygon may have
>> multiple holes.  I want to disaggregate each of the Polygons objects into its
>> individual disjoint polygons and construct an adjacency matrix for all the
>> disjoint components, and I was using disaggregate() to do this.  However, when
>> I run gTouches() on the disaggregated data, in order to compute the
>> adjacencies, I get a number of warnings like this:
>>
>> Error in RGEOSBinPredFunc(spgeom1, spgeom2, byid, "rgeos_touches") : Polygons
>> object missing comment attribute ignoring hole(s).  See function
>> createSPComment.
>>
>> Looking at the Polygons "comment" attributes in the SpatialPolygonsDataFrame
>> output by disaggregate(), I see that the only comment values are "0"
>> (indicating a single polygon with no holes), "0 1" (indicating a single
>> polygon with a single hole), and NULL (apparently no comment was written).
>> Since I know my dataset contains several Polygons objects which are composed
>> of multiple disjoint regions, and also several Polygons which contain more
>> than one hole, this is not the expected result.  In reading the disaggregate()
>> code in the sp GitHub repository (specifically, explodePolygons()), I also
>> can't see anywhere the comment is being added for the cases where a Polygons
>> object has more than two parts or more than two holes.  It actually seems like
>> it's getting carried along almost accidentally in the few cases that do get
>> comments, and neglected otherwise.
>>
>> Assuming I'm not failing to understand the code and the desired behavior
>> (entirely possible, as I am new at working with this software!), this seems
>> suboptimal to me.  My dataset is pretty well-behaved (despite its complexity),
>> so I should be able to fix my issues with judicious application of
>> createPolygonsComment.  But I had a heck of a time figuring out what was going
>> wrong with gTouches, since Polygons comment management appears to be a pretty
>> obscure field (and createSPComment wasn't working for me, for whatever
>> reason).  So it seems like it might be better if disaggregate() just parses
>> and passes along the comments from its input correctly, or, if it's absolutely
>> necessary to not create comments, passes nothing and warns clearly in the
>> manual that comments and associated hole information are being lost.  Passing
>> along comments in some cases while silently dropping them in others seems like
>> kind of the worst of both worlds.
>>
>> I've attached a set of tests I wrote to demonstrate desired/undesired
>> behavior: disaggregate_comment_tests.R.  My R version is 3.4.0, my sp version
>> is 1.2-4, my rgeos version is 0.3-23 (SVN revision 546), and my GEOS runtime
>> version is 3.5.1-CAPI-1.9.1 r4246.  I am using Debian Release 9.0 with kernel
>> version 4.9.0-2-amd64.  I hope this is useful; please let me know if you need
>> more info or if there is a better place to post this issue.
>>
>> - Anne
>>
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170428/11a2e103/attachment.sig>

From zhang_t_y at hotmail.com  Sat Apr 29 02:22:25 2017
From: zhang_t_y at hotmail.com (Zhang Tianyi)
Date: Sat, 29 Apr 2017 00:22:25 +0000
Subject: [R-sig-Geo] Using gdalUtils to reproject hdf file
In-Reply-To: <PS1PR0601MB18504C846F532B003CF51990B8120@PS1PR0601MB1850.apcprd06.prod.outlook.com>
References: <mailman.0.1482129909.21593.r-sig-geo@r-project.org>,
 <PS1PR0601MB1850DEA755A3C8909787184EB8130@PS1PR0601MB1850.apcprd06.prod.outlook.com>,
 <PS1PR0601MB18504C846F532B003CF51990B8120@PS1PR0601MB1850.apcprd06.prod.outlook.com>
Message-ID: <PS1PR0601MB185072C549828459B80E871EB8120@PS1PR0601MB1850.apcprd06.prod.outlook.com>

 Dear all,

I am now working with GEFED dataset, which is a global wild file dataset.You can download one from FTP: fuoco.geog.umd.edu; Login name: fire; Password: burnt; The file is stored into gfed4/monthly/. Any one file will be example.

My task is to translate the hdf file into tif, which can be done by gdalUtils package. But there is no georeferecing information in the file. Instead, the only information I could find is from the data manual (can be found in gfed4 folder in that FTP): "Each product file contains seven data layers, each stored as a separate HDF4 Scientific Data Set (SDS). Units apply to each layer after multiplying by the specified scale factor. Each data layer has 720 rows and 1440 columns which correspond to the global 0.25? GFED grid. The center of the upper left grid cell is located at longitude 179.875?W, 89.875?N.". I think the original src can be written from this statement. And I can pass it to a_src and a_ullr variable in gdal_translate command. But I have no idea what is the string for a_src.

Any recommendation will be appreciated.

Tianyi



	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Sat Apr 29 16:06:55 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 29 Apr 2017 16:06:55 +0200
Subject: [R-sig-Geo] sp::disaggregate() does not seem to add the
 necessary "comment" attribute to identify holes if Polygons does not
 contain exactly 1 polygon with 0 or 1 holes
In-Reply-To: <78988b99-67ef-8129-6622-aad0a4946920@ofb.net>
References: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>
 <alpine.LFD.2.20.1704281804080.27020@reclus.nhh.no>
 <78988b99-67ef-8129-6622-aad0a4946920@ofb.net>
Message-ID: <alpine.LFD.2.20.1704291551270.27676@reclus.nhh.no>

On Sat, 29 Apr 2017, Anne C. Hanna wrote:

> Roger,
>
> This looks great, and I will try it out ASAP.  I do have one reservation 
> though --- it seems you are using createSPComment() to reconstruct the 
> comments, and I have seen some discussion that that may not be reliable 
> in all cases (e.g. if the initial polygons are wonky in some way).  I 
> don't know a lot about this, but is it possible that it would be 
> preferable to parse and appropriately disaggregate the original comments 
> strings (if they exist), so as to deal slightly more smoothly with such 
> cases (i.e., the polygons would still be wonky, but at least the 
> hole/polygon matching would track with whatever was in the original 
> data)?

Contributions very welcome - this was code moved from the raster package, 
so I really have little feeling for why it is necessary.

Please provide references and use cases. GEOS is strict in its treatments 
of geometries, but does work in most cases. People just expressing 
opinions doesn't help, and may well be misleading. It is possible to clean 
geometries too, see the cleangeo package. createPolygonsComment probably 
isn't foolproof, but specific reports help, because then it is possible to 
do something about it. sp classes didn't use simple features when written 
because sf was not widely used then - introduced in 2004, when sp was 
being developed. Using sf helps, and rgeos::createPolygonsComment was a 
work-around from 2010 when rgeos was written.

Even when you use sf in the sf package, you will still run into invalid 
geometries because the geometries are in fact invalid, the sf package 
also uses GEOS.

Roger

>
> I also had no success using createSPComment to fix disaggregate()'s 
> output previously, even though my polygons are perfectly non-wonky, so I 
> am perhaps a little more untrusting of it than I should be.  But I'll 
> let you know how this version works with my data.  Thanks for addressing 
> this so quickly!
>
> - Anne
>
>
> On 04/28/2017 12:11 PM, Roger Bivand wrote:
>> I've pushed the fix to my fork:
>>
>> https://github.com/rsbivand/sp
>>
>> and created a pull request:
>>
>> https://github.com/edzer/sp/pull/28
>>
>> Only one part of a complicated set if nested if() in disaggregate() was adding
>> comments, but in some settings the existing comments survived the
>> disaggregation. Now the Polygons object comment attribute is re-created for
>> all Polygons objects. This is version 1.2-6, also including code changes that
>> internally affect rgdal and rgeos - you may like to re-install them from
>> source after installing this sp version (shouldn't matter).
>>
>> Roger
>>
>> On Fri, 28 Apr 2017, Anne C. Hanna wrote:
>>
>>> Hello.  I first posted this issue report on the sp GitHub repo
>>> (https://github.com/edzer/sp/issues/27) and it was suggested that I redirect
>>> it to here.
>>>
>>> I am working with a geographic dataset with complex borders. The data is
>>> stored as a SpatialPolygonsDataFrame.  Each Polygons object in the data frame
>>> may be composed of multiple disjoint polygons, and each polygon may have
>>> multiple holes.  I want to disaggregate each of the Polygons objects into its
>>> individual disjoint polygons and construct an adjacency matrix for all the
>>> disjoint components, and I was using disaggregate() to do this.  However, when
>>> I run gTouches() on the disaggregated data, in order to compute the
>>> adjacencies, I get a number of warnings like this:
>>>
>>> Error in RGEOSBinPredFunc(spgeom1, spgeom2, byid, "rgeos_touches") : Polygons
>>> object missing comment attribute ignoring hole(s).  See function
>>> createSPComment.
>>>
>>> Looking at the Polygons "comment" attributes in the SpatialPolygonsDataFrame
>>> output by disaggregate(), I see that the only comment values are "0"
>>> (indicating a single polygon with no holes), "0 1" (indicating a single
>>> polygon with a single hole), and NULL (apparently no comment was written).
>>> Since I know my dataset contains several Polygons objects which are composed
>>> of multiple disjoint regions, and also several Polygons which contain more
>>> than one hole, this is not the expected result.  In reading the disaggregate()
>>> code in the sp GitHub repository (specifically, explodePolygons()), I also
>>> can't see anywhere the comment is being added for the cases where a Polygons
>>> object has more than two parts or more than two holes.  It actually seems like
>>> it's getting carried along almost accidentally in the few cases that do get
>>> comments, and neglected otherwise.
>>>
>>> Assuming I'm not failing to understand the code and the desired behavior
>>> (entirely possible, as I am new at working with this software!), this seems
>>> suboptimal to me.  My dataset is pretty well-behaved (despite its complexity),
>>> so I should be able to fix my issues with judicious application of
>>> createPolygonsComment.  But I had a heck of a time figuring out what was going
>>> wrong with gTouches, since Polygons comment management appears to be a pretty
>>> obscure field (and createSPComment wasn't working for me, for whatever
>>> reason).  So it seems like it might be better if disaggregate() just parses
>>> and passes along the comments from its input correctly, or, if it's absolutely
>>> necessary to not create comments, passes nothing and warns clearly in the
>>> manual that comments and associated hole information are being lost.  Passing
>>> along comments in some cases while silently dropping them in others seems like
>>> kind of the worst of both worlds.
>>>
>>> I've attached a set of tests I wrote to demonstrate desired/undesired
>>> behavior: disaggregate_comment_tests.R.  My R version is 3.4.0, my sp version
>>> is 1.2-4, my rgeos version is 0.3-23 (SVN revision 546), and my GEOS runtime
>>> version is 3.5.1-CAPI-1.9.1 r4246.  I am using Debian Release 9.0 with kernel
>>> version 4.9.0-2-amd64.  I hope this is useful; please let me know if you need
>>> more info or if there is a better place to post this issue.
>>>
>>> - Anne
>>>
>>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From orion at ofb.net  Sun Apr 30 18:18:17 2017
From: orion at ofb.net (Anne C. Hanna)
Date: Sun, 30 Apr 2017 12:18:17 -0400
Subject: [R-sig-Geo] sp::disaggregate() does not seem to add the
 necessary "comment" attribute to identify holes if Polygons does not
 contain exactly 1 polygon with 0 or 1 holes
In-Reply-To: <alpine.LFD.2.20.1704291551270.27676@reclus.nhh.no>
References: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>
 <alpine.LFD.2.20.1704281804080.27020@reclus.nhh.no>
 <78988b99-67ef-8129-6622-aad0a4946920@ofb.net>
 <alpine.LFD.2.20.1704291551270.27676@reclus.nhh.no>
Message-ID: <dbf4f008-23ae-3767-efae-6df317dc21c9@ofb.net>

Roger,

Unfortunately I have a use case for you of createSPComment() not working.

I tried your new version of disaggregate() on the original test script I sent
you, and it does seem to have fixed all the cases in there.  However, when I
tried it on my actual data, it had the same failure mode as before. The
original dataset has what appear to be correct comments, but the
disaggregate() output is full of NULLs in place of comments for the multi-part
and multi-hole polygons.

createSPComment() appears to be what's failing, rather than just some logic in
your disaggregate() function --- when I try to run createSPComment() on the
disaggregate() output, I still get the same set of NULLs.  I can fix it if I
run createPolygonsComment() individually on every Polygons object in my
SpatialPolygonsDataFrame.

I'm not sure what is different between my dataset and the test polygons I was
using earlier, as createSPComment() seems to handle the test shapes just fine
(and converting my SpatialPolygonsDataFrame into just a set of SpatialPolygons
also doesn't help).  But presumably it is better to have createSPComment()
fixed than to have to work around it in disaggregate().  So I guess I'll look
at the code for that and see if I can find what's wrong.

Just in case you want to see this in action, I've attached a little test
script.  The data set I am working with is available at:

http://aws.redistricting.state.pa.us/Redistricting/Resources/GISData/2011-Voting-District-Boundary-Shapefiles.zip

 - Anne


On 04/29/2017 10:06 AM, Roger Bivand wrote:
> On Sat, 29 Apr 2017, Anne C. Hanna wrote:
> 
>> Roger,
>>
>> This looks great, and I will try it out ASAP.  I do have one reservation
>> though --- it seems you are using createSPComment() to reconstruct the
>> comments, and I have seen some discussion that that may not be reliable in
>> all cases (e.g. if the initial polygons are wonky in some way).  I don't
>> know a lot about this, but is it possible that it would be preferable to
>> parse and appropriately disaggregate the original comments strings (if they
>> exist), so as to deal slightly more smoothly with such cases (i.e., the
>> polygons would still be wonky, but at least the hole/polygon matching would
>> track with whatever was in the original data)?
> 
> Contributions very welcome - this was code moved from the raster package, so I
> really have little feeling for why it is necessary.
> 
> Please provide references and use cases. GEOS is strict in its treatments of
> geometries, but does work in most cases. People just expressing opinions
> doesn't help, and may well be misleading. It is possible to clean geometries
> too, see the cleangeo package. createPolygonsComment probably isn't foolproof,
> but specific reports help, because then it is possible to do something about
> it. sp classes didn't use simple features when written because sf was not
> widely used then - introduced in 2004, when sp was being developed. Using sf
> helps, and rgeos::createPolygonsComment was a work-around from 2010 when rgeos
> was written.
> 
> Even when you use sf in the sf package, you will still run into invalid
> geometries because the geometries are in fact invalid, the sf package also
> uses GEOS.
> 
> Roger
> 
>>
>> I also had no success using createSPComment to fix disaggregate()'s output
>> previously, even though my polygons are perfectly non-wonky, so I am perhaps
>> a little more untrusting of it than I should be.  But I'll let you know how
>> this version works with my data.  Thanks for addressing this so quickly!
>>
>> - Anne
>>
>>
>> On 04/28/2017 12:11 PM, Roger Bivand wrote:
>>> I've pushed the fix to my fork:
>>>
>>> https://github.com/rsbivand/sp
>>>
>>> and created a pull request:
>>>
>>> https://github.com/edzer/sp/pull/28
>>>
>>> Only one part of a complicated set if nested if() in disaggregate() was adding
>>> comments, but in some settings the existing comments survived the
>>> disaggregation. Now the Polygons object comment attribute is re-created for
>>> all Polygons objects. This is version 1.2-6, also including code changes that
>>> internally affect rgdal and rgeos - you may like to re-install them from
>>> source after installing this sp version (shouldn't matter).
>>>
>>> Roger
>>>
>>> On Fri, 28 Apr 2017, Anne C. Hanna wrote:
>>>
>>>> Hello.  I first posted this issue report on the sp GitHub repo
>>>> (https://github.com/edzer/sp/issues/27) and it was suggested that I redirect
>>>> it to here.
>>>>
>>>> I am working with a geographic dataset with complex borders. The data is
>>>> stored as a SpatialPolygonsDataFrame.  Each Polygons object in the data frame
>>>> may be composed of multiple disjoint polygons, and each polygon may have
>>>> multiple holes.  I want to disaggregate each of the Polygons objects into its
>>>> individual disjoint polygons and construct an adjacency matrix for all the
>>>> disjoint components, and I was using disaggregate() to do this.  However,
>>>> when
>>>> I run gTouches() on the disaggregated data, in order to compute the
>>>> adjacencies, I get a number of warnings like this:
>>>>
>>>> Error in RGEOSBinPredFunc(spgeom1, spgeom2, byid, "rgeos_touches") : Polygons
>>>> object missing comment attribute ignoring hole(s).  See function
>>>> createSPComment.
>>>>
>>>> Looking at the Polygons "comment" attributes in the SpatialPolygonsDataFrame
>>>> output by disaggregate(), I see that the only comment values are "0"
>>>> (indicating a single polygon with no holes), "0 1" (indicating a single
>>>> polygon with a single hole), and NULL (apparently no comment was written).
>>>> Since I know my dataset contains several Polygons objects which are composed
>>>> of multiple disjoint regions, and also several Polygons which contain more
>>>> than one hole, this is not the expected result.  In reading the
>>>> disaggregate()
>>>> code in the sp GitHub repository (specifically, explodePolygons()), I also
>>>> can't see anywhere the comment is being added for the cases where a Polygons
>>>> object has more than two parts or more than two holes.  It actually seems
>>>> like
>>>> it's getting carried along almost accidentally in the few cases that do get
>>>> comments, and neglected otherwise.
>>>>
>>>> Assuming I'm not failing to understand the code and the desired behavior
>>>> (entirely possible, as I am new at working with this software!), this seems
>>>> suboptimal to me.  My dataset is pretty well-behaved (despite its
>>>> complexity),
>>>> so I should be able to fix my issues with judicious application of
>>>> createPolygonsComment.  But I had a heck of a time figuring out what was
>>>> going
>>>> wrong with gTouches, since Polygons comment management appears to be a pretty
>>>> obscure field (and createSPComment wasn't working for me, for whatever
>>>> reason).  So it seems like it might be better if disaggregate() just parses
>>>> and passes along the comments from its input correctly, or, if it's
>>>> absolutely
>>>> necessary to not create comments, passes nothing and warns clearly in the
>>>> manual that comments and associated hole information are being lost.  Passing
>>>> along comments in some cases while silently dropping them in others seems
>>>> like
>>>> kind of the worst of both worlds.
>>>>
>>>> I've attached a set of tests I wrote to demonstrate desired/undesired
>>>> behavior: disaggregate_comment_tests.R.  My R version is 3.4.0, my sp version
>>>> is 1.2-4, my rgeos version is 0.3-23 (SVN revision 546), and my GEOS runtime
>>>> version is 3.5.1-CAPI-1.9.1 r4246.  I am using Debian Release 9.0 with kernel
>>>> version 4.9.0-2-amd64.  I hope this is useful; please let me know if you need
>>>> more info or if there is a better place to post this issue.
>>>>
>>>> - Anne
>>>>
>>>
>>
>>
> 
-------------- next part --------------
require(sp);
require(rgeos);
require(rgdal);

# Load the shapefile data (download from http://aws.redistricting.state.pa.us/Redistricting/Resources/GISData/2011-Voting-District-Boundary-Shapefiles.zip)
sf <- readOGR(dsn = "/home/orion/research/gerrymandering/PA data/2011 Voting District Boundary Shapefiles/", layer = "MCDS", stringsAsFactors = FALSE);

# check the original polygon comments
c_orig <- unlist(lapply(lapply(sf at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));  # c_orig seems to contain a sensible set of comments

# disaggregate and check the resulting comments
sfd <- disaggregate(sf);
c_dis <- unlist(lapply(lapply(sfd at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));  # c_dis has a lot of NAs

# try createSPComment() on the disaggregated data
sfd_cspc <- createSPComment(sfd);
c_cspc <- unlist(lapply(lapply(sfd_cspc at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));  # still full of NAs, in the same places

# fix the comments with createPolygonsComment
sfdc <- sfd;
for(i in 1:nrow(sfdc))
 { 
  attr(sfdc at polygons[[i]], "comment") <- createPolygonsComment(sfdc at polygons[[i]]);
 }
c_dis_fixed <-  unlist(lapply(lapply(sfdc at polygons, function(p) {attr(p, "comment")}), function(x) {ifelse(is.null(x), NA, x)}));  # c_dis_fixed seems to show sensible comments
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170430/a165fffc/attachment.sig>

From orion at ofb.net  Sun Apr 30 21:35:53 2017
From: orion at ofb.net (Anne C. Hanna)
Date: Sun, 30 Apr 2017 15:35:53 -0400
Subject: [R-sig-Geo] sp::disaggregate() does not seem to add the
 necessary "comment" attribute to identify holes if Polygons does not
 contain exactly 1 polygon with 0 or 1 holes
In-Reply-To: <dbf4f008-23ae-3767-efae-6df317dc21c9@ofb.net>
References: <66e48d8f-dc98-aa0c-9cae-4c8d56dcbe10@ofb.net>
 <alpine.LFD.2.20.1704281804080.27020@reclus.nhh.no>
 <78988b99-67ef-8129-6622-aad0a4946920@ofb.net>
 <alpine.LFD.2.20.1704291551270.27676@reclus.nhh.no>
 <dbf4f008-23ae-3767-efae-6df317dc21c9@ofb.net>
Message-ID: <30d71999-b99a-dabd-8de9-709b782fae03@ofb.net>

Okay, I think I've tracked it down.  This is... absurdly complicated and I
don't know what the actual correct resolution is, but I'm pretty sure that at
least I know what's going wrong.

Basically it comes down to the fact that createSPComment() does not actually
check whether each individual polygon in the SpatialPolygons object has a
comment (much less a correct comment).  Instead, it looks at a top-level
"comment" attribute.  If that top-level comment attribute (i.e. sf at comment for
my SpatialPolygonsDataFrame object stored as "sf") is "TRUE", then
createSPComment() does nothing.

And here's where the second layer of the problem comes in for my dataset ---
my original data, pre-disaggregation, has complete and correct comments.  Some
of those comments (for objects where it's a single polygon with 0 or 1 holes)
are being retained by the basic disaggregation algorithm, while others are
not.  Your patch attempts to fill in the holes by running createSPComment() on
the final results.  But... the thing you are running createSPComment() on is
SpatialPolygons(p), where p is the list of disaggregated polygons.  And the
SpatialPolygons() constructor looks at the list of the polygons you feed it,
and if *any* of those polygons have comments, it sets the top-level "comment"
attribute of its output to "TRUE", even if other polygons are missing
comments.  (And it also doesn't check the comments it gets for correctness.)
So the fact that some polygon comments are retained in p means that
SpatialPolygons() lies to createSPComment() about whether it has any missing
comments that need to be fixed.

On the other hand, when I manually run createPolygonComment() on the
individual Polygons objects in disaggregate()'s output, I don't even look at
the top-level "comment" attribute, so it works just fine.

My little toy test polygons, on the other hand, were just not complex enough
to end up with partial comments in them, so they also didn't experience this
error.

I am not familiar enough with the design philosophy behind sp to suggest at
what level this issue should be fixed, but I hope this is at least enough info
to help those who do know what they're doing get it corrected.

 - Anne


On 04/30/2017 12:18 PM, Anne C. Hanna wrote:
> Roger,
> 
> Unfortunately I have a use case for you of createSPComment() not working.
> 
> I tried your new version of disaggregate() on the original test script I sent
> you, and it does seem to have fixed all the cases in there.  However, when I
> tried it on my actual data, it had the same failure mode as before. The
> original dataset has what appear to be correct comments, but the
> disaggregate() output is full of NULLs in place of comments for the multi-part
> and multi-hole polygons.
> 
> createSPComment() appears to be what's failing, rather than just some logic in
> your disaggregate() function --- when I try to run createSPComment() on the
> disaggregate() output, I still get the same set of NULLs.  I can fix it if I
> run createPolygonsComment() individually on every Polygons object in my
> SpatialPolygonsDataFrame.
> 
> I'm not sure what is different between my dataset and the test polygons I was
> using earlier, as createSPComment() seems to handle the test shapes just fine
> (and converting my SpatialPolygonsDataFrame into just a set of SpatialPolygons
> also doesn't help).  But presumably it is better to have createSPComment()
> fixed than to have to work around it in disaggregate().  So I guess I'll look
> at the code for that and see if I can find what's wrong.
> 
> Just in case you want to see this in action, I've attached a little test
> script.  The data set I am working with is available at:
> 
> http://aws.redistricting.state.pa.us/Redistricting/Resources/GISData/2011-Voting-District-Boundary-Shapefiles.zip
> 
>  - Anne
> 
> 
> On 04/29/2017 10:06 AM, Roger Bivand wrote:
>> On Sat, 29 Apr 2017, Anne C. Hanna wrote:
>>
>>> Roger,
>>>
>>> This looks great, and I will try it out ASAP.  I do have one reservation
>>> though --- it seems you are using createSPComment() to reconstruct the
>>> comments, and I have seen some discussion that that may not be reliable in
>>> all cases (e.g. if the initial polygons are wonky in some way).  I don't
>>> know a lot about this, but is it possible that it would be preferable to
>>> parse and appropriately disaggregate the original comments strings (if they
>>> exist), so as to deal slightly more smoothly with such cases (i.e., the
>>> polygons would still be wonky, but at least the hole/polygon matching would
>>> track with whatever was in the original data)?
>>
>> Contributions very welcome - this was code moved from the raster package, so I
>> really have little feeling for why it is necessary.
>>
>> Please provide references and use cases. GEOS is strict in its treatments of
>> geometries, but does work in most cases. People just expressing opinions
>> doesn't help, and may well be misleading. It is possible to clean geometries
>> too, see the cleangeo package. createPolygonsComment probably isn't foolproof,
>> but specific reports help, because then it is possible to do something about
>> it. sp classes didn't use simple features when written because sf was not
>> widely used then - introduced in 2004, when sp was being developed. Using sf
>> helps, and rgeos::createPolygonsComment was a work-around from 2010 when rgeos
>> was written.
>>
>> Even when you use sf in the sf package, you will still run into invalid
>> geometries because the geometries are in fact invalid, the sf package also
>> uses GEOS.
>>
>> Roger
>>
>>>
>>> I also had no success using createSPComment to fix disaggregate()'s output
>>> previously, even though my polygons are perfectly non-wonky, so I am perhaps
>>> a little more untrusting of it than I should be.  But I'll let you know how
>>> this version works with my data.  Thanks for addressing this so quickly!
>>>
>>> - Anne
>>>
>>>
>>> On 04/28/2017 12:11 PM, Roger Bivand wrote:
>>>> I've pushed the fix to my fork:
>>>>
>>>> https://github.com/rsbivand/sp
>>>>
>>>> and created a pull request:
>>>>
>>>> https://github.com/edzer/sp/pull/28
>>>>
>>>> Only one part of a complicated set if nested if() in disaggregate() was adding
>>>> comments, but in some settings the existing comments survived the
>>>> disaggregation. Now the Polygons object comment attribute is re-created for
>>>> all Polygons objects. This is version 1.2-6, also including code changes that
>>>> internally affect rgdal and rgeos - you may like to re-install them from
>>>> source after installing this sp version (shouldn't matter).
>>>>
>>>> Roger
>>>>
>>>> On Fri, 28 Apr 2017, Anne C. Hanna wrote:
>>>>
>>>>> Hello.  I first posted this issue report on the sp GitHub repo
>>>>> (https://github.com/edzer/sp/issues/27) and it was suggested that I redirect
>>>>> it to here.
>>>>>
>>>>> I am working with a geographic dataset with complex borders. The data is
>>>>> stored as a SpatialPolygonsDataFrame.  Each Polygons object in the data frame
>>>>> may be composed of multiple disjoint polygons, and each polygon may have
>>>>> multiple holes.  I want to disaggregate each of the Polygons objects into its
>>>>> individual disjoint polygons and construct an adjacency matrix for all the
>>>>> disjoint components, and I was using disaggregate() to do this.  However,
>>>>> when
>>>>> I run gTouches() on the disaggregated data, in order to compute the
>>>>> adjacencies, I get a number of warnings like this:
>>>>>
>>>>> Error in RGEOSBinPredFunc(spgeom1, spgeom2, byid, "rgeos_touches") : Polygons
>>>>> object missing comment attribute ignoring hole(s).  See function
>>>>> createSPComment.
>>>>>
>>>>> Looking at the Polygons "comment" attributes in the SpatialPolygonsDataFrame
>>>>> output by disaggregate(), I see that the only comment values are "0"
>>>>> (indicating a single polygon with no holes), "0 1" (indicating a single
>>>>> polygon with a single hole), and NULL (apparently no comment was written).
>>>>> Since I know my dataset contains several Polygons objects which are composed
>>>>> of multiple disjoint regions, and also several Polygons which contain more
>>>>> than one hole, this is not the expected result.  In reading the
>>>>> disaggregate()
>>>>> code in the sp GitHub repository (specifically, explodePolygons()), I also
>>>>> can't see anywhere the comment is being added for the cases where a Polygons
>>>>> object has more than two parts or more than two holes.  It actually seems
>>>>> like
>>>>> it's getting carried along almost accidentally in the few cases that do get
>>>>> comments, and neglected otherwise.
>>>>>
>>>>> Assuming I'm not failing to understand the code and the desired behavior
>>>>> (entirely possible, as I am new at working with this software!), this seems
>>>>> suboptimal to me.  My dataset is pretty well-behaved (despite its
>>>>> complexity),
>>>>> so I should be able to fix my issues with judicious application of
>>>>> createPolygonsComment.  But I had a heck of a time figuring out what was
>>>>> going
>>>>> wrong with gTouches, since Polygons comment management appears to be a pretty
>>>>> obscure field (and createSPComment wasn't working for me, for whatever
>>>>> reason).  So it seems like it might be better if disaggregate() just parses
>>>>> and passes along the comments from its input correctly, or, if it's
>>>>> absolutely
>>>>> necessary to not create comments, passes nothing and warns clearly in the
>>>>> manual that comments and associated hole information are being lost.  Passing
>>>>> along comments in some cases while silently dropping them in others seems
>>>>> like
>>>>> kind of the worst of both worlds.
>>>>>
>>>>> I've attached a set of tests I wrote to demonstrate desired/undesired
>>>>> behavior: disaggregate_comment_tests.R.  My R version is 3.4.0, my sp version
>>>>> is 1.2-4, my rgeos version is 0.3-23 (SVN revision 546), and my GEOS runtime
>>>>> version is 3.5.1-CAPI-1.9.1 r4246.  I am using Debian Release 9.0 with kernel
>>>>> version 4.9.0-2-amd64.  I hope this is useful; please let me know if you need
>>>>> more info or if there is a better place to post this issue.
>>>>>
>>>>> - Anne
>>>>>
>>>>
>>>
>>>
>>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 195 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170430/09e590ef/attachment.sig>

