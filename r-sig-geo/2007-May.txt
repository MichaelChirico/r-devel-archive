From mgallay01 at Queens-Belfast.AC.UK  Wed May  2 12:45:07 2007
From: mgallay01 at Queens-Belfast.AC.UK (Michal Gallay)
Date: 02 May 2007 11:45:07 +0100
Subject: [R-sig-Geo] no plots into viewport
Message-ID: <Prayer.1.0.12.0705021145070.14914@localhost.localdomain>

Dear R Users,

could anyone help me and explain why no plot with 'plot'function is plotted 
in the viewport, but as a single plot on the next page?

grid.text() works, but neither xyplot() nor plot() do.

library(graphics)
library(grid)
library(lattice)

top.vp <- viewport(layout = grid.layout(1,2))
vp.plot1 <- viewport(layout.pos.col=1,layout.pos.row=1, name="vp.plot1")
vp.plot2 <- viewport(layout.pos.col=2,layout.pos.row=1, name="vp.plot2")
splot <- vpTree(top.vp, vpList(vp.plot1, vp.plot2))

pushViewport(splot)
seekViewport("vp.plot1")
grid.text("plot1 should be here")
plot <- xyplot(1:10~1:10)
print(plot)

Thank you very much for advice.

Michal



-- 
Michal Gallay

Postgraduate Research Student
School of Geography, Archaeology and Palaeoecology
Queen's University
Belfast BT7 1NN
Northern Ireland

Tel: +44(0)2890 273929
Fax: +44(0)2890 973212
email: mgallay01 at qub.ac.uk
www: www.qub.ac.uk/geog



From Roger.Bivand at nhh.no  Wed May  2 14:01:22 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 2 May 2007 14:01:22 +0200 (CEST)
Subject: [R-sig-Geo] no plots into viewport
In-Reply-To: <Prayer.1.0.12.0705021145070.14914@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0705021353420.21650-100000@reclus.nhh.no>

On 2 May 2007, Michal Gallay wrote:

> Dear R Users,
> 
> could anyone help me and explain why no plot with 'plot'function is plotted 
> in the viewport, but as a single plot on the next page?
> 
> grid.text() works, but neither xyplot() nor plot() do.
> 
> library(graphics)
> library(grid)
> library(lattice)
> 
> top.vp <- viewport(layout = grid.layout(1,2))
> vp.plot1 <- viewport(layout.pos.col=1,layout.pos.row=1, name="vp.plot1")
> vp.plot2 <- viewport(layout.pos.col=2,layout.pos.row=1, name="vp.plot2")
> splot <- vpTree(top.vp, vpList(vp.plot1, vp.plot2))
> 
> pushViewport(splot)
> seekViewport("vp.plot1")
> grid.text("plot1 should be here")
> plot <- xyplot(1:10~1:10)
> print(plot)
> 
> Thank you very much for advice.

Since the density of experienced lattice graphics users on this list 
doesn't seem to be high - you didn't get any reply to your similarly 
careful question a couple of days ago - perhaps you should ask on the R 
help list? I see that:

pushViewport(splot)
seekViewport("vp.plot1")
grid.text("plot1 should be here")
seekViewport("vp.plot2")
grid.text("plot2 should be here")

seems to put the device into two-column mode, but xyplot over-rides the 
viewport set-up.

Roger

> 
> Michal
> 
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ggrothendieck at gmail.com  Wed May  2 14:32:14 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 2 May 2007 08:32:14 -0400
Subject: [R-sig-Geo] no plots into viewport
In-Reply-To: <Prayer.1.0.12.0705021145070.14914@localhost.localdomain>
References: <Prayer.1.0.12.0705021145070.14914@localhost.localdomain>
Message-ID: <971536df0705020532j3c4d8029t57530243131e005c@mail.gmail.com>

Missed this but just noticed it when Roger replied.  Try:

print(plot, newpage = FALSE)

On 02 May 2007 11:45:07 +0100, Michal Gallay
<mgallay01 at queens-belfast.ac.uk> wrote:
> Dear R Users,
>
> could anyone help me and explain why no plot with 'plot'function is plotted
> in the viewport, but as a single plot on the next page?
>
> grid.text() works, but neither xyplot() nor plot() do.
>
> library(graphics)
> library(grid)
> library(lattice)
>
> top.vp <- viewport(layout = grid.layout(1,2))
> vp.plot1 <- viewport(layout.pos.col=1,layout.pos.row=1, name="vp.plot1")
> vp.plot2 <- viewport(layout.pos.col=2,layout.pos.row=1, name="vp.plot2")
> splot <- vpTree(top.vp, vpList(vp.plot1, vp.plot2))
>
> pushViewport(splot)
> seekViewport("vp.plot1")
> grid.text("plot1 should be here")
> plot <- xyplot(1:10~1:10)
> print(plot)
>
> Thank you very much for advice.
>
> Michal
>
>
>
> --
> Michal Gallay
>
> Postgraduate Research Student
> School of Geography, Archaeology and Palaeoecology
> Queen's University
> Belfast BT7 1NN
> Northern Ireland
>
> Tel: +44(0)2890 273929
> Fax: +44(0)2890 973212
> email: mgallay01 at qub.ac.uk
> www: www.qub.ac.uk/geog
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From mgallay01 at Queens-Belfast.AC.UK  Wed May  2 20:54:27 2007
From: mgallay01 at Queens-Belfast.AC.UK (Michal Gallay)
Date: 02 May 2007 19:54:27 +0100
Subject: [R-sig-Geo] no plots into viewport
In-Reply-To: <Pine.LNX.4.44.0705021353420.21650-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0705021353420.21650-100000@reclus.nhh.no>
Message-ID: <Prayer.1.0.12.0705021954270.26005@localhost.localdomain>

On May 2 2007, Roger Bivand wrote:

> On 2 May 2007, Michal Gallay wrote:
> 
> > Dear R Users,
> > 
> > could anyone help me and explain why no plot with 'plot'function is 
> > plotted in the viewport, but as a single plot on the next page?
> > 
> > grid.text() works, but neither xyplot() nor plot() do.
> > 
> > library(graphics)
> > library(grid)
> > library(lattice)
> > 
> > top.vp <- viewport(layout = grid.layout(1,2))
> > vp.plot1 <- viewport(layout.pos.col=1,layout.pos.row=1, name="vp.plot1")
> > vp.plot2 <- viewport(layout.pos.col=2,layout.pos.row=1, name="vp.plot2")
> > splot <- vpTree(top.vp, vpList(vp.plot1, vp.plot2))
> > 
> > pushViewport(splot)
> > seekViewport("vp.plot1")
> > grid.text("plot1 should be here")
> > plot <- xyplot(1:10~1:10)
> > print(plot)
> > 
> > Thank you very much for advice.
> 
> Since the density of experienced lattice graphics users on this list 
> doesn't seem to be high - you didn't get any reply to your similarly 
> careful question a couple of days ago - perhaps you should ask on the R 
> help list? I see that:
> 
> pushViewport(splot)
> seekViewport("vp.plot1")
> grid.text("plot1 should be here")
> seekViewport("vp.plot2")
> grid.text("plot2 should be here")
> 
> seems to put the device into two-column mode, but xyplot over-rides the 
> viewport set-up.
> 
> Roger
> 
Thanks very much Roger,

yes, as Gabor suggested to add 'newpage=FALSE' was the small thing I was 
missing.

Best wishes to all

Michal

-- 
Michal Gallay

Postgraduate Research Student
School of Geography, Archaeology and Palaeoecology
Queen's University
Belfast BT7 1NN
Northern Ireland

Tel: +44(0)2890 273929
Fax: +44(0)2890 973212
email: mgallay01 at qub.ac.uk
www: www.qub.ac.uk/geog



From e.pebesma at geo.uu.nl  Thu May  3 10:58:09 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Thu, 03 May 2007 10:58:09 +0200
Subject: [R-sig-Geo] Several gstat variograms on a single page
In-Reply-To: <Prayer.1.0.12.0704291209590.20146@localhost.localdomain>
References: <Prayer.1.0.12.0704291209590.20146@localhost.localdomain>
Message-ID: <4639A421.30802@geo.uu.nl>

Michal,

I think (and hope for you) it is not necessary to search the solution at 
the level of grid. The plot function for variograms in package gstat is 
a rather (?) thin wrapper around xyplot, so if you carefully read how to 
control the main and sub arguments of ?xyplot, then you're done:

library(gstat)
data(meuse)
coordinates(meuse)=~x+y
v = variogram(log(zinc)~1,meuse)
v.pl = plot(v, main="log-zinc",sub=list(label = "xx", cex=.5))

Next you can print the v.pl etc. objects side by side using print(v.pl, 
more = TRUE) etc.
--
Edzer

Michal Gallay wrote:
> Dear R Users,
>
> I'd like to ask you for help. I am R beginner, tackling R for about three 
> months. I'd like to produce variograms with 0, 1st, 2nd order detrending 
> for 30 ascii grid files containing elevation data.
>
> I've got problem with a simple thing but as it seems to me not as easy one 
> as I thought. I hope I just can't see because of my eyes.
>
> I'd like to plot three gstat variograms in one row on one page, to give 
> them a main title (just single one for all three of the plots), and for 
> each of the plots a subtitle, controlling the fontsize.
>
> At first I used following approach:
>
> #Calculation of the variogram and store it in a variable var...
> g <- gstat(id = "xyz.values", 
> 		formula=z~1, 
> 		locations = ~x+y, 
> 		data = xyz.values,
> 		set = list(fraction = 0.33, width = 2)) 
> var.0ot <- variogram(g)
>
> g <- gstat(id = "xyz.values", 
> 		formula=z~x+y, 
> 		locations = ~x+y, 
> 		data = xyz.values,
> 		set = list(fraction = 0.33, width = 2)) 
> var.1ot <- variogram(g)
>
> g <- gstat(id = "xyz.values", 
> 		formula= z~x + y + I(x*y) + I(x^2) + I(y^2), 
> 		locations = ~x+y, 
> 		data = xyz.values,
> 		set = list(fraction = 0.33, width = 2))
> var.2ot <- variogram(g)
>
> #Store the plot it in a variable plot.var...
>
> plot.var.0ot <- plot(var.0ot, pch=3, cex=0.5, col="red", 
> 			main="No trend removed")
> plot.var.1ot <- plot(var.1ot, pch=3, cex=0.5, col="red", 
>                 main="1st order trend removed")
> plot.var.2ot <- plot(var.2ot, pch=3, cex=0.5, col="red", 
> 		      main="2st order trend removed")
>
> #Print the variograms into regions as specified 
>
> print(plot.var.0ot, split=c(1,1,3,1), more=T)
> print(plot.var.1ot, split=c(2,1,3,1), more=T)
> print(plot.var.2ot, split=c(3,1,3,1), more=F)
>
> The variograms were plotted OK, however, via that approach I couldn't 
> figure out how to change the font size of the subtitles of each graph 
> (specified by main="No trend" etc.)'par'or 'title' didn't work or I didn't 
> use them right way. Further, I wasn't able to plot the main title 
> positioned above the three subtitles and graphs.
>
> So I thought the problem will be sorted if I use 'grid' package as 
> plot.gstat Variogram function obeys 'lattice'.
>
> I've created a viewport of 3 by 3 grid layout (after Paul Murrell 
> http://www.stat.auckland.ac.nz/~paul/grid/grid.html#docs) and viewport 
> tree. First row accommodates main title. This works fine. The second row 
> and first column includes just the subtitle 1, second column subtitle 2, 
> third column subtitle 3.
>
> Third row is dedicated to the variogram plots. However, they are not 
> plotted into the viewports plot1, plot2, plot3, but on the whole page.
>
> The approach was as follows:
>
> top.vp <- viewport(layout = grid.layout(3,3,widths=unit(c(1,1,1), c("null", 
> "null", "null")), heights=unit(c(1,1,1), c("lines", "lines", "null"))))
>
> vp.main.title <- viewport(layout.pos.col=c(1:3),layout.pos.row=1, 
> name="vp.main.title") vp.subtitle1 <- 
> viewport(layout.pos.col=1,layout.pos.row=2, name="vp.subtitle1") 
> vp.subtitle2 <- viewport(layout.pos.col=2,layout.pos.row=2, 
> name="vp.subtitle2") vp.subtitle3 <- 
> viewport(layout.pos.col=3,layout.pos.row=2, name="vp.subtitle3") vp.plot1 
> <- viewport(layout.pos.col=1,layout.pos.row=3, name="vp.plot1") vp.plot2 <- 
> viewport(layout.pos.col=2,layout.pos.row=3, name="vp.plot2") vp.plot3 <- 
> viewport(layout.pos.col=3,layout.pos.row=3, name="vp.plot3")
>
> splot <- vpTree(top.vp, vpList(vp.main.title, vp.subtitle1, vp.subtitle2, 
> vp.subtitle3,vp.plot1, vp.plot2, vp.plot3))
>
> pushViewport(splot)
>
> seekViewport("vp.main.title")
> grid.text("main title", gp=gpar(cex=1.2))
> seekViewport("vp.subtitle1")
> grid.text("subtitle 1", gp=gpar(cex=0.8))
> seekViewport("vp.plot1")
> plot(var.0ot, pch=3, cex=0.5, col="red")
>
> I much appreciate any suggestions and criticism.
> Thank you very much in advance.
>
> Michal
>
>



From e.pebesma at geo.uu.nl  Thu May  3 11:06:16 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Thu, 03 May 2007 11:06:16 +0200
Subject: [R-sig-Geo] Simulating raster or spatialgriddataframe images
In-Reply-To: <000001c789ff$ecbd4040$0302a8c0@D3K86YB1>
References: <2E9C414912813E4EB981326983E0A10402E528E6@inboexch.inbo.be>	<Pine.LNX.4.44.0704271222380.16144-100000@reclus.nhh.no>
	<000001c789ff$ecbd4040$0302a8c0@D3K86YB1>
Message-ID: <4639A608.5060105@geo.uu.nl>

Andrew,

If you want to simulate particular random fields that are Gaussian or 
binary, described by a second order properties (mean, variogram), then 
you may want to look into the simulation capabilities of packages 
RandomFields or gstat. If you want to simulate objects (circles, 
ellipses, stars), then you may need to do some point-in-polygon stuff, 
which is fairly easy to automate. Perhaps using overlay (in package sp) 
if you have the object as a SpatialPolygons[DataFrame] object.

Hth,
--
Edzer

Andrew Niccolai wrote:
> Greetings,
>
> At the risk of being ridiculed for such a general question, can anyone offer
> suggestions or decent code to simulate grid images with specific patterns?
> I have been reading R Graphics and I am not sure if a grob will suffice, but
> I would like to create a "controlled" circular or rectangular image in which
> I can insert a star pattern or a pattern of high valued clumps evenly
> distributed around the image.  I am interested in creating these controlled
> or simulated images in order to better understand the ability of gstar and
> local moran's I statistics in discriminating among image objects with
> different internal patterns but that share similar overall averages and
> standard deviations.  Can anyone suggest code or techniques inside of R that
> will allow me to create a series of SGDF objects that have a specific set of
> internal patterns associated with them.
>
> Thanks in advance and I apologize for the "vagueness" of the query.
>
> Andrew Niccolai
> Doctoral Candidate
> Yale School of Forestry
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From mgallay01 at Queens-Belfast.AC.UK  Fri May  4 00:05:49 2007
From: mgallay01 at Queens-Belfast.AC.UK (Michal Gallay)
Date: 03 May 2007 23:05:49 +0100
Subject: [R-sig-Geo] Several gstat variograms on a single page
In-Reply-To: <4639A421.30802@geo.uu.nl>
References: <Prayer.1.0.12.0704291209590.20146@localhost.localdomain>
	<4639A421.30802@geo.uu.nl>
Message-ID: <Prayer.1.0.12.0705032305490.7743@localhost.localdomain>

Thank you Edzer,

you're right, the layout I wanted to get can be produced just using simple 
'graphics' functions. I've already 'discovered' how to do it.

At this stage, I find Lattice and Grid graphics too complex. The biggest 
problem is to understand how particular features of a lattice/grid plot are 
controlled. However, step by step, I am getting there.

Best wishes,

Michal

On May 3 2007, Edzer J. Pebesma wrote:

> Michal,
> 
> I think (and hope for you) it is not necessary to search the solution at 
> the level of grid. The plot function for variograms in package gstat is 
> a rather (?) thin wrapper around xyplot, so if you carefully read how to 
> control the main and sub arguments of ?xyplot, then you're done:
> 
> library(gstat)
> data(meuse)
> coordinates(meuse)=~x+y
> v = variogram(log(zinc)~1,meuse)
> v.pl = plot(v, main="log-zinc",sub=list(label = "xx", cex=.5))
> 
> Next you can print the v.pl etc. objects side by side using print(v.pl, 
> more = TRUE) etc.
> --
> Edzer
> 
> Michal Gallay wrote:
> > Dear R Users,
> >
> > I'd like to ask you for help. I am R beginner, tackling R for about 
> > three months. I'd like to produce variograms with 0, 1st, 2nd order 
> > detrending for 30 ascii grid files containing elevation data.
> >
> > I've got problem with a simple thing but as it seems to me not as easy 
> > one as I thought. I hope I just can't see because of my eyes.
> >
> > I'd like to plot three gstat variograms in one row on one page, to give 
> > them a main title (just single one for all three of the plots), and for 
> > each of the plots a subtitle, controlling the fontsize.
> >
> > At first I used following approach:
> >
> > #Calculation of the variogram and store it in a variable var...
> > g <- gstat(id = "xyz.values", 
> > 		formula=z~1, 
> > 		locations = ~x+y, 
> > 		data = xyz.values,
> > 		set = list(fraction = 0.33, width = 2)) 
> > var.0ot <- variogram(g)
> >
> > g <- gstat(id = "xyz.values", 
> > 		formula=z~x+y, 
> > 		locations = ~x+y, 
> > 		data = xyz.values,
> > 		set = list(fraction = 0.33, width = 2)) 
> > var.1ot <- variogram(g)
> >
> > g <- gstat(id = "xyz.values", 
> > 		formula= z~x + y + I(x*y) + I(x^2) + I(y^2), 
> > 		locations = ~x+y, 
> > 		data = xyz.values,
> > 		set = list(fraction = 0.33, width = 2))
> > var.2ot <- variogram(g)
> >
> > #Store the plot it in a variable plot.var...
> >
> > plot.var.0ot <- plot(var.0ot, pch=3, cex=0.5, col="red", 
> > 			main="No trend removed")
> > plot.var.1ot <- plot(var.1ot, pch=3, cex=0.5, col="red", 
> >                 main="1st order trend removed")
> > plot.var.2ot <- plot(var.2ot, pch=3, cex=0.5, col="red", 
> > 		      main="2st order trend removed")
> >
> > #Print the variograms into regions as specified 
> >
> > print(plot.var.0ot, split=c(1,1,3,1), more=T)
> > print(plot.var.1ot, split=c(2,1,3,1), more=T)
> > print(plot.var.2ot, split=c(3,1,3,1), more=F)
> >
> > The variograms were plotted OK, however, via that approach I couldn't 
> > figure out how to change the font size of the subtitles of each graph 
> > (specified by main="No trend" etc.)'par'or 'title' didn't work or I 
> > didn't use them right way. Further, I wasn't able to plot the main 
> > title positioned above the three subtitles and graphs.
> >
> > So I thought the problem will be sorted if I use 'grid' package as 
> > plot.gstat Variogram function obeys 'lattice'.
> >
> > I've created a viewport of 3 by 3 grid layout (after Paul Murrell 
> > http://www.stat.auckland.ac.nz/~paul/grid/grid.html#docs) and viewport 
> > tree. First row accommodates main title. This works fine. The second 
> > row and first column includes just the subtitle 1, second column 
> > subtitle 2, third column subtitle 3.
> >
> > Third row is dedicated to the variogram plots. However, they are not 
> > plotted into the viewports plot1, plot2, plot3, but on the whole page.
> >
> > The approach was as follows:
> >
> > top.vp <- viewport(layout = grid.layout(3,3,widths=unit(c(1,1,1), 
> > c("null", "null", "null")), heights=unit(c(1,1,1), c("lines", "lines", 
> > "null"))))
> >
> > vp.main.title <- viewport(layout.pos.col=c(1:3),layout.pos.row=1, 
> > name="vp.main.title") vp.subtitle1 <- 
> > viewport(layout.pos.col=1,layout.pos.row=2, name="vp.subtitle1") 
> > vp.subtitle2 <- viewport(layout.pos.col=2,layout.pos.row=2, 
> > name="vp.subtitle2") vp.subtitle3 <- 
> > viewport(layout.pos.col=3,layout.pos.row=2, name="vp.subtitle3") 
> > vp.plot1 <- viewport(layout.pos.col=1,layout.pos.row=3, 
> > name="vp.plot1") vp.plot2 <- 
> > viewport(layout.pos.col=2,layout.pos.row=3, name="vp.plot2") vp.plot3 
> > <- viewport(layout.pos.col=3,layout.pos.row=3, name="vp.plot3")
> >
> > splot <- vpTree(top.vp, vpList(vp.main.title, vp.subtitle1, 
> > vp.subtitle2, vp.subtitle3,vp.plot1, vp.plot2, vp.plot3))
> >
> > pushViewport(splot)
> >
> > seekViewport("vp.main.title")
> > grid.text("main title", gp=gpar(cex=1.2))
> > seekViewport("vp.subtitle1")
> > grid.text("subtitle 1", gp=gpar(cex=0.8))
> > seekViewport("vp.plot1")
> > plot(var.0ot, pch=3, cex=0.5, col="red")
> >
> > I much appreciate any suggestions and criticism.
> > Thank you very much in advance.
> >
> > Michal
> >
> >
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Michal Gallay

Postgraduate Research Student
School of Geography, Archaeology and Palaeoecology
Queen's University
Belfast BT7 1NN
Northern Ireland

Tel: +44(0)2890 273929
Fax: +44(0)2890 973212
email: mgallay01 at qub.ac.uk
www: www.qub.ac.uk/geog



From tom.boonen.maiden at gmail.com  Fri May  4 18:40:25 2007
From: tom.boonen.maiden at gmail.com (Tom Boonen)
Date: Fri, 4 May 2007 12:40:25 -0400
Subject: [R-sig-Geo] merge vector into a SpatialPolygonsDataFrame
Message-ID: <cc088e260705040940h1d071fday6d3e846bf718db97@mail.gmail.com>

Hi,

I am a newbie to the spatial stats (but know R). I would like to plot
a map where the colors for the various polygons are given by the value
of a vector.

# So I do my vector
# vector with 437 datapoints for 437 polygons
x <- 1:437

I# Then I read in my shapefile which works fine:
myshp   <- readShapePoly("cd99_109.shp")

> summary(myshp)
Object of class SpatialPolygonsDataFrame
Coordinates:
          min       max
r1 -179.14734 179.77847
r2   17.88481  71.35256
Is projected: NA
proj4string : [NA]
Data attributes:
     STATE           CD      LSAD          NAME        GEOCODE
                               LSAD_TRANS
 06     : 53   01     : 43   C1:  7   1      : 43   0101   :  1
Congressional District                   :428
 48     : 32   02     : 43   C2:428   2      : 43   0102   :  1
Congressional District (at Large)        :  7
 36     : 29   03     : 38   C3:  1   3      : 38   0103   :  1
Delegate District (at Large)             :  1
 12     : 25   04     : 33   C4:  1   4      : 33   0104   :  1
Resident Commissioner District (at Large):  1
 17     : 19   05     : 30            5      : 30   0105   :  1
 42     : 19   06     : 26            6      : 26   0106   :  1
 (Other):260   (Other):224            (Other):224   (Other):431

Now I would like to plot all 437 polygons, with the colors of the
polygons determined by the values of my x vector.

# this works fine
spplot(myshp, zcol = c("GEOCODE") ) plots the map niceley

# but this does not
spplot(myshp, zcol = x )

because x is not part of myshp. How can I merge the x data into the
SpatialPolygonsDataFrame?

Alternatively, I could replace the names of the GEOCODES, but:

myshp$GEOCODE[1] <- x[1] also does not work.

Thanks for your help.

Tom



From Roger.Bivand at nhh.no  Fri May  4 21:31:52 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 4 May 2007 21:31:52 +0200 (CEST)
Subject: [R-sig-Geo] merge vector into a SpatialPolygonsDataFrame
In-Reply-To: <cc088e260705040940h1d071fday6d3e846bf718db97@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0705042117130.28218-100000@reclus.nhh.no>

On Fri, 4 May 2007, Tom Boonen wrote:

> Hi,
> 
> I am a newbie to the spatial stats (but know R). I would like to plot
> a map where the colors for the various polygons are given by the value
> of a vector.
> 
> # So I do my vector
> # vector with 437 datapoints for 437 polygons
> x <- 1:437
> 
> I# Then I read in my shapefile which works fine:
> myshp   <- readShapePoly("cd99_109.shp")
> 
> > summary(myshp)
> Object of class SpatialPolygonsDataFrame
> Coordinates:
>           min       max
> r1 -179.14734 179.77847
> r2   17.88481  71.35256
> Is projected: NA
> proj4string : [NA]
> Data attributes:
>      STATE           CD      LSAD          NAME        GEOCODE
>                                LSAD_TRANS
>  06     : 53   01     : 43   C1:  7   1      : 43   0101   :  1
> Congressional District                   :428
>  48     : 32   02     : 43   C2:428   2      : 43   0102   :  1
> Congressional District (at Large)        :  7
>  36     : 29   03     : 38   C3:  1   3      : 38   0103   :  1
> Delegate District (at Large)             :  1
>  12     : 25   04     : 33   C4:  1   4      : 33   0104   :  1
> Resident Commissioner District (at Large):  1
>  17     : 19   05     : 30            5      : 30   0105   :  1
>  42     : 19   06     : 26            6      : 26   0106   :  1
>  (Other):260   (Other):224            (Other):224   (Other):431
> 
> Now I would like to plot all 437 polygons, with the colors of the
> polygons determined by the values of my x vector.
> 
> # this works fine
> spplot(myshp, zcol = c("GEOCODE") ) plots the map niceley
> 
> # but this does not
> spplot(myshp, zcol = x )
> 
> because x is not part of myshp. How can I merge the x data into the
> SpatialPolygonsDataFrame?

The direct answer is:

myshp$x <- x
spplot(myshp, zcol = "x")

but for this data set, two more steps might be helpful, because the 
coordinates are geographic, and cross 180 degrees:

proj4string(myshp) <- CRS("+proj=longlat")

to set the projection, and

rSP <- recenter(myshp)
myshp1 <- SpatialPolygonsDataFrame(rSP, data=as(myshp, "data.frame"))

to avoid the split between the -180 and +180 longitudes. 

spplot(myshp1, "x")

ought now to be OK, but gives a lot of visual weight to Alaska.

Hope this helps,

Roger

> 
> Alternatively, I could replace the names of the GEOCODES, but:
> 
> myshp$GEOCODE[1] <- x[1] also does not work.
> 
> Thanks for your help.
> 
> Tom
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From pierces1 at msu.edu  Fri May  4 23:44:57 2007
From: pierces1 at msu.edu (Steven J. Pierce)
Date: Fri, 4 May 2007 17:44:57 -0400
Subject: [R-sig-Geo] Several gstat variograms on a single page
In-Reply-To: <Prayer.1.0.12.0705032305490.7743@localhost.localdomain>
References: <Prayer.1.0.12.0704291209590.20146@localhost.localdomain><4639A421.30802@geo.uu.nl>
	<Prayer.1.0.12.0705032305490.7743@localhost.localdomain>
Message-ID: <001d01c78e95$75b86ac0$0a00a8c0@TheVoid>

Michal,

The best source I've found for understanding how to work with the graphics
capabilities in R is a recently published book:

Murrell, Paul (2006). R Graphics. Boca Raton, FL: Chapman & Hall/CRC. 
http://www.amazon.com/Graphics-Computer-Science-Data-Analysis/dp/158488486X/
ref=pd_bbs_sr_1/103-4142948-9887845?ie=UTF8&s=books&qid=1178315019&sr=8-1


Steven J. Pierce, M.S.
Doctoral Student in Ecological/Community Psychology
Department of Psychology
Michigan State University
240B Psychology Building
East Lansing, MI 48824-1116

E-mail: pierces1 at msu.edu
Web: http://www.psychology.msu.edu/eco/


-----Original Message-----
From: Michal Gallay [mailto:mgallay01 at Queens-Belfast.AC.UK] 
Sent: Thursday, May 03, 2007 6:06 PM
To: Edzer J. Pebesma
Cc: R geo forum; Michal Gallay
Subject: Re: [R-sig-Geo] Several gstat variograms on a single page

Thank you Edzer,

you're right, the layout I wanted to get can be produced just using simple
'graphics' functions. I've already 'discovered' how to do it.

At this stage, I find Lattice and Grid graphics too complex. The biggest
problem is to understand how particular features of a lattice/grid plot are
controlled. However, step by step, I am getting there.

Best wishes,

Michal

On May 3 2007, Edzer J. Pebesma wrote:

> Michal,
> 
> I think (and hope for you) it is not necessary to search the solution 
> at the level of grid. The plot function for variograms in package 
> gstat is a rather (?) thin wrapper around xyplot, so if you carefully 
> read how to control the main and sub arguments of ?xyplot, then you're
done:
> 
> library(gstat)
> data(meuse)
> coordinates(meuse)=~x+y
> v = variogram(log(zinc)~1,meuse)
> v.pl = plot(v, main="log-zinc",sub=list(label = "xx", cex=.5))
> 
> Next you can print the v.pl etc. objects side by side using 
> print(v.pl, more = TRUE) etc.
> --
> Edzer
> 
> Michal Gallay wrote:
> > Dear R Users,
> >
> > I'd like to ask you for help. I am R beginner, tackling R for about 
> > three months. I'd like to produce variograms with 0, 1st, 2nd order 
> > detrending for 30 ascii grid files containing elevation data.
> >
> > I've got problem with a simple thing but as it seems to me not as 
> > easy one as I thought. I hope I just can't see because of my eyes.
> >
> > I'd like to plot three gstat variograms in one row on one page, to 
> > give them a main title (just single one for all three of the plots), 
> > and for each of the plots a subtitle, controlling the fontsize.
> >
> > At first I used following approach:
> >
> > #Calculation of the variogram and store it in a variable var...
> > g <- gstat(id = "xyz.values", 
> > 		formula=z~1, 
> > 		locations = ~x+y, 
> > 		data = xyz.values,
> > 		set = list(fraction = 0.33, width = 2)) var.0ot <-
variogram(g)
> >
> > g <- gstat(id = "xyz.values", 
> > 		formula=z~x+y, 
> > 		locations = ~x+y, 
> > 		data = xyz.values,
> > 		set = list(fraction = 0.33, width = 2)) var.1ot <-
variogram(g)
> >
> > g <- gstat(id = "xyz.values", 
> > 		formula= z~x + y + I(x*y) + I(x^2) + I(y^2), 
> > 		locations = ~x+y, 
> > 		data = xyz.values,
> > 		set = list(fraction = 0.33, width = 2)) var.2ot <-
variogram(g)
> >
> > #Store the plot it in a variable plot.var...
> >
> > plot.var.0ot <- plot(var.0ot, pch=3, cex=0.5, col="red", 
> > 			main="No trend removed")
> > plot.var.1ot <- plot(var.1ot, pch=3, cex=0.5, col="red", 
> >                 main="1st order trend removed") plot.var.2ot <- 
> > plot(var.2ot, pch=3, cex=0.5, col="red",
> > 		      main="2st order trend removed")
> >
> > #Print the variograms into regions as specified
> >
> > print(plot.var.0ot, split=c(1,1,3,1), more=T) print(plot.var.1ot, 
> > split=c(2,1,3,1), more=T) print(plot.var.2ot, split=c(3,1,3,1), 
> > more=F)
> >
> > The variograms were plotted OK, however, via that approach I 
> > couldn't figure out how to change the font size of the subtitles of 
> > each graph (specified by main="No trend" etc.)'par'or 'title' didn't 
> > work or I didn't use them right way. Further, I wasn't able to plot 
> > the main title positioned above the three subtitles and graphs.
> >
> > So I thought the problem will be sorted if I use 'grid' package as 
> > plot.gstat Variogram function obeys 'lattice'.
> >
> > I've created a viewport of 3 by 3 grid layout (after Paul Murrell
> > http://www.stat.auckland.ac.nz/~paul/grid/grid.html#docs) and 
> > viewport tree. First row accommodates main title. This works fine. 
> > The second row and first column includes just the subtitle 1, second 
> > column subtitle 2, third column subtitle 3.
> >
> > Third row is dedicated to the variogram plots. However, they are not 
> > plotted into the viewports plot1, plot2, plot3, but on the whole page.
> >
> > The approach was as follows:
> >
> > top.vp <- viewport(layout = grid.layout(3,3,widths=unit(c(1,1,1),
> > c("null", "null", "null")), heights=unit(c(1,1,1), c("lines", 
> > "lines",
> > "null"))))
> >
> > vp.main.title <- viewport(layout.pos.col=c(1:3),layout.pos.row=1,
> > name="vp.main.title") vp.subtitle1 <- 
> > viewport(layout.pos.col=1,layout.pos.row=2, name="vp.subtitle1")
> > vp.subtitle2 <- viewport(layout.pos.col=2,layout.pos.row=2,
> > name="vp.subtitle2") vp.subtitle3 <- 
> > viewport(layout.pos.col=3,layout.pos.row=2, name="vp.subtitle3")
> > vp.plot1 <- viewport(layout.pos.col=1,layout.pos.row=3,
> > name="vp.plot1") vp.plot2 <-
> > viewport(layout.pos.col=2,layout.pos.row=3, name="vp.plot2") 
> > vp.plot3
> > <- viewport(layout.pos.col=3,layout.pos.row=3, name="vp.plot3")
> >
> > splot <- vpTree(top.vp, vpList(vp.main.title, vp.subtitle1, 
> > vp.subtitle2, vp.subtitle3,vp.plot1, vp.plot2, vp.plot3))
> >
> > pushViewport(splot)
> >
> > seekViewport("vp.main.title")
> > grid.text("main title", gp=gpar(cex=1.2))
> > seekViewport("vp.subtitle1")
> > grid.text("subtitle 1", gp=gpar(cex=0.8))
> > seekViewport("vp.plot1")
> > plot(var.0ot, pch=3, cex=0.5, col="red")
> >
> > I much appreciate any suggestions and criticism.
> > Thank you very much in advance.
> >
> > Michal
> >
> >
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

--
Michal Gallay

Postgraduate Research Student
School of Geography, Archaeology and Palaeoecology Queen's University
Belfast BT7 1NN Northern Ireland

Tel: +44(0)2890 273929
Fax: +44(0)2890 973212
email: mgallay01 at qub.ac.uk
www: www.qub.ac.uk/geog



From loecher at eden.rutgers.edu  Mon May  7 02:31:44 2007
From: loecher at eden.rutgers.edu (Markus Loecher)
Date: Sun, 06 May 2007 20:31:44 -0400
Subject: [R-sig-Geo] geocoding street addresses
In-Reply-To: <mailman.13.1178272806.26606.r-sig-geo@stat.math.ethz.ch>
References: <mailman.13.1178272806.26606.r-sig-geo@stat.math.ethz.ch>
Message-ID: <20070507003050.0C3F07CF@annwn14.rutgers.edu>

Hello,
is there an R package which would let me geocode street addresses 
into lat/lon pairs ? I have searched quite a bit but it seems as if 
the ESRI software is my only way to go ?

Thanks !

Markus



From putler at sauder.ubc.ca  Mon May  7 02:37:01 2007
From: putler at sauder.ubc.ca (Dan Putler)
Date: Sun, 6 May 2007 17:37:01 -0700
Subject: [R-sig-Geo] geocoding street addresses
In-Reply-To: <20070507003050.0C3F07CF@annwn14.rutgers.edu>
References: <mailman.13.1178272806.26606.r-sig-geo@stat.math.ethz.ch>
	<20070507003050.0C3F07CF@annwn14.rutgers.edu>
Message-ID: <B8545342-E9DC-4B3C-BE96-D5194175549C@sauder.ubc.ca>

Hi Markus,

There is a command line program called PAGC that does postal address  
geocoding, which is open source (written to C). A link to it is:
http://sourceforge.net/projects/pagc/

Dan

On 6-May-07, at 5:31 PM, Markus Loecher wrote:

> Hello,
> is there an R package which would let me geocode street addresses
> into lat/lon pairs ? I have searched quite a bit but it seems as if
> the ESRI software is my only way to go ?
>
> Thanks !
>
> Markus
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo



From mdsumner at utas.edu.au  Mon May  7 02:45:22 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Mon, 07 May 2007 10:45:22 +1000
Subject: [R-sig-Geo] geocoding street addresses
In-Reply-To: <20070507003050.0C3F07CF@annwn14.rutgers.edu>
References: <mailman.13.1178272806.26606.r-sig-geo@stat.math.ethz.ch>
	<20070507003050.0C3F07CF@annwn14.rutgers.edu>
Message-ID: <463E76A2.1080609@utas.edu.au>

Markus Loecher wrote:
> Hello,
> is there an R package which would let me geocode street addresses 
> into lat/lon pairs ? I have searched quite a bit but it seems as if 
> the ESRI software is my only way to go ?
>   
 Manifold GIS also has geocoding (not open source).

Cheers, Mike.



From ggrothendieck at gmail.com  Mon May  7 03:10:18 2007
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 6 May 2007 21:10:18 -0400
Subject: [R-sig-Geo] geocoding street addresses
In-Reply-To: <20070507003050.0C3F07CF@annwn14.rutgers.edu>
References: <mailman.13.1178272806.26606.r-sig-geo@stat.math.ethz.ch>
	<20070507003050.0C3F07CF@annwn14.rutgers.edu>
Message-ID: <971536df0705061810o35a8879fu515990f79fc9fe42@mail.gmail.com>

On 5/6/07, Markus Loecher <loecher at eden.rutgers.edu> wrote:

> is there an R package which would let me geocode street addresses
> into lat/lon pairs ? I have searched quite a bit but it seems as if
> the ESRI software is my only way to go ?

Non-commercial terms of use or commercial with payment but if that's
ok then for US addresses:

URL <- "http://rpc.geocoder.us/service/csv?address"
addr <- "1600 Pennsylvania  Ave, Washington DC"
addr <- gsub(" ", "+", addr)
addr <- gsub(",", "%2C", addr)
tmp <- tempfile()
download.file(paste(URL, addr, sep = "="), destfile = tmp)
cn <- c("Lat", "Long", "Addr", "City", "State", "zip")
read.table(tmp, sep = ",", col.names = cn)

See web indicated web site for more info.



From Roger.Bivand at nhh.no  Mon May  7 08:52:39 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 7 May 2007 08:52:39 +0200 (CEST)
Subject: [R-sig-Geo] geocoding street addresses
In-Reply-To: <971536df0705061810o35a8879fu515990f79fc9fe42@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0705070829420.15694-100000@reclus.nhh.no>

On Sun, 6 May 2007, Gabor Grothendieck wrote:

> On 5/6/07, Markus Loecher <loecher at eden.rutgers.edu> wrote:
> 
> > is there an R package which would let me geocode street addresses
> > into lat/lon pairs ? I have searched quite a bit but it seems as if
> > the ESRI software is my only way to go ?
> 
> Non-commercial terms of use or commercial with payment but if that's
> ok then for US addresses:
> 
> URL <- "http://rpc.geocoder.us/service/csv?address"
> addr <- "1600 Pennsylvania  Ave, Washington DC"
> addr <- gsub(" ", "+", addr)
> addr <- gsub(",", "%2C", addr)
> tmp <- tempfile()
> download.file(paste(URL, addr, sep = "="), destfile = tmp)
> cn <- c("Lat", "Long", "Addr", "City", "State", "zip")
> read.table(tmp, sep = ",", col.names = cn)
> 
> See web indicated web site for more info.

This approach is also documented in Hack 80, pp. 403-408, in Erle, Gibson
and Walsh (2005) Mapping Hacks, O'Reilly. The hack shows how to ask for
multiple points, and how to (try to) disambiguate multiple replies to a
single query. Sadly, geocoding outside the US is far more difficult.

Roger

> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Thierry.ONKELINX at inbo.be  Mon May  7 16:49:23 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Mon, 7 May 2007 16:49:23 +0200
Subject: [R-sig-Geo] variograms do not satisfy a legal model
Message-ID: <2E9C414912813E4EB981326983E0A10402F18D9F@inboexch.inbo.be>

Dear useRs,

I'm trying to do some cokriging. I try fit the variograms with fit.lmc
(gstat package).

#Problem 1: how to get the model in an elegant way into the gstat
object? This is how I do it now.
library("gstat")
data(meuse)
g <- gstat(id = "ln.zinc", formula = log(zinc)~1, locations = ~x+y, data
= meuse)
g <- gstat(g, id = "ln.lead", formula = log(lead)~1, locations = ~x+y,
data = meuse)
#fit the variogram models using fit.lmc
model <- fit.lmc(variogram(g), g, model = vgm(.55, "Sph", 900, .05),
fit.ranges = FALSE)
g <- gstat(g, id = "ln.zinc", model = model[["model"]][["ln.zinc"]])
g <- gstat(g, id = "ln.lead", model = model[["model"]][["ln.lead"]])
g <- gstat(g, id = c("ln.zinc", "ln.lead"), model =
model[["model"]][["ln.zinc.ln.lead"]])
predict(g, newdata = meuse)

#Problem 2. the gstat object doesn't accept different ranges. Or am I
doing something wrong?
library("gstat")
data(meuse)
g <- gstat(id = "ln.zinc", formula = log(zinc)~1, locations = ~x+y, data
= meuse)
g <- gstat(g, id = "ln.lead", formula = log(lead)~1, locations = ~x+y,
data = meuse)
# examine variograms and cross variogram:
plot(variogram(g))
# enter direct variograms:
g <- gstat(g, id = "ln.zinc", model = vgm(.55, "Sph", 800, .05))
g <- gstat(g, id = "ln.lead", model = vgm(.55, "Sph", 900, .05))
# enter cross variogram:
g <- gstat(g, id = c("ln.zinc", "ln.lead"), model = vgm(.47, "Sph", 900,
.03))
predict(g, newdata = meuse)
# Error in predict.gstat(g, newdata = meuse) : gstat: value not allowed
for: variograms do not satisfy a legal model

Thanks,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney



From e.pebesma at geo.uu.nl  Mon May  7 17:00:10 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Mon, 07 May 2007 17:00:10 +0200
Subject: [R-sig-Geo] variograms do not satisfy a legal model
In-Reply-To: <2E9C414912813E4EB981326983E0A10402F18D9F@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A10402F18D9F@inboexch.inbo.be>
Message-ID: <463F3EFA.2000600@geo.uu.nl>

Thierry, you'll have to find out what the linear model of 
regionalization or intrinsic correlation mean; they are legal models, 
meaning that they lead to predictions with guaranteed non-negative 
prediction variances. There's a way to override the check (meaning it 
will not stop on error) but not to get that guarantee.
--
Edzer

ONKELINX, Thierry wrote:
> Dear useRs,
>
> I'm trying to do some cokriging. I try fit the variograms with fit.lmc
> (gstat package).
>
> #Problem 1: how to get the model in an elegant way into the gstat
> object? This is how I do it now.
> library("gstat")
> data(meuse)
> g <- gstat(id = "ln.zinc", formula = log(zinc)~1, locations = ~x+y, data
> = meuse)
> g <- gstat(g, id = "ln.lead", formula = log(lead)~1, locations = ~x+y,
> data = meuse)
> #fit the variogram models using fit.lmc
> model <- fit.lmc(variogram(g), g, model = vgm(.55, "Sph", 900, .05),
> fit.ranges = FALSE)
> g <- gstat(g, id = "ln.zinc", model = model[["model"]][["ln.zinc"]])
> g <- gstat(g, id = "ln.lead", model = model[["model"]][["ln.lead"]])
> g <- gstat(g, id = c("ln.zinc", "ln.lead"), model =
> model[["model"]][["ln.zinc.ln.lead"]])
> predict(g, newdata = meuse)
>
> #Problem 2. the gstat object doesn't accept different ranges. Or am I
> doing something wrong?
> library("gstat")
> data(meuse)
> g <- gstat(id = "ln.zinc", formula = log(zinc)~1, locations = ~x+y, data
> = meuse)
> g <- gstat(g, id = "ln.lead", formula = log(lead)~1, locations = ~x+y,
> data = meuse)
> # examine variograms and cross variogram:
> plot(variogram(g))
> # enter direct variograms:
> g <- gstat(g, id = "ln.zinc", model = vgm(.55, "Sph", 800, .05))
> g <- gstat(g, id = "ln.lead", model = vgm(.55, "Sph", 900, .05))
> # enter cross variogram:
> g <- gstat(g, id = c("ln.zinc", "ln.lead"), model = vgm(.47, "Sph", 900,
> .03))
> predict(g, newdata = meuse)
> # Error in predict.gstat(g, newdata = meuse) : gstat: value not allowed
> for: variograms do not satisfy a legal model
>
> Thanks,
>
> Thierry
>
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be 
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From perrygeo at gmail.com  Mon May  7 20:46:02 2007
From: perrygeo at gmail.com (Matthew Perry)
Date: Mon, 7 May 2007 11:46:02 -0700
Subject: [R-sig-Geo] geocoding street addresses
In-Reply-To: <Pine.LNX.4.44.0705070829420.15694-100000@reclus.nhh.no>
References: <971536df0705061810o35a8879fu515990f79fc9fe42@mail.gmail.com>
	<Pine.LNX.4.44.0705070829420.15694-100000@reclus.nhh.no>
Message-ID: <5383fa5e0705071146j7fce8a3cxecc3ea388f8fdf1b@mail.gmail.com>

On 5/6/07, Roger Bivand <Roger.Bivand at nhh.no> wrote:
> This approach is also documented in Hack 80, pp. 403-408, in Erle, Gibson
> and Walsh (2005) Mapping Hacks, O'Reilly. The hack shows how to ask for
> multiple points, and how to (try to) disambiguate multiple replies to a
> single query. Sadly, geocoding outside the US is far more difficult.
>

You may also want to consider using one of the other major geocoding
engines if their terms of use fit your needs. I know the Yahoo
geocoder has decent (though far from perfect) geocoding globally.

Another solution might be to use the python library geopy
(http://exogen.case.edu/projects/geopy/) which provides a consistent
api on top of the major geocoding services.

-- 
Matthew T. Perry
http://www.perrygeo.net

"You never change things by fighting the existing reality.
To change something, build a new model that makes
the existing model obsolete" - R. Buckminster Fuller



From Thierry.ONKELINX at inbo.be  Tue May  8 16:16:20 2007
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Tue, 8 May 2007 16:16:20 +0200
Subject: [R-sig-Geo] variograms do not satisfy a legal model
In-Reply-To: <463F3EFA.2000600@geo.uu.nl>
Message-ID: <2E9C414912813E4EB981326983E0A10402F1924E@inboexch.inbo.be>

Edzer,

Could you tell me how to override this check? I have been reading the
helpfile but I can't find it.

Thanks,

Thierry

------------------------------------------------------------------------
----
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
and Forest
Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
methodology and quality assurance
Gaverstraat 4
9500 Geraardsbergen
Belgium
tel. + 32 54/436 185
Thierry.Onkelinx at inbo.be
www.inbo.be 

Do not put your faith in what statistics say until you have carefully
considered what they do not say.  ~William W. Watt
A statistical analysis, properly conducted, is a delicate dissection of
uncertainties, a surgery of suppositions. ~M.J.Moroney

 

> -----Oorspronkelijk bericht-----
> Van: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
> Verzonden: maandag 7 mei 2007 17:00
> Aan: ONKELINX, Thierry
> CC: r-sig-geo at stat.math.ethz.ch
> Onderwerp: Re: [R-sig-Geo] variograms do not satisfy a legal model
> 
> Thierry, you'll have to find out what the linear model of 
> regionalization or intrinsic correlation mean; they are legal 
> models, meaning that they lead to predictions with guaranteed 
> non-negative prediction variances. There's a way to override 
> the check (meaning it will not stop on error) but not to get 
> that guarantee.
> --
> Edzer
> 
> ONKELINX, Thierry wrote:
> > Dear useRs,
> >
> > I'm trying to do some cokriging. I try fit the variograms 
> with fit.lmc 
> > (gstat package).
> >
> > #Problem 1: how to get the model in an elegant way into the gstat 
> > object? This is how I do it now.
> > library("gstat")
> > data(meuse)
> > g <- gstat(id = "ln.zinc", formula = log(zinc)~1, locations = ~x+y, 
> > data = meuse) g <- gstat(g, id = "ln.lead", formula = log(lead)~1, 
> > locations = ~x+y, data = meuse) #fit the variogram models using 
> > fit.lmc model <- fit.lmc(variogram(g), g, model = vgm(.55, 
> "Sph", 900, 
> > .05), fit.ranges = FALSE) g <- gstat(g, id = "ln.zinc", model = 
> > model[["model"]][["ln.zinc"]]) g <- gstat(g, id = 
> "ln.lead", model = 
> > model[["model"]][["ln.lead"]]) g <- gstat(g, id = c("ln.zinc", 
> > "ln.lead"), model =
> > model[["model"]][["ln.zinc.ln.lead"]])
> > predict(g, newdata = meuse)
> >
> > #Problem 2. the gstat object doesn't accept different 
> ranges. Or am I 
> > doing something wrong?
> > library("gstat")
> > data(meuse)
> > g <- gstat(id = "ln.zinc", formula = log(zinc)~1, locations = ~x+y, 
> > data = meuse) g <- gstat(g, id = "ln.lead", formula = log(lead)~1, 
> > locations = ~x+y, data = meuse) # examine variograms and cross 
> > variogram:
> > plot(variogram(g))
> > # enter direct variograms:
> > g <- gstat(g, id = "ln.zinc", model = vgm(.55, "Sph", 800, 
> .05)) g <- 
> > gstat(g, id = "ln.lead", model = vgm(.55, "Sph", 900, .05)) # enter 
> > cross variogram:
> > g <- gstat(g, id = c("ln.zinc", "ln.lead"), model = vgm(.47, "Sph", 
> > 900,
> > .03))
> > predict(g, newdata = meuse)
> > # Error in predict.gstat(g, newdata = meuse) : gstat: value not 
> > allowed
> > for: variograms do not satisfy a legal model
> >
> > Thanks,
> >
> > Thierry
> >
> > 
> ----------------------------------------------------------------------
> > --
> > ----
> > ir. Thierry Onkelinx
> > Instituut voor natuur- en bosonderzoek / Reseach Institute 
> for Nature 
> > and Forest Cel biometrie, methodologie en kwaliteitszorg / Section 
> > biometrics, methodology and quality assurance Gaverstraat 4 9500 
> > Geraardsbergen Belgium tel. + 32 54/436 185 
> Thierry.Onkelinx at inbo.be 
> > www.inbo.be
> >
> > Do not put your faith in what statistics say until you have 
> carefully 
> > considered what they do not say.  ~William W. Watt A statistical 
> > analysis, properly conducted, is a delicate dissection of 
> > uncertainties, a surgery of suppositions. ~M.J.Moroney
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at stat.math.ethz.ch
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >   
> 
>



From e.pebesma at geo.uu.nl  Tue May  8 17:24:13 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Tue, 08 May 2007 17:24:13 +0200
Subject: [R-sig-Geo] variograms do not satisfy a legal model
In-Reply-To: <2E9C414912813E4EB981326983E0A10402F1924E@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A10402F1924E@inboexch.inbo.be>
Message-ID: <4640961D.8090607@geo.uu.nl>

Please try adding set in a call to create the gstat object:

g = gstat(... , set = list(nocheck = 1))
predict(g, newdata)
--
Edzer


ONKELINX, Thierry wrote:
> Edzer,
>
> Could you tell me how to override this check? I have been reading the
> helpfile but I can't find it.
>
> Thanks,
>
> Thierry
>
> ------------------------------------------------------------------------
> ----
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Reseach Institute for Nature
> and Forest
> Cel biometrie, methodologie en kwaliteitszorg / Section biometrics,
> methodology and quality assurance
> Gaverstraat 4
> 9500 Geraardsbergen
> Belgium
> tel. + 32 54/436 185
> Thierry.Onkelinx at inbo.be
> www.inbo.be 
>
> Do not put your faith in what statistics say until you have carefully
> considered what they do not say.  ~William W. Watt
> A statistical analysis, properly conducted, is a delicate dissection of
> uncertainties, a surgery of suppositions. ~M.J.Moroney
>
>  
>
>   
>> -----Oorspronkelijk bericht-----
>> Van: Edzer J. Pebesma [mailto:e.pebesma at geo.uu.nl] 
>> Verzonden: maandag 7 mei 2007 17:00
>> Aan: ONKELINX, Thierry
>> CC: r-sig-geo at stat.math.ethz.ch
>> Onderwerp: Re: [R-sig-Geo] variograms do not satisfy a legal model
>>
>> Thierry, you'll have to find out what the linear model of 
>> regionalization or intrinsic correlation mean; they are legal 
>> models, meaning that they lead to predictions with guaranteed 
>> non-negative prediction variances. There's a way to override 
>> the check (meaning it will not stop on error) but not to get 
>> that guarantee.
>> --
>> Edzer
>>
>> ONKELINX, Thierry wrote:
>>     
>>> Dear useRs,
>>>
>>> I'm trying to do some cokriging. I try fit the variograms 
>>>       
>> with fit.lmc 
>>     
>>> (gstat package).
>>>
>>> #Problem 1: how to get the model in an elegant way into the gstat 
>>> object? This is how I do it now.
>>> library("gstat")
>>> data(meuse)
>>> g <- gstat(id = "ln.zinc", formula = log(zinc)~1, locations = ~x+y, 
>>> data = meuse) g <- gstat(g, id = "ln.lead", formula = log(lead)~1, 
>>> locations = ~x+y, data = meuse) #fit the variogram models using 
>>> fit.lmc model <- fit.lmc(variogram(g), g, model = vgm(.55, 
>>>       
>> "Sph", 900, 
>>     
>>> .05), fit.ranges = FALSE) g <- gstat(g, id = "ln.zinc", model = 
>>> model[["model"]][["ln.zinc"]]) g <- gstat(g, id = 
>>>       
>> "ln.lead", model = 
>>     
>>> model[["model"]][["ln.lead"]]) g <- gstat(g, id = c("ln.zinc", 
>>> "ln.lead"), model =
>>> model[["model"]][["ln.zinc.ln.lead"]])
>>> predict(g, newdata = meuse)
>>>
>>> #Problem 2. the gstat object doesn't accept different 
>>>       
>> ranges. Or am I 
>>     
>>> doing something wrong?
>>> library("gstat")
>>> data(meuse)
>>> g <- gstat(id = "ln.zinc", formula = log(zinc)~1, locations = ~x+y, 
>>> data = meuse) g <- gstat(g, id = "ln.lead", formula = log(lead)~1, 
>>> locations = ~x+y, data = meuse) # examine variograms and cross 
>>> variogram:
>>> plot(variogram(g))
>>> # enter direct variograms:
>>> g <- gstat(g, id = "ln.zinc", model = vgm(.55, "Sph", 800, 
>>>       
>> .05)) g <- 
>>     
>>> gstat(g, id = "ln.lead", model = vgm(.55, "Sph", 900, .05)) # enter 
>>> cross variogram:
>>> g <- gstat(g, id = c("ln.zinc", "ln.lead"), model = vgm(.47, "Sph", 
>>> 900,
>>> .03))
>>> predict(g, newdata = meuse)
>>> # Error in predict.gstat(g, newdata = meuse) : gstat: value not 
>>> allowed
>>> for: variograms do not satisfy a legal model
>>>
>>> Thanks,
>>>
>>> Thierry
>>>
>>>
>>>       
>> ----------------------------------------------------------------------
>>     
>>> --
>>> ----
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Reseach Institute 
>>>       
>> for Nature 
>>     
>>> and Forest Cel biometrie, methodologie en kwaliteitszorg / Section 
>>> biometrics, methodology and quality assurance Gaverstraat 4 9500 
>>> Geraardsbergen Belgium tel. + 32 54/436 185 
>>>       
>> Thierry.Onkelinx at inbo.be 
>>     
>>> www.inbo.be
>>>
>>> Do not put your faith in what statistics say until you have 
>>>       
>> carefully 
>>     
>>> considered what they do not say.  ~William W. Watt A statistical 
>>> analysis, properly conducted, is a delicate dissection of 
>>> uncertainties, a surgery of suppositions. ~M.J.Moroney
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at stat.math.ethz.ch
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>   
>>>       
>>



From sebastien.ollier at u-psud.fr  Wed May  9 09:18:12 2007
From: sebastien.ollier at u-psud.fr (=?ISO-8859-1?Q?S=E9bastien_Ollier?=)
Date: Wed, 09 May 2007 09:18:12 +0200
Subject: [R-sig-Geo] managing road network
Message-ID: <464175B4.5040205@u-psud.fr>

Dear UseRs,

I am working on a road network of an agricultural landscape and I want 
to find the shortest path between two points on that network.
The road network is currently on an polyline form, each road being 
represented by a polyline.
To do that, I have thought to convert my polyline theme into a spatial 
graph and use for example the function shortest.path of the package 
igraph. SO, I need to :
- import the shapefile into R (not a problem with R)
- convert the polyline into a graph : I need to find the crossings of 
the polyline and define them as the vertice of the graph (nb lass of the 
package spdep, neig class of the package ade4, or igraph of the package 
igraph...maybe another class of another package?). Then I will cut the 
polyline btween vertices and define the edge of the graph.
- find the shortest path in a graph (using the function shortest path of 
the package igraph...maybe another function of anpother package)
I think I will manage to do it like this but I would like to know if 
there is a cleaver, or a shortest way to do this with R.
Thanks for you experience,

S?bastien Ollier
Universit? Paris-Sud
France



From quimbarris at gmail.com  Wed May  9 11:37:18 2007
From: quimbarris at gmail.com (Quim Barris)
Date: Wed, 9 May 2007 11:37:18 +0200
Subject: [R-sig-Geo] managing road network
In-Reply-To: <464175B4.5040205@u-psud.fr>
References: <464175B4.5040205@u-psud.fr>
Message-ID: <2a5e41220705090237m48075048s331fe0ce04fdfbb8@mail.gmail.com>

Dear UseRs,

I need an algorithm to determine if two points are in the same
continuous area.

The extended problem is:

I've a number of points, in the next step I do an interpolation
method, and obtain a grid of points; with this grid we create class
groups. With the class groups we decide if I can use the points or
not, because the grid interpolation define an area of the class
groups.

The problem is than I only can use the points whose are in a
continuous area.    If two points are in the same class group but not
in the same continuous area we don't can use it.

Thanks in advance and sorry for my English level

Quim Barris



From brzw at crosslibrary.com  Thu May 10 18:17:37 2007
From: brzw at crosslibrary.com (Cyril)
Date: Thu, 10 May 2007 18:17:37 +0200
Subject: [R-sig-Geo] Posted by: murc  at May  6,
	2007 10:57 PM Subsonic only sounds good to the bean-counters cause
	it costs less money.
Message-ID: <000f01c7931e$b9be0ad0$c5d9aeb1@viqlp>

STOCK TRADER ALERT! THURSDAY MAY 10,2007

FRANKFURT STOCK EXCHANGE
CARBON RACE CORP.
Symbol: NDGB.F
Price: .97
WKN A0LCLD
ISIN US14115Q1058

DON'T TAKE YOU EYES OFF THIS STOCK FOR ONE MINUTE THURSDAY... IS
FRANKFURT THE NEW, "HOME OF THE WINNERS?" THE "ALERT" IS ON!!!

the crows figure it out in days and then ignore it. blog: Has Blogging
Peaked?
Has anyone else has noticed a similar phenomenon or used a similar
approach?
These are the conclusions of the Air Force's recently completed analysis
of alternatives for a next-generation bomber to be fielded around 2018.
Expose these jerks like the Wizard of Oz and see if the lion doesn't get
his courage back. They then decide to prune their list of feeds.
" by Robert Vamosi just might open a few eyes and cause lawyers to
rethink how pervasive electronic discovery is really becoming. There are
undoubtedly several of you now reading this post from whom I have an
email that needs a reply. It's nice to see legal publications link
outside the family every now and then.
[I await to be corrected on all points by the DT readership ;-)].
i believe the british get 'prissy' because the alternative engine
involves rolls royce, and was promised as part of the deal that saw
britain become the only tier 1 partner in the jsf project.
It's definitely become one consideration that should be on your list as
you consider your technology options inside and outside your office.
Even worse is poor mic placement, when the sounds of the audience
opening pop cans and blowing their noses is more prominent than the
speaker's voice. From turning your computers off to better power
management to server virtualization to software as a service and hosted
applications, many things are happening in this space.
Posted by: Lee Wahler  at May  3, 2007 12:42 AM ". Excerpt feeds require
that a reader click-through and visit your blog. Let us know if there
are topics you'd like us to cover. And, that guess is clearly that there
continues to be significant growth.
Bit of a strange decision, it being such a gamer's game, but I suppose
Puerto Rico has sold well enough. The article is also a must-read for
the discussion of privacy, criminal investigation, insurance, and other
issues raised by these devices. In the past year, I hit on the new terms
"electronic discovery 2. blog: Do Excerpt Feeds and Poor Sound Quality
Podcasts Have Something in Common? There seems to be a smidgen of
unwarranted optimism as to secure forward basing opportunities. First it
was Peak Oil. Buzz Moseley, who told us that air power is inflicting the
most casualties in Iraq and Afghanistan.
Chandler adds anecdotal evidence that computers do better than humans,
at least considering cost.
blog: Green Legal Technology: Is the Time Ripe?
The more authentic you are the more effective the communicator you are.
Does anyone think it won't make lawyers jobs in electronic discovery
even more complicated.
Reports of Death of PowerPoint Greatly Exaggerated?
Let me know if you might be interested in reading a new version of that
article or becoming one of the participants.
You save yourself the time and effort of clicking through to see the
rest of the post. Some the programming from law schools, such as panel
discussions with excellent speakers, is largely spoiled for me by
terrible production quality.
You can jump to any slide without having to exit out of your
presentation, and the Presenter View displays a big, fat timer to keep
you honest. We monitor over 300 podcasts and blogs on behalf of our
clients.
This one-of-a kind CaseMap Client Summit is conducted almost exclusively
by peers who are successfully using CaseMap in their practice.
Electronic discovery is an ever-changing universe.
com article to one of my articles (and my most popular article ever, at
that). i believe the british get 'prissy' because the alternative engine
involves rolls royce, and was promised as part of the deal that saw
britain become the only tier 1 partner in the jsf project. I'm also
doing more podcasts and publishing in other outlets.
Once I listen to a podcast, I generally delete it in iTunes. 0," and
"litigation 2.
What new terms should you be coining?
I didn't use the standard automatic "first 20 or 50 words" excerpt that
people commonly use today.
Yes, that's an inside joke that I'm sure Denise will appreciate (listen
to an earlier audio version of same joke).
When I feel I have too much on my iPod (it's pretty full), I'll start to
delete podcasts.
It does not address the way ahead, and there have been several
signifcant steps taken by Adm Allen to address problems.
Are they the ones that go red when we're not looking?
com, here are some statistics for the period Feb 1 2007 to April 5,
2007: total posts were 43,278, with a daily average of around 677 posts.
No, not about their utility and value - I could see that from the first
podcast I listened to (an interview with William Gibson). I seem to have
let this post drift away from "green legal technology," but I'll
follow-up on that idea soon. In his recent post "Is it finally time to
ditch PowerPoint?
Shame them into stopping.
Dick Cheney cancelled the A-12 program on 7 January 1991, just as the
bombs started to fall on Baghdad during Operation Desert Storm. Two
weeks ago DeKort testified before a House committee investigating
Deepwater.
That's not exactly what Rubel's point was, but I've noticed that my
posts per week, a more useful metric to me, has been dropping from about
5 to more like 3.



From fieldsh at mail.med.upenn.edu  Fri May 11 16:55:20 2007
From: fieldsh at mail.med.upenn.edu (Sam Field)
Date: Fri, 11 May 2007 10:55:20 -0400
Subject: [R-sig-Geo] efficient code/function for rectangular SP weight
	Matrix and gwr
In-Reply-To: <2E9C414912813E4EB981326983E0A10402E52331@inboexch.inbo.be>
References: <2E9C414912813E4EB981326983E0A10402E52331@inboexch.inbo.be>
Message-ID: <464483D8.3040808@mail.med.upenn.edu>

List,

I need to create a rectangular spatial weight matrix for a set of n and 
m objects. I quickly run in to memory allocation problems when 
constructing the full matrix in a single pass. I am looking for a more 
efficient way of doing this. There appears to be efficient procedures in 
spdep for constructing SQUARE spatial weight matrices (e.g. 
dnearneigh()). Are there analogous procedures for constructing distance 
based weights between two different point patterns? I am doing this in 
preparation for implementing an approximate geographically weighted 
logistic regression procedure. I was thinking about using re sampling 
procedure as an inferential frame- perhaps I might get some feedback. 
This is what I was going to do.

I have a point pattern of 30,000 diabetic people based on where they 
lived during a 2 year period. During that period, approximately 4% of 
them developed diabetes. I am interested in isolating the impact of 
ecological factors on the geographic variation" of the disease, so it is 
necessary to control for the spatial clustering of individual level risk 
factors associated with the disease (diabetes).

Step 1: Estimate a logistic regression using the full sample and predict 
incidence diabetes using individual level covariates (i.e. who developed 
diabetes over the two year period).

Step 2. Estimate a weighted logit model at each location (grid). The 
observations would be the people (not the geographic units) and the 
weights would be kernel weights based on distance. The model would only 
contain a single freely estimated parameter, the intercept, but it would 
also contain an offset term. For each patient, the offset term would 
simply be an evaluation of the linear predictor of the global model 
estimated above (based on the observed covariate values), but without 
the intercept. This would effectively fix the estimates of the patient 
level coefficients to their global values, requiring only a local 
estimate of the intercept. My hope is that I could interpret geographic 
variability in the intercept as evidence for a "location effect" net of 
the patient composition or "risk profile" at a particular location. It 
would probably make sense to center the X variables so that the 
intercept was interpretable and estimated in a region of the response 
plane where their is plenty of data. I would let the other covariates 
vary as well, but I doubt the model could be estimated in large portions 
of the study area because of sparse data.

Step 3. If I were going to do inference on the location specific 
intercepts, I would generate a sampling distribution at each location by 
re sampling from the global model, and repeat Step 2 for each randomly 
drawn sample. This would give me a local sampling distribution of 
intercept estimates at each location and I could compare it to the the 
single one generated from the observed data. The global model represents 
a kind of null because the intercept is fixed to its global value and 
geographic variability is driven entirely by the spatial clustering of 
patient level factors.


thanks!

Sam



From dray at biomserv.univ-lyon1.fr  Fri May 11 17:55:01 2007
From: dray at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?St=E9phane_Dray?=)
Date: Fri, 11 May 2007 17:55:01 +0200
Subject: [R-sig-Geo] efficient code/function for rectangular SP weight
 Matrix and gwr
In-Reply-To: <464483D8.3040808@mail.med.upenn.edu>
References: <2E9C414912813E4EB981326983E0A10402E52331@inboexch.inbo.be>
	<464483D8.3040808@mail.med.upenn.edu>
Message-ID: <464491D5.9070809@biomserv.univ-lyon1.fr>

Hi Sam,

I think that this question is quite general and could interest other 
people, including me, with very different aims. I have developed a 
method to look for the relationships between two data sets that have 
been sampled on the same area but for different locations. In my 
example, the two samples are two polygons layers. In this approach, I 
compute a rectangular weighting matrix where each weight correspond to 
the area of intersection between polygons of each layer. I have used 
also the matrix form to store these weights (my data set was very small 
compared to you). I remember that Roger was also interested by these 
rectangular weights in another context. Here we  have different  problems:
- how to compute these kind  of weights
- how to store them.

For the first point, I think that for each method/application, the 
solution  is different. We could develop/extend classical tools for 
square weights (one set of spatial units) to rectangular weights (two 
sets of spatial units).
For the second one, It would be probably interesting to define a class 
of object in spdep. nb objects are lists, and I think that it would be 
the solution for rectangular neighborhood.

If I consider two sets of spatial units (A and B) where the number of 
units is equal to na and nb.  We could store the neighbors in a list of 
length 2. The first element of this list is a list of length na. In this 
list, the j-th element is a vector of the neighbors of the j-th unit of 
the layer A. These neighbors are spatial units of the layer B.  The 
second element of the global list is a list of length nb where each 
element is a vector of neighbors.

I think that we have to think to a class of object that could be useful 
for everybody dealing with this kind of rectangular weights. If this 
class is properly defined (second point), we could then develop tools to 
construct this kind of neighborhoods (first point). The eventual 
extension to more than two data sets could also be taken into account in 
this reflexion.

Cheers,


Sam Field wrote:
> List,
>
> I need to create a rectangular spatial weight matrix for a set of n and 
> m objects. I quickly run in to memory allocation problems when 
> constructing the full matrix in a single pass. I am looking for a more 
> efficient way of doing this. There appears to be efficient procedures in 
> spdep for constructing SQUARE spatial weight matrices (e.g. 
> dnearneigh()). Are there analogous procedures for constructing distance 
> based weights between two different point patterns? I am doing this in 
> preparation for implementing an approximate geographically weighted 
> logistic regression procedure. I was thinking about using re sampling 
> procedure as an inferential frame- perhaps I might get some feedback. 
> This is what I was going to do.
>
> I have a point pattern of 30,000 diabetic people based on where they 
> lived during a 2 year period. During that period, approximately 4% of 
> them developed diabetes. I am interested in isolating the impact of 
> ecological factors on the geographic variation" of the disease, so it is 
> necessary to control for the spatial clustering of individual level risk 
> factors associated with the disease (diabetes).
>
> Step 1: Estimate a logistic regression using the full sample and predict 
> incidence diabetes using individual level covariates (i.e. who developed 
> diabetes over the two year period).
>
> Step 2. Estimate a weighted logit model at each location (grid). The 
> observations would be the people (not the geographic units) and the 
> weights would be kernel weights based on distance. The model would only 
> contain a single freely estimated parameter, the intercept, but it would 
> also contain an offset term. For each patient, the offset term would 
> simply be an evaluation of the linear predictor of the global model 
> estimated above (based on the observed covariate values), but without 
> the intercept. This would effectively fix the estimates of the patient 
> level coefficients to their global values, requiring only a local 
> estimate of the intercept. My hope is that I could interpret geographic 
> variability in the intercept as evidence for a "location effect" net of 
> the patient composition or "risk profile" at a particular location. It 
> would probably make sense to center the X variables so that the 
> intercept was interpretable and estimated in a region of the response 
> plane where their is plenty of data. I would let the other covariates 
> vary as well, but I doubt the model could be estimated in large portions 
> of the study area because of sparse data.
>
> Step 3. If I were going to do inference on the location specific 
> intercepts, I would generate a sampling distribution at each location by 
> re sampling from the global model, and repeat Step 2 for each randomly 
> drawn sample. This would give me a local sampling distribution of 
> intercept estimates at each location and I could compare it to the the 
> single one generated from the observed data. The global model represents 
> a kind of null because the intercept is fixed to its global value and 
> geographic variability is driven entirely by the spatial clustering of 
> patient level factors.
>
>
> thanks!
>
> Sam
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>   


-- 
St?phane DRAY (dray at biomserv.univ-lyon1.fr )
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - Lyon I
43, Bd du 11 Novembre 1918, 69622 Villeurbanne Cedex, France
Tel: 33 4 72 43 27 57       Fax: 33 4 72 43 13 88
http://biomserv.univ-lyon1.fr/~dray/



From fieldsh at mail.med.upenn.edu  Fri May 11 18:07:57 2007
From: fieldsh at mail.med.upenn.edu (Sam Field)
Date: Fri, 11 May 2007 12:07:57 -0400
Subject: [R-sig-Geo] efficient code/function for rectangular SP weight
 Matrix and gwr
In-Reply-To: <464491D5.9070809@biomserv.univ-lyon1.fr>
References: <2E9C414912813E4EB981326983E0A10402E52331@inboexch.inbo.be>
	<464483D8.3040808@mail.med.upenn.edu>
	<464491D5.9070809@biomserv.univ-lyon1.fr>
Message-ID: <464494DD.70308@mail.med.upenn.edu>

Stephane,

Thanks for the quick response.  I only just now discovered how useful 
lists are for storing non-zero spatial weights.  I have been dealing 
with the entire matrix, but with lists I can just deal with distances 
less then a specific bandwidth.  I sense that this will more or less 
deal with my problem.  Your comments about creating a new kind of "nb" 
object sound like a good idea - especially if rectangular weight 
matrices have such general usage. 

Sam




St?phane Dray wrote:
> Hi Sam,
>
> I think that this question is quite general and could interest other 
> people, including me, with very different aims. I have developed a 
> method to look for the relationships between two data sets that have 
> been sampled on the same area but for different locations. In my 
> example, the two samples are two polygons layers. In this approach, I 
> compute a rectangular weighting matrix where each weight correspond to 
> the area of intersection between polygons of each layer. I have used 
> also the matrix form to store these weights (my data set was very 
> small compared to you). I remember that Roger was also interested by 
> these rectangular weights in another context. Here we  have different  
> problems:
> - how to compute these kind  of weights
> - how to store them.
>
> For the first point, I think that for each method/application, the 
> solution  is different. We could develop/extend classical tools for 
> square weights (one set of spatial units) to rectangular weights (two 
> sets of spatial units).
> For the second one, It would be probably interesting to define a class 
> of object in spdep. nb objects are lists, and I think that it would be 
> the solution for rectangular neighborhood.
>
> If I consider two sets of spatial units (A and B) where the number of 
> units is equal to na and nb.  We could store the neighbors in a list 
> of length 2. The first element of this list is a list of length na. In 
> this list, the j-th element is a vector of the neighbors of the j-th 
> unit of the layer A. These neighbors are spatial units of the layer 
> B.  The second element of the global list is a list of length nb where 
> each element is a vector of neighbors.
>
> I think that we have to think to a class of object that could be 
> useful for everybody dealing with this kind of rectangular weights. If 
> this class is properly defined (second point), we could then develop 
> tools to construct this kind of neighborhoods (first point). The 
> eventual extension to more than two data sets could also be taken into 
> account in this reflexion.
>
> Cheers,
>
>
> Sam Field wrote:
>> List,
>>
>> I need to create a rectangular spatial weight matrix for a set of n 
>> and m objects. I quickly run in to memory allocation problems when 
>> constructing the full matrix in a single pass. I am looking for a 
>> more efficient way of doing this. There appears to be efficient 
>> procedures in spdep for constructing SQUARE spatial weight matrices 
>> (e.g. dnearneigh()). Are there analogous procedures for constructing 
>> distance based weights between two different point patterns? I am 
>> doing this in preparation for implementing an approximate 
>> geographically weighted logistic regression procedure. I was thinking 
>> about using re sampling procedure as an inferential frame- perhaps I 
>> might get some feedback. This is what I was going to do.
>>
>> I have a point pattern of 30,000 diabetic people based on where they 
>> lived during a 2 year period. During that period, approximately 4% of 
>> them developed diabetes. I am interested in isolating the impact of 
>> ecological factors on the geographic variation" of the disease, so it 
>> is necessary to control for the spatial clustering of individual 
>> level risk factors associated with the disease (diabetes).
>>
>> Step 1: Estimate a logistic regression using the full sample and 
>> predict incidence diabetes using individual level covariates (i.e. 
>> who developed diabetes over the two year period).
>>
>> Step 2. Estimate a weighted logit model at each location (grid). The 
>> observations would be the people (not the geographic units) and the 
>> weights would be kernel weights based on distance. The model would 
>> only contain a single freely estimated parameter, the intercept, but 
>> it would also contain an offset term. For each patient, the offset 
>> term would simply be an evaluation of the linear predictor of the 
>> global model estimated above (based on the observed covariate 
>> values), but without the intercept. This would effectively fix the 
>> estimates of the patient level coefficients to their global values, 
>> requiring only a local estimate of the intercept. My hope is that I 
>> could interpret geographic variability in the intercept as evidence 
>> for a "location effect" net of the patient composition or "risk 
>> profile" at a particular location. It would probably make sense to 
>> center the X variables so that the intercept was interpretable and 
>> estimated in a region of the response plane where their is plenty of 
>> data. I would let the other covariates vary as well, but I doubt the 
>> model could be estimated in large portions of the study area because 
>> of sparse data.
>>
>> Step 3. If I were going to do inference on the location specific 
>> intercepts, I would generate a sampling distribution at each location 
>> by re sampling from the global model, and repeat Step 2 for each 
>> randomly drawn sample. This would give me a local sampling 
>> distribution of intercept estimates at each location and I could 
>> compare it to the the single one generated from the observed data. 
>> The global model represents a kind of null because the intercept is 
>> fixed to its global value and geographic variability is driven 
>> entirely by the spatial clustering of patient level factors.
>>
>>
>> thanks!
>>
>> Sam
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
>>   
>
>



From Laurence.Amilhat at toulouse.inra.fr  Mon May 14 11:16:22 2007
From: Laurence.Amilhat at toulouse.inra.fr (Laurence Amilhat)
Date: Mon, 14 May 2007 11:16:22 +0200
Subject: [R-sig-Geo] starting with Rmap
Message-ID: <464828E6.2000006@toulouse.inra.fr>

Hello,

I am a beginner in R programming, so sorry for the low level of this post...

I would like to create a Europe map and plot some Informations on it, 
using lat/long coordinates.
I've seen the Rmap project, which seems to do what I want.
The thing is that I can't get started.
I download the packages: maps, mapdata, spmaps, shapefiles
I have a text file for the europe continent downloaded from the World 
data bank, but I don't know how to create a database that can be used by 
Rmap.
So I tried with shapefiles, I have a shapefile for Europe, as is it 
noticed in the tutorial http://www.maths.lancs.ac.uk/Software/Rmap/
i tried:
 >europe <- shapefile("europe.shp")
Erreur : impossible de trouver la fonction "shapefiles"
= The function shapefile was not found.

Do someone have an idea on what I am doing wrong, or know a good 
tutorial that can be helpfull for what I what to do.

Many thanks,


Laurence Amihat



From Roger.Bivand at nhh.no  Mon May 14 12:30:00 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 14 May 2007 12:30:00 +0200 (CEST)
Subject: [R-sig-Geo] starting with Rmap
In-Reply-To: <464828E6.2000006@toulouse.inra.fr>
Message-ID: <Pine.LNX.4.44.0705141218370.20388-100000@reclus.nhh.no>

On Mon, 14 May 2007, Laurence Amilhat wrote:

> Hello,
> 
> I am a beginner in R programming, so sorry for the low level of this post...

That's OK, everyone begins sometime ...

Perhaps you would find the information in the CRAN "Spatial" Task View 
useful.

To read and plot a shapefile, I think that you may find the examples in 
the help page of readShapePoly() in the maptools package helpful. This 
package depends on the sp package, which provides classes for spatial 
data. In your case:

list.files(pattern="shp$")
# will tell you if your shapefile is in the working directory
getinfo.shape("europe.shp")
# will tell you what kind of shapefile it is
europe <- readShapePoly("europe.shp", proj4string=CRS("+proj=longlat"))
# will read it
plot(europe)
# will plot it

Some of the work in the original Rmap package is now in the rgdal package, 
so that if you need to project the data, you can go on to that package 
later.

To plot information, simply overplot the outline map with you choice of 
base graphics functions.

Roger

> 
> I would like to create a Europe map and plot some Informations on it, 
> using lat/long coordinates.
> I've seen the Rmap project, which seems to do what I want.
> The thing is that I can't get started.
> I download the packages: maps, mapdata, spmaps, shapefiles
> I have a text file for the europe continent downloaded from the World 
> data bank, but I don't know how to create a database that can be used by 
> Rmap.
> So I tried with shapefiles, I have a shapefile for Europe, as is it 
> noticed in the tutorial http://www.maths.lancs.ac.uk/Software/Rmap/
> i tried:
>  >europe <- shapefile("europe.shp")
> Erreur : impossible de trouver la fonction "shapefiles"
> = The function shapefile was not found.
> 
> Do someone have an idea on what I am doing wrong, or know a good 
> tutorial that can be helpfull for what I what to do.
> 
> Many thanks,
> 
> 
> Laurence Amihat
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From krueger at wikisquare.de  Mon May 14 12:54:22 2007
From: krueger at wikisquare.de (Stefan =?iso-8859-1?q?Kr=FCger?=)
Date: Mon, 14 May 2007 12:54:22 +0200
Subject: [R-sig-Geo] starting with Rmap
In-Reply-To: <464828E6.2000006@toulouse.inra.fr>
References: <464828E6.2000006@toulouse.inra.fr>
Message-ID: <200705141254.22467.krueger@wikisquare.de>

Hi

Dounds like you didn't import the library.. Try 

library (rmap)

or whatever  your library is called.



Am Montag 14 Mai 2007 schrieb Laurence Amilhat:
> Hello,
>
> I am a beginner in R programming, so sorry for the low level of this
> post...
>
> I would like to create a Europe map and plot some Informations on it,
> using lat/long coordinates.
> I've seen the Rmap project, which seems to do what I want.
> The thing is that I can't get started.
> I download the packages: maps, mapdata, spmaps, shapefiles
> I have a text file for the europe continent downloaded from the World
> data bank, but I don't know how to create a database that can be used by
> Rmap.
> So I tried with shapefiles, I have a shapefile for Europe, as is it
> noticed in the tutorial http://www.maths.lancs.ac.uk/Software/Rmap/
>
> i tried:
>  >europe <- shapefile("europe.shp")
>
> Erreur : impossible de trouver la fonction "shapefiles"
> = The function shapefile was not found.
>
> Do someone have an idea on what I am doing wrong, or know a good
> tutorial that can be helpfull for what I what to do.
>
> Many thanks,
>
>
> Laurence Amihat
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo



-- 
wiki? - Softwareentwicklung
Stefan Kr?ger
Stra?burger Weg 26
53113 Bonn

email	krueger at wikisquare.de
phone	0228 9437868
web	www.wikisquare.de
icq	310673006



From hpolesi at gmail.com  Mon May 14 14:06:39 2007
From: hpolesi at gmail.com (=?ISO-8859-1?Q?Herv=E9_Polesi?=)
Date: Mon, 14 May 2007 14:06:39 +0200
Subject: [R-sig-Geo] re : starting with Rmap
Message-ID: <464850CF.7050005@gmail.com>

Bonjour Laurence

Je crois que, pour une fois, un peu de fran?ais est possible...
Apr?s avoir t?l?charg? les packages, il convient ? chaque ouverture de 
session de les rendres "actifs".
Utilise la commande library.

Hi Laurence
Let's talk in french...
Once the packages downloaded, you have to call them.
Try the libray command.


Cordialement / Greetings
Herv? Polesi



From Roger.Bivand at nhh.no  Mon May 14 14:18:54 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 14 May 2007 14:18:54 +0200 (CEST)
Subject: [R-sig-Geo] starting with Rmap
In-Reply-To: <46484D47.2040202@toulouse.inra.fr>
Message-ID: <Pine.LNX.4.44.0705141409140.20388-100000@reclus.nhh.no>

On Mon, 14 May 2007, Laurence Amilhat wrote:

> thank you for your quick answer.
> 
> By following your steps, I was able to plot the shapefile.
> But it didn't contain what I was expected: an european map.

Please do not drop list threads, please do include your code verbatim. 

I guess that you have a map of North Carolina (as distributed with the
package). Your original question suggested that you have a shapefile of
Europe already. If you do not have, do you need just a shoreline, or do
you need country boundaries, and if so which boundaries, and from when?  
This thread may also be relevant:

http://article.gmane.org/gmane.comp.lang.r.general/84644

Roger

> 
> Do you have any idea where I can found such a map?
> A map of europe that can be read in R, so I can plot more infos on it.
> 
> thanks again,
> 
> 
> Laurence Amilhat.
> 
> 
> 
> 
> Roger Bivand a ?crit :
> > On Mon, 14 May 2007, Laurence Amilhat wrote:
> >
> >   
> >> Hello,
> >>
> >> I am a beginner in R programming, so sorry for the low level of this post...
> >>     
> >
> > That's OK, everyone begins sometime ...
> >
> > Perhaps you would find the information in the CRAN "Spatial" Task View 
> > useful.
> >
> > To read and plot a shapefile, I think that you may find the examples in 
> > the help page of readShapePoly() in the maptools package helpful. This 
> > package depends on the sp package, which provides classes for spatial 
> > data. In your case:
> >
> > list.files(pattern="shp$")
> > # will tell you if your shapefile is in the working directory
> > getinfo.shape("europe.shp")
> > # will tell you what kind of shapefile it is
> > europe <- readShapePoly("europe.shp", proj4string=CRS("+proj=longlat"))
> > # will read it
> > plot(europe)
> > # will plot it
> >
> > Some of the work in the original Rmap package is now in the rgdal package, 
> > so that if you need to project the data, you can go on to that package 
> > later.
> >
> > To plot information, simply overplot the outline map with you choice of 
> > base graphics functions.
> >
> > Roger
> >
> >   
> >> I would like to create a Europe map and plot some Informations on it, 
> >> using lat/long coordinates.
> >> I've seen the Rmap project, which seems to do what I want.
> >> The thing is that I can't get started.
> >> I download the packages: maps, mapdata, spmaps, shapefiles
> >> I have a text file for the europe continent downloaded from the World 
> >> data bank, but I don't know how to create a database that can be used by 
> >> Rmap.
> >> So I tried with shapefiles, I have a shapefile for Europe, as is it 
> >> noticed in the tutorial http://www.maths.lancs.ac.uk/Software/Rmap/
> >> i tried:
> >>  >europe <- shapefile("europe.shp")
> >> Erreur : impossible de trouver la fonction "shapefiles"
> >> = The function shapefile was not found.
> >>
> >> Do someone have an idea on what I am doing wrong, or know a good 
> >> tutorial that can be helpfull for what I what to do.
> >>
> >> Many thanks,
> >>
> >>
> >> Laurence Amihat
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at stat.math.ethz.ch
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >>     
> >
> >   
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Giovanni_Millo at Generali.com  Mon May 14 14:53:29 2007
From: Giovanni_Millo at Generali.com (Millo Giovanni)
Date: Mon, 14 May 2007 14:53:29 +0200
Subject: [R-sig-Geo] R: starting with Rmap
Message-ID: <7C95FD2FC68FBB45B9E9FDC1ECD49AF502962EE1@BEMAILEXTV03.corp.generali.net>

Hello. May I suggest you try 'maptools' as well? I suspect it could be a
little more newbie-friendly (but I'm not sure, as being forced to stick
to Windows I never really checked Rmap out).

For now you should be able to get what you want in a relatively short
time. See
?read.shape
?plot.Map
but if you are serious on it, take heed plot.Map is currently
'deprecated' so it won't be there in the future.

You can then add text via low-level functions, see ?text.

good luck!
Giovanni

----------------------------------------------------------------------

Message: 1
Date: Mon, 14 May 2007 11:16:22 +0200
From: Laurence Amilhat <Laurence.Amilhat at toulouse.inra.fr>
Subject: [R-sig-Geo] starting with Rmap
To: R-sig-Geo at stat.math.ethz.ch
Message-ID: <464828E6.2000006 at toulouse.inra.fr>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hello,

I am a beginner in R programming, so sorry for the low level of this
post...

I would like to create a Europe map and plot some Informations on it, 
using lat/long coordinates.
I've seen the Rmap project, which seems to do what I want.
The thing is that I can't get started.
I download the packages: maps, mapdata, spmaps, shapefiles
I have a text file for the europe continent downloaded from the World 
data bank, but I don't know how to create a database that can be used by

Rmap.
So I tried with shapefiles, I have a shapefile for Europe, as is it 
noticed in the tutorial http://www.maths.lancs.ac.uk/Software/Rmap/
i tried:
 >europe <- shapefile("europe.shp")
Erreur : impossible de trouver la fonction "shapefiles"
= The function shapefile was not found.

Do someone have an idea on what I am doing wrong, or know a good 
tutorial that can be helpfull for what I what to do.

Many thanks,


Laurence Amihat



------------------------------

Giovanni Millo
Research Dept.,
Assicurazioni Generali SpA
Via Machiavelli 4, 
34131 Trieste (Italy)
tel. +39 040 671184 
fax  +39 040 671160
 
Ai sensi del D.Lgs. 196/2003 si precisa che le informazioni ...{{dropped}}



From knussear at mac.com  Mon May 14 15:00:10 2007
From: knussear at mac.com (Ken Nussear)
Date: Mon, 14 May 2007 06:00:10 -0700
Subject: [R-sig-Geo] R-sig-Geo Digest, Vol 45, Issue 10
In-Reply-To: <mailman.11.1179136806.25293.r-sig-geo@stat.math.ethz.ch>
References: <mailman.11.1179136806.25293.r-sig-geo@stat.math.ethz.ch>
Message-ID: <039D9CF7-A0A7-4627-B0AA-1220A14A141E@mac.com>

Did you load the library first using library(Rmap)?


Ken


On May 14, 2007, at 3:00 AM, r-sig-geo-request at stat.math.ethz.ch wrote:

> Send R-sig-Geo mailing list submissions to
> 	r-sig-geo at stat.math.ethz.ch
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> or, via email, send a message with subject or body 'help' to
> 	r-sig-geo-request at stat.math.ethz.ch
>
> You can reach the person managing the list at
> 	r-sig-geo-owner at stat.math.ethz.ch
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-sig-Geo digest..."
>
>
> Today's Topics:
>
>    1. starting with Rmap (Laurence Amilhat)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 14 May 2007 11:16:22 +0200
> From: Laurence Amilhat <Laurence.Amilhat at toulouse.inra.fr>
> Subject: [R-sig-Geo] starting with Rmap
> To: R-sig-Geo at stat.math.ethz.ch
> Message-ID: <464828E6.2000006 at toulouse.inra.fr>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> Hello,
>
> I am a beginner in R programming, so sorry for the low level of  
> this post...
>
> I would like to create a Europe map and plot some Informations on it,
> using lat/long coordinates.
> I've seen the Rmap project, which seems to do what I want.
> The thing is that I can't get started.
> I download the packages: maps, mapdata, spmaps, shapefiles
> I have a text file for the europe continent downloaded from the World
> data bank, but I don't know how to create a database that can be  
> used by
> Rmap.
> So I tried with shapefiles, I have a shapefile for Europe, as is it
> noticed in the tutorial http://www.maths.lancs.ac.uk/Software/Rmap/
> i tried:
>> europe <- shapefile("europe.shp")
> Erreur : impossible de trouver la fonction "shapefiles"
> = The function shapefile was not found.
>
> Do someone have an idea on what I am doing wrong, or know a good
> tutorial that can be helpfull for what I what to do.
>
> Many thanks,
>
>
> Laurence Amihat
>
>
>
> ------------------------------
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
> End of R-sig-Geo Digest, Vol 45, Issue 10
> *****************************************



From Laurence.Amilhat at toulouse.inra.fr  Tue May 15 15:04:31 2007
From: Laurence.Amilhat at toulouse.inra.fr (Laurence Amilhat)
Date: Tue, 15 May 2007 15:04:31 +0200
Subject: [R-sig-Geo] Background behind text in a map
Message-ID: <4649AFDF.9060507@toulouse.inra.fr>

Hello everyone,


I have this code to create a map of europe, and plot some cities on it 
with their names:

library(maps)
library(mapdata)
library(spmaps)
library(grid)

mesvilles 
<-"/home/lamilhat/AQUAGENOME/DRAW_MAP/extra_maps/villes_coords_R.txt" 
#file with 3 columns
 a <-read.table(mesvilles)
 x<-a[,1] #first column longitude
 y<-a[,2] #second column lattitude
 z<-a[,3] #third column city name

# I plot the map on a png file
png(file="/home/lamilhat/AQUAGENOME/DRAW_MAP/extra_maps/R_output.png", 
width= 1200, height= 700)
map(database="world", xlim=c(-25,70),ylim=c(35,71), fill=TRUE, col="grey")
points(x,y, col="red", pch=16)
text(x,y,z, pos=3)
dev.off()

This code give me the map attached to this message.
This works fine, but the names of the cities are not readable in some case.
I would like to add a background color (white) behind the text to be 
more readable.

Do you known a function that can be used for that?


Thank you for your help,


Laurence Amilhat



-------------- next part --------------
A non-text attachment was scrubbed...
Name: map_test.png
Type: image/png
Size: 83157 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070515/4d963cbc/attachment.png>

From mrufino at cripsul.ipimar.pt  Tue May 15 16:11:44 2007
From: mrufino at cripsul.ipimar.pt (Marta Rufino)
Date: Tue, 15 May 2007 15:11:44 +0100
Subject: [R-sig-Geo] graphical doubts: lines colour,
 order objects and print in only one graph
Message-ID: <4649BFA0.8090105@cripsul.ipimar.pt>

Dear list members,


I have three doubts: :-)

1. How to colour the lines from a shape file (which has been inserted 
into a spplot, through sp.layout):

#For example
nc <- readShapeLines("costa.shp",  proj4string=CRS("+proj=longlat 
+datum=WGS84"))
spplot(grid[1], sp.layout=list(nc))                                  
#How do I indicate the colour of the lines in the graph? Sorry I dont 
give a real example... was unable to find shp+gridded data to fit in


2. How to include a dataframe with text into spplot (i.e. several text 
positions in the graph). I managed to do it for one point, but not for 
many at the same time. Is it possible?

data(meuse.grid); coordinates(meuse.grid) = c("x", "y"); 
gridded(meuse.grid) = TRUE #insert data
tx1=data.frame(t(bbox(coordinates(meuse.grid))), lab=c("Bom dia","Boa 
tarde"))#text labels to plot, for example...
tx1 = list("sp.text", loc=tx1[1,1:2], tx1$lab[1] ,col = "brown", cex=.5, 
pos=3)
spplot(meuse.grid, sp.layout=list(tx1)) #map... works because it is only 
one label

tx1=data.frame(t(bbox(coordinates(meuse.grid))), lab=c("Bom dia","Boa 
tarde"))#text labels to plot, for example...
tx1 = list("sp.text", loc=tx1[,1:2], tx1$lab ,col = "brown", cex=.5, pos=3)
spplot(meuse.grid, sp.layout=list(tx1)) #map... does not work


3. How do I define that the lines/text should only appear in the first 
plot (if I have multiple plots)?
spplot(meuse.grid, sp.layout=list(tx1)) # If I wanted that the labels 
only appear in the soil plot, not in the others

 
Thank you very much in advance,
Best wishes,
Marta


>

-- 
.......................................................................
Marta M. Rufino (PhD)

.....
Instituto Nacional de Investiga??o Agr?ria e das Pescas (INIAP/IPIMAR),
Centro Regional de Investiga??o Pesqueira do Sul (CRIPSul)
Avenida 5 de Outubro s/n
P-8700-305 Olh?o, Portugal
+351 289 700 541

..... 
Institut de Ci?ncies del Mar - CMIMA (CSIC)
Passeig Mar?tim de la Barceloneta, 37-49      
08003 BARCELONA - Catalunya
Spain



From e.pebesma at geo.uu.nl  Tue May 15 17:16:25 2007
From: e.pebesma at geo.uu.nl (Edzer J. Pebesma)
Date: Tue, 15 May 2007 17:16:25 +0200
Subject: [R-sig-Geo] graphical doubts: lines colour,
 order objects and print in only one graph
In-Reply-To: <4649BFA0.8090105@cripsul.ipimar.pt>
References: <4649BFA0.8090105@cripsul.ipimar.pt>
Message-ID: <4649CEC9.2080807@geo.uu.nl>

Marta Rufino wrote:
> Dear list members,
>
>
> I have three doubts: :-)
>
> 1. How to colour the lines from a shape file (which has been inserted 
> into a spplot, through sp.layout):
>
> #For example
> nc <- readShapeLines("costa.shp",  proj4string=CRS("+proj=longlat 
> +datum=WGS84"))
> spplot(grid[1], sp.layout=list(nc))                                  
> #How do I indicate the colour of the lines in the graph? Sorry I dont 
> give a real example... was unable to find shp+gridded data to fit in
>
>   
try passing sp.layout = list(sp.lines, nc, col = 'red')
> 2. How to include a dataframe with text into spplot (i.e. several text 
> positions in the graph). I managed to do it for one point, but not for 
> many at the same time. Is it possible?
>
> data(meuse.grid); coordinates(meuse.grid) = c("x", "y"); 
> gridded(meuse.grid) = TRUE #insert data
> tx1=data.frame(t(bbox(coordinates(meuse.grid))), lab=c("Bom dia","Boa 
> tarde"))#text labels to plot, for example...
> tx1 = list("sp.text", loc=tx1[1,1:2], tx1$lab[1] ,col = "brown", cex=.5, 
> pos=3)
> spplot(meuse.grid, sp.layout=list(tx1)) #map... works because it is only 
> one label
>
> tx1=data.frame(t(bbox(coordinates(meuse.grid))), lab=c("Bom dia","Boa 
> tarde"))#text labels to plot, for example...
> tx1 = list("sp.text", loc=tx1[,1:2], tx1$lab ,col = "brown", cex=.5, pos=3)
> spplot(meuse.grid, sp.layout=list(tx1)) #map... does not work
>
>   
I think it now only works one label at a time; would be nice if it 
worked for vectors, will look at it. Have a look at example(spplot) for 
an example.
> 3. How do I define that the lines/text should only appear in the first 
> plot (if I have multiple plots)?
> spplot(meuse.grid, sp.layout=list(tx1)) # If I wanted that the labels 
> only appear in the soil plot, not in the others
>   
Add a which=n with n the panel number to the list for the label.
--
Edzer



From diisy at dircon.co.uk  Tue May 15 21:42:35 2007
From: diisy at dircon.co.uk (Mayes Bridget)
Date: Tue, 15 May 2007 22:42:35 +0300
Subject: [R-sig-Geo] The Baha'i Faith has had followers in what is now
	Slovakia since the early 20th century.
Message-ID: <000d01c79729$2fdd3cf0$4e2dc4bc@wyksg>

DER INVESTORALARM! BJ5N.F BEGINNT HOCHGEHEN! MITTWOCH 16. MAG STARTET
DIE HAUSSE!

Company: BOERSE INVEST BETEI
WKN : 797639
ISIN : CH0012802093
Markt: Frankfurt

Kurzel : BJ5N.F
Letztr Kurs: 1.79
3 Tages Prognose: 5.00

ES IST EIN UNGLAUBLICHES PROFITPOTENTIAL! VERLIERE DIESE CHANCE NICHT!

A spokesman for Samoan Advisory Council, Tino Pareira, says his death is
being felt in Samoan communities around the world and especially in New
Zealand. "The state guarantees registered churches and religious
communities the legal status and possibility of functioning in public
life.
Marcic developed what originally was a one-woman show - she was the
woman - into a four-person musical with professional singers. As was
characteristic of his gentle, humble nature, when asked why he didn't
call himself a king, he replied, "Because in Samoa everyone's a king.
Users can submit photographs which are then voted on by their peers.
He attended St Stephens and Wesley colleges in New Zealand.
CBS Records will be launched primarily utilizing the existing
infrastructure of CBS Entertainment and CBS Interactive.
cities, and when a top Australian producer saw a performance in Boston,
he signed on, too.
As was characteristic of his gentle, humble nature, when asked why he
didn't call himself a king, he replied, "Because in Samoa everyone's a
king.



From krandolph at fs.fed.us  Wed May 16 18:20:24 2007
From: krandolph at fs.fed.us (KaDonna C Randolph)
Date: Wed, 16 May 2007 12:20:24 -0400
Subject: [R-sig-Geo] meaning of "size" in the iscluster results
Message-ID: <OFAE1EED8B.C09212F1-ON852572DD.00575F45-852572DD.0059C1F9@fs.fed.us>


As a new R user I am running the examples in the DCluster Package manual.
The output of the iscluster function includes an element called "size."
The definition of this value as given in the manual is unclear to me.  Can
someone clarify what is meant by "number of regions from the centre."


Thank you.
KaDonna Randolph



From patrick.giraudoux at univ-fcomte.fr  Thu May 17 15:00:55 2007
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Thu, 17 May 2007 15:00:55 +0200
Subject: [R-sig-Geo] read prj file to CRS
Message-ID: <464C5207.8060305@univ-fcomte.fr>

Dear all,

Definitely, I am not that clever with projection systems: I would like 
to read a *.prj file (part of a set of ESRI shapefiles) and get it in a 
CRS compatible format (handled by the  PROJ.4 projection system) and 
possibly added to sp objects where needed. What would would be the 
simplest way to do that? I am messing with rgdal and sp without result...

Cheers,

Patrick



From Roger.Bivand at nhh.no  Thu May 17 16:34:30 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 17 May 2007 16:34:30 +0200 (CEST)
Subject: [R-sig-Geo] read prj file to CRS
In-Reply-To: <464C5207.8060305@univ-fcomte.fr>
Message-ID: <Pine.LNX.4.44.0705171629170.24205-100000@reclus.nhh.no>

On Thu, 17 May 2007, Patrick Giraudoux wrote:

> Dear all,
> 
> Definitely, I am not that clever with projection systems: I would like 
> to read a *.prj file (part of a set of ESRI shapefiles) and get it in a 
> CRS compatible format (handled by the  PROJ.4 projection system) and 
> possibly added to sp objects where needed. What would would be the 
> simplest way to do that? I am messing with rgdal and sp without result...
> 

Yes, good idea, given that the reverse is provided in showWKT() in rgdal. 
As things are, you'll need to read it as part of a shapefile - readOGR() 
will read it when given a shapefile to read, or readGDAL() with an Arc 
Ascii grid, driver AAIGrid. I'll look at putting a simple *.prj reader 
into rgdal, but for now giving an arbitrary shapefile or Ascii grid the 
same name should do it.

Hope this helps,

Roger

> Cheers,
> 
> Patrick
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From patrick.giraudoux at univ-fcomte.fr  Thu May 17 17:28:50 2007
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Thu, 17 May 2007 17:28:50 +0200
Subject: [R-sig-Geo] read prj file to CRS
In-Reply-To: <Pine.LNX.4.44.0705171629170.24205-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0705171629170.24205-100000@reclus.nhh.no>
Message-ID: <464C74B2.4030309@univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070517/14eddf11/attachment.pl>

From Roger.Bivand at nhh.no  Thu May 17 20:34:21 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 17 May 2007 20:34:21 +0200 (CEST)
Subject: [R-sig-Geo] read prj file to CRS
In-Reply-To: <464C74B2.4030309@univ-fcomte.fr>
Message-ID: <Pine.LNX.4.44.0705172026590.24205-100000@reclus.nhh.no>

On Thu, 17 May 2007, Patrick Giraudoux wrote:

> Roger Bivand a ?crit :
> > On Thu, 17 May 2007, Patrick Giraudoux wrote:
> >
> >   
> >> Dear all,
> >>
> >> Definitely, I am not that clever with projection systems: I would like 
> >> to read a *.prj file (part of a set of ESRI shapefiles) and get it in a 
> >> CRS compatible format (handled by the  PROJ.4 projection system) and 
> >> possibly added to sp objects where needed. What would would be the 
> >> simplest way to do that? I am messing with rgdal and sp without result...
> >>
> >>     
> >
> > Yes, good idea, given that the reverse is provided in showWKT() in rgdal. 
> > As things are, you'll need to read it as part of a shapefile - readOGR() 
> > will read it when given a shapefile to read, or readGDAL() with an Arc 
> > Ascii grid, driver AAIGrid. I'll look at putting a simple *.prj reader 
> > into rgdal, but for now giving an arbitrary shapefile or Ascii grid the 
> > same name should do it.
> >
> > Hope this helps,
> >
> > Roger
> >
> >   
> OK, thanks. I went through it with readOGR(). I was messing with the 
> readOGR documentation not understanding about how to read a shapefile 
> specifically. Googling on the internet I found an example 
> (https://stat.ethz.ch/pipermail/r-sig-geo/2007-February/001761.html) 
> which made me make:
> 
> mymap<-readOGR("Mth030607.shp", layer="Mth030607")

Or simpler:

mymap <- readOGR(dsn=".", layer="Mth030607")

where dsn= is a directory (data source name) containing one or more 
shapefiles. Because OGR supports many formats, it has to handle many 
different ways of defining the data sources, and that is why dsn= and 
layer= are split in this way.

writeOGR() uses similar mechanisms to readOGR(), and for most users should 
be prefered to the maptools shapefile functions. The exception is if you 
have difficulty in installing rgdal on your platform, because both 
Linux/Unix and Mac OSX need to link to external libraries.

Roger

> 
> which worked perfectly.
> 
> The trick was to understand that the first argurment 'dsn' expected a 
> full shapefile name (before I tried the name "ESRI Shapefile" given by 
> ogrDrivers(), and other deadlocks), and the second, 'layer', the file 
> name truncated without its suffix. May I suggest to write an example 
> about shapefiles in the readOGR() doc ?
> 
> Now there is no trouble to extract de CRS
> 
> getSlots(class(mymap))
>         data   coords.nrs       coords         bbox  proj4string
> "data.frame"    "numeric"     "matrix"     "matrix"        "CRS"
> 
> mymap at proj4string
> CRS arguments:
>  +proj=lcc +lat_1=46.8 +lat_0=46.8 +lon_0=2.337229166666667 
> +k_0=0.99987742 +x_0=600000
> +y_0=2200000 +a=6378249.2 +b=6356514.999904194 +units=m +no_defs
> 
> or more simply:
>  > proj4string(mymap)
> [1] " +proj=lcc +lat_1=46.8 +lat_0=46.8 +lon_0=2.337229166666667 
> +k_0=0.99987742 +x_0=600000 +y_0=2200000 +a=6378249.2 
> +b=6356514.999904194 +units=m +no_defs"
> 
> Actually, I was used to WRITE *.prj files like that:
> 
> myproj<-showWKT(proj4string(mymap))
> cat(myproj, file="Mth030607.prj")
> 
> Now I can close the circle writing AND reading... I am quite lazy with 
> projection files (writing is source of many mistakes), so it will be 
> easy to pick them up from already existing *.prj files and to create and 
> add CRS to spatial objects and write the corresponding *.prj when 
> shapefiles are written (eg with readShapePoly(), etc.). No longer need 
> the ESRI ArcToolbox.
> 
> Thanks,
> 
> Patrick
> 
> 
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From andrew.niccolai at yale.edu  Thu May 17 23:25:26 2007
From: andrew.niccolai at yale.edu (Andrew Niccolai)
Date: Thu, 17 May 2007 17:25:26 -0400
Subject: [R-sig-Geo] Irregularly spaced 3D point clustering / segmentation
In-Reply-To: <Pine.LNX.4.44.0705172026590.24205-100000@reclus.nhh.no>
Message-ID: <200705172125.l4HLPQCK028539@pantheon-po06.its.yale.edu>

Greetings fellow R users,

I would really enjoy (and eagerly anticipate) any discussions on ideas for
handling a LIDAR (laser) data set of a New England forest.  The LIDAR
dataset is essentially xyz coordinates that form an irregularly spaced 3D
data cloud of points.  I have brought the data in as SpatialPointsDataFrame,
SpatialPixelsDataFrame, SpatialGridDataFrame, marked Point Pattern Process
objects, matrices etc.  I can view the interpolated surface with ?interp in
library(akima) as well as 3D points and surfaces in library(rgl).

So, importing the LIDAR data and viewing it or exporting it so that ImageJ
can handle it is not the issue.  

The LIDAR data set essentially produces a set of "mounds" from the elevation
data recorded in the z variable.  Each "mound" represents a tree in the
forest.  I am hoping to get some ideas on ways to cluster this data set so
that I can isolate each mound for further analysis and segmentation.  One
possibility that I have looked into with Matlab software is
"marker-controlled watershed segmentation".  This essentially inverts the
interpolated surface and "fills" the inverted image with "water" starting at
the local minimas until the water starts to spill over into the next
watershed at which point it builds a "dam" between local valleys.  This is a
function in Matlab and I haven't been able to see the code to bring it to R.

Any ideas on this method or suggestions for better methods to isolate
"mounds" in 3D space?  Template matching, perhaps??

Thanks in advance and thanks to all the innovative producers and users of
the R domain!!  
Andrew



From mdsumner at utas.edu.au  Thu May 17 23:32:52 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Fri, 18 May 2007 07:32:52 +1000
Subject: [R-sig-Geo] Irregularly spaced 3D point clustering /
	segmentation
In-Reply-To: <200705172125.l4HLPQCK028539@pantheon-po06.its.yale.edu>
References: <200705172125.l4HLPQCK028539@pantheon-po06.its.yale.edu>
Message-ID: <464CCA04.4080505@utas.edu.au>

Hello,

 I have no suggestions for helping yet, but I'd rather like to play with 
some LIDAR forest data. Is there some publicly available that you can 
point me to?  I have messed around with rgl for interactive 3-D view of 
similar things, and would like to explore more.

Cheers, Mike.

Andrew Niccolai wrote:
> Greetings fellow R users,
>
> I would really enjoy (and eagerly anticipate) any discussions on ideas for
> handling a LIDAR (laser) data set of a New England forest.  The LIDAR
> dataset is essentially xyz coordinates that form an irregularly spaced 3D
> data cloud of points.  I have brought the data in as SpatialPointsDataFrame,
> SpatialPixelsDataFrame, SpatialGridDataFrame, marked Point Pattern Process
> objects, matrices etc.  I can view the interpolated surface with ?interp in
> library(akima) as well as 3D points and surfaces in library(rgl).
>
> So, importing the LIDAR data and viewing it or exporting it so that ImageJ
> can handle it is not the issue.  
>
> The LIDAR data set essentially produces a set of "mounds" from the elevation
> data recorded in the z variable.  Each "mound" represents a tree in the
> forest.  I am hoping to get some ideas on ways to cluster this data set so
> that I can isolate each mound for further analysis and segmentation.  One
> possibility that I have looked into with Matlab software is
> "marker-controlled watershed segmentation".  This essentially inverts the
> interpolated surface and "fills" the inverted image with "water" starting at
> the local minimas until the water starts to spill over into the next
> watershed at which point it builds a "dam" between local valleys.  This is a
> function in Matlab and I haven't been able to see the code to bring it to R.
>
> Any ideas on this method or suggestions for better methods to isolate
> "mounds" in 3D space?  Template matching, perhaps??
>
> Thanks in advance and thanks to all the innovative producers and users of
> the R domain!!  
> Andrew
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>



From andrew.niccolai at yale.edu  Thu May 17 23:46:31 2007
From: andrew.niccolai at yale.edu (Andrew Niccolai)
Date: Thu, 17 May 2007 17:46:31 -0400
Subject: [R-sig-Geo] Irregularly spaced 3D point clustering /
	segmentation
In-Reply-To: <464CCA04.4080505@utas.edu.au>
Message-ID: <200705172146.l4HLkVsi029711@pantheon-po05.its.yale.edu>

Absolutely.  I am waiting on a LIDAR data set for my research site and
thought that I would get a head start with building the code to segment the
data. So, I downloaded the US Forest Service free software FUSION that was
built to visualize LIDAR data.  This software comes with an example set from
the Pacific Northwest including an orthophoto of the same site.  This is the
data that I am currently using while awaiting my own.

Hope this helps.  Let me know if you end up making any headway with
separating out the trees....

cheers

Andrew Niccolai
Doctoral Candidate
Yale University
(203) 432-5144
-----Original Message-----
From: Michael Sumner [mailto:mdsumner at utas.edu.au] 
Sent: Thursday, May 17, 2007 5:33 PM
To: Andrew Niccolai
Cc: r-sig-geo at stat.math.ethz.ch
Subject: Re: [R-sig-Geo] Irregularly spaced 3D point clustering /
segmentation

Hello,

 I have no suggestions for helping yet, but I'd rather like to play with 
some LIDAR forest data. Is there some publicly available that you can 
point me to?  I have messed around with rgl for interactive 3-D view of 
similar things, and would like to explore more.

Cheers, Mike.

Andrew Niccolai wrote:
> Greetings fellow R users,
>
> I would really enjoy (and eagerly anticipate) any discussions on ideas for
> handling a LIDAR (laser) data set of a New England forest.  The LIDAR
> dataset is essentially xyz coordinates that form an irregularly spaced 3D
> data cloud of points.  I have brought the data in as
SpatialPointsDataFrame,
> SpatialPixelsDataFrame, SpatialGridDataFrame, marked Point Pattern Process
> objects, matrices etc.  I can view the interpolated surface with ?interp
in
> library(akima) as well as 3D points and surfaces in library(rgl).
>
> So, importing the LIDAR data and viewing it or exporting it so that ImageJ
> can handle it is not the issue.  
>
> The LIDAR data set essentially produces a set of "mounds" from the
elevation
> data recorded in the z variable.  Each "mound" represents a tree in the
> forest.  I am hoping to get some ideas on ways to cluster this data set so
> that I can isolate each mound for further analysis and segmentation.  One
> possibility that I have looked into with Matlab software is
> "marker-controlled watershed segmentation".  This essentially inverts the
> interpolated surface and "fills" the inverted image with "water" starting
at
> the local minimas until the water starts to spill over into the next
> watershed at which point it builds a "dam" between local valleys.  This is
a
> function in Matlab and I haven't been able to see the code to bring it to
R.
>
> Any ideas on this method or suggestions for better methods to isolate
> "mounds" in 3D space?  Template matching, perhaps??
>
> Thanks in advance and thanks to all the innovative producers and users of
> the R domain!!  
> Andrew
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>



From mdsumner at utas.edu.au  Fri May 18 04:51:45 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Fri, 18 May 2007 12:51:45 +1000
Subject: [R-sig-Geo] Irregularly spaced 3D point clustering /
	segmentation
In-Reply-To: <200705172146.l4HLkVsi029711@pantheon-po05.its.yale.edu>
References: <200705172146.l4HLkVsi029711@pantheon-po05.its.yale.edu>
Message-ID: <464D14C1.4040401@utas.edu.au>

Hello,

I had some success with visualization as follows.

I calculated the "forest floor" elevation "minElev" by calculating the 
minimum height in a set of grid cells - you could use 
as.SpatialPolygons.GridTopology
and overlay to achieve the same, but I reverted to GIS to get a quick 
result.  If you have a good DEM you will only need to overlay those 
heights to the points.


library(rgdal)
library(trip)  ## just for my oc.colors function  - any color function 
will do

z <- d$elev - d$minElev  ## colourize based on height above ground

## this is taken from surface3d
z <- z - min(z)
zlim <- range(z)
zlim <- zlim - min(zlim)
zlen <- zlim[2] - zlim[1] + 1
colorlut <- oc.colors(zlen) # height color lookup table
col <- colorlut[z -zlim[1]+1 ] # assign colors to heights for each point
   
## subsample it for sanity until you are happy it's right
 sub <- sample(1:nrow(d), 5000) 
## sub <- 1:nrow(d)
 plot3d(d[sub,1], d[sub,2], d[sub, 3], aspect = "iso", col = col, size = 3)

Sample images here:
http://staff.acecrc.org.au/~mdsumner/R/lidar.png
http://staff.acecrc.org.au/~mdsumner/R/lidar2.png

That might help you with your clustering explorations.

Cheers, Mike.

Andrew Niccolai wrote:
> Absolutely.  I am waiting on a LIDAR data set for my research site and
> thought that I would get a head start with building the code to segment the
> data. So, I downloaded the US Forest Service free software FUSION that was
> built to visualize LIDAR data.  This software comes with an example set from
> the Pacific Northwest including an orthophoto of the same site.  This is the
> data that I am currently using while awaiting my own.
>
> Hope this helps.  Let me know if you end up making any headway with
> separating out the trees....
>
> cheers
>
> Andrew Niccolai
> Doctoral Candidate
> Yale University
> (203) 432-5144
> -----Original Message-----
> From: Michael Sumner [mailto:mdsumner at utas.edu.au] 
> Sent: Thursday, May 17, 2007 5:33 PM
> To: Andrew Niccolai
> Cc: r-sig-geo at stat.math.ethz.ch
> Subject: Re: [R-sig-Geo] Irregularly spaced 3D point clustering /
> segmentation
>
> Hello,
>
>  I have no suggestions for helping yet, but I'd rather like to play with 
> some LIDAR forest data. Is there some publicly available that you can 
> point me to?  I have messed around with rgl for interactive 3-D view of 
> similar things, and would like to explore more.
>
> Cheers, Mike.
>
> Andrew Niccolai wrote:
>   
>> Greetings fellow R users,
>>
>> I would really enjoy (and eagerly anticipate) any discussions on ideas for
>> handling a LIDAR (laser) data set of a New England forest.  The LIDAR
>> dataset is essentially xyz coordinates that form an irregularly spaced 3D
>> data cloud of points.  I have brought the data in as
>>     
> SpatialPointsDataFrame,
>   
>> SpatialPixelsDataFrame, SpatialGridDataFrame, marked Point Pattern Process
>> objects, matrices etc.  I can view the interpolated surface with ?interp
>>     
> in
>   
>> library(akima) as well as 3D points and surfaces in library(rgl).
>>
>> So, importing the LIDAR data and viewing it or exporting it so that ImageJ
>> can handle it is not the issue.  
>>
>> The LIDAR data set essentially produces a set of "mounds" from the
>>     
> elevation
>   
>> data recorded in the z variable.  Each "mound" represents a tree in the
>> forest.  I am hoping to get some ideas on ways to cluster this data set so
>> that I can isolate each mound for further analysis and segmentation.  One
>> possibility that I have looked into with Matlab software is
>> "marker-controlled watershed segmentation".  This essentially inverts the
>> interpolated surface and "fills" the inverted image with "water" starting
>>     
> at
>   
>> the local minimas until the water starts to spill over into the next
>> watershed at which point it builds a "dam" between local valleys.  This is
>>     
> a
>   
>> function in Matlab and I haven't been able to see the code to bring it to
>>     
> R.
>   
>> Any ideas on this method or suggestions for better methods to isolate
>> "mounds" in 3D space?  Template matching, perhaps??
>>
>> Thanks in advance and thanks to all the innovative producers and users of
>> the R domain!!  
>> Andrew
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
>>   
>>     
>
>
>



From mdsumner at utas.edu.au  Fri May 18 04:56:50 2007
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Fri, 18 May 2007 12:56:50 +1000
Subject: [R-sig-Geo] Irregularly spaced 3D point clustering /segmentation
In-Reply-To: <464D14C1.4040401@utas.edu.au>
References: <200705172146.l4HLkVsi029711@pantheon-po05.its.yale.edu>
	<464D14C1.4040401@utas.edu.au>
Message-ID: <464D15F2.10109@utas.edu.au>

Sorry, that should be

library(rgl)  ## not rgdal!

And "elev" and "minElev" are the lidar elevation, and the forest floor 
(approx) elevation.

Cheers, Mike.
> library(rgdal)
> library(trip)  ## just for my oc.colors function  - any color function 
> will do
>
> z <- d$elev - d$minElev  ## colourize based on height above ground
>
> ## this is taken from surface3d
> z <- z - min(z)
> zlim <- range(z)
> zlim <- zlim - min(zlim)
> zlen <- zlim[2] - zlim[1] + 1
> colorlut <- oc.colors(zlen) # height color lookup table
> col <- colorlut[z -zlim[1]+1 ] # assign colors to heights for each point
>    
> ## subsample it for sanity until you are happy it's right
>  sub <- sample(1:nrow(d), 5000) 
> ## sub <- 1:nrow(d)
>  plot3d(d[sub,1], d[sub,2], d[sub, 3], aspect = "iso", col = col, size = 3)
>
> Sample images here:
> http://staff.acecrc.org.au/~mdsumner/R/lidar.png
> http://staff.acecrc.org.au/~mdsumner/R/lidar2.png
>
> That might help you with your clustering explorations.
>
> Cheers, Mike.
>
> Andrew Niccolai wrote:
>   
>> Absolutely.  I am waiting on a LIDAR data set for my research site and
>> thought that I would get a head start with building the code to segment the
>> data. So, I downloaded the US Forest Service free software FUSION that was
>> built to visualize LIDAR data.  This software comes with an example set from
>> the Pacific Northwest including an orthophoto of the same site.  This is the
>> data that I am currently using while awaiting my own.
>>
>> Hope this helps.  Let me know if you end up making any headway with
>> separating out the trees....
>>
>> cheers
>>
>> Andrew Niccolai
>> Doctoral Candidate
>> Yale University
>> (203) 432-5144
>> -----Original Message-----
>> From: Michael Sumner [mailto:mdsumner at utas.edu.au] 
>> Sent: Thursday, May 17, 2007 5:33 PM
>> To: Andrew Niccolai
>> Cc: r-sig-geo at stat.math.ethz.ch
>> Subject: Re: [R-sig-Geo] Irregularly spaced 3D point clustering /
>> segmentation
>>
>> Hello,
>>
>>  I have no suggestions for helping yet, but I'd rather like to play with 
>> some LIDAR forest data. Is there some publicly available that you can 
>> point me to?  I have messed around with rgl for interactive 3-D view of 
>> similar things, and would like to explore more.
>>
>> Cheers, Mike.
>>
>> Andrew Niccolai wrote:
>>   
>>     
>>> Greetings fellow R users,
>>>
>>> I would really enjoy (and eagerly anticipate) any discussions on ideas for
>>> handling a LIDAR (laser) data set of a New England forest.  The LIDAR
>>> dataset is essentially xyz coordinates that form an irregularly spaced 3D
>>> data cloud of points.  I have brought the data in as
>>>     
>>>       
>> SpatialPointsDataFrame,
>>   
>>     
>>> SpatialPixelsDataFrame, SpatialGridDataFrame, marked Point Pattern Process
>>> objects, matrices etc.  I can view the interpolated surface with ?interp
>>>     
>>>       
>> in
>>   
>>     
>>> library(akima) as well as 3D points and surfaces in library(rgl).
>>>
>>> So, importing the LIDAR data and viewing it or exporting it so that ImageJ
>>> can handle it is not the issue.  
>>>
>>> The LIDAR data set essentially produces a set of "mounds" from the
>>>     
>>>       
>> elevation
>>   
>>     
>>> data recorded in the z variable.  Each "mound" represents a tree in the
>>> forest.  I am hoping to get some ideas on ways to cluster this data set so
>>> that I can isolate each mound for further analysis and segmentation.  One
>>> possibility that I have looked into with Matlab software is
>>> "marker-controlled watershed segmentation".  This essentially inverts the
>>> interpolated surface and "fills" the inverted image with "water" starting
>>>     
>>>       
>> at
>>   
>>     
>>> the local minimas until the water starts to spill over into the next
>>> watershed at which point it builds a "dam" between local valleys.  This is
>>>     
>>>       
>> a
>>   
>>     
>>> function in Matlab and I haven't been able to see the code to bring it to
>>>     
>>>       
>> R.
>>   
>>     
>>> Any ideas on this method or suggestions for better methods to isolate
>>> "mounds" in 3D space?  Template matching, perhaps??
>>>
>>> Thanks in advance and thanks to all the innovative producers and users of
>>> the R domain!!  
>>> Andrew
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at stat.math.ethz.ch
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>
>>>   
>>>     
>>>       
>>
>>     
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>



From Roger.Bivand at nhh.no  Fri May 18 21:11:23 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 18 May 2007 21:11:23 +0200 (CEST)
Subject: [R-sig-Geo] efficient code/function for rectangular SP weight
 Matrix and gwr
In-Reply-To: <464491D5.9070809@biomserv.univ-lyon1.fr>
Message-ID: <Pine.LNX.4.44.0705182107160.24878-100000@reclus.nhh.no>

On Fri, 11 May 2007, St?phane Dray wrote:

> Hi Sam,
> 
> I think that this question is quite general and could interest other 
> people, including me, with very different aims. I have developed a 
> method to look for the relationships between two data sets that have 
> been sampled on the same area but for different locations. In my 
> example, the two samples are two polygons layers. In this approach, I 
> compute a rectangular weighting matrix where each weight correspond to 
> the area of intersection between polygons of each layer. I have used 
> also the matrix form to store these weights (my data set was very small 
> compared to you). I remember that Roger was also interested by these 
> rectangular weights in another context. Here we  have different  problems:
> - how to compute these kind  of weights
> - how to store them.
> 
> For the first point, I think that for each method/application, the 
> solution  is different. We could develop/extend classical tools for 
> square weights (one set of spatial units) to rectangular weights (two 
> sets of spatial units).
> For the second one, It would be probably interesting to define a class 
> of object in spdep. nb objects are lists, and I think that it would be 
> the solution for rectangular neighborhood.
> 
> If I consider two sets of spatial units (A and B) where the number of 
> units is equal to na and nb.  We could store the neighbors in a list of 
> length 2. The first element of this list is a list of length na. In this 
> list, the j-th element is a vector of the neighbors of the j-th unit of 
> the layer A. These neighbors are spatial units of the layer B.  The 
> second element of the global list is a list of length nb where each 
> element is a vector of neighbors.
> 
> I think that we have to think to a class of object that could be useful 
> for everybody dealing with this kind of rectangular weights. If this 
> class is properly defined (second point), we could then develop tools to 
> construct this kind of neighborhoods (first point). The eventual 
> extension to more than two data sets could also be taken into account in 
> this reflexion.
> 

I would welcome input on this. I'm looking at an alternative weights 
representation through classes in the Matrix package, which is evolving 
fast, and which seems to be promising. If the dimnames slot is used to 
hold the region.id values, it might be possible to make progress.

Best wishes,

Roger

> Cheers,
> 
> 
> Sam Field wrote:
> > List,
> >
> > I need to create a rectangular spatial weight matrix for a set of n and 
> > m objects. I quickly run in to memory allocation problems when 
> > constructing the full matrix in a single pass. I am looking for a more 
> > efficient way of doing this. There appears to be efficient procedures in 
> > spdep for constructing SQUARE spatial weight matrices (e.g. 
> > dnearneigh()). Are there analogous procedures for constructing distance 
> > based weights between two different point patterns? I am doing this in 
> > preparation for implementing an approximate geographically weighted 
> > logistic regression procedure. I was thinking about using re sampling 
> > procedure as an inferential frame- perhaps I might get some feedback. 
> > This is what I was going to do.
> >
> > I have a point pattern of 30,000 diabetic people based on where they 
> > lived during a 2 year period. During that period, approximately 4% of 
> > them developed diabetes. I am interested in isolating the impact of 
> > ecological factors on the geographic variation" of the disease, so it is 
> > necessary to control for the spatial clustering of individual level risk 
> > factors associated with the disease (diabetes).
> >
> > Step 1: Estimate a logistic regression using the full sample and predict 
> > incidence diabetes using individual level covariates (i.e. who developed 
> > diabetes over the two year period).
> >
> > Step 2. Estimate a weighted logit model at each location (grid). The 
> > observations would be the people (not the geographic units) and the 
> > weights would be kernel weights based on distance. The model would only 
> > contain a single freely estimated parameter, the intercept, but it would 
> > also contain an offset term. For each patient, the offset term would 
> > simply be an evaluation of the linear predictor of the global model 
> > estimated above (based on the observed covariate values), but without 
> > the intercept. This would effectively fix the estimates of the patient 
> > level coefficients to their global values, requiring only a local 
> > estimate of the intercept. My hope is that I could interpret geographic 
> > variability in the intercept as evidence for a "location effect" net of 
> > the patient composition or "risk profile" at a particular location. It 
> > would probably make sense to center the X variables so that the 
> > intercept was interpretable and estimated in a region of the response 
> > plane where their is plenty of data. I would let the other covariates 
> > vary as well, but I doubt the model could be estimated in large portions 
> > of the study area because of sparse data.
> >
> > Step 3. If I were going to do inference on the location specific 
> > intercepts, I would generate a sampling distribution at each location by 
> > re sampling from the global model, and repeat Step 2 for each randomly 
> > drawn sample. This would give me a local sampling distribution of 
> > intercept estimates at each location and I could compare it to the the 
> > single one generated from the observed data. The global model represents 
> > a kind of null because the intercept is fixed to its global value and 
> > geographic variability is driven entirely by the spatial clustering of 
> > patient level factors.
> >
> >
> > thanks!
> >
> > Sam
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at stat.math.ethz.ch
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
> >
> >   
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Sun May 20 18:07:25 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 20 May 2007 18:07:25 +0200 (CEST)
Subject: [R-sig-Geo] R spatial workshop, Bergen June 15
Message-ID: <Pine.LNX.4.44.0705201801580.31246-100000@reclus.nhh.no>

There are some places available on the R spatial workshop to be held at 
the Law Faculty building, University of Bergen, 15 June, 1330-1715, in 
Seminar Room A:

http://spatial.nhh.no/ngm07/Nordic_Geographers_Meeting.kmz

Please contact me directly if you are interested, there is no charge for 
the workshop itself.

The workshop is described at:

http://www.uib.no/ngm/content/13w.shop.htm

Best wishes,

Roger Bivand

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From dtaxk at columbus.rr.com  Tue May 22 13:16:30 2007
From: dtaxk at columbus.rr.com (Morgan O. Kate)
Date: Tue, 22 May 2007 13:16:30 +0200
Subject: [R-sig-Geo] NET development experience.
Message-ID: <002601c79c62$a5e4ecc0$aec2667a@lbuo>

News Releases Warming Investors Up On GPSI

Global Pay Solutions Inc.
Symbol: GPSI
Close: $0.031

Recent news releases have excited investors. Heavy trading pushed share
prices up to highs of $0.08 (UP 266%). Closing at $0.031 (UP 3%) we
expect to see more traffic and prices shooting up all week. This one is
getting started. Don't miss it. Get on GPSI first thing Tuesday!

Ability to work with different teams and provide thought leadership in
the development area.
Knowledge of trading and risk systems. Experience in Use Case Analysis
is preferred.
Whether it be Equities or Fixed Income, Foreign Exchange or Commodities,
to be an elite performer,  you must combine a gift for software
engineering, with a strong financial  services acumen.
Familiarities with different capital markets.
, mainframe to distributed. Good problem solving skills.
Knowledge of Java HTML, and Perl. NET, and XML Application development.
Analytics behind various Trading Strategies. Ability to work
independently in a fast-paced environment. This will include reviewing
client configurations and infrastructure configurations; reproducing
issues in a simulation environment; and in some cases working with
customer directly to troubleshoot.
Hands-on experience in deploying and maintaining web applications with
intensive disk IO and database system workload.
Successful track record in building and managing strong IT team.
Experiences in solving complex problems within computation.
In this role you may fill several roles on a single project or play a
single role across several projects.
Specific experience with ETL tools, Business Intelligence tools, and
parallel database servers.
Experience working with outsource teams is a plus.
Strong attention to details. Strong work ethic, self-starter, and result
oriented.
NET, that implement and track the GWM release management process from
start to finish. Familiarities with different capital markets.
Fixed Income knowledge is a plus. Programming experience with Windows .
NET, that implement and track the GWM release management process from
start to finish. NET experience is also a plus. If that describes you,
we'd like to hear from you.
In this role you will analyze, design, develop, and deploy business
application systems and document coded procedures and business
processes. FIX knowledge - EMS, Execution Management System experience.
Team player with strong communication skills.
Good communication skills.
Outstanding OO knowledge and strong UML experience.
Position would be Front Office facing. Knowledge of Java programming.
Programming experience with Windows .
This candidate will support personnel act as the front line of
development and support to the ABS Trading Desk in NY.
Proven experience designing and operating critical production systems on
UNIX.
Experience in managing QA processes and QA team.
Experience in developing test plans. NET and Java technology stacks.
Wall Street is looking for candidates who can solve real business
problems using financial technology.
Experience with data modeling, UI development and a familiarity with GTK
US day shift is a plus.
Initially most of the work will be focused on implementing new features
and modifications to the existing components.



From fieldsh at mail.med.upenn.edu  Tue May 22 21:27:44 2007
From: fieldsh at mail.med.upenn.edu (Sam Field)
Date: Tue, 22 May 2007 15:27:44 -0400
Subject: [R-sig-Geo] efficient code/function for rectangular SP weight
 Matrix and gwr
In-Reply-To: <Pine.LNX.4.44.0705182107160.24878-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0705182107160.24878-100000@reclus.nhh.no>
Message-ID: <46534430.3040708@mail.med.upenn.edu>

In the event that others are following this thread,  I have included 
some r-code for constructing a rectangular weight matrix from two sets 
of coordinates (in feet). The first set of coords (nhood_pts[,1] and 
nhood_pts[,2]) come from a regular grid, the second set refer to the 
location of patients ( combo2$X and combo2$Y).  I don't provide the data 
only the code below.  The original data consists of 1000 grids and 
30000+ patients. It takes 30 minutes on my fairly speedy computer to 
complete the first step- extracting pair-wise distances less then a 
given bandwidth (D). I store the neighbor ids and counts in separate 
lists.   The rest of the code calculates kernel weights based on these 
distances. The code is not efficient from any number of standpoints, but 
storing these distances in a list rather then a matrix avoided running 
into memory allocation problems. Here is the code.

# define bandwidth
D <- 2*5280


#This creates a list of distances < 1 mile
w_raw <- vector("list",length(nhood_pts[,1]))
for (i in 1:length(nhood_pts[,1])){
w_raw[[i]] <- rep(NA,length(combo2$X))}

system.time(for (i in 1:length(nhood_pts[,1])){for(j in 1:length(combo2$X)){
if(sqrt((nhood_pts[,1][i]-combo2$X[j])^2 + 
(nhood_pts[,2][i]-combo2$Y[j])^2) < D) w_raw[[i]][j] <-  
sqrt((nhood_pts[,1][i]-combo2$X[j])^2 + 
(nhood_pts[,2][i]-combo2$Y[j])^2)}} )



w_raw.id <-   vector("list",length(nhood_pts[,1]))
for (i in 1:length(nhood_pts[,1])){
w_raw.id[[i]] <-    which(!is.na(w_raw[[i]]))}

for (i in 1:length(nhood_pts[,1])){
w_raw[[i]] <-  w_raw[[i]][w_raw.id[[i]]]}


w_raw.count <-   rep(1,length(nhood_pts[,1]))
for (i in 1:length(nhood_pts[,1])){
w_raw.count[i] <-  length(w_raw[[i]])}

w_weights <-  vector("list",length(nhood_pts[,1]))
for (i in 1:length(nhood_pts[,1])){
w_weights[[i]] <- (1-(w_raw[[i]]/D)^2)^2}

w <-  vector("list",length(nhood_pts[,1]))
for (i in 1:length(nhood_pts[,1])){
w[[i]] <- w_weights[[i]]/sum(w_weights[[i]])}





Roger Bivand wrote:
> On Fri, 11 May 2007, St?phane Dray wrote:
>
>   
>> Hi Sam,
>>
>> I think that this question is quite general and could interest other 
>> people, including me, with very different aims. I have developed a 
>> method to look for the relationships between two data sets that have 
>> been sampled on the same area but for different locations. In my 
>> example, the two samples are two polygons layers. In this approach, I 
>> compute a rectangular weighting matrix where each weight correspond to 
>> the area of intersection between polygons of each layer. I have used 
>> also the matrix form to store these weights (my data set was very small 
>> compared to you). I remember that Roger was also interested by these 
>> rectangular weights in another context. Here we  have different  problems:
>> - how to compute these kind  of weights
>> - how to store them.
>>
>> For the first point, I think that for each method/application, the 
>> solution  is different. We could develop/extend classical tools for 
>> square weights (one set of spatial units) to rectangular weights (two 
>> sets of spatial units).
>> For the second one, It would be probably interesting to define a class 
>> of object in spdep. nb objects are lists, and I think that it would be 
>> the solution for rectangular neighborhood.
>>
>> If I consider two sets of spatial units (A and B) where the number of 
>> units is equal to na and nb.  We could store the neighbors in a list of 
>> length 2. The first element of this list is a list of length na. In this 
>> list, the j-th element is a vector of the neighbors of the j-th unit of 
>> the layer A. These neighbors are spatial units of the layer B.  The 
>> second element of the global list is a list of length nb where each 
>> element is a vector of neighbors.
>>
>> I think that we have to think to a class of object that could be useful 
>> for everybody dealing with this kind of rectangular weights. If this 
>> class is properly defined (second point), we could then develop tools to 
>> construct this kind of neighborhoods (first point). The eventual 
>> extension to more than two data sets could also be taken into account in 
>> this reflexion.
>>
>>     
>
> I would welcome input on this. I'm looking at an alternative weights 
> representation through classes in the Matrix package, which is evolving 
> fast, and which seems to be promising. If the dimnames slot is used to 
> hold the region.id values, it might be possible to make progress.
>
> Best wishes,
>
> Roger
>
>   
>> Cheers,
>>
>>
>> Sam Field wrote:
>>     
>>> List,
>>>
>>> I need to create a rectangular spatial weight matrix for a set of n and 
>>> m objects. I quickly run in to memory allocation problems when 
>>> constructing the full matrix in a single pass. I am looking for a more 
>>> efficient way of doing this. There appears to be efficient procedures in 
>>> spdep for constructing SQUARE spatial weight matrices (e.g. 
>>> dnearneigh()). Are there analogous procedures for constructing distance 
>>> based weights between two different point patterns? I am doing this in 
>>> preparation for implementing an approximate geographically weighted 
>>> logistic regression procedure. I was thinking about using re sampling 
>>> procedure as an inferential frame- perhaps I might get some feedback. 
>>> This is what I was going to do.
>>>
>>> I have a point pattern of 30,000 diabetic people based on where they 
>>> lived during a 2 year period. During that period, approximately 4% of 
>>> them developed diabetes. I am interested in isolating the impact of 
>>> ecological factors on the geographic variation" of the disease, so it is 
>>> necessary to control for the spatial clustering of individual level risk 
>>> factors associated with the disease (diabetes).
>>>
>>> Step 1: Estimate a logistic regression using the full sample and predict 
>>> incidence diabetes using individual level covariates (i.e. who developed 
>>> diabetes over the two year period).
>>>
>>> Step 2. Estimate a weighted logit model at each location (grid). The 
>>> observations would be the people (not the geographic units) and the 
>>> weights would be kernel weights based on distance. The model would only 
>>> contain a single freely estimated parameter, the intercept, but it would 
>>> also contain an offset term. For each patient, the offset term would 
>>> simply be an evaluation of the linear predictor of the global model 
>>> estimated above (based on the observed covariate values), but without 
>>> the intercept. This would effectively fix the estimates of the patient 
>>> level coefficients to their global values, requiring only a local 
>>> estimate of the intercept. My hope is that I could interpret geographic 
>>> variability in the intercept as evidence for a "location effect" net of 
>>> the patient composition or "risk profile" at a particular location. It 
>>> would probably make sense to center the X variables so that the 
>>> intercept was interpretable and estimated in a region of the response 
>>> plane where their is plenty of data. I would let the other covariates 
>>> vary as well, but I doubt the model could be estimated in large portions 
>>> of the study area because of sparse data.
>>>
>>> Step 3. If I were going to do inference on the location specific 
>>> intercepts, I would generate a sampling distribution at each location by 
>>> re sampling from the global model, and repeat Step 2 for each randomly 
>>> drawn sample. This would give me a local sampling distribution of 
>>> intercept estimates at each location and I could compare it to the the 
>>> single one generated from the observed data. The global model represents 
>>> a kind of null because the intercept is fixed to its global value and 
>>> geographic variability is driven entirely by the spatial clustering of 
>>> patient level factors.
>>>
>>>
>>> thanks!
>>>
>>> Sam
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at stat.math.ethz.ch
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>
>>>   
>>>       
>>
>>     
>
>



From Roger.Bivand at nhh.no  Tue May 22 22:49:23 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 22 May 2007 22:49:23 +0200 (CEST)
Subject: [R-sig-Geo] efficient code/function for rectangular SP weight
 Matrix and gwr
In-Reply-To: <46534430.3040708@mail.med.upenn.edu>
Message-ID: <Pine.LNX.4.44.0705222241080.9681-100000@reclus.nhh.no>

On Tue, 22 May 2007, Sam Field wrote:

> In the event that others are following this thread,  I have included 
> some r-code for constructing a rectangular weight matrix from two sets 
> of coordinates (in feet). The first set of coords (nhood_pts[,1] and 
> nhood_pts[,2]) come from a regular grid, the second set refer to the 
> location of patients ( combo2$X and combo2$Y).  I don't provide the data 
> only the code below.  The original data consists of 1000 grids and 
> 30000+ patients. It takes 30 minutes on my fairly speedy computer to 
> complete the first step- extracting pair-wise distances less then a 
> given bandwidth (D). I store the neighbor ids and counts in separate 
> lists.   The rest of the code calculates kernel weights based on these 
> distances. The code is not efficient from any number of standpoints, but 
> storing these distances in a list rather then a matrix avoided running 
> into memory allocation problems. Here is the code.

Would spDistsN1() in sp have helped to speed things up a little? Looping
over the 1k grid points and passing out the 30k patient points shouldn't
be very demanding (34 seconds on a oldish box, more if you need Great
Circle distances). For heavyweight things, quadtrees would be a better
choice, I have an unpublished package for approximate nearest neighbours.

Roger

> 
> # define bandwidth
> D <- 2*5280
> 
> 
> #This creates a list of distances < 1 mile
> w_raw <- vector("list",length(nhood_pts[,1]))
> for (i in 1:length(nhood_pts[,1])){
> w_raw[[i]] <- rep(NA,length(combo2$X))}
> 
> system.time(for (i in 1:length(nhood_pts[,1])){for(j in 1:length(combo2$X)){
> if(sqrt((nhood_pts[,1][i]-combo2$X[j])^2 + 
> (nhood_pts[,2][i]-combo2$Y[j])^2) < D) w_raw[[i]][j] <-  
> sqrt((nhood_pts[,1][i]-combo2$X[j])^2 + 
> (nhood_pts[,2][i]-combo2$Y[j])^2)}} )
> 
> 
> 
> w_raw.id <-   vector("list",length(nhood_pts[,1]))
> for (i in 1:length(nhood_pts[,1])){
> w_raw.id[[i]] <-    which(!is.na(w_raw[[i]]))}
> 
> for (i in 1:length(nhood_pts[,1])){
> w_raw[[i]] <-  w_raw[[i]][w_raw.id[[i]]]}
> 
> 
> w_raw.count <-   rep(1,length(nhood_pts[,1]))
> for (i in 1:length(nhood_pts[,1])){
> w_raw.count[i] <-  length(w_raw[[i]])}
> 
> w_weights <-  vector("list",length(nhood_pts[,1]))
> for (i in 1:length(nhood_pts[,1])){
> w_weights[[i]] <- (1-(w_raw[[i]]/D)^2)^2}
> 
> w <-  vector("list",length(nhood_pts[,1]))
> for (i in 1:length(nhood_pts[,1])){
> w[[i]] <- w_weights[[i]]/sum(w_weights[[i]])}
> 
> 
> 
> 
> 
> Roger Bivand wrote:
> > On Fri, 11 May 2007, St?phane Dray wrote:
> >
> >   
> >> Hi Sam,
> >>
> >> I think that this question is quite general and could interest other 
> >> people, including me, with very different aims. I have developed a 
> >> method to look for the relationships between two data sets that have 
> >> been sampled on the same area but for different locations. In my 
> >> example, the two samples are two polygons layers. In this approach, I 
> >> compute a rectangular weighting matrix where each weight correspond to 
> >> the area of intersection between polygons of each layer. I have used 
> >> also the matrix form to store these weights (my data set was very small 
> >> compared to you). I remember that Roger was also interested by these 
> >> rectangular weights in another context. Here we  have different  problems:
> >> - how to compute these kind  of weights
> >> - how to store them.
> >>
> >> For the first point, I think that for each method/application, the 
> >> solution  is different. We could develop/extend classical tools for 
> >> square weights (one set of spatial units) to rectangular weights (two 
> >> sets of spatial units).
> >> For the second one, It would be probably interesting to define a class 
> >> of object in spdep. nb objects are lists, and I think that it would be 
> >> the solution for rectangular neighborhood.
> >>
> >> If I consider two sets of spatial units (A and B) where the number of 
> >> units is equal to na and nb.  We could store the neighbors in a list of 
> >> length 2. The first element of this list is a list of length na. In this 
> >> list, the j-th element is a vector of the neighbors of the j-th unit of 
> >> the layer A. These neighbors are spatial units of the layer B.  The 
> >> second element of the global list is a list of length nb where each 
> >> element is a vector of neighbors.
> >>
> >> I think that we have to think to a class of object that could be useful 
> >> for everybody dealing with this kind of rectangular weights. If this 
> >> class is properly defined (second point), we could then develop tools to 
> >> construct this kind of neighborhoods (first point). The eventual 
> >> extension to more than two data sets could also be taken into account in 
> >> this reflexion.
> >>
> >>     
> >
> > I would welcome input on this. I'm looking at an alternative weights 
> > representation through classes in the Matrix package, which is evolving 
> > fast, and which seems to be promising. If the dimnames slot is used to 
> > hold the region.id values, it might be possible to make progress.
> >
> > Best wishes,
> >
> > Roger
> >
> >   
> >> Cheers,
> >>
> >>
> >> Sam Field wrote:
> >>     
> >>> List,
> >>>
> >>> I need to create a rectangular spatial weight matrix for a set of n and 
> >>> m objects. I quickly run in to memory allocation problems when 
> >>> constructing the full matrix in a single pass. I am looking for a more 
> >>> efficient way of doing this. There appears to be efficient procedures in 
> >>> spdep for constructing SQUARE spatial weight matrices (e.g. 
> >>> dnearneigh()). Are there analogous procedures for constructing distance 
> >>> based weights between two different point patterns? I am doing this in 
> >>> preparation for implementing an approximate geographically weighted 
> >>> logistic regression procedure. I was thinking about using re sampling 
> >>> procedure as an inferential frame- perhaps I might get some feedback. 
> >>> This is what I was going to do.
> >>>
> >>> I have a point pattern of 30,000 diabetic people based on where they 
> >>> lived during a 2 year period. During that period, approximately 4% of 
> >>> them developed diabetes. I am interested in isolating the impact of 
> >>> ecological factors on the geographic variation" of the disease, so it is 
> >>> necessary to control for the spatial clustering of individual level risk 
> >>> factors associated with the disease (diabetes).
> >>>
> >>> Step 1: Estimate a logistic regression using the full sample and predict 
> >>> incidence diabetes using individual level covariates (i.e. who developed 
> >>> diabetes over the two year period).
> >>>
> >>> Step 2. Estimate a weighted logit model at each location (grid). The 
> >>> observations would be the people (not the geographic units) and the 
> >>> weights would be kernel weights based on distance. The model would only 
> >>> contain a single freely estimated parameter, the intercept, but it would 
> >>> also contain an offset term. For each patient, the offset term would 
> >>> simply be an evaluation of the linear predictor of the global model 
> >>> estimated above (based on the observed covariate values), but without 
> >>> the intercept. This would effectively fix the estimates of the patient 
> >>> level coefficients to their global values, requiring only a local 
> >>> estimate of the intercept. My hope is that I could interpret geographic 
> >>> variability in the intercept as evidence for a "location effect" net of 
> >>> the patient composition or "risk profile" at a particular location. It 
> >>> would probably make sense to center the X variables so that the 
> >>> intercept was interpretable and estimated in a region of the response 
> >>> plane where their is plenty of data. I would let the other covariates 
> >>> vary as well, but I doubt the model could be estimated in large portions 
> >>> of the study area because of sparse data.
> >>>
> >>> Step 3. If I were going to do inference on the location specific 
> >>> intercepts, I would generate a sampling distribution at each location by 
> >>> re sampling from the global model, and repeat Step 2 for each randomly 
> >>> drawn sample. This would give me a local sampling distribution of 
> >>> intercept estimates at each location and I could compare it to the the 
> >>> single one generated from the observed data. The global model represents 
> >>> a kind of null because the intercept is fixed to its global value and 
> >>> geographic variability is driven entirely by the spatial clustering of 
> >>> patient level factors.
> >>>
> >>>
> >>> thanks!
> >>>
> >>> Sam
> >>>
> >>> _______________________________________________
> >>> R-sig-Geo mailing list
> >>> R-sig-Geo at stat.math.ethz.ch
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>>
> >>>
> >>>   
> >>>       
> >>
> >>     
> >
> >
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From pcfj at daleyfamily.com  Wed May 23 01:58:04 2007
From: pcfj at daleyfamily.com (Polly Mathis)
Date: Wed, 23 May 2007 08:58:04 +0900
Subject: [R-sig-Geo] Legislators announce hearingSeveral members of Congress
	expressed alarm.
Message-ID: <002701c79ccd$09beb3b0$2243e5ee@uohtr>

Uhranleger ***T3R*** Die Hast ist begonnen am Mittwoch 23. Mai

Firma: SkyFlyer
Symbol: T3R.F
Preis: 0.36
3-T Prognose: 0.98
WKN : A0LCMC
ISIN : US83082R1077

Kaufen, kaufen und kaufen! Heutzutage ist es eine schune Muglichkeit
viel Geld zu verdinen! Sehen Sie es am Mittwoch 23. Mai

Food and Drug Administration.
Food and Drug Administration. What would a picnic be without potato
salad? The type of vehicles so many families now use. Beta carotene-rich
foods: carrots, sweet potatoes, kale, cantaloupe, apricots and cherries.
After they receive your money order, they promise to send you your cash
prize. TIPSFor a potato salad that doesn't end up mushy, use waxy
potatoes, which keep their shape better than starchy baking potatoes.
Make sure the end date of the job is specified.
Check a supplier to see how much fixtures, stones, lighting, and
appliances should run so you have some knowledge of a fair price.
Legislators announce hearingSeveral members of Congress expressed alarm.
And for more on Blondie, visit blondie. Guest host Monica Novotny talks
with radio talk show host Maria Milito.
You will be spending a lot of time with your contractor.
Make sure to use ripe cantaloupe and tomato at the height of the summer
for the best results.
For Schneider, the planets must have been in perfect alignment.
and what people say about me is gonna be what I accomplished and what I
did in my life and how my children are.
Kitchen and bathroom remodeling is always a good investment if you
choose wisely.
So see if you can get along with your contractor. Chill, then add lemon
juice to taste and adjust the seasoning. Before bringing houseplant back
indoors in fall, check for insects or signs of disease.
You always want to look for something smooth that can minimize your
chest at least one inch.
If granite's not for you, Cahlin says the hot materials these days are
agglomerates, which are man made quartz materials.
The others were backed over and injured by their own parents.
On the other hand, Simon tends to be looking for a specific sound from a
specific singer, and that changes every week depending on his mood.
You call to claim your winnings.
's Paramount Vantage film.
In the meantime, if you are concerned and want to make sure blind zones
are lessened, you can purchase a vehicle with a camera or sensors or you
could purchase them as an after-market item. Make sure you know who is
responsible for demolition and clean-up. It's quite an event, as pretty
much all of Okazaki turns out to see it. Look to a great all-in-one suit
from Donna Karan Hosiery.



From fieldsh at mail.med.upenn.edu  Wed May 23 17:24:51 2007
From: fieldsh at mail.med.upenn.edu (Sam Field)
Date: Wed, 23 May 2007 11:24:51 -0400
Subject: [R-sig-Geo] efficient code/function for rectangular SP weight
 Matrix and gwr
In-Reply-To: <Pine.LNX.4.44.0705222241080.9681-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0705222241080.9681-100000@reclus.nhh.no>
Message-ID: <46545CC3.9040907@mail.med.upenn.edu>

Roger Bivand wrote:
> On Tue, 22 May 2007, Sam Field wrote:
>
>   
>> In the event that others are following this thread,  I have included 
>> some r-code for constructing a rectangular weight matrix from two sets 
>> of coordinates (in feet). The first set of coords (nhood_pts[,1] and 
>> nhood_pts[,2]) come from a regular grid, the second set refer to the 
>> location of patients ( combo2$X and combo2$Y).  I don't provide the data 
>> only the code below.  The original data consists of 1000 grids and 
>> 30000+ patients. It takes 30 minutes on my fairly speedy computer to 
>> complete the first step- extracting pair-wise distances less then a 
>> given bandwidth (D). I store the neighbor ids and counts in separate 
>> lists.   The rest of the code calculates kernel weights based on these 
>> distances. The code is not efficient from any number of standpoints, but 
>> storing these distances in a list rather then a matrix avoided running 
>> into memory allocation problems. Here is the code.
>>     
>
> Would spDistsN1() in sp have helped to speed things up a little? Looping
> over the 1k grid points and passing out the 30k patient points shouldn't
> be very demanding (34 seconds on a oldish box, more if you need Great
> Circle distances). For heavyweight things, quadtrees would be a better
> choice, I have an unpublished package for approximate nearest neighbours.
>
> Roger
>
>   
The short answer is yes.  The whole calculation went from 30 minutes to 
under 30 seconds!

thanks Roger!  R is well documented, but these lists are invaluable!

I am using these weights in a logistic GWR.  The bootstrapping step took 
3 days.  Are there functions that someone has written in R for logistic 
GWR (or any other GLM for that matter)?  I need a function that allows 
for an offset term in the linear predictor.  What I am using now works 
(looping through glm()), but on this problem it takes a very, very long 
time.


Sam

apologies to Roger for sending this directly.


>> # define bandwidth
>> D <- 2*5280
>>
>>
>> #This creates a list of distances < 1 mile
>> w_raw <- vector("list",length(nhood_pts[,1]))
>> for (i in 1:length(nhood_pts[,1])){
>> w_raw[[i]] <- rep(NA,length(combo2$X))}
>>
>> system.time(for (i in 1:length(nhood_pts[,1])){for(j in 1:length(combo2$X)){
>> if(sqrt((nhood_pts[,1][i]-combo2$X[j])^2 + 
>> (nhood_pts[,2][i]-combo2$Y[j])^2) < D) w_raw[[i]][j] <-  
>> sqrt((nhood_pts[,1][i]-combo2$X[j])^2 + 
>> (nhood_pts[,2][i]-combo2$Y[j])^2)}} )
>>
>>
>>
>> w_raw.id <-   vector("list",length(nhood_pts[,1]))
>> for (i in 1:length(nhood_pts[,1])){
>> w_raw.id[[i]] <-    which(!is.na(w_raw[[i]]))}
>>
>> for (i in 1:length(nhood_pts[,1])){
>> w_raw[[i]] <-  w_raw[[i]][w_raw.id[[i]]]}
>>
>>
>> w_raw.count <-   rep(1,length(nhood_pts[,1]))
>> for (i in 1:length(nhood_pts[,1])){
>> w_raw.count[i] <-  length(w_raw[[i]])}
>>
>> w_weights <-  vector("list",length(nhood_pts[,1]))
>> for (i in 1:length(nhood_pts[,1])){
>> w_weights[[i]] <- (1-(w_raw[[i]]/D)^2)^2}
>>
>> w <-  vector("list",length(nhood_pts[,1]))
>> for (i in 1:length(nhood_pts[,1])){
>> w[[i]] <- w_weights[[i]]/sum(w_weights[[i]])}
>>
>>
>>
>>
>>
>> Roger Bivand wrote:
>>     
>>> On Fri, 11 May 2007, St?phane Dray wrote:
>>>
>>>   
>>>       
>>>> Hi Sam,
>>>>
>>>> I think that this question is quite general and could interest other 
>>>> people, including me, with very different aims. I have developed a 
>>>> method to look for the relationships between two data sets that have 
>>>> been sampled on the same area but for different locations. In my 
>>>> example, the two samples are two polygons layers. In this approach, I 
>>>> compute a rectangular weighting matrix where each weight correspond to 
>>>> the area of intersection between polygons of each layer. I have used 
>>>> also the matrix form to store these weights (my data set was very small 
>>>> compared to you). I remember that Roger was also interested by these 
>>>> rectangular weights in another context. Here we  have different  problems:
>>>> - how to compute these kind  of weights
>>>> - how to store them.
>>>>
>>>> For the first point, I think that for each method/application, the 
>>>> solution  is different. We could develop/extend classical tools for 
>>>> square weights (one set of spatial units) to rectangular weights (two 
>>>> sets of spatial units).
>>>> For the second one, It would be probably interesting to define a class 
>>>> of object in spdep. nb objects are lists, and I think that it would be 
>>>> the solution for rectangular neighborhood.
>>>>
>>>> If I consider two sets of spatial units (A and B) where the number of 
>>>> units is equal to na and nb.  We could store the neighbors in a list of 
>>>> length 2. The first element of this list is a list of length na. In this 
>>>> list, the j-th element is a vector of the neighbors of the j-th unit of 
>>>> the layer A. These neighbors are spatial units of the layer B.  The 
>>>> second element of the global list is a list of length nb where each 
>>>> element is a vector of neighbors.
>>>>
>>>> I think that we have to think to a class of object that could be useful 
>>>> for everybody dealing with this kind of rectangular weights. If this 
>>>> class is properly defined (second point), we could then develop tools to 
>>>> construct this kind of neighborhoods (first point). The eventual 
>>>> extension to more than two data sets could also be taken into account in 
>>>> this reflexion.
>>>>
>>>>     
>>>>         
>>> I would welcome input on this. I'm looking at an alternative weights 
>>> representation through classes in the Matrix package, which is evolving 
>>> fast, and which seems to be promising. If the dimnames slot is used to 
>>> hold the region.id values, it might be possible to make progress.
>>>
>>> Best wishes,
>>>
>>> Roger
>>>
>>>   
>>>       
>>>> Cheers,
>>>>
>>>>
>>>> Sam Field wrote:
>>>>     
>>>>         
>>>>> List,
>>>>>
>>>>> I need to create a rectangular spatial weight matrix for a set of n and 
>>>>> m objects. I quickly run in to memory allocation problems when 
>>>>> constructing the full matrix in a single pass. I am looking for a more 
>>>>> efficient way of doing this. There appears to be efficient procedures in 
>>>>> spdep for constructing SQUARE spatial weight matrices (e.g. 
>>>>> dnearneigh()). Are there analogous procedures for constructing distance 
>>>>> based weights between two different point patterns? I am doing this in 
>>>>> preparation for implementing an approximate geographically weighted 
>>>>> logistic regression procedure. I was thinking about using re sampling 
>>>>> procedure as an inferential frame- perhaps I might get some feedback. 
>>>>> This is what I was going to do.
>>>>>
>>>>> I have a point pattern of 30,000 diabetic people based on where they 
>>>>> lived during a 2 year period. During that period, approximately 4% of 
>>>>> them developed diabetes. I am interested in isolating the impact of 
>>>>> ecological factors on the geographic variation" of the disease, so it is 
>>>>> necessary to control for the spatial clustering of individual level risk 
>>>>> factors associated with the disease (diabetes).
>>>>>
>>>>> Step 1: Estimate a logistic regression using the full sample and predict 
>>>>> incidence diabetes using individual level covariates (i.e. who developed 
>>>>> diabetes over the two year period).
>>>>>
>>>>> Step 2. Estimate a weighted logit model at each location (grid). The 
>>>>> observations would be the people (not the geographic units) and the 
>>>>> weights would be kernel weights based on distance. The model would only 
>>>>> contain a single freely estimated parameter, the intercept, but it would 
>>>>> also contain an offset term. For each patient, the offset term would 
>>>>> simply be an evaluation of the linear predictor of the global model 
>>>>> estimated above (based on the observed covariate values), but without 
>>>>> the intercept. This would effectively fix the estimates of the patient 
>>>>> level coefficients to their global values, requiring only a local 
>>>>> estimate of the intercept. My hope is that I could interpret geographic 
>>>>> variability in the intercept as evidence for a "location effect" net of 
>>>>> the patient composition or "risk profile" at a particular location. It 
>>>>> would probably make sense to center the X variables so that the 
>>>>> intercept was interpretable and estimated in a region of the response 
>>>>> plane where their is plenty of data. I would let the other covariates 
>>>>> vary as well, but I doubt the model could be estimated in large portions 
>>>>> of the study area because of sparse data.
>>>>>
>>>>> Step 3. If I were going to do inference on the location specific 
>>>>> intercepts, I would generate a sampling distribution at each location by 
>>>>> re sampling from the global model, and repeat Step 2 for each randomly 
>>>>> drawn sample. This would give me a local sampling distribution of 
>>>>> intercept estimates at each location and I could compare it to the the 
>>>>> single one generated from the observed data. The global model represents 
>>>>> a kind of null because the intercept is fixed to its global value and 
>>>>> geographic variability is driven entirely by the spatial clustering of 
>>>>> patient level factors.
>>>>>
>>>>>
>>>>> thanks!
>>>>>
>>>>> Sam
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at stat.math.ethz.ch
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>
>>>>>
>>>>>   
>>>>>       
>>>>>           
>>>>     
>>>>         
>>>       
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>     
>
>



From Roger.Bivand at nhh.no  Thu May 24 00:03:07 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 24 May 2007 00:03:07 +0200 (CEST)
Subject: [R-sig-Geo] efficient code/function for rectangular SP weight
 Matrix and gwr
In-Reply-To: <46545CC3.9040907@mail.med.upenn.edu>
Message-ID: <Pine.LNX.4.44.0705232354510.10418-100000@reclus.nhh.no>

On Wed, 23 May 2007, Sam Field wrote:

> Roger Bivand wrote:
> > On Tue, 22 May 2007, Sam Field wrote:
> >
> >   
> >> In the event that others are following this thread,  I have included 
> >> some r-code for constructing a rectangular weight matrix from two sets 
> >> of coordinates (in feet). The first set of coords (nhood_pts[,1] and 
> >> nhood_pts[,2]) come from a regular grid, the second set refer to the 
> >> location of patients ( combo2$X and combo2$Y).  I don't provide the data 
> >> only the code below.  The original data consists of 1000 grids and 
> >> 30000+ patients. It takes 30 minutes on my fairly speedy computer to 
> >> complete the first step- extracting pair-wise distances less then a 
> >> given bandwidth (D). I store the neighbor ids and counts in separate 
> >> lists.   The rest of the code calculates kernel weights based on these 
> >> distances. The code is not efficient from any number of standpoints, but 
> >> storing these distances in a list rather then a matrix avoided running 
> >> into memory allocation problems. Here is the code.
> >>     
> >
> > Would spDistsN1() in sp have helped to speed things up a little? Looping
> > over the 1k grid points and passing out the 30k patient points shouldn't
> > be very demanding (34 seconds on a oldish box, more if you need Great
> > Circle distances). For heavyweight things, quadtrees would be a better
> > choice, I have an unpublished package for approximate nearest neighbours.
> >
> > Roger
> >
> >   
> The short answer is yes.  The whole calculation went from 30 minutes to 
> under 30 seconds!
> 
> thanks Roger!  R is well documented, but these lists are invaluable!

Please believe me that developers also search the lists - finding a 
fruitful and complete thread can really help!

> 
> I am using these weights in a logistic GWR.  The bootstrapping step took 
> 3 days.  Are there functions that someone has written in R for logistic 
> GWR (or any other GLM for that matter)?  I need a function that allows 
> for an offset term in the linear predictor.  What I am using now works 
> (looping through glm()), but on this problem it takes a very, very long 
> time.
> 

There is a very preminary ggwr() in the spgwr package, but it doesn't pass 
the offset= argument through. It should, though, pass an + offset(var) 
through inside the formula - could you try that? It uses spDistsN1() 
internally, so should run reasonably. gwr() has a cryptic and experimental 
cl= argument for a snow cluster, but ggwr() hasn't been modified to suit 
computing on a cluster (yet). Would that help (for non-Windows users, I 
believe)?

Best wishes,

Roger

> 
> Sam
> 
> apologies to Roger for sending this directly.
> 

Thanks for remembering!

> 
> >> # define bandwidth
> >> D <- 2*5280
> >>
> >>
> >> #This creates a list of distances < 1 mile
> >> w_raw <- vector("list",length(nhood_pts[,1]))
> >> for (i in 1:length(nhood_pts[,1])){
> >> w_raw[[i]] <- rep(NA,length(combo2$X))}
> >>
> >> system.time(for (i in 1:length(nhood_pts[,1])){for(j in 1:length(combo2$X)){
> >> if(sqrt((nhood_pts[,1][i]-combo2$X[j])^2 + 
> >> (nhood_pts[,2][i]-combo2$Y[j])^2) < D) w_raw[[i]][j] <-  
> >> sqrt((nhood_pts[,1][i]-combo2$X[j])^2 + 
> >> (nhood_pts[,2][i]-combo2$Y[j])^2)}} )
> >>
> >>
> >>
> >> w_raw.id <-   vector("list",length(nhood_pts[,1]))
> >> for (i in 1:length(nhood_pts[,1])){
> >> w_raw.id[[i]] <-    which(!is.na(w_raw[[i]]))}
> >>
> >> for (i in 1:length(nhood_pts[,1])){
> >> w_raw[[i]] <-  w_raw[[i]][w_raw.id[[i]]]}
> >>
> >>
> >> w_raw.count <-   rep(1,length(nhood_pts[,1]))
> >> for (i in 1:length(nhood_pts[,1])){
> >> w_raw.count[i] <-  length(w_raw[[i]])}
> >>
> >> w_weights <-  vector("list",length(nhood_pts[,1]))
> >> for (i in 1:length(nhood_pts[,1])){
> >> w_weights[[i]] <- (1-(w_raw[[i]]/D)^2)^2}
> >>
> >> w <-  vector("list",length(nhood_pts[,1]))
> >> for (i in 1:length(nhood_pts[,1])){
> >> w[[i]] <- w_weights[[i]]/sum(w_weights[[i]])}
> >>
> >>
> >>
> >>
> >>
> >> Roger Bivand wrote:
> >>     
> >>> On Fri, 11 May 2007, St?phane Dray wrote:
> >>>
> >>>   
> >>>       
> >>>> Hi Sam,
> >>>>
> >>>> I think that this question is quite general and could interest other 
> >>>> people, including me, with very different aims. I have developed a 
> >>>> method to look for the relationships between two data sets that have 
> >>>> been sampled on the same area but for different locations. In my 
> >>>> example, the two samples are two polygons layers. In this approach, I 
> >>>> compute a rectangular weighting matrix where each weight correspond to 
> >>>> the area of intersection between polygons of each layer. I have used 
> >>>> also the matrix form to store these weights (my data set was very small 
> >>>> compared to you). I remember that Roger was also interested by these 
> >>>> rectangular weights in another context. Here we  have different  problems:
> >>>> - how to compute these kind  of weights
> >>>> - how to store them.
> >>>>
> >>>> For the first point, I think that for each method/application, the 
> >>>> solution  is different. We could develop/extend classical tools for 
> >>>> square weights (one set of spatial units) to rectangular weights (two 
> >>>> sets of spatial units).
> >>>> For the second one, It would be probably interesting to define a class 
> >>>> of object in spdep. nb objects are lists, and I think that it would be 
> >>>> the solution for rectangular neighborhood.
> >>>>
> >>>> If I consider two sets of spatial units (A and B) where the number of 
> >>>> units is equal to na and nb.  We could store the neighbors in a list of 
> >>>> length 2. The first element of this list is a list of length na. In this 
> >>>> list, the j-th element is a vector of the neighbors of the j-th unit of 
> >>>> the layer A. These neighbors are spatial units of the layer B.  The 
> >>>> second element of the global list is a list of length nb where each 
> >>>> element is a vector of neighbors.
> >>>>
> >>>> I think that we have to think to a class of object that could be useful 
> >>>> for everybody dealing with this kind of rectangular weights. If this 
> >>>> class is properly defined (second point), we could then develop tools to 
> >>>> construct this kind of neighborhoods (first point). The eventual 
> >>>> extension to more than two data sets could also be taken into account in 
> >>>> this reflexion.
> >>>>
> >>>>     
> >>>>         
> >>> I would welcome input on this. I'm looking at an alternative weights 
> >>> representation through classes in the Matrix package, which is evolving 
> >>> fast, and which seems to be promising. If the dimnames slot is used to 
> >>> hold the region.id values, it might be possible to make progress.
> >>>
> >>> Best wishes,
> >>>
> >>> Roger
> >>>
> >>>   
> >>>       
> >>>> Cheers,
> >>>>
> >>>>
> >>>> Sam Field wrote:
> >>>>     
> >>>>         
> >>>>> List,
> >>>>>
> >>>>> I need to create a rectangular spatial weight matrix for a set of n and 
> >>>>> m objects. I quickly run in to memory allocation problems when 
> >>>>> constructing the full matrix in a single pass. I am looking for a more 
> >>>>> efficient way of doing this. There appears to be efficient procedures in 
> >>>>> spdep for constructing SQUARE spatial weight matrices (e.g. 
> >>>>> dnearneigh()). Are there analogous procedures for constructing distance 
> >>>>> based weights between two different point patterns? I am doing this in 
> >>>>> preparation for implementing an approximate geographically weighted 
> >>>>> logistic regression procedure. I was thinking about using re sampling 
> >>>>> procedure as an inferential frame- perhaps I might get some feedback. 
> >>>>> This is what I was going to do.
> >>>>>
> >>>>> I have a point pattern of 30,000 diabetic people based on where they 
> >>>>> lived during a 2 year period. During that period, approximately 4% of 
> >>>>> them developed diabetes. I am interested in isolating the impact of 
> >>>>> ecological factors on the geographic variation" of the disease, so it is 
> >>>>> necessary to control for the spatial clustering of individual level risk 
> >>>>> factors associated with the disease (diabetes).
> >>>>>
> >>>>> Step 1: Estimate a logistic regression using the full sample and predict 
> >>>>> incidence diabetes using individual level covariates (i.e. who developed 
> >>>>> diabetes over the two year period).
> >>>>>
> >>>>> Step 2. Estimate a weighted logit model at each location (grid). The 
> >>>>> observations would be the people (not the geographic units) and the 
> >>>>> weights would be kernel weights based on distance. The model would only 
> >>>>> contain a single freely estimated parameter, the intercept, but it would 
> >>>>> also contain an offset term. For each patient, the offset term would 
> >>>>> simply be an evaluation of the linear predictor of the global model 
> >>>>> estimated above (based on the observed covariate values), but without 
> >>>>> the intercept. This would effectively fix the estimates of the patient 
> >>>>> level coefficients to their global values, requiring only a local 
> >>>>> estimate of the intercept. My hope is that I could interpret geographic 
> >>>>> variability in the intercept as evidence for a "location effect" net of 
> >>>>> the patient composition or "risk profile" at a particular location. It 
> >>>>> would probably make sense to center the X variables so that the 
> >>>>> intercept was interpretable and estimated in a region of the response 
> >>>>> plane where their is plenty of data. I would let the other covariates 
> >>>>> vary as well, but I doubt the model could be estimated in large portions 
> >>>>> of the study area because of sparse data.
> >>>>>
> >>>>> Step 3. If I were going to do inference on the location specific 
> >>>>> intercepts, I would generate a sampling distribution at each location by 
> >>>>> re sampling from the global model, and repeat Step 2 for each randomly 
> >>>>> drawn sample. This would give me a local sampling distribution of 
> >>>>> intercept estimates at each location and I could compare it to the the 
> >>>>> single one generated from the observed data. The global model represents 
> >>>>> a kind of null because the intercept is fixed to its global value and 
> >>>>> geographic variability is driven entirely by the spatial clustering of 
> >>>>> patient level factors.
> >>>>>
> >>>>>
> >>>>> thanks!
> >>>>>
> >>>>> Sam
> >>>>>
> >>>>> _______________________________________________
> >>>>> R-sig-Geo mailing list
> >>>>> R-sig-Geo at stat.math.ethz.ch
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>>>>
> >>>>>
> >>>>>   
> >>>>>       
> >>>>>           
> >>>>     
> >>>>         
> >>>       
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at stat.math.ethz.ch
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >>     
> >
> >
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From loecher at eden.rutgers.edu  Thu May 24 19:51:14 2007
From: loecher at eden.rutgers.edu (Markus Loecher)
Date: Thu, 24 May 2007 13:51:14 -0400
Subject: [R-sig-Geo] modifying shpfiles in maptools
Message-ID: <20070524175011.A6F3832407A@annwn13.rutgers.edu>

Hello,
I am trying to manipulate a shpfile (actually subsetting it) but am 
not getting anywhere. I would like to plot the census blocks of San 
Francisco county (ID = 075) only but the shpfile I have is for all of CA.
So I am executing the following lines of code, where shpfile.CA is 
the big one for CA:

shpfile.CA <- read.shape("bg06_d00.shp", dbf.data=TRUE, verbose=TRUE);
#keep only those entries for SF county:
shpfile.CA$att.data[,"COUNTY"] <- as.character(shpfile.CA$att.data[,"COUNTY"]);
i.keep <- shpfile.CA$att.data[,"COUNTY"] == "075";
shpfile$att.data <- shpfile.CA$att.data[i.keep,];
shpfile$Shapes <- shpfile.CA$Shapes[i.keep];

To me this seems like a correct subsetting of a list, but when I try 
to plot the new, smaller shpfile, I get the error message:
	Error in 1:attr(theMap$Shapes, "nshps") : NA/NaN argument
What foolish mistake am I committing ?

Also, it seems that plot.Map is deprecated. I wished I had an example 
of how to use plot.Spatial instead, with a shpfile...

Thanks!

markus



From Roger.Bivand at nhh.no  Thu May 24 23:43:48 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 24 May 2007 23:43:48 +0200 (CEST)
Subject: [R-sig-Geo] modifying shpfiles in maptools
In-Reply-To: <20070524175011.A6F3832407A@annwn13.rutgers.edu>
Message-ID: <Pine.LNX.4.44.0705242337500.14213-100000@reclus.nhh.no>

On Thu, 24 May 2007, Markus Loecher wrote:

> Hello,
> I am trying to manipulate a shpfile (actually subsetting it) but am 
> not getting anywhere. I would like to plot the census blocks of San 
> Francisco county (ID = 075) only but the shpfile I have is for all of CA.
> So I am executing the following lines of code, where shpfile.CA is 
> the big one for CA:
> 
> shpfile.CA <- read.shape("bg06_d00.shp", dbf.data=TRUE, verbose=TRUE);
> #keep only those entries for SF county:
> shpfile.CA$att.data[,"COUNTY"] <- as.character(shpfile.CA$att.data[,"COUNTY"]);
> i.keep <- shpfile.CA$att.data[,"COUNTY"] == "075";
> shpfile$att.data <- shpfile.CA$att.data[i.keep,];
> shpfile$Shapes <- shpfile.CA$Shapes[i.keep];
> 
> To me this seems like a correct subsetting of a list, but when I try 
> to plot the new, smaller shpfile, I get the error message:
> 	Error in 1:attr(theMap$Shapes, "nshps") : NA/NaN argument
> What foolish mistake am I committing ?
> 
> Also, it seems that plot.Map is deprecated. I wished I had an example 
> of how to use plot.Spatial instead, with a shpfile...

Yes, the Map object structure is too close to the input shapefile to be 
easy to manipulate. Try rather:

shpfile.CA <- readShapePoly("bg06_d00.shp")
shpfile.CA$COUNTY <- as.character(shpfile.CA$COUNTY)
shpfile.SF <- shpfile.CA[shpfile.CA$COUNTY == "075",]

should work if shpfile.CA$COUNTY is correct - I'd do a 
table(shpfile.CA$COUNTY) first to check that the values came out right.

The examples are in maptools (and rgdal) for reading and writing, and in 
sp for manipulating and plotting - for easy plots see spplot().

Roger

> 
> Thanks!
> 
> markus
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From marco.helbich at gmx.at  Sat May 26 14:19:46 2007
From: marco.helbich at gmx.at (Marco Helbich)
Date: Sat, 26 May 2007 14:19:46 +0200
Subject: [R-sig-Geo] splancs and the use of a shapefile with a hole
Message-ID: <000801c79f90$26487be0$0200a8c0@mirk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070526/6f57b1cf/attachment.pl>

From Roger.Bivand at nhh.no  Sat May 26 20:25:01 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 26 May 2007 20:25:01 +0200 (CEST)
Subject: [R-sig-Geo] splancs and the use of a shapefile with a hole
In-Reply-To: <000801c79f90$26487be0$0200a8c0@mirk>
Message-ID: <Pine.LNX.4.44.0705262020380.15733-100000@reclus.nhh.no>

On Sat, 26 May 2007, Marco Helbich wrote:

> Dear list,
> 
> my problem concerns the library splancs and the use of a shapefile with
> a hole. I found a part of code in another thread
> (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/61525.html) saying that
> if I've "more than one shape, and/or more than one ring in that shape,
> adjust the [[Shape]])[[ring]]" in the
> "getPolygonCoordsSlot(getPolygonsPolygonsSlot(getSpPpolygonsSlot(pol)[[2]])[[1]])".
> My problem is that I can choose with the two parameters between the hole
> studyarea (inkluding the hole) or the hole itself but not among the
> studyarea excluding the hole. Below you can find the code.

The polygon used by splancs can only consist of a single ring. The typical 
fix is to join the external ring to the internal ring by two lines, 
creating an link between inner and outer, so that there no longer is a 
hole. This can be done with care and coffee, and the example you found 
from some time ago is not too easy to follow. Are you able to make an 
Rdata file of your polygon-and-hole object available?

Best wishes,

Roger

> 
> I appreciate every hint! Thanks
> 
> Marco
> 
> 
> > library(splancs)
> > library(maptools)
> > setwd("c:/test")
> > getwd()
> [1] "c:/test"
> > 
> > pol <- readShapePoly("stadtreg_owin.shp", verbose=T)
> Shapefile type: Polygon, (5), # of Shapes: 1
> > edv <- read.table("C:/test/edv.txt", header=T,
> +   sep=";", na.strings="NA", dec=",", strip.white=T)
> > poi <- as.points(edv[,-c(1:25)])
> > ls()
> [1] "edv"          "poi"          "pol"          "splancs_poly"
> > 
> > poi <- coordinates(poi)
> > splancs_poly <- getPolygonCoordsSlot(getPolygonsPolygonsSlot(
> +   getSpPpolygonsSlot(pol)[[1]])[[2]])
> > polymap(splancs_poly)
> 
> __________________________
> Marco Helbich
> ISR
> A-1010 Vienna, Postgasse 7/4/2
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From yair.levy at ulb.ac.be  Mon May 28 16:55:07 2007
From: yair.levy at ulb.ac.be (Yair Levy)
Date: Mon, 28 May 2007 16:55:07 +0200 (CEST)
Subject: [R-sig-Geo] Getting started with RSIG
Message-ID: <20070528145507.8E412630@bonito.ulb.ac.be>

Dear all,


I'm a rather new user of RSIG and would strongly beneficiate from any form of introduction to it. Some RSIG related packages list or some guidance text such as a user instructions manuel, start up guide or personal synthesized notes of a set of key practices to start with could be of great use.


Best regards,


Y L



    
Levy Ya?r
Tel: 0476/75.47.25
yairlbe at yahoo.com
Rue Gretry 1
2018 Anvers



From Jan.Verbesselt at csiro.au  Wed May 30 08:57:37 2007
From: Jan.Verbesselt at csiro.au (Jan.Verbesselt at csiro.au)
Date: Wed, 30 May 2007 16:57:37 +1000
Subject: [R-sig-Geo] Plotting satellite image in R via sp package -> grid
Message-ID: <0393960FC6AFA142B0234E5CC8DA3B0C3FE283@exnswn2-syd.nexus.csiro.au>

Dear,

I would like to plot an grid (subset of a satellite image in Sinusoidal
projection) as an spatialobject in R via the sp package (R version 2.5,
winXP).
 
The following works fine:
sinusgrid <- read.table( see for data sample below , header=TRUE,
sep=",", 	na.strings="NA", dec=".", strip.white=TRUE)
sinusgrid <- as.data.frame(sinusgrid)
coordinates(sinusgrid)=~x+y
proj4string(sinusgrid) = CRS("+proj=sinu +lon_0=0 +x_0=0 +y_0=0
+a=6371000 +b=6371000 +units=m")  # Sinusoidal projection
	
l2 = list("SpatialPolygonsRescale", layout.north.arrow(), offset =
c(13416000,-3939000), scale = 500)
l3 = list("SpatialPolygonsRescale", layout.scale.bar(), offset =
c(13416000,-3940000), scale = 1000, fill=c("transparent","black"))
        spplot(sinusgrid, "ndvi", do.log = TRUE,
key.space=list(x=0.2,y=0.8,corner=c(0,1)), scales=list(draw=T), cuts =
4,
     sp.layout=list(l2,l3) )

But when I try ;
gridded(sinusgrid) <- TRUE

I obtain the following error message:
suggested tolerance minimum: 4.31585196603024e-06Error in
points2grid(points, tolerance) : dimension 1 : coordinate intervals are
not constant

before I can use image(sinusgrid) in order to create a spatial map to
which I could overlay vectorlayers and points.

The data is extracted from a satellite image which was projected in
sinusoidal projection (ProjParams =
(6371007.181000,0,0,0,0,0,0,0,21600,0,1,0,0).

How could this satellite data be projected in R such that other shape
files and points can be overlaid on the map?

Regards and thx,
Jan


### Data set: pixels of an satellite image.
x,y,index
13415234.67,-3935966.551,0.83
13415234.67,-3936198.255,0.83
13415234.67,-3936429.96,0.82
13415234.67,-3936661.665,0.87
13415234.67,-3936893.369,0.87
13415234.67,-3937125.074,0.87
13415234.67,-3937356.779,0.88
13415234.67,-3937588.483,0.86
13415234.67,-3937820.188,0.88
13415234.67,-3938051.893,0.77
13415234.67,-3938283.597,0.77
13415234.67,-3938515.302,0.92
13415234.67,-3938747.006,0.89
13415234.67,-3938978.711,0.89
13415234.67,-3939210.416,0.81
13415234.67,-3939442.12,0.76
13415234.67,-3939673.825,0.76
13415234.67,-3939905.53,0.85
13415234.67,-3940137.234,0.85
13415466.38,-3935966.551,0.83
13415466.38,-3936198.255,0.82
13415466.38,-3936429.96,0.83
13415466.38,-3936661.665,0.83
13415466.38,-3936893.369,0.87
13415466.38,-3937125.074,0.89
13415466.38,-3937356.779,0.80
13415466.38,-3937588.483,0.86
13415466.38,-3937820.188,0.88
13415466.38,-3938051.893,0.87


______________________________________________
Jan Verbesselt, Phd
Post-Doctoral Research Fellow - Remote sensing
Ensis - the joint forces of CSIRO and Scion
Private Bag 10
Clayton South, VIC 3169, Australia
Tel: + 61 3 9545 2265 
Fax: + 61 3 9545 2448



From Roger.Bivand at nhh.no  Wed May 30 14:31:04 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 30 May 2007 14:31:04 +0200 (CEST)
Subject: [R-sig-Geo] Plotting satellite image in R via sp package -> grid
In-Reply-To: <0393960FC6AFA142B0234E5CC8DA3B0C3FE283@exnswn2-syd.nexus.csiro.au>
Message-ID: <Pine.LNX.4.44.0705301426390.22911-100000@reclus.nhh.no>

On Wed, 30 May 2007 Jan.Verbesselt at csiro.au wrote:

> Dear,
> 
> I would like to plot an grid (subset of a satellite image in Sinusoidal
> projection) as an spatialobject in R via the sp package (R version 2.5,
> winXP).
>  
> The following works fine:
> sinusgrid <- read.table( see for data sample below , header=TRUE,
> sep=",", 	na.strings="NA", dec=".", strip.white=TRUE)
> sinusgrid <- as.data.frame(sinusgrid)
> coordinates(sinusgrid)=~x+y
> proj4string(sinusgrid) = CRS("+proj=sinu +lon_0=0 +x_0=0 +y_0=0
> +a=6371000 +b=6371000 +units=m")  # Sinusoidal projection
> 	
> l2 = list("SpatialPolygonsRescale", layout.north.arrow(), offset =
> c(13416000,-3939000), scale = 500)
> l3 = list("SpatialPolygonsRescale", layout.scale.bar(), offset =
> c(13416000,-3940000), scale = 1000, fill=c("transparent","black"))
>         spplot(sinusgrid, "ndvi", do.log = TRUE,
> key.space=list(x=0.2,y=0.8,corner=c(0,1)), scales=list(draw=T), cuts =
> 4,
>      sp.layout=list(l2,l3) )
> 
> But when I try ;
> gridded(sinusgrid) <- TRUE
> 
> I obtain the following error message:
> suggested tolerance minimum: 4.31585196603024e-06Error in
> points2grid(points, tolerance) : dimension 1 : coordinate intervals are
> not constant
> 
> before I can use image(sinusgrid) in order to create a spatial map to
> which I could overlay vectorlayers and points.
> 
> The data is extracted from a satellite image which was projected in
> sinusoidal projection (ProjParams =
> (6371007.181000,0,0,0,0,0,0,0,21600,0,1,0,0).
> 
> How could this satellite data be projected in R such that other shape
> files and points can be overlaid on the map?

As a guess, the ASCII coordinates are losing digits somewhere, so I'd take 
cbind() of the data frame coordinate columns and use SpatialPixels() 
directly, setting the tolerance= argument to a value that works for you. 
Then add back the index column as a data frame in the data= argument to 
SpatialPixelsDataFrame(). The gridded<- method doesn't let you set the 
tolerance, so go round by the side door.

Hope this helps,

Roger

> 
> Regards and thx,
> Jan
> 
> 
> ### Data set: pixels of an satellite image.
> x,y,index
> 13415234.67,-3935966.551,0.83
> 13415234.67,-3936198.255,0.83
> 13415234.67,-3936429.96,0.82
> 13415234.67,-3936661.665,0.87
> 13415234.67,-3936893.369,0.87
> 13415234.67,-3937125.074,0.87
> 13415234.67,-3937356.779,0.88
> 13415234.67,-3937588.483,0.86
> 13415234.67,-3937820.188,0.88
> 13415234.67,-3938051.893,0.77
> 13415234.67,-3938283.597,0.77
> 13415234.67,-3938515.302,0.92
> 13415234.67,-3938747.006,0.89
> 13415234.67,-3938978.711,0.89
> 13415234.67,-3939210.416,0.81
> 13415234.67,-3939442.12,0.76
> 13415234.67,-3939673.825,0.76
> 13415234.67,-3939905.53,0.85
> 13415234.67,-3940137.234,0.85
> 13415466.38,-3935966.551,0.83
> 13415466.38,-3936198.255,0.82
> 13415466.38,-3936429.96,0.83
> 13415466.38,-3936661.665,0.83
> 13415466.38,-3936893.369,0.87
> 13415466.38,-3937125.074,0.89
> 13415466.38,-3937356.779,0.80
> 13415466.38,-3937588.483,0.86
> 13415466.38,-3937820.188,0.88
> 13415466.38,-3938051.893,0.87
> 
> 
> ______________________________________________
> Jan Verbesselt, Phd
> Post-Doctoral Research Fellow - Remote sensing
> Ensis - the joint forces of CSIRO and Scion
> Private Bag 10
> Clayton South, VIC 3169, Australia
> Tel: + 61 3 9545 2265 
> Fax: + 61 3 9545 2448
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Laurence.Amilhat at toulouse.inra.fr  Wed May 30 14:37:54 2007
From: Laurence.Amilhat at toulouse.inra.fr (Laurence Amilhat)
Date: Wed, 30 May 2007 14:37:54 +0200
Subject: [R-sig-Geo] time diagram
Message-ID: <465D7022.50106@toulouse.inra.fr>

Dear R users,

I had several projects with different starting and ending dates.
I would like to plot the duration of the different project along a time 
axis,
as it is shown in the following scheme:
diagram

Do you know if there is a nice R function to do such diagram.
I'am sorry if this is a redundant question, I tried to find some 
informations over the internet but as I don't know the name for such 
diagram, informations are hard to find.

Thank you for your help,


Regards,



Laurence Amilhat.



From rob.robinson at bto.org  Wed May 30 14:43:37 2007
From: rob.robinson at bto.org (Rob Robinson)
Date: Wed, 30 May 2007 13:43:37 +0100
Subject: [R-sig-Geo] time diagram
In-Reply-To: <465D7022.50106@toulouse.inra.fr>
Message-ID: <004501c7a2b8$249894d0$5285c3c1@btodomain.bto.org>

Laurence,
 I think what you are after are known as Gantt charts - chucking this into a
search engine seems to reveal plenty of examples, eg
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=74
http://tolstoy.newcastle.edu.au/R/help/05/01/10602.html
Hope that helps
Best wishes
rob

*** Want to know about Britain's birds? Try  www.bto.org/birdfacts ***

Dr Rob Robinson, Senior Population Biologist
British Trust for Ornithology, The Nunnery, Thetford, Norfolk, IP24 2PU
Ph: +44 (0)1842 750050         E: rob.robinson at bto.org
Fx: +44 (0)1842 750030         W: http://www.bto.org
eSafe scanned this email for viruses, vandals and malicious content (!)

==== "How can anyone be enlightened, when truth is so poorly lit" =====
  

> -----Original Message-----
> From: r-sig-geo-bounces at stat.math.ethz.ch 
> [mailto:r-sig-geo-bounces at stat.math.ethz.ch] On Behalf Of 
> Laurence Amilhat
> Sent: 30 May 2007 13:38
> To: r-sig-geo at stat.math.ethz.ch
> Subject: [R-sig-Geo] time diagram
> 
> Dear R users,
> 
> I had several projects with different starting and ending dates.
> I would like to plot the duration of the different project 
> along a time axis, as it is shown in the following scheme:
> diagram
> 
> Do you know if there is a nice R function to do such diagram.
> I'am sorry if this is a redundant question, I tried to find 
> some informations over the internet but as I don't know the 
> name for such diagram, informations are hard to find.
> 
> Thank you for your help,
> 
> 
> Regards,
> 
> 
> 
> Laurence Amilhat.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From Laurence.Amilhat at toulouse.inra.fr  Wed May 30 14:53:10 2007
From: Laurence.Amilhat at toulouse.inra.fr (Laurence Amilhat)
Date: Wed, 30 May 2007 14:53:10 +0200
Subject: [R-sig-Geo] time diagram
In-Reply-To: <004501c7a2b8$249894d0$5285c3c1@btodomain.bto.org>
References: <004501c7a2b8$249894d0$5285c3c1@btodomain.bto.org>
Message-ID: <465D73B6.8040801@toulouse.inra.fr>

Thank you,
This is exactly what I was looking for,
I will look at the example code to implement it
Thanks again,

laurence


Rob Robinson a ?crit :
> Laurence,
>  I think what you are after are known as Gantt charts - chucking this into a
> search engine seems to reveal plenty of examples, eg
> http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=74
> http://tolstoy.newcastle.edu.au/R/help/05/01/10602.html
> Hope that helps
> Best wishes
> rob
>
> *** Want to know about Britain's birds? Try  www.bto.org/birdfacts ***
>
> Dr Rob Robinson, Senior Population Biologist
> British Trust for Ornithology, The Nunnery, Thetford, Norfolk, IP24 2PU
> Ph: +44 (0)1842 750050         E: rob.robinson at bto.org
> Fx: +44 (0)1842 750030         W: http://www.bto.org
> eSafe scanned this email for viruses, vandals and malicious content (!)
>
> ==== "How can anyone be enlightened, when truth is so poorly lit" =====
>   
>
>   
>> -----Original Message-----
>> From: r-sig-geo-bounces at stat.math.ethz.ch 
>> [mailto:r-sig-geo-bounces at stat.math.ethz.ch] On Behalf Of 
>> Laurence Amilhat
>> Sent: 30 May 2007 13:38
>> To: r-sig-geo at stat.math.ethz.ch
>> Subject: [R-sig-Geo] time diagram
>>
>> Dear R users,
>>
>> I had several projects with different starting and ending dates.
>> I would like to plot the duration of the different project 
>> along a time axis, as it is shown in the following scheme:
>> diagram
>>
>> Do you know if there is a nice R function to do such diagram.
>> I'am sorry if this is a redundant question, I tried to find 
>> some informations over the internet but as I don't know the 
>> name for such diagram, informations are hard to find.
>>
>> Thank you for your help,
>>
>>
>> Regards,
>>
>>
>>
>> Laurence Amilhat.
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at stat.math.ethz.ch
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>     
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From mgleahy at alumni.uwaterloo.ca  Wed May 30 21:11:00 2007
From: mgleahy at alumni.uwaterloo.ca (Mike Leahy)
Date: Wed, 30 May 2007 16:11:00 -0300
Subject: [R-sig-Geo] Connecting to PostgreSQL/PostGIS from R (rgdal?)
Message-ID: <BAY102-DAV170E36744719728702285BBC2E0@phx.gbl>

Hello,

I've been trying every now and then to find a cross operating system
solution that would let me access PostgreSQL (and PostGIS) from R, or to
access R from PostgreSQL.  I know of pl/r, which accomplishes the
latter, but has yet to be successfully ported to Windows.  Similarly,
I've tried to use Rdbi and DBI, but I haven't had luck with those on
Windows either for connecting to PostgreSQL from R.  Can anyone suggest
a solution for this?

It would seem that rgdal could also help me in this case. Unfortunately,
the version of the GDAL library that is included in the rdgal binary
available on CRAN (for windows) doesn't include the PostgreSQL driver
for OGR (i.e., it's not listed by the ogrDrivers() function).

I compiled rgdal on Windows myself using the GDAL library from
FWTools-1.3.1, but I was unsuccessful at creating a proper binary
package for R.  I was only able to get it to work by substituting the
rgdal.dll that was installed by CRAN with the one that I compiled that
links against the GDAL library from FWTools.  Even though it works (at
first glance with ogrInfo(), and readOGR()), I still get a warning
message when I load the libary: "DLL attempted to change FPU control
word from 8001f to 9001f".

So my question with respect to rgdal is a) is it likely that an rgdal
package is going to be released in the future with the PostgreSQL driver
included in GDAL/OGR, or b) are there any suggestions/instructions that
might get me through the compilation and packaging process for rgdal
with better success?

Thanks in advance for any help,
Mike



From Roger.Bivand at nhh.no  Wed May 30 23:00:12 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 30 May 2007 23:00:12 +0200 (CEST)
Subject: [R-sig-Geo] Connecting to PostgreSQL/PostGIS from R (rgdal?)
In-Reply-To: <BAY102-DAV170E36744719728702285BBC2E0@phx.gbl>
	<465DCC44.2070502@alumni.uwaterloo.ca>
Message-ID: <Pine.LNX.4.44.0705302252410.22911-100000@reclus.nhh.no>

On Wed, 30 May 2007, Mike Leahy wrote:

> Hello,
> 
> I've been trying every now and then to find a cross operating system
> solution that would let me access PostgreSQL (and PostGIS) from R, or to
> access R from PostgreSQL.  I know of pl/r, which accomplishes the
> latter, but has yet to be successfully ported to Windows.  Similarly,
> I've tried to use Rdbi and DBI, but I haven't had luck with those on
> Windows either for connecting to PostgreSQL from R.  Can anyone suggest
> a solution for this?
> 
> It would seem that rgdal could also help me in this case. Unfortunately,
> the version of the GDAL library that is included in the rdgal binary
> available on CRAN (for windows) doesn't include the PostgreSQL driver
> for OGR (i.e., it's not listed by the ogrDrivers() function).
> 
> I compiled rgdal on Windows myself using the GDAL library from
> FWTools-1.3.1, but I was unsuccessful at creating a proper binary
> package for R.  I was only able to get it to work by substituting the
> rgdal.dll that was installed by CRAN with the one that I compiled that
> links against the GDAL library from FWTools.  Even though it works (at
> first glance with ogrInfo(), and readOGR()), I still get a warning
> message when I load the libary: "DLL attempted to change FPU control
> word from 8001f to 9001f".
> 
> So my question with respect to rgdal is a) is it likely that an rgdal
> package is going to be released in the future with the PostgreSQL driver
> included in GDAL/OGR, or b) are there any suggestions/instructions that
> might get me through the compilation and packaging process for rgdal
> with better success?

The warning is harmless - R is just reporting that it has stopped the 
dynamically linked libraries resetting a flag that they should not change 
while R is running. If you followed the notes in README.windows in rgdal, 
you ought to be OK. There are no plans to provide more Windows binary 
drivers than those present now, because the others involve further 
external dependencies, which most users would not welcome. 

So please try to do an ogrinfo at the command line in Windows using
FWTools to your PostGIS data, and then the equivalent within R with your
locally built rgdal, and see how it goes. Even on Linux, getting all the
components lined up isn't easy, according to people who have tried, but
can be done if you need to do it.

Hope this helps,

Roger

> 
> Thanks in advance for any help,
> Mike
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From tkeitt at gmail.com  Wed May 30 23:31:27 2007
From: tkeitt at gmail.com (Tim Keitt)
Date: Wed, 30 May 2007 16:31:27 -0500
Subject: [R-sig-Geo] Connecting to PostgreSQL/PostGIS from R (rgdal?)
In-Reply-To: <BAY102-DAV170E36744719728702285BBC2E0@phx.gbl>
References: <BAY102-DAV170E36744719728702285BBC2E0@phx.gbl>
Message-ID: <6262c54c0705301431pb61a390h9b755586e4d1bca8@mail.gmail.com>

I would also recommend you take a look at RODBC for general purpose
database access. My impression is that it has received a lot more
maintenance attention lately.

I can't comment on the windows gdal binaries as I'm not too familiar
with that platform.

THK

On 5/30/07, Mike Leahy <mgleahy at alumni.uwaterloo.ca> wrote:
> Hello,
>
> I've been trying every now and then to find a cross operating system
> solution that would let me access PostgreSQL (and PostGIS) from R, or to
> access R from PostgreSQL.  I know of pl/r, which accomplishes the
> latter, but has yet to be successfully ported to Windows.  Similarly,
> I've tried to use Rdbi and DBI, but I haven't had luck with those on
> Windows either for connecting to PostgreSQL from R.  Can anyone suggest
> a solution for this?
>
> It would seem that rgdal could also help me in this case. Unfortunately,
> the version of the GDAL library that is included in the rdgal binary
> available on CRAN (for windows) doesn't include the PostgreSQL driver
> for OGR (i.e., it's not listed by the ogrDrivers() function).
>
> I compiled rgdal on Windows myself using the GDAL library from
> FWTools-1.3.1, but I was unsuccessful at creating a proper binary
> package for R.  I was only able to get it to work by substituting the
> rgdal.dll that was installed by CRAN with the one that I compiled that
> links against the GDAL library from FWTools.  Even though it works (at
> first glance with ogrInfo(), and readOGR()), I still get a warning
> message when I load the libary: "DLL attempted to change FPU control
> word from 8001f to 9001f".
>
> So my question with respect to rgdal is a) is it likely that an rgdal
> package is going to be released in the future with the PostgreSQL driver
> included in GDAL/OGR, or b) are there any suggestions/instructions that
> might get me through the compilation and packaging process for rgdal
> with better success?
>
> Thanks in advance for any help,
> Mike
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


-- 
Timothy H. Keitt, University of Texas at Austin
Contact info and schedule at http://www.keittlab.org/tkeitt/
Reprints at http://www.keittlab.org/tkeitt/papers/
ODF attachment? See http://www.openoffice.org/



From Jan.Verbesselt at csiro.au  Thu May 31 10:28:47 2007
From: Jan.Verbesselt at csiro.au (Jan.Verbesselt at csiro.au)
Date: Thu, 31 May 2007 18:28:47 +1000
Subject: [R-sig-Geo] Plotting satellite image in R via sp package ->
	grid AND overlay shape file
References: <Pine.LNX.4.44.0705301426390.22911-100000@reclus.nhh.no>
Message-ID: <0393960FC6AFA142B0234E5CC8DA3B0C3FE28E@exnswn2-syd.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20070531/23e09719/attachment.pl>

From Roger.Bivand at nhh.no  Thu May 31 21:25:20 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 31 May 2007 21:25:20 +0200 (CEST)
Subject: [R-sig-Geo] Plotting satellite image in R via sp package ->
 grid AND overlay shape file
In-Reply-To: <0393960FC6AFA142B0234E5CC8DA3B0C3FE28E@exnswn2-syd.nexus.csiro.au>
Message-ID: <Pine.LNX.4.44.0705312119230.23714-100000@reclus.nhh.no>

On Thu, 31 May 2007 Jan.Verbesselt at csiro.au wrote:

> Hi Roger,
> 
>  
> 
> Thanks for the help! It works now via the code below. When you now ask
> for the variable GRID, R says that there is no projection system. How
> could a projection be added?  Thx. I would like to add points,  and
> shape files on the georeferenced - map (with scale bar - north arrow?).
> 
>  
> 
> *This is the message:
> 
>  
> 
> >GRID
> 
> ****
> 
> [549,]  13421722  -3939674
> 
> [550,]  13421722  -3939906
> 
> [551,]  13421722  -3940137
> 
> Coordinate Reference System (CRS) arguments: NA
> 
> ***

Thanks, these are longstanding bugs in sp - the projection strings should 
be passed through and are not being. Until we release a new version, 
please do:

proj4string(GRID) <- CRS(
"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371000 +b=6371000 +units=m")

to add it after creation.

> 
>  
> 
> S <- SpatialPoints(cbind( sinusgrid[,1], sinusgrid[,2]), proj4string =
> CRS("+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371000 +b=6371000 +units=m"))
> 
> SP <- SpatialPixels(S, tolerance =4.3159264735194e-05, proj4string =
> CRS("+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371000 +b=6371000 +units=m"))
> 
> GRID <- SpatialPixelsDataFrame(points = SP, tolerance
> =4.3159264735194e-05, data = sinusgrid["index"], proj4string =
> CRS("+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371000 +b=6371000 +units=m"))
> 
> ***
> 
> With this message -->>
> 
> Warning messages:
> 
> 1: grid has empty column/rows in dimension 1 in: points2grid(points,
> tolerance) 
> 
> 2: grid has empty column/rows in dimension 2 in: points2grid(points,
> tolerance)
> 
> ***
> 
>  
> 
> xyz <- as.image.SpatialGridDataFrame(GRID)
> 
> image(GRID, axes=TRUE) # good plot, with coordinates in the axes but
> without a legend! ;/ 
> 
> contour(xyz, add=TRUE)
> 
>  
> 
> In order to add a part of the shape file on top of the map I tried the
> following: (although the grid is only a small part of the shape file!).
> It would be handy to see where in the shape file the grid is positioned.
> 
>  
> 
> library(maptools)
> 
> # --> overlay shape file on top of map (extend of shape file are larger
> than grid)
> 
> pdir <- "C:\\DATA\\shapefile\\Greenhills\\";
> 
> shape<- readShapePoly(paste(pdir,"greenhills_SINUS.shp",sep=""),
> proj4string = CRS("+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371000
> +b=6371000 +units=m")) 
> 
> plot(shape) # plot looks fine)
> 
>  
> 
> l3 = list("SpatialPolygonsRescale", layout.scale.bar(), offset
> =c(data[sy,1],data[sy,2]), scale = 500, fill=c("transparent","black")) #
> add scale bar
> 
> rv = list("sp.polygons",shape) # add shape file
> 
> spplot(GRID, c("ndvi"), sp.layout=list(l3,rv))
> 
>  
> 
> * Could the shape file first be plotted and than the grid on top of it? 

Perhaps, but getting the layers as wanted in spplot is not easy.

> 
> * Can a projection be added to the grid - see GRID? 

Do you mean a grid representing some coordinate reference system plotted 
on top of the graphic? In principle yes, but more details would be useful.

> 
> (which other function can be used to fine-tune the creation of spatial
> maps from a combination of a grid and shape file with legend &
> scalebar?)
> 

Knowledge of the underlying graphics engines is necessary, because the 
details are at that level.

Hope this helps,

Roger

>  
> 
> Thanks a lot for your help,
> 
> Jan
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From yud at mail.montclair.edu  Thu May 31 22:20:13 2007
From: yud at mail.montclair.edu (Danlin Yu)
Date: Thu, 31 May 2007 16:20:13 -0400
Subject: [R-sig-Geo] All zero link when reading a modified shapefile into R
Message-ID: <8a0b8c8a64ba.8a64ba8a0b8c@montclair.edu>

Hi all-

When I read a shapefile using read.shape into R, and used Map2poly -> poly2nb to create my neighbor list, the summary of the newly created neighbor list indicates that all the regions have zero link.

The shapefile is a modified census tract shapefile of New Jersey: a few tracts that have no useful attributal information were deleted from the map (hence left some holes) in ArcGIS &#65288;Start Edit -> select -> delete).

The original shapefile (with all the tracts) was read fine and create the correct neighbor list. But when using this modified one, I got all zero link. The same happened when I used GeoDa. Is this because of the holes created in the shapefile by deleting some polygons? If so, how can I make it work? If not, can anyone please direct me to the right solution?

Many thanks.

Best,
Danlin

___________________________________________
Danlin Yu, Ph.D.
Assistant Professor
Department of Earth & Environmental Studies
Montclair State University
Montclair, NJ, 07043
Tel: 973-655-4313
Fax: 973-655-4072
email: yud at mail.montclair.edu



From Roger.Bivand at nhh.no  Thu May 31 22:39:19 2007
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 31 May 2007 22:39:19 +0200 (CEST)
Subject: [R-sig-Geo] All zero link when reading a modified shapefile
 into R
In-Reply-To: <8a0b8c8a64ba.8a64ba8a0b8c@montclair.edu>
Message-ID: <Pine.LNX.4.44.0705312226490.23714-100000@reclus.nhh.no>

On Thu, 31 May 2007, Danlin Yu wrote:

> Hi all-
> 
> When I read a shapefile using read.shape into R, and used Map2poly ->
> poly2nb to create my neighbor list, the summary of the newly created
> neighbor list indicates that all the regions have zero link.
> 
> The shapefile is a modified census tract shapefile of New Jersey: a few
> tracts that have no useful attributal information were deleted from the
> map (hence left some holes) in ArcGIS &#65288;Start Edit -> select ->
> delete).
> 
> The original shapefile (with all the tracts) was read fine and create
> the correct neighbor list. But when using this modified one, I got all
> zero link. The same happened when I used GeoDa. Is this because of the
> holes created in the shapefile by deleting some polygons? If so, how can
> I make it work? If not, can anyone please direct me to the right
> solution?

Could you try with just one deleted polygon first? Is it possible that 
ArcGIS does some cleaning at the same time, and moves the shapes apart? 
If that is the case, might increasing the snap= argument in poly2nb() 
help?

Incidentally, poly2nb() does take an object extending SpatialPolygons too, 
so you can also do:

x <- readShapePoly("x.shp")
plot(x)
x_nb <- poly2nb(x) # maybe increase snap=

avoiding using Map and polylist objects. Then

plot(x, border="grey")
plot(x_nb, coordinates(x), pch=".")

plots the neighbour graph on the tract boundaries.

In the last resort, do the deletion in R:

x <- readShapePoly("x.shp")
keep <- x$attribute is useful, logical condition
table(keep)
plot(x, col=c("red", "green")[(keep+1)])
x_subset <- x[keep,]
writePolyShape(x_subset, fn="x_subset")

or the equivalent readOGR() and writeOGR() functions in rgdal.

Hope this helps,

Roger

> 
> Many thanks.
> 
> Best,
> Danlin
> 
> ___________________________________________
> Danlin Yu, Ph.D.
> Assistant Professor
> Department of Earth & Environmental Studies
> Montclair State University
> Montclair, NJ, 07043
> Tel: 973-655-4313
> Fax: 973-655-4072
> email: yud at mail.montclair.edu
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From andrew.niccolai at yale.edu  Thu May 31 22:54:17 2007
From: andrew.niccolai at yale.edu (Andrew Niccolai)
Date: Thu, 31 May 2007 16:54:17 -0400
Subject: [R-sig-Geo] Follow up: [R] surfaces and digital terrain model
In-Reply-To: <Pine.LNX.4.44.0705302252410.22911-100000@reclus.nhh.no>
References: <BAY102-DAV170E36744719728702285BBC2E0@phx.gbl><465DCC44.2070502@alumni.uwaterloo.ca>
	<Pine.LNX.4.44.0705302252410.22911-100000@reclus.nhh.no>
Message-ID: <000001c7a3c5$db20c070$0601a8c0@D3K86YB1>

I realize that as of yesterday, this message thread is 4 years old but can
someone possibly post the clines function that Renaud mentions in the
posting below?  That would be wonderful and most appreciated.

Thanks,
Andrew 


Andrew Niccolai
Doctoral Candidate
Yale School of Forestry


 
From: Renaud Lancelot <lancelot>
Date: Fri May 30 22:37:02 2003

Yesterday, I posted the following:

>>I have computed a digital terrain model from a set of points (x, y, z)
>>using the function interp() in package akima. I want to predict flooded
>>surfaces given target values of z. I can display the flooded surfaces
>>with contour() or image(), but I don't know how to get the polygons
>>delimiting the surfaces. Did anyone write a function for this purpose ?

Many thanks to Roger Bivand, Paul Murrel, Deepayan Sarkar, Barry
Rowlingson and Thomas W Blackwell for their replies and their help. Paul
Murrel provided me with a function "clines", kindly ported to Windows by
Duncan Murdoch. This function does exactly what I need, i.e. it returns
a list of polygons corresponding to target value(s) of z.

I wrote a function to compute (hopefully !) what I want, i.e. predicted
flooded surfaces given target values of z (managing the cases of several
independent watered surfaces, possibly with islands). Provided that Paul
Murrel agrees to share his function, I will be happy to send it to
anyone wishing to use and improve it (and debug it ;-) ).

Best regards and thanks again,

Renaud

-- 
Dr Renaud Lancelot, v?t?rinaire
CIRAD, D?partement Elevage et M?decine V?t?rinaire (CIRAD-Emvt)
Programme Productions Animales
http://www.cirad.fr/fr/pg_recherche/page.php?id=14
ISRA-LNERV                      tel    +221 832 49 02
BP 2057 Dakar-Hann              fax    +221 821 18 79 (CIRAD)
Senegal                         e-mail renaud.lancelot_at_cirad.fr



From andrew.niccolai at yale.edu  Thu May 31 22:56:14 2007
From: andrew.niccolai at yale.edu (Andrew Niccolai)
Date: Thu, 31 May 2007 16:56:14 -0400
Subject: [R-sig-Geo] Clines library
In-Reply-To: <Pine.LNX.4.44.0705302252410.22911-100000@reclus.nhh.no>
References: <BAY102-DAV170E36744719728702285BBC2E0@phx.gbl><465DCC44.2070502@alumni.uwaterloo.ca>
	<Pine.LNX.4.44.0705302252410.22911-100000@reclus.nhh.no>
Message-ID: <000101c7a3c6$21cbe090$0601a8c0@D3K86YB1>

I truly apologize, I just found the clines package.  Thanks. 


Andrew Niccolai
Doctoral Candidate
Yale School of Forestry



From h.wickham at gmail.com  Thu May 31 23:16:12 2007
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 31 May 2007 23:16:12 +0200
Subject: [R-sig-Geo] Clines library
In-Reply-To: <000101c7a3c6$21cbe090$0601a8c0@D3K86YB1>
References: <BAY102-DAV170E36744719728702285BBC2E0@phx.gbl>
	<465DCC44.2070502@alumni.uwaterloo.ca>
	<Pine.LNX.4.44.0705302252410.22911-100000@reclus.nhh.no>
	<000101c7a3c6$21cbe090$0601a8c0@D3K86YB1>
Message-ID: <f8e6ff050705311416w637daaa2m38b012f3078a2d81@mail.gmail.com>

You can now use contourLines in the grDevices package included with R.

Hadley

On 5/31/07, Andrew Niccolai <andrew.niccolai at yale.edu> wrote:
> I truly apologize, I just found the clines package.  Thanks.
>
>
> Andrew Niccolai
> Doctoral Candidate
> Yale School of Forestry
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



From ripley at stats.ox.ac.uk  Thu May 31 23:34:34 2007
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 31 May 2007 22:34:34 +0100 (BST)
Subject: [R-sig-Geo] [R] Follow up:  surfaces and digital terrain model
In-Reply-To: <000001c7a3c5$db20c070$0601a8c0@D3K86YB1>
References: <BAY102-DAV170E36744719728702285BBC2E0@phx.gbl><465DCC44.2070502@alumni.uwaterloo.ca>
	<Pine.LNX.4.44.0705302252410.22911-100000@reclus.nhh.no>
	<000001c7a3c5$db20c070$0601a8c0@D3K86YB1>
Message-ID: <Pine.LNX.4.64.0705312229090.8722@gannet.stats.ox.ac.uk>

I believe you are looking for the function contourLines() in package 
graphics.

At some point in the intervening years there was a clines package on CRAN, 
and if you know that you can find it at 
http://cran.r-project.org/src/contrib/Archive/C/clines_2.0-2.tar.gz
That confirms it has been superseded by contourLines.

BTW, it is Paul Murrell (two l's) we have to thank for this.


On Thu, 31 May 2007, Andrew Niccolai wrote:

> I realize that as of yesterday, this message thread is 4 years old but can
> someone possibly post the clines function that Renaud mentions in the
> posting below?  That would be wonderful and most appreciated.
>
> Thanks,
> Andrew
>
>
> Andrew Niccolai
> Doctoral Candidate
> Yale School of Forestry
>
>
>
> From: Renaud Lancelot <lancelot>
> Date: Fri May 30 22:37:02 2003
>
> Yesterday, I posted the following:
>
>>> I have computed a digital terrain model from a set of points (x, y, z)
>>> using the function interp() in package akima. I want to predict flooded
>>> surfaces given target values of z. I can display the flooded surfaces
>>> with contour() or image(), but I don't know how to get the polygons
>>> delimiting the surfaces. Did anyone write a function for this purpose ?
>
> Many thanks to Roger Bivand, Paul Murrel, Deepayan Sarkar, Barry
> Rowlingson and Thomas W Blackwell for their replies and their help. Paul
> Murrel provided me with a function "clines", kindly ported to Windows by
> Duncan Murdoch. This function does exactly what I need, i.e. it returns
> a list of polygons corresponding to target value(s) of z.
>
> I wrote a function to compute (hopefully !) what I want, i.e. predicted
> flooded surfaces given target values of z (managing the cases of several
> independent watered surfaces, possibly with islands). Provided that Paul
> Murrel agrees to share his function, I will be happy to send it to
> anyone wishing to use and improve it (and debug it ;-) ).
>
> Best regards and thanks again,
>
> Renaud
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



