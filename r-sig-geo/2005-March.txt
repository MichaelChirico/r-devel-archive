From abunn at whrc.org  Wed Mar  2 14:54:18 2005
From: abunn at whrc.org (abunn)
Date: Wed, 2 Mar 2005 08:54:18 -0500
Subject: [R-sig-Geo] Making GIS and R play nicely - some tips?
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEHADBAA.abunn@whrc.org>

Hi R Spatial People:

I am an experienced R user and an experienced GIS user - however, that GIS
is Arc. In the past when I've worked with spatial data in R, I've done so by
exporting the GIS data (always grids) from Arc as ASCII files. Then I read
them into R and go through some gymnastics to dump the no data values
(-9999) while I do my analysis (say a regression tree) and then write the
ASCII back out and suck it into Arc for display. This lose coupling seems
inelegant.

I'm starting on a new project where I have a time series of about 250 grids
that are roughly 1000 by 1000. There is a response variable and up to six or
so predictors. So, it's enough data that I feel it's time to learn how to do
it right. Question is, what's right?

Is the state of the art approach to use GRASS a la Bivand and Neteler's
paper (http://agec144.agecon.uiuc.edu/csiss/Rgeo/)? Is it better to dump the
grids to an imagine file (or bil) and read them with rgdal? I work mostly in
Windows (b/c of ESRI - damn them!) but switch gears onto Linux when the task
requires it. I have never used GRASS. Show me the way!

Thanks in advance, Andy



From Roger.Bivand at nhh.no  Wed Mar  2 16:12:11 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 2 Mar 2005 16:12:11 +0100 (CET)
Subject: [R-sig-Geo] Making GIS and R play nicely - some tips?
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIKEHADBAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.44.0503021554030.25208-100000@reclus.nhh.no>

On Wed, 2 Mar 2005, abunn wrote:

> Hi R Spatial People:
> 
> I am an experienced R user and an experienced GIS user - however, that GIS
> is Arc. In the past when I've worked with spatial data in R, I've done so by
> exporting the GIS data (always grids) from Arc as ASCII files. Then I read
> them into R and go through some gymnastics to dump the no data values
> (-9999) while I do my analysis (say a regression tree) and then write the
> ASCII back out and suck it into Arc for display. This lose coupling seems
> inelegant.
> 
> I'm starting on a new project where I have a time series of about 250 grids
> that are roughly 1000 by 1000. There is a response variable and up to six or
> so predictors. So, it's enough data that I feel it's time to learn how to do
> it right. Question is, what's right?
> 
> Is the state of the art approach to use GRASS a la Bivand and Neteler's
> paper (http://agec144.agecon.uiuc.edu/csiss/Rgeo/)? Is it better to dump the
> grids to an imagine file (or bil) and read them with rgdal? I work mostly in
> Windows (b/c of ESRI - damn them!) but switch gears onto Linux when the task
> requires it. I have never used GRASS. Show me the way!
> 

Well, there are plenty of possibilities. The first question (abstracting 
from the size of the data) is whether to go the way suggested in:

http://www.gisvet.org/Documents/GisVet04/RegularPapers/Tait.pdf

which leaves Arc in front, using R just as the compute engine, and passing 
data through StatConnector. I don't think that the code used has been 
published, but a very simple VBA example is at:

http://perso.univ-lr.fr/csaintje/Recherche/RArcgis/index.html

That permits the Arc user to avoid knowing about R if you need later to 
package the procedure for others to use. My guess is that this hasn't been 
checked on ArcGIS 9 (I'll try to check soon).

Another is as you suggest to use the rgdal package to move the data into 
R, and once we get rgdal better checked for writing, back out too. That 
puts R in front of the loose-coupled data.

A third is to use GRASS 5.4 and the R GRASS interface package, which reads 
and writes GRASS raster layers; GRASS can read from many formats and to 
many formats, and can run (as can the interface) under cygwin. This runs 
R on top of GRASS, which can be accessed through system(). The state 
of the art is certainly M. Neteler, H. Mitasova, 2004. Open Source GIS: A 
GRASS GIS Approach. Second Edition. 424 pages, Kluwer Academic Publishers, 
Boston, Dordrecht, ISBN 1-4020-8064-6 (Also published as eBook, ISBN 
1-4020-8065-4), chapter 13. You might also enjoy: 

http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Proceedings/FurlanelloEtAl.pdf

showing GRASS, R and PostgreSQL working together.

But isn't the real challenge going to be shoehorning 250 by 1000 by 1000 
by 8 into R at once - or are the data layers needed just one after the 
other? If I could grasp the workflow better, the advice above could become 
more focussed, I think.

Best wishes,

Roger

> Thanks in advance, Andy
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From abunn at whrc.org  Wed Mar  2 17:26:16 2005
From: abunn at whrc.org (abunn)
Date: Wed, 2 Mar 2005 11:26:16 -0500
Subject: [R-sig-Geo] Making GIS and R play nicely - some tips?
In-Reply-To: <Pine.LNX.4.44.0503021554030.25208-100000@reclus.nhh.no>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEHEDBAA.abunn@whrc.org>

Great. Thanks Roger and other others that replied.

> But isn't the real challenge going to be shoehorning 250 by 1000 by 1000
> by 8 into R at once - or are the data layers needed just one after the
> other? If I could grasp the workflow better, the advice above could become
> more focussed, I think.

I'm not positive on the workflow yet! That's one of the problems. I'm
finding it tough to wrap my head around the data since it is in both time
and space.

The data are not as huge as I thought at first. Each layer has 567 rows and
409 columns (231903 elements) but 137113 are no data. So, that makes the
actual amount of data to crunch much more manageable at 94790 elements per
layer.

The goal is to predict satellite reflectance as a function of climate. The
response data are monthly satellite reflectance over 19 years (So, that's
228 images). The predictor data are interpolated climate data and there
should be a minimum of three predictors and I'd like to use two more if they
are reliable. So, assuming three predictors the data would only run to 500mb
or so.

R > object.size(rep(runif(94790), times = 19*12*3)) / 2^20
[1] 494.6622

And if I can get all five then I'd be under a gig still.

R > object.size(rep(runif(94790), times = 19*12*5)) / 2^20
[1] 824.437

That's cumbersome but possible.

I've managed to read in a test example using erdas img files using rgdal
(thanks Tim Keitt):

R > library(rgdal)
R > junk <- list()
R > try1 <- GDAL.open("test.img")
R > getDriverLongName(getDriver(try1))
[1] "Erdas Imagine Images (.img)"
R > junk[[1]] <- c(getRasterData(try1))
R > GDAL.close(try1)
Closing GDAL dataset handle 00CACBF0...  destroyed ... done.
R > try2 <- GDAL.open("test2.img")
R > getDriverLongName(getDriver(try2))
[1] "Erdas Imagine Images (.img)"
R > junk[[2]] <- c(getRasterData(try2))
R > GDAL.close(try2)
Closing GDAL dataset handle 00C7F180...  destroyed ... done.
R > str(junk)
List of 2
 $ : int [1:231903] 0 0 0 0 0 0 0 0 0 0 ...
 $ : int [1:231903] 0 0 0 0 0 0 0 0 0 0 ...


So, to read all this in I can pipe a script to Arc to convert the grids to
images and loop through all the data. Does that sound like a reasonable
plan?

Thanks for all the help, Andy



From Roger.Bivand at nhh.no  Wed Mar  2 18:16:55 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 2 Mar 2005 18:16:55 +0100 (CET)
Subject: [R-sig-Geo] Making GIS and R play nicely - some tips?
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIKEHEDBAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.44.0503021751110.25208-100000@reclus.nhh.no>

On Wed, 2 Mar 2005, abunn wrote:

> Great. Thanks Roger and other others that replied.
> 
> > But isn't the real challenge going to be shoehorning 250 by 1000 by 1000
> > by 8 into R at once - or are the data layers needed just one after the
> > other? If I could grasp the workflow better, the advice above could become
> > more focussed, I think.
> 
> I'm not positive on the workflow yet! That's one of the problems. I'm
> finding it tough to wrap my head around the data since it is in both time
> and space.
> 
> The data are not as huge as I thought at first. Each layer has 567 rows and
> 409 columns (231903 elements) but 137113 are no data. So, that makes the
> actual amount of data to crunch much more manageable at 94790 elements per
> layer.
> 
> The goal is to predict satellite reflectance as a function of climate. The
> response data are monthly satellite reflectance over 19 years (So, that's
> 228 images). The predictor data are interpolated climate data and there
> should be a minimum of three predictors and I'd like to use two more if they
> are reliable. So, assuming three predictors the data would only run to 500mb
> or so.
> 
> R > object.size(rep(runif(94790), times = 19*12*3)) / 2^20
> [1] 494.6622
> 
> And if I can get all five then I'd be under a gig still.
> 
> R > object.size(rep(runif(94790), times = 19*12*5)) / 2^20
> [1] 824.437
> 
> That's cumbersome but possible.
> 
> I've managed to read in a test example using erdas img files using rgdal
> (thanks Tim Keitt):
> 
> R > library(rgdal)
> R > junk <- list()
> R > try1 <- GDAL.open("test.img")
> R > getDriverLongName(getDriver(try1))
> [1] "Erdas Imagine Images (.img)"
> R > junk[[1]] <- c(getRasterData(try1))
> R > GDAL.close(try1)
> Closing GDAL dataset handle 00CACBF0...  destroyed ... done.
> R > try2 <- GDAL.open("test2.img")
> R > getDriverLongName(getDriver(try2))
> [1] "Erdas Imagine Images (.img)"
> R > junk[[2]] <- c(getRasterData(try2))
> R > GDAL.close(try2)
> Closing GDAL dataset handle 00C7F180...  destroyed ... done.
> R > str(junk)
> List of 2
>  $ : int [1:231903] 0 0 0 0 0 0 0 0 0 0 ...
>  $ : int [1:231903] 0 0 0 0 0 0 0 0 0 0 ...
> 
> 
> So, to read all this in I can pipe a script to Arc to convert the grids to
> images and loop through all the data. Does that sound like a reasonable
> plan?

What are the volumes going back?

Another itch to scratch is how much more you will get in terms of
precision in the coefficient point estimates by actually forcing all this
data down lm() or whatever's throat? Would it be possible to reduce the
load by using subsetting in rgdal, given that you had the *.img and model
based files lined up (or just use locations for which you have observed
climate data)?  Won't the interpolation errors in the predictors feed
through? I think the Furlanello et al. paper may be very helpful (they use
randomForest on regression trees of climate/topography drivers).

Interesting problem.

Roger

> 
> Thanks for all the help, Andy
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From tkeitt at mail.utexas.edu  Wed Mar  2 18:47:37 2005
From: tkeitt at mail.utexas.edu (Timothy H. Keitt)
Date: Wed, 02 Mar 2005 11:47:37 -0600
Subject: [R-sig-Geo] Making GIS and R play nicely - some tips?
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIKEHEDBAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIKEHEDBAA.abunn@whrc.org>
Message-ID: <1109785657.8155.14.camel@localhost>

Hi Andy,

I've had pretty good luck reading large images line-by-line in rgdal.
Its kind of slow, but not a bad as one might imagine. ArcGIS went down
in flames trying to do any sort of processing on these files (10K x
10K). A better solution I think though will be to write space-time
points into a postgis database (given lots of disk space) and use
queries to subset the data as needed. I've found the database approach
increadibly useful for working with BBS + climate data (~1e7 records).

Ciao,
THK

On Wed, 2005-03-02 at 11:26 -0500, abunn wrote:
> Great. Thanks Roger and other others that replied.
> 
> > But isn't the real challenge going to be shoehorning 250 by 1000 by 1000
> > by 8 into R at once - or are the data layers needed just one after the
> > other? If I could grasp the workflow better, the advice above could become
> > more focussed, I think.
> 
> I'm not positive on the workflow yet! That's one of the problems. I'm
> finding it tough to wrap my head around the data since it is in both time
> and space.
> 
> The data are not as huge as I thought at first. Each layer has 567 rows and
> 409 columns (231903 elements) but 137113 are no data. So, that makes the
> actual amount of data to crunch much more manageable at 94790 elements per
> layer.
> 
> The goal is to predict satellite reflectance as a function of climate. The
> response data are monthly satellite reflectance over 19 years (So, that's
> 228 images). The predictor data are interpolated climate data and there
> should be a minimum of three predictors and I'd like to use two more if they
> are reliable. So, assuming three predictors the data would only run to 500mb
> or so.
> 
> R > object.size(rep(runif(94790), times = 19*12*3)) / 2^20
> [1] 494.6622
> 
> And if I can get all five then I'd be under a gig still.
> 
> R > object.size(rep(runif(94790), times = 19*12*5)) / 2^20
> [1] 824.437
> 
> That's cumbersome but possible.
> 
> I've managed to read in a test example using erdas img files using rgdal
> (thanks Tim Keitt):
> 
> R > library(rgdal)
> R > junk <- list()
> R > try1 <- GDAL.open("test.img")
> R > getDriverLongName(getDriver(try1))
> [1] "Erdas Imagine Images (.img)"
> R > junk[[1]] <- c(getRasterData(try1))
> R > GDAL.close(try1)
> Closing GDAL dataset handle 00CACBF0...  destroyed ... done.
> R > try2 <- GDAL.open("test2.img")
> R > getDriverLongName(getDriver(try2))
> [1] "Erdas Imagine Images (.img)"
> R > junk[[2]] <- c(getRasterData(try2))
> R > GDAL.close(try2)
> Closing GDAL dataset handle 00C7F180...  destroyed ... done.
> R > str(junk)
> List of 2
>  $ : int [1:231903] 0 0 0 0 0 0 0 0 0 0 ...
>  $ : int [1:231903] 0 0 0 0 0 0 0 0 0 0 ...
> 
> 
> So, to read all this in I can pipe a script to Arc to convert the grids to
> images and loop through all the data. Does that sound like a reasonable
> plan?
> 
> Thanks for all the help, Andy
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo



From abunn at whrc.org  Wed Mar  2 22:17:24 2005
From: abunn at whrc.org (abunn)
Date: Wed, 2 Mar 2005 16:17:24 -0500
Subject: [R-sig-Geo] Making GIS and R play nicely - some tips?
In-Reply-To: <1109785657.8155.14.camel@localhost>
Message-ID: <NEBBIPHDAMMOKDKPOFFIIEHLDBAA.abunn@whrc.org>

Hey Tim:

I'm writing the database as we speak which will take awhile but will then be
done. It is going faster than I thought it would. I figured out how to call
arc from the prompt and pass it an aml and variables from R:

        aml2pass <- paste('arc "&r make.an.image.aml', which.var, year,
                           formatC(month, width = 2, flag = 0), '"', sep = "
")
        system(aml2pass)

This means that I can write temporary files from arc, process them and clean
up without having a lot of loose ends. So, with that and rgdal, everything
is smooth. I'll leave GRASS for another day (I've been saying that for about
seven years now).

Thanks, A



From mdsumner at utas.edu.au  Wed Mar  2 23:25:39 2005
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Thu, 03 Mar 2005 09:25:39 +1100
Subject: [R-sig-Geo] Making GIS and R play nicely - some tips?
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIKEHADBAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIKEHADBAA.abunn@whrc.org>
Message-ID: <6.0.0.22.1.20050303091534.01da1d68@postoffice.utas.edu.au>


>
>Is the state of the art approach to use GRASS a la Bivand and Neteler's
>paper (http://agec144.agecon.uiuc.edu/csiss/Rgeo/)? Is it better to dump the
>grids to an imagine file (or bil) and read them with rgdal? I work mostly in
>Windows (b/c of ESRI - damn them!) but switch gears onto Linux when the task
>requires it. I have never used GRASS. Show me the way!

Hello

Disclaimer - I've never used GRASS, and I haven't used ESRI much.

You might consider using Manifold GIS:  http://www.manifold.net/

Much more Windows (and user) friendly IMO, and has many options for input 
and output for most formats, as well as generic text and binary.    (Give 
it a few years and Manifold will have taken over much of the GIS market.)

You could output your grids to binary files (or ASCII), and with some 
modification of my simple script write the projection metadata to XML - 
then script-import these to Manifold.  This scripting could be done all 
within R if that was desirable using RDCOMClient

I have some R routines that export with Manifold-friendly metadata in XML, 
very rudimentary but might give you some ideas.

Passing grid (matrix) data from R to Manifold:
http://www.georeference.org/Forums/forums/thread-view.asp?tid=789&posts=9

Running R from Manifold:
http://www.georeference.org/Forums/forums/thread-view.asp?tid=650&posts=8

Running Manifold from R:
http://www.georeference.org/Forums/forums/thread-view.asp?tid=876&posts=9

Cheers, Mike.




###############################################

Michael Sumner - PhD. candidate
Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
University of Tasmania
Private Bag 77, Hobart, Tas 7001, Australia
Phone: (03) 6226 1752

http://www.acecrc.org.au/



From andersm at maths.lth.se  Fri Mar  4 15:45:41 2005
From: andersm at maths.lth.se (Anders Malmberg)
Date: Fri, 04 Mar 2005 15:45:41 +0100
Subject: [R-sig-Geo] geoR
Message-ID: <42287495.4060405@maths.lth.se>

Hi,

A quick question on the likfit routine in the geoR package.
It is not possible to bound both tausq.rel and sigmasq. If both of them
are unknown the optimization is carried out wrt tausq.rel
and sigmasq is not considered in the optimization. Correct?

bon weekend.

best regards,
Anders Malmberg



From olefc at daimi.au.dk  Fri Mar  4 18:53:01 2005
From: olefc at daimi.au.dk (Ole F. Christensen)
Date: Fri, 04 Mar 2005 18:53:01 +0100
Subject: [R-sig-Geo] geoR
In-Reply-To: <42287495.4060405@maths.lth.se>
References: <42287495.4060405@maths.lth.se>
Message-ID: <4228A07D.2070109@daimi.au.dk>

Dear Anders

* The maximum likelihood estimate of the partial sill parameter sigmasq 
can be expressed explictly as function of the other covariance parameters.
Therefore parameter estimate for sigmasq is NOT found by numerical 
maximisation, and as consequence in likfit you cannot specify that that 
sigmasq should be within certain limits.

* The parameter estimate for the relative nugget tausq.rel is found by 
numerical maximisation. Therefore you can specify that this parameter 
should be within certain limits.

Best

Ole

Anders Malmberg wrote:

> Hi,
>
> A quick question on the likfit routine in the geoR package.
> It is not possible to bound both tausq.rel and sigmasq. If both of them
> are unknown the optimization is carried out wrt tausq.rel
> and sigmasq is not considered in the optimization. Correct?
>
> bon weekend.
>
> best regards,
> Anders Malmberg
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>

-- 
Ole F. Christensen
BiRC - Bioinformatics Research Center
University of Aarhus



From RRoa at fisheries.gov.fk  Sat Mar  5 14:18:53 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Sat, 5 Mar 2005 11:18:53 -0200
Subject: [R-sig-Geo] geoR
Message-ID: <AEF7C40954294D42990B3F90C6C73A45010164BA@figmail.fig.fk>

-----Original Message-----
From:	r-sig-geo-bounces at stat.math.ethz.ch on behalf of Ole F. Christensen
Sent:	Fri 3/4/2005 3:53 PM
To:	Anders Malmberg
Cc:	R-sig-Geo at stat.math.ethz.ch
Subject:	Re: [R-sig-Geo] geoR
Dear Anders

* The maximum likelihood estimate of the partial sill parameter sigmasq 
can be expressed explictly as function of the other covariance parameters.
Therefore parameter estimate for sigmasq is NOT found by numerical 
maximisation, and as consequence in likfit you cannot specify that that 
sigmasq should be within certain limits.

* The parameter estimate for the relative nugget tausq.rel is found by 
numerical maximisation. Therefore you can specify that this parameter 
should be within certain limits.

Best

Ole

----------------
I understand that the range parameter phi is also a 'free parameter'. Am i correct?

Another quick question: i haven't been able to obtain the estimation variance of the phi parameter estimate as i do for the beta estimate with likfitobject$beta.var. What is the necessary command?
Cheers

Ruben

[snip Anders Malmberg's original question]



From olefc at daimi.au.dk  Mon Mar  7 08:12:56 2005
From: olefc at daimi.au.dk (Ole F. Christensen)
Date: Mon, 07 Mar 2005 08:12:56 +0100
Subject: [R-sig-Geo] geoR
In-Reply-To: <AEF7C40954294D42990B3F90C6C73A45010164BA@figmail.fig.fk>
References: <AEF7C40954294D42990B3F90C6C73A45010164BA@figmail.fig.fk>
Message-ID: <422BFEF8.6080508@daimi.au.dk>

Yes, the ``range parameter'' phi is a free parameter.
Calculation of variance of the estimate for phi  is not implemented in 
function likfit.
Instead you should use the proflik function to obtain the profile 
likelihood for this parameter.

Ole

Ruben Roa wrote:

>-----Original Message-----
>From:	r-sig-geo-bounces at stat.math.ethz.ch on behalf of Ole F. Christensen
>Sent:	Fri 3/4/2005 3:53 PM
>To:	Anders Malmberg
>Cc:	R-sig-Geo at stat.math.ethz.ch
>Subject:	Re: [R-sig-Geo] geoR
>Dear Anders
>
>* The maximum likelihood estimate of the partial sill parameter sigmasq 
>can be expressed explictly as function of the other covariance parameters.
>Therefore parameter estimate for sigmasq is NOT found by numerical 
>maximisation, and as consequence in likfit you cannot specify that that 
>sigmasq should be within certain limits.
>
>* The parameter estimate for the relative nugget tausq.rel is found by 
>numerical maximisation. Therefore you can specify that this parameter 
>should be within certain limits.
>
>Best
>
>Ole
>
>----------------
>I understand that the range parameter phi is also a 'free parameter'. Am i correct?
>
>Another quick question: i haven't been able to obtain the estimation variance of the phi parameter estimate as i do for the beta estimate with likfitobject$beta.var. What is the necessary command?
>Cheers
>
>Ruben
>
>[snip Anders Malmberg's original question]
>
>_______________________________________________
>R-sig-Geo mailing list
>R-sig-Geo at stat.math.ethz.ch
>https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>  
>

-- 
Ole F. Christensen
BiRC - Bioinformatics Research Center
University of Aarhus



From andersm at maths.lth.se  Mon Mar  7 16:08:32 2005
From: andersm at maths.lth.se (Anders Malmberg)
Date: Mon, 07 Mar 2005 16:08:32 +0100
Subject: [R-sig-Geo] Limiting problems in geoR
Message-ID: <422C6E70.1010605@maths.lth.se>


Hi,

Thanks for the answer on my previous question. However, it leads me to new
problems.

I am struggling to bound the relative tausq parameter to not be too high 
in my
estimations. I do not want it to be above a certain limit that would 
violate the
model; it yields surfaces that are too ruff. First I want estimates from 
variofit,
but this does not seem to work. Shouldn't this give a tausq^2/sigmasq^2 
in (0,0.05)?

library("geoR")
data(s100)
vario100 <- variog(s100, max.dist=1)
ini.vals <- c(1,0.5)
limits <- pars.limits(phi=c(0,2),nugget.rel=c(0,0.05))
ols <- variofit(vario100, ini=ini.vals, fix.nug=FALSE,
                limits=limits,wei="equal")
ols$nugget/ols$cov.pars[1]


Thanks in advance,

Anders Malmberg



From tghoward at gw.dec.state.ny.us  Wed Mar  9 16:01:03 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Wed, 09 Mar 2005 10:01:03 -0500
Subject: [R-sig-Geo] Sampling polygons for analysis in CART/randomForests
Message-ID: <s22ec97c.044@gwsmtp.DEC.STATE.NY.US>

Dear R - Geo community:
   This is perhaps a bit off topic (really a geostats question), but
we've been struggling with these questions for about 6 months now, and
it just reared its head again when I posed a question on one of ESRI's
discussion groups. I realized you would be much more experienced in this
issue and might be interested in offering advice. And this might be
something others are struggling with as well. 
   Background: We are creating predictive habitat distribution models
(a good synthesis paper is Guisan and Zimmerman 2000. Ecological
Modelling 135:147-186); using known locations of species to create a
model (and subsequent raster) that predicts the locations for additional
similar habitats. Many different algorithms have been used for actually
doing the predictive modeling (e.g. logistic regression, Classification
and regression trees [CART], ordination, maximum entropy), but in all
cases the user needs a full coverage of all the environmental variables
to be used and known locations for the element (species) being modeled.
Known locations must be attributed with values from all the
environmental layers. We've been using two different different
algorithms, one parametric (MaxEnt - separate package) and one
non-parametric (the RandomForests implementation of CART). I'm using the
R implementation of randomForest, but (embarrasingly) I'm currently
doing it by dumping the data to ascii and then reading the tables back
into R (by looping through them to keep the size manageable).  I saw the
recent post on this listserv
(https://stat.ethz.ch/pipermail/r-sig-geo/2005-March/000338.html) that
mentions Furlanello et al. (wow!) and someday we'll strive to integrate
in a similar way. 

This Furlanello paper is a good example of how we diverge and our
resulting dilemma. Furlanello et al. (and most others, as far as I can
tell, but please enlighten me if I'm wrong), begin with known POINT
locations. Point locations are easy to attribute with data from your
environmental layers.   We have good-quality POLYGON data and we want to
be sure the environmental variability captured by the polygon is
represented in the 'presence' environmental data we pass on for
analysis. How to best capture that full variability?  Note one of our
algorithms is parametric (MaxEnt) and one non-parametric
(randomForest).. and I have no problem with deviating from traditional
parametric sampling methodologies and leaving many formerly required
parametric assumptions behind (or is that a big mistake?). We've thought
long and hard about random sampling, regular sampling, and grabbing all
the points. 

We currently have about 36 environmental variables, we've resampled
them all to one size (30m). That meant bringing all the rasters derived
from 10m DEM up to 30 meter and all the coarser rasters down to 30m. I'm
being transparent about breaking the 'rules' here just so you know we
are aware of the issues surrounding scale. Our rasters are pretty big
(22578 rows x 17160 cols.... a single ascii is about 2GB).

Again, our goal is to, as fully as possible, represent the environment
within each polygon. For random (or regular) sampling; if you do a
consistent number of samples for every polygon, small polygons are
overweighted. If you sample on a per/area basis, larger polygons get
more 'weight'.  We settled on grabbing ALL the raster cells in each
polygon and sticking a point in the center of each cell and attributing
these points.  This  obviously blew up with larger polygons (much to
many cells), and we realized we'd need to go back to subsampling.   I
thought the best step would be to sample randomly without replacement so
that I'd get as full representation as possible (e.g. if a sample of 90
cells/points are requested, return 90 different cells/points). One
problem that's been pointed out is that I'm treating rasters as discrete
data when most are really a representation of continuous data and
interpolating between cell centers is more appropriate. Unfortunately I
don't have an easy way of doing this, but I'm trying to look into it.

My questions: 

Has anyone ever used *all* the cells in a polygon representation when
attributing data for later statistcal analyses?

Would sampling without replacement be an appropriate approach for
returning as much within polygon variability as possible?  If so, how
would you do it?

If randomly sampling, has anyone sampled relatively more cells in
smaller polygons, and relatively fewer cells in larger polygons, so that
you reach an asymptote in number sampled per polygon?

and related: If randomly sampling, do you tend to sample a static
number of cells per polygon or a per-area number of cells. 


Thank you for your time.
Tim Howard



From tghoward at gw.dec.state.ny.us  Tue Mar 15 13:46:46 2005
From: tghoward at gw.dec.state.ny.us (Tim Howard)
Date: Tue, 15 Mar 2005 07:46:46 -0500
Subject: [R-sig-Geo] Sampling polygons for analysis in CART/randomForests
Message-ID: <s23692f7.091@gwsmtp.DEC.STATE.NY.US>

The lack of response indicates I was straying too far from the intent of
the list. I apologize for my momentary lack of judgement... it won't
happen again.

Sincerely,
Tim Howard

****
Dear R - Geo community:
   This is perhaps a bit off topic (really a geostats question), but
we've been struggling with these questions for about 6 months now, and
it just reared its head again when I posed a question on one of ESRI's
discussion groups. I realized you would be much more experienced in
this
issue and might be interested in offering advice. And this might be
something others are struggling with as well. 
   Background: We are creating predictive habitat distribution models
(a good synthesis paper is Guisan and Zimmerman 2000. Ecological
Modelling 135:147-186); using known locations of species to create a

<snip>



From mario.gellrich at wsl.ch  Tue Mar 22 19:17:50 2005
From: mario.gellrich at wsl.ch (Mario Gellrich)
Date: Tue, 22 Mar 2005 19:17:50 +0100
Subject: [R-sig-Geo] exporting weiths matrix in .txt-file in spdep
Message-ID: <5.2.1.1.1.20050322191005.03633388@mail.wsl.ch>

Hi,

I have created a row-standardized spatial weigths matrix in spdep and 
wonder how it can be transformed to a simple .txt-file for export to other 
programs or simply visualisation of the weights matrix in form of rows and 
columns. Does anybody have a clue how to write out a weights matrix in spdep?

Best regards

Mario Gellrich
--------------------------------------------------
Mario Gellrich
Swiss Federal Research Institute WSL
Economics Section
Z?rcherstrasse 111
CH-8903 Birmensdorf
Tel:       ++41 (0)1 739 2520
Fax:       ++41 (0)1 739 22 15
e-mail:    mario.gellrich at wsl.ch
Web: http://www.wsl.ch/phonebook/STAFF/2409.ehtml



From Roger.Bivand at nhh.no  Tue Mar 22 21:16:36 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 22 Mar 2005 21:16:36 +0100 (CET)
Subject: [R-sig-Geo] exporting weiths matrix in .txt-file in spdep
In-Reply-To: <5.2.1.1.1.20050322191005.03633388@mail.wsl.ch>
Message-ID: <Pine.LNX.4.44.0503222110030.24244-100000@reclus.nhh.no>

On Tue, 22 Mar 2005, Mario Gellrich wrote:

> Hi,
> 
> I have created a row-standardized spatial weigths matrix in spdep and 
> wonder how it can be transformed to a simple .txt-file for export to other 
> programs or simply visualisation of the weights matrix in form of rows and 
> columns. Does anybody have a clue how to write out a weights matrix in spdep?

For example:

> library(spdep)
> data(columbus)
> colnbMat <- nb2mat(col.gal.nb, style="B")
> str(colnbMat)
 num [1:49, 1:49] 0 1 1 0 0 0 0 0 0 0 ...
 - attr(*, "dimnames")=List of 2
  ..$ : chr [1:49] "1005" "1001" "1006" "1002" ...
  ..$ : NULL
 - attr(*, "call")= language nb2mat(neighbours = col.gal.nb, style = "B")
> library(MASS)
> mattmp <- tempfile()
> write.matrix(colnbMat, file=mattmp)
> system(paste("cat", mattmp))
0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
....

For visualisation, this is nice, and scales to largish n:

> image(asMatrixCsrListw(nb2listw(col.gal.nb, style="B")))

Roger


> 
> Best regards
> 
> Mario Gellrich
> --------------------------------------------------
> Mario Gellrich
> Swiss Federal Research Institute WSL
> Economics Section
> Z?rcherstrasse 111
> CH-8903 Birmensdorf
> Tel:       ++41 (0)1 739 2520
> Fax:       ++41 (0)1 739 22 15
> e-mail:    mario.gellrich at wsl.ch
> Web: http://www.wsl.ch/phonebook/STAFF/2409.ehtml
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From hi_ono2001 at ybb.ne.jp  Wed Mar 23 21:15:25 2005
From: hi_ono2001 at ybb.ne.jp (Hisaji ONO)
Date: Thu, 24 Mar 2005 05:15:25 +0900
Subject: [R-sig-Geo] 2D kernel density estimation using Epanechnikov
	function on R
Message-ID: <005201c52fe5$0cbf5850$4d8001db@webgis>

Hello.

 Does anyone know whether any function for  2D kernel density estimation 
using Epanechnikov function, which seemed to be used in ArcGIS's Spatial 
Analyst, of R?


 Regards.



From tkeitt at mail.utexas.edu  Wed Mar 23 21:40:49 2005
From: tkeitt at mail.utexas.edu (Tim Keitt)
Date: Wed, 23 Mar 2005 14:40:49 -0600
Subject: [R-sig-Geo] 2D kernel density estimation using Epanechnikov
	function on R
In-Reply-To: <005201c52fe5$0cbf5850$4d8001db@webgis>
References: <005201c52fe5$0cbf5850$4d8001db@webgis>
Message-ID: <1111610449.1111.13.camel@localhost>

If there is one, it would probably be in the 'fields' package.

THK

On Thu, 2005-03-24 at 05:15 +0900, Hisaji ONO wrote:
> Hello.
> 
>  Does anyone know whether any function for  2D kernel density estimation 
> using Epanechnikov function, which seemed to be used in ArcGIS's Spatial 
> Analyst, of R?
> 
> 
>  Regards.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo



From slist at oomvanlieshout.net  Thu Mar 24 11:04:39 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Thu, 24 Mar 2005 12:04:39 +0200
Subject: [R-sig-Geo] Using adehabitat and splancs to achieve
 point-in-polygon analysis on raster?
Message-ID: <424290B7.8000906@oomvanlieshout.net>

Dear Clement and Roger,

I send this email to you directly, because it concerns the link between 
Adehabitat and Splancs.

I have two sources of polygons:
- polygon shape files with areas of interest;
- polygons calculated using the home range methods in Adehabitat.
For the moment, each polygon source will only contain a single polygon.

I have one source of points: a grid representing NDVI values. I read the 
grid using the import.asc function in Adehabitat.

I would like to use both polygon sources to select the points in the 
NDVI raster which are inside the relevant polygons. Then I want to run 
several statistics (mean, standard deviation, etc) on the NDVI values of 
these points.

I intend to use the function inpip from Splancs, but it is not clear to 
me how I get the two polygon sources in the format used by Splancs.

Would you be able to point me in the right direction?

Thanks in advance for your help,

Sander.


-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From calenge at biomserv.univ-lyon1.fr  Fri Mar 25 15:35:29 2005
From: calenge at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?Cl=E9ment_Calenge?=)
Date: Fri, 25 Mar 2005 15:35:29 +0100
Subject: [R-sig-Geo] Re: Using adehabitat and splancs to achieve
 point-in-polygon analysis on raster?
In-Reply-To: <424290B7.8000906@oomvanlieshout.net>
References: <424290B7.8000906@oomvanlieshout.net>
Message-ID: <424421B1.7030900@biomserv.univ-lyon1.fr>

Dear Sander,

Sorry for my late answer, I was in the field.

> I send this email to you directly, because it concerns the link 
> between Adehabitat and Splancs.
>
> I have two sources of polygons:
> - polygon shape files with areas of interest;
> - polygons calculated using the home range methods in Adehabitat.
> For the moment, each polygon source will only contain a single polygon.
> I have one source of points: a grid representing NDVI values. I read 
> the grid using the import.asc function in Adehabitat.
> I would like to use both polygon sources to select the points in the 
> NDVI raster which are inside the relevant polygons. Then I want to run 
> several statistics (mean, standard deviation, etc) on the NDVI values 
> of these points.

The solution depends on what you call "polygon". The function mcp.rast() 
is intended to "rasterize"
a polygon on a raster grid of class "asc" or "kasc". The polygon is here 
stored as a data frame with
two columns (the X and Y coordinates of the polygon). The function 
hr.rast() can be used if you have
an object of class "area", i.e. a data.frame with three columns (the X 
and Y coordinates of the polygon,
and a third column defining the ID of the polygon).

Now, it seems that  you work with more complex type of polygons, e.g. 
kernel estimations of home ranges
(where each animal has an home range, and each home range can be made of 
several polygons). In such cases,
the "Spatial join" operation can be more complex. I detail below an 
example of "spatial join" with a kernel
estimation of wild boar home ranges. Note, that in this case, you do not 
have to compute the contour of
the estimation. Just copy and paste the lines below on R:

## loads and display the data
library(adehabitat)
data(puechabon)

## keep only the elevation
el<-getkasc(puechabon$kasc,1)
image(el)

## estimate a kernel home-range
 kud<-kernelUD(puechabon$locs[,c("X","Y")], puechabon$locs$Name, grid=el)
## kud contains a density
## note that the grid containing the values of the elevation is used for 
the estimation

vud<-getvolumeUD(kud)
image(vud)
## vud contains the limits of the home ranges estimated at different levels

toto<-lapply(vud, function(x) x$UD)
## toto is the list of maps of Utilization Distribution
tutu<-lapply(toto, function(x) {x[x<95]<-1; x[x>95]<-NA; return(x)})
## tutu is a list of raster maps containing 1 if the pixel is inside
## the 95% home range and NA otherwise

foo<-as.kasc(tutu)
image(foo)
## foo is a kasc object

jj=lapply(foo, function(x) x*el)
image(jj)
## jj contains the values of elevation inside the home range of the animals

The last method that is available for home-range estimation of animals 
home ranges
is the Nearest Neighbor Convex hull method of Getz and Wilmers (2004). 
The function
NNCH.rast() can be used to "rasterize" the home range estimation in the 
same way, and then
you can multiply the resulting maps with your NDVI in the same way.
Hope this helps,


Cl?ment.

-- 
Cl?ment CALENGE
LBBE - UMR CNRS 5558 - Universit? 
Claude Bernard Lyon 1 - FRANCE
tel. (+33) 04.72.43.27.57
fax. (+33) 04.72.43.13.88



From slist at oomvanlieshout.net  Fri Mar 25 16:45:10 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 25 Mar 2005 17:45:10 +0200
Subject: [R-sig-Geo] Re: Using adehabitat and splancs to achieve
 point-in-polygon analysis on raster?
In-Reply-To: <424421B1.7030900@biomserv.univ-lyon1.fr>
References: <424290B7.8000906@oomvanlieshout.net>
	<424421B1.7030900@biomserv.univ-lyon1.fr>
Message-ID: <42443206.6050505@oomvanlieshout.net>

Hi Clement,

Hope you had a good time in the field! Is it spring in France?

Thanks for the very helpful reply! I will try it right now! I need to
present at a conference in early April and would like to get these
results before that!

Just three quick questions before you go to enjoy the Easter weekend:

- how do I get the values out of 'jj' (## jj contains the values of
elevation inside the home range of the animals) to calculate mean,
variance, etc, across all the points in 'jj'? I assume 'jj' is of class
'kasc'. In my case it might be 'asc' as I will have only a single home
range at the time (one animal).

- can I get the points out of the 'kasc' object in a way that I can
still run spatial statistics on them? I would like to calculate the
nugget and range for the NDVI values within the home range.

- do you know how to convert a polygon shapefile into the same class as
produced by the command kernelUD? Alternatively I could import a raster
version of the polygon shape file, convert it to the correct format, and
repeat the spatial join method you are suggesting for the home ranges!

Thanks again,

Sander.


Cl?ment Calenge wrote:
> Dear Sander,
> 
> Sorry for my late answer, I was in the field.
> 
>> I send this email to you directly, because it concerns the link 
>> between Adehabitat and Splancs.
>>
>> I have two sources of polygons:
>> - polygon shape files with areas of interest;
>> - polygons calculated using the home range methods in Adehabitat.
>> For the moment, each polygon source will only contain a single polygon.
>> I have one source of points: a grid representing NDVI values. I read 
>> the grid using the import.asc function in Adehabitat.
>> I would like to use both polygon sources to select the points in the 
>> NDVI raster which are inside the relevant polygons. Then I want to run 
>> several statistics (mean, standard deviation, etc) on the NDVI values 
>> of these points.
> 
> The solution depends on what you call "polygon". The function mcp.rast() 
> is intended to "rasterize"
> a polygon on a raster grid of class "asc" or "kasc". The polygon is here 
> stored as a data frame with
> two columns (the X and Y coordinates of the polygon). The function 
> hr.rast() can be used if you have
> an object of class "area", i.e. a data.frame with three columns (the X 
> and Y coordinates of the polygon,
> and a third column defining the ID of the polygon).
> 
> Now, it seems that  you work with more complex type of polygons, e.g. 
> kernel estimations of home ranges
> (where each animal has an home range, and each home range can be made of 
> several polygons). In such cases,
> the "Spatial join" operation can be more complex. I detail below an 
> example of "spatial join" with a kernel
> estimation of wild boar home ranges. Note, that in this case, you do not 
> have to compute the contour of
> the estimation. Just copy and paste the lines below on R:
> 
> ## loads and display the data
> library(adehabitat)
> data(puechabon)
> 
> ## keep only the elevation
> el<-getkasc(puechabon$kasc,1)
> image(el)
> 
> ## estimate a kernel home-range
> kud<-kernelUD(puechabon$locs[,c("X","Y")], puechabon$locs$Name, grid=el)
> ## kud contains a density
> ## note that the grid containing the values of the elevation is used for 
> the estimation
> 
> vud<-getvolumeUD(kud)
> image(vud)
> ## vud contains the limits of the home ranges estimated at different levels
> 
> toto<-lapply(vud, function(x) x$UD)
> ## toto is the list of maps of Utilization Distribution
> tutu<-lapply(toto, function(x) {x[x<95]<-1; x[x>95]<-NA; return(x)})
> ## tutu is a list of raster maps containing 1 if the pixel is inside
> ## the 95% home range and NA otherwise
> 
> foo<-as.kasc(tutu)
> image(foo)
> ## foo is a kasc object
> 
> jj=lapply(foo, function(x) x*el)
> image(jj)
> ## jj contains the values of elevation inside the home range of the animals
> 
> The last method that is available for home-range estimation of animals 
> home ranges
> is the Nearest Neighbor Convex hull method of Getz and Wilmers (2004). 
> The function
> NNCH.rast() can be used to "rasterize" the home range estimation in the 
> same way, and then
> you can multiply the resulting maps with your NDVI in the same way.
> Hope this helps,
> 
> 
> Cl?ment.
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From calenge at biomserv.univ-lyon1.fr  Fri Mar 25 18:12:21 2005
From: calenge at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?Cl=E9ment_Calenge?=)
Date: Fri, 25 Mar 2005 18:12:21 +0100
Subject: [R-sig-Geo] Re: Using adehabitat and splancs to achieve
 point-in-polygon analysis on raster?
In-Reply-To: <42443206.6050505@oomvanlieshout.net>
References: <424290B7.8000906@oomvanlieshout.net>
	<424421B1.7030900@biomserv.univ-lyon1.fr>
	<42443206.6050505@oomvanlieshout.net>
Message-ID: <42444675.7050408@biomserv.univ-lyon1.fr>


>
> Hope you had a good time in the field! Is it spring in France?

Yes, though a little cloudy. And it is certainly colder than in South 
Africa!

> Thanks for the very helpful reply! I will try it right now! I need to
> present at a conference in early April and would like to get these
> results before that!
> Just three quick questions before you go to enjoy the Easter weekend:
>
> - how do I get the values out of 'jj' (## jj contains the values of
> elevation inside the home range of the animals) to calculate mean,
> variance, etc, across all the points in 'jj'? I assume 'jj' is of class
> 'kasc'. In my case it might be 'asc' as I will have only a single home
> range at the time (one animal).

Just a small correction to my previous e-mail. If you creates jj with:

jj<-lapply(foo, function(x) x*el)

jj is not a map of class "kasc". To convert jj to the class "kasc", copy the
attributes of the map "foo" to jj with getkascattr:

jj<-getkascattr(foo, jj)
image(jj)

And now, jj is map of class "kasc", i.e. a data.frame with each column
corresponding to a map. All the pixels located inside the home ranges
are coded by numeric values in the data frame, and pixels located outside
the home ranges are coded by NA. So you can get the pixels values with
something like:

oo<-lapply(jj, function(x) x[!is.na(x)])

oo is a list with one component per animal. Each component corresponds to a
vector of pixels inside the animals home range. It is quite easy after 
that to get
values such as average, mean, median, etc.:

lapply(oo, summary)
lapply(oo, mean)
lapply(oo, sd)
etc.

> - can I get the points out of the 'kasc' object in a way that I can
> still run spatial statistics on them? I would like to calculate the
> nugget and range for the NDVI values within the home range.


I never performed such spatial statistics with home ranges but this
could certainly be done... IMHO, the best way to achieve is also to get
the coordinates of the kept pixels when computing the vector oo:

xyc<-getXYcoords(jj)
## xyc contains the coordinates of the pixels of the maps
## it is a list with two components, the vectors of coordinates
## of the pixels of the maps
## compute the x values
xc <- rep(xyc$x, times=length(xyc$y))
yc <- rep(xyc$y, each=length(xyc$x))
xyc<-data.frame(x=xc,y=yc)

lixyc<-lapply(1:ncol(jj), function(x) xyc[!is.na(jj[,x]),])

## So that each point in oo is associated to its coordinates
## in lixyc
## for example
plot(lixyc[[1]], col=grey(oo[[1]]/300), pch=15, cex=4, asp=1)
plot(lixyc[[2]], col=grey(oo[[2]]/300), pch=15, cex=4, asp=1)
plot(lixyc[[3]], col=grey(oo[[3]]/300), pch=15, cex=4, asp=1)

Then you have to compute a variogram. I never used functions
allowing such analyses, but it seems that the package nlme has a function
Variogram...

> - do you know how to convert a polygon shapefile into the same class as
> produced by the command kernelUD? Alternatively I could import a raster
> version of the polygon shape file, convert it to the correct format, and
> repeat the spatial join method you are suggesting for the home ranges!

I'm sorry, I do not have any easy solution to this problem. The function 
kernelUD
returns a *raster* map of class "asc", a central class in adehabitat. I 
don't know how
to rasterize a shapefile in R (the library shapefiles or Maptools, which 
deal with
shapefiles do not allow such operations). Maybe the simpler way would be to
rasterize the shapefile with a GIS...
Hope this helps,


Clem.

>
> Thanks again,
>
> Sander.
>
>
> Cl?ment Calenge wrote:
>
>> Dear Sander,
>>
>> Sorry for my late answer, I was in the field.
>>
>>> I send this email to you directly, because it concerns the link 
>>> between Adehabitat and Splancs.
>>>
>>> I have two sources of polygons:
>>> - polygon shape files with areas of interest;
>>> - polygons calculated using the home range methods in Adehabitat.
>>> For the moment, each polygon source will only contain a single polygon.
>>> I have one source of points: a grid representing NDVI values. I read 
>>> the grid using the import.asc function in Adehabitat.
>>> I would like to use both polygon sources to select the points in the 
>>> NDVI raster which are inside the relevant polygons. Then I want to 
>>> run several statistics (mean, standard deviation, etc) on the NDVI 
>>> values of these points.
>>
>>
>> The solution depends on what you call "polygon". The function 
>> mcp.rast() is intended to "rasterize"
>> a polygon on a raster grid of class "asc" or "kasc". The polygon is 
>> here stored as a data frame with
>> two columns (the X and Y coordinates of the polygon). The function 
>> hr.rast() can be used if you have
>> an object of class "area", i.e. a data.frame with three columns (the 
>> X and Y coordinates of the polygon,
>> and a third column defining the ID of the polygon).
>>
>> Now, it seems that  you work with more complex type of polygons, e.g. 
>> kernel estimations of home ranges
>> (where each animal has an home range, and each home range can be made 
>> of several polygons). In such cases,
>> the "Spatial join" operation can be more complex. I detail below an 
>> example of "spatial join" with a kernel
>> estimation of wild boar home ranges. Note, that in this case, you do 
>> not have to compute the contour of
>> the estimation. Just copy and paste the lines below on R:
>>
>> ## loads and display the data
>> library(adehabitat)
>> data(puechabon)
>>
>> ## keep only the elevation
>> el<-getkasc(puechabon$kasc,1)
>> image(el)
>>
>> ## estimate a kernel home-range
>> kud<-kernelUD(puechabon$locs[,c("X","Y")], puechabon$locs$Name, grid=el)
>> ## kud contains a density
>> ## note that the grid containing the values of the elevation is used 
>> for the estimation
>>
>> vud<-getvolumeUD(kud)
>> image(vud)
>> ## vud contains the limits of the home ranges estimated at different 
>> levels
>>
>> toto<-lapply(vud, function(x) x$UD)
>> ## toto is the list of maps of Utilization Distribution
>> tutu<-lapply(toto, function(x) {x[x<95]<-1; x[x>95]<-NA; return(x)})
>> ## tutu is a list of raster maps containing 1 if the pixel is inside
>> ## the 95% home range and NA otherwise
>>
>> foo<-as.kasc(tutu)
>> image(foo)
>> ## foo is a kasc object
>>
>> jj=lapply(foo, function(x) x*el)
>> image(jj)
>> ## jj contains the values of elevation inside the home range of the 
>> animals
>>
>> The last method that is available for home-range estimation of 
>> animals home ranges
>> is the Nearest Neighbor Convex hull method of Getz and Wilmers 
>> (2004). The function
>> NNCH.rast() can be used to "rasterize" the home range estimation in 
>> the same way, and then
>> you can multiply the resulting maps with your NDVI in the same way.
>> Hope this helps,
>>
>>
>> Cl?ment.
>>
>


-- 
Cl?ment CALENGE
LBBE - UMR CNRS 5558 - Universit? 
Claude Bernard Lyon 1 - FRANCE
tel. (+33) 04.72.43.27.57
fax. (+33) 04.72.43.13.88



From Roger.Bivand at nhh.no  Fri Mar 25 20:58:44 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 25 Mar 2005 20:58:44 +0100 (CET)
Subject: [R-sig-Geo] Re: Using adehabitat and splancs to achieve
 point-in-polygon analysis on raster?
In-Reply-To: <42444675.7050408@biomserv.univ-lyon1.fr>
Message-ID: <Pine.LNX.4.44.0503252047520.26451-100000@reclus.nhh.no>

On Fri, 25 Mar 2005, Cl?ment Calenge wrote:

> 
> >
> > Hope you had a good time in the field! Is it spring in France?
> 
> Yes, though a little cloudy. And it is certainly colder than in South 
> Africa!

Probably about the same in Bergen, quite warm for the time of year, snow 
still on the hills around town.

> 
> > Thanks for the very helpful reply! I will try it right now! I need to
> > present at a conference in early April and would like to get these
> > results before that!
> > Just three quick questions before you go to enjoy the Easter weekend:
> >
> > - how do I get the values out of 'jj' (## jj contains the values of
> > elevation inside the home range of the animals) to calculate mean,
> > variance, etc, across all the points in 'jj'? I assume 'jj' is of class
> > 'kasc'. In my case it might be 'asc' as I will have only a single home
> > range at the time (one animal).
> 
> Just a small correction to my previous e-mail. If you creates jj with:
> 
> jj<-lapply(foo, function(x) x*el)
> 
> jj is not a map of class "kasc". To convert jj to the class "kasc", copy the
> attributes of the map "foo" to jj with getkascattr:
> 
> jj<-getkascattr(foo, jj)
> image(jj)
> 
> And now, jj is map of class "kasc", i.e. a data.frame with each column
> corresponding to a map. All the pixels located inside the home ranges
> are coded by numeric values in the data frame, and pixels located outside
> the home ranges are coded by NA. So you can get the pixels values with
> something like:
> 
> oo<-lapply(jj, function(x) x[!is.na(x)])
> 
> oo is a list with one component per animal. Each component corresponds to a
> vector of pixels inside the animals home range. It is quite easy after 
> that to get
> values such as average, mean, median, etc.:
> 
> lapply(oo, summary)
> lapply(oo, mean)
> lapply(oo, sd)
> etc.
> 
> > - can I get the points out of the 'kasc' object in a way that I can
> > still run spatial statistics on them? I would like to calculate the
> > nugget and range for the NDVI values within the home range.
> 
> 
> I never performed such spatial statistics with home ranges but this
> could certainly be done... IMHO, the best way to achieve is also to get
> the coordinates of the kept pixels when computing the vector oo:
> 
> xyc<-getXYcoords(jj)
> ## xyc contains the coordinates of the pixels of the maps
> ## it is a list with two components, the vectors of coordinates
> ## of the pixels of the maps
> ## compute the x values
> xc <- rep(xyc$x, times=length(xyc$y))
> yc <- rep(xyc$y, each=length(xyc$x))
> xyc<-data.frame(x=xc,y=yc)
> 
> lixyc<-lapply(1:ncol(jj), function(x) xyc[!is.na(jj[,x]),])
> 
> ## So that each point in oo is associated to its coordinates
> ## in lixyc
> ## for example
> plot(lixyc[[1]], col=grey(oo[[1]]/300), pch=15, cex=4, asp=1)
> plot(lixyc[[2]], col=grey(oo[[2]]/300), pch=15, cex=4, asp=1)
> plot(lixyc[[3]], col=grey(oo[[3]]/300), pch=15, cex=4, asp=1)
> 
> Then you have to compute a variogram. I never used functions
> allowing such analyses, but it seems that the package nlme has a function
> Variogram...
> 
> > - do you know how to convert a polygon shapefile into the same class as
> > produced by the command kernelUD? Alternatively I could import a raster
> > version of the polygon shape file, convert it to the correct format, and
> > repeat the spatial join method you are suggesting for the home ranges!
> 
> I'm sorry, I do not have any easy solution to this problem. The function 
> kernelUD
> returns a *raster* map of class "asc", a central class in adehabitat. I 
> don't know how
> to rasterize a shapefile in R (the library shapefiles or Maptools, which 
> deal with
> shapefiles do not allow such operations). Maybe the simpler way would be to
> rasterize the shapefile with a GIS...

This is something we've been looking at for the sp package, how to make an 
object that represents the raster cells falling into one or more rings. It 
could be a grid with NA outside the vector MASK, or a cell object with 
just the (regularly spaced) points inside the MASK. We are not there yet, 
and point-in-polygon is as close as we've got yet (that is, generate the 
full raster using expand.grid() and test each point in turn. This is 
needed, but isn't finished yet. Writing good functions to suit adehabitat 
package objects is going to be important, so the work flow you, Sander, 
have described, is very useful. It would be nice to be able to say that 
this is ready now, but it isn't yet ...

At least one of the things sp is trying to do is provide shared object 
structures so that writing functions from/to different polygon 
representations can be done from -> sp and sp -> to, so that various 
packages should be able to "talk to" each other with fewer barriers.

Best wishes,

Roger


> Hope this helps,
> 
> 
> Clem.
> 
> >
> > Thanks again,
> >
> > Sander.
> >
> >
> > Cl?ment Calenge wrote:
> >
> >> Dear Sander,
> >>
> >> Sorry for my late answer, I was in the field.
> >>
> >>> I send this email to you directly, because it concerns the link 
> >>> between Adehabitat and Splancs.
> >>>
> >>> I have two sources of polygons:
> >>> - polygon shape files with areas of interest;
> >>> - polygons calculated using the home range methods in Adehabitat.
> >>> For the moment, each polygon source will only contain a single polygon.
> >>> I have one source of points: a grid representing NDVI values. I read 
> >>> the grid using the import.asc function in Adehabitat.
> >>> I would like to use both polygon sources to select the points in the 
> >>> NDVI raster which are inside the relevant polygons. Then I want to 
> >>> run several statistics (mean, standard deviation, etc) on the NDVI 
> >>> values of these points.
> >>
> >>
> >> The solution depends on what you call "polygon". The function 
> >> mcp.rast() is intended to "rasterize"
> >> a polygon on a raster grid of class "asc" or "kasc". The polygon is 
> >> here stored as a data frame with
> >> two columns (the X and Y coordinates of the polygon). The function 
> >> hr.rast() can be used if you have
> >> an object of class "area", i.e. a data.frame with three columns (the 
> >> X and Y coordinates of the polygon,
> >> and a third column defining the ID of the polygon).
> >>
> >> Now, it seems that  you work with more complex type of polygons, e.g. 
> >> kernel estimations of home ranges
> >> (where each animal has an home range, and each home range can be made 
> >> of several polygons). In such cases,
> >> the "Spatial join" operation can be more complex. I detail below an 
> >> example of "spatial join" with a kernel
> >> estimation of wild boar home ranges. Note, that in this case, you do 
> >> not have to compute the contour of
> >> the estimation. Just copy and paste the lines below on R:
> >>
> >> ## loads and display the data
> >> library(adehabitat)
> >> data(puechabon)
> >>
> >> ## keep only the elevation
> >> el<-getkasc(puechabon$kasc,1)
> >> image(el)
> >>
> >> ## estimate a kernel home-range
> >> kud<-kernelUD(puechabon$locs[,c("X","Y")], puechabon$locs$Name, grid=el)
> >> ## kud contains a density
> >> ## note that the grid containing the values of the elevation is used 
> >> for the estimation
> >>
> >> vud<-getvolumeUD(kud)
> >> image(vud)
> >> ## vud contains the limits of the home ranges estimated at different 
> >> levels
> >>
> >> toto<-lapply(vud, function(x) x$UD)
> >> ## toto is the list of maps of Utilization Distribution
> >> tutu<-lapply(toto, function(x) {x[x<95]<-1; x[x>95]<-NA; return(x)})
> >> ## tutu is a list of raster maps containing 1 if the pixel is inside
> >> ## the 95% home range and NA otherwise
> >>
> >> foo<-as.kasc(tutu)
> >> image(foo)
> >> ## foo is a kasc object
> >>
> >> jj=lapply(foo, function(x) x*el)
> >> image(jj)
> >> ## jj contains the values of elevation inside the home range of the 
> >> animals
> >>
> >> The last method that is available for home-range estimation of 
> >> animals home ranges
> >> is the Nearest Neighbor Convex hull method of Getz and Wilmers 
> >> (2004). The function
> >> NNCH.rast() can be used to "rasterize" the home range estimation in 
> >> the same way, and then
> >> you can multiply the resulting maps with your NDVI in the same way.
> >> Hope this helps,
> >>
> >>
> >> Cl?ment.
> >>
> >
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From slist at oomvanlieshout.net  Sat Mar 26 21:58:35 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Sat, 26 Mar 2005 22:58:35 +0200
Subject: [R-sig-Geo] A few more questions about Adehabitat!?
Message-ID: <4245CCFB.9050700@oomvanlieshout.net>

Dear Clement,

I have made enormous progress, but I am still struggling with a couple 
of things, hence the following questions:

- How can I determine the area of the polygons in an object of class
'asc'? The 'ararea' function only works on objects of class 'area', and
I can not convert 'asc' to 'area'. I would prefer a list of areas for 
each polygon, which I can than total to the whole home range area.

- Would it be possible to create a function to extract the extent (x and
y limits) of an 'asc' or 'kasc' object, such that I can use the result
as an argument in the 'subsetmap' function to set xl and yl?

- Mean('asc object') returns a single value, as I was expecting, but
var('asc object') returns a matrix and sd('asc object') returns a
vector. I must be missing something! Also mean() gives NA if part of the 
grid is NA. See example below!

- Is it possible to do calculations with an 'asc' object? Such as 
multiplying each cell with a value? I read a grid from file and would 
like to change the cell values. I would also like to change certain cell 
values to NA,before doing the calculations as above. The same would be 
possible if I could get the map out of the 'asc' object and put it in a 
matrix of data frame.

Thanks in advance,

Sander.

Sample code for reading in an ascii grid:

 > (file1 <-  paste(system.file(package = "adehabitat"),
+                "ascfiles/elevation.asc", sep = "/"))
[1] "/usr/lib/R/library/adehabitat/ascfiles/elevation.asc"
 > xl <- c(701000, 702000)
 > yl <- c(3160000, 3161000)
 >
 > el <- import.asc(file1)
 > image(el)
 > el
Raster map of class "asc":
Cell size:  100
Number of rows:  121
Number of columns:  111
Type:  numeric
 > mean(el)
[1] NA
 > elsub<-subsetmap(el, xlim = xl, ylim = yl)
 > image(elsub)
 > mean(elsub)
[1] 265.281
 > rr <- lapply(el, function(x) x*1000)
 > mean(rr)
[1] NA
Warning message:
argument is not numeric or logical: returning NA in: mean.default(rr)
 > var(elsub)
             [,1]  [,2]      [,3]       [,4]       [,5]        [,6] 
  [,7]
  [1,] 456.818182 397.1 345.58182 304.172727 298.054545 179.9727273 
170.61818
  [2,] 397.100000 348.6 304.80000 269.300000 262.400000 158.6000000 
146.90000
  [3,] 345.581818 304.8 270.01818 241.027273 235.945455 140.8272727 
132.18182
  [4,] 304.172727 269.3 241.02727 224.490909 219.818182 127.3909091 
118.17273
  [5,] 298.054545 262.4 235.94545 219.818182 217.963636 125.4181818 
117.65455
  [6,] 179.972727 158.6 140.82727 127.390909 125.418182  80.6909091 
71.17273
  [7,] 170.618182 146.9 132.18182 118.172727 117.654545  71.1727273 
80.01818
  [8,] 110.600000  93.6  84.10000  70.400000  69.400000  45.8000000 
61.70000
  [9,]  38.972727  26.8  23.52727   7.490909   7.318182  16.7909091 
49.07273
[10,]  -8.809091 -14.1 -10.89091 -26.536364 -26.027273  -0.6363636  38.39091
[11,] -70.254545 -73.4 -66.44545 -81.718182 -79.963636 -30.0181818  18.94545
        [,8]       [,9]       [,10]     [,11]
  [1,] 110.6  38.972727  -8.8090909 -70.25455
  [2,]  93.6  26.800000 -14.1000000 -73.40000
  [3,]  84.1  23.527273 -10.8909091 -66.44545
  [4,]  70.4   7.490909 -26.5363636 -81.71818
  [5,]  69.4   7.318182 -26.0272727 -79.96364
  [6,]  45.8  16.790909  -0.6363636 -30.01818
  [7,]  61.7  49.072727  38.3909091  18.94545
  [8,]  59.8  73.000000  75.7000000  74.10000
  [9,]  73.0 136.290909 161.5636364 191.18182
[10,]  75.7 161.563636 201.8545455 242.62727
[11,]  74.1 191.181818 242.6272727 315.56364
 >
-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From basille at biomserv.univ-lyon1.fr  Sun Mar 27 15:17:00 2005
From: basille at biomserv.univ-lyon1.fr (Mathieu Basille)
Date: Sun, 27 Mar 2005 15:17:00 +0200
Subject: [R-sig-Geo] Some comments on calculations with 'asc' (and 'kasc')
 objects with Adehabitat
Message-ID: <4246B24C.7030102@biomserv.univ-lyon1.fr>

Dear Sander,

Calculations with objects of class 'asc' and 'kasc' are quite simple, as 
soon as you know that 'asc' ARE actually matrices and 'kasc' data 
frames. The difference is just that they have a few additional 
parameters ('xll', 'yll', 'cellsize' and 'type' - eventually 'levels' - 
for both and 'nrow' and 'ncol' for kasc). Note that the first value of 
the matrix (top-left corner) is actually the bottom-left corner of the 
map (clockwise rotation of 90? between the map and the matrix). Note 
also that each column of a 'kasc' represent one map (one variable). An 
illustration :

   data(puechabon)
   kasc <- puechabon$kasc
   asc <- getkasc(kasc, "Elevation")
   is.matrix(asc)
   # [1] TRUE
   is.data.frame(kasc)
   # [1] TRUE

Then, the calculations are exactly the same as for matrices or data 
frames. An exemple with a simple matrix :

   m <- matrix(1:20, nr = 5)
   mean(m)
   # [1] 10.5
   var(m)
   #      [,1] [,2] [,3] [,4]
   # [1,]  2.5  2.5  2.5  2.5
   # [2,]  2.5  2.5  2.5  2.5
   # [3,]  2.5  2.5  2.5  2.5
   # [4,]  2.5  2.5  2.5  2.5

The 'var' function applied to a matrix gives the variance-covariance 
matrix. To avoid that, just type :

   var(as.vector(m))
   # [1] 35

   sd(m)
   # [1] 1.581139 1.581139 1.581139 1.581139

The 'sd' function applied to a matrix returns a vector of the standard 
deviation of the columns. To avoid that :

   sd(as.vector(m))
   # [1] 5.91608


The same with a simple data frame :

   df <- cbind(a = 1:20, b = runif(20)*100, c = rnorm(20, 50, 10))
   mean(df)
   # [1] 38.64825

The 'mean' function gives the overall mean of the data frame. As for 
kasc we obviously need means for the columns, we can try :

   colMeans(df)
   #        a        b        c
   # 10.50000 56.74289 48.70187

   colSums(df)
   #        a         b         c
   # 210.0000 1134.8577  974.0374

For the variance or standard deviation, use the 'apply' function :

   apply(df, 2, var)
   #        a         b         c
   # 35.00000 748.78361  68.40536

   apply(df, 2, sd)
   #        a         b         c
   # 5.916080 27.363911  8.270753


Now, the same, but applied to a 'real' asc object.

   mean(asc)
   # [1] NA
   var(as.vector(asc))
   # Error in var(as.vector(asc)) : missing observations in cov/cor
   sd(as.vector(asc))
   # Error in var(x, na.rm = na.rm) : missing observations in cov/cor

For all these functions, you get something wrong because of the NAs. Use 
then the parameter na.rm = TRUE :

   mean(asc, na.rm = TRUE)
   # [1] 240.6568
   var(as.vector(asc), na.rm = TRUE)
   # [1] 7369.139
   sd(as.vector(asc), na.rm = TRUE)
   # [1] 85.84369

And with a kasc object :

   colSums(kasc)
   # Error in colSums(x, n, prod(dn), na.rm) : `x' must be numeric

You get an error because the variable 'Aspect' is a factor. We don't 
need to compute means and others statistics to this variable.

   colSums(kasc[-2])
   #  Elevation      Slope Herbaceous
   #         NA         NA         NA

Again, the same problem with the NAs, solved with the na.rm parameter :

   colSums(kasc[-2], na.rm = TRUE)
   #   Elevation       Slope  Herbaceous
   # 1053836.000   40881.457    1905.650

   colMeans(kasc[-2], na.rm = TRUE)
   #   Elevation       Slope  Herbaceous
   # 240.6567710   9.3357975   0.4351792

   apply(kasc[-2], 2, var, na.rm = TRUE)
   #    Elevation        Slope   Herbaceous
   # 7.369139e+03 6.208515e+01 5.703959e-02

   apply(kasc[-2], 2, sd, na.rm = TRUE)
   #  Elevation      Slope Herbaceous
   # 85.8436875  7.8794130  0.2388296


Finally, check the usefull functions 'summary' as applied to asc and kasc :

   summary(as.vector(asc))
   #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
   #    65.0   176.0   256.0   240.7   290.0   479.0  9052.0

   summary(kasc)
   #    Elevation            Aspect         Slope            
Herbaceous         #  Min.   :  65.0   NorthEast: 537   Min.   :   
0.000   Min.   :   0.0000    #  1st Qu.: 176.0   SouthEast:1504   1st 
Qu.:   3.124   1st Qu.:   0.2000    #  Median : 256.0   SouthWest:1262   
Median :   6.658   Median :   0.4222    #  Mean   : 240.7   
NorthWest:1076   Mean   :   9.336   Mean   :   0.4352    #  3rd Qu.: 
290.0   NA's     :9052   3rd Qu.:  13.710   3rd Qu.:   0.7000    #  
Max.   : 479.0                    Max.   :  40.631   Max.   :   0.7000 
   #  NA's   :9052.0                    NA's   :9052.000   NA's   
:9052.0000

I'll let Cl?ment answer about your two first questions, that are far 
from my skills.

HTH,
Mathieu



From calenge at biomserv.univ-lyon1.fr  Sun Mar 27 17:31:20 2005
From: calenge at biomserv.univ-lyon1.fr (=?ISO-8859-1?Q?Cl=E9ment_Calenge?=)
Date: Sun, 27 Mar 2005 17:31:20 +0200
Subject: [R-sig-Geo] Re: A few more questions about Adehabitat!?
In-Reply-To: <4245CCFB.9050700@oomvanlieshout.net>
References: <4245CCFB.9050700@oomvanlieshout.net>
Message-ID: <4246D1C8.2080507@biomserv.univ-lyon1.fr>

Dear Sander,


Sander Oom a ?crit :

> - How can I determine the area of the polygons in an object of class
> 'asc'? The 'ararea' function only works on objects of class 'area', and
> I can not convert 'asc' to 'area'. I would prefer a list of areas for 
> each polygon, which I can than total to the whole home range area.

Let's take an example, just copy and paste the following
code under R:

library(adehabitat)
data(puechabon)
kud<-getvolumeUD(kernelUD(puechabon$locs[,c("X","Y")],
                               h=250))[[1]]$UD
kud[kud<95]<-1
kud[kud>95]<-NA
image(kud)

kud is an object of class "asc", containing 1's for pixels
inside the home range (here, all animals are pooled) and
NA otherwise. If you want to know the whole area of
the home range:

 > table(kud)
kud
  1
672
 > attr(kud, "cellsize")
[1] 95.75
 > table(kud)*(attr(kud, "cellsize")^2)/10000
kud
       1
616.0938

672 pixels have the value 1 in kud. Each pixel covers an
area of 95.75 * 95.75 m?.
The total area is therefore of 616.09 ha.
Now if you want to know the area covered by each
connected component, you can use the function labcon(),
which gives a label to each connected component:

 > kudbis <- labcon(kud)
 > image(kudbis, clfac = c("red","green"))
 > table(kudbis)*(attr(kud, "cellsize")^2)/10000
kudbis
       1        2
296.1284 319.9654

The red connected component covers 296.13 ha,
and the green connected component, 319.97 ha.

> - Would it be possible to create a function to extract the extent (x and
> y limits) of an 'asc' or 'kasc' object, such that I can use the result
> as an argument in the 'subsetmap' function to set xl and yl?

Something like that ?

foo<-function(x)
{
toto<-getXYcoords(x)
xx<-range(toto$x[apply(x, 1, function(y) !all(is.na(y)))])
yy<-range(toto$y[apply(x, 2, function(y) !all(is.na(y)))])
return(list(x=xx, y=yy))
}

Using the above example:

image(kud)
toto<-foo(kud)
kudter<-subsetmap(kud, toto$x, toto$y)
image(kudter)


Hope this helps,

Clem.

>
> - Mean('asc object') returns a single value, as I was expecting, but
> var('asc object') returns a matrix and sd('asc object') returns a
> vector. I must be missing something! Also mean() gives NA if part of 
> the grid is NA. See example below!
>
> - Is it possible to do calculations with an 'asc' object? Such as 
> multiplying each cell with a value? I read a grid from file and would 
> like to change the cell values. I would also like to change certain 
> cell values to NA,before doing the calculations as above. The same 
> would be possible if I could get the map out of the 'asc' object and 
> put it in a matrix of data frame.
>
> Thanks in advance,
>
> Sander.
>
> Sample code for reading in an ascii grid:
>
> > (file1 <-  paste(system.file(package = "adehabitat"),
> +                "ascfiles/elevation.asc", sep = "/"))
> [1] "/usr/lib/R/library/adehabitat/ascfiles/elevation.asc"
> > xl <- c(701000, 702000)
> > yl <- c(3160000, 3161000)
> >
> > el <- import.asc(file1)
> > image(el)
> > el
> Raster map of class "asc":
> Cell size:  100
> Number of rows:  121
> Number of columns:  111
> Type:  numeric
> > mean(el)
> [1] NA
> > elsub<-subsetmap(el, xlim = xl, ylim = yl)
> > image(elsub)
> > mean(elsub)
> [1] 265.281
> > rr <- lapply(el, function(x) x*1000)
> > mean(rr)
> [1] NA
> Warning message:
> argument is not numeric or logical: returning NA in: mean.default(rr)
> > var(elsub)
>             [,1]  [,2]      [,3]       [,4]       [,5]        [,6]  [,7]
>  [1,] 456.818182 397.1 345.58182 304.172727 298.054545 179.9727273 
> 170.61818
>  [2,] 397.100000 348.6 304.80000 269.300000 262.400000 158.6000000 
> 146.90000
>  [3,] 345.581818 304.8 270.01818 241.027273 235.945455 140.8272727 
> 132.18182
>  [4,] 304.172727 269.3 241.02727 224.490909 219.818182 127.3909091 
> 118.17273
>  [5,] 298.054545 262.4 235.94545 219.818182 217.963636 125.4181818 
> 117.65455
>  [6,] 179.972727 158.6 140.82727 127.390909 125.418182  80.6909091 
> 71.17273
>  [7,] 170.618182 146.9 132.18182 118.172727 117.654545  71.1727273 
> 80.01818
>  [8,] 110.600000  93.6  84.10000  70.400000  69.400000  45.8000000 
> 61.70000
>  [9,]  38.972727  26.8  23.52727   7.490909   7.318182  16.7909091 
> 49.07273
> [10,]  -8.809091 -14.1 -10.89091 -26.536364 -26.027273  -0.6363636  
> 38.39091
> [11,] -70.254545 -73.4 -66.44545 -81.718182 -79.963636 -30.0181818  
> 18.94545
>        [,8]       [,9]       [,10]     [,11]
>  [1,] 110.6  38.972727  -8.8090909 -70.25455
>  [2,]  93.6  26.800000 -14.1000000 -73.40000
>  [3,]  84.1  23.527273 -10.8909091 -66.44545
>  [4,]  70.4   7.490909 -26.5363636 -81.71818
>  [5,]  69.4   7.318182 -26.0272727 -79.96364
>  [6,]  45.8  16.790909  -0.6363636 -30.01818
>  [7,]  61.7  49.072727  38.3909091  18.94545
>  [8,]  59.8  73.000000  75.7000000  74.10000
>  [9,]  73.0 136.290909 161.5636364 191.18182
> [10,]  75.7 161.563636 201.8545455 242.62727
> [11,]  74.1 191.181818 242.6272727 315.56364
> >



-- 
Cl?ment CALENGE
LBBE - UMR CNRS 5558 - Universit? 
Claude Bernard Lyon 1 - FRANCE
tel. (+33) 04.72.43.27.57
fax. (+33) 04.72.43.13.88



From jakob.petersen at qmul.ac.uk  Mon Mar 28 16:42:18 2005
From: jakob.petersen at qmul.ac.uk (Jakob Petersen)
Date: Mon, 28 Mar 2005 15:42:18 +0100
Subject: [R-sig-Geo] DCluster
Message-ID: <424817CA.4050102@qmul.ac.uk>

Hi r-sig-geo,
I am looking at the spatial distribution of poor households in a region 
comprising a gradient of urban-rural postcodes. The data are counts and 
they fit a negative binomial distribution, rather than a poisson 
distribution.

I am applying DCluster (ver. 0.1-3, windows) and would be grateful for 
advice on a few topics.

1. GAM

A) As I understand it the default setting is based on a poisson 
distribution. This creates some not implausible clusters, but I wonder 
whether I could set the opgam, so that it uses a negative binomial 
distribution (for which I have the parameters for the ?disease? 
variable; size and mu) or to use a bootstrap procedure instead. Some of 
the internal functions, like opgam.iscluster.negbin, seem to support 
this, but I am uncertain about how to incorporate them.

B) To reduce the multiple testing problem (Waller & Gotway 2004, 
?Applied Spatial Statistics for Public Health Data?, Wiley, p.208) I 
wonder whether to set radius to <50% of step size, e.g. 100m radius in a 
300m grid, so that the smallest circles won't touch?

2. Besag-Newell

I am getting results with ?poisson? (almost everything becomes a cluster 
- possibly because the sites are clumped and not randomly distributed) 
and with ?permutation?, but wonders how the ?negbin? is used? Not like 
this:

>  bnresults<-opgam(pcpoor, thegrid=pcpoor[,c("x","y")], alpha=.05,

+ iscluster=bn.iscluster, set.idxorder=TRUE, k=20, model="negbin",

+ R=100, mle=calculate.mle(pcpoor) )

> > Error in rnbinom(n, size, prob) : invalid arguments

3. Kulldorff & Nagarwalla

Again I struggle with the parameters. Not like this:

>  #K&N's method over the centroids

>  mle<-calculate.mle(pcpoor, model="negbin")

> > Error in while (((abs(m - m0) > tol * (m + m0)) || (abs(v - v0) > tol 
* :

missing value where TRUE/FALSE needed

>  knresults<-opgam(data=pcpoor, thegrid=pcpoor[,c("x","y")], alpha=.05,

+ iscluster=kn.iscluster, fractpop=.5, R=100, model="negbin", mle=mle)

> > Error in rnbinom(n, size, prob) : invalid arguments

4. Turnbull. Is Turnbull analysis possible in DCluster yet?. Some 
references in the manual, but haven?t been able to locate it.

5. General

A) I am considering increasing the study area (p.t. working with 1262 
postcode points) and wonder what the limits might be for a desktop pc. I 
gather that the distance matrices (created by tripack or spdep) could be 
a limiting factor? Would it be an idea to run this step first and once 
the table is created run the cluster detection algorithm?

B) I wonder whether permutations always are superior to standard stats. 
Distributions, and if not, then why not?


Best wishes, Jakob

Jakob Petersen
GISc student (MSc)
Birkbeck, University of London



From Chloe.ARCHINARD at cefe.cnrs.fr  Tue Mar 29 17:13:52 2005
From: Chloe.ARCHINARD at cefe.cnrs.fr (Chloe ARCHINARD)
Date: Tue, 29 Mar 2005 17:13:52 +0200
Subject: [R-sig-Geo] Problem for plotting Moran's I estimates
Message-ID: <EB09E5B9F0E2684F863B525E5C7E0F890B36DB@ZZML.newcefe.newage.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20050329/1d7eba7e/attachment.pl>

From Roger.Bivand at nhh.no  Tue Mar 29 19:37:20 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 29 Mar 2005 19:37:20 +0200 (CEST)
Subject: [R-sig-Geo] Problem for plotting Moran's I estimates
In-Reply-To: <EB09E5B9F0E2684F863B525E5C7E0F890B36DB@ZZML.newcefe.newage.fr>
Message-ID: <Pine.LNX.4.44.0503291925110.7846-100000@reclus.nhh.no>

On Tue, 29 Mar 2005, Chloe ARCHINARD wrote:

> Hello all, I'm a new user of R and I have used the spdep package to
> calculate Moran's I estimates. I used the dnearneigh() function and
> moran.test() function and obtained what I wanted, but now I want to plot
> my results with the p-values. I regrouped my results for the Moran's I
> like
> >m=c(m1$e,m2$e...)        #m1 contain the results of the moran.test for lag1
> >p=c(m1$p.v,m2$p.v...)
> The problem is that in mx$e there are three values (moran'I statistic,
> expectation and variance) and when I plot my results I plot all the
> three values for each lag. What should I do to choose only the statistic
> for plotting and maybe the variance for each I statistic? (For the
> p-value it's ok I have found how to differentiate the significant and
> non significant values.) I hope someone understand my question and could
> help me! Thanks a lot.

If you have the output object from moran.test()

data(oldcol)
res <- moran.test(COL.OLD$CRIME, nb2listw(COL.nb))
str(res) # shows the structure of the object
est <- res$estimate[1]
pv <- res$p.value

I suggest you make a list of test result objects for your distance lags, 
and then use sapply(mylist, function(x) c(x$estimate[1], x$p.value)) to 
extract the values (you may need t() around the result to transpose the 
output). I'll try to add an example like this to moran.test() - any 
suggestions of a well-known suitable data set with known Moran's I values 
and distance bands?

Roger

> 
> Chlo? ARCHINARD
> Centre d'Ecologie Fonctionnelle et Evolutive
> (C.N.R.S.-U.M.R. 5175)
> 1919, Route de Mende
> 34293 Montpellier Cedex 5 France
> chloe.archinard at cefe.cnrs.fr
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Chloe.ARCHINARD at cefe.cnrs.fr  Wed Mar 30 17:31:35 2005
From: Chloe.ARCHINARD at cefe.cnrs.fr (Chloe ARCHINARD)
Date: Wed, 30 Mar 2005 17:31:35 +0200
Subject: [R-sig-Geo] Problem for plotting Moran's I estimates
Message-ID: <EB09E5B9F0E2684F863B525E5C7E0F890B36E1@ZZML.newcefe.newage.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20050330/675a4abf/attachment.pl>

From reeves at nceas.ucsb.edu  Wed Mar 30 21:42:17 2005
From: reeves at nceas.ucsb.edu (Rick Reeves)
Date: Wed, 30 Mar 2005 11:42:17 -0800
Subject: [R-sig-Geo] Reading and projecting lat/long point coordinates using
	R?
Message-ID: <000001c53560$9484c970$8bdc6f80@BURAN>



Hello: 

I've searched the literature, havent found an answer, so here goes? 

We have a set of polygon features defined as a series of unprojected
vertices represented by lat/long pairs, in an ASCII-format file.

We need to display (on a map-like plot) and then calculate the areas 
of the polygons, and would like to do this entirely within R if
possible.
Is there a map projection package for R? Or, has anyone solved this
or a related problem using R? I could do this with the MATLAB mapping
toolbox, but prefer to use R if possible....


Thanks, 

Rick Reeves

Scientific Programmer / Quantitative Analyst
National Center for Ecological Analysis and Synthesis
University of California, Santa Barbara
805 892 2534
reeves at nceas.ucsb.edu



From Virgilio.Gomez at uv.es  Thu Mar 31 12:56:18 2005
From: Virgilio.Gomez at uv.es (Virgilio.Gomez at uv.es)
Date: Thu, 31 Mar 2005 12:56:18 +0200 (CEST)
Subject: [R-sig-Geo] Re: DCluster
In-Reply-To: <200503291017.j2TAHggk006198@hypatia.math.ethz.ch>
References: <200503291017.j2TAHggk006198@hypatia.math.ethz.ch>
Message-ID: <7441127199virgil@uv.es>

Hi all,

I am out of my office until next week. For this readon, I will not be 
able to check the code in DCluster and answer your other question, 
Jakob. Don't desperate!! :)

The only advice I can give yiou a the moment is to try to follow as 
much as possible the code in the examples, although I am quite sure 
that you have already done it. I remember some problems like those 
that you described but I think I fixed them (or maybe not) for the 
current release.

Best regards,

Virgilio



From slist at oomvanlieshout.net  Thu Mar 31 13:32:31 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Thu, 31 Mar 2005 13:32:31 +0200
Subject: [R-sig-Geo] Reading and projecting lat/long point coordinates
	using	R?
In-Reply-To: <000001c53560$9484c970$8bdc6f80@BURAN>
References: <000001c53560$9484c970$8bdc6f80@BURAN>
Message-ID: <424BDFCF.2030407@oomvanlieshout.net>

Hi Rick,

Map projections can be done with the package proj4R developed by Roger 
Bivand (http://spatial.nhh.no/R/Devel/). Roger send me a development 
version to use with R2.0.1.

This is the code I use to project to Albers equal area and UTM in South 
Africa (the projection settings listed in the comments come from GRASS):

   ## Project data with proj4R
   datSable.XY <- cbind(datSable$Longitude, datSable$Latitude)
   ## Albers Equal Area projection for SA
   ## elipse or datum    WGS84
   ## false eastings     0
   ## false northing     0
   ## central meridian   24
   ## stand parr_1       -18
   ## stand parr_2       -32
   ## lat of origin      0
   ## output of project is a matrix!
   datSable.XYaea <- project(datSable.XY, "+proj=aea +ellps=WGS84 
+lon_0=24 +lat_0=0 +lat_1=18s +lat_2=32s +x_0=0 +y_0=0 ")
   nrow(datSable.XYaea)
   tmp <- data.frame(datSable.XYaea)
   datSable <- data.frame(datSable, Xaea=tmp$X1, Yaea=tmp$X2)
# name: UTM
# datum: WGS84
# towgs84: 0.000,0.000,0.000
# proj: utm
# ellps: wgs84
# a: 6378137.0000000000
# es: 0.0066943800
# f: 298.2572235630
# zone: 36
# south: defined
   datSable.XYutm <- project(datSable.XY, "+proj=utm +ellps=WGS84 
+zone=36S +south +units=m ")

This should work on polygon coordinates to!

The packages maps and maptools might provide a solution to the area of 
the polygons.

Let me know what you manage to do. I am trying similar things: see 
earlier posts on the r-geo list.

Cheers,

Sander.

Rick Reeves wrote:
> 
> Hello: 
> 
> I've searched the literature, havent found an answer, so here goes? 
> 
> We have a set of polygon features defined as a series of unprojected
> vertices represented by lat/long pairs, in an ASCII-format file.
> 
> We need to display (on a map-like plot) and then calculate the areas 
> of the polygons, and would like to do this entirely within R if
> possible.
> Is there a map projection package for R? Or, has anyone solved this
> or a related problem using R? I could do this with the MATLAB mapping
> toolbox, but prefer to use R if possible....
> 
> 
> Thanks, 
> 
> Rick Reeves
> 
> Scientific Programmer / Quantitative Analyst
> National Center for Ecological Analysis and Synthesis
> University of California, Santa Barbara
> 805 892 2534
> reeves at nceas.ucsb.edu
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
--------------------------------------------
Dr. Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From leroy at ucsd.edu  Thu Mar 31 16:54:47 2005
From: leroy at ucsd.edu (Anthony Westerling)
Date: Thu, 31 Mar 2005 06:54:47 -0800
Subject: [R-sig-Geo] Re: Reading and projecting lat/long point coordinates
	using	R?
In-Reply-To: <200503311012.j2VACU3j030597@hypatia.math.ethz.ch>
References: <200503311012.j2VACU3j030597@hypatia.math.ethz.ch>
Message-ID: <0d30320f79bcabe52e4c482082ed4e9a@ucsd.edu>

If you want a base map to add your polygons to, you can use the Maps 
package in R, or plot a map from a shapefile or arcinfo coverage using 
Rmap or maptools libraries (for shapefile format) or RArcInfo (for e00 
or avc formats) in R.  To add your polygons to the figure just use 
polygon().  If you need to reproject the coordinates in your polygons 
you can use mapproject() in the mapproj library or project() in Rmap(). 
  I prefer project() (it can convert from a projection to geographic 
coordinates (ie, lat/lon), while I think mapproject() just takes 
geographic coordinates) but mapproject() has more documentation.  Both 
use proj4 library (libproj), so you will need to install that.  To use 
shapefiles you need libshp.

I think Rmap is still not distributed via CRAN.  You can find it at 
http://www.maths.lancs.ac.uk/Software/Rmap/

I've linked the documentation for the proj4 library to Barry 
Rowlingson's Rmap documentation, which can be useful when you are 
re-projecting coordinates.  If you want it,  the zipped html files are 
~9 MB.  The original sources are on the web of course.

areapl() in the splancs library can calculate the area of a polygon.

Anthony L. Westerling
Climate Research Division
Scripps Institution of Oceanography
University of California, San Diego
858 822 4057
leroy at ucsd.edu

> Date: Wed, 30 Mar 2005 11:42:17 -0800
> From: "Rick Reeves" <reeves at nceas.ucsb.edu>
> Subject: [R-sig-Geo] Reading and projecting lat/long point coordinates
> 	using	R?
> To: <r-sig-geo at stat.math.ethz.ch>
> Message-ID: <000001c53560$9484c970$8bdc6f80 at BURAN>
> Content-Type: text/plain;	charset="us-ascii"
>
>
>
> Hello:
>
> I've searched the literature, havent found an answer, so here goes?
>
> We have a set of polygon features defined as a series of unprojected
> vertices represented by lat/long pairs, in an ASCII-format file.
>
> We need to display (on a map-like plot) and then calculate the areas
> of the polygons, and would like to do this entirely within R if
> possible.
> Is there a map projection package for R? Or, has anyone solved this
> or a related problem using R? I could do this with the MATLAB mapping
> toolbox, but prefer to use R if possible....
>
>
> Thanks,
>
> Rick Reeves
>
> Scientific Programmer / Quantitative Analyst
> National Center for Ecological Analysis and Synthesis
> University of California, Santa Barbara
> 805 892 2534
> reeves at nceas.ucsb.edu
>
>
>
> ------------------------------
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
> End of R-sig-Geo Digest, Vol 19, Issue 15
> *****************************************
>



