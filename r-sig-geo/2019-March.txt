From er|nm@hodge@@ @end|ng |rom gm@||@com  Mon Mar  4 19:43:14 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Mon, 4 Mar 2019 11:43:14 -0700
Subject: [R-sig-Geo] iPhone app for KML files that has time slider
Message-ID: <CACxE24nEyL10+_mpbgBpBOWKUFBy3-0HSVaJuiRYUnFFbsLRWQ@mail.gmail.com>

Hello!

I am producing some KML files with time spans in them.  They work fine with
a time slider on Google Earth Pro on a laptop.

However, the iPhone and iPad Google Earth apps do not support the time
sliders.

Does anyone know if there is an app which does support the time sliders,
please?

Thanks,
Erin


Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From j@||o@|ub|nd@ @end|ng |rom gm@||@com  Wed Mar  6 03:12:01 2019
From: j@||o@|ub|nd@ @end|ng |rom gm@||@com (Jailos Lubinda)
Date: Wed, 6 Mar 2019 02:12:01 +0000
Subject: [R-sig-Geo] Line graph with over 70 lines plotted against time
In-Reply-To: <alpine.LFD.2.21.1902250948120.2813@reclus.nhh.no>
References: <trinity-85cb4a0d-c8d3-405f-b6c0-44e669ee17f2-1550839072949@3c-app-webde-bs50>
 <CAM_vjuk-BcNwJT6YQBaawp1zoPyhqTquqOfoSHNu-4=h=Ki=4Q@mail.gmail.com>
 <trinity-23510f46-de29-4120-aaaa-506190935271-1550911144437@3c-app-webde-bs05>
 <trinity-e0778366-5638-4eb8-b0ef-ac3e61f40cec-1550926786291@3c-app-webde-bap53>
 <alpine.LFD.2.21.1902232106001.14877@reclus.nhh.no>
 <trinity-cf687c3c-a46e-45a4-b145-9b6ab5b7ef04-1550998892067@3c-app-webde-bs38>
 <alpine.LFD.2.21.1902250948120.2813@reclus.nhh.no>
Message-ID: <b815597d-b4ed-ed13-9a8d-e7258d83a615@gmail.com>

Dear all,

I'm currently working on a dataset with 72 areas, recorded in quarterly 
dates (2000-2016). I want to visualise some variables using line graphs 
in R but have not found suitable tools in weeks now, as many examples 
are for creating a couple lines only. When I use ggplot with date 
(factor), it jumbles the dates making the lines uninterpretable. when I 
convert the date to "date", it breaks at Year points and doesn't give 
continuous lines.

Kindly help.

Thanks

Jailos

On 25/02/2019 08:50, Roger Bivand wrote:
> On Sun, 24 Feb 2019, f-c-b at web.de wrote:
>
>> Hello Roger,
>>
>> thank you for your answer.
>>
>> I tried:
>> > gwrGp <- gwr(formula=Ziel~ Var1 + Var3 + Var4, data = Daten90,
>> + bandwidth = bwG, gweight = gwr.Gauss,hatmatrix = TRUE,
>> + fit.points=Daten10, predictions=TRUE, fittedGWRobject=gwrG)
>>
>> Now the prediction works, but when I give out the result, I get this 
>> error:
>>
>> Error in sqrt(x$results$sigma2.b) :
>> ? non-numeric argument to mathematical function
>> How can I solve this problem?
>
> I am not looking over your shoulder. Always provide the code verbatim, 
> if I guess which mess you are in, I may get the wrong mess.
>
> Generally, I advise against GWR in all settings, it should only ever 
> be used for exploring data for non-stationarity.
>
> Roger
>
>>
>> Thank you,
>> Christoph
>>
>>
>> > Thank you for your answer.
>> > ?I tried to do the transformations before the calculation, but the
>> > Problem is still the same. I think the problem lies in the factor. But
>> > in my model I need the factor, because there is no linear influence of
>> > the year.
>> >
>> > Is there any solution for this Problem?
>> > ?https://www.dropbox.com/s/fbhwsy3sd333ung/Example.zip?dl=0 The 
>> zip-file
>> > in the link contains a picture of str(Daten90) and str(Daten10) and 
>> also
>> > my data and R-skript. Var4 is log(Var2).
>> >
>>
>> I see:
>>
>> > gwrGp <- gwr(formula=Ziel~ Var1 + Var3 + Var4, data = Daten90,
>> + bandwidth = bwG, gweight = gwr.Gauss,hatmatrix = TRUE,
>> + fit.points=Daten10, predictions=TRUE)
>> Warning message:
>> In gwr(formula = Ziel ~ Var1 + Var3 + Var4, data = Daten90, bandwidth =
>> bwG, :
>> standard errors set to NA, normalised RSS not available
>>
>> but using your gwrG object and:
>>
>> > gwrGp <- gwr(formula=Ziel~ Var1 + Var3 + Var4, data = Daten90,
>> + bandwidth = bwG, gweight = gwr.Gauss,hatmatrix = TRUE,
>> + fit.points=Daten10, predictions=TRUE, fittedGWRobject=gwrG)
>>
>> I think the model matrix handling of the factor is OK. The problem in 
>> the
>> GWmodel::gwr.predict() approach is that the full model matrix 
>> approach is
>> used on the formula and data arguments, but not on predictdata, which is
>> coerced to data frame but never regularised by going through 
>> model.matrix,
>> hence the error message:
>>
>> Browse[2]>
>> debug: if (any((inde_vars %in% names(predictdata)) == F)) stop("All the
>> independent variables should be included in the predictdata")
>> Browse[2]> inde_vars
>> [1] "Var12006" "Var12007" "Var12008" "Var12009" "Var3" "Var4"
>> Browse[2]> names(predictdata)
>> [1] "OBJECTID" "Rechtswert" "Hochwert" "Var1" "Var2"
>> [6] "Var3" "Ziel" "Var4" "coords.x1" "coords.x2"
>>
>> Had the proper approach been used, the names would have been the same.
>>
>> The relevant part of spgwr::gwr() in R/gwr.R is:
>>
>> if (predictions) {
>> t1 <- try(slot(fit.points, "data"), silent=TRUE)
>> if (class(t1) == "try-error")
>> stop("No data slot in fit.points")
>> predx <- try(model.matrix(delete.response(mt), fit.points))
>> if (class(predx) == "try-error")
>> stop("missing RHS variable in fit.points")
>> if (ncol(predx) != ncol(x))
>> stop("new data matrix columns mismatch")
>> }
>>
>> (lines 71-80)
>>
>> which uses model.matrix() and uses try() to catch mis-matches.
>>
>> Hope this helps,
>>
>> Roger
>>
>> > Christoph
>> >
>> >
>> >
>> >
>> > The first step should be to look at
>> >
>> > str(Daten90)
>> > str(Daten10)
>> >
>> > and if that doesn't solve the problem, then consider a reproducible
>> > example, or at the very least posting the results of the above to this
>> > list.
>> >
>> > Sarah
>> >
>> > On Fri, Feb 22, 2019 at 7:38 AM <f-c-b at web.de> wrote:
>> > >
>> > > Dear all,
>> > >
>> > > I am currently working out a geographically weighted regression, in
>> which
>> > 90% of the data set the model should be calculated and for 10% of the
>> values
>> > to be predicted. For the prediction I use the function gwr.predict 
>> from
>> the
>> > package GWModel:
>> > >
>> > > Erg<-gwr.predict(formula=Ziel~ as.factor(Var1) + log(Var2, base =
>> exp(1))
>> > + Var3, data = Daten90,predictdata = Daten10,bw = bwG, kernel =
>> > "gaussian",adaptive = FALSE, p = 2, theta = 0, longlat = FALSE)
>> > >
>> > > I always get this error, although Daten10 and Daten90 have the same
>> > structure:
>> > > Error in gwr.predict(formula = Ziel~ as.factor(Var1) + log(Var2, 
>> base =
>> > exp(1)) + Var3, :
>> > > All the independent variables should be included in the predictdata.
>> > >
>> > > Can you tell me what the problem with this code is?
>> > > Or is there any other way for a GWR and the prediction?
>> > >
>> > > Thank you,
>> > > Christoph
>> > > _______________________________________________
>> > > R-sig-Geo mailing list
>> > > R-sig-Geo at r-project.org
>> > > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >
>> >
>> >
>> > --
>> > Sarah Goslee (she/her)
>> > http://www.numberwright.com
>> >
>> >
>>
>> -- 
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From m@|N@m@|J@ @end|ng |rom gmx@de  Wed Mar  6 14:06:41 2019
From: m@|N@m@|J@ @end|ng |rom gmx@de (=?UTF-8?Q?=22Jannes_M=C3=BCnchow=22?=)
Date: Wed, 6 Mar 2019 14:06:41 +0100
Subject: [R-sig-Geo] Announcing the "Summer School on Geospatial Data
 Science with R in Jena"
Message-ID: <trinity-d2c694e4-495a-4337-b51a-eff752277d3c-1551877601446@3c-app-gmx-bs75>

We are proud to announce our "International Summer School on Geospatial Data Science with R" taking place at the GIScience department of Jena (Germany) from 25. August to 1. September 2019. The summer school will consist of a mixture of lectures and lab classes and teaches advanced geospatial techniques for analyzing and predicting global change phenomena on the command line in a fully reproducible workflow. This includes predictive modeling using statistical and machine learning techniques, model assessment and data science challenges with geospatial data. The summer school is generously funded by the German Academic Exchange Service which means that each accepted international participant will receive at least a funding of 450 Euros upon successful completion of the summer school.*

Please share this fantastic opportunity with all interested parties and refer to the official website for more information on how to apply, the program and the invited lecturers:
https://jupiter.geogr.uni-jena.de/summerschool/about/

We are looking forward to welcoming you in Jena!
Jannes

*Applicants from Germany are welcome to apply but only a limited number can be admitted and no funding can be granted.


From zw@ng3603 @end|ng |rom gm@||@com  Wed Mar  6 22:31:35 2019
From: zw@ng3603 @end|ng |rom gm@||@com (Jay Wang)
Date: Wed, 6 Mar 2019 15:31:35 -0600
Subject: [R-sig-Geo] inference of local Gi* using permutations
Message-ID: <CANZ=wL3bJdgDv_HOVHRwhzx2AQNEG5VR9Qr=_C2KWY5GrFt1mg@mail.gmail.com>

Hello,

I am currently using the localG () in spdep package, I was wondering if we
can have a conditional permutation-based inference to get the P value for
every Gi*. I saw that a Mote Carlo simulation is used in Moran.MC(), and I
borrowed the following codes from this function and tried to see if I can
do a permutation for localG():

pvals<-matrix(0, nrow = V, ncol = 1)
for (i in 1:V){
  rankresi<-rank(res[i, ])
  xranki <- rankresi[length(res[i, ])]
  diffi <- nsim - xranki
  diffi <- ifelse(diffi > 0, diffi, 0)
  pvali <- punif((diffi + 1)/(nsim + 1))
  pvals[i,]<-pvali
}

After running these codes with several different datasets, I found that all
the negative Gi*s have very high P values say 0.999 with 999 permutations,
meaning that there are no significant cold spots. Where is the problem? How
can we do conditional permutation-based inference for localG() with R
spdep? I understand the critics of permutation-based inference for local
indicators, but I just want to explore this. Thank you!

Best

	[[alternative HTML version deleted]]


From j@||o@|ub|nd@ @end|ng |rom gm@||@com  Thu Mar  7 11:18:43 2019
From: j@||o@|ub|nd@ @end|ng |rom gm@||@com (Jailos Lubinda)
Date: Thu, 7 Mar 2019 10:18:43 +0000
Subject: [R-sig-Geo] Line graph with over 70 lines plotted against
 time-Resolved
In-Reply-To: <20f20a62-12c4-cc20-c235-2697d40cc5a6@gmail.com>
References: <CAF2H4=ZakjGCOf3u9R7yFM021i90QOck6fScio4GdnXL9vAP3w@mail.gmail.com>
 <b5028aa3-092f-8a4d-6088-43e95250b62c@gmail.com>
 <CAF2H4=avT8bGJwYwNniV9_yR2GjOGG=S2FUpjRLwS1BbYQ3H9w@mail.gmail.com>
 <7ad4d97c-79ec-ccd9-6569-b64e72084f4d@gmail.com>
 <CAF2H4=YbRgUsCN6spO0Y2EK671KMC5E7FJy4CEB1FG58XcTuXg@mail.gmail.com>
 <CAF2H4=ayqxD85Ji=dNDXhtMo_WOmOF2d1cNDT2jH2rPLLH+1Pw@mail.gmail.com>
 <367e9b4b-b60d-b76c-318e-8a661f0d1892@gmail.com>
 <CAF2H4=aDBbUwQUkn5Azr54Pd76Rgt4f2HiexukBcP98sf6qvtg@mail.gmail.com>
 <ec977290-b9be-d71b-db3d-ecb3810b476f@gmail.com>
 <CAF2H4=azoK4TZ_vSDCTD=MRbnV6h4PaQTua7RmAX+J5n6mvWeg@mail.gmail.com>
 <CAF2H4=bjFf61jj2bnxcbfOSvbCx4y4AdneFG_MupBT3w5Yj+5A@mail.gmail.com>
 <20f20a62-12c4-cc20-c235-2697d40cc5a6@gmail.com>
Message-ID: <4f15376c-2490-113c-f90a-49828478a502@gmail.com>

Dear all,

 ?My issue has been resolved, thanks to Byman, a R-sig-Geo member for 
the help. Just to refresh my question,/**//*I needed to plot a line 
graph of one variable collected over 72 sites(areas) by time (date) 
which meant 72 lines on the same graph*/.
 ?Here is the solution/code below in case others might have similar issue.

/?# Plot several line graphs//
//?library(ggplot2)//
//?library(reshape2)//
//
//?#changes this line below accordingly//
//?dat <- read.table(file.path(""), header = TRUE, sep = ",")//
//
//?#Melt the data for easy plotting//
//?data <- melt(dat, id = c("DISTRICTS_", "area_name", "Year","Quarter", 
"Date"))//
//
//?# format the date column//
//?data$Date <- as.Date(data$Date, format = "%d/%m/%Y")//
//?#Subset and select the target variable//
//?g <- ggplot()//
//?g <- g + geom_line(data = data %% filter(variable == 
"Mean.Max.Temp"), aes(x = Date, y = value, group = area_name)) #, colour 
= area_name))//
//?####end of script##############/

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Mar  7 15:05:03 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 7 Mar 2019 15:05:03 +0100
Subject: [R-sig-Geo] inference of local Gi* using permutations
In-Reply-To: <CANZ=wL3bJdgDv_HOVHRwhzx2AQNEG5VR9Qr=_C2KWY5GrFt1mg@mail.gmail.com>
References: <CANZ=wL3bJdgDv_HOVHRwhzx2AQNEG5VR9Qr=_C2KWY5GrFt1mg@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903071439540.18964@reclus.nhh.no>

On Wed, 6 Mar 2019, Jay Wang wrote:

> Hello,
>
> I am currently using the localG () in spdep package, I was wondering if we
> can have a conditional permutation-based inference to get the P value for
> every Gi*. I saw that a Mote Carlo simulation is used in Moran.MC(), and I
> borrowed the following codes from this function and tried to see if I can
> do a permutation for localG():
>
> pvals<-matrix(0, nrow = V, ncol = 1)
> for (i in 1:V){
>  rankresi<-rank(res[i, ])
>  xranki <- rankresi[length(res[i, ])]
>  diffi <- nsim - xranki
>  diffi <- ifelse(diffi > 0, diffi, 0)
>  pvali <- punif((diffi + 1)/(nsim + 1))
>  pvals[i,]<-pvali
> }

Monte Carlo or Hope type tests or permutation bootstraps work like 
analytical permutation in the global case, and are redundant for that 
reason. Monte Carlo (conditional permutation) in the local case requires 
that there is no global autocorrelation or other mis-specifications.

Your code snippet is not an example, and I think is not how permutation is 
actually done. In the local case in ArcGIS, GeoDa, etc., you fix the value 
of interest at i, randomly re-assign all the values to the other 
observations, and recompute the local indicator at i, doing this say 499 
times. Then you move on to the next i, and repeat. In your snippet, we 
cannot see where res is coming from. Is this an n by nsim matrix from nsim 
runs? Might you be needing to compare the sample values with the observed 
value? If you are outputting Z-values of G_i anyway, you may need to step 
back to the actual G_i (here from spdep):

attr(localG(..., return_internals=TRUE), "internals")[,1]

Then

mns <- apply(res, 1, mean)
sds <- apply(res, 1, sd)
permZ <- (obs_localG - mns)/sds

are z values that can be considered as drawn from the normal. However, I 
advise against inference unless the assumptions are met, and p-values must 
be adjusted anyway.

Roger

>
> After running these codes with several different datasets, I found that all
> the negative Gi*s have very high P values say 0.999 with 999 permutations,
> meaning that there are no significant cold spots. Where is the problem? How
> can we do conditional permutation-based inference for localG() with R
> spdep? I understand the critics of permutation-based inference for local
> indicators, but I just want to explore this. Thank you!
>
> Best
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From zw@ng3603 @end|ng |rom gm@||@com  Thu Mar  7 17:11:50 2019
From: zw@ng3603 @end|ng |rom gm@||@com (Jay Wang)
Date: Thu, 7 Mar 2019 10:11:50 -0600
Subject: [R-sig-Geo] inference of local Gi* using permutations
In-Reply-To: <alpine.LFD.2.21.1903071439540.18964@reclus.nhh.no>
References: <CANZ=wL3bJdgDv_HOVHRwhzx2AQNEG5VR9Qr=_C2KWY5GrFt1mg@mail.gmail.com>
 <alpine.LFD.2.21.1903071439540.18964@reclus.nhh.no>
Message-ID: <CANZ=wL2Wx8cNObQVMU6vhCh2r8QY26ZmVk5JBexiu1c0rJFL=A@mail.gmail.com>

Thank you for the detailed explanation. Sorry for missing the explanation
of res. It is an n by nsim+1 matrix where the n columns are the permutation
results using sample() and the last column is the actual z values of G_i*s.
The following line of code was modified to do the permutations: (i in
1:nsim) res[i] <- moran(sample(x), listw, n, S0, zero.policy)$I. This could
be wrong since the sample() does not fix the value of interest at i. Are
there any functions in R can do this in a correct way? How do we calculate
the p-values after we get the n by nsim matrix? Thank you!


On Thu, Mar 7, 2019 at 8:05 AM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Wed, 6 Mar 2019, Jay Wang wrote:
>
> > Hello,
> >
> > I am currently using the localG () in spdep package, I was wondering if
> we
> > can have a conditional permutation-based inference to get the P value for
> > every Gi*. I saw that a Mote Carlo simulation is used in Moran.MC(), and
> I
> > borrowed the following codes from this function and tried to see if I can
> > do a permutation for localG():
> >
> > pvals<-matrix(0, nrow = V, ncol = 1)
> > for (i in 1:V){
> >  rankresi<-rank(res[i, ])
> >  xranki <- rankresi[length(res[i, ])]
> >  diffi <- nsim - xranki
> >  diffi <- ifelse(diffi > 0, diffi, 0)
> >  pvali <- punif((diffi + 1)/(nsim + 1))
> >  pvals[i,]<-pvali
> > }
>
> Monte Carlo or Hope type tests or permutation bootstraps work like
> analytical permutation in the global case, and are redundant for that
> reason. Monte Carlo (conditional permutation) in the local case requires
> that there is no global autocorrelation or other mis-specifications.
>
> Your code snippet is not an example, and I think is not how permutation is
> actually done. In the local case in ArcGIS, GeoDa, etc., you fix the value
> of interest at i, randomly re-assign all the values to the other
> observations, and recompute the local indicator at i, doing this say 499
> times. Then you move on to the next i, and repeat. In your snippet, we
> cannot see where res is coming from. Is this an n by nsim matrix from nsim
> runs? Might you be needing to compare the sample values with the observed
> value? If you are outputting Z-values of G_i anyway, you may need to step
> back to the actual G_i (here from spdep):
>
> attr(localG(..., return_internals=TRUE), "internals")[,1]
>
> Then
>
> mns <- apply(res, 1, mean)
> sds <- apply(res, 1, sd)
> permZ <- (obs_localG - mns)/sds
>
> are z values that can be considered as drawn from the normal. However, I
> advise against inference unless the assumptions are met, and p-values must
> be adjusted anyway.
>
> Roger
>
> >
> > After running these codes with several different datasets, I found that
> all
> > the negative Gi*s have very high P values say 0.999 with 999
> permutations,
> > meaning that there are no significant cold spots. Where is the problem?
> How
> > can we do conditional permutation-based inference for localG() with R
> > spdep? I understand the critics of permutation-based inference for local
> > indicators, but I just want to explore this. Thank you!
> >
> > Best
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From zw@ng3603 @end|ng |rom gm@||@com  Fri Mar  8 20:37:20 2019
From: zw@ng3603 @end|ng |rom gm@||@com (Jay Wang)
Date: Fri, 8 Mar 2019 13:37:20 -0600
Subject: [R-sig-Geo] inconsistent results from moran.test()
Message-ID: <CANZ=wL0fvjvOOMadBDaCLNnNqt4A8m_JrKpJf749Vqi=Z7R-Vg@mail.gmail.com>

Hello,

Having my own spatial weight matrix, I converted it to a listw format and
calculated the Moran index. The codes are listed below:

Wod_lw<-mat2listw(Wod)
moran.test(mydata$Ties_processed,Wod_lw, randomisation=TRUE)

However, after I generated a gal file, imported it, converted it back to a
listw file, and re-calculated the Moran index with the following codes, the
results became totally different (0.1624 vs. 0.1942):
write.nb.gal(Wod_lw$neighbours,"Wod.gal")
Wod.gal.nb <- read.gal(file="Wod.gal")
Wod_lw2<-nb2listw(Wod.gal.nb)
moran.test(mydata$Ties_processed,Wod_lw, randomisation=TRUE)
moran.test(mydata$Ties_processed,Wod_lw2, randomisation=TRUE)

I don't know where the problem is. I would appreciate if anyone can help me
with this. Thank you

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Fri Mar  8 21:25:36 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Fri, 8 Mar 2019 21:25:36 +0100
Subject: [R-sig-Geo] inconsistent results from moran.test()
In-Reply-To: <CANZ=wL0fvjvOOMadBDaCLNnNqt4A8m_JrKpJf749Vqi=Z7R-Vg@mail.gmail.com>
References: <CANZ=wL0fvjvOOMadBDaCLNnNqt4A8m_JrKpJf749Vqi=Z7R-Vg@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903082121160.29157@reclus.nhh.no>

On Fri, 8 Mar 2019, Jay Wang wrote:

> Hello,
>
> Having my own spatial weight matrix, I converted it to a listw format and
> calculated the Moran index. The codes are listed below:
>
> Wod_lw<-mat2listw(Wod)
> moran.test(mydata$Ties_processed,Wod_lw, randomisation=TRUE)

Here, Wod_lw has an unknown style, "M" for matrix.

>
> However, after I generated a gal file, imported it, converted it back to a
> listw file, and re-calculated the Moran index with the following codes, the
> results became totally different (0.1624 vs. 0.1942):
> write.nb.gal(Wod_lw$neighbours,"Wod.gal")

This only writes the neighbours (read the help page), not the weights.

> Wod.gal.nb <- read.gal(file="Wod.gal")
> Wod_lw2<-nb2listw(Wod.gal.nb)

The default style in nb2listw() is "W", row standardised.

What does:

all.equal(Wod_lw$weights, Wod_lw2$weights, check.attributes=FALSE)

say? And:

all.equal(Wod_lw$neighbours, Wod_lw2$neighbours, check.attributes=FALSE)

> moran.test(mydata$Ties_processed,Wod_lw, randomisation=TRUE)
> moran.test(mydata$Ties_processed,Wod_lw2, randomisation=TRUE)
>
> I don't know where the problem is. I would appreciate if anyone can help me
> with this. Thank you

Do read the help pages, you'll find they explain these things.

Roger

>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Fri Mar  8 22:41:29 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Fri, 8 Mar 2019 22:41:29 +0100
Subject: [R-sig-Geo] inference of local Gi* using permutations
In-Reply-To: <CANZ=wL2Wx8cNObQVMU6vhCh2r8QY26ZmVk5JBexiu1c0rJFL=A@mail.gmail.com>
References: <CANZ=wL3bJdgDv_HOVHRwhzx2AQNEG5VR9Qr=_C2KWY5GrFt1mg@mail.gmail.com>
 <alpine.LFD.2.21.1903071439540.18964@reclus.nhh.no>
 <CANZ=wL2Wx8cNObQVMU6vhCh2r8QY26ZmVk5JBexiu1c0rJFL=A@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903082212050.29157@reclus.nhh.no>

On Thu, 7 Mar 2019, Jay Wang wrote:

> Thank you for the detailed explanation. Sorry for missing the explanation
> of res. It is an n by nsim+1 matrix where the n columns are the permutation
> results using sample() and the last column is the actual z values of G_i*s.
> The following line of code was modified to do the permutations: (i in
> 1:nsim) res[i] <- moran(sample(x), listw, n, S0, zero.policy)$I. This could
> be wrong since the sample() does not fix the value of interest at i. Are
> there any functions in R can do this in a correct way? How do we calculate
> the p-values after we get the n by nsim matrix? Thank you!

This is an exercise based on the built-in Getis-Ord dataset:

library(spdep)
data(getisord, package="spData")
xycoords <- cbind(xyz$x, xyz$y)
nb30 <- dnearneigh(xycoords, 0, 30)
lw <- nb2listw(nb30, style="B")
G30 <- localG(xyz$val, lw, return_internals=TRUE)
G_obs <- attr(G30, "internals")[,1]
val <- xyz$val
loc_G <- function(x, lw) {
   x_star <- sum(x)
   lx <- lag.listw(lw, x)
   G <- lx/(x_star-c(x))
   G
}
nsim <- 499L
set.seed(1)
res <- matrix(0, nrow=length(val), ncol=nsim)
thisx <- numeric(length(val))
idx <- 1:length(val)
for (j in 1:nsim) {
   for (i in seq(along=val)) {
     thisx[i] <- val[i]
     thisx[!idx %in% i] <- sample(val[-i])
     res[i, j] <- loc_G(thisx, lw)[i]
   }
}
mns <- apply(res, 1, mean)
sds <- apply(res, 1, sd)
G_Zi <- (G_obs - mns)/sds
plot(density(G30), ylim=c(0, 0.25))
lines(density(G_Zi), lty=2, col=2)

As you can see, the analytical z-values of the match the permutations of G 
values closely. We're doing a permutation bootstrap on the G values, not 
the z-values of G. Next, try pysal (here old pysal < 2):

library(reticulate)
pysal <- import("pysal")
np <- import("numpy")
write.nb.gal(nb30, "nb30.gal")
w <- pysal$open("nb30.gal")$read()
y <- np$array(val)
w$transformation = "B"
lg_30_499 <- pysal$esda$getisord$G_Local(y, w,
   transform="B", permutations=nsim)
lines(density(lg_30_499$z_sim), lty=3, col=3)
all.equal(G_obs, c(lg_30_499$Gs))
all.equal(c(G30), c(lg_30_499$Zs))

The G_Local() function provides both analytical and permutation z-values; 
the G values and the analytical z-values agree exactly with spdep, the 
permutations are not using the same random number generator or seed, but 
are broadly similar. If you read pysal/esda/getisord.py, you'll see that 
some indexing is being done, avoiding the double loop. The code is fairly 
hard to read, but in optimising the permutation may cut corners; the 
self.z_sim is the same code as the simpler R code above.

So while for the local Moran, permutation is demonstrably misleading in 
the presence of mis-specification, for local G permutation simply 
reproduces the analytical z-values. If you use these to look up normal 
p-values, then adjust them for the false discovery rate, usually nothing 
is left significant anyway.

Roger


>
>
> On Thu, Mar 7, 2019 at 8:05 AM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Wed, 6 Mar 2019, Jay Wang wrote:
>>
>>> Hello,
>>>
>>> I am currently using the localG () in spdep package, I was wondering if
>> we
>>> can have a conditional permutation-based inference to get the P value for
>>> every Gi*. I saw that a Mote Carlo simulation is used in Moran.MC(), and
>> I
>>> borrowed the following codes from this function and tried to see if I can
>>> do a permutation for localG():
>>>
>>> pvals<-matrix(0, nrow = V, ncol = 1)
>>> for (i in 1:V){
>>>  rankresi<-rank(res[i, ])
>>>  xranki <- rankresi[length(res[i, ])]
>>>  diffi <- nsim - xranki
>>>  diffi <- ifelse(diffi > 0, diffi, 0)
>>>  pvali <- punif((diffi + 1)/(nsim + 1))
>>>  pvals[i,]<-pvali
>>> }
>>
>> Monte Carlo or Hope type tests or permutation bootstraps work like
>> analytical permutation in the global case, and are redundant for that
>> reason. Monte Carlo (conditional permutation) in the local case requires
>> that there is no global autocorrelation or other mis-specifications.
>>
>> Your code snippet is not an example, and I think is not how permutation is
>> actually done. In the local case in ArcGIS, GeoDa, etc., you fix the value
>> of interest at i, randomly re-assign all the values to the other
>> observations, and recompute the local indicator at i, doing this say 499
>> times. Then you move on to the next i, and repeat. In your snippet, we
>> cannot see where res is coming from. Is this an n by nsim matrix from nsim
>> runs? Might you be needing to compare the sample values with the observed
>> value? If you are outputting Z-values of G_i anyway, you may need to step
>> back to the actual G_i (here from spdep):
>>
>> attr(localG(..., return_internals=TRUE), "internals")[,1]
>>
>> Then
>>
>> mns <- apply(res, 1, mean)
>> sds <- apply(res, 1, sd)
>> permZ <- (obs_localG - mns)/sds
>>
>> are z values that can be considered as drawn from the normal. However, I
>> advise against inference unless the assumptions are met, and p-values must
>> be adjusted anyway.
>>
>> Roger
>>
>>>
>>> After running these codes with several different datasets, I found that
>> all
>>> the negative Gi*s have very high P values say 0.999 with 999
>> permutations,
>>> meaning that there are no significant cold spots. Where is the problem?
>> How
>>> can we do conditional permutation-based inference for localG() with R
>>> spdep? I understand the critics of permutation-based inference for local
>>> indicators, but I just want to explore this. Thank you!
>>>
>>> Best
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From zw@ng3603 @end|ng |rom gm@||@com  Sat Mar  9 17:59:34 2019
From: zw@ng3603 @end|ng |rom gm@||@com (Jay Wang)
Date: Sat, 9 Mar 2019 10:59:34 -0600
Subject: [R-sig-Geo] inconsistent results from moran.test()
In-Reply-To: <alpine.LFD.2.21.1903082121160.29157@reclus.nhh.no>
References: <CANZ=wL0fvjvOOMadBDaCLNnNqt4A8m_JrKpJf749Vqi=Z7R-Vg@mail.gmail.com>
 <alpine.LFD.2.21.1903082121160.29157@reclus.nhh.no>
Message-ID: <CANZ=wL02dEeSaFKvzUPa-e6NWkJK4LZn8A+SUWySm63aTL-pVw@mail.gmail.com>

It solved my problem. Thank you!

On Fri, Mar 8, 2019 at 2:25 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Fri, 8 Mar 2019, Jay Wang wrote:
>
> > Hello,
> >
> > Having my own spatial weight matrix, I converted it to a listw format and
> > calculated the Moran index. The codes are listed below:
> >
> > Wod_lw<-mat2listw(Wod)
> > moran.test(mydata$Ties_processed,Wod_lw, randomisation=TRUE)
>
> Here, Wod_lw has an unknown style, "M" for matrix.
>
> >
> > However, after I generated a gal file, imported it, converted it back to
> a
> > listw file, and re-calculated the Moran index with the following codes,
> the
> > results became totally different (0.1624 vs. 0.1942):
> > write.nb.gal(Wod_lw$neighbours,"Wod.gal")
>
> This only writes the neighbours (read the help page), not the weights.
>
> > Wod.gal.nb <- read.gal(file="Wod.gal")
> > Wod_lw2<-nb2listw(Wod.gal.nb)
>
> The default style in nb2listw() is "W", row standardised.
>
> What does:
>
> all.equal(Wod_lw$weights, Wod_lw2$weights, check.attributes=FALSE)
>
> say? And:
>
> all.equal(Wod_lw$neighbours, Wod_lw2$neighbours, check.attributes=FALSE)
>
> > moran.test(mydata$Ties_processed,Wod_lw, randomisation=TRUE)
> > moran.test(mydata$Ties_processed,Wod_lw2, randomisation=TRUE)
> >
> > I don't know where the problem is. I would appreciate if anyone can help
> me
> > with this. Thank you
>
> Do read the help pages, you'll find they explain these things.
>
> Roger
>
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Mar 11 13:56:10 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 11 Mar 2019 13:56:10 +0100
Subject: [R-sig-Geo] rgdal 1.4-2 on CRAN
Message-ID: <alpine.LFD.2.21.1903111304340.19658@reclus.nhh.no>

The latest release of rgdal is the first step towards support for PROJ 6 
(PROJ is the generic name for proj4). From PROJ 5, some internals were 
changed but the API and metadata files remained unchanged. PROJ 6, 
released 1 March, requires software to declare that it uses the old API, 
which rgdal still does, but changes the metadata files to better 
accommodate the changes in the internals. These are stepping away from the 
+datum= tag, and there are now some coordinate reference systems (CRS) 
that cannot be represented using proj4 strings. PROJ 7 is due in March 
2020, and will remove the old API.

The sp and rgdal packages will continue to use the proj4 string 
representation of CRS; sf also permits the use of EPSG codes, so 
(potentially) permitting non-proj4 string CRS to be handled. rgdal and sf 
will migrate to the new PROJ API during 2019 and early 2020, but will 
attempt to maintain backward compatibility for more recent releases of 
PROJ. Why the changes? Well, WGS84 was from 1984, and the world has 
changed quite a lot since then, so pivoting CRS to CRS transformation 
through geographical coordinates using the WGS84 datum is becoming more 
error-prone. The new PROJ internals pipeline transformations without 
pivoting through WGS84, so enhancing precision and making calculations 
(more) future-proof.

I've notified the maintainers of several affected packages so that they 
have time to adapt before PROJ 6 propagates to upstream packagers (GRASS 
already uses the new API and GRASS master is PROJ 6 ready, GDAL master is 
PROJ 6 ready). We'll hold off putting PROJ 6 into Windows and OSX binaries 
for some months, but please be ready us to do that, latest end of year 
2019, to be ready for PROJ 7.

Roger

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From o|@tunde@mo@ob@|@je @end|ng |rom coven@ntun|ver@|ty@edu@ng  Wed Mar 13 05:02:33 2019
From: o|@tunde@mo@ob@|@je @end|ng |rom coven@ntun|ver@|ty@edu@ng (Olatunde Mosobalaje)
Date: Wed, 13 Mar 2019 05:02:33 +0100
Subject: [R-sig-Geo] Zonal Anisotropy versus Geometric Anisotropy
Message-ID: <CAO1OqVSue6DH5SrK+uScc5KRV5sDktLorCb0p6XPHXcoA-L=0A@mail.gmail.com>

To what extent would the vertical variogram sill be less than the data
variance before a zonal anisotropy can be considered?

I have this data with horizontal variogram sill (along all azimuths) of
0.0042 but a vertical variogram sill of 0.004. The variance of the data is
around 0.0044. I have taken the horizontal sill to be approximating the
variance. I wonder if I should do the same for the vertical sill and
therefore declare a fully geometric anisotropy; or, if I should take the
vertical sill as indeed less than the sill and therefore declare a zonal
anisotropy.

Thank you in advance for your response.

	[[alternative HTML version deleted]]


From bunn@ @end|ng |rom wwu@edu  Wed Mar 13 19:13:55 2019
From: bunn@ @end|ng |rom wwu@edu (Andy Bunn)
Date: Wed, 13 Mar 2019 18:13:55 +0000
Subject: [R-sig-Geo] Aggregating points based on distance
In-Reply-To: <447BFEA7-6A5D-4196-8598-BD6DF34E7A9B@wwu.edu>
References: <447BFEA7-6A5D-4196-8598-BD6DF34E7A9B@wwu.edu>
Message-ID: <9A0D338D-7786-42F1-895C-6BFAD069D859@wwu.edu>

I would like to create averages of all the variables in a SpatialPointsDataFrame when points are within a specified distance of each other. I have a method for doing this but it seems like a silly way to approach the problem. Any ideas for doing this using modern syntax (especially of the tidy variety) would be appreciated.
    

To start, I have a SpatialPointsDataFrame with several variables measured for each point. I'd like to get an average value for each variable for points within a specified distance. E.g., getting average cadmium values from the meuse data for points within 100 m of each other:
    
    library(sf)
    library(sp)
    data(meuse)
    pts <- st_as_sf(meuse, coords = c("x", "y"), remove=FALSE)
    pts100 <- st_is_within_distance(pts, dist = 100)
    # can use sapply to get mean of a variable. E.g., cadmium
    sapply(pts100, function(x){ mean(pts$cadmium[x]) })

Above, I've figured out how to use sapply to do this variable by variable. So I could, if I wanted, calculate the mean for each variable, generate a centroid for each point and then a SpatialPointsDataFrame of the unique values. E.g., for the first few variables:
    
    res <- data.frame(id=1:length(pts100),
                      x=NA, y=NA,
                      cadmium=NA, copper=NA, lead=NA)
    res$x <- sapply(pts100, function(p){ mean(pts$x[p]) })
    res$y <- sapply(pts100, function(p){ mean(pts$y[p]) })
    res$cadmium <- sapply(pts100, function(p){ mean(pts$cadmium[p]) })
    res$copper <- sapply(pts100, function(p){ mean(pts$copper[p]) })
    res$lead <- sapply(pts100, function(p){ mean(pts$lead[p]) })
    res2 <- res[duplicated(res$cadmium),]
    coordinates(res2) <- c("x","y")
    bubble(res2,"cadmium")
    
    
This works but seems cumbersome and like there must be a more efficient way. 
    
    
Thanks for any help, Andy
    
    


From b@row||ng@on @end|ng |rom gm@||@com  Wed Mar 13 19:33:53 2019
From: b@row||ng@on @end|ng |rom gm@||@com (Barry Rowlingson)
Date: Wed, 13 Mar 2019 18:33:53 +0000
Subject: [R-sig-Geo] Aggregating points based on distance
In-Reply-To: <9A0D338D-7786-42F1-895C-6BFAD069D859@wwu.edu>
References: <447BFEA7-6A5D-4196-8598-BD6DF34E7A9B@wwu.edu>
 <9A0D338D-7786-42F1-895C-6BFAD069D859@wwu.edu>
Message-ID: <CANVKczMqkHbnA73t+JBum8J5zttrLF5B7HsjBhLguB1xdiLWJw@mail.gmail.com>

On Wed, Mar 13, 2019 at 6:14 PM Andy Bunn <bunna at wwu.edu> wrote:

> I would like to create averages of all the variables in a
> SpatialPointsDataFrame when points are within a specified distance of each
> other. I have a method for doing this but it seems like a silly way to
> approach the problem. Any ideas for doing this using modern syntax
> (especially of the tidy variety) would be appreciated.
>
>
> To start, I have a SpatialPointsDataFrame with several variables measured
> for each point. I'd like to get an average value for each variable for
> points within a specified distance. E.g., getting average cadmium values
> from the meuse data for points within 100 m of each other:
>
>     library(sf)
>     library(sp)
>     data(meuse)
>     pts <- st_as_sf(meuse, coords = c("x", "y"), remove=FALSE)
>     pts100 <- st_is_within_distance(pts, dist = 100)
>     # can use sapply to get mean of a variable. E.g., cadmium
>     sapply(pts100, function(x){ mean(pts$cadmium[x]) })
>
>
If this is the method you call "silly" then I don't see anything silly at
all here, only efficient well-written use of base R constructs. The problem
with "modern" syntax is that its subject to rapid change and often slower
than using base R, which has had years to stabilise and optimise.

If you want to iterate this over variables then nest your sapplys:

items = c("cadmium", "copper","lead")
sapply(items, function(item){
 sapply(pts100, function(x){ mean(pts[[item]][x]) })
})

gets you:

         cadmium    copper      lead
  [1,] 10.150000  83.00000 288.00000
  [2,] 10.150000  83.00000 288.00000
  [3,]  6.500000  68.00000 199.00000
  [4,]  2.600000  81.00000 116.00000


Barry


> Above, I've figured out how to use sapply to do this variable by variable.
> So I could, if I wanted, calculate the mean for each variable, generate a
> centroid for each point and then a SpatialPointsDataFrame of the unique
> values. E.g., for the first few variables:
>
>     res <- data.frame(id=1:length(pts100),
>                       x=NA, y=NA,
>                       cadmium=NA, copper=NA, lead=NA)
>     res$x <- sapply(pts100, function(p){ mean(pts$x[p]) })
>     res$y <- sapply(pts100, function(p){ mean(pts$y[p]) })
>     res$cadmium <- sapply(pts100, function(p){ mean(pts$cadmium[p]) })
>     res$copper <- sapply(pts100, function(p){ mean(pts$copper[p]) })
>     res$lead <- sapply(pts100, function(p){ mean(pts$lead[p]) })
>     res2 <- res[duplicated(res$cadmium),]
>     coordinates(res2) <- c("x","y")
>     bubble(res2,"cadmium")
>
>
> This works but seems cumbersome and like there must be a more efficient
> way.
>
>
> Thanks for any help, Andy
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From |@r@@@|p@@||v@ @end|ng |rom gm@||@com  Thu Mar 14 00:16:31 2019
From: |@r@@@|p@@||v@ @end|ng |rom gm@||@com (Lara Silva)
Date: Wed, 13 Mar 2019 22:16:31 -0100
Subject: [R-sig-Geo] Error in running Maxent within biomod2
Message-ID: <CALN9TES7N-h2-=Vouj6WWryEZBkkgyKOsrFEFH1x_3MC452PzA@mail.gmail.com>

I got an error when running Maxent within biomod2

 Running Maxent...Error in system(command = paste("java ",
ifelse(is.null(Options at MAXENT.Phillips$memory_allocated),  :

What does it means?

Thanks

	[[alternative HTML version deleted]]


From bunn@ @end|ng |rom wwu@edu  Thu Mar 14 20:21:02 2019
From: bunn@ @end|ng |rom wwu@edu (Andy Bunn)
Date: Thu, 14 Mar 2019 19:21:02 +0000
Subject: [R-sig-Geo] Aggregating points based on distance
In-Reply-To: <CANVKczMqkHbnA73t+JBum8J5zttrLF5B7HsjBhLguB1xdiLWJw@mail.gmail.com>
References: <447BFEA7-6A5D-4196-8598-BD6DF34E7A9B@wwu.edu>
 <9A0D338D-7786-42F1-895C-6BFAD069D859@wwu.edu>
 <CANVKczMqkHbnA73t+JBum8J5zttrLF5B7HsjBhLguB1xdiLWJw@mail.gmail.com>
Message-ID: <DEFBFA31-A859-4BAA-8BF1-070052E691E9@wwu.edu>

Ha! That is a great take. Thanks Barry.

?On 3/13/19, 11:34 AM, "R-sig-Geo on behalf of Barry Rowlingson" <r-sig-geo-bounces at r-project.org on behalf of b.rowlingson at gmail.com> wrote:

    On Wed, Mar 13, 2019 at 6:14 PM Andy Bunn <bunna at wwu.edu> wrote:
    
    > I would like to create averages of all the variables in a
    > SpatialPointsDataFrame when points are within a specified distance of each
    > other. I have a method for doing this but it seems like a silly way to
    > approach the problem. Any ideas for doing this using modern syntax
    > (especially of the tidy variety) would be appreciated.
    >
    >
    > To start, I have a SpatialPointsDataFrame with several variables measured
    > for each point. I'd like to get an average value for each variable for
    > points within a specified distance. E.g., getting average cadmium values
    > from the meuse data for points within 100 m of each other:
    >
    >     library(sf)
    >     library(sp)
    >     data(meuse)
    >     pts <- st_as_sf(meuse, coords = c("x", "y"), remove=FALSE)
    >     pts100 <- st_is_within_distance(pts, dist = 100)
    >     # can use sapply to get mean of a variable. E.g., cadmium
    >     sapply(pts100, function(x){ mean(pts$cadmium[x]) })
    >
    >
    If this is the method you call "silly" then I don't see anything silly at
    all here, only efficient well-written use of base R constructs. The problem
    with "modern" syntax is that its subject to rapid change and often slower
    than using base R, which has had years to stabilise and optimise.
    
    If you want to iterate this over variables then nest your sapplys:
    
    items = c("cadmium", "copper","lead")
    sapply(items, function(item){
     sapply(pts100, function(x){ mean(pts[[item]][x]) })
    })
    
    gets you:
    
             cadmium    copper      lead
      [1,] 10.150000  83.00000 288.00000
      [2,] 10.150000  83.00000 288.00000
      [3,]  6.500000  68.00000 199.00000
      [4,]  2.600000  81.00000 116.00000
    
    
    Barry
    
    
    > Above, I've figured out how to use sapply to do this variable by variable.
    > So I could, if I wanted, calculate the mean for each variable, generate a
    > centroid for each point and then a SpatialPointsDataFrame of the unique
    > values. E.g., for the first few variables:
    >
    >     res <- data.frame(id=1:length(pts100),
    >                       x=NA, y=NA,
    >                       cadmium=NA, copper=NA, lead=NA)
    >     res$x <- sapply(pts100, function(p){ mean(pts$x[p]) })
    >     res$y <- sapply(pts100, function(p){ mean(pts$y[p]) })
    >     res$cadmium <- sapply(pts100, function(p){ mean(pts$cadmium[p]) })
    >     res$copper <- sapply(pts100, function(p){ mean(pts$copper[p]) })
    >     res$lead <- sapply(pts100, function(p){ mean(pts$lead[p]) })
    >     res2 <- res[duplicated(res$cadmium),]
    >     coordinates(res2) <- c("x","y")
    >     bubble(res2,"cadmium")
    >
    >
    > This works but seems cumbersome and like there must be a more efficient
    > way.
    >
    >
    > Thanks for any help, Andy
    >
    >
    >
    > _______________________________________________
    > R-sig-Geo mailing list
    > R-sig-Geo at r-project.org
    > https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-geo&amp;data=02%7C01%7Cbunna%40wwu.edu%7C5470ab0ee3cb407f76ef08d6a7e2828f%7Cdc46140ce26f43efb0ae00f257f478ff%7C0%7C0%7C636880988634768989&amp;sdata=upduDGbDHMYznJ35Bv6sJZL8t3JBeJB%2FmCqgePjvmlo%3D&amp;reserved=0
    >
    
    	[[alternative HTML version deleted]]
    
    _______________________________________________
    R-sig-Geo mailing list
    R-sig-Geo at r-project.org
    https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-geo&amp;data=02%7C01%7Cbunna%40wwu.edu%7C5470ab0ee3cb407f76ef08d6a7e2828f%7Cdc46140ce26f43efb0ae00f257f478ff%7C0%7C0%7C636880988634768989&amp;sdata=upduDGbDHMYznJ35Bv6sJZL8t3JBeJB%2FmCqgePjvmlo%3D&amp;reserved=0
    


From dc@r|@on @end|ng |rom t@mu@edu  Thu Mar 14 22:01:35 2019
From: dc@r|@on @end|ng |rom t@mu@edu (David L Carlson)
Date: Thu, 14 Mar 2019 21:01:35 +0000
Subject: [R-sig-Geo] Error in running Maxent within biomod2
In-Reply-To: <CALN9TES7N-h2-=Vouj6WWryEZBkkgyKOsrFEFH1x_3MC452PzA@mail.gmail.com>
References: <CALN9TES7N-h2-=Vouj6WWryEZBkkgyKOsrFEFH1x_3MC452PzA@mail.gmail.com>
Message-ID: <1dfb9f45d6104eefa6bc05e533439c87@tamu.edu>

If you don't get an answer here, contact the biomod2 package maintainer. The email address is shown on following webpage:

https://cran.r-project.org/web/packages/biomod2/index.html

or you can use the R command:

maintainer("biomod2")

David L. Carlson
Department of Anthropology
Texas A&M University

-----Original Message-----
From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Lara Silva
Sent: Wednesday, March 13, 2019 6:17 PM
To: r-sig-ecology at r-project.org; r-sig-geo at r-project.org
Subject: [R-sig-Geo] Error in running Maxent within biomod2

I got an error when running Maxent within biomod2

 Running Maxent...Error in system(command = paste("java ",
ifelse(is.null(Options at MAXENT.Phillips$memory_allocated),  :

What does it means?

Thanks

	[[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From btupper @end|ng |rom b|ge|ow@org  Thu Mar 14 22:27:30 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Thu, 14 Mar 2019 17:27:30 -0400
Subject: [R-sig-Geo] Error in running Maxent within biomod2
In-Reply-To: <1dfb9f45d6104eefa6bc05e533439c87@tamu.edu>
References: <CALN9TES7N-h2-=Vouj6WWryEZBkkgyKOsrFEFH1x_3MC452PzA@mail.gmail.com>
 <1dfb9f45d6104eefa6bc05e533439c87@tamu.edu>
Message-ID: <C88886F6-820B-49F5-A89C-74657B257B5F@bigelow.org>

Hi,

You might want to verify that you have the required maxent jar file location properly identified.  See the MAXENT.Philips section here ...

https://www.rdocumentation.org/packages/biomod2/versions/3.3-7.1/topics/BIOMOD_ModelingOptions <https://www.rdocumentation.org/packages/biomod2/versions/3.3-7.1/topics/BIOMOD_ModelingOptions>

Cheers,
Ben



> On Mar 14, 2019, at 5:01 PM, David L Carlson <dcarlson at tamu.edu> wrote:
> 
> If you don't get an answer here, contact the biomod2 package maintainer. The email address is shown on following webpage:
> 
> https://cran.r-project.org/web/packages/biomod2/index.html
> 
> or you can use the R command:
> 
> maintainer("biomod2")
> 
> David L. Carlson
> Department of Anthropology
> Texas A&M University
> 
> -----Original Message-----
> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Lara Silva
> Sent: Wednesday, March 13, 2019 6:17 PM
> To: r-sig-ecology at r-project.org; r-sig-geo at r-project.org
> Subject: [R-sig-Geo] Error in running Maxent within biomod2
> 
> I got an error when running Maxent within biomod2
> 
> Running Maxent...Error in system(command = paste("java ",
> ifelse(is.null(Options at MAXENT.Phillips$memory_allocated),  :
> 
> What does it means?
> 
> Thanks
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/






	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Thu Mar 14 22:30:03 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 15 Mar 2019 10:30:03 +1300
Subject: [R-sig-Geo] [FORGED] Re:  Aggregating points based on distance
In-Reply-To: <CANVKczMqkHbnA73t+JBum8J5zttrLF5B7HsjBhLguB1xdiLWJw@mail.gmail.com>
References: <447BFEA7-6A5D-4196-8598-BD6DF34E7A9B@wwu.edu>
 <9A0D338D-7786-42F1-895C-6BFAD069D859@wwu.edu>
 <CANVKczMqkHbnA73t+JBum8J5zttrLF5B7HsjBhLguB1xdiLWJw@mail.gmail.com>
Message-ID: <b154a3d7-bab9-48eb-a7b6-5257621f2a2b@auckland.ac.nz>


On 14/03/19 7:33 AM, Barry Rowlingson wrote:

<SNIP>

> The problem with "modern" syntax is that it's subject to rapid change
> and often slower than using base R, which has had years to stabilise
> and optimise.

<SNIP>

Fortune nomination.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dexter@|ocke @end|ng |rom gm@||@com  Thu Mar 14 23:42:57 2019
From: dexter@|ocke @end|ng |rom gm@||@com (Dexter Locke)
Date: Thu, 14 Mar 2019 18:42:57 -0400
Subject: [R-sig-Geo] [FORGED] Re: Aggregating points based on distance
In-Reply-To: <b154a3d7-bab9-48eb-a7b6-5257621f2a2b@auckland.ac.nz>
References: <447BFEA7-6A5D-4196-8598-BD6DF34E7A9B@wwu.edu>
 <9A0D338D-7786-42F1-895C-6BFAD069D859@wwu.edu>
 <CANVKczMqkHbnA73t+JBum8J5zttrLF5B7HsjBhLguB1xdiLWJw@mail.gmail.com>
 <b154a3d7-bab9-48eb-a7b6-5257621f2a2b@auckland.ac.nz>
Message-ID: <CAA=SVwEn7UBHCTnL=VxUc-LjXC3ZKA4je4wP0Upyzj1dJ1L4_g@mail.gmail.com>

FWIW: I agree with Rolfs nomination.

+1

-Dexter
http://dexterlocke.com



On Thu, Mar 14, 2019 at 5:30 PM Rolf Turner <r.turner at auckland.ac.nz> wrote:

>
> On 14/03/19 7:33 AM, Barry Rowlingson wrote:
>
> <SNIP>
>
> > The problem with "modern" syntax is that it's subject to rapid change
> > and often slower than using base R, which has had years to stabilise
> > and optimise.
>
> <SNIP>
>
> Fortune nomination.
>
> cheers,
>
> Rolf
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From |@r@@@|p@@||v@ @end|ng |rom gm@||@com  Fri Mar 15 01:12:14 2019
From: |@r@@@|p@@||v@ @end|ng |rom gm@||@com (Lara Silva)
Date: Thu, 14 Mar 2019 23:12:14 -0100
Subject: [R-sig-Geo] Error in running Maxent within biomod2
In-Reply-To: <C88886F6-820B-49F5-A89C-74657B257B5F@bigelow.org>
References: <CALN9TES7N-h2-=Vouj6WWryEZBkkgyKOsrFEFH1x_3MC452PzA@mail.gmail.com>
 <1dfb9f45d6104eefa6bc05e533439c87@tamu.edu>
 <C88886F6-820B-49F5-A89C-74657B257B5F@bigelow.org>
Message-ID: <CALN9TETsNar_+0vCV6Y=TvO-+thTYudV-Jr-sn2q0OMMtJPUWQ@mail.gmail.com>

Thanks!

Ben Tupper <btupper at bigelow.org> escreveu no dia quinta, 14/03/2019 ?(s)
20:27:

> Hi,
>
> You might want to verify that you have the required maxent jar file
> location properly identified.  See the MAXENT.Philips section here ...
>
>
> https://www.rdocumentation.org/packages/biomod2/versions/3.3-7.1/topics/BIOMOD_ModelingOptions
>
> Cheers,
> Ben
>
>
>
> On Mar 14, 2019, at 5:01 PM, David L Carlson <dcarlson at tamu.edu> wrote:
>
> If you don't get an answer here, contact the biomod2 package maintainer.
> The email address is shown on following webpage:
>
> https://cran.r-project.org/web/packages/biomod2/index.html
>
> or you can use the R command:
>
> maintainer("biomod2")
>
> David L. Carlson
> Department of Anthropology
> Texas A&M University
>
> -----Original Message-----
> From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
> Lara Silva
> Sent: Wednesday, March 13, 2019 6:17 PM
> To: r-sig-ecology at r-project.org; r-sig-geo at r-project.org
> Subject: [R-sig-Geo] Error in running Maxent within biomod2
>
> I got an error when running Maxent within biomod2
>
> Running Maxent...Error in system(command = paste("java ",
> ifelse(is.null(Options at MAXENT.Phillips$memory_allocated),  :
>
> What does it means?
>
> Thanks
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
>
> Ecological Forecasting: https://eco.bigelow.org/
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From v|j@y|u||@ @end|ng |rom gm@||@com  Fri Mar 15 15:17:35 2019
From: v|j@y|u||@ @end|ng |rom gm@||@com (Vijay Lulla)
Date: Fri, 15 Mar 2019 10:17:35 -0400
Subject: [R-sig-Geo] [FORGED] Re: Aggregating points based on distance
In-Reply-To: <CAA=SVwEn7UBHCTnL=VxUc-LjXC3ZKA4je4wP0Upyzj1dJ1L4_g@mail.gmail.com>
References: <447BFEA7-6A5D-4196-8598-BD6DF34E7A9B@wwu.edu>
 <9A0D338D-7786-42F1-895C-6BFAD069D859@wwu.edu>
 <CANVKczMqkHbnA73t+JBum8J5zttrLF5B7HsjBhLguB1xdiLWJw@mail.gmail.com>
 <b154a3d7-bab9-48eb-a7b6-5257621f2a2b@auckland.ac.nz>
 <CAA=SVwEn7UBHCTnL=VxUc-LjXC3ZKA4je4wP0Upyzj1dJ1L4_g@mail.gmail.com>
Message-ID: <CAKkiGbtXYQ7fMDPMC+gVi9m+9XVBtQdTnE3jHSTv3=5zO3Fh4Q@mail.gmail.com>

+1 from me too for Rolf's nomination!

On Thu, Mar 14, 2019 at 6:43 PM Dexter Locke <dexter.locke at gmail.com> wrote:

> FWIW: I agree with Rolfs nomination.
>
> +1
>
> -Dexter
> http://dexterlocke.com
>
>
>
> On Thu, Mar 14, 2019 at 5:30 PM Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
>
> >
> > On 14/03/19 7:33 AM, Barry Rowlingson wrote:
> >
> > <SNIP>
> >
> > > The problem with "modern" syntax is that it's subject to rapid change
> > > and often slower than using base R, which has had years to stabilise
> > > and optimise.
> >
> > <SNIP>
> >
> > Fortune nomination.
> >
> > cheers,
> >
> > Rolf
> >
> > --
> > Honorary Research Fellow
> > Department of Statistics
> > University of Auckland
> > Phone: +64-9-373-7599 ext. 88276
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From |eon@rdom@erv|no @end|ng |rom gm@||@com  Sat Mar 16 16:03:07 2019
From: |eon@rdom@erv|no @end|ng |rom gm@||@com (Leonardo Matheus Servino)
Date: Sat, 16 Mar 2019 12:03:07 -0300
Subject: [R-sig-Geo] editing a correlogram
Message-ID: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>

Hello,

I've been tried to use the function "sp.correlogram". After plot the
correlogram, I would like to edit the grafic's appearence, to publish it.
It is possible?

Thanks

-- 
Leonardo Matheus Servino
P?s-Gradua??o em Evolu??o e Diversidade
Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e Humanas

LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202

Rua Arcturus, 3. Jardim Antares
09606-070 S?o Bernardo do Campo - SP

	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Sat Mar 16 17:48:01 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Sat, 16 Mar 2019 12:48:01 -0400
Subject: [R-sig-Geo] editing a correlogram
In-Reply-To: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>
References: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>
Message-ID: <CAM_vju=Kjvo2bNyn-W9L_fDTKfUBKUiVguDdqcGfiXoyJM_oLg@mail.gmail.com>

Of course.

The ... argument to the plot method means that you can use standard
base graphics options to customize as you wish.

?par gives the whole list, although they may not all be useful for correlograms.

If you have specific questions after you try customizing to your
liking, the list can certainly help with details.

Sarah

On Sat, Mar 16, 2019 at 11:00 AM Leonardo Matheus Servino
<leonardomservino at gmail.com> wrote:
>
> Hello,
>
> I've been tried to use the function "sp.correlogram". After plot the
> correlogram, I would like to edit the grafic's appearence, to publish it.
> It is possible?
>
> Thanks
>
> --
> Leonardo Matheus Servino
> P?s-Gradua??o em Evolu??o e Diversidade
> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e Humanas
>

-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From |eon@rdom@erv|no @end|ng |rom gm@||@com  Sat Mar 16 18:16:11 2019
From: |eon@rdom@erv|no @end|ng |rom gm@||@com (Leonardo Matheus Servino)
Date: Sat, 16 Mar 2019 14:16:11 -0300
Subject: [R-sig-Geo] editing a correlogram
In-Reply-To: <CAM_vju=Kjvo2bNyn-W9L_fDTKfUBKUiVguDdqcGfiXoyJM_oLg@mail.gmail.com>
References: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>
 <CAM_vju=Kjvo2bNyn-W9L_fDTKfUBKUiVguDdqcGfiXoyJM_oLg@mail.gmail.com>
Message-ID: <CA+TKu0Rc7+oR+1Z7zWN7P3yqfRyiSTuCudx91ytK0QrVGsLJ4Q@mail.gmail.com>

I tried the function par() and arguments inside the plot(), but some
parameters doesn't change.
For example, the argument pch=, which changes the symbols that represents
the points in the plot doesn't work.


Em s?b, 16 de mar de 2019 ?s 13:48, Sarah Goslee <sarah.goslee at gmail.com>
escreveu:

> Of course.
>
> The ... argument to the plot method means that you can use standard
> base graphics options to customize as you wish.
>
> ?par gives the whole list, although they may not all be useful for
> correlograms.
>
> If you have specific questions after you try customizing to your
> liking, the list can certainly help with details.
>
> Sarah
>
> On Sat, Mar 16, 2019 at 11:00 AM Leonardo Matheus Servino
> <leonardomservino at gmail.com> wrote:
> >
> > Hello,
> >
> > I've been tried to use the function "sp.correlogram". After plot the
> > correlogram, I would like to edit the grafic's appearence, to publish it.
> > It is possible?
> >
> > Thanks
> >
> > --
> > Leonardo Matheus Servino
> > P?s-Gradua??o em Evolu??o e Diversidade
> > Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e
> Humanas
> >
>
> --
> Sarah Goslee (she/her)
> http://www.numberwright.com
>


-- 
Leonardo Matheus Servino
P?s-Gradua??o em Evolu??o e Diversidade
Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e Humanas

LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202

Rua Arcturus, 3. Jardim Antares
09606-070 S?o Bernardo do Campo - SP

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat Mar 16 21:35:44 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 17 Mar 2019 09:35:44 +1300
Subject: [R-sig-Geo] [FORGED] Re:  editing a correlogram
In-Reply-To: <CA+TKu0Rc7+oR+1Z7zWN7P3yqfRyiSTuCudx91ytK0QrVGsLJ4Q@mail.gmail.com>
References: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>
 <CAM_vju=Kjvo2bNyn-W9L_fDTKfUBKUiVguDdqcGfiXoyJM_oLg@mail.gmail.com>
 <CA+TKu0Rc7+oR+1Z7zWN7P3yqfRyiSTuCudx91ytK0QrVGsLJ4Q@mail.gmail.com>
Message-ID: <be53706c-9463-0d43-aed3-3757662c07a2@auckland.ac.nz>


On 17/03/19 6:16 AM, Leonardo Matheus Servino wrote:

> I tried the function par() and arguments inside the plot(), but some
> parameters doesn't change.
> For example, the argument pch=, which changes the symbols that represents
> the points in the plot doesn't work.

If you want useful advice show the commands that you actually used, in 
the context of a *minimal reproducible example* (including the *data* 
involved, provided in such a way --- use dput()!!! --- that it can be 
accessed by your advisors.

cheers,

Rolf Turner


-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From btupper @end|ng |rom b|ge|ow@org  Sat Mar 16 22:02:19 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Sat, 16 Mar 2019 17:02:19 -0400
Subject: [R-sig-Geo] editing a correlogram
In-Reply-To: <CA+TKu0Rc7+oR+1Z7zWN7P3yqfRyiSTuCudx91ytK0QrVGsLJ4Q@mail.gmail.com>
References: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>
 <CAM_vju=Kjvo2bNyn-W9L_fDTKfUBKUiVguDdqcGfiXoyJM_oLg@mail.gmail.com>
 <CA+TKu0Rc7+oR+1Z7zWN7P3yqfRyiSTuCudx91ytK0QrVGsLJ4Q@mail.gmail.com>
Message-ID: <E82DE294-4B77-4871-813E-5DF898589329@bigelow.org>

Hi,

In this case the spdep::plot.spcor() function, which I think you are using, doesn't provide the mechanism for the caller to override the default pch value.  You can see this by looking at the pl.spcor function (as shown way below.)  I think it may be easiest for you to simply rewrite the function with the plotting parameters assigned as arguments.

Cheers,
Ben


> spdep::plot.spcor
function (x, main, ylab, ylim, ...) 
{
    if (missing(main)) 
        main <- x$var
    if ((x$method == "I") || (x$method == "C")) {
        lags <- as.integer(rownames(x$res))
        to.plot <- which((x$res[, 3] > 0) & (x$res[, 3] != Inf))
        sd2 <- rep(0, nrow(x$res))
        sd2[to.plot] <- 2 * sqrt(x$res[to.plot, 3])
        if (missing(ylim)) {
            ylim <- range(c(x$res[, 1] + sd2, x$res[, 1] - sd2))
        }
        if (missing(ylab)) 
            if (x$method == "I") 
                ylab <- "Moran's  I"
        if (x$method == "C") 
            ylab <- "Geary's  C"
        plot(lags, x$res[, 1], type = "p", pch = 18, ylim = ylim, 
            main = main, ylab = ylab, xaxt = "n")
        arrows(lags, x$res[, 1] + sd2, lags, x$res[, 1] - sd2, 
            length = 0.1, angle = 90)
        arrows(lags, x$res[, 1] - sd2, lags, x$res[, 1] + sd2, 
            length = 0.1, angle = 90)
        axis(1, at = lags)
        abline(h = x$res[1, 2])
    }
    else {
        res <- as.vector(x$res)
        lags <- as.integer(names(x$res))
        if (missing(ylim)) 
            ylim <- c(-1, 1)
        if (missing(ylab)) 
            ylab <- "Spatial autocorrelation"
        plot(lags, res, type = "h", ylim = ylim, main = main, 
            ylab = ylab, lwd = 4, xaxt = "n")
        axis(1, at = lags)
        abline(h = 0)
    }
}
<bytecode: 0x7fb8799d0e40>
<environment: namespace:spdep>



> On Mar 16, 2019, at 1:16 PM, Leonardo Matheus Servino <leonardomservino at gmail.com> wrote:
> 
> I tried the function par() and arguments inside the plot(), but some
> parameters doesn't change.
> For example, the argument pch=, which changes the symbols that represents
> the points in the plot doesn't work.
> 
> 
> Em s?b, 16 de mar de 2019 ?s 13:48, Sarah Goslee <sarah.goslee at gmail.com <mailto:sarah.goslee at gmail.com>>
> escreveu:
> 
>> Of course.
>> 
>> The ... argument to the plot method means that you can use standard
>> base graphics options to customize as you wish.
>> 
>> ?par gives the whole list, although they may not all be useful for
>> correlograms.
>> 
>> If you have specific questions after you try customizing to your
>> liking, the list can certainly help with details.
>> 
>> Sarah
>> 
>> On Sat, Mar 16, 2019 at 11:00 AM Leonardo Matheus Servino
>> <leonardomservino at gmail.com> wrote:
>>> 
>>> Hello,
>>> 
>>> I've been tried to use the function "sp.correlogram". After plot the
>>> correlogram, I would like to edit the grafic's appearence, to publish it.
>>> It is possible?
>>> 
>>> Thanks
>>> 
>>> --
>>> Leonardo Matheus Servino
>>> P?s-Gradua??o em Evolu??o e Diversidade
>>> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e
>> Humanas
>>> 
>> 
>> --
>> Sarah Goslee (she/her)
>> http://www.numberwright.com
>> 
> 
> 
> -- 
> Leonardo Matheus Servino
> P?s-Gradua??o em Evolu??o e Diversidade
> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e Humanas
> 
> LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202
> 
> Rua Arcturus, 3. Jardim Antares
> 09606-070 S?o Bernardo do Campo - SP
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo <https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/






	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Sat Mar 16 23:04:37 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Sat, 16 Mar 2019 23:04:37 +0100
Subject: [R-sig-Geo] editing a correlogram
In-Reply-To: <E82DE294-4B77-4871-813E-5DF898589329@bigelow.org>
References: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>
 <CAM_vju=Kjvo2bNyn-W9L_fDTKfUBKUiVguDdqcGfiXoyJM_oLg@mail.gmail.com>
 <CA+TKu0Rc7+oR+1Z7zWN7P3yqfRyiSTuCudx91ytK0QrVGsLJ4Q@mail.gmail.com>
 <E82DE294-4B77-4871-813E-5DF898589329@bigelow.org>
Message-ID: <alpine.LFD.2.21.1903162236310.30577@reclus.nhh.no>

On Sat, 16 Mar 2019, Ben Tupper wrote:

> Hi,
>
> In this case the spdep::plot.spcor() function, which I think you are 
> using, doesn't provide the mechanism for the caller to override the 
> default pch value.  You can see this by looking at the pl.spcor function 
> (as shown way below.)  I think it may be easiest for you to simply 
> rewrite the function with the plotting parameters assigned as arguments.

Thanks, indeed the ... should be passed through to plotting functions, 
and I'll look at doing this. My preference would be to extract the 
components of the returned object needed for customised plotting, not 
trying to finesse the plot method, which was always meant for guidance, 
like other diagnostic plots.

As Rolf said, an example of the code you are using (on a built-in data 
set) to show what you want would make things easier, say based on the help 
page example. Note that print method returns its calculations invisibly 
too.

nc.sids <- st_read(system.file("shapes/sids.shp", package="spData")[1], 
quiet=TRUE)
rn <- as.character(nc.sids$FIPS)
ncCC89_nb <- read.gal(system.file("weights/ncCC89.gal", 
package="spData")[1],
  region.id=rn)
ft.SID74 <- sqrt(1000)*(sqrt(nc.sids$SID74/nc.sids$BIR74) +
   sqrt((nc.sids$SID74+1)/nc.sids$BIR74))
tr.SIDS74 <- ft.SID74*sqrt(nc.sids$BIR74)
Ispc <- sp.correlogram(ncCC89_nb, tr.SIDS74, order=8, method="I",
  zero.policy=TRUE)
str(Ispc)
str(print(Ispc, "bonferroni"))
Ispc_b <- as.data.frame(print(Ispc, "bonferroni"))
Ispc_b$lag <- 1:8
library(ggplot2)
ggplot(Ispc_b) + geom_point(aes(x=lag, y=estimate))
   + geom_hline(yintercept=0)

Or whatever you feel like doing.

Roger

>
> Cheers,
> Ben
>
>
>> spdep::plot.spcor
> function (x, main, ylab, ylim, ...)
> {
>    if (missing(main))
>        main <- x$var
>    if ((x$method == "I") || (x$method == "C")) {
>        lags <- as.integer(rownames(x$res))
>        to.plot <- which((x$res[, 3] > 0) & (x$res[, 3] != Inf))
>        sd2 <- rep(0, nrow(x$res))
>        sd2[to.plot] <- 2 * sqrt(x$res[to.plot, 3])
>        if (missing(ylim)) {
>            ylim <- range(c(x$res[, 1] + sd2, x$res[, 1] - sd2))
>        }
>        if (missing(ylab))
>            if (x$method == "I")
>                ylab <- "Moran's  I"
>        if (x$method == "C")
>            ylab <- "Geary's  C"
>        plot(lags, x$res[, 1], type = "p", pch = 18, ylim = ylim,
>            main = main, ylab = ylab, xaxt = "n")
>        arrows(lags, x$res[, 1] + sd2, lags, x$res[, 1] - sd2,
>            length = 0.1, angle = 90)
>        arrows(lags, x$res[, 1] - sd2, lags, x$res[, 1] + sd2,
>            length = 0.1, angle = 90)
>        axis(1, at = lags)
>        abline(h = x$res[1, 2])
>    }
>    else {
>        res <- as.vector(x$res)
>        lags <- as.integer(names(x$res))
>        if (missing(ylim))
>            ylim <- c(-1, 1)
>        if (missing(ylab))
>            ylab <- "Spatial autocorrelation"
>        plot(lags, res, type = "h", ylim = ylim, main = main,
>            ylab = ylab, lwd = 4, xaxt = "n")
>        axis(1, at = lags)
>        abline(h = 0)
>    }
> }
> <bytecode: 0x7fb8799d0e40>
> <environment: namespace:spdep>
>
>
>
>> On Mar 16, 2019, at 1:16 PM, Leonardo Matheus Servino <leonardomservino at gmail.com> wrote:
>>
>> I tried the function par() and arguments inside the plot(), but some
>> parameters doesn't change.
>> For example, the argument pch=, which changes the symbols that represents
>> the points in the plot doesn't work.
>>
>>
>> Em s?b, 16 de mar de 2019 ?s 13:48, Sarah Goslee <sarah.goslee at gmail.com <mailto:sarah.goslee at gmail.com>>
>> escreveu:
>>
>>> Of course.
>>>
>>> The ... argument to the plot method means that you can use standard
>>> base graphics options to customize as you wish.
>>>
>>> ?par gives the whole list, although they may not all be useful for
>>> correlograms.
>>>
>>> If you have specific questions after you try customizing to your
>>> liking, the list can certainly help with details.
>>>
>>> Sarah
>>>
>>> On Sat, Mar 16, 2019 at 11:00 AM Leonardo Matheus Servino
>>> <leonardomservino at gmail.com> wrote:
>>>>
>>>> Hello,
>>>>
>>>> I've been tried to use the function "sp.correlogram". After plot the
>>>> correlogram, I would like to edit the grafic's appearence, to publish it.
>>>> It is possible?
>>>>
>>>> Thanks
>>>>
>>>> --
>>>> Leonardo Matheus Servino
>>>> P?s-Gradua??o em Evolu??o e Diversidade
>>>> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e
>>> Humanas
>>>>
>>>
>>> --
>>> Sarah Goslee (she/her)
>>> http://www.numberwright.com
>>>
>>
>>
>> --
>> Leonardo Matheus Servino
>> P?s-Gradua??o em Evolu??o e Diversidade
>> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e Humanas
>>
>> LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202
>>
>> Rua Arcturus, 3. Jardim Antares
>> 09606-070 S?o Bernardo do Campo - SP
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo <https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
>
> Ecological Forecasting: https://eco.bigelow.org/
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From m||@er|n1919 @end|ng |rom gm@||@com  Sun Mar 17 00:48:31 2019
From: m||@er|n1919 @end|ng |rom gm@||@com (Mingke Li)
Date: Sat, 16 Mar 2019 20:48:31 -0300
Subject: [R-sig-Geo] Correlograms of Moran's I with progressive Bonferroni
 correction
Message-ID: <CALZ9tR8KB5fQSKpoR8Q=0cOz93YUTiEyQJKoaYObzVNt-NBJaw@mail.gmail.com>

Dear list,

I am trying to generate a correlogram of Moran's I, with symbols showing if
the coefficient is significant or not after the progressive Bonferroni
correction. Now I'm using the "correlog" function in the package ncf to
calculate the coefficients among all distance classes. I symbolized the
dots in the plot to show if they are significant or not (after progressive
Bonferroni correction), say, open circles for non-significant coefficients,
and solid circles for significant coefficients.

These are the codes I used in R:

>correlog.L2.2014new <- correlog(Grid.2014.all$Lng, Grid.2014.all$Lat,
Grid.2014.all$L2_Average,latlon = T, na.rm=T, increment=5, resamp=200)
>for (i in list(1:length(correlog.L2.2014new$p))) {
  correlog.L2.2014new$adjp[i]=correlog.L2.2014new$p[i]*i
}
>plot(correlog.L2.2014new$correlation[1:80], type="b", cex=1.5, lwd=1.5,
ylim=c(-0.1,0.7),
     pch=ifelse(((correlog.L2.2014new$adjp>0.05)),1,16),
     xlab="Distance (km)", ylab="Moran's I", cex.lab=2, cex.axis=1.5);
abline(h=0)

I also found the function "sp.correlogram" in the package spdep to make the
correlogram with Bonferroni correction.

Now I have three questions:

1. Can I set a "up-limit" to the "correlog" function? For example, if I
just need the first 80 distance classes to generate the graph (as shown in
the plot code), is it possible to let the "correlog" function skip the rest
distance classes to save the computation time?

2. Is my code of generating p values after the progressive Bonferroni
correction correct? If not, how should I fix it?

3. Now I'm kind of correcting the p value manually based on my
understanding of progressive Bonferroni correction. Is there any function
in any R packages for creating correlograms with progressive Bonferroni
correction? "sp.correlogram"  has a parameter setting "p.adj.method" but it
seems do not have a progressive Bonferroni correction option?

I just start to work with the correlograms recently; any advice or
solutions are welcome. Thank you in advance!

Erin

	[[alternative HTML version deleted]]


From |eon@rdom@erv|no @end|ng |rom gm@||@com  Sun Mar 17 21:07:41 2019
From: |eon@rdom@erv|no @end|ng |rom gm@||@com (Leonardo Matheus Servino)
Date: Sun, 17 Mar 2019 17:07:41 -0300
Subject: [R-sig-Geo] editing a correlogram
In-Reply-To: <alpine.LFD.2.21.1903162236310.30577@reclus.nhh.no>
References: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>
 <CAM_vju=Kjvo2bNyn-W9L_fDTKfUBKUiVguDdqcGfiXoyJM_oLg@mail.gmail.com>
 <CA+TKu0Rc7+oR+1Z7zWN7P3yqfRyiSTuCudx91ytK0QrVGsLJ4Q@mail.gmail.com>
 <E82DE294-4B77-4871-813E-5DF898589329@bigelow.org>
 <alpine.LFD.2.21.1903162236310.30577@reclus.nhh.no>
Message-ID: <CA+TKu0TNzt_J4ZmjdKSZC91JFC_2WbE5SFdSp-65FzUpUWYkTw@mail.gmail.com>

My script:

library(spdep)
setwd('C:/Dados')
dados_I_langsdorffii<-read.csv('I_langsdorffii.csv', sep=';')
coords<-cbind(dados_I_langsdorffii$LONG, dados_I_langsdorffii$LAT)
coords1<-as.matrix(coords)
source("plot.links.r")
plot.links(coords, thresh = 0.5, text = F)
vizinho<-dnearneigh(as.matrix(coords1), 0, 0.5)
summary(vizinho)
corre.sp <- sp.correlogram(vizinho, dados_I_langsdorffii$MEDIANA_CRC, order
=5, method = "I",zero.policy = TRUE)
print(corre.sp, p.adj.method = "holm")
par(adj="0.5",family="serif",cex.axis="1.5",xaxs="r",cex.axis="2.5",cex.lab="2.5",las="1",tcl="-0.25",lwd="1.8",
ps="5", mex="0.7", lty="blank")
plot(corre.sp, p.adj.method = "holm", main="",ylim=c(-1,1))

Em s?b, 16 de mar de 2019 ?s 19:04, Roger Bivand <Roger.Bivand at nhh.no>
escreveu:

> On Sat, 16 Mar 2019, Ben Tupper wrote:
>
> > Hi,
> >
> > In this case the spdep::plot.spcor() function, which I think you are
> > using, doesn't provide the mechanism for the caller to override the
> > default pch value.  You can see this by looking at the pl.spcor function
> > (as shown way below.)  I think it may be easiest for you to simply
> > rewrite the function with the plotting parameters assigned as arguments.
>
> Thanks, indeed the ... should be passed through to plotting functions,
> and I'll look at doing this. My preference would be to extract the
> components of the returned object needed for customised plotting, not
> trying to finesse the plot method, which was always meant for guidance,
> like other diagnostic plots.
>
> As Rolf said, an example of the code you are using (on a built-in data
> set) to show what you want would make things easier, say based on the help
> page example. Note that print method returns its calculations invisibly
> too.
>
> nc.sids <- st_read(system.file("shapes/sids.shp", package="spData")[1],
> quiet=TRUE)
> rn <- as.character(nc.sids$FIPS)
> ncCC89_nb <- read.gal(system.file("weights/ncCC89.gal",
> package="spData")[1],
>   region.id=rn)
> ft.SID74 <- sqrt(1000)*(sqrt(nc.sids$SID74/nc.sids$BIR74) +
>    sqrt((nc.sids$SID74+1)/nc.sids$BIR74))
> tr.SIDS74 <- ft.SID74*sqrt(nc.sids$BIR74)
> Ispc <- sp.correlogram(ncCC89_nb, tr.SIDS74, order=8, method="I",
>   zero.policy=TRUE)
> str(Ispc)
> str(print(Ispc, "bonferroni"))
> Ispc_b <- as.data.frame(print(Ispc, "bonferroni"))
> Ispc_b$lag <- 1:8
> library(ggplot2)
> ggplot(Ispc_b) + geom_point(aes(x=lag, y=estimate))
>    + geom_hline(yintercept=0)
>
> Or whatever you feel like doing.
>
> Roger
>
> >
> > Cheers,
> > Ben
> >
> >
> >> spdep::plot.spcor
> > function (x, main, ylab, ylim, ...)
> > {
> >    if (missing(main))
> >        main <- x$var
> >    if ((x$method == "I") || (x$method == "C")) {
> >        lags <- as.integer(rownames(x$res))
> >        to.plot <- which((x$res[, 3] > 0) & (x$res[, 3] != Inf))
> >        sd2 <- rep(0, nrow(x$res))
> >        sd2[to.plot] <- 2 * sqrt(x$res[to.plot, 3])
> >        if (missing(ylim)) {
> >            ylim <- range(c(x$res[, 1] + sd2, x$res[, 1] - sd2))
> >        }
> >        if (missing(ylab))
> >            if (x$method == "I")
> >                ylab <- "Moran's  I"
> >        if (x$method == "C")
> >            ylab <- "Geary's  C"
> >        plot(lags, x$res[, 1], type = "p", pch = 18, ylim = ylim,
> >            main = main, ylab = ylab, xaxt = "n")
> >        arrows(lags, x$res[, 1] + sd2, lags, x$res[, 1] - sd2,
> >            length = 0.1, angle = 90)
> >        arrows(lags, x$res[, 1] - sd2, lags, x$res[, 1] + sd2,
> >            length = 0.1, angle = 90)
> >        axis(1, at = lags)
> >        abline(h = x$res[1, 2])
> >    }
> >    else {
> >        res <- as.vector(x$res)
> >        lags <- as.integer(names(x$res))
> >        if (missing(ylim))
> >            ylim <- c(-1, 1)
> >        if (missing(ylab))
> >            ylab <- "Spatial autocorrelation"
> >        plot(lags, res, type = "h", ylim = ylim, main = main,
> >            ylab = ylab, lwd = 4, xaxt = "n")
> >        axis(1, at = lags)
> >        abline(h = 0)
> >    }
> > }
> > <bytecode: 0x7fb8799d0e40>
> > <environment: namespace:spdep>
> >
> >
> >
> >> On Mar 16, 2019, at 1:16 PM, Leonardo Matheus Servino <
> leonardomservino at gmail.com> wrote:
> >>
> >> I tried the function par() and arguments inside the plot(), but some
> >> parameters doesn't change.
> >> For example, the argument pch=, which changes the symbols that
> represents
> >> the points in the plot doesn't work.
> >>
> >>
> >> Em s?b, 16 de mar de 2019 ?s 13:48, Sarah Goslee <
> sarah.goslee at gmail.com <mailto:sarah.goslee at gmail.com>>
> >> escreveu:
> >>
> >>> Of course.
> >>>
> >>> The ... argument to the plot method means that you can use standard
> >>> base graphics options to customize as you wish.
> >>>
> >>> ?par gives the whole list, although they may not all be useful for
> >>> correlograms.
> >>>
> >>> If you have specific questions after you try customizing to your
> >>> liking, the list can certainly help with details.
> >>>
> >>> Sarah
> >>>
> >>> On Sat, Mar 16, 2019 at 11:00 AM Leonardo Matheus Servino
> >>> <leonardomservino at gmail.com> wrote:
> >>>>
> >>>> Hello,
> >>>>
> >>>> I've been tried to use the function "sp.correlogram". After plot the
> >>>> correlogram, I would like to edit the grafic's appearence, to publish
> it.
> >>>> It is possible?
> >>>>
> >>>> Thanks
> >>>>
> >>>> --
> >>>> Leonardo Matheus Servino
> >>>> P?s-Gradua??o em Evolu??o e Diversidade
> >>>> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e
> >>> Humanas
> >>>>
> >>>
> >>> --
> >>> Sarah Goslee (she/her)
> >>> http://www.numberwright.com
> >>>
> >>
> >>
> >> --
> >> Leonardo Matheus Servino
> >> P?s-Gradua??o em Evolu??o e Diversidade
> >> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e
> Humanas
> >>
> >> LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202
> >>
> >> Rua Arcturus, 3. Jardim Antares
> >> 09606-070 S?o Bernardo do Campo - SP
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo <
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
> > Ben Tupper
> > Bigelow Laboratory for Ocean Sciences
> > 60 Bigelow Drive, P.O. Box 380
> > East Boothbay, Maine 04544
> > http://www.bigelow.org
> >
> > Ecological Forecasting: https://eco.bigelow.org/
> >
> >
> >
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en



-- 
Leonardo Matheus Servino
P?s-Gradua??o em Evolu??o e Diversidade
Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e Humanas

LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202

Rua Arcturus, 3. Jardim Antares
09606-070 S?o Bernardo do Campo - SP

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Mar 18 09:35:53 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 18 Mar 2019 09:35:53 +0100
Subject: [R-sig-Geo] editing a correlogram
In-Reply-To: <CA+TKu0TNzt_J4ZmjdKSZC91JFC_2WbE5SFdSp-65FzUpUWYkTw@mail.gmail.com>
References: <CA+TKu0TQxPg+tg2=E320XfB2wzghPf2MyhF5KDfGXCok6RKF1w@mail.gmail.com>
 <CAM_vju=Kjvo2bNyn-W9L_fDTKfUBKUiVguDdqcGfiXoyJM_oLg@mail.gmail.com>
 <CA+TKu0Rc7+oR+1Z7zWN7P3yqfRyiSTuCudx91ytK0QrVGsLJ4Q@mail.gmail.com>
 <E82DE294-4B77-4871-813E-5DF898589329@bigelow.org>
 <alpine.LFD.2.21.1903162236310.30577@reclus.nhh.no>
 <CA+TKu0TNzt_J4ZmjdKSZC91JFC_2WbE5SFdSp-65FzUpUWYkTw@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903180921000.26249@reclus.nhh.no>

On Sun, 17 Mar 2019, Leonardo Matheus Servino wrote:

> My script:

No, not your script, a *reproducible* example, using a built-in data set. 
Nobody can run your code without your data, and we don't need your data. 
Did you read my reply? Why have you quoted the numerical arguments - a 
clear error?

For me:

> par(adj=0.5, family="serif", cex.axis=1.5, xaxs="r", cex.axis=2.5,
+  cex.lab=2.5, las=1, tcl=-0.25, lwd=1.8, ps=5, mex=0.7, lty="blank")
> plot(Ispc)

does do something, but whether this is what you consider prettier isn't my 
problem. If you prefer, take the output of the print method and use any 
plotting functionality you chose.

Roger

>
> library(spdep)
> setwd('C:/Dados')
> dados_I_langsdorffii<-read.csv('I_langsdorffii.csv', sep=';')
> coords<-cbind(dados_I_langsdorffii$LONG, dados_I_langsdorffii$LAT)
> coords1<-as.matrix(coords)
> source("plot.links.r")
> plot.links(coords, thresh = 0.5, text = F)
> vizinho<-dnearneigh(as.matrix(coords1), 0, 0.5)
> summary(vizinho)
> corre.sp <- sp.correlogram(vizinho, dados_I_langsdorffii$MEDIANA_CRC, order
> =5, method = "I",zero.policy = TRUE)
> print(corre.sp, p.adj.method = "holm")
> par(adj="0.5",family="serif",cex.axis="1.5",xaxs="r",cex.axis="2.5",cex.lab="2.5",las="1",tcl="-0.25",lwd="1.8",
> ps="5", mex="0.7", lty="blank")
> plot(corre.sp, p.adj.method = "holm", main="",ylim=c(-1,1))
>
> Em s?b, 16 de mar de 2019 ?s 19:04, Roger Bivand <Roger.Bivand at nhh.no>
> escreveu:
>
>> On Sat, 16 Mar 2019, Ben Tupper wrote:
>>
>>> Hi,
>>>
>>> In this case the spdep::plot.spcor() function, which I think you are
>>> using, doesn't provide the mechanism for the caller to override the
>>> default pch value.  You can see this by looking at the pl.spcor function
>>> (as shown way below.)  I think it may be easiest for you to simply
>>> rewrite the function with the plotting parameters assigned as arguments.
>>
>> Thanks, indeed the ... should be passed through to plotting functions,
>> and I'll look at doing this. My preference would be to extract the
>> components of the returned object needed for customised plotting, not
>> trying to finesse the plot method, which was always meant for guidance,
>> like other diagnostic plots.
>>
>> As Rolf said, an example of the code you are using (on a built-in data
>> set) to show what you want would make things easier, say based on the help
>> page example. Note that print method returns its calculations invisibly
>> too.
>>
>> nc.sids <- st_read(system.file("shapes/sids.shp", package="spData")[1],
>> quiet=TRUE)
>> rn <- as.character(nc.sids$FIPS)
>> ncCC89_nb <- read.gal(system.file("weights/ncCC89.gal",
>> package="spData")[1],
>>   region.id=rn)
>> ft.SID74 <- sqrt(1000)*(sqrt(nc.sids$SID74/nc.sids$BIR74) +
>>    sqrt((nc.sids$SID74+1)/nc.sids$BIR74))
>> tr.SIDS74 <- ft.SID74*sqrt(nc.sids$BIR74)
>> Ispc <- sp.correlogram(ncCC89_nb, tr.SIDS74, order=8, method="I",
>>   zero.policy=TRUE)
>> str(Ispc)
>> str(print(Ispc, "bonferroni"))
>> Ispc_b <- as.data.frame(print(Ispc, "bonferroni"))
>> Ispc_b$lag <- 1:8
>> library(ggplot2)
>> ggplot(Ispc_b) + geom_point(aes(x=lag, y=estimate))
>>    + geom_hline(yintercept=0)
>>
>> Or whatever you feel like doing.
>>
>> Roger
>>
>>>
>>> Cheers,
>>> Ben
>>>
>>>
>>>> spdep::plot.spcor
>>> function (x, main, ylab, ylim, ...)
>>> {
>>>    if (missing(main))
>>>        main <- x$var
>>>    if ((x$method == "I") || (x$method == "C")) {
>>>        lags <- as.integer(rownames(x$res))
>>>        to.plot <- which((x$res[, 3] > 0) & (x$res[, 3] != Inf))
>>>        sd2 <- rep(0, nrow(x$res))
>>>        sd2[to.plot] <- 2 * sqrt(x$res[to.plot, 3])
>>>        if (missing(ylim)) {
>>>            ylim <- range(c(x$res[, 1] + sd2, x$res[, 1] - sd2))
>>>        }
>>>        if (missing(ylab))
>>>            if (x$method == "I")
>>>                ylab <- "Moran's  I"
>>>        if (x$method == "C")
>>>            ylab <- "Geary's  C"
>>>        plot(lags, x$res[, 1], type = "p", pch = 18, ylim = ylim,
>>>            main = main, ylab = ylab, xaxt = "n")
>>>        arrows(lags, x$res[, 1] + sd2, lags, x$res[, 1] - sd2,
>>>            length = 0.1, angle = 90)
>>>        arrows(lags, x$res[, 1] - sd2, lags, x$res[, 1] + sd2,
>>>            length = 0.1, angle = 90)
>>>        axis(1, at = lags)
>>>        abline(h = x$res[1, 2])
>>>    }
>>>    else {
>>>        res <- as.vector(x$res)
>>>        lags <- as.integer(names(x$res))
>>>        if (missing(ylim))
>>>            ylim <- c(-1, 1)
>>>        if (missing(ylab))
>>>            ylab <- "Spatial autocorrelation"
>>>        plot(lags, res, type = "h", ylim = ylim, main = main,
>>>            ylab = ylab, lwd = 4, xaxt = "n")
>>>        axis(1, at = lags)
>>>        abline(h = 0)
>>>    }
>>> }
>>> <bytecode: 0x7fb8799d0e40>
>>> <environment: namespace:spdep>
>>>
>>>
>>>
>>>> On Mar 16, 2019, at 1:16 PM, Leonardo Matheus Servino <
>> leonardomservino at gmail.com> wrote:
>>>>
>>>> I tried the function par() and arguments inside the plot(), but some
>>>> parameters doesn't change.
>>>> For example, the argument pch=, which changes the symbols that
>> represents
>>>> the points in the plot doesn't work.
>>>>
>>>>
>>>> Em s?b, 16 de mar de 2019 ?s 13:48, Sarah Goslee <
>> sarah.goslee at gmail.com <mailto:sarah.goslee at gmail.com>>
>>>> escreveu:
>>>>
>>>>> Of course.
>>>>>
>>>>> The ... argument to the plot method means that you can use standard
>>>>> base graphics options to customize as you wish.
>>>>>
>>>>> ?par gives the whole list, although they may not all be useful for
>>>>> correlograms.
>>>>>
>>>>> If you have specific questions after you try customizing to your
>>>>> liking, the list can certainly help with details.
>>>>>
>>>>> Sarah
>>>>>
>>>>> On Sat, Mar 16, 2019 at 11:00 AM Leonardo Matheus Servino
>>>>> <leonardomservino at gmail.com> wrote:
>>>>>>
>>>>>> Hello,
>>>>>>
>>>>>> I've been tried to use the function "sp.correlogram". After plot the
>>>>>> correlogram, I would like to edit the grafic's appearence, to publish
>> it.
>>>>>> It is possible?
>>>>>>
>>>>>> Thanks
>>>>>>
>>>>>> --
>>>>>> Leonardo Matheus Servino
>>>>>> P?s-Gradua??o em Evolu??o e Diversidade
>>>>>> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e
>>>>> Humanas
>>>>>>
>>>>>
>>>>> --
>>>>> Sarah Goslee (she/her)
>>>>> http://www.numberwright.com
>>>>>
>>>>
>>>>
>>>> --
>>>> Leonardo Matheus Servino
>>>> P?s-Gradua??o em Evolu??o e Diversidade
>>>> Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e
>> Humanas
>>>>
>>>> LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202
>>>>
>>>> Rua Arcturus, 3. Jardim Antares
>>>> 09606-070 S?o Bernardo do Campo - SP
>>>>
>>>>      [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo <
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
>>> Ben Tupper
>>> Bigelow Laboratory for Ocean Sciences
>>> 60 Bigelow Drive, P.O. Box 380
>>> East Boothbay, Maine 04544
>>> http://www.bigelow.org
>>>
>>> Ecological Forecasting: https://eco.bigelow.org/
>>>
>>>
>>>
>>>
>>>
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger@B|v@nd @end|ng |rom nhh@no  Mon Mar 18 09:41:22 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 18 Mar 2019 09:41:22 +0100
Subject: [R-sig-Geo] 
 Correlograms of Moran's I with progressive Bonferroni correction
In-Reply-To: <CALZ9tR8KB5fQSKpoR8Q=0cOz93YUTiEyQJKoaYObzVNt-NBJaw@mail.gmail.com>
References: <CALZ9tR8KB5fQSKpoR8Q=0cOz93YUTiEyQJKoaYObzVNt-NBJaw@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903180936200.26249@reclus.nhh.no>

On Sun, 17 Mar 2019, Mingke Li wrote:

> Dear list,
>
> I am trying to generate a correlogram of Moran's I, with symbols showing if
> the coefficient is significant or not after the progressive Bonferroni
> correction. Now I'm using the "correlog" function in the package ncf to
> calculate the coefficients among all distance classes. I symbolized the
> dots in the plot to show if they are significant or not (after progressive
> Bonferroni correction), say, open circles for non-significant coefficients,
> and solid circles for significant coefficients.

Explain what you mean here by progressive Bonferroni correction, please. 
Could you rather see what is done in the mpmcorrelogram package, and 
report back if it makes sense?

Roger

>
> These are the codes I used in R:
>
>> correlog.L2.2014new <- correlog(Grid.2014.all$Lng, Grid.2014.all$Lat,
> Grid.2014.all$L2_Average,latlon = T, na.rm=T, increment=5, resamp=200)
>> for (i in list(1:length(correlog.L2.2014new$p))) {
>  correlog.L2.2014new$adjp[i]=correlog.L2.2014new$p[i]*i
> }
>> plot(correlog.L2.2014new$correlation[1:80], type="b", cex=1.5, lwd=1.5,
> ylim=c(-0.1,0.7),
>     pch=ifelse(((correlog.L2.2014new$adjp>0.05)),1,16),
>     xlab="Distance (km)", ylab="Moran's I", cex.lab=2, cex.axis=1.5);
> abline(h=0)
>
> I also found the function "sp.correlogram" in the package spdep to make the
> correlogram with Bonferroni correction.
>
> Now I have three questions:
>
> 1. Can I set a "up-limit" to the "correlog" function? For example, if I
> just need the first 80 distance classes to generate the graph (as shown in
> the plot code), is it possible to let the "correlog" function skip the rest
> distance classes to save the computation time?
>
> 2. Is my code of generating p values after the progressive Bonferroni
> correction correct? If not, how should I fix it?
>
> 3. Now I'm kind of correcting the p value manually based on my
> understanding of progressive Bonferroni correction. Is there any function
> in any R packages for creating correlograms with progressive Bonferroni
> correction? "sp.correlogram"  has a parameter setting "p.adj.method" but it
> seems do not have a progressive Bonferroni correction option?
>
> I just start to work with the correlograms recently; any advice or
> solutions are welcome. Thank you in advance!
>
> Erin
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |@r@@@|p@@||v@ @end|ng |rom gm@||@com  Mon Mar 18 12:28:14 2019
From: |@r@@@|p@@||v@ @end|ng |rom gm@||@com (Lara Silva)
Date: Mon, 18 Mar 2019 10:28:14 -0100
Subject: [R-sig-Geo] Error in run GAM (Biomod 2)
Message-ID: <CALN9TERQrqKBNqseuJnmBr-9hJW1=kAwW8t6012TABFCt=jhhQ@mail.gmail.com>

Hello

I am trying to run several algorithms in biomod 2 (GLM, GAM, ANN, SRE) but
I received the following menssage.

Model=GAM
         GAM_mgcv algorithm chosen
        Automatic formula generation...
        > GAM (mgcv) modelling...Error in
smooth.construct.tp.smooth.spec(object, dk$data, dk$knots) :
  A term has fewer unique covariate combinations than specified maximum
degrees of freedom
Error in predict(model.bm, Data[, expl_var_names, drop = FALSE], on_0_1000
= TRUE) :
  object 'model.bm' not found

*** inherits(g.pred,'try-error')
   ! Note :  Lactuca.global_AllData_RUN1_GAM failed!

I have 19 presences and 1019 pseudo-absences ...
It is because the low number of presences?

Thanks

Lara

	[[alternative HTML version deleted]]


From |@r@@@|p@@||v@ @end|ng |rom gm@||@com  Mon Mar 18 14:04:22 2019
From: |@r@@@|p@@||v@ @end|ng |rom gm@||@com (Lara Silva)
Date: Mon, 18 Mar 2019 12:04:22 -0100
Subject: [R-sig-Geo] [R-sig-eco] Error in run GAM (Biomod 2)
In-Reply-To: <9E338616-959D-43CA-A6DB-A3E2E02797AC@unl.edu>
References: <CALN9TERQrqKBNqseuJnmBr-9hJW1=kAwW8t6012TABFCt=jhhQ@mail.gmail.com>
 <9E338616-959D-43CA-A6DB-A3E2E02797AC@unl.edu>
Message-ID: <CALN9TEQ7GdMWXCcRSYmsj0ZEYZpcnzpauMTC=uZ=f0WBKdv8vg@mail.gmail.com>

Thanks for the information.

Drew Tyre <atyre2 at unl.edu> escreveu no dia segunda, 18/03/2019 ?(s) 11:48:

> I don't think the number of presences is the problem.
> "A term has fewer unique covariate combinations than specified maximum
> degrees of freedom"
> One of your covariates has a small number of unique values.
> lapply(your_data_frame, function(x)length(unique(x)))
> will print out the number of unique values in each variable. Values < 30
> could be problematic -- the docs are a bit ambiguous on the default values
> of k, the maximum degrees of freedom for a smooth term. Another approach to
> diagnose the problem is to set k to a low value, like 5, and see if that
> makes the problem go away. I'm not familiar with biomod2, but I think doing
> something like
> MO <- BIOMOD_ModelingOptions(GAM  = list(k = 5))
> BIOMOD_Modeling( ..., models.options = MO, ...)
>
> should do it. Note that setting k to a low value causes other problems, I
> would use mcgv::gam.check() to ensure k is large enough, but I don't know
> how to do that with biomod2.
>
> hth
>
> --
> Drew Tyre
>
> School of Natural Resources
> University of Nebraska-Lincoln
> 416 Hardin Hall, East Campus
> 3310 Holdrege Street
> Lincoln, NE 68583-0974
>
> phone: +1 402 472 4054
> fax: +1 402 472 2946
> email: atyre2 at unl.edu
> http://snr.unl.edu/tyre
> http://drewtyre.rbind.io
>     The point is that our true nature is not some ideal that we have to
> live up to. It?s who we are right now, and that?s what we can make friends
> with and celebrate.
> Excerpted from: Awakening Loving-Kindness by Pema Ch?dr?n
>
>
> ?On 3/18/19, 6:28 AM, "R-sig-ecology on behalf of Lara Silva" <
> r-sig-ecology-bounces at r-project.org on behalf of lara.sfp.silva at gmail.com>
> wrote:
>
>     Hello
>
>     I am trying to run several algorithms in biomod 2 (GLM, GAM, ANN, SRE)
> but
>     I received the following menssage.
>
>     Model=GAM
>              GAM_mgcv algorithm chosen
>             Automatic formula generation...
>             > GAM (mgcv) modelling...Error in
>     smooth.construct.tp.smooth.spec(object, dk$data, dk$knots) :
>       A term has fewer unique covariate combinations than specified maximum
>     degrees of freedom
>     Error in predict(model.bm, Data[, expl_var_names, drop = FALSE],
> on_0_1000
>     = TRUE) :
>       object 'model.bm' not found
>
>     *** inherits(g.pred,'try-error')
>        ! Note :  Lactuca.global_AllData_RUN1_GAM failed!
>
>     I have 19 presences and 1019 pseudo-absences ...
>     It is because the low number of presences?
>
>     Thanks
>
>     Lara
>
>         [[alternative HTML version deleted]]
>
>     _______________________________________________
>     R-sig-ecology mailing list
>     R-sig-ecology at r-project.org
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Decology&d=DwICAg&c=Cu5g146wZdoqVuKpTNsYHeFX_rg6kWhlkLF8Eft-wwo&r=aLEaSryyUcERVqcKVZl7lQ&m=2aNX9-kGSZ1EivodV2ZAVeJGY76pX4TqGpx2QXqsx00&s=-syyYDDESQAcvQx2Khl1KE_jbj3yBD2UqgKcFE4mBpw&e=
>
>
>

	[[alternative HTML version deleted]]


From m||@er|n1919 @end|ng |rom gm@||@com  Mon Mar 18 14:59:02 2019
From: m||@er|n1919 @end|ng |rom gm@||@com (Mingke Li)
Date: Mon, 18 Mar 2019 10:59:02 -0300
Subject: [R-sig-Geo] 
 Correlograms of Moran's I with progressive Bonferroni correction
In-Reply-To: <alpine.LFD.2.21.1903180936200.26249@reclus.nhh.no>
References: <CALZ9tR8KB5fQSKpoR8Q=0cOz93YUTiEyQJKoaYObzVNt-NBJaw@mail.gmail.com>
 <alpine.LFD.2.21.1903180936200.26249@reclus.nhh.no>
Message-ID: <CALZ9tR9kFoN6+3sEuz5Faa1TuK=VHFWAt69cY0g74+GqhD6Cig@mail.gmail.com>

Hi Roger,

According to the book "Numerical Ecology" by Legendre&Legendre in 2012, a
progressive (sequential) Bonferroni correction is a modified version of
Bonferroni correction, where the Bonferroni-corrected level is computed for
each distance class separately instead of using a consistent level for all
distance classes. This is done by using for each distance class the number
of tests actually performed up to that distance class. For example, if we
have 10 distance classes, and we use a=0.05 as a probability level, then
the adjusted a (a') for the first distance class is a'=a/1=0.05, for the
second distance class is a'=a/2=0.025 ... for the 10th distance class is
a'=a/10=0.005.

I've checked the mpmcorrelogram package, it seems that this package is for
creating multivariate correlograms. But what I hope to get is correlograms
of autocorrelation over one variate (e.g. Moran's I).

Hope this is clearer. Thank you for your help.

Mingke

On Mon, Mar 18, 2019 at 5:41 AM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Sun, 17 Mar 2019, Mingke Li wrote:
>
> > Dear list,
> >
> > I am trying to generate a correlogram of Moran's I, with symbols showing
> if
> > the coefficient is significant or not after the progressive Bonferroni
> > correction. Now I'm using the "correlog" function in the package ncf to
> > calculate the coefficients among all distance classes. I symbolized the
> > dots in the plot to show if they are significant or not (after
> progressive
> > Bonferroni correction), say, open circles for non-significant
> coefficients,
> > and solid circles for significant coefficients.
>
> Explain what you mean here by progressive Bonferroni correction, please.
> Could you rather see what is done in the mpmcorrelogram package, and
> report back if it makes sense?
>
> Roger
>
> >
> > These are the codes I used in R:
> >
> >> correlog.L2.2014new <- correlog(Grid.2014.all$Lng, Grid.2014.all$Lat,
> > Grid.2014.all$L2_Average,latlon = T, na.rm=T, increment=5, resamp=200)
> >> for (i in list(1:length(correlog.L2.2014new$p))) {
> >  correlog.L2.2014new$adjp[i]=correlog.L2.2014new$p[i]*i
> > }
> >> plot(correlog.L2.2014new$correlation[1:80], type="b", cex=1.5, lwd=1.5,
> > ylim=c(-0.1,0.7),
> >     pch=ifelse(((correlog.L2.2014new$adjp>0.05)),1,16),
> >     xlab="Distance (km)", ylab="Moran's I", cex.lab=2, cex.axis=1.5);
> > abline(h=0)
> >
> > I also found the function "sp.correlogram" in the package spdep to make
> the
> > correlogram with Bonferroni correction.
> >
> > Now I have three questions:
> >
> > 1. Can I set a "up-limit" to the "correlog" function? For example, if I
> > just need the first 80 distance classes to generate the graph (as shown
> in
> > the plot code), is it possible to let the "correlog" function skip the
> rest
> > distance classes to save the computation time?
> >
> > 2. Is my code of generating p values after the progressive Bonferroni
> > correction correct? If not, how should I fix it?
> >
> > 3. Now I'm kind of correcting the p value manually based on my
> > understanding of progressive Bonferroni correction. Is there any function
> > in any R packages for creating correlograms with progressive Bonferroni
> > correction? "sp.correlogram"  has a parameter setting "p.adj.method" but
> it
> > seems do not have a progressive Bonferroni correction option?
> >
> > I just start to work with the correlograms recently; any advice or
> > solutions are welcome. Thank you in advance!
> >
> > Erin
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Mon Mar 18 16:00:56 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Mon, 18 Mar 2019 15:00:56 +0000 (UTC)
Subject: [R-sig-Geo] Calculating weighted spatial means globally across grid
 cells using NetCDF file
References: <251823730.6149880.1552921256228.ref@mail.yahoo.com>
Message-ID: <251823730.6149880.1552921256228@mail.yahoo.com>

Hi there,
I am currently working on a project that involves climate model data stored in a NetCDF file. I am currently trying to calculate "weighted" spatial annual "global" averages for precipitation. I need to do this for each of the 95 years of global precipitation data that I have. The idea would be to somehow apply weights to each grid cell by using the cosine of its latitude (which means latitude grid cells at the equator would have a weight of 1 (i.e. the cosine of 0 degrees is 1), and the poles would have a value of 1 (as the cosine of 90 is 1)). Then, I would be in a position to calculate annual weighted averages based on averaging each grid cell.?
I have an idea how to do this conceptually, but I am not sure where to begin writing a script in R to apply the weights across all grid cells and then average these for each of the 95 years. I would greatly appreciate any help with this, or any resources that may be helpful!!!
At the very least, I have opened the .nc file and read-in the NetCDF variables, as shown below:
ncfname<-"MaxPrecCCCMACanESM2rcp45.nc"
Prec<-raster(ncfname)
print(Prec)
Model<-nc_open(ncfname)
get<-ncvar_get(Model,"onedaymax")longitude<-ncvar_get(Model, "lon")
latitude<-ncvar_get(Model, "lat")
Year<-ncvar_get(Model, "Year")

Additionally, let's say that I wanted to create a time series of these newly derived weighted averaged for a specific location or region, the following code that I previously used to show trends over the 95 years for one-day maximum precipitation works, but I would just need to change it slightly to use the annual weighted means? :
r_brick<-brick(get, xmn=min(latitude), xmx=max(latitude), ymn=min(longitude), ymx=max(longitude), crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0"))
r_brick<-flip(t(r_brick), direction='y')
randompointlon<-13.178
randompointlat<--59.548
Hope<-extract(r_brick, SpatialPoints(cbind(randompointlon,randompointlat)),method='simple')
df<-data.frame(year=seq(from=1, to=95, by=1), Precipitation=t(Hope))
ggplot(data=df, aes(x=Year, y=Precipitation, group=1))+geom_line()+ggtitle("One-day maximum precipitation (mm/day) trend for Barbados for CanESM2 RCP4.5")


Also, if it helps, here is what the .nc file contains:

3 variables (excluding dimension variables):
        double onedaymax[lon,lat,time]   (Contiguous storage)  
            units: mm/day
        double fivedaymax[lon,lat,time]   (Contiguous storage)  
            units: mm/day
        short Year[time]   (Contiguous storage)  

     3 dimensions:
        time  Size:95
        lat  Size:64
            units: degree North
        lon  Size:128
            units: degree East
Again, any assistance would be extremely valuable with this! I look forward to your response!

	[[alternative HTML version deleted]]


From kent3737 @end|ng |rom gm@||@com  Tue Mar 19 14:55:27 2019
From: kent3737 @end|ng |rom gm@||@com (Kent Johnson)
Date: Tue, 19 Mar 2019 09:55:27 -0400
Subject: [R-sig-Geo] Reverse y-axis with geom_sf
Message-ID: <CAPP0wygjBzDM2UUoT6ceAtmjCXWyfB3MxApi=8Ot_JzVN9yeaA@mail.gmail.com>

Hi,

I am working with data representing cells in a tissue sample with lines and
polygonal regions defined in the same space. The cells and polygons are
represented and manipulated as simple features objects and visualized using
ggplot2 and geom_sf. Mostly this works very well. The problem is that the
coordinate system of my data has the origin at the top left corner. For
data with no CRS, geom_sf puts the origin at the bottom left so all my
plots are inverted.

Is there a way to invert the y axis while staying within sf (or possibly
sp?) The only answers I have found involve extracting the coordinates from
the sf objects and inverting or plotting from the raw data. Maybe by
creating a CRS with the correct orientation?

For a very simple example with just a few vectors - the code below plots an
arrow pointing down; I would like to invert the y-axis so it points up.
library(sf)
library(ggplot2)
s1 <- rbind(c(9, 11), c(10, 10))
s2 <- rbind(c(11, 11), c(10, 10))
s3 <- rbind(c(10,14), c(10, 12), c(10,10))
mls <- st_multilinestring(list(s1,s2,s3))

ggplot(mls) + geom_sf()

Thank you for any help,
Kent Johnson

	[[alternative HTML version deleted]]


From edzer@pebe@m@ @end|ng |rom un|-muen@ter@de  Tue Mar 19 15:13:37 2019
From: edzer@pebe@m@ @end|ng |rom un|-muen@ter@de (Edzer Pebesma)
Date: Tue, 19 Mar 2019 15:13:37 +0100
Subject: [R-sig-Geo] Reverse y-axis with geom_sf
In-Reply-To: <CAPP0wygjBzDM2UUoT6ceAtmjCXWyfB3MxApi=8Ot_JzVN9yeaA@mail.gmail.com>
References: <CAPP0wygjBzDM2UUoT6ceAtmjCXWyfB3MxApi=8Ot_JzVN9yeaA@mail.gmail.com>
Message-ID: <7f743557-7a41-d93a-cbc6-5b778f862cac@uni-muenster.de>



On 3/19/19 2:55 PM, Kent Johnson wrote:
> Hi,
> 
> I am working with data representing cells in a tissue sample with lines and
> polygonal regions defined in the same space. The cells and polygons are
> represented and manipulated as simple features objects and visualized using
> ggplot2 and geom_sf. Mostly this works very well. The problem is that the
> coordinate system of my data has the origin at the top left corner. For
> data with no CRS, geom_sf puts the origin at the bottom left so all my
> plots are inverted.
> 
> Is there a way to invert the y axis while staying within sf (or possibly
> sp?) The only answers I have found involve extracting the coordinates from
> the sf objects and inverting or plotting from the raw data. Maybe by
> creating a CRS with the correct orientation?
> 
> For a very simple example with just a few vectors - the code below plots an
> arrow pointing down; I would like to invert the y-axis so it points up.
> library(sf)
> library(ggplot2)
> s1 <- rbind(c(9, 11), c(10, 10))
> s2 <- rbind(c(11, 11), c(10, 10))
> s3 <- rbind(c(10,14), c(10, 12), c(10,10))
> mls <- st_multilinestring(list(s1,s2,s3))
> 
> ggplot(mls) + geom_sf()

You mean, like

ggplot(mls * c(1, -1)) + geom_sf()

?

> 
> Thank you for any help,
> Kent Johnson
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics
Heisenbergstrasse 2, 48151 Muenster, Germany
Phone: +49 251 8333081

-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 2472 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20190319/bd508983/attachment.bin>

From ge@u@j| @end|ng |rom gm@||@com  Tue Mar 19 18:54:13 2019
From: ge@u@j| @end|ng |rom gm@||@com (=?UTF-8?B?SmVzw7pz?=)
Date: Tue, 19 Mar 2019 17:54:13 +0000
Subject: [R-sig-Geo] Reverse y-axis with geom_sf
In-Reply-To: <CAPP0wygjBzDM2UUoT6ceAtmjCXWyfB3MxApi=8Ot_JzVN9yeaA@mail.gmail.com>
References: <CAPP0wygjBzDM2UUoT6ceAtmjCXWyfB3MxApi=8Ot_JzVN9yeaA@mail.gmail.com>
Message-ID: <CAGt6bQ7BQV4xHPUH0XZRqdKtdvZZXLBXp6xhYpbKODtVqBOn_g@mail.gmail.com>

Hi Kent
Did you try using the coord_flip function from ggplot?
Cheers
Jes?s


El mar., 19 mar. 2019 a las 13:55, Kent Johnson (<kent3737 at gmail.com>)
escribi?:

> Hi,
>
> I am working with data representing cells in a tissue sample with lines and
> polygonal regions defined in the same space. The cells and polygons are
> represented and manipulated as simple features objects and visualized using
> ggplot2 and geom_sf. Mostly this works very well. The problem is that the
> coordinate system of my data has the origin at the top left corner. For
> data with no CRS, geom_sf puts the origin at the bottom left so all my
> plots are inverted.
>
> Is there a way to invert the y axis while staying within sf (or possibly
> sp?) The only answers I have found involve extracting the coordinates from
> the sf objects and inverting or plotting from the raw data. Maybe by
> creating a CRS with the correct orientation?
>
> For a very simple example with just a few vectors - the code below plots an
> arrow pointing down; I would like to invert the y-axis so it points up.
> library(sf)
> library(ggplot2)
> s1 <- rbind(c(9, 11), c(10, 10))
> s2 <- rbind(c(11, 11), c(10, 10))
> s3 <- rbind(c(10,14), c(10, 12), c(10,10))
> mls <- st_multilinestring(list(s1,s2,s3))
>
> ggplot(mls) + geom_sf()
>
> Thank you for any help,
> Kent Johnson
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


-- 
Jes?s

	[[alternative HTML version deleted]]


From kent3737 @end|ng |rom gm@||@com  Tue Mar 19 19:54:55 2019
From: kent3737 @end|ng |rom gm@||@com (Kent Johnson)
Date: Tue, 19 Mar 2019 14:54:55 -0400
Subject: [R-sig-Geo] Reverse y-axis with geom_sf
In-Reply-To: <CAGt6bQ7BQV4xHPUH0XZRqdKtdvZZXLBXp6xhYpbKODtVqBOn_g@mail.gmail.com>
References: <CAPP0wygjBzDM2UUoT6ceAtmjCXWyfB3MxApi=8Ot_JzVN9yeaA@mail.gmail.com>
 <CAGt6bQ7BQV4xHPUH0XZRqdKtdvZZXLBXp6xhYpbKODtVqBOn_g@mail.gmail.com>
Message-ID: <CAPP0wyhWH0LcfBMHaLx__ReigOAkCvOpcEj4gQ171xkMY3jkSQ@mail.gmail.com>

On Tue, Mar 19, 2019 at 1:54 PM Jes?s <gesusjl at gmail.com> wrote:

> Hi Kent
> Did you try using the coord_flip function from ggplot?
> Cheers
> Jes?s
>

Yes, I did, coord_flip() doesn't work with geom_sf():

> ggplot(mls) + geom_sf() + coord_flip()
Error: geom_sf() must be used with coord_sf()

Thanks for the suggestion!
Kent

>
> El mar., 19 mar. 2019 a las 13:55, Kent Johnson (<kent3737 at gmail.com>)
> escribi?:
>
>> Hi,
>>
>> I am working with data representing cells in a tissue sample with lines
>> and
>> polygonal regions defined in the same space. The cells and polygons are
>> represented and manipulated as simple features objects and visualized
>> using
>> ggplot2 and geom_sf. Mostly this works very well. The problem is that the
>> coordinate system of my data has the origin at the top left corner. For
>> data with no CRS, geom_sf puts the origin at the bottom left so all my
>> plots are inverted.
>>
>> Is there a way to invert the y axis while staying within sf (or possibly
>> sp?) The only answers I have found involve extracting the coordinates from
>> the sf objects and inverting or plotting from the raw data. Maybe by
>> creating a CRS with the correct orientation?
>>
>> For a very simple example with just a few vectors - the code below plots
>> an
>> arrow pointing down; I would like to invert the y-axis so it points up.
>> library(sf)
>> library(ggplot2)
>> s1 <- rbind(c(9, 11), c(10, 10))
>> s2 <- rbind(c(11, 11), c(10, 10))
>> s3 <- rbind(c(10,14), c(10, 12), c(10,10))
>> mls <- st_multilinestring(list(s1,s2,s3))
>>
>> ggplot(mls) + geom_sf()
>>
>> Thank you for any help,
>> Kent Johnson
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
> --
> Jes?s
>

	[[alternative HTML version deleted]]


From ge@u@j| @end|ng |rom gm@||@com  Tue Mar 19 20:11:03 2019
From: ge@u@j| @end|ng |rom gm@||@com (=?UTF-8?B?SmVzw7pz?=)
Date: Tue, 19 Mar 2019 19:11:03 +0000
Subject: [R-sig-Geo] Reverse y-axis with geom_sf
In-Reply-To: <CAPP0wyhWH0LcfBMHaLx__ReigOAkCvOpcEj4gQ171xkMY3jkSQ@mail.gmail.com>
References: <CAPP0wygjBzDM2UUoT6ceAtmjCXWyfB3MxApi=8Ot_JzVN9yeaA@mail.gmail.com>
 <CAGt6bQ7BQV4xHPUH0XZRqdKtdvZZXLBXp6xhYpbKODtVqBOn_g@mail.gmail.com>
 <CAPP0wyhWH0LcfBMHaLx__ReigOAkCvOpcEj4gQ171xkMY3jkSQ@mail.gmail.com>
Message-ID: <CAGt6bQ50AcnY_FXaqeQPT3cyfL4OehGP_5UF1UyugNi-swh5Fg@mail.gmail.com>

You are welcome

You could try converting sf to sp and  use the corresponding geoms
(geom_polygon, geom_point..etc.)  plus coord_flip() or map the x and y
aesthetics accordingly.

El mar., 19 mar. 2019 a las 18:55, Kent Johnson (<kent3737 at gmail.com>)
escribi?:

> On Tue, Mar 19, 2019 at 1:54 PM Jes?s <gesusjl at gmail.com> wrote:
>
>> Hi Kent
>> Did you try using the coord_flip function from ggplot?
>> Cheers
>> Jes?s
>>
>
> Yes, I did, coord_flip() doesn't work with geom_sf():
>
> > ggplot(mls) + geom_sf() + coord_flip()
> Error: geom_sf() must be used with coord_sf()
>
> Thanks for the suggestion!
> Kent
>
>>
>> El mar., 19 mar. 2019 a las 13:55, Kent Johnson (<kent3737 at gmail.com>)
>> escribi?:
>>
>>> Hi,
>>>
>>> I am working with data representing cells in a tissue sample with lines
>>> and
>>> polygonal regions defined in the same space. The cells and polygons are
>>> represented and manipulated as simple features objects and visualized
>>> using
>>> ggplot2 and geom_sf. Mostly this works very well. The problem is that the
>>> coordinate system of my data has the origin at the top left corner. For
>>> data with no CRS, geom_sf puts the origin at the bottom left so all my
>>> plots are inverted.
>>>
>>> Is there a way to invert the y axis while staying within sf (or possibly
>>> sp?) The only answers I have found involve extracting the coordinates
>>> from
>>> the sf objects and inverting or plotting from the raw data. Maybe by
>>> creating a CRS with the correct orientation?
>>>
>>> For a very simple example with just a few vectors - the code below plots
>>> an
>>> arrow pointing down; I would like to invert the y-axis so it points up.
>>> library(sf)
>>> library(ggplot2)
>>> s1 <- rbind(c(9, 11), c(10, 10))
>>> s2 <- rbind(c(11, 11), c(10, 10))
>>> s3 <- rbind(c(10,14), c(10, 12), c(10,10))
>>> mls <- st_multilinestring(list(s1,s2,s3))
>>>
>>> ggplot(mls) + geom_sf()
>>>
>>> Thank you for any help,
>>> Kent Johnson
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>>
>> --
>> Jes?s
>>
>

-- 
Jes?s

	[[alternative HTML version deleted]]


From kent3737 @end|ng |rom gm@||@com  Wed Mar 20 16:50:56 2019
From: kent3737 @end|ng |rom gm@||@com (Kent Johnson)
Date: Wed, 20 Mar 2019 11:50:56 -0400
Subject: [R-sig-Geo] Reverse y-axis with geom_sf
In-Reply-To: <mailman.27169.3.1553079602.3785.r-sig-geo@r-project.org>
References: <mailman.27169.3.1553079602.3785.r-sig-geo@r-project.org>
Message-ID: <CAPP0wyjQbHMB2A2OwquOwdjLKVwy=7+5fiGL1gvWmVE_bC4rrw@mail.gmail.com>

>
> Date: Tue, 19 Mar 2019 15:13:37 +0100
> From: Edzer Pebesma <edzer.pebesma at uni-muenster.de>
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Reverse y-axis with geom_sf
> Message-ID: <7f743557-7a41-d93a-cbc6-5b778f862cac at uni-muenster.de>
> Content-Type: text/plain; charset="utf-8"
>
>
> On 3/19/19 2:55 PM, Kent Johnson wrote:
>
> > Is there a way to invert the y axis while staying within sf (or possibly
> > sp?) The only answers I have found involve extracting the coordinates
> from
> > the sf objects and inverting or plotting from the raw data. Maybe by
> > creating a CRS with the correct orientation?
> >
> > For a very simple example with just a few vectors - the code below plots
> an
> > arrow pointing down; I would like to invert the y-axis so it points up.
> > library(sf)
> > library(ggplot2)
> > s1 <- rbind(c(9, 11), c(10, 10))
> > s2 <- rbind(c(11, 11), c(10, 10))
> > s3 <- rbind(c(10,14), c(10, 12), c(10,10))
> > mls <- st_multilinestring(list(s1,s2,s3))
> >
> > ggplot(mls) + geom_sf()
>
> You mean, like
>
> ggplot(mls * c(1, -1)) + geom_sf()
>
> ?
>

Yes, that might work well, with the addition of a correction to the axis
labels:
ggplot(mls * c(1, -1)) + geom_sf() +
  scale_y_continuous(labels=function(x) -x)

I have a raster background that will need some adjustment too, that should
be manageable.
Thanks!
Kent

Date: Tue, 19 Mar 2019 19:11:03 +0000
> From: =?UTF-8?B?SmVzw7pz?= <gesusjl at gmail.com>
> To: Kent Johnson <kent3737 at gmail.com>
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Reverse y-axis with geom_sf
> Message-ID:
>         <
> CAGt6bQ50AcnY_FXaqeQPT3cyfL4OehGP_5UF1UyugNi-swh5Fg at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> You are welcome
>
> You could try converting sf to sp and  use the corresponding geoms
> (geom_polygon, geom_point..etc.)  plus coord_flip() or map the x and y
> aesthetics accordingly.
>

That, or just pulling the raw data out and plotting it directly, is kind of
a last resort. I would rather not have to do this level of data munging as
my actual data is moderately complex.

Thanks,
Kent

	[[alternative HTML version deleted]]


From |eon@rdom@erv|no @end|ng |rom gm@||@com  Wed Mar 20 18:53:57 2019
From: |eon@rdom@erv|no @end|ng |rom gm@||@com (Leonardo Matheus Servino)
Date: Wed, 20 Mar 2019 14:53:57 -0300
Subject: [R-sig-Geo] R squared in lagsarlm
Message-ID: <CA+TKu0Tq8Cj1w5Jgbi=HQFogVtie1z14Uvf1CQiiHMNaKXty9Q@mail.gmail.com>

Hello,

I would like to know where I can find the Rsquared value in lagsarlm

Thanks

-- 
Leonardo Matheus Servino
P?s-Gradua??o em Evolu??o e Diversidade
Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e Humanas

LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202

Rua Arcturus, 3. Jardim Antares
09606-070 S?o Bernardo do Campo - SP

	[[alternative HTML version deleted]]


From phiiipov@ m@iii@g oii imbm@b@s@bg  Thu Mar 21 09:35:48 2019
From: phiiipov@ m@iii@g oii imbm@b@s@bg (phiiipov@ m@iii@g oii imbm@b@s@bg)
Date: Thu, 21 Mar 2019 10:35:48 +0200
Subject: [R-sig-Geo] Unsubscribe from your list
Message-ID: <9dc2dbe5dca0640b4c5bcbcc8b2aaff5.squirrel@info.imbm.bas.bg>

Dear Mr/Madam,
Please unsubscribe from your list.
Thank you in advance
Kind Regards
Nina Philipova


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Mar 21 14:09:43 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 21 Mar 2019 14:09:43 +0100
Subject: [R-sig-Geo] R squared in lagsarlm
In-Reply-To: <CA+TKu0Tq8Cj1w5Jgbi=HQFogVtie1z14Uvf1CQiiHMNaKXty9Q@mail.gmail.com>
References: <CA+TKu0Tq8Cj1w5Jgbi=HQFogVtie1z14Uvf1CQiiHMNaKXty9Q@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903211406160.7214@reclus.nhh.no>

The Nagelkerke measure is optionally provided in the summary method:
Nagelkerke NJD (1991) A note on a general definition of the coefficient of 
determination. Biometrika 78: 691-692.

See https://r-spatial.github.io/spdep/reference/summary.sarlm.html or 
?summary.sarlm.

There is no "R squared" for these models, the likelihood ratio is a better 
comparative measure.

Roger

On Wed, 20 Mar 2019, Leonardo Matheus Servino wrote:

> Hello,
>
> I would like to know where I can find the Rsquared value in lagsarlm
>
> Thanks
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |eon@rdom@erv|no @end|ng |rom gm@||@com  Thu Mar 21 14:19:16 2019
From: |eon@rdom@erv|no @end|ng |rom gm@||@com (Leonardo Matheus Servino)
Date: Thu, 21 Mar 2019 10:19:16 -0300
Subject: [R-sig-Geo] R squared in lagsarlm
In-Reply-To: <alpine.LFD.2.21.1903211406160.7214@reclus.nhh.no>
References: <CA+TKu0Tq8Cj1w5Jgbi=HQFogVtie1z14Uvf1CQiiHMNaKXty9Q@mail.gmail.com>
 <alpine.LFD.2.21.1903211406160.7214@reclus.nhh.no>
Message-ID: <CA+TKu0QDNiaNfZeWSnk_O-Eqj7DA6f3e=G05oJUnUUY5bZWYQg@mail.gmail.com>

Thanks, and one more question: what parameters of SAR we should expose in
articles?

<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Livre
de v?rus. www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>.
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

Em qui, 21 de mar de 2019 ?s 10:09, Roger Bivand <Roger.Bivand at nhh.no>
escreveu:

> The Nagelkerke measure is optionally provided in the summary method:
> Nagelkerke NJD (1991) A note on a general definition of the coefficient of
> determination. Biometrika 78: 691-692.
>
> See https://r-spatial.github.io/spdep/reference/summary.sarlm.html or
> ?summary.sarlm.
>
> There is no "R squared" for these models, the likelihood ratio is a better
> comparative measure.
>
> Roger
>
> On Wed, 20 Mar 2019, Leonardo Matheus Servino wrote:
>
> > Hello,
> >
> > I would like to know where I can find the Rsquared value in lagsarlm
> >
> > Thanks
> >
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>


-- 
Leonardo Matheus Servino
P?s-Gradua??o em Evolu??o e Diversidade
Universidade Federal do ABC - UFABC - Centro de Ci?ncias Naturais e Humanas

LED I - Laborat?rio de Evolu??o e Diversidade I - Bloco Delta, sala 202

Rua Arcturus, 3. Jardim Antares
09606-070 S?o Bernardo do Campo - SP

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Mar 21 14:28:59 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 21 Mar 2019 14:28:59 +0100
Subject: [R-sig-Geo] R squared in lagsarlm
In-Reply-To: <CA+TKu0QDNiaNfZeWSnk_O-Eqj7DA6f3e=G05oJUnUUY5bZWYQg@mail.gmail.com>
References: <CA+TKu0Tq8Cj1w5Jgbi=HQFogVtie1z14Uvf1CQiiHMNaKXty9Q@mail.gmail.com>
 <alpine.LFD.2.21.1903211406160.7214@reclus.nhh.no>
 <CA+TKu0QDNiaNfZeWSnk_O-Eqj7DA6f3e=G05oJUnUUY5bZWYQg@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903211425400.7214@reclus.nhh.no>

Do not post HTML-mail!

What do you mean by: "what parameters of SAR we should expose"? What do 
you mean by SAR? Do you mean the SAR model (simultaneous autoregressive 
model, a.k.a. the simultaneous spatial error model) or the SAR model 
(spatial autoregressive model a.k.a. the simultaneous spatial lag model)? 
If the spatial lag model, only ever report impacts, never report the 
betas. Please be more precise.

Roger

On Thu, 21 Mar 2019, Leonardo Matheus Servino wrote:

> Thanks, and one more question: what parameters of SAR we should expose in
> articles?
>
> <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> Livre
> de v?rus. www.avast.com
> <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>.
> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>
> Em qui, 21 de mar de 2019 ?s 10:09, Roger Bivand <Roger.Bivand at nhh.no>
> escreveu:
>
>> The Nagelkerke measure is optionally provided in the summary method:
>> Nagelkerke NJD (1991) A note on a general definition of the coefficient of
>> determination. Biometrika 78: 691-692.
>>
>> See https://r-spatial.github.io/spdep/reference/summary.sarlm.html or
>> ?summary.sarlm.
>>
>> There is no "R squared" for these models, the likelihood ratio is a better
>> comparative measure.
>>
>> Roger
>>
>> On Wed, 20 Mar 2019, Leonardo Matheus Servino wrote:
>>
>>> Hello,
>>>
>>> I would like to know where I can find the Rsquared value in lagsarlm
>>>
>>> Thanks
>>>
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From |eon@rdom@erv|no @end|ng |rom gm@||@com  Thu Mar 21 14:54:43 2019
From: |eon@rdom@erv|no @end|ng |rom gm@||@com (Leonardo Matheus Servino)
Date: Thu, 21 Mar 2019 10:54:43 -0300
Subject: [R-sig-Geo] R squared in lagsarlm
In-Reply-To: <alpine.LFD.2.21.1903211425400.7214@reclus.nhh.no>
References: <CA+TKu0Tq8Cj1w5Jgbi=HQFogVtie1z14Uvf1CQiiHMNaKXty9Q@mail.gmail.com>
 <alpine.LFD.2.21.1903211406160.7214@reclus.nhh.no>
 <CA+TKu0QDNiaNfZeWSnk_O-Eqj7DA6f3e=G05oJUnUUY5bZWYQg@mail.gmail.com>
 <alpine.LFD.2.21.1903211425400.7214@reclus.nhh.no>
Message-ID: <CA+TKu0QJQWqiJvrQ5Mtu4G7-1VUzjk8mXrE9xpEqCSXk1Um3tg@mail.gmail.com>

When a linear regression is used, usually the degree of freedom, F-value,
p-value are exposed, in text or in a table. In a lagsarlm, what parameters
we should expose?



<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
Livre
de v?rus. www.avast.com
<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>.
<#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Mar 21 15:05:18 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 21 Mar 2019 15:05:18 +0100
Subject: [R-sig-Geo] R squared in lagsarlm
In-Reply-To: <CA+TKu0QJQWqiJvrQ5Mtu4G7-1VUzjk8mXrE9xpEqCSXk1Um3tg@mail.gmail.com>
References: <CA+TKu0Tq8Cj1w5Jgbi=HQFogVtie1z14Uvf1CQiiHMNaKXty9Q@mail.gmail.com>
 <alpine.LFD.2.21.1903211406160.7214@reclus.nhh.no>
 <CA+TKu0QDNiaNfZeWSnk_O-Eqj7DA6f3e=G05oJUnUUY5bZWYQg@mail.gmail.com>
 <alpine.LFD.2.21.1903211425400.7214@reclus.nhh.no>
 <CA+TKu0QJQWqiJvrQ5Mtu4G7-1VUzjk8mXrE9xpEqCSXk1Um3tg@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903211455590.7214@reclus.nhh.no>

On Thu, 21 Mar 2019, Leonardo Matheus Servino wrote:

> When a linear regression is used, usually the degree of freedom, F-value,
> p-value are exposed, in text or in a table. In a lagsarlm, what parameters
> we should expose?
>

None of these make any sense in this case. This model is fitted by maximum 
likelihood, so likelihood-based measures may be appropriate, but the model 
is also non-linear in the spatial coefficient, so it is simply not like 
OLS. However, you could represent OLS in its maximum likelihood form, 
correcting the t-values not to subtract k. The provided measure is a 
likelihood ratio test of the model fitted with and without the spatial 
coefficient (equivalent to a test of the spatial coefficient). You can run 
your own LR tests against other alternatives, and the Nagelkerke measure 
is also likelihood-based. STSLS may provide other measures, but they are 
not OLS-based either, being IV. Just because a supervisor or referee wants 
the same measures as OLS, it doesn't mean they can get them. Certainly you 
should avoid p-values as they give little guidance.

Roger

>
>
> <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> Livre
> de v?rus. www.avast.com
> <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>.
> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From m|ch|e|@v@nd|jk @end|ng |rom wur@n|  Tue Mar 26 00:08:10 2019
From: m|ch|e|@v@nd|jk @end|ng |rom wur@n| (Dijk, Michiel van)
Date: Mon, 25 Mar 2019 23:08:10 +0000
Subject: [R-sig-Geo] read error after mosaic_rasters
In-Reply-To: <e6ce89caef464b96a4fcdc77b7e96643@scomp5296.wurnet.nl>
References: <e6ce89caef464b96a4fcdc77b7e96643@scomp5296.wurnet.nl>
Message-ID: <cdabb4fbbe7f412aaaf433fc7f8305d3@scomp5296.wurnet.nl>

Hi,

I am using mosaic_rasters from gdalUtils to combine different raster files. More specifically, I am using the 12 tiles that cover Southern Africa from the well-known Hansen et al. (2013) forest map (1.1) that can be downloaded here: http://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.1.html. I am using the tree cover, forest gain and forest loss data. All is working fine for the forest gain and forest loss files but I receive an error when combing the tree cover files (which are the largest). 

Below you will find my code. Note that "tree_cover_files" is a vector that points towards the 12 tiles stored in a local folder. I am receiving the following error after some time:

Checking gdal_installation...
Scanning for GDAL installations...
Checking the gdalUtils_gdalPath option...
GDAL version 2.2.4
GDAL command being used: "C:\OSGeo4W64\bin\gdalbuildvrt.exe" -input_file_list "c:\Temp\RtmpkRd30l\file4687024480a.txt" "c:\Temp\RtmpkRd30l\file4684596162f.vrt" 
Checking gdal_installation...
Scanning for GDAL installations...
Checking the gdalUtils_gdalPath option...
GDAL version 2.2.4
GDAL command being used: "C:\OSGeo4W64\bin\gdal_translate.exe" -of "GTiff" "c:\Temp\RtmpkRd30l\file4684596162f.vrt" "P:/globiom/Projects/ISWEL/data/forest/combined_tiles/tree_cover.tif"
Input file size is 120000, 1600000...10...20...30...40...
ERROR 1: TIFFFillStrip:Read error at scanline 39921; got 3204 bytes, expected 10047
ERROR 1: TIFFReadEncodedStrip() failed.
ERROR 1: P:/globiom/Projects/ISWEL/data/forest/tree_cover/Hansen_GFC2014_treecover2000_10S_030E.tif, band 1: IReadBlock failed at X offset 0, Y offset 39922
ERROR 1: GetBlockRef failed at X block offset 0, Y block offset 39922

I understand that this might be thread error, related to how the files are read and intermediate vrt file is constructed (https://github.com/mapnik/node-mapnik/issues/437 ) and could be solved by setting "VRT_SHARED_SOURCE" to 0. I tried to do this using setCPLConfigOption("VRT_SHARED_SOURCE", "0") in R but still receive the same error. Is this really the way to solve this or is the file perhaps corrupt (unlikely as this dataset is used by many people - I also downloaded them twice)? I hope somebody can give me advice on how to make this work.

Many thanks,
Michiel



# prepare and save template for mosaic
e <- extent(20, 40, -20, -10)
template <- raster(e)
proj4string(template) <- CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0") writeRaster(template, file = file.path(proj_path, "data/forest/combined_tiles/tree_cover.tif"), format="GTiff", overwrite = T)

# create mosaic
setCPLConfigOption("VRT_SHARED_SOURCE", "0") mosaic_rasters(gdalfile = tree_cover_files,
               dst_dataset = file.path(proj_path, "data/forest/combined_tiles/tree_cover.tif"), of="GTiff",
               verbose = T)
 


M. (Michiel) van Dijk, PhD
Research scholar | Ecosystems Services and Management (ESM) | International Institute for Applied Systems Analysis (IIASA) Senior researcher (out of office) | International Policy Division (IB) | Wageningen Economic Research


From jkg@rrett90 @end|ng |rom gm@||@com  Tue Mar 26 02:23:48 2019
From: jkg@rrett90 @end|ng |rom gm@||@com (James Garrett)
Date: Mon, 25 Mar 2019 20:23:48 -0500
Subject: [R-sig-Geo] GWR Output Attributes
Message-ID: <CAE5EhCUP1pEKxymde-rHM7WEF-knxdBAmK4UpoqnOXkJL746rQ@mail.gmail.com>

Dear R-Sig-Geo List,

I am hoping to verify the output variables from a gwr model (package
spgwr). I've searched through the archives and haven't been able to find
exactly what a few are, although I have a strong guess from the package
description:

 SDF a SpatialPointsDataFrame (may be gridded) or SpatialPolygonsDataFrame
object (see package "sp") with *fit.points, weights, GWR coefficient
estimates, Rsquared, and coefficient standard errors in its "data" slot.*

Is this correct?
sum_w = sum of the weights
Intrc = X Intercept
gtVI(F) = GWR coefficient estimates of the X intercept
gwr_e = gwr residuals
pred = Y prediction
localR2 = local R2.

I apologize if this is rudimentary. Thanks so much in advance,

James

-- 

*James Garrett*

Clemson University

Graduate Student, Forest Resources

School of Agricultural, Forest, and Environmental Sciences

Phone (334)-790-2483

Advisor: Dr. Skip Van Bloem

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Mar 26 12:46:26 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 26 Mar 2019 12:46:26 +0100
Subject: [R-sig-Geo] GWR Output Attributes
In-Reply-To: <CAE5EhCUP1pEKxymde-rHM7WEF-knxdBAmK4UpoqnOXkJL746rQ@mail.gmail.com>
References: <CAE5EhCUP1pEKxymde-rHM7WEF-knxdBAmK4UpoqnOXkJL746rQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1903261235540.4138@reclus.nhh.no>

On Tue, 26 Mar 2019, James Garrett wrote:

> Dear R-Sig-Geo List,
>
> I am hoping to verify the output variables from a gwr model (package
> spgwr). I've searched through the archives and haven't been able to find
> exactly what a few are, although I have a strong guess from the package
> description:
>
> SDF a SpatialPointsDataFrame (may be gridded) or SpatialPolygonsDataFrame
> object (see package "sp") with *fit.points, weights, GWR coefficient
> estimates, Rsquared, and coefficient standard errors in its "data" slot.*

There is no need to guess. Help pages have examples. Run:

library(spgwr)
example(gwr)
str(col.gauss)
str(col.gauss$SDF)
names(col.gauss$SDF)

to examine the contents of the objects of the first run. Note that the 
rest of the example script shows why GWR is unreliable.

Don't guess, don't google or SO, just use the examples in the help pages 
actively, for example changing the formula or argument values to see what 
the arguments do. Do use alternative implementations to check your 
assumptions, such as GWmodel. Do read vignettes: vignette("GWR").

Hope this helps,

Roger


>
> Is this correct?
> sum_w = sum of the weights
> Intrc = X Intercept
> gtVI(F) = GWR coefficient estimates of the X intercept
> gwr_e = gwr residuals
> pred = Y prediction
> localR2 = local R2.
>
> I apologize if this is rudimentary. Thanks so much in advance,
>
> James
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Mar 26 13:52:35 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 26 Mar 2019 13:52:35 +0100
Subject: [R-sig-Geo] GWR Output Attributes
In-Reply-To: <alpine.LFD.2.21.1903261235540.4138@reclus.nhh.no>
References: <CAE5EhCUP1pEKxymde-rHM7WEF-knxdBAmK4UpoqnOXkJL746rQ@mail.gmail.com>
 <alpine.LFD.2.21.1903261235540.4138@reclus.nhh.no>
Message-ID: <alpine.LFD.2.21.1903261348040.4138@reclus.nhh.no>

On Tue, 26 Mar 2019, Roger Bivand wrote:

> On Tue, 26 Mar 2019, James Garrett wrote:
>
>>  Dear R-Sig-Geo List,
>>
>>  I am hoping to verify the output variables from a gwr model (package
>>  spgwr). I've searched through the archives and haven't been able to find
>>  exactly what a few are, although I have a strong guess from the package
>>  description:
>>
>>  SDF a SpatialPointsDataFrame (may be gridded) or SpatialPolygonsDataFrame
>>  object (see package "sp") with *fit.points, weights, GWR coefficient
>>  estimates, Rsquared, and coefficient standard errors in its "data" slot.*
>
> There is no need to guess. Help pages have examples. Run:
>
> library(spgwr)
> example(gwr)
> str(col.gauss)
> str(col.gauss$SDF)
> names(col.gauss$SDF)
>
> to examine the contents of the objects of the first run. Note that the rest 
> of the example script shows why GWR is unreliable.

I've just used pkgdown to commit the processed help pages of spgwr to 
R-Forge:

http://rspatial.r-forge.r-project.org/spgwr/reference/gwr.html#examples

is the rendered version of running example(gwr) yourself, and is less 
effective because you can't play with the output.

Roger

>
> Don't guess, don't google or SO, just use the examples in the help pages 
> actively, for example changing the formula or argument values to see what the 
> arguments do. Do use alternative implementations to check your assumptions, 
> such as GWmodel. Do read vignettes: vignette("GWR").
>
> Hope this helps,
>
> Roger
>
>
>>
>>  Is this correct?
>>  sum_w = sum of the weights
>>  Intrc = X Intercept
>>  gtVI(F) = GWR coefficient estimates of the X intercept
>>  gwr_e = gwr residuals
>>  pred = Y prediction
>>  localR2 = local R2.
>>
>>  I apologize if this is rudimentary. Thanks so much in advance,
>>
>>  James
>> 
>> 
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |@r@@@|p@@||v@ @end|ng |rom gm@||@com  Tue Mar 26 16:44:17 2019
From: |@r@@@|p@@||v@ @end|ng |rom gm@||@com (Lara Silva)
Date: Tue, 26 Mar 2019 14:44:17 -0100
Subject: [R-sig-Geo] Convert map pdf in raster
Message-ID: <CALN9TESfjAfJV75fe1cjLnsrmYf8KZOBFP8Lt9WWHC2RkoUOyw@mail.gmail.com>

Hello,

Is it possible to convert a map im pdf to a raster?
Another question. I need to obtain a raster of land use and land cover of
Europe. Which site to choose?

Thanks,

Lara

	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Tue Mar 26 19:29:18 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Tue, 26 Mar 2019 18:29:18 +0000 (UTC)
Subject: [R-sig-Geo] Plotting x and y values using data from two separate
 netCDF files in R
References: <160292064.10371408.1553624958429.ref@mail.yahoo.com>
Message-ID: <160292064.10371408.1553624958429@mail.yahoo.com>

Hi there,
I am currently trying to plot precipitation data (y-axis values) with cumulative emissions data (x-axis) using R. Both of these data are found on two separate netCDF files that I have already read into R. Ultimately, What I would like to do is plot precipitation as a function of cumulative emissions for a selected location (as shown below in the following code). I have, so far, used the following code (with "#" to highlight each step):?? ? library(raster)
??? library(ncdf4)
??? library(maps)
??? library(maptools)
??? library(rasterVis)
??? library(ggplot2)
??? library(rgdal)
??? library(sp)? ??#Geting cumulative emissions data for x-axis? ?? ? ncfname<-"cumulative_emissions_1pctCO2.nc"
??? Model1<-nc_open(ncfname)
??? print(Model1)
??? get<-ncvar_get(Model1, "cum_co2_emi-CanESM2") #units of terratones of?? 
??? carbon (TtC) for x-axis (140 values)
??? print(get)
??? Year<-ncvar_get(Model1, "time") #140 years
?#Getting Model data for extreme precipitation (units of millimeters/day) for y-axis? ?? ? ncfname1<-"MaxPrecCCCMACanESM21pctCO2.nc"
??? Model2<-nc_open(ncfname1)
??? print(Model2)
??? get1<-ncvar_get(Model2, "onedaymax") #units of millimeters/day
??? print(get1)
? ??#Reading in latitude, longitude and time from this file:
? ??? ? latitude<-ncvar_get(Model2, "lat") #64 degrees latitude
??? longitude<-ncvar_get(Model2, "lon") #128 degrees longitude
??? Year1<-ncvar_get(Model2, "Year") #140 years
? ? #Plotting attempt? ??? ? randompointlon<-30 #selecting a longitude
??? randompointlat<--5 #selecting a latitude
??? Hope<-extract(r_brick, 
??? SpatialPoints(cbind(randompointlon,randompointlat)),method='simple')
??? df<-data.frame(cumulativeemissions=seq(from=1, to=140, by=1),?? 
??? Precipitation=t(Hope))
??? ggplot(data=df, aes(x=get, y=Precipitation, 
??? group=1))+geom_line()+ggtitle("One-day maximum precipitation (mm/day)?? 
??? for random location for CanESM2 1pctCO2 as a function of cumulative 
??? emissions")
print(Model1) yields the following (I read in variable #2 for now):
File cumulative_emissions_1pctCO2.nc (NC_FORMAT_NETCDF4):
14 variables (excluding dimension variables):
? ? ? ??? ? ? ? float cum_co2_emi-BNU-ESM[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for BNU-ESM
??????????? units: Tt C
??????? float cum_co2_emi-CanESM2[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for CanESM2
??????????? units: Tt C
??????? float cum_co2_emi-CESM1-BGC[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for CESM1-BGC
??????????? units: Tt C
??????? float cum_co2_emi-HadGEM2-ES[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for HadGEM2-ES
??????????? units: Tt C
??????? float cum_co2_emi-inmcm4[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for inmcm4
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5A-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5A-LR
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5A-MR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5A-MR
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5B-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5B-LR
??????????? units: Tt C
??????? float cum_co2_emi-MIROC-ESM[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MIROC-ESM
??????????? units: Tt C
??????? float cum_co2_emi-MPI-ESM-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MPI-ESM-LR
??????????? units: Tt C
??????? float cum_co2_emi-MPI-ESM-MR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MPI-ESM-MR
??????????? units: Tt C
??????? float cum_co2_emi-NorESM1-ME[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for NorESM1-ME
??????????? units: Tt C
??????? float cum_co2_emi-GFDL-ESM2G[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for GFDL-ESM2G
??????????? units: Tt C
??????? float cum_co2_emi-GFDL-ESM2M[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for GFDL-ESM2M
??????????? units: Tt C? ???1 dimensions:
??????? time? Size:140
??????????? units: years since 0-1-1 0:0:0
??????????? long_name: time
??????????? standard_name: time
??????????? calender: noleap? ?4 global attributes:
??????? description: Cumulative carbon emissions for the 1pctCO2 scenario from the CMIP5 dataset.
??????? history: Created Fri Jul 21 14:50:39 2017
??????? source: CMIP5 archieve
??????? 
print(Model2) yields the following:File MaxPrecCCCMACanESM21pctCO2.nc (NC_FORMAT_NETCDF4):? ? ?3 variables (excluding dimension variables):
??????? double onedaymax[lon,lat,time]?? (Contiguous storage)? 
??????????? units: mm/day
??????? double fivedaymax[lon,lat,time]?? (Contiguous storage)? 
??????????? units: mm/day
??????? short Year[time]?? (Contiguous storage)? ? ? ?3 dimensions:
??????? time? Size:140
??????? lat? Size:64
??????????? units: degree North
??????? lon? Size:128
??????????? units: degree East? ? ??3 global attributes:
??????? description: Annual global maximum precipitation from the CanESM2 1pctCO2 scenario
??????? history: Created Mon Jun? 4 11:24:02 2018
??????? contact: rain1290 at aim.com
So, in general, this is what I am trying to achieve, but I am not sure if what I am doing in the ggplot function is the right approach for this. 
Any assistance with this would be greatly appreciated!
Thanks,
	[[alternative HTML version deleted]]


From md@umner @end|ng |rom gm@||@com  Tue Mar 26 21:33:54 2019
From: md@umner @end|ng |rom gm@||@com (Michael Sumner)
Date: Wed, 27 Mar 2019 07:33:54 +1100
Subject: [R-sig-Geo] 
 Plotting x and y values using data from two separate
 netCDF files in R
In-Reply-To: <160292064.10371408.1553624958429@mail.yahoo.com>
References: <160292064.10371408.1553624958429.ref@mail.yahoo.com>
 <160292064.10371408.1553624958429@mail.yahoo.com>
Message-ID: <CAAcGz99B0bYgey2NXMQnEa6YT_g1Xo=i3dzBgZBpBtMPR=ZuZw@mail.gmail.com>

I would try for a single point:

x <- raster::brick(ncfname, varname = "cum_co2_emi-CanESM2")
y <- raster::brick(ncfname1, varname = "onedaymax")

pt <- cbind(30, -5)
to_plot <- cbind(raster::extract(x, pt), raster::extract(y, pt))

plot(to_plot)

Is that close?  You might be better off using raster::as.data.frame(x, xy =
TRUE, long = TRUE) if you want all locations at their actual centre.

See if the times of the 3rd axis are valid (and the same) in getZ(x) and
getZ(y).

There's rarely a need to use ncdf4 directly, though that's important
sometimes, more so for grids that raster's regular-affine referencing model
doesn't support.

cheers, Mike



On Wed, 27 Mar 2019 at 05:29 rain1290--- via R-sig-Geo <
r-sig-geo at r-project.org> wrote:

> Hi there,
> I am currently trying to plot precipitation data (y-axis values) with
> cumulative emissions data (x-axis) using R. Both of these data are found on
> two separate netCDF files that I have already read into R. Ultimately, What
> I would like to do is plot precipitation as a function of cumulative
> emissions for a selected location (as shown below in the following code). I
> have, so far, used the following code (with "#" to highlight each step):
>   library(raster)
>     library(ncdf4)
>     library(maps)
>     library(maptools)
>     library(rasterVis)
>     library(ggplot2)
>     library(rgdal)
>     library(sp)    #Geting cumulative emissions data for x-axis
> ncfname<-"cumulative_emissions_1pctCO2.nc"
>     Model1<-nc_open(ncfname)
>     print(Model1)
>     get<-ncvar_get(Model1, "cum_co2_emi-CanESM2") #units of terratones
> of
>     carbon (TtC) for x-axis (140 values)
>     print(get)
>     Year<-ncvar_get(Model1, "time") #140 years
>  #Getting Model data for extreme precipitation (units of millimeters/day)
> for y-axis       ncfname1<-"MaxPrecCCCMACanESM21pctCO2.nc"
>     Model2<-nc_open(ncfname1)
>     print(Model2)
>     get1<-ncvar_get(Model2, "onedaymax") #units of millimeters/day
>     print(get1)
>     #Reading in latitude, longitude and time from this file:
>         latitude<-ncvar_get(Model2, "lat") #64 degrees latitude
>     longitude<-ncvar_get(Model2, "lon") #128 degrees longitude
>     Year1<-ncvar_get(Model2, "Year") #140 years
>     #Plotting attempt        randompointlon<-30 #selecting a longitude
>     randompointlat<--5 #selecting a latitude
>     Hope<-extract(r_brick,
>     SpatialPoints(cbind(randompointlon,randompointlat)),method='simple')
>     df<-data.frame(cumulativeemissions=seq(from=1, to=140, by=1),
>     Precipitation=t(Hope))
>     ggplot(data=df, aes(x=get, y=Precipitation,
>     group=1))+geom_line()+ggtitle("One-day maximum precipitation
> (mm/day)
>     for random location for CanESM2 1pctCO2 as a function of cumulative
>     emissions")
> print(Model1) yields the following (I read in variable #2 for now):
> File cumulative_emissions_1pctCO2.nc (NC_FORMAT_NETCDF4):
> 14 variables (excluding dimension variables):
>                 float cum_co2_emi-BNU-ESM[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for BNU-ESM
>             units: Tt C
>         float cum_co2_emi-CanESM2[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for CanESM2
>             units: Tt C
>         float cum_co2_emi-CESM1-BGC[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for CESM1-BGC
>             units: Tt C
>         float cum_co2_emi-HadGEM2-ES[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for HadGEM2-ES
>             units: Tt C
>         float cum_co2_emi-inmcm4[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for inmcm4
>             units: Tt C
>         float cum_co2_emi-IPSL-CM5A-LR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for IPSL-CM5A-LR
>             units: Tt C
>         float cum_co2_emi-IPSL-CM5A-MR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for IPSL-CM5A-MR
>             units: Tt C
>         float cum_co2_emi-IPSL-CM5B-LR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for IPSL-CM5B-LR
>             units: Tt C
>         float cum_co2_emi-MIROC-ESM[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for MIROC-ESM
>             units: Tt C
>         float cum_co2_emi-MPI-ESM-LR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for MPI-ESM-LR
>             units: Tt C
>         float cum_co2_emi-MPI-ESM-MR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for MPI-ESM-MR
>             units: Tt C
>         float cum_co2_emi-NorESM1-ME[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for NorESM1-ME
>             units: Tt C
>         float cum_co2_emi-GFDL-ESM2G[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for GFDL-ESM2G
>             units: Tt C
>         float cum_co2_emi-GFDL-ESM2M[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for GFDL-ESM2M
>             units: Tt C     1 dimensions:
>         time  Size:140
>             units: years since 0-1-1 0:0:0
>             long_name: time
>             standard_name: time
>             calender: noleap   4 global attributes:
>         description: Cumulative carbon emissions for the 1pctCO2 scenario
> from the CMIP5 dataset.
>         history: Created Fri Jul 21 14:50:39 2017
>         source: CMIP5 archieve
>
> print(Model2) yields the following:File MaxPrecCCCMACanESM21pctCO2.nc
> (NC_FORMAT_NETCDF4):     3 variables (excluding dimension variables):
>         double onedaymax[lon,lat,time]   (Contiguous storage)
>             units: mm/day
>         double fivedaymax[lon,lat,time]   (Contiguous storage)
>             units: mm/day
>         short Year[time]   (Contiguous storage)       3 dimensions:
>         time  Size:140
>         lat  Size:64
>             units: degree North
>         lon  Size:128
>             units: degree East      3 global attributes:
>         description: Annual global maximum precipitation from the CanESM2
> 1pctCO2 scenario
>         history: Created Mon Jun  4 11:24:02 2018
>         contact: rain1290 at aim.com
> So, in general, this is what I am trying to achieve, but I am not sure if
> what I am doing in the ggplot function is the right approach for this.
> Any assistance with this would be greatly appreciated!
> Thanks,
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From md@umner @end|ng |rom gm@||@com  Tue Mar 26 21:43:20 2019
From: md@umner @end|ng |rom gm@||@com (Michael Sumner)
Date: Wed, 27 Mar 2019 07:43:20 +1100
Subject: [R-sig-Geo] Convert map pdf in raster
In-Reply-To: <CALN9TESfjAfJV75fe1cjLnsrmYf8KZOBFP8Lt9WWHC2RkoUOyw@mail.gmail.com>
References: <CALN9TESfjAfJV75fe1cjLnsrmYf8KZOBFP8Lt9WWHC2RkoUOyw@mail.gmail.com>
Message-ID: <CAAcGz9-RgbD578PdwV6ZmObKnJk-+or4CDDV8_QCbgdKnLzHYg@mail.gmail.com>

Try

 r <- raster::brick(pdffile)

#(requires rgdal with PDF driver)

raster::plotRGB(r)

Alternatively, try stars::read_stars

HTH

On Wed, Mar 27, 2019, 02:43 Lara Silva <lara.sfp.silva at gmail.com> wrote:

> Hello,
>
> Is it possible to convert a map im pdf to a raster?
> Another question. I need to obtain a raster of land use and land cover of
> Europe. Which site to choose?
>
> Thanks,
>
> Lara
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From |@r@@@|p@@||v@ @end|ng |rom gm@||@com  Tue Mar 26 21:48:39 2019
From: |@r@@@|p@@||v@ @end|ng |rom gm@||@com (Lara Silva)
Date: Tue, 26 Mar 2019 19:48:39 -0100
Subject: [R-sig-Geo] Convert map pdf in raster
In-Reply-To: <CAAcGz9-RgbD578PdwV6ZmObKnJk-+or4CDDV8_QCbgdKnLzHYg@mail.gmail.com>
References: <CALN9TESfjAfJV75fe1cjLnsrmYf8KZOBFP8Lt9WWHC2RkoUOyw@mail.gmail.com>
 <CAAcGz9-RgbD578PdwV6ZmObKnJk-+or4CDDV8_QCbgdKnLzHYg@mail.gmail.com>
Message-ID: <CALN9TETKK4s2gPB1yhyuD3TJksY9qpNjahYwR+kkHd6H+58D=A@mail.gmail.com>

Thanks a lot!

Michael Sumner <mdsumner at gmail.com> escreveu no dia ter?a, 26/03/2019 ?(s)
19:43:

> Try
>
>  r <- raster::brick(pdffile)
>
> #(requires rgdal with PDF driver)
>
> raster::plotRGB(r)
>
> Alternatively, try stars::read_stars
>
> HTH
>
> On Wed, Mar 27, 2019, 02:43 Lara Silva <lara.sfp.silva at gmail.com> wrote:
>
>> Hello,
>>
>> Is it possible to convert a map im pdf to a raster?
>> Another question. I need to obtain a raster of land use and land cover of
>> Europe. Which site to choose?
>>
>> Thanks,
>>
>> Lara
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>

	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Tue Mar 26 22:05:07 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Tue, 26 Mar 2019 21:05:07 +0000 (UTC)
Subject: [R-sig-Geo] 
 Plotting x and y values using data from two separate
 netCDF files in R
In-Reply-To: <CAAcGz99B0bYgey2NXMQnEa6YT_g1Xo=i3dzBgZBpBtMPR=ZuZw@mail.gmail.com>
References: <160292064.10371408.1553624958429.ref@mail.yahoo.com>
 <160292064.10371408.1553624958429@mail.yahoo.com>
 <CAAcGz99B0bYgey2NXMQnEa6YT_g1Xo=i3dzBgZBpBtMPR=ZuZw@mail.gmail.com>
Message-ID: <1277294526.10474373.1553634307566@mail.yahoo.com>

Hi Michael,
Thank you so much for your reply!?
I was just trying your suggestion, but when I run the following in R:

x<-raster::brick(ncfname, varname="cum_co2_emi-CanESM2")

I receive the following error:
Error in .varName(nc, varname, warn = warn) : 
  varname: cum_co2_emi-CanESM2 does not exist in the file. Select one from:

I tried switching "ncfname" with "Model1", but I then receive this error:
Error in (function (classes, fdef, mtable)  : 
  unable to find an inherited method for function ?brick? for signature ?"ncdf4"?
Is there a reason for that?
Thanks, again,

-----Original Message-----
From: Michael Sumner <mdsumner at gmail.com>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Tue, Mar 26, 2019 4:34 pm
Subject: Re: [R-sig-Geo] Plotting x and y values using data from two separate netCDF files in R

I would try for a single point:?
x <- raster::brick(ncfname, varname =?"cum_co2_emi-CanESM2")y <- raster::brick(ncfname1, varname = "onedaymax")
pt <- cbind(30, -5)to_plot <- cbind(raster::extract(x, pt), raster::extract(y, pt))
plot(to_plot)
Is that close?? You might be better off using raster::as.data.frame(x, xy = TRUE, long = TRUE) if you want all locations at their actual centre.?
See if the times of the 3rd axis are valid (and the same) in getZ(x) and getZ(y).?
There's rarely a need to use ncdf4 directly, though that's important sometimes, more so for grids that raster's regular-affine referencing model doesn't support.?
cheers, Mike


On Wed, 27 Mar 2019 at 05:29 rain1290--- via R-sig-Geo <r-sig-geo at r-project.org> wrote:

Hi there,
I am currently trying to plot precipitation data (y-axis values) with cumulative emissions data (x-axis) using R. Both of these data are found on two separate netCDF files that I have already read into R. Ultimately, What I would like to do is plot precipitation as a function of cumulative emissions for a selected location (as shown below in the following code). I have, so far, used the following code (with "#" to highlight each step):?? ? library(raster)
??? library(ncdf4)
??? library(maps)
??? library(maptools)
??? library(rasterVis)
??? library(ggplot2)
??? library(rgdal)
??? library(sp)? ??#Geting cumulative emissions data for x-axis? ?? ? ncfname<-"cumulative_emissions_1pctCO2.nc"
??? Model1<-nc_open(ncfname)
??? print(Model1)
??? get<-ncvar_get(Model1, "cum_co2_emi-CanESM2") #units of terratones of?? 
??? carbon (TtC) for x-axis (140 values)
??? print(get)
??? Year<-ncvar_get(Model1, "time") #140 years
?#Getting Model data for extreme precipitation (units of millimeters/day) for y-axis? ?? ? ncfname1<-"MaxPrecCCCMACanESM21pctCO2.nc"
??? Model2<-nc_open(ncfname1)
??? print(Model2)
??? get1<-ncvar_get(Model2, "onedaymax") #units of millimeters/day
??? print(get1)
? ??#Reading in latitude, longitude and time from this file:
? ??? ? latitude<-ncvar_get(Model2, "lat") #64 degrees latitude
??? longitude<-ncvar_get(Model2, "lon") #128 degrees longitude
??? Year1<-ncvar_get(Model2, "Year") #140 years
? ? #Plotting attempt? ??? ? randompointlon<-30 #selecting a longitude
??? randompointlat<--5 #selecting a latitude
??? Hope<-extract(r_brick, 
??? SpatialPoints(cbind(randompointlon,randompointlat)),method='simple')
??? df<-data.frame(cumulativeemissions=seq(from=1, to=140, by=1),?? 
??? Precipitation=t(Hope))
??? ggplot(data=df, aes(x=get, y=Precipitation, 
??? group=1))+geom_line()+ggtitle("One-day maximum precipitation (mm/day)?? 
??? for random location for CanESM2 1pctCO2 as a function of cumulative 
??? emissions")
print(Model1) yields the following (I read in variable #2 for now):
File cumulative_emissions_1pctCO2.nc (NC_FORMAT_NETCDF4):
14 variables (excluding dimension variables):
? ? ? ??? ? ? ? float cum_co2_emi-BNU-ESM[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for BNU-ESM
??????????? units: Tt C
??????? float cum_co2_emi-CanESM2[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for CanESM2
??????????? units: Tt C
??????? float cum_co2_emi-CESM1-BGC[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for CESM1-BGC
??????????? units: Tt C
??????? float cum_co2_emi-HadGEM2-ES[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for HadGEM2-ES
??????????? units: Tt C
??????? float cum_co2_emi-inmcm4[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for inmcm4
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5A-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5A-LR
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5A-MR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5A-MR
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5B-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5B-LR
??????????? units: Tt C
??????? float cum_co2_emi-MIROC-ESM[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MIROC-ESM
??????????? units: Tt C
??????? float cum_co2_emi-MPI-ESM-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MPI-ESM-LR
??????????? units: Tt C
??????? float cum_co2_emi-MPI-ESM-MR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MPI-ESM-MR
??????????? units: Tt C
??????? float cum_co2_emi-NorESM1-ME[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for NorESM1-ME
??????????? units: Tt C
??????? float cum_co2_emi-GFDL-ESM2G[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for GFDL-ESM2G
??????????? units: Tt C
??????? float cum_co2_emi-GFDL-ESM2M[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for GFDL-ESM2M
??????????? units: Tt C? ???1 dimensions:
??????? time? Size:140
??????????? units: years since 0-1-1 0:0:0
??????????? long_name: time
??????????? standard_name: time
??????????? calender: noleap? ?4 global attributes:
??????? description: Cumulative carbon emissions for the 1pctCO2 scenario from the CMIP5 dataset.
??????? history: Created Fri Jul 21 14:50:39 2017
??????? source: CMIP5 archieve
??????? 
print(Model2) yields the following:File MaxPrecCCCMACanESM21pctCO2.nc (NC_FORMAT_NETCDF4):? ? ?3 variables (excluding dimension variables):
??????? double onedaymax[lon,lat,time]?? (Contiguous storage)? 
??????????? units: mm/day
??????? double fivedaymax[lon,lat,time]?? (Contiguous storage)? 
??????????? units: mm/day
??????? short Year[time]?? (Contiguous storage)? ? ? ?3 dimensions:
??????? time? Size:140
??????? lat? Size:64
??????????? units: degree North
??????? lon? Size:128
??????????? units: degree East? ? ??3 global attributes:
??????? description: Annual global maximum precipitation from the CanESM2 1pctCO2 scenario
??????? history: Created Mon Jun? 4 11:24:02 2018
??????? contact: rain1290 at aim.com
So, in general, this is what I am trying to achieve, but I am not sure if what I am doing in the ggplot function is the right approach for this. 
Any assistance with this would be greatly appreciated!
Thanks,
? ? ? ? [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia


	[[alternative HTML version deleted]]


From md@umner @end|ng |rom gm@||@com  Tue Mar 26 22:09:57 2019
From: md@umner @end|ng |rom gm@||@com (Michael Sumner)
Date: Wed, 27 Mar 2019 08:09:57 +1100
Subject: [R-sig-Geo] 
 Plotting x and y values using data from two separate
 netCDF files in R
In-Reply-To: <1277294526.10474373.1553634307566@mail.yahoo.com>
References: <160292064.10371408.1553624958429.ref@mail.yahoo.com>
 <160292064.10371408.1553624958429@mail.yahoo.com>
 <CAAcGz99B0bYgey2NXMQnEa6YT_g1Xo=i3dzBgZBpBtMPR=ZuZw@mail.gmail.com>
 <1277294526.10474373.1553634307566@mail.yahoo.com>
Message-ID: <CAAcGz9-C8dY2A1k_yJybh9XXApJgXbnJEaq0HgKFAHhH02xTHA@mail.gmail.com>

Use the file name as the first argument, and the variable name you want as
varname =

Raster doesn't work with output of nc_open

See ?brick

Good luck

On Wed, Mar 27, 2019, 08:05 <rain1290 at aim.com> wrote:

> Hi Michael,
>
> Thank you so much for your reply!
>
> I was just trying your suggestion, but when I run the following in R:
>
>
> x<-raster::brick(ncfname, varname="cum_co2_emi-CanESM2")
>
> I receive the following error:
>
> Error in .varName(nc, varname, warn = warn) :
>   varname: cum_co2_emi-CanESM2 does not exist in the file. Select one from:
>
>
>
> I tried switching "ncfname" with "Model1", but I then receive this error:
>
> Error in (function (classes, fdef, mtable)  :
>   unable to find an inherited method for function ?brick? for signature ?"ncdf4"?
>
>
> Is there a reason for that?
>
> Thanks, again,
>
> -----Original Message-----
> From: Michael Sumner <mdsumner at gmail.com>
> To: rain1290 <rain1290 at aim.com>
> Cc: r-sig-geo <r-sig-geo at r-project.org>
> Sent: Tue, Mar 26, 2019 4:34 pm
> Subject: Re: [R-sig-Geo] Plotting x and y values using data from two
> separate netCDF files in R
>
> I would try for a single point:
>
> x <- raster::brick(ncfname, varname = "cum_co2_emi-CanESM2")
> y <- raster::brick(ncfname1, varname = "onedaymax")
>
> pt <- cbind(30, -5)
> to_plot <- cbind(raster::extract(x, pt), raster::extract(y, pt))
>
> plot(to_plot)
>
> Is that close?  You might be better off using raster::as.data.frame(x, xy
> = TRUE, long = TRUE) if you want all locations at their actual centre.
>
> See if the times of the 3rd axis are valid (and the same) in getZ(x) and
> getZ(y).
>
> There's rarely a need to use ncdf4 directly, though that's important
> sometimes, more so for grids that raster's regular-affine referencing model
> doesn't support.
>
> cheers, Mike
>
>
>
> On Wed, 27 Mar 2019 at 05:29 rain1290--- via R-sig-Geo <
> r-sig-geo at r-project.org> wrote:
>
> Hi there,
> I am currently trying to plot precipitation data (y-axis values) with
> cumulative emissions data (x-axis) using R. Both of these data are found on
> two separate netCDF files that I have already read into R. Ultimately, What
> I would like to do is plot precipitation as a function of cumulative
> emissions for a selected location (as shown below in the following code). I
> have, so far, used the following code (with "#" to highlight each step):
>   library(raster)
>     library(ncdf4)
>     library(maps)
>     library(maptools)
>     library(rasterVis)
>     library(ggplot2)
>     library(rgdal)
>     library(sp)    #Geting cumulative emissions data for x-axis
> ncfname<-"cumulative_emissions_1pctCO2.nc"
>     Model1<-nc_open(ncfname)
>     print(Model1)
>     get<-ncvar_get(Model1, "cum_co2_emi-CanESM2") #units of terratones
> of
>     carbon (TtC) for x-axis (140 values)
>     print(get)
>     Year<-ncvar_get(Model1, "time") #140 years
>  #Getting Model data for extreme precipitation (units of millimeters/day)
> for y-axis       ncfname1<-"MaxPrecCCCMACanESM21pctCO2.nc"
>     Model2<-nc_open(ncfname1)
>     print(Model2)
>     get1<-ncvar_get(Model2, "onedaymax") #units of millimeters/day
>     print(get1)
>     #Reading in latitude, longitude and time from this file:
>         latitude<-ncvar_get(Model2, "lat") #64 degrees latitude
>     longitude<-ncvar_get(Model2, "lon") #128 degrees longitude
>     Year1<-ncvar_get(Model2, "Year") #140 years
>     #Plotting attempt        randompointlon<-30 #selecting a longitude
>     randompointlat<--5 #selecting a latitude
>     Hope<-extract(r_brick,
>     SpatialPoints(cbind(randompointlon,randompointlat)),method='simple')
>     df<-data.frame(cumulativeemissions=seq(from=1, to=140, by=1),
>     Precipitation=t(Hope))
>     ggplot(data=df, aes(x=get, y=Precipitation,
>     group=1))+geom_line()+ggtitle("One-day maximum precipitation
> (mm/day)
>     for random location for CanESM2 1pctCO2 as a function of cumulative
>     emissions")
> print(Model1) yields the following (I read in variable #2 for now):
> File cumulative_emissions_1pctCO2.nc (NC_FORMAT_NETCDF4):
> 14 variables (excluding dimension variables):
>                 float cum_co2_emi-BNU-ESM[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for BNU-ESM
>             units: Tt C
>         float cum_co2_emi-CanESM2[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for CanESM2
>             units: Tt C
>         float cum_co2_emi-CESM1-BGC[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for CESM1-BGC
>             units: Tt C
>         float cum_co2_emi-HadGEM2-ES[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for HadGEM2-ES
>             units: Tt C
>         float cum_co2_emi-inmcm4[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for inmcm4
>             units: Tt C
>         float cum_co2_emi-IPSL-CM5A-LR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for IPSL-CM5A-LR
>             units: Tt C
>         float cum_co2_emi-IPSL-CM5A-MR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for IPSL-CM5A-MR
>             units: Tt C
>         float cum_co2_emi-IPSL-CM5B-LR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for IPSL-CM5B-LR
>             units: Tt C
>         float cum_co2_emi-MIROC-ESM[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for MIROC-ESM
>             units: Tt C
>         float cum_co2_emi-MPI-ESM-LR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for MPI-ESM-LR
>             units: Tt C
>         float cum_co2_emi-MPI-ESM-MR[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for MPI-ESM-MR
>             units: Tt C
>         float cum_co2_emi-NorESM1-ME[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for NorESM1-ME
>             units: Tt C
>         float cum_co2_emi-GFDL-ESM2G[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for GFDL-ESM2G
>             units: Tt C
>         float cum_co2_emi-GFDL-ESM2M[time]   (Contiguous storage)
>             long_name: Cumulative carbon emissions for GFDL-ESM2M
>             units: Tt C     1 dimensions:
>         time  Size:140
>             units: years since 0-1-1 0:0:0
>             long_name: time
>             standard_name: time
>             calender: noleap   4 global attributes:
>         description: Cumulative carbon emissions for the 1pctCO2 scenario
> from the CMIP5 dataset.
>         history: Created Fri Jul 21 14:50:39 2017
>         source: CMIP5 archieve
>
> print(Model2) yields the following:File MaxPrecCCCMACanESM21pctCO2.nc
> (NC_FORMAT_NETCDF4):     3 variables (excluding dimension variables):
>         double onedaymax[lon,lat,time]   (Contiguous storage)
>             units: mm/day
>         double fivedaymax[lon,lat,time]   (Contiguous storage)
>             units: mm/day
>         short Year[time]   (Contiguous storage)       3 dimensions:
>         time  Size:140
>         lat  Size:64
>             units: degree North
>         lon  Size:128
>             units: degree East      3 global attributes:
>         description: Annual global maximum precipitation from the CanESM2
> 1pctCO2 scenario
>         history: Created Mon Jun  4 11:24:02 2018
>         contact: rain1290 at aim.com
> So, in general, this is what I am trying to achieve, but I am not sure if
> what I am doing in the ggplot function is the right approach for this.
> Any assistance with this would be greatly appreciated!
> Thanks,
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> <https://maps.google.com/?q=203+Channel+Highway+Kingston+Tasmania+7050+Australia&entry=gmail&source=g>
> Kingston Tasmania 7050 Australia
> <https://maps.google.com/?q=203+Channel+Highway+Kingston+Tasmania+7050+Australia&entry=gmail&source=g>
>
> --
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Tue Mar 26 22:35:00 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Tue, 26 Mar 2019 21:35:00 +0000 (UTC)
Subject: [R-sig-Geo] 
 Plotting x and y values using data from two separate
 netCDF files in R
References: <2069401553.10467431.1553636100356.ref@mail.yahoo.com>
Message-ID: <2069401553.10467431.1553636100356@mail.yahoo.com>

Thanks, again.?
It's strange, as the variable "ncfname" is reading off the original file name "cumulative_emissions_1pctCO2.nc", and yet, it says that it cannot find the variable"cum_co2_emi-CanESM2 (and that is the correct variable name with no typos). ?


-----Original Message-----
From: Michael Sumner <mdsumner at gmail.com>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Tue, Mar 26, 2019 5:10 pm
Subject: Re: [R-sig-Geo] Plotting x and y values using data from two separate netCDF files in R

Use the file name as the first argument, and the variable name you want as varname =?
Raster doesn't work with output of nc_open
See ?brick
Good luck

On Wed, Mar 27, 2019, 08:05 <rain1290 at aim.com> wrote:

Hi Michael,
Thank you so much for your reply!?
I was just trying your suggestion, but when I run the following in R:

x<-raster::brick(ncfname, varname="cum_co2_emi-CanESM2")

I receive the following error:
Error in .varName(nc, varname, warn = warn) : 
  varname: cum_co2_emi-CanESM2 does not exist in the file. Select one from:

I tried switching "ncfname" with "Model1", but I then receive this error:
Error in (function (classes, fdef, mtable)  : 
  unable to find an inherited method for function ?brick? for signature ?"ncdf4"?
Is there a reason for that?
Thanks, again,

-----Original Message-----
From: Michael Sumner <mdsumner at gmail.com>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Tue, Mar 26, 2019 4:34 pm
Subject: Re: [R-sig-Geo] Plotting x and y values using data from two separate netCDF files in R

I would try for a single point:?
x <- raster::brick(ncfname, varname =?"cum_co2_emi-CanESM2")y <- raster::brick(ncfname1, varname = "onedaymax")
pt <- cbind(30, -5)to_plot <- cbind(raster::extract(x, pt), raster::extract(y, pt))
plot(to_plot)
Is that close?? You might be better off using raster::as.data.frame(x, xy = TRUE, long = TRUE) if you want all locations at their actual centre.?
See if the times of the 3rd axis are valid (and the same) in getZ(x) and getZ(y).?
There's rarely a need to use ncdf4 directly, though that's important sometimes, more so for grids that raster's regular-affine referencing model doesn't support.?
cheers, Mike


On Wed, 27 Mar 2019 at 05:29 rain1290--- via R-sig-Geo <r-sig-geo at r-project.org> wrote:

Hi there,
I am currently trying to plot precipitation data (y-axis values) with cumulative emissions data (x-axis) using R. Both of these data are found on two separate netCDF files that I have already read into R. Ultimately, What I would like to do is plot precipitation as a function of cumulative emissions for a selected location (as shown below in the following code). I have, so far, used the following code (with "#" to highlight each step):?? ? library(raster)
??? library(ncdf4)
??? library(maps)
??? library(maptools)
??? library(rasterVis)
??? library(ggplot2)
??? library(rgdal)
??? library(sp)? ??#Geting cumulative emissions data for x-axis? ?? ? ncfname<-"cumulative_emissions_1pctCO2.nc"
??? Model1<-nc_open(ncfname)
??? print(Model1)
??? get<-ncvar_get(Model1, "cum_co2_emi-CanESM2") #units of terratones of?? 
??? carbon (TtC) for x-axis (140 values)
??? print(get)
??? Year<-ncvar_get(Model1, "time") #140 years
?#Getting Model data for extreme precipitation (units of millimeters/day) for y-axis? ?? ? ncfname1<-"MaxPrecCCCMACanESM21pctCO2.nc"
??? Model2<-nc_open(ncfname1)
??? print(Model2)
??? get1<-ncvar_get(Model2, "onedaymax") #units of millimeters/day
??? print(get1)
? ??#Reading in latitude, longitude and time from this file:
? ??? ? latitude<-ncvar_get(Model2, "lat") #64 degrees latitude
??? longitude<-ncvar_get(Model2, "lon") #128 degrees longitude
??? Year1<-ncvar_get(Model2, "Year") #140 years
? ? #Plotting attempt? ??? ? randompointlon<-30 #selecting a longitude
??? randompointlat<--5 #selecting a latitude
??? Hope<-extract(r_brick, 
??? SpatialPoints(cbind(randompointlon,randompointlat)),method='simple')
??? df<-data.frame(cumulativeemissions=seq(from=1, to=140, by=1),?? 
??? Precipitation=t(Hope))
??? ggplot(data=df, aes(x=get, y=Precipitation, 
??? group=1))+geom_line()+ggtitle("One-day maximum precipitation (mm/day)?? 
??? for random location for CanESM2 1pctCO2 as a function of cumulative 
??? emissions")
print(Model1) yields the following (I read in variable #2 for now):
File cumulative_emissions_1pctCO2.nc (NC_FORMAT_NETCDF4):
14 variables (excluding dimension variables):
? ? ? ??? ? ? ? float cum_co2_emi-BNU-ESM[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for BNU-ESM
??????????? units: Tt C
??????? float cum_co2_emi-CanESM2[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for CanESM2
??????????? units: Tt C
??????? float cum_co2_emi-CESM1-BGC[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for CESM1-BGC
??????????? units: Tt C
??????? float cum_co2_emi-HadGEM2-ES[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for HadGEM2-ES
??????????? units: Tt C
??????? float cum_co2_emi-inmcm4[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for inmcm4
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5A-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5A-LR
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5A-MR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5A-MR
??????????? units: Tt C
??????? float cum_co2_emi-IPSL-CM5B-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for IPSL-CM5B-LR
??????????? units: Tt C
??????? float cum_co2_emi-MIROC-ESM[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MIROC-ESM
??????????? units: Tt C
??????? float cum_co2_emi-MPI-ESM-LR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MPI-ESM-LR
??????????? units: Tt C
??????? float cum_co2_emi-MPI-ESM-MR[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for MPI-ESM-MR
??????????? units: Tt C
??????? float cum_co2_emi-NorESM1-ME[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for NorESM1-ME
??????????? units: Tt C
??????? float cum_co2_emi-GFDL-ESM2G[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for GFDL-ESM2G
??????????? units: Tt C
??????? float cum_co2_emi-GFDL-ESM2M[time]?? (Contiguous storage)? 
??????????? long_name: Cumulative carbon emissions for GFDL-ESM2M
??????????? units: Tt C? ???1 dimensions:
??????? time? Size:140
??????????? units: years since 0-1-1 0:0:0
??????????? long_name: time
??????????? standard_name: time
??????????? calender: noleap? ?4 global attributes:
??????? description: Cumulative carbon emissions for the 1pctCO2 scenario from the CMIP5 dataset.
??????? history: Created Fri Jul 21 14:50:39 2017
??????? source: CMIP5 archieve
??????? 
print(Model2) yields the following:File MaxPrecCCCMACanESM21pctCO2.nc (NC_FORMAT_NETCDF4):? ? ?3 variables (excluding dimension variables):
??????? double onedaymax[lon,lat,time]?? (Contiguous storage)? 
??????????? units: mm/day
??????? double fivedaymax[lon,lat,time]?? (Contiguous storage)? 
??????????? units: mm/day
??????? short Year[time]?? (Contiguous storage)? ? ? ?3 dimensions:
??????? time? Size:140
??????? lat? Size:64
??????????? units: degree North
??????? lon? Size:128
??????????? units: degree East? ? ??3 global attributes:
??????? description: Annual global maximum precipitation from the CanESM2 1pctCO2 scenario
??????? history: Created Mon Jun? 4 11:24:02 2018
??????? contact: rain1290 at aim.com
So, in general, this is what I am trying to achieve, but I am not sure if what I am doing in the ggplot function is the right approach for this. 
Any assistance with this would be greatly appreciated!
Thanks,
? ? ? ? [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia


-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia


	[[alternative HTML version deleted]]


From @o||nto@|@t @end|ng |rom gm@||@com  Tue Mar 26 22:46:14 2019
From: @o||nto@|@t @end|ng |rom gm@||@com (Antonio Silva)
Date: Tue, 26 Mar 2019 18:46:14 -0300
Subject: [R-sig-Geo] select points out of a polygon
Message-ID: <CAE8g1gNEiKd-5TWkwUeOP5+RXSYqwBe6FWD7cz0qOdH2-SXBVA@mail.gmail.com>

Dear list members

I have a spatial point data frame (spt.points) and a spatial polygon data
frame (spt.poly).

With spt.points[spt.poly,] I can select the points that overlap the polygon.

How can I get the points that do not overlap the polygon?

Thanks a lot.

All the best,

Ant?nio Olinto ?vila da Silva
S?o Paulo, Brasil

	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Thu Mar 28 16:55:29 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Thu, 28 Mar 2019 15:55:29 +0000 (UTC)
Subject: [R-sig-Geo] Converting a variable's units from gigatons to teratons
References: <257060013.11500405.1553788529279.ref@mail.yahoo.com>
Message-ID: <257060013.11500405.1553788529279@mail.yahoo.com>

Hi there,
I am currently working with a variable whose units are expressed in gigatons. However, I would like to convert all of these values in this variable to teratons. Effectively, I would have to somehow have R divide all values (90 values) within the variable by 1000.
So far, I have read in the variable (called CanESM2) as follows:
ncfname1 <- "cumulative_emissions_RCP45.nc"
Model3 <- nc_open(ncfname1)
get2 <- ncvar_get(Model3, "CanESM2")
Is there a way to accomplish this conversion?
Thanks, and any assistance would be greatly appreciated!

	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu Mar 28 17:03:56 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 28 Mar 2019 12:03:56 -0400
Subject: [R-sig-Geo] 
 Converting a variable's units from gigatons to teratons
In-Reply-To: <257060013.11500405.1553788529279@mail.yahoo.com>
References: <257060013.11500405.1553788529279.ref@mail.yahoo.com>
 <257060013.11500405.1553788529279@mail.yahoo.com>
Message-ID: <CAM_vjun+RM+Gu0fVYAPW0SyJmkjrj6syKB6q1OtyuJUMKNBYGw@mail.gmail.com>

ncvar_get() should return a data frame, so just

get2.teratons <- get2 / 1000

Even if it's a higher-dimensional array, that will work.

You could then write it back with ncvar_put(), subject to the data
type of that variable in the NetCDF file.

Sarah


On Thu, Mar 28, 2019 at 11:55 AM rain1290--- via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
>
> Hi there,
> I am currently working with a variable whose units are expressed in gigatons. However, I would like to convert all of these values in this variable to teratons. Effectively, I would have to somehow have R divide all values (90 values) within the variable by 1000.
> So far, I have read in the variable (called CanESM2) as follows:
> ncfname1 <- "cumulative_emissions_RCP45.nc"
> Model3 <- nc_open(ncfname1)
> get2 <- ncvar_get(Model3, "CanESM2")
> Is there a way to accomplish this conversion?
> Thanks, and any assistance would be greatly appreciated!
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From r@i@1290 m@iii@g oii @im@com  Sat Mar 30 15:56:49 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sat, 30 Mar 2019 14:56:49 +0000 (UTC)
Subject: [R-sig-Geo] Modifying the length of a matrix variable
References: <1897521905.10552756.1553957809997.ref@mail.yahoo.com>
Message-ID: <1897521905.10552756.1553957809997@mail.yahoo.com>

Hello there,
I am currently trying to modify a variable's length. It is called "Model4" and is a matrix. It currently has the length of 95, as per "length(Model4)". However, I would like to create a new Model4 (let's say "NewModel4"), in which it has a length of 90, instead of 95.
Is there a way to do this??
Thanks, and any assistance would be greatly appreciated! ?
	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat Mar 30 22:12:21 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 31 Mar 2019 10:12:21 +1300
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
In-Reply-To: <1897521905.10552756.1553957809997@mail.yahoo.com>
References: <1897521905.10552756.1553957809997.ref@mail.yahoo.com>
 <1897521905.10552756.1553957809997@mail.yahoo.com>
Message-ID: <0848bcad-9cfe-c221-1b2c-4518289c5bd9@auckland.ac.nz>


On 31/03/19 3:56 AM, rain1290--- via R-sig-Geo wrote:

> Hello there, I am currently trying to modify a variable's length. It
> is called "Model4" and is a matrix. It currently has the length of
> 95, as per "length(Model4)". However, I would like to create a new
> Model4 (let's say "NewModel4"), in which it has a length of 90,
> instead of 95. Is there a way to do this? Thanks, and any assistance
> would be greatly appreciated! [[alternative HTML version deleted]]

This is a plain text mailing list.  Please *DO NOT* post in HTML.
(In general this scrambles your post and makes it incomprehensible.)

To get to your question:  What you ask makes little sense.  The "length" 
of a matrix is the total number of entries --- nrow(<matrix>) * 
ncol(<matrix>).  Changing the "length" of a matrix would either involve 
changing the number of rows or the number of columns (or both).

Why do you want to do this?  What are you trying to accomplish?
What does dim(Model4) produce?

Don't you think it's time you got serious and learned a bit about R?
(There are many excellent introductory documents available online.)

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat Mar 30 22:57:39 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 31 Mar 2019 10:57:39 +1300
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
In-Reply-To: <1819095072.3912681.1553980958982@mail.yahoo.com>
References: <1897521905.10552756.1553957809997.ref@mail.yahoo.com>
 <1897521905.10552756.1553957809997@mail.yahoo.com>
 <0848bcad-9cfe-c221-1b2c-4518289c5bd9@auckland.ac.nz>
 <1819095072.3912681.1553980958982@mail.yahoo.com>
Message-ID: <101aaf0b-e8b6-e312-a985-e50083b8eb37@auckland.ac.nz>


On 31/03/19 10:22 AM, rain1290 at aim.com wrote:

> Hi Rolf,
> 
> My apologies - I meant "layers" as opposed to "length". The goal is to 
> reduce the number of layers to 90 (from 95).
> 
> dim (Model4) yields:
> 
> 64 ? 128 ? 95
> 
> 
> You can see the 95 there. That is what I would like to reduce to 90, or 
> isolate layer 1 to layer 90.

Please keep the list in the set of recipients.  I am not your private 
consultant, and furthermore others on the list may be able to provide 
better advice than I.  I have CC-ed this message to the list.

To keep only "layers" 1 through 90 you could do:

     Model4.chopped <- Model4[,,1:90]

As I said before, it really is time that you learned something about R 
(e.g. by studying a tutorial).

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@i@1290 m@iii@g oii @im@com  Sat Mar 30 23:14:49 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sat, 30 Mar 2019 22:14:49 +0000 (UTC)
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
In-Reply-To: <101aaf0b-e8b6-e312-a985-e50083b8eb37@auckland.ac.nz>
References: <1897521905.10552756.1553957809997.ref@mail.yahoo.com>
 <1897521905.10552756.1553957809997@mail.yahoo.com>
 <0848bcad-9cfe-c221-1b2c-4518289c5bd9@auckland.ac.nz>
 <1819095072.3912681.1553980958982@mail.yahoo.com>
 <101aaf0b-e8b6-e312-a985-e50083b8eb37@auckland.ac.nz>
Message-ID: <1862845355.12508342.1553984089701@mail.yahoo.com>

Hi Rolf (and others),

I tried your suggestion, but when I used dim(Model4.chopped), it still shows 95 layers, as shown below:
8192 ?? 95?
I also find that the total number of cells is rather low for that many layers. I started with 778240 cells over 95 layers.?
-----Original Message-----
From: Rolf Turner <r.turner at auckland.ac.nz>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo at r-project.org <r-sig-geo at r-project.org>
Sent: Sat, Mar 30, 2019 5:57 pm
Subject: Re: [FORGED] [R-sig-Geo] Modifying the length of a matrix variable


On 31/03/19 10:22 AM, rain1290 at aim.com wrote:

> Hi Rolf,
> 
> My apologies - I meant "layers" as opposed to "length". The goal is to 
> reduce the number of layers to 90 (from 95).
> 
> dim (Model4) yields:
> 
> 64 ? 128 ? 95
> 
> 
> You can see the 95 there. That is what I would like to reduce to 90, or 
> isolate layer 1 to layer 90.

Please keep the list in the set of recipients.? I am not your private 
consultant, and furthermore others on the list may be able to provide 
better advice than I.? I have CC-ed this message to the list.

To keep only "layers" 1 through 90 you could do:

? ? Model4.chopped <- Model4[,,1:90]

As I said before, it really is time that you learned something about R 
(e.g. by studying a tutorial).

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat Mar 30 23:49:07 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 31 Mar 2019 11:49:07 +1300
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
In-Reply-To: <1862845355.12508342.1553984089701@mail.yahoo.com>
References: <1897521905.10552756.1553957809997.ref@mail.yahoo.com>
 <1897521905.10552756.1553957809997@mail.yahoo.com>
 <0848bcad-9cfe-c221-1b2c-4518289c5bd9@auckland.ac.nz>
 <1819095072.3912681.1553980958982@mail.yahoo.com>
 <101aaf0b-e8b6-e312-a985-e50083b8eb37@auckland.ac.nz>
 <1862845355.12508342.1553984089701@mail.yahoo.com>
Message-ID: <e8730773-663d-9da1-3b66-feeed3334655@auckland.ac.nz>


On 31/03/19 11:14 AM, rain1290 at aim.com wrote:

> Hi Rolf (and others),
> 
> I tried your suggestion, but when I used dim(Model4.chopped), it still 
> shows 95 layers, as shown below:
> 
> 8192 ?? 95
> 
> I also find that the total number of cells is rather low for that many 
> layers. I started with 778240 cells over 95 layers.

Well then you're doing something wrong, or there is something that you 
haven't told us.

E.g.:
> junk <- array(runif(64*128*95),dim=c(64,128,95))
> junk.chopped <- junk[,,1:90]
> dim(junk)
> [1]  64 128  95
> dim(junk.chopped)
> [1]  64 128  90

Perhaps Model.4 has some structure other than that of an array. 
(Originally you said it was a matrix.)

You really need to get your terminology and ideas *clear* in order to 
have any hope of receiving useful advice.

I have no idea what you are on about in respect of "the number of 
cells".  My mind-reading machine is in the repair shop.  I strongly 
suspect that your thoughts are confused.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@i@1290 m@iii@g oii @im@com  Sun Mar 31 00:30:06 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sat, 30 Mar 2019 23:30:06 +0000 (UTC)
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
In-Reply-To: <e8730773-663d-9da1-3b66-feeed3334655@auckland.ac.nz>
References: <1897521905.10552756.1553957809997.ref@mail.yahoo.com>
 <1897521905.10552756.1553957809997@mail.yahoo.com>
 <0848bcad-9cfe-c221-1b2c-4518289c5bd9@auckland.ac.nz>
 <1819095072.3912681.1553980958982@mail.yahoo.com>
 <101aaf0b-e8b6-e312-a985-e50083b8eb37@auckland.ac.nz>
 <1862845355.12508342.1553984089701@mail.yahoo.com>
 <e8730773-663d-9da1-3b66-feeed3334655@auckland.ac.nz>
Message-ID: <54122690.12552736.1553988606723@mail.yahoo.com>

Yes, I reproduced the example above and it works just fine (and is what I want!!), but I cannot see why it does not work with my data, as it is a 3-dimensional array (latitude, longitude and time).?
This is what comes from print(Model4):
3 variables (excluding dimension variables):
        double onedaymax[lon,lat,time]   (Contiguous storage)  
            units: mm/day
        double fivedaymax[lon,lat,time]   (Contiguous storage)  
            units: mm/day
        short Year[time]   (Contiguous storage)  

     3 dimensions:
        time  Size:95
        lat  Size:64
            units: degree North
        lon  Size:128
            units: degree East
I reviewed it over and over again, but I cannot see why this would not work?
Thanks,?


-----Original Message-----
From: Rolf Turner <r.turner at auckland.ac.nz>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Sat, Mar 30, 2019 6:49 pm
Subject: Re: [FORGED] [R-sig-Geo] Modifying the length of a matrix variable


On 31/03/19 11:14 AM, rain1290 at aim.com wrote:

> Hi Rolf (and others),
> 
> I tried your suggestion, but when I used dim(Model4.chopped), it still 
> shows 95 layers, as shown below:
> 
> 8192 ?? 95
> 
> I also find that the total number of cells is rather low for that many 
> layers. I started with 778240 cells over 95 layers.

Well then you're doing something wrong, or there is something that you 
haven't told us.

E.g.:
> junk <- array(runif(64*128*95),dim=c(64,128,95))
> junk.chopped <- junk[,,1:90]
> dim(junk)
> [1]? 64 128? 95
> dim(junk.chopped)
> [1]? 64 128? 90

Perhaps Model.4 has some structure other than that of an array. 
(Originally you said it was a matrix.)

You really need to get your terminology and ideas *clear* in order to 
have any hope of receiving useful advice.

I have no idea what you are on about in respect of "the number of 
cells".? My mind-reading machine is in the repair shop.? I strongly 
suspect that your thoughts are confused.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Mar 31 00:43:23 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 31 Mar 2019 12:43:23 +1300
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
In-Reply-To: <54122690.12552736.1553988606723@mail.yahoo.com>
References: <1897521905.10552756.1553957809997.ref@mail.yahoo.com>
 <1897521905.10552756.1553957809997@mail.yahoo.com>
 <0848bcad-9cfe-c221-1b2c-4518289c5bd9@auckland.ac.nz>
 <1819095072.3912681.1553980958982@mail.yahoo.com>
 <101aaf0b-e8b6-e312-a985-e50083b8eb37@auckland.ac.nz>
 <1862845355.12508342.1553984089701@mail.yahoo.com>
 <e8730773-663d-9da1-3b66-feeed3334655@auckland.ac.nz>
 <54122690.12552736.1553988606723@mail.yahoo.com>
Message-ID: <1253765a-a688-97ab-dba5-ae9185fa0e9b@auckland.ac.nz>


On 31/03/19 12:30 PM, rain1290 at aim.com wrote:

> Yes, I reproduced the example above and it works just fine (and is what 
> I want!!), but I cannot see why it does not work with my data, as it is 
> a 3-dimensional array (latitude, longitude and time).
> 
> This is what comes from print(Model4):
> 
> 3 variables (excluding dimension variables): double 
> onedaymax[lon,lat,time] (Contiguous storage) units: mm/day double 
> fivedaymax[lon,lat,time] (Contiguous storage) units: mm/day short 
> Year[time] (Contiguous storage) 3 dimensions: time Size:95 lat Size:64 
> units: degree North lon Size:128 units: degree East
> 
> 
> I reviewed it over and over again, but I cannot see why this would not work?

Psigh!  Clearly Model4 is *not* an array!!!  It is an object of some 
"specialised" class (for which there is specialised print() method).  I 
have no idea what that class might be, but *you can tell.  What does

    class(Model4)

return?

Where did this "Model4" object come from?  What are you trying to *do*?

You might be able to get somewhere by searching (e.g. via Google) on
"subsetting objects of class melvin" where "melvin" is what is returned 
by "class(Model4)".

Doing

     str(Model4)

could be enlightening (but given your stubborn refusal to acquire 
insight into the workings of R, I am not optimistic).

This is not magic or religion.  You need to *understand* what you are 
dealing with, and proceed rationally.  Don't just hammer and hope.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r@i@1290 m@iii@g oii @im@com  Sun Mar 31 00:56:02 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sat, 30 Mar 2019 23:56:02 +0000 (UTC)
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
References: <1797913282.12565475.1553990162139.ref@mail.yahoo.com>
Message-ID: <1797913282.12565475.1553990162139@mail.yahoo.com>

Model4 <- brick("MaxPrecCCCMACanESM2rcp45.nc", var="onedaymax")
That is how Model4 is derived.?
When trying class(Model4), I receive:
[1] "RasterBrick"
attr(,"package")
[1] "raster"

Meanwhile, I will check on Google to see what I come up with in terms of your suggestion. :)
-----Original Message-----
From: Rolf Turner <r.turner at auckland.ac.nz>
To: rain1290 <rain1290 at aim.com>
Cc: r-sig-geo <r-sig-geo at r-project.org>
Sent: Sat, Mar 30, 2019 7:43 pm
Subject: Re: [FORGED] [R-sig-Geo] Modifying the length of a matrix variable


On 31/03/19 12:30 PM, rain1290 at aim.com wrote:

> Yes, I reproduced the example above and it works just fine (and is what 
> I want!!), but I cannot see why it does not work with my data, as it is 
> a 3-dimensional array (latitude, longitude and time).
> 
> This is what comes from print(Model4):
> 
> 3 variables (excluding dimension variables): double 
> onedaymax[lon,lat,time] (Contiguous storage) units: mm/day double 
> fivedaymax[lon,lat,time] (Contiguous storage) units: mm/day short 
> Year[time] (Contiguous storage) 3 dimensions: time Size:95 lat Size:64 
> units: degree North lon Size:128 units: degree East
> 
> 
> I reviewed it over and over again, but I cannot see why this would not work?

Psigh!? Clearly Model4 is *not* an array!!!? It is an object of some 
"specialised" class (for which there is specialised print() method).? I 
have no idea what that class might be, but *you can tell.? What does

? ? class(Model4)

return?

Where did this "Model4" object come from?? What are you trying to *do*?

You might be able to get somewhere by searching (e.g. via Google) on
"subsetting objects of class melvin" where "melvin" is what is returned 
by "class(Model4)".

Doing

? ? str(Model4)

could be enlightening (but given your stubborn refusal to acquire 
insight into the workings of R, I am not optimistic).

This is not magic or religion.? You need to *understand* what you are 
dealing with, and proceed rationally.? Don't just hammer and hope.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Mar 31 01:01:05 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 31 Mar 2019 13:01:05 +1300
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
In-Reply-To: <1797913282.12565475.1553990162139@mail.yahoo.com>
References: <1797913282.12565475.1553990162139.ref@mail.yahoo.com>
 <1797913282.12565475.1553990162139@mail.yahoo.com>
Message-ID: <c5ea8f42-7c82-7cda-ac33-059723975a75@auckland.ac.nz>


On 31/03/19 12:56 PM, rain1290 at aim.com wrote:

> Model4 <- brick("MaxPrecCCCMACanESM2rcp45.nc", var="onedaymax")
> 
> 
> That is how Model4 is derived.
> 
> When trying class(Model4), I receive:
> 
> [1] "RasterBrick" attr(,"package") [1] "raster"
> 
> **//___^
> 
> Meanwhile, I will check on Google to see what I come up with in terms of 
> your suggestion. :)

I know nothing about rasters, the brick() function, or the raster 
package, so include me out at this stage.

Others on the list may be able to help you.  Particular if you can force 
yourself to ask a *focussed* question.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From btupper @end|ng |rom b|ge|ow@org  Sun Mar 31 01:06:13 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Sat, 30 Mar 2019 20:06:13 -0400
Subject: [R-sig-Geo] [FORGED]  Modifying the length of a matrix variable
In-Reply-To: <54122690.12552736.1553988606723@mail.yahoo.com>
References: <1897521905.10552756.1553957809997.ref@mail.yahoo.com>
 <1897521905.10552756.1553957809997@mail.yahoo.com>
 <0848bcad-9cfe-c221-1b2c-4518289c5bd9@auckland.ac.nz>
 <1819095072.3912681.1553980958982@mail.yahoo.com>
 <101aaf0b-e8b6-e312-a985-e50083b8eb37@auckland.ac.nz>
 <1862845355.12508342.1553984089701@mail.yahoo.com>
 <e8730773-663d-9da1-3b66-feeed3334655@auckland.ac.nz>
 <54122690.12552736.1553988606723@mail.yahoo.com>
Message-ID: <8D18CE7C-8C46-4D08-BEE7-0B8ED0FDB11C@bigelow.org>

Hi,

It looks like you are accessing a NetCDF file or it's kin.  Like Rolf, I'm challenged to understand from your description what you are after.  Assuming you do indeed have a NetCDF file and are using the ncdf4 package, you can retrieve the first 90 layers as shown below using the varid, start and count arguments to ncvar_get().  Obviously, this is untested since your data isn't available to us.

library(ncdf4)
filename = "/path/to/foo.nc"
ncObject <- nc_open(filename)
m <- ncvar_get(ncObject varid = "onedaymax", start = c(1,1,1), count = c(-1,-1, 90))
nc_close(ncObject)


See more details and examples here... https://www.rdocumentation.org/packages/ncdf4/versions/1.16.1/topics/ncvar_get


You might also consider using the raster package which will retrieve multilayer georeferenced rasters for you from the same NetCDF file.

library(raster)
B <- brick(filename, varname = "onedaymax")
B <- B[[-c(91:95)]]    # <- drop the last layers 91, 92, ...95

See the docs here... https://www.rdocumentation.org/packages/raster/versions/2.8-19/topics/brick

Cheers,
Ben

> On Mar 30, 2019, at 7:30 PM, rain1290--- via R-sig-Geo <r-sig-geo at r-project.org> wrote:
> 
> Yes, I reproduced the example above and it works just fine (and is what I want!!), but I cannot see why it does not work with my data, as it is a 3-dimensional array (latitude, longitude and time). 
> This is what comes from print(Model4):
> 3 variables (excluding dimension variables):
>        double onedaymax[lon,lat,time]   (Contiguous storage)  
>            units: mm/day
>        double fivedaymax[lon,lat,time]   (Contiguous storage)  
>            units: mm/day
>        short Year[time]   (Contiguous storage)  
> 
>     3 dimensions:
>        time  Size:95
>        lat  Size:64
>            units: degree North
>        lon  Size:128
>            units: degree East
> I reviewed it over and over again, but I cannot see why this would not work?
> Thanks, 
> 
> 
> -----Original Message-----
> From: Rolf Turner <r.turner at auckland.ac.nz>
> To: rain1290 <rain1290 at aim.com>
> Cc: r-sig-geo <r-sig-geo at r-project.org>
> Sent: Sat, Mar 30, 2019 6:49 pm
> Subject: Re: [FORGED] [R-sig-Geo] Modifying the length of a matrix variable
> 
> 
> On 31/03/19 11:14 AM, rain1290 at aim.com wrote:
> 
>> Hi Rolf (and others),
>> 
>> I tried your suggestion, but when I used dim(Model4.chopped), it still 
>> shows 95 layers, as shown below:
>> 
>> 8192    95
>> 
>> I also find that the total number of cells is rather low for that many 
>> layers. I started with 778240 cells over 95 layers.
> 
> Well then you're doing something wrong, or there is something that you 
> haven't told us.
> 
> E.g.:
>> junk <- array(runif(64*128*95),dim=c(64,128,95))
>> junk.chopped <- junk[,,1:90]
>> dim(junk)
>> [1]  64 128  95
>> dim(junk.chopped)
>> [1]  64 128  90
> 
> Perhaps Model.4 has some structure other than that of an array. 
> (Originally you said it was a matrix.)
> 
> You really need to get your terminology and ideas *clear* in order to 
> have any hope of receiving useful advice.
> 
> I have no idea what you are on about in respect of "the number of 
> cells".  My mind-reading machine is in the repair shop.  I strongly 
> suspect that your thoughts are confused.
> 
> cheers,
> 
> Rolf
> 
> -- 
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/


From @tephen@@tew@rt85 @end|ng |rom gm@||@com  Sun Mar 31 01:16:07 2019
From: @tephen@@tew@rt85 @end|ng |rom gm@||@com (Stephen Stewart)
Date: Sun, 31 Mar 2019 11:16:07 +1100
Subject: [R-sig-Geo] [FORGED] Modifying the length of a matrix variable
In-Reply-To: <c5ea8f42-7c82-7cda-ac33-059723975a75@auckland.ac.nz>
References: <1797913282.12565475.1553990162139.ref@mail.yahoo.com>
 <1797913282.12565475.1553990162139@mail.yahoo.com>
 <c5ea8f42-7c82-7cda-ac33-059723975a75@auckland.ac.nz>
Message-ID: <CAJefdj6i5svYqOVdqrwcyD-6boWD7qfYefCHyUqoSycayeO-Sw@mail.gmail.com>

The last few messages provided important information which was previously
absent. Try:

subset = Model4[[1:90]]

This will subset the brick to layers 1 through 90.

I would also suggest some further reading around the raster package and
NetCDF files (e.g. the ncdf4 package) would be useful to you.

On Sun, 31 Mar 2019 at 11:06, Rolf Turner <r.turner at auckland.ac.nz> wrote:

>
> On 31/03/19 12:56 PM, rain1290 at aim.com wrote:
>
> > Model4 <- brick("MaxPrecCCCMACanESM2rcp45.nc", var="onedaymax")
> >
> >
> > That is how Model4 is derived.
> >
> > When trying class(Model4), I receive:
> >
> > [1] "RasterBrick" attr(,"package") [1] "raster"
> >
> > **//___^
> >
> > Meanwhile, I will check on Google to see what I come up with in terms of
> > your suggestion. :)
>
> I know nothing about rasters, the brick() function, or the raster
> package, so include me out at this stage.
>
> Others on the list may be able to help you.  Particular if you can force
> yourself to ask a *focussed* question.
>
> cheers,
>
> Rolf
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From r@i@1290 m@iii@g oii @im@com  Sun Mar 31 03:49:58 2019
From: r@i@1290 m@iii@g oii @im@com (r@i@1290 m@iii@g oii @im@com)
Date: Sun, 31 Mar 2019 01:49:58 +0000 (UTC)
Subject: [R-sig-Geo] [FORGED] Modifying the length of a matrix variable
References: <129960056.12555659.1553996998656.ref@mail.yahoo.com>
Message-ID: <129960056.12555659.1553996998656@mail.yahoo.com>

SUCCESS! Thank you so, so much, Stephen! Indeed, that worked out well, and dim(subset) shows:
64 ? 128 ?? 90
I am very, very grateful for this! Thanks to the others, too, for their assistance with this!

-----Original Message-----
From: Stephen Stewart <stephen.stewart85 at gmail.com>
To: Rolf Turner <r.turner at auckland.ac.nz>
Cc: rain1290 <rain1290 at aim.com>; R-sig-geo Mailing List <r-sig-geo at r-project.org>
Sent: Sat, Mar 30, 2019 8:16 pm
Subject: Re: [R-sig-Geo] [FORGED] Modifying the length of a matrix variable

The last few messages provided important information which was previously absent. Try:

subset = Model4[[1:90]] 

This will subset the brick to layers 1 through 90.

I would also suggest some further reading around the raster package and NetCDF files (e.g. the ncdf4 package) would be useful to you.

On Sun, 31 Mar 2019 at 11:06, Rolf Turner <r.turner at auckland.ac.nz> wrote:


On 31/03/19 12:56 PM, rain1290 at aim.com wrote:

> Model4 <- brick("MaxPrecCCCMACanESM2rcp45.nc", var="onedaymax")
> 
> 
> That is how Model4 is derived.
> 
> When trying class(Model4), I receive:
> 
> [1] "RasterBrick" attr(,"package") [1] "raster"
> 
> **//___^
> 
> Meanwhile, I will check on Google to see what I come up with in terms of 
> your suggestion. :)

I know nothing about rasters, the brick() function, or the raster 
package, so include me out at this stage.

Others on the list may be able to help you.? Particular if you can force 
yourself to ask a *focussed* question.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


