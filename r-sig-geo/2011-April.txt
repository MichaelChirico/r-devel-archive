From hzambran.newsgroups at gmail.com  Fri Apr  1 09:59:18 2011
From: hzambran.newsgroups at gmail.com (Mauricio Zambrano)
Date: Fri, 1 Apr 2011 09:59:18 +0200
Subject: [R-sig-Geo] gstat::krige: different spatial extent of 'newdata' and
	kriged object
Message-ID: <AANLkTimxdKp_HqFRsF73CU-r0OPGcYk5-GWxbEACUNfr@mail.gmail.com>

Dear list,

While using the 'krige' function of gstat, I realised that -in some
cases- the spatial extent of the object obtained with the 'krige'
function is different from the spatial extent of the object provided
as 'newdata' argument to the function.

In particular this occurred when the map passed as 'new data' argument
('SpatialGridDataFrame' object, read with readGDAL) had some empty
rows in the upper part of the map.

Below there is a reproducible example (R version 2.12.2 and gstat_0.9-79 ):


----------------- START --------------------
library(gstat)

data(meuse)
coordinates(meuse) = ~x+y

data(meuse.grid)

# Removing unnecessary bands
meuse.grid$part.a <- NULL
meuse.grid$part.b <- NULL
meuse.grid$dist <- NULL
meuse.grid$ffreq <- NULL
gridded(meuse.grid) = ~x+y

# Getting the class of 'meuse.grid' (SpatialPixelsDataFrame)
class(meuse.grid)
#"SpatialPixelsDataFrame"

summary(meuse.grid)
#nrows=104
#ncols=78

# B) Transforming 'meuse.grid' from 'SpatialPixelsDataFrame' to
'SpatialGridDataFrame'
meuse.grid <- as(meuse.grid,'SpatialGridDataFrame')

# Variogram
m <- vgm(.59, "Sph", 874, .04)

x1 <- krige(log(zinc)~1, locations=meuse, newdata=meuse.grid, model = m)
summary(x1)
#nrows=104
#ncols=78

# In this example, the resulting object 'x1' has the same amount of
rows and column of 'meuse.grid'

###################################
# Putting some NA's in 'meuse.grid'
meuse.grid at data[1:156,] <- NA
summary(meuse.grid)
#nrows=104
#ncols=78

x2 <- krige(log(zinc)~1, locations=meuse, newdata=meuse.grid, model = m)
summary(x2)
#nrows=102
#ncols=78

----------------- END --------------------

In the latest example, why 'x2' has 102 rows if 'meuse.grid' has 104 ?


Thanks in advance for any help,

Mauricio Zambrano B.

-- 
=======================================================
FLOODS Action
Land Management and Natural Hazards Unit
Institute for Environment and Sustainability
European Commission, Joint Research Centre
=======================================================
DISCLAIMER:\ "The views expressed are purely those of th...{{dropped:11}}


From PDowney at urban.org  Fri Apr  1 14:49:09 2011
From: PDowney at urban.org (Downey, Patrick)
Date: Fri, 1 Apr 2011 08:49:09 -0400
Subject: [R-sig-Geo] Spatio-Temporal Models
Message-ID: <0F96478603980B46AAAFBA77069582ED131C73E2@UIEXCH.urban.org>

Hello,

I'm looking to estimate some spatio-temporal regressions controlling for
spatial and temporal autocorrelation and with a few independent variables.
The package {Stem} has exactly what I want, except my data is measured in
polygons, not at points.

Is there a package which can estimate these models when your data is not
point data (such as county-level population)?

I am under the impression that my estimates will be biased if I assign each
county's value to the coordinates at the centroid of the county and proceed
imagining that I have point data. Is this correct?

Thanks in advance for any guidance.

-Mitch


From rshepard at appl-ecosys.com  Fri Apr  1 22:22:39 2011
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 1 Apr 2011 13:22:39 -0700 (PDT)
Subject: [R-sig-Geo] Question on lags in h-Scattergrams
Message-ID: <alpine.LNX.2.00.1104011314550.3160@salmo.appl-ecosys.com>

   My question is not R specific, but about h-scattergrams as described in
Pierre Goovaerts's "Geostatistics for Natural Resource Evaluation." No
matter how many times I read these sections I still cannot grok the
relationship between delta-h (which he sets at 100m in the discussions
beginning in Section 2.3.2 on page 25) and the examples in following
sections.

   For example, on page 27, Figure 2.8 shows Cd concentrations at 6 classes
of distances, 'h' with distance tolerance 100m (the delta-h). Where does the
100m distance show up in the plots with h=43m through h=1024m? This is what
confuses me.

   Learning spatial statistics on my own (all my stat classes in grad school
were biometry focused) leaves me lacking a local resource to ask.

TIA,

Rich


From haileyeckstrand at gmail.com  Fri Apr  1 23:36:37 2011
From: haileyeckstrand at gmail.com (Hailey Eckstrand)
Date: Fri, 1 Apr 2011 14:36:37 -0700
Subject: [R-sig-Geo] SpatialPoints to SpatialGridDataFrame problems
Message-ID: <AANLkTimmR9xKGXG9Cq1=OH5xGVugps8C-=KxJkmdcdzV@mail.gmail.com>

Hello All,
I am trying to convert some points to a SpatialGridDataFrame and am running
into difficulties.
When I convert the SpatialPointsDataFrame into a gridded object, it gives
many warning messages
and the output is incorrect. It says that the cellsize is 500x500 which is
incorrect, it is actually 50,000 by 50,000)

my.pts <- SpatialPointsDataFrame(coords=spatial.coords, data=my.df,
proj4string=CRS(p4s))
 gridded(my.pts) <- T

Warning messages:
1: In points2grid(points, tolerance, round) :
  grid has empty column/rows in dimension 1
2: In points2grid(points, tolerance, round) :
  grid topology may be corrupt in dimension 1
3: In points2grid(points, tolerance, round) :
  grid has empty column/rows in dimension 2
4: In points2grid(points, tolerance, round) :
  grid topology may be corrupt in dimension 2

str(my.pts)
Formal class 'SpatialPixelsDataFrame' [package "sp"] with 7 slots
  ..@ data       :'data.frame': 35 obs. of  2 variables:
  .. ..$ X1: num [1:35] 140.5 84.2 66.3 74.7 98.2 ...
  .. ..$ X2: num [1:35] 155.8 73.9 57.4 72.6 88.4 ...
  ..@ coords.nrs : num(0)
  ..@ grid       :Formal class 'GridTopology' [package "sp"] with 3 slots
  .. .. ..@ cellcentre.offset: Named num [1:2] -633000 -4422000
  .. .. .. ..- attr(*, "names")= chr [1:2] "x" "y"
  .. .. ..@ cellsize         : Named num [1:2] 500 500
  .. .. .. ..- attr(*, "names")= chr [1:2] "x" "y"
  .. .. ..@ cells.dim        : Named int [1:2] 503 902
  .. .. .. ..- attr(*, "names")= chr [1:2] "x" "y"
  ..@ grid.index : int [1:35] 453405 453505 453605 453706 402904 403005
403105 403205 403305 403406 ...
  ..@ coords     : num [1:35, 1:2] -532500 -482500 -432500 -382000 -633000
...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:2] "x" "y"
  ..@ bbox       : num [1:2, 1:2] -633250 -4422250 -381750 -3971250
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : chr [1:2] "x" "y"
  .. .. ..$ : chr [1:2] "min" "max"
  ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slots
  .. .. ..@ projargs: chr " +proj=stere +lat_0=90 +lat_ts=60 +lon_0=-110
+k=1 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs
+towgs84=0,0,0"


I have also tried to build the GridTopology & glue it together with my data:

gtop.coords.min <- c(min(spatial.coords[,1]), min(spatial.coords[,2]))
gtop.coords.max <- c(max(my.coords[,1]), max(my.coords[,2]))
gtop.cell.size <- c(50000, 50000)
gtop.cells.dim.xc <- length(seq(gtop.coords.min[1], gtop.coords.max[1],
by=gtop.cell.size[1]))
gtop.cells.dim.yc <- length(seq(gtop.coords.min[2], gtop.coords.max[2],
by=gtop.cell.size[2]))
 gtop.cells.dim <- c(gtop.cells.dim.xc, gtop.cells.dim.yc)
cangrid.gtop <- GridTopology(gtop.coords.min, gtop.cell.size,
gtop.cells.dim)
str(cangrid.gtop)
Formal class 'GridTopology' [package "sp"] with 3 slots
  ..@ cellcentre.offset: num [1:2] -633000 -4422000
  ..@ cellsize         : num [1:2] 50000 50000
  ..@ cells.dim        : int [1:2] 6 10

SpatialGridDataFrame(grid=cangrid.gtop, data=my.df, proj4string=CRS(p4s))
Error in validityMethod(object) :
  unequal number of objects in full grid and data slot


I have included an image of what the coordinates look like. Here is a
description of my variables:
p4s
"+proj=stere +lat_0=90 +lat_ts=60 +lon_0=-110 +k=1 +x_0=0 +y_0=0
+ellps=GRS80 +datum=NAD83 +units=m +no_defs"

spatial.coords
         x        y
1  -532500 -4422000
2  -482500 -4422000
3  -432500 -4422000
4  -382000 -4422000
5  -633000 -4372000
6  -582500 -4372000
7  -532500 -4372000
8  -482500 -4372000
9  -432500 -4372000
10 -382000 -4372000
11 -583000 -4322000
12 -532500 -4322000
13 -482500 -4322000
14 -432500 -4322000
15 -382000 -4322000
16 -583000 -4272000
17 -532500 -4272000
18 -482500 -4272000
19 -432500 -4272000
20 -382000 -4272000
21 -583000 -4222000
22 -532500 -4222000
23 -482500 -4222000
24 -432500 -4222000
25 -583000 -4172000
26 -532500 -4172000
27 -482500 -4172000
28 -583000 -4122000
29 -532500 -4122000
30 -482500 -4122000
31 -633000 -4071500
32 -583000 -4072000
33 -532500 -4072000
34 -583000 -4021500
35 -633000 -3971500

str(my.df)
'data.frame':   35 obs. of  2 variables:
 $ X1: num  140.5 84.2 66.3 74.7 98.2 ...
 $ X2: num  155.8 73.9 57.4 72.6 88.4 ...

Many thanks,
Hailey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110401/64f627fc/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: points_to_sgdf.JPG
Type: image/jpeg
Size: 12893 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110401/64f627fc/attachment.jpe>

From Roger.Bivand at nhh.no  Sat Apr  2 00:44:48 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 2 Apr 2011 00:44:48 +0200 (CEST)
Subject: [R-sig-Geo] SpatialPoints to SpatialGridDataFrame problems
In-Reply-To: <AANLkTimmR9xKGXG9Cq1=OH5xGVugps8C-=KxJkmdcdzV@mail.gmail.com>
References: <AANLkTimmR9xKGXG9Cq1=OH5xGVugps8C-=KxJkmdcdzV@mail.gmail.com>
Message-ID: <alpine.LRH.2.00.1104020009000.27675@reclus.nhh.no>

On Fri, 1 Apr 2011, Hailey Eckstrand wrote:

> Hello All, I am trying to convert some points to a SpatialGridDataFrame 
> and am running into difficulties. When I convert the 
> SpatialPointsDataFrame into a gridded object, it gives many warning 
> messages and the output is incorrect. It says that the cellsize is 
> 500x500 which is incorrect, it is actually 50,000 by 50,000)

Looking more closely shows that something is odd in your data (pts.txt is 
your data below):

library(sp)
pts <- read.table("pts.txt", header=TRUE)
coordinates(pts) <- c("x", "y")
cangrid.gtop <- GridTopology(c(-633000, -4422000), c(50000, 50000), c(6,
   10))
SG <- SpatialGrid(cangrid.gtop)
plot(pts)
plot(SG, add=TRUE, pch=4)
# go full-screen to see that they don't match
sapply(apply(coordinates(pts), 2, unique), sort)
sapply(sapply(apply(coordinates(pts), 2, unique), sort), function(x)
   unique(diff(x)))

shows that they are not on a 50k by 50k grid, but some are offset by 500m. 
So gridded() was trying hard to guess what you wanted. If they are meant 
to be on the grid you specify, continue constructing a SpatialGrid, and 
use an over() method to find out how to associate the grid cells with the 
data from the SpatialPointsDataFrame:

plot(as(pts, "Spatial"), axes=TRUE)
text(coordinates(pts), label=pts$i)

oo <- over(pts, SG)
unique(table(oo))

# only one point per SG cell

SGi <- rep(as.integer(NA), 60)
SGi[oo] <- pts$i
SGDF <- SpatialGridDataFrame(SG, data=data.frame(i=SGi))
plot(as(SGDF, "Spatial"), axes=TRUE)
text(coordinates(SGDF), label=SGDF$i)

Here i is just the row number, but you would need to create new variables 
for each input variable. Once you've sorted out your data, you can assign 
the proj4string values.

Hope this helps,

Roger

>
> my.pts <- SpatialPointsDataFrame(coords=spatial.coords, data=my.df,
> proj4string=CRS(p4s))
> gridded(my.pts) <- T
>
> Warning messages:
> 1: In points2grid(points, tolerance, round) :
>  grid has empty column/rows in dimension 1
> 2: In points2grid(points, tolerance, round) :
>  grid topology may be corrupt in dimension 1
> 3: In points2grid(points, tolerance, round) :
>  grid has empty column/rows in dimension 2
> 4: In points2grid(points, tolerance, round) :
>  grid topology may be corrupt in dimension 2
>
> str(my.pts)
> Formal class 'SpatialPixelsDataFrame' [package "sp"] with 7 slots
>  ..@ data       :'data.frame': 35 obs. of  2 variables:
>  .. ..$ X1: num [1:35] 140.5 84.2 66.3 74.7 98.2 ...
>  .. ..$ X2: num [1:35] 155.8 73.9 57.4 72.6 88.4 ...
>  ..@ coords.nrs : num(0)
>  ..@ grid       :Formal class 'GridTopology' [package "sp"] with 3 slots
>  .. .. ..@ cellcentre.offset: Named num [1:2] -633000 -4422000
>  .. .. .. ..- attr(*, "names")= chr [1:2] "x" "y"
>  .. .. ..@ cellsize         : Named num [1:2] 500 500
>  .. .. .. ..- attr(*, "names")= chr [1:2] "x" "y"
>  .. .. ..@ cells.dim        : Named int [1:2] 503 902
>  .. .. .. ..- attr(*, "names")= chr [1:2] "x" "y"
>  ..@ grid.index : int [1:35] 453405 453505 453605 453706 402904 403005
> 403105 403205 403305 403406 ...
>  ..@ coords     : num [1:35, 1:2] -532500 -482500 -432500 -382000 -633000
> ...
>  .. ..- attr(*, "dimnames")=List of 2
>  .. .. ..$ : NULL
>  .. .. ..$ : chr [1:2] "x" "y"
>  ..@ bbox       : num [1:2, 1:2] -633250 -4422250 -381750 -3971250
>  .. ..- attr(*, "dimnames")=List of 2
>  .. .. ..$ : chr [1:2] "x" "y"
>  .. .. ..$ : chr [1:2] "min" "max"
>  ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slots
>  .. .. ..@ projargs: chr " +proj=stere +lat_0=90 +lat_ts=60 +lon_0=-110
> +k=1 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs
> +towgs84=0,0,0"
>
>
> I have also tried to build the GridTopology & glue it together with my data:
>
> gtop.coords.min <- c(min(spatial.coords[,1]), min(spatial.coords[,2]))
> gtop.coords.max <- c(max(my.coords[,1]), max(my.coords[,2]))
> gtop.cell.size <- c(50000, 50000)
> gtop.cells.dim.xc <- length(seq(gtop.coords.min[1], gtop.coords.max[1],
> by=gtop.cell.size[1]))
> gtop.cells.dim.yc <- length(seq(gtop.coords.min[2], gtop.coords.max[2],
> by=gtop.cell.size[2]))
> gtop.cells.dim <- c(gtop.cells.dim.xc, gtop.cells.dim.yc)
> cangrid.gtop <- GridTopology(gtop.coords.min, gtop.cell.size,
> gtop.cells.dim)
> str(cangrid.gtop)
> Formal class 'GridTopology' [package "sp"] with 3 slots
>  ..@ cellcentre.offset: num [1:2] -633000 -4422000
>  ..@ cellsize         : num [1:2] 50000 50000
>  ..@ cells.dim        : int [1:2] 6 10
>
> SpatialGridDataFrame(grid=cangrid.gtop, data=my.df, proj4string=CRS(p4s))
> Error in validityMethod(object) :
>  unequal number of objects in full grid and data slot
>
>
> I have included an image of what the coordinates look like. Here is a
> description of my variables:
> p4s
> "+proj=stere +lat_0=90 +lat_ts=60 +lon_0=-110 +k=1 +x_0=0 +y_0=0
> +ellps=GRS80 +datum=NAD83 +units=m +no_defs"
>
> spatial.coords
>         x        y
> 1  -532500 -4422000
> 2  -482500 -4422000
> 3  -432500 -4422000
> 4  -382000 -4422000
> 5  -633000 -4372000
> 6  -582500 -4372000
> 7  -532500 -4372000
> 8  -482500 -4372000
> 9  -432500 -4372000
> 10 -382000 -4372000
> 11 -583000 -4322000
> 12 -532500 -4322000
> 13 -482500 -4322000
> 14 -432500 -4322000
> 15 -382000 -4322000
> 16 -583000 -4272000
> 17 -532500 -4272000
> 18 -482500 -4272000
> 19 -432500 -4272000
> 20 -382000 -4272000
> 21 -583000 -4222000
> 22 -532500 -4222000
> 23 -482500 -4222000
> 24 -432500 -4222000
> 25 -583000 -4172000
> 26 -532500 -4172000
> 27 -482500 -4172000
> 28 -583000 -4122000
> 29 -532500 -4122000
> 30 -482500 -4122000
> 31 -633000 -4071500
> 32 -583000 -4072000
> 33 -532500 -4072000
> 34 -583000 -4021500
> 35 -633000 -3971500
>
> str(my.df)
> 'data.frame':   35 obs. of  2 variables:
> $ X1: num  140.5 84.2 66.3 74.7 98.2 ...
> $ X2: num  155.8 73.9 57.4 72.6 88.4 ...
>
> Many thanks,
> Hailey
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From ayman.abdulrahman at gmail.com  Sat Apr  2 13:33:45 2011
From: ayman.abdulrahman at gmail.com (Ayman Abdulrahman)
Date: Sat, 2 Apr 2011 14:33:45 +0300
Subject: [R-sig-Geo] salam
Message-ID: <BANLkTi=9nuCiUj07dQqcgBMjV10zB9VBUg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110402/32520f14/attachment.pl>

From edzer.pebesma at uni-muenster.de  Sat Apr  2 16:10:15 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Sat, 02 Apr 2011 16:10:15 +0200
Subject: [R-sig-Geo] Question on lags in h-Scattergrams
In-Reply-To: <alpine.LNX.2.00.1104011314550.3160@salmo.appl-ecosys.com>
References: <alpine.LNX.2.00.1104011314550.3160@salmo.appl-ecosys.com>
Message-ID: <4D972E47.7050205@uni-muenster.de>



On 04/01/2011 10:22 PM, Rich Shepard wrote:
>   My question is not R specific, but about h-scattergrams as described in
> Pierre Goovaerts's "Geostatistics for Natural Resource Evaluation." No
> matter how many times I read these sections I still cannot grok the
> relationship between delta-h (which he sets at 100m in the discussions
> beginning in Section 2.3.2 on page 25) and the examples in following
> sections.
> 
>   For example, on page 27, Figure 2.8 shows Cd concentrations at 6 classes
> of distances, 'h' with distance tolerance 100m (the delta-h). Where does
> the
> 100m distance show up in the plots with h=43m through h=1024m? This is what
> confuses me.

It seems to me that in figure 2.8 the average distance of all point
pairs in a particular lag class is shown, but the exact class boundaries
are not given (delta h = 100, but the h values in h +/- delta h are
not). Looking at the average values, I would assume lag intervals 0-100,
100-300, 300-500 etc. If I try to reproduce this, as in

library(gstat)
data(jura)
example(jura)
hscat(Cd~1, jura.pred, c(0,100,300,500,700,900,1100)/1000,
variogram.alpha=90, tol.hor=22.5, asp="iso")

I'm getting quite similar, but not identical results, so some of my
assumptions must be false. Maybe Pierre can help us out.

Note that the intervals in this plot are in km.

With best regards,

>   Learning spatial statistics on my own (all my stat classes in grad school
> were biometry focused) leaves me lacking a local resource to ask.
> 
> TIA,
> 
> Rich
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From nick at hamm.org  Sun Apr  3 14:24:43 2011
From: nick at hamm.org (Nick Hamm)
Date: Sun, 3 Apr 2011 14:24:43 +0200
Subject: [R-sig-Geo] focalFilter problem raster package
Message-ID: <BANLkTi=TZZg1CnCqw4ZDb3u7NUUPV_TtkQ@mail.gmail.com>

Dear all

I am experiencing a strange problem when using the focalFilter
function in the raster package.

Whilst checking some code, I tried it on a toy dataset (p267) and a
simple function.  I have a set of filters in a list (w.ij).  These can
be found at http://dl.dropbox.com/u/15122401/Rproblem/toyexample.zip
or http://dl.dropbox.com/u/15122401/Rproblem/toyexample.Rdata

p267 is a 16x16 matrix.  The filters are all 15x15 binary filters.

I take the filter and move it around the matrix (4 centre points
(8,8), (8,9), (9,8), (9,9)). At each of these points I multiply the
filter by the underlying values in p267.  I then sum the new matrix,
as follows

sum(w.ij[[2]] * p267[-16,-16])
sum(w.ij[[2]] * p267[-16,-1])
sum(w.ij[[2]] * p267[-1,-16])
sum(w.ij[[2]] * p267[-1,-1])

I can repeat this for the other filters in the w.ij list.

I then try to do this using raster (eventualy I want to apply such
functions to a raster image not a small matrix), so I try:


p267.r <- raster(p267)

for(i in 1:5){
	tmp	<- focalFilter(p267.r, filter=w.ij[[i]], fun=sum)
	print(tmp at data@values[which(tmp at data@values != "NA")])
}


As you can see, the values obtained from the focalFilter function do
not match my test.  I cannot see why this should be.  Can anybody
help?

Nick





---
> sessionInfo()
R version 2.12.2 (2011-02-25)
Platform: i386-pc-mingw32/i386 (32-bit)

locale:
[1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United
Kingdom.1252
[3] LC_MONETARY=English_United Kingdom.1252 LC_NUMERIC=C
[5] LC_TIME=English_United Kingdom.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] raster_1.8-3 sp_0.9-79

loaded via a namespace (and not attached):
[1] grid_2.12.2     lattice_0.19-17 tools_2.12.2
>
----


From karl at huftis.org  Mon Apr  4 10:31:00 2011
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Mon, 4 Apr 2011 10:31:00 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
Message-ID: <inbvj0$gvv$1@dough.gmane.org>

It?s sometimes useful to visualise maps with colours for each
polygon, but where adjacent polygons have different colours.

I recently had to do this, and hope the resulting code may be
of use to others on this list. Note that the code uses a quick
and simple greedy algorithm, which is not guaranteed to find
a colouring using the *fewest* number of colours.

It is *possible* to colour every map using at most four colours
(try searching for ?four colour theorem? in your favourite search
engine), but finding the optimal colouring isn?t easy, and an
algorithm implementing this would be much more complicated, and
in some cases rather slow. In practice, the greedy algorithm
usually works very well. Note that sometimes it helps to order
the polygons in some systematic way first, e.g., by the number 
of neighbours (the polygon with most neighbours first); see the 
example at the end.

I would appreciate any suggestions for improvements to the code.

# Generate colour numbers (indices) for colouring maps
generateMapColours=function(x) {
	nb=spdep::poly2nb(x)   # Generate neighbours lists
	n=length(x)            # Number of polygons
	cols=numeric(n)        # Initial colouring
	cols[1]=1              # Let the first polygon have colour 1
	cols1n=1:n             # Available colour indices
	for(i in 2:n)
		cols[i]=which.min(cols1n %in% cols[nb[[i]]])
	cols
}

## Example
library(rgdal)    # Needed for maps in EPSG 4326 not to be interpreted as projected
library(maps)     # For map data
library(maptools) # For converting map data to ?sp? objects

# Fetch a map
x.map = map("state", fill=TRUE, col="transparent", plot=FALSE)
x.sp = map2SpatialPolygons(x.map, x.map$names, proj4string=CRS("+init=epsg:4326"))

# Plot the map with the calculated colours
cols = generateMapColours(x.sp) # Generate the colour indices
ncols = max(cols)               # Number of colours used
library(RColorBrewer)           # Nice colour palettes for maps
pal = brewer.pal(ncols, "Set1") # Try ?display.brewer.all()? for some alternative palettes
plot(x.sp, col=pal[cols])       # Plot the map

# Ordering the polygons may help.
# For instance, running the colouring algorithm
# on the following produces a four-colouring.
nb = spdep::poly2nb(x.sp)
ord = order(sapply(nb, length), decreasing=TRUE)
x.sp = x.sp[ord,]

-- 
Karl Ove Hufthammer


From karl at huftis.org  Mon Apr  4 11:26:50 2011
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Mon, 4 Apr 2011 11:26:50 +0200
Subject: [R-sig-Geo] Circles as polygons?
References: <95DCE398-2242-4646-ADE5-9482C6E6D77A@sevenc.co.za>
	<4D9326AB.4050004@gmail.com>
Message-ID: <inc2rm$4j8$1@dough.gmane.org>

Alexandre Villers wrote:

> Have a look at the disc() function in spatstat. I guess you can then
> either do what you aim at directly with spatstat, or after converting
> spatstat objects to sp class with maptools appropriate functions.

You can also use ?gBuffer? in the new ?rgeos? package. This turns 
SpatialPoints into SpatialPolygons (circles), and works directly on ?sp? 
objects, without any need for conversion.

Make sure the data is in an appropriate projection (e.g., a UTM projection), 
as ?circles? generated on unprojected points (i.e., points with coordinates 
given in longitude and latitude) are *not* circles, unless all your data lie 
on the equator.

-- 
Karl Ove Hufthammer


From edzer.pebesma at uni-muenster.de  Mon Apr  4 12:38:21 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Mon, 04 Apr 2011 12:38:21 +0200
Subject: [R-sig-Geo] gstat::krige: different spatial extent of 'newdata'
 and	kriged object
In-Reply-To: <AANLkTimxdKp_HqFRsF73CU-r0OPGcYk5-GWxbEACUNfr@mail.gmail.com>
References: <AANLkTimxdKp_HqFRsF73CU-r0OPGcYk5-GWxbEACUNfr@mail.gmail.com>
Message-ID: <4D999F9D.4010203@uni-muenster.de>

Thanks for the clear report Mauricio - very helpful. Package gstat
version 0.9-80, now submitted to CRAN, should resolve this.

With best regards,

On 04/01/2011 09:59 AM, Mauricio Zambrano wrote:
> Dear list,
> 
> While using the 'krige' function of gstat, I realised that -in some
> cases- the spatial extent of the object obtained with the 'krige'
> function is different from the spatial extent of the object provided
> as 'newdata' argument to the function.
> 
> In particular this occurred when the map passed as 'new data' argument
> ('SpatialGridDataFrame' object, read with readGDAL) had some empty
> rows in the upper part of the map.
> 
> Below there is a reproducible example (R version 2.12.2 and gstat_0.9-79 ):
> 
> 
> ----------------- START --------------------
> library(gstat)
> 
> data(meuse)
> coordinates(meuse) = ~x+y
> 
> data(meuse.grid)
> 
> # Removing unnecessary bands
> meuse.grid$part.a <- NULL
> meuse.grid$part.b <- NULL
> meuse.grid$dist <- NULL
> meuse.grid$ffreq <- NULL
> gridded(meuse.grid) = ~x+y
> 
> # Getting the class of 'meuse.grid' (SpatialPixelsDataFrame)
> class(meuse.grid)
> #"SpatialPixelsDataFrame"
> 
> summary(meuse.grid)
> #nrows=104
> #ncols=78
> 
> # B) Transforming 'meuse.grid' from 'SpatialPixelsDataFrame' to
> 'SpatialGridDataFrame'
> meuse.grid <- as(meuse.grid,'SpatialGridDataFrame')
> 
> # Variogram
> m <- vgm(.59, "Sph", 874, .04)
> 
> x1 <- krige(log(zinc)~1, locations=meuse, newdata=meuse.grid, model = m)
> summary(x1)
> #nrows=104
> #ncols=78
> 
> # In this example, the resulting object 'x1' has the same amount of
> rows and column of 'meuse.grid'
> 
> ###################################
> # Putting some NA's in 'meuse.grid'
> meuse.grid at data[1:156,] <- NA
> summary(meuse.grid)
> #nrows=104
> #ncols=78
> 
> x2 <- krige(log(zinc)~1, locations=meuse, newdata=meuse.grid, model = m)
> summary(x2)
> #nrows=102
> #ncols=78
> 
> ----------------- END --------------------
> 
> In the latest example, why 'x2' has 102 rows if 'meuse.grid' has 104 ?
> 
> 
> Thanks in advance for any help,
> 
> Mauricio Zambrano B.
> 

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From tom.gottfried at wzw.tum.de  Mon Apr  4 14:02:45 2011
From: tom.gottfried at wzw.tum.de (Tom Gottfried)
Date: Mon, 4 Apr 2011 14:02:45 +0200
Subject: [R-sig-Geo] Residual variogram from known trend
Message-ID: <4D99B365.3060101@wzw.tum.de>

Dear list,

is there a way in gstat to calculate a variogram from residuals of a known trend (that can not be 
estimated from the data) without passing the residuals to variogram() or gstat()? I tried the 
arguments trend.beta respectively beta. The first one gives an error, the second gives the same 
result as when I do not give it:

library(gstat)
data(meuse)
# trying with variogram(..., trend.beta=...)
var1 <- variogram(zinc~sqrt(dist), ~x+y, meuse)
var2 <- variogram(zinc~sqrt(dist), ~x+y, meuse, trend.beta=c(1000, -1000))

# trying with gstat(..., beta=...)
coordinates(meuse) <- ~x+y
meuse1 <- gstat(NULL, "zinc", zinc~sqrt(dist), meuse)
var1 <- variogram(meuse1)
meuse2 <- gstat(NULL, "zinc", zinc~sqrt(dist), meuse, beta=c(1000, -1000))
var2 <- variogram(meuse2)
identical(var1, var2) # both variograms are identical

I see it's possible to calculate the residuals with

residuals(lm(zinc~sqrt(dist), meuse))

and pass them directly. But I wonder whether a more convenient direct passing of the trend 
coefficients is possible.

Thanks!
Tom

-- 
Technische Universit?t M?nchen
Department f?r Pflanzenwissenschaften
Lehrstuhl f?r Gr?nlandlehre
Alte Akademie 12
85350 Freising / Germany
Phone: ++49 (0)8161 715324
Fax:   ++49 (0)8161 713243
email: tom.gottfried at wzw.tum.de
http://www.wzw.tum.de/gruenland


From gianni.lavaredo at gmail.com  Mon Apr  4 16:37:25 2011
From: gianni.lavaredo at gmail.com (gianni lavaredo)
Date: Mon, 4 Apr 2011 16:37:25 +0200
Subject: [R-sig-Geo] help to rotate around a centre a owin square
Message-ID: <BANLkTim8Enw18djNWkOQJejQHqCRy+4fzA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110404/4afa096f/attachment.pl>

From roman.lustrik at gmail.com  Mon Apr  4 16:41:53 2011
From: roman.lustrik at gmail.com (=?UTF-8?Q?Roman_Lu=C5=A1trik?=)
Date: Mon, 4 Apr 2011 16:41:53 +0200
Subject: [R-sig-Geo] help to rotate around a centre a owin square
In-Reply-To: <BANLkTim8Enw18djNWkOQJejQHqCRy+4fzA@mail.gmail.com>
References: <BANLkTim8Enw18djNWkOQJejQHqCRy+4fzA@mail.gmail.com>
Message-ID: <BANLkTim_wkofS2GLxwwzUGgrcp=WkngqWA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110404/60abc61e/attachment.pl>

From gianni.lavaredo at gmail.com  Mon Apr  4 16:58:23 2011
From: gianni.lavaredo at gmail.com (gianni lavaredo)
Date: Mon, 4 Apr 2011 16:58:23 +0200
Subject: [R-sig-Geo] help to rotate around a centre a owin square
In-Reply-To: <BANLkTim_wkofS2GLxwwzUGgrcp=WkngqWA@mail.gmail.com>
References: <BANLkTim8Enw18djNWkOQJejQHqCRy+4fzA@mail.gmail.com>
	<BANLkTim_wkofS2GLxwwzUGgrcp=WkngqWA@mail.gmail.com>
Message-ID: <BANLkTimxKQ-XCxYqOf0wbVV4q4A+R1GZWQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110404/ec62d819/attachment.pl>

From roman.lustrik at gmail.com  Mon Apr  4 17:20:55 2011
From: roman.lustrik at gmail.com (=?UTF-8?Q?Roman_Lu=C5=A1trik?=)
Date: Mon, 4 Apr 2011 17:20:55 +0200
Subject: [R-sig-Geo] help to rotate around a centre a owin square
In-Reply-To: <BANLkTimxKQ-XCxYqOf0wbVV4q4A+R1GZWQ@mail.gmail.com>
References: <BANLkTim8Enw18djNWkOQJejQHqCRy+4fzA@mail.gmail.com>
	<BANLkTim_wkofS2GLxwwzUGgrcp=WkngqWA@mail.gmail.com>
	<BANLkTimxKQ-XCxYqOf0wbVV4q4A+R1GZWQ@mail.gmail.com>
Message-ID: <BANLkTimWgF+haQxbcLG_imGAX2ycWtBN0g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110404/81f406aa/attachment.pl>

From mathieu.rajerison at gmail.com  Mon Apr  4 17:47:49 2011
From: mathieu.rajerison at gmail.com (Mathieu Rajerison)
Date: Mon, 4 Apr 2011 17:47:49 +0200
Subject: [R-sig-Geo] Order a SpatialPolygonsDataFrame by a column: area
Message-ID: <BANLkTi=xQW2xYy1HZN7PTejHkq17==b6-g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110404/5533c816/attachment.pl>

From jianfeng.mao at gmail.com  Mon Apr  4 18:17:32 2011
From: jianfeng.mao at gmail.com (Mao Jianfeng)
Date: Mon, 4 Apr 2011 18:17:32 +0200
Subject: [R-sig-Geo] summarize cells of different value intervals in a
 raster file by items in ESRI shapefile
Message-ID: <BANLkTikNJ56ABPHDOWk6U6KeqB0ONPVS+g@mail.gmail.com>

Dear R-Geo listers,

I have a raster file (cell value varies from 0 to 1) and a ESRI
shapefile (there are countries, and provinces inside each countries),
the two files overlapped in the same region. I want to summarized the
cells (with different value intervals, say 0.0-0.25, 0.25-0.5,
0.5-0.75, 0.75-0.1) by countries and provinces in countries. This
means I want to output how many cells of (0.0-0.25, et al.) in each
country and each province.

Are there tools in R to do my job? Could you show me a dummy example of that?

Thanks in advance.


From gianni.lavaredo at gmail.com  Mon Apr  4 18:51:12 2011
From: gianni.lavaredo at gmail.com (gianni lavaredo)
Date: Mon, 4 Apr 2011 18:51:12 +0200
Subject: [R-sig-Geo] help to rotate around a centre a owin square
In-Reply-To: <BANLkTimWgF+haQxbcLG_imGAX2ycWtBN0g@mail.gmail.com>
References: <BANLkTim8Enw18djNWkOQJejQHqCRy+4fzA@mail.gmail.com>
	<BANLkTim_wkofS2GLxwwzUGgrcp=WkngqWA@mail.gmail.com>
	<BANLkTimxKQ-XCxYqOf0wbVV4q4A+R1GZWQ@mail.gmail.com>
	<BANLkTimWgF+haQxbcLG_imGAX2ycWtBN0g@mail.gmail.com>
Message-ID: <BANLkTi=U0k7ZHGjsJOhjKZc8j4Wut-tUEg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110404/4f87a90f/attachment.pl>

From edzer.pebesma at uni-muenster.de  Mon Apr  4 18:52:43 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Mon, 04 Apr 2011 18:52:43 +0200
Subject: [R-sig-Geo] Order a SpatialPolygonsDataFrame by a column: area
In-Reply-To: <BANLkTi=xQW2xYy1HZN7PTejHkq17==b6-g@mail.gmail.com>
References: <BANLkTi=xQW2xYy1HZN7PTejHkq17==b6-g@mail.gmail.com>
Message-ID: <4D99F75B.1030207@uni-muenster.de>



On 04/04/2011 05:47 PM, Mathieu Rajerison wrote:
> Hi List!
> 
> I've got a Spatial Polygons Data Frame and have calculated the area of the
> polygons (area column)
> 
> I wondered how I could now order my features by area with small features on
> the top..

If you mean by "on the top" as "from first to last", then try

x[order(x$area),]

If you mean by "on the top", being plotted later than any other, you
might want the reverse ordering:

x[rev(order(x$area)),]

?
> 
> Any idea?
> 
> Thanks,
> 
> Mathieu
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From r.turner at auckland.ac.nz  Tue Apr  5 00:05:19 2011
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 5 Apr 2011 10:05:19 +1200
Subject: [R-sig-Geo] help to rotate around a centre a owin square
In-Reply-To: <BANLkTim8Enw18djNWkOQJejQHqCRy+4fzA@mail.gmail.com>
References: <BANLkTim8Enw18djNWkOQJejQHqCRy+4fzA@mail.gmail.com>
Message-ID: <4D9A409F.9050901@auckland.ac.nz>

On 05/04/11 02:37, gianni lavaredo wrote:
> Dear Reasearcher,
>
> I wish to rotate randomly a owin square aroud the centre of owin square. I
> try to use "rotate" in spatstat but it's not the function i need.


Actually "rotate" ***is*** the function you need, but you need to do a 
little
bit more work.  First *shift* the window you wish to rotate so that its
centroid is at the origin.  Then rotate the result, and finally shift back
again.

This can all be bundled up in a simple function as follows:

foo <- function (W,angle=NULL) {
U <- shift(W,origin="centroid")
if(is.null(angle)) angle <- runif(1,-pi/2,pi/2)
U <- rotate(U,angle)
shift(U,vec=unlist(centroid.owin(W)))
}

Check:

SQ <- unit.square()
plot(owin(c(-1,2),c(-1,2)),main="")
plot(SQ,add=TRUE)
plot(foo(SQ),add=TRUE,border="red")
plot(foo(SQ,angle=-pi/3),add=TRUE,border="blue")


     cheers,

         Rolf Turner


From r.hijmans at gmail.com  Tue Apr  5 07:23:18 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Mon, 4 Apr 2011 22:23:18 -0700 (PDT)
Subject: [R-sig-Geo] focalFilter problem raster package
In-Reply-To: <BANLkTi=TZZg1CnCqw4ZDb3u7NUUPV_TtkQ@mail.gmail.com>
References: <BANLkTi=TZZg1CnCqw4ZDb3u7NUUPV_TtkQ@mail.gmail.com>
Message-ID: <1301980998310-6241121.post@n2.nabble.com>


> I am experiencing a strange problem when using the focalFilter 
> function in the raster package. 

Nick, 
Could you provide a self-contained example that does not require downloading
files? Given your description that should not be very hard.
Robert

--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/focalFilter-problem-raster-package-tp6235925p6241121.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From r.hijmans at gmail.com  Tue Apr  5 07:27:08 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Mon, 4 Apr 2011 22:27:08 -0700 (PDT)
Subject: [R-sig-Geo] summarize cells of different value intervals in a
 raster file by items in ESRI shapefile
In-Reply-To: <BANLkTikNJ56ABPHDOWk6U6KeqB0ONPVS+g@mail.gmail.com>
References: <BANLkTikNJ56ABPHDOWk6U6KeqB0ONPVS+g@mail.gmail.com>
Message-ID: <1301981228354-6241129.post@n2.nabble.com>

> I have a raster file (cell value varies from 0 to 1) and a ESRI 
> shapefile (there are countries, and provinces inside each countries), 
> the two files overlapped in the same region. I want to summarized the 
> cells (with different value intervals, say 0.0-0.25, 0.25-0.5, 
> 0.5-0.75, 0.75-0.1) by countries and provinces in countries. This 
> means I want to output how many cells of (0.0-0.25, et al.) in each 
> country and each province. 

Jianfeng, you can have a look at the polygon example for the extract
function in raster. Robert

library(raster)
?extract 


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/summarize-cells-of-different-value-intervals-in-a-raster-file-by-items-in-ESRI-shapefile-tp6239189p6241129.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From r.m.krug at gmail.com  Tue Apr  5 10:28:53 2011
From: r.m.krug at gmail.com (Rainer M Krug)
Date: Tue, 5 Apr 2011 10:28:53 +0200
Subject: [R-sig-Geo] [R] Euclidean Distance in R
In-Reply-To: <BANLkTikQO5ced7U1sPPYdJufh-EFVTqQgg@mail.gmail.com>
References: <BANLkTikQO5ced7U1sPPYdJufh-EFVTqQgg@mail.gmail.com>
Message-ID: <4D9AD2C5.1060907@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 05/04/11 03:43, Paul Duckett wrote:
> Hi

Hi

> 
> 1. I have two raster files *.asc (identical size)

This question is much more appropriate for the r-sig-geo mailing list
(https://stat.ethz.ch/mailman/listinfo/r-sig-geo), which focusses on
spatial analysis / modelling in R.

I am sure you will get an answer there.

I take the liberty to CC this mail to the list - and I would encourage
you to subscribe to the mailing list.

Cheers,

Rainer


> 2. The data in each contain presence or absence data in each cell
> represented by a 1 or 0 respectively
> 3. I would like to take the location of each 1 (presence cell) in
> raster file 1 and measure the euclidean distance to the nearest 1
> (presence cell) in raster file 2.
> 
> Obviously in some cases there will be overlap so the distance will be zero.
> 
> 4. I would like the output file to have each individual measurement on
> a seperate line in a single file.
> 
> 
> I am very new to R, so any help would be appreciated.
> 
> Best regards
> Paul


- -- 
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation
Biology, UCT), Dipl. Phys. (Germany)

Centre of Excellence for Invasion Biology
Natural Sciences Building
Office Suite 2039
Stellenbosch University
Main Campus, Merriman Avenue
Stellenbosch
South Africa

Tel:        +33 - (0)9 53 10 27 44
Cell:       +27 - (0)8 39 47 90 42
Fax (SA):   +27 - (0)8 65 16 27 82
Fax (D) :   +49 - (0)3 21 21 25 22 44
Fax (FR):   +33 - (0)9 58 10 27 44
email:      Rainer at krugs.de

Skype:      RMkrug
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/

iEYEARECAAYFAk2a0sUACgkQoYgNqgF2egqpqACfa2FdwXYwn7i+woC6RnFnURE8
p2kAn1Q833jkNyG9EfkQUIoycsdlDJWp
=aykq
-----END PGP SIGNATURE-----


From sebastian.schwindt at hoki.ibp.fraunhofer.de  Tue Apr  5 15:03:14 2011
From: sebastian.schwindt at hoki.ibp.fraunhofer.de (ssc)
Date: Tue, 05 Apr 2011 15:03:14 +0200
Subject: [R-sig-Geo] Creating a heat map
Message-ID: <4D9B1312.80701@hoki.ibp.fraunhofer.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110405/b3920c83/attachment.pl>

From mathieu.rajerison at gmail.com  Tue Apr  5 15:54:39 2011
From: mathieu.rajerison at gmail.com (Mathieu Rajerison)
Date: Tue, 5 Apr 2011 15:54:39 +0200
Subject: [R-sig-Geo] Order a SpatialPolygonsDataFrame by a column: area
In-Reply-To: <4D99F75B.1030207@uni-muenster.de>
References: <BANLkTi=xQW2xYy1HZN7PTejHkq17==b6-g@mail.gmail.com>
	<4D99F75B.1030207@uni-muenster.de>
Message-ID: <BANLkTinhu-h5nDnmBVivaHncKO2d8YKzRQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110405/39b77b81/attachment.pl>

From mathieu.rajerison at gmail.com  Tue Apr  5 16:13:52 2011
From: mathieu.rajerison at gmail.com (Mathieu Rajerison)
Date: Tue, 5 Apr 2011 16:13:52 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
In-Reply-To: <inbvj0$gvv$1@dough.gmane.org>
References: <inbvj0$gvv$1@dough.gmane.org>
Message-ID: <BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110405/188affcf/attachment.pl>

From r.m.krug at gmail.com  Tue Apr  5 16:20:50 2011
From: r.m.krug at gmail.com (Rainer M Krug)
Date: Tue, 5 Apr 2011 16:20:50 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
 colour
In-Reply-To: <inbvj0$gvv$1@dough.gmane.org>
References: <inbvj0$gvv$1@dough.gmane.org>
Message-ID: <4D9B2542.1020400@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 04/04/11 10:31, Karl Ove Hufthammer wrote:
> It?s sometimes useful to visualise maps with colours for each
> polygon, but where adjacent polygons have different colours.
> 
> I recently had to do this, and hope the resulting code may be
> of use to others on this list. Note that the code uses a quick
> and simple greedy algorithm, which is not guaranteed to find
> a colouring using the *fewest* number of colours.
> 
> It is *possible* to colour every map using at most four colours
> (try searching for ?four colour theorem? in your favourite search
> engine), but finding the optimal colouring isn?t easy, and an
> algorithm implementing this would be much more complicated, and
> in some cases rather slow. In practice, the greedy algorithm
> usually works very well. Note that sometimes it helps to order
> the polygons in some systematic way first, e.g., by the number 
> of neighbours (the polygon with most neighbours first); see the 
> example at the end.
> 
> I would appreciate any suggestions for improvements to the code.

I think the idea is really great. I couldn't try it out yet, but if it
is working, I think it should be added to one of the spatial packages -
maybe sp?

Rainer

> 
> # Generate colour numbers (indices) for colouring maps
> generateMapColours=function(x) {
> 	nb=spdep::poly2nb(x)   # Generate neighbours lists
> 	n=length(x)            # Number of polygons
> 	cols=numeric(n)        # Initial colouring
> 	cols[1]=1              # Let the first polygon have colour 1
> 	cols1n=1:n             # Available colour indices
> 	for(i in 2:n)
> 		cols[i]=which.min(cols1n %in% cols[nb[[i]]])
> 	cols
> }
> 
> ## Example
> library(rgdal)    # Needed for maps in EPSG 4326 not to be interpreted as projected
> library(maps)     # For map data
> library(maptools) # For converting map data to ?sp? objects
> 
> # Fetch a map
> x.map = map("state", fill=TRUE, col="transparent", plot=FALSE)
> x.sp = map2SpatialPolygons(x.map, x.map$names, proj4string=CRS("+init=epsg:4326"))
> 
> # Plot the map with the calculated colours
> cols = generateMapColours(x.sp) # Generate the colour indices
> ncols = max(cols)               # Number of colours used
> library(RColorBrewer)           # Nice colour palettes for maps
> pal = brewer.pal(ncols, "Set1") # Try ?display.brewer.all()? for some alternative palettes
> plot(x.sp, col=pal[cols])       # Plot the map
> 
> # Ordering the polygons may help.
> # For instance, running the colouring algorithm
> # on the following produces a four-colouring.
> nb = spdep::poly2nb(x.sp)
> ord = order(sapply(nb, length), decreasing=TRUE)
> x.sp = x.sp[ord,]
> 


- -- 
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation
Biology, UCT), Dipl. Phys. (Germany)

Centre of Excellence for Invasion Biology
Natural Sciences Building
Office Suite 2039
Stellenbosch University
Main Campus, Merriman Avenue
Stellenbosch
South Africa

Tel:        +33 - (0)9 53 10 27 44
Cell:       +27 - (0)8 39 47 90 42
Fax (SA):   +27 - (0)8 65 16 27 82
Fax (D) :   +49 - (0)3 21 21 25 22 44
Fax (FR):   +33 - (0)9 58 10 27 44
email:      Rainer at krugs.de

Skype:      RMkrug
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/

iEYEARECAAYFAk2bJUIACgkQoYgNqgF2egqM/wCeOe1+UNZaix8PTpLiGY1hZpGD
HHsAn0XGObs/D9QyN8JC3L1V2oFUgMdq
=1EVt
-----END PGP SIGNATURE-----


From b.rowlingson at lancaster.ac.uk  Tue Apr  5 16:24:30 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 5 Apr 2011 15:24:30 +0100
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
In-Reply-To: <BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
References: <inbvj0$gvv$1@dough.gmane.org>
	<BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
Message-ID: <BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>

On Tue, Apr 5, 2011 at 3:13 PM, Mathieu Rajerison
<mathieu.rajerison at gmail.com> wrote:
> I don't know how to accomplish that in R, but in QGIS, you can use the
> Topocolours plugin to accomplish that.

 topocolours (I wrote that!) uses the same greedy algorithm once its
worked out the connectivity. I did make topocolours have the
flexibility to choose other algorithms but only implemented the greedy
one. The problem with real world maps is that some features have more
than one polygon (islands) and if you want them all coloured the same
you might not be able to do a 4-colour colouring!

 I also used ColorBrewer colours!

 It might be worth taking out the construction of the adjacency list
to another function, since I found recently that poly2nb gave very
different adjacencies to functions in rgeos.

Barry


From virgilio.gomez at uclm.es  Tue Apr  5 16:38:55 2011
From: virgilio.gomez at uclm.es (Virgilio =?ISO-8859-1?Q?G=F3mez-Rubio?=)
Date: Tue, 5 Apr 2011 16:38:55 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
 colour
In-Reply-To: <BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>
References: <inbvj0$gvv$1@dough.gmane.org>
	<BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
	<BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>
Message-ID: <1302014335.1824.51.camel@virgil-HP-Compaq-8000-Elite-USDT-PC>

Hi,

Apparently, package gcolor provides an algorithm to colour a map:


library(spdep)
library(gcolor)

example(readShapePoly)

nb.nc<-poly2nb(xx)
mat.nc<-nb2mat(nb.nc, style="B")

gcol<-ineq(mat.nc)

xx$COL<-as.factor(gcol)
spplot(xx, "COL")


It uses 4 colour for North Carolina, but I am not sure whether this is
THE 4-colour algorithm:

> table(gcol)
gcol
 1  2  3  4 
17 27 28 28 


Hope this helps,

Virgilio


El mar, 05-04-2011 a las 15:24 +0100, Barry Rowlingson escribi?:
> On Tue, Apr 5, 2011 at 3:13 PM, Mathieu Rajerison
> <mathieu.rajerison at gmail.com> wrote:
> > I don't know how to accomplish that in R, but in QGIS, you can use the
> > Topocolours plugin to accomplish that.
> 
>  topocolours (I wrote that!) uses the same greedy algorithm once its
> worked out the connectivity. I did make topocolours have the
> flexibility to choose other algorithms but only implemented the greedy
> one. The problem with real world maps is that some features have more
> than one polygon (islands) and if you want them all coloured the same
> you might not be able to do a 4-colour colouring!
> 
>  I also used ColorBrewer colours!
> 
>  It might be worth taking out the construction of the adjacency list
> to another function, since I found recently that poly2nb gave very
> different adjacencies to functions in rgeos.
> 
> Barry
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From gianni.lavaredo at gmail.com  Tue Apr  5 17:32:01 2011
From: gianni.lavaredo at gmail.com (gianni lavaredo)
Date: Tue, 5 Apr 2011 17:32:01 +0200
Subject: [R-sig-Geo] problem with "overlay" in sp Error type: object of type
 'closure' is not subsettable
Message-ID: <BANLkTi=CQwcNvX3h=9nSB6hUWo2w5uFOXA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110405/c3cf8550/attachment.pl>

From b.rowlingson at lancaster.ac.uk  Tue Apr  5 18:04:23 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 5 Apr 2011 17:04:23 +0100
Subject: [R-sig-Geo] problem with "overlay" in sp Error type: object of
 type 'closure' is not subsettable
In-Reply-To: <BANLkTi=CQwcNvX3h=9nSB6hUWo2w5uFOXA@mail.gmail.com>
References: <BANLkTi=CQwcNvX3h=9nSB6hUWo2w5uFOXA@mail.gmail.com>
Message-ID: <BANLkTinBbmEDff+Q1-Nwnirse1kn__yw1Q@mail.gmail.com>

>> p.ov <- overlay[onePLOT["Id"],p]
> Error in overlay[onePLOT["Id"], p] :
> ?object of type 'closure' is not subsettable
>
> thanks for help or suggestions

 'overlay' is a function, you are using square brackets!

 Try overlay(a,b) instead of overlay[a,b]

Barry


From massimodisasha at gmail.com  Tue Apr  5 18:42:38 2011
From: massimodisasha at gmail.com (Massimo Di Stefano)
Date: Tue, 5 Apr 2011 12:42:38 -0400
Subject: [R-sig-Geo] dendrogram of large dataset
Message-ID: <A168D464-1D79-490E-9B60-DE5DB73C4732@gmail.com>

Hi All,

I'm tring to generate a dendrogram from a set of observations.

i have all my data inside postgis, using some spatial query i extractd the data for a precise boundy-box and generate the following output in a csv file format :


a,b,c,d,e,f
1      67.12 4.280 1.78250 30 3 16001
2      67.12 4.280 1.78250 30 3 16001
3      66.57 4.280 1.35500 30 3 16001
...
16665  68.21 3.605 2.34190 48 2 18004
16666  68.21 3.605 2.34190 48 2 18004
...
18665 ... ... ... ... 18004
18666 ... ... ... ... 18004
...


where : 

a,b,c,d,e are parameters associated to a speciphic ID (f)  that represent a biological specie.



> data <- read.csv('x.txt', header = TRUE)
> data
           a     b       c  d e     f
1      67.12 4.280 1.78250 30 3 16001
2      67.12 4.280 1.78250 30 3 16001
3      66.57 4.280 1.35500 30 3 16001
...
16665  68.21 3.605 2.34190 48 2 16001
16666  68.21 3.605 2.34190 48 2 16001
 [ reached getOption("max.print") -- omitted 36039 rows ]]

> summary(data)
       a                b               c                d        
 Min.   : 57.97   Min.   :3.594   Min.   :0.7037   Min.   : 1.00  
 1st Qu.: 64.74   1st Qu.:4.299   1st Qu.:1.1792   1st Qu.: 6.00  
 Median : 67.30   Median :4.551   Median :1.4144   Median : 6.00  
 Mean   : 69.47   Mean   :5.796   Mean   :1.5356   Mean   :17.55  
 3rd Qu.: 69.13   3rd Qu.:8.861   3rd Qu.:1.7391   3rd Qu.:30.00  
 Max.   :133.69   Max.   :9.745   Max.   :4.9751   Max.   :54.00  
       e               f        
 Min.   :1.000   Min.   :10022  
 1st Qu.:1.000   1st Qu.:11027  
 Median :2.000   Median :16001  
 Mean   :2.088   Mean   :15191  
 3rd Qu.:3.000   3rd Qu.:16001  
 Max.   :4.000   Max.   :30016  

> str(data)
'data.frame':	52705 obs. of  6 variables:
 $ a: num  67.1 67.1 66.6 66.2 66.2 ...
 $ b: num  4.28 4.28 4.28 4.28 4.28 4.28 4.28 4.28 4.28 4.28 ...
 $ c: num  1.78 1.78 1.35 1.35 1.35 ...
 $ d: int  30 30 30 13 13 13 13 13 13 13 ...
 $ e: int  3 3 3 3 3 3 3 3 3 3 ...
 $ f: int  16001 16001 16001 16001 16001 16001 16001 16001 16001 16001 ...

# f is the specie-id (a,b,c,d,e are environmental parameters derived from measurments)
# i'm tring to grouping my species in several community based on common environmental parameters 

> x <- as.matrix(data[-6]) 
> x
          a     b       c  d e
[1,]  67.12 4.280 1.78250 30 3
[2,]  67.12 4.280 1.78250 30 3
[3,]  66.57 4.280 1.35500 30 3
...
[19997,]  67.30 9.736 1.25000  2 2
[19998,]  67.30 9.736 1.76990  2 2
[19999,]  67.85 9.735 1.15740  2 2
 [ reached getOption("max.print") -- omitted 32706 rows ]]



> x <- x[sample(seq_len(nrow(x))), ] 
> d <- dist(x)
Error: cannot allocate vector of size 10.3 Gb


> sessionInfo()
R version 2.12.1 (2010-12-16)
Platform: x86_64-apple-darwin9.8.0/x86_64 (64-bit)

locale:
[1] C

attached base packages:
[1] grid      stats     graphics  grDevices utils     datasets  methods  
[8] base     

other attached packages:
[1] seriation_1.0-3  colorspace_1.0-1 gclus_1.3        TSP_1.0-2       
[5] cluster_1.13.3  
> 



i'm using the package :  Seriation .. googling it seems to do what i need ... but gived me the previouse error
do you know i'f i'm doing something wrong ? or maybe i've to change something in the memory managment ?
i'm running R on a 64 bit debian sid distro (dual quad core, 4 gb ram)

(i tried to do a similar analisys on matlab, using the function "Manova" [1] , maybe it uses a different approach, but using the same dataset if no memory issue)
[1] http://www.mathworks.com/help/toolbox/stats/manovacluster.html


thanks for any hints!

Massimo.


From edzer.pebesma at uni-muenster.de  Tue Apr  5 20:11:38 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Tue, 05 Apr 2011 20:11:38 +0200
Subject: [R-sig-Geo] problem with "overlay" in sp Error type: object of
 type 'closure' is not subsettable
In-Reply-To: <BANLkTi=CQwcNvX3h=9nSB6hUWo2w5uFOXA@mail.gmail.com>
References: <BANLkTi=CQwcNvX3h=9nSB6hUWo2w5uFOXA@mail.gmail.com>
Message-ID: <4D9B5B5A.6020201@uni-muenster.de>



On 04/05/2011 05:32 PM, gianni lavaredo wrote:
> Dear Researchers,
> 
> I have two "SpatialPointsDataFrame" layer "p" and "onePLOT"  and I wish to
> do an overlay, but I have an error message
> 
>> str(p)
> Formal class 'SpatialPointsDataFrame' [package "sp"] with 5 slots
>   ..@ data       :'data.frame': 980000 obs. of  1 variable:
>   .. ..$ V3: num [1:980000] 1 1 1 1 1 1 1 1 1 1 ...
>   ..@ coords.nrs : int [1:2] 1 2
>   ..@ coords     : num [1:980000, 1:2] 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5
> 9.5 ...
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : NULL
>   .. .. ..$ : chr [1:2] "x" "y"
>   ..@ bbox       : num [1:2, 1:2] 0.5 0.5 1399.5 699.5
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : chr [1:2] "x" "y"
>   .. .. ..$ : chr [1:2] "min" "max"
>   ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slots
>   .. .. ..@ projargs: chr NA
> 
> 
>> str(onePLOT)
> Formal class 'SpatialPolygonsDataFrame' [package "sp"] with 5 slots
>   ..@ data       :'data.frame': 1 obs. of  2 variables:
>   .. ..$ Id      : Factor w/ 98 levels "LIDAR_Sicily_Area1_Plot1",..: 1
>   .. ..$ firecode: num 1
>   ..@ polygons   :List of 1
>   .. ..$ :Formal class 'Polygons' [package "sp"] with 5 slots
>   .. .. .. ..@ Polygons :List of 1
>   .. .. .. .. ..$ :Formal class 'Polygon' [package "sp"] with 5 slots
>   .. .. .. .. .. .. ..@ labpt  : num [1:2] 59 648
>   .. .. .. .. .. .. ..@ area   : num 900
>   .. .. .. .. .. .. ..@ hole   : logi FALSE
>   .. .. .. .. .. .. ..@ ringDir: int 1
>   .. .. .. .. .. .. ..@ coords : num [1:5, 1:2] 39.9 50 78.2 68.1 39.9 ...
>   .. .. .. .. .. .. .. ..- attr(*, "dimnames")=List of 2
>   .. .. .. .. .. .. .. .. ..$ : NULL
>   .. .. .. .. .. .. .. .. ..$ : chr [1:2] "x" "y"
>   .. .. .. ..@ plotOrder: int 1
>   .. .. .. ..@ labpt    : num [1:2] 59 648
>   .. .. .. ..@ ID       : chr "1"
>   .. .. .. ..@ area     : num 900
>   ..@ plotOrder  : int 1
>   ..@ bbox       : num [1:2, 1:2] 39.9 628.7 78.2 667.1
>   .. ..- attr(*, "dimnames")=List of 2
>   .. .. ..$ : chr [1:2] "x" "y"
>   .. .. ..$ : chr [1:2] "min" "max"
>   ..@ proj4string:Formal class 'CRS' [package "sp"] with 1 slots
>   .. .. ..@ projargs: chr NA
> 
> 
> when I try overlay I have this error message
> 
>> p.ov <- overlay[onePLOT["Id"],p]
> Error in overlay[onePLOT["Id"], p] :

Maybe you mean

overlay(onePlot["Id"], p)

?
Are you looking for coinciding points? In that case, there is also the
function zerodist.

Hth,

>   object of type 'closure' is not subsettable
> 
> thanks for help or suggestions
> Gianni
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From david.holstius at berkeley.edu  Tue Apr  5 22:14:03 2011
From: david.holstius at berkeley.edu (David Holstius)
Date: Tue, 5 Apr 2011 13:14:03 -0700
Subject: [R-sig-Geo] writeOGR with 2+ layers in 1 KML file
Message-ID: <23DE6D62-91B6-46B2-8740-C4A03D351D4A@berkeley.edu>

Hi, I would like to write two different Spatial* objects, as two different layers, to the same KML file using rgdal::writeOGR(). 

If I do something like:

  > writeOGR(bar, driver="KML", dsn="foo.kml", layer="bar")

... I get a KML Document with a Folder element inside named "bar". So far, so good. 

But if I follow that with:

  > writeOGR(baz, driver="KML", dsn="foo.kml", layer="baz")

... the contents of foo.kml are obliterated and replaced with new contents. I would prefer that another Folder, named "baz", be created in "foo.kml" without erasing any other Folder elements.

Is this possible? Is there a flag I didn't find? Am I doing it wrong? Many thanks in advance,

Very best,
David

--
David Holstius
PhD Student in Environmental Health Sciences
UC Berkeley School of Public Health


From Roger.Bivand at nhh.no  Tue Apr  5 22:47:22 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 5 Apr 2011 22:47:22 +0200 (CEST)
Subject: [R-sig-Geo] writeOGR with 2+ layers in 1 KML file
In-Reply-To: <23DE6D62-91B6-46B2-8740-C4A03D351D4A@berkeley.edu>
References: <23DE6D62-91B6-46B2-8740-C4A03D351D4A@berkeley.edu>
Message-ID: <alpine.LRH.2.00.1104052236520.3703@reclus.nhh.no>

On Tue, 5 Apr 2011, David Holstius wrote:

> Hi, I would like to write two different Spatial* objects, as two 
> different layers, to the same KML file using rgdal::writeOGR().
>
> If I do something like:
>
>  > writeOGR(bar, driver="KML", dsn="foo.kml", layer="bar")
>
> ... I get a KML Document with a Folder element inside named "bar". So 
> far, so good.
>
> But if I follow that with:
>
>  > writeOGR(baz, driver="KML", dsn="foo.kml", layer="baz")
>
> ... the contents of foo.kml are obliterated and replaced with new 
> contents. I would prefer that another Folder, named "baz", be created in 
> "foo.kml" without erasing any other Folder elements.
>
> Is this possible? Is there a flag I didn't find? Am I doing it wrong? 
> Many thanks in advance,

First, document which platform you are using, which version of rgdal, 
which version of GDAL, and how you installed them. The OGR/GDAL drivers 
change in behaviour between versions of GDAL. Next, check that ogr2ogr can 
write what you want outside R - that will show that the driver is capable 
of writing multiple layers to a file (that is, updating an existing file) 
for your driver of choice.

If it can, think how this might be done in writeOGR() and contribute your 
solution. My guess, based on http://www.gdal.org/ogr/drv_kml.html and 
http://www.gdal.org/ogr/drv_libkml.html, is that the representation of 
vector objects in sp is not compatible with a multi-layer OGC SFS 
representation (for this driver). I cannot see how one could merge 
multiple *.kml files either. It is in general not desirable to try to 
tweak writeOGR() to suit particular drivers.

Hope this clarifies,

Roger

>
> Very best,
> David
>
> --
> David Holstius
> PhD Student in Environmental Health Sciences
> UC Berkeley School of Public Health
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From tech_dev at wildintellect.com  Tue Apr  5 23:00:12 2011
From: tech_dev at wildintellect.com (Alex Mandel)
Date: Tue, 05 Apr 2011 14:00:12 -0700
Subject: [R-sig-Geo] writeOGR with 2+ layers in 1 KML file
In-Reply-To: <23DE6D62-91B6-46B2-8740-C4A03D351D4A@berkeley.edu>
References: <23DE6D62-91B6-46B2-8740-C4A03D351D4A@berkeley.edu>
Message-ID: <4D9B82DC.3080803@wildintellect.com>

On 04/05/2011 01:14 PM, David Holstius wrote:
> Hi, I would like to write two different Spatial* objects, as two different layers, to the same KML file using rgdal::writeOGR(). 
> 
> If I do something like:
> 
>   > writeOGR(bar, driver="KML", dsn="foo.kml", layer="bar")
> 
> ... I get a KML Document with a Folder element inside named "bar". So far, so good. 
> 
> But if I follow that with:
> 
>   > writeOGR(baz, driver="KML", dsn="foo.kml", layer="baz")
> 
> ... the contents of foo.kml are obliterated and replaced with new contents. I would prefer that another Folder, named "baz", be created in "foo.kml" without erasing any other Folder elements.
> 
> Is this possible? Is there a flag I didn't find? Am I doing it wrong? Many thanks in advance,
> 
> Very best,
> David
> 
> --
> David Holstius
> PhD Student in Environmental Health Sciences
> UC Berkeley School of Public Health
> 

Are you sure you don't mean 2 kml files in one kmz?
A kmz is a zip of multiple kml files and is the common format for Google
Earth.

It sounds like you're describing the behavior that the OGR shapefile
writer has which creates a folder and writes the shp, shx, dbf, prj all
to that folder. I'm not sure that model works from kml, in particular
the dsn would need to be a folder path not a path to a specific file.

I think you need to write 2 different kml files and then zip them into a
kmz. I have no idea if there's an R package to create Zip files.

Enjoy,
Alex


From pazurrobert at gmail.com  Wed Apr  6 00:52:52 2011
From: pazurrobert at gmail.com (Robert Pazur)
Date: Wed, 6 Apr 2011 00:52:52 +0200
Subject: [R-sig-Geo] =?iso-8859-1?q?values_of_Moran=B4s_I_index?=
Message-ID: <BANLkTi=EkPLeHhPEw=xAq_kOfEacsV6wqA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110406/0f1aba9f/attachment.pl>

From Roger.Bivand at nhh.no  Wed Apr  6 08:50:19 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 6 Apr 2011 08:50:19 +0200 (CEST)
Subject: [R-sig-Geo] =?iso-8859-1?q?values_of_Moran=B4s_I_index?=
In-Reply-To: <BANLkTi=EkPLeHhPEw=xAq_kOfEacsV6wqA@mail.gmail.com>
References: <BANLkTi=EkPLeHhPEw=xAq_kOfEacsV6wqA@mail.gmail.com>
Message-ID: <alpine.LRH.2.00.1104060847080.11822@reclus.nhh.no>

On Wed, 6 Apr 2011, Robert Pazur wrote:

> Dear all,
>
> with usage of spdep package I tried to explore the impact of changing
> location of occurrence of  1?s in binary fashion on Moran index  by shifting
> an artificial 9pixel- square (small square  left below in the picture) in
> all direction (based on 1 pixel move) in my squared area (500x500 pixels).
> www.scandinavia.sk/data/morans.jpg
> I was looking at the distances where the values of Moran?s I index decrease
> below particular threshold (in this case 0.6,0.5,0.4) and record this
> distances as some indexes in centroids of corresponding square.
> I expected that this values in my  area would by symmetrical in all
> directions.  But there are not.
> Have somebody some ideas why?

I'm afraid that there is far too little information here to know what your 
question is. Provide very concise sample code (for a 50 by 50 grid, for 
example) instead. You have not said how the weights were defined, or 
whether you are row-standardising the weights. The images suggest edge 
effects, but without a reproducible example, it isn't possible to say. Can 
you reproduce the same effect with a grid on a torus?

Roger

> Thanks in advance.
>
> Robert
> -------------------------------------------------------
> Robert Pazur
> PhD student
> Institute of Geography
> Slovak Academy Of Sciences
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From karl at huftis.org  Wed Apr  6 08:57:44 2011
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Wed, 6 Apr 2011 08:57:44 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
References: <inbvj0$gvv$1@dough.gmane.org>
	<BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
	<BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>
	<1302014335.1824.51.camel@virgil-HP-Compaq-8000-Elite-USDT-PC>
Message-ID: <inh2s2$o23$1@dough.gmane.org>

Virgilio G?mez-Rubio wrote:

> Apparently, package gcolor provides an algorithm to colour a map:
> 
> It uses 4 colour for North Carolina, but I am not sure whether this is
> THE 4-colour algorithm:

No, it?s not a 4-colouring algorithm. Indeed, on one map I have it only
produces a 5-colouring. It?s seems to be better than the greedy algorithm, 
though, even with reordering of the vertices by degrees, which produces a
6-colouring of the same data.

It?s very slow on large maps, though.

-- 
Karl Ove Hufthammer


From karl at huftis.org  Wed Apr  6 09:22:19 2011
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Wed, 6 Apr 2011 09:22:19 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
References: <inbvj0$gvv$1@dough.gmane.org>
	<BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
	<BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>
Message-ID: <inh4a5$vql$1@dough.gmane.org>

Barry Rowlingson wrote:

> topocolours (I wrote that!) uses the same greedy algorithm once its
> worked out the connectivity. I did make topocolours have the
> flexibility to choose other algorithms but only implemented the greedy
> one. The problem with real world maps is that some features have more
> than one polygon (islands) and if you want them all coloured the same
> you might not be able to do a 4-colour colouring!

Indeed. With multi-polygons you might really have a *non-planar* graph, for 
which the four colour theorem doesn?t apply. (But for polygons that are 
?real? islands, i.e., have no neighbouring polygons, this will never be a 
problem.)

> I also used ColorBrewer colours!

I love ColorBrewer, and wish ?Set1? it was the default palette for R ? :-)

> It might be worth taking out the construction of the adjacency list
> to another function, since I found recently that poly2nb gave very
> different adjacencies to functions in rgeos.

That?s very surprising, and seems to imply a bug in at least one of the 
functions. Do you have a publically available example showing this?

The ?poly2nb? does have a somewhat strange behaviour with polygons with no 
members, BTW. Instead of the corresponding neighbours vector being empty, it 
has one element, namely 0. This means that the length of the neighbour 
vector doesn?t necessarily correspond to the number of neighbours. It?s not 
documented, and can at least be seen as a misfeature.

(It also means that in the last example of my colouring code, which sorts 
the polygons by degree before colouring them, polygons with no neighbours 
are mixed with polygons with one neighbour. This will never have any effect 
on the resulting colouring, though, as the ?islands? will automatically get 
colour 1, and don?t influence the colouring of the other polygons.)

-- 
Karl Ove Hufthammer


From Roger.Bivand at nhh.no  Wed Apr  6 09:38:58 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 6 Apr 2011 09:38:58 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
 colour
In-Reply-To: <inh4a5$vql$1@dough.gmane.org>
References: <inbvj0$gvv$1@dough.gmane.org>
	<BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
	<BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>
	<inh4a5$vql$1@dough.gmane.org>
Message-ID: <alpine.LRH.2.00.1104060928510.11822@reclus.nhh.no>

On Wed, 6 Apr 2011, Karl Ove Hufthammer wrote:

> Barry Rowlingson wrote:
>
>> topocolours (I wrote that!) uses the same greedy algorithm once its
>> worked out the connectivity. I did make topocolours have the
>> flexibility to choose other algorithms but only implemented the greedy
>> one. The problem with real world maps is that some features have more
>> than one polygon (islands) and if you want them all coloured the same
>> you might not be able to do a 4-colour colouring!
>
> Indeed. With multi-polygons you might really have a *non-planar* graph, for 
> which the four colour theorem doesn?t apply. (But for polygons that are 
> ?real? islands, i.e., have no neighbouring polygons, this will never be a 
> problem.)
>
>> I also used ColorBrewer colours!
>
> I love ColorBrewer, and wish ?Set1? it was the default palette for R ? :-)
>
>> It might be worth taking out the construction of the adjacency list
>> to another function, since I found recently that poly2nb gave very
>> different adjacencies to functions in rgeos.
>
> That?s very surprising, and seems to imply a bug in at least one of the 
> functions. Do you have a publically available example showing this?

I've asked Barry off-list about this, there was a case a little while ago 
with unexpected results because of inserted sliver polygons causing 
apparently separate observations to be neighbours (the slivers were only 
visible when zooming right in). There are also differences in the way 
software may handle precision (snapping). More will follow ...

>
> The ?poly2nb? does have a somewhat strange behaviour with polygons with no 
> members, BTW. Instead of the corresponding neighbours vector being empty, it 
> has one element, namely 0. This means that the length of the neighbour 
> vector doesn?t necessarily correspond to the number of neighbours. It?s not 
> documented, and can at least be seen as a misfeature.

This is a feature of nb objects - card(nb) returns the number of 
neighbours, and should always be used. This is documented in ASDAR (p. 
240), and at length in Bivand & Portnov (2004). I'll add to the 
documentation in the package. If you use card(), you get the outcomes you 
expect.

Hope this helps,

Roger

>
> (It also means that in the last example of my colouring code, which sorts 
> the polygons by degree before colouring them, polygons with no neighbours 
> are mixed with polygons with one neighbour. This will never have any effect 
> on the resulting colouring, though, as the ?islands? will automatically get 
> colour 1, and don?t influence the colouring of the other polygons.)
>
> -- 
> Karl Ove Hufthammer
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From paul.hiemstra at knmi.nl  Wed Apr  6 09:50:15 2011
From: paul.hiemstra at knmi.nl (Paul Hiemstra)
Date: Wed, 6 Apr 2011 09:50:15 +0200
Subject: [R-sig-Geo] Residual variogram from known trend
In-Reply-To: <4D99B365.3060101@wzw.tum.de>
References: <4D99B365.3060101@wzw.tum.de>
Message-ID: <4D9C1B37.5090700@knmi.nl>

Hi,

You could try to (ab)use the standard lm function. First calculate the
regression using lm, than replace the fitted coefficients by the values
you want them to have and than use predict to estimate the values at
your measurement locations. Subtracting that value from the measurements
gives you the residuals. Than you can use variogram to create a
variogram. In code:

library(gstat)
loadMeuse()

meuse$logzinc = log(meuse$zinc)
lmfit = lm(log(zinc)~dist, meuse)
lmfit
lmfit$coefficients = c(10,-5)
lmfit
meuse$logzinctrend2 = predict(lmfit, meuse['dist'])
head(meuse at data[c('logzinc','logzinctrend')])
meuse$residuals = meuse$logzinc - meuse$logzinctrend
resvariogram = variogram(residuals~1, meuse)
plot(resvariogram)

cheers,
Paul

On 04/04/2011 02:02 PM, Tom Gottfried wrote:
> Dear list,
>
> is there a way in gstat to calculate a variogram from residuals of a
> known trend (that can not be estimated from the data) without passing
> the residuals to variogram() or gstat()? I tried the arguments
> trend.beta respectively beta. The first one gives an error, the second
> gives the same result as when I do not give it:
>
> library(gstat)
> data(meuse)
> # trying with variogram(..., trend.beta=...)
> var1 <- variogram(zinc~sqrt(dist), ~x+y, meuse)
> var2 <- variogram(zinc~sqrt(dist), ~x+y, meuse, trend.beta=c(1000,
> -1000))
>
> # trying with gstat(..., beta=...)
> coordinates(meuse) <- ~x+y
> meuse1 <- gstat(NULL, "zinc", zinc~sqrt(dist), meuse)
> var1 <- variogram(meuse1)
> meuse2 <- gstat(NULL, "zinc", zinc~sqrt(dist), meuse, beta=c(1000,
> -1000))
> var2 <- variogram(meuse2)
> identical(var1, var2) # both variograms are identical
>
> I see it's possible to calculate the residuals with
>
> residuals(lm(zinc~sqrt(dist), meuse))
>
> and pass them directly. But I wonder whether a more convenient direct
> passing of the trend coefficients is possible.
>
> Thanks!
> Tom
>


-- 
Paul Hiemstra, MSc
Global Climate Division
Royal Netherlands Meteorological Institute (KNMI)
Wilhelminalaan 10 | 3732 GK | De Bilt | Kamer B 3.39
P.O. Box 201 | 3730 AE | De Bilt
tel: +31 30 2206 494

http://intamap.geo.uu.nl/~paul
http://nl.linkedin.com/pub/paul-hiemstra/20/30b/770


From b.rowlingson at lancaster.ac.uk  Wed Apr  6 10:14:26 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 6 Apr 2011 09:14:26 +0100
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
In-Reply-To: <alpine.LRH.2.00.1104060928510.11822@reclus.nhh.no>
References: <inbvj0$gvv$1@dough.gmane.org>
	<BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
	<BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>
	<inh4a5$vql$1@dough.gmane.org>
	<alpine.LRH.2.00.1104060928510.11822@reclus.nhh.no>
Message-ID: <BANLkTinAg2R+vPXN5JWaED1hYESU6iOY1Q@mail.gmail.com>

On Wed, Apr 6, 2011 at 8:38 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> I've asked Barry off-list about this, there was a case a little while ago
> with unexpected results because of inserted sliver polygons causing
> apparently separate observations to be neighbours (the slivers were only
> visible when zooming right in). There are also differences in the way
> software may handle precision (snapping). More will follow ...

 Yes, I suspect its precision. poly2nb was showing two km-sized
regions as disconnected which were 0.01mm (yes, milimetres, about a
hair's breadth) apart. The rgeos functions had them as connected.

 If I get a chance today I'll investigate more.

Barry


From karl at huftis.org  Wed Apr  6 10:54:12 2011
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Wed, 6 Apr 2011 10:54:12 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
References: <inbvj0$gvv$1@dough.gmane.org>
	<BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
	<BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>
	<inh4a5$vql$1@dough.gmane.org>
	<alpine.LRH.2.00.1104060928510.11822@reclus.nhh.no>
	<BANLkTinAg2R+vPXN5JWaED1hYESU6iOY1Q@mail.gmail.com>
Message-ID: <inh9mf$t2r$1@dough.gmane.org>

Barry Rowlingson wrote:

>> I've asked Barry off-list about this, there was a case a little while ago
>> with unexpected results because of inserted sliver polygons causing
>> apparently separate observations to be neighbours (the slivers were only
>> visible when zooming right in). There are also differences in the way
>> software may handle precision (snapping). More will follow ...
> 
> Yes, I suspect its precision. poly2nb was showing two km-sized
> regions as disconnected which were 0.01mm (yes, milimetres, about a
> hair's breadth) apart. The rgeos functions had them as connected.

The ?poly2nb? function have a ?snap? parameter, set to 
?sqrt(.Machine$double.eps)? by default. Increasing this
should make the two regions treated as connected.

I guess it?s more surprising that ?rgeos? was treating
the two regions as connected by default.

BTW, when talking about the ?rgeos functions?, do you mean
applying for example ?gTouches? to the two regions, or are there
actually a ?poly2nb?-like function available?

-- 
Karl Ove Hufthammer


From karl at huftis.org  Wed Apr  6 11:11:57 2011
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Wed, 6 Apr 2011 11:11:57 +0200
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
References: <inbvj0$gvv$1@dough.gmane.org> <4D9B2542.1020400@gmail.com>
Message-ID: <inhann$37n$1@dough.gmane.org>

Rainer M Krug wrote:

> I think the idea is really great. I couldn't try it out yet, but if it
> is working, I think it should be added to one of the spatial packages -
> maybe sp?

If anyone is interested, feel free to add the function to any of the spatial 
packages, under a suitable free license (at least GPL2+ is OK).

I hope to some time in the future also implement one of the linear 5-
colouring algorithms. It annoys me somewhat that the greedy algorithm only 
manages a 6-colouring on one of my map data sets ? :-)

(The ?gcolor? algorithm gives nice results, but is *extremely* slow for 
large maps. It has been trying to colour the ?world? map for over two hours 
now, and is still not finished ?)

One final thought: For creating beautiful maps other criteria than 
minimising the number of colours may be useful. One example is balancing the 
number of polygons using each of the colours. The greedy algorithm naturally 
tends to use most of the low colours. For instance, on a map of 500 
polygons, 470 may be coloured with colours 1?3, 29 by colour 4 and only 1 by 
colour 5. Colours 4 and (especially) 5 may therefor look ?out of place?. If 
the algorithms only manges a 5-colouring, it could at least use equal 
amounts of each colour (here one might also want to take into account the 
area of the various polygons). On the other hand, a map using all colours 
equally often will tend to look more ?busy?, so perhaps a greedy colouring 
gives the nicest maps after all ? ?

-- 
Karl Ove Hufthammer


From piero.campa at gmail.com  Wed Apr  6 11:43:21 2011
From: piero.campa at gmail.com (piero campa)
Date: Wed, 6 Apr 2011 02:43:21 -0700 (PDT)
Subject: [R-sig-Geo] Cokriging unbiasedness condition
Message-ID: <1302083001863-6245350.post@n2.nabble.com>

Dear list and dear Edzer,
I was wondering which unbiasedness condition(s) is/are used in the
predict.gstat function when cokriging methods are called.

Thank you in advance.
/Piero

--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Cokriging-unbiasedness-condition-tp6245350p6245350.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From david.march at uib.es  Wed Apr  6 12:11:16 2011
From: david.march at uib.es (david.march at uib.es)
Date: Wed,  6 Apr 2011 12:11:16 +0200
Subject: [R-sig-Geo] error when converting shapefile to psp object in
	spatstat
Message-ID: <20110406121116.raugm36nqcs4ksco@wm.uib.es>

Dear all,

I'm trying to convert a shapefile into psp object of spatstat, but I  
found an error message. I'm following the guidelness provided here:  
http://cran.r-project.org/web/packages/spatstat/vignettes/shapefiles.pdf

#script following the instructions with a sample shapefile from maptools:
library(sp)
library(spatstat)
library(gpclib)
library(maptools)
spatstat.options(gpclib=TRUE) #here I suspect is only relevant for  
importing polygon data

setwd(system.file("shapes", package = "maptools"))
fylk <- readShapeSpatial("fylk-val.shp")
fdata <- slot(fylk, "data")
fl <- as(fylk, "SpatialLines")
fcurves <- slot(fl, "lines")
fcurves <- lapply(fcurves, function(x) {SpatialLines(list(x))})
fpatterns <- lapply(fcurves, as.psp)

Here I get the following message error:
Error en as.data.frame.vector(x, ..., nm = nm) :   el argumento formal  
"nm" concuerda con m?ltiples argumentos especificados

# also the same problem when trying:
x <- as.psp(fylk)

# and also here:
x<-as.psp.SpatialLinesDataFrame(fylk)

I'm running R version 2.12.2  for windows with the following version packages:
spatstat 1.21-6
General Polygon Clipper Library for R (version 1.5-1)
maptools 0.8-6


Any about what is this error regarding some "nm" formal argument?

Thanks in advance,
David

David March Morla
Doctorando / PhD student
Instituto Mediterraneo de Estudios Avanzados (UIB-CSIC)
C/Miquel Marques,21
07190 Esporles
Islas Baleares - Spain
Tel: +34 971 611 722
Fax: +34 971 611 761
Email: david.march at uib.es


From b.rowlingson at lancaster.ac.uk  Wed Apr  6 12:16:00 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 6 Apr 2011 11:16:00 +0100
Subject: [R-sig-Geo] Colouring maps so that adjacent polygons differ in
	colour
In-Reply-To: <inh9mf$t2r$1@dough.gmane.org>
References: <inbvj0$gvv$1@dough.gmane.org>
	<BANLkTi=EJY34T0hirxVNoRv=qsQdgtqueA@mail.gmail.com>
	<BANLkTim+CdF+bEv9Lswk6dSaSh7zZTMDAg@mail.gmail.com>
	<inh4a5$vql$1@dough.gmane.org>
	<alpine.LRH.2.00.1104060928510.11822@reclus.nhh.no>
	<BANLkTinAg2R+vPXN5JWaED1hYESU6iOY1Q@mail.gmail.com>
	<inh9mf$t2r$1@dough.gmane.org>
Message-ID: <BANLkTinc-xoSFKxYLki0=A1H8d00dfe7Cw@mail.gmail.com>

On Wed, Apr 6, 2011 at 9:54 AM, Karl Ove Hufthammer <karl at huftis.org> wrote:

> The ?poly2nb? function have a ?snap? parameter, set to
> ?sqrt(.Machine$double.eps)? by default. Increasing this
> should make the two regions treated as connected.
>
> I guess it?s more surprising that ?rgeos? was treating
> the two regions as connected by default.
>
> BTW, when talking about the ?rgeos functions?, do you mean
> applying for example ?gTouches? to the two regions, or are there
> actually a ?poly2nb?-like function available?

 Here's what happens with 3 polygons extracted from the wider region:

 > poly2nb(gT)
Neighbour list object:
Number of regions: 3
Number of nonzero links: 2
Percentage nonzero weights: 22.22222
Average number of links: 0.6666667
1 region with no links:
1360

 - so that shows one region not connected to the others. Up the snap
(which I suppose I should have noticed) to 1 metre:

> poly2nb(gT,snap=1)
Neighbour list object:
Number of regions: 3
Number of nonzero links: 6
Percentage nonzero weights: 66.66667
Average number of links: 2

 yay, everything is connected.

With rgeos, there's gTouches:

 > gTouches(gT,byid=TRUE)
      1360  2474  2475
1360 FALSE FALSE FALSE
2474 FALSE FALSE  TRUE
2475 FALSE  TRUE FALSE

 - which I think is in agreement with the poly2nb with default
tolerance. If I use gIntersects:

 > gIntersects(gT,byid=TRUE)
     1360 2474 2475
1360 TRUE TRUE TRUE
2474 TRUE TRUE TRUE
2475 TRUE TRUE TRUE

 everything is connected. Now that is because "intersects" is a
different spatial operation to "touches", since it is true for
overlapping polygons that may have no common boundary, and 'touches'
is only true if they share common points. I've not had a chance to
examine the intersections of the polygons yet.

 I guess the best thing to use is poly2nb with a good sized snap.

Barry


From edzer.pebesma at uni-muenster.de  Wed Apr  6 13:02:13 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 06 Apr 2011 13:02:13 +0200
Subject: [R-sig-Geo] Cokriging unbiasedness condition
In-Reply-To: <1302083001863-6245350.post@n2.nabble.com>
References: <1302083001863-6245350.post@n2.nabble.com>
Message-ID: <4D9C4835.8010905@uni-muenster.de>



On 04/06/2011 11:43 AM, piero campa wrote:
> Dear list and dear Edzer,
> I was wondering which unbiasedness condition(s) is/are used in the
> predict.gstat function when cokriging methods are called.

The usual ones; if no trend function is specified (...~1), then an
unbiasedness condition for the mean value is used; if some trend
function is specified the conditions extend to the mean parameters.

Function gstat() has a merge parameter that allows one to join the
unbiasedness condition for any two parameters to one.
> 
> Thank you in advance.
> /Piero
> 
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Cokriging-unbiasedness-condition-tp6245350p6245350.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From v8extra at gmail.com  Wed Apr  6 13:29:05 2011
From: v8extra at gmail.com (=?iso-8859-1?Q?S=E9bastien_Durand?=)
Date: Wed, 6 Apr 2011 07:29:05 -0400
Subject: [R-sig-Geo]  Variance estimation of an krigged area
Message-ID: <C7910E9F-CC0D-4E4A-994B-B365A168E4E6@gmail.com>

Hello to all,

Since I did not get any answers I will reformulate my question may be I was not clear enough.

I am try to obtain one estimation variance of a whole area. That area is found within a SpatialPolygons, and it was also krigged using a regular grid.

I am looking for a similar "var" value as the one obtained for each point of the grid, but a value that would apply to the whole surface surface of interest.

That question would also apply to block krigging.  

I guess my question would be when we do block krigging using krige, is the variance error (var) given by the krige function is computed for the whole block krigged or only for the central point of the block (a punctual estimation)....

 Thanks to all!

S?bastien

From edzer.pebesma at uni-muenster.de  Wed Apr  6 13:36:40 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 06 Apr 2011 13:36:40 +0200
Subject: [R-sig-Geo] Variance estimation of an krigged area
In-Reply-To: <C7910E9F-CC0D-4E4A-994B-B365A168E4E6@gmail.com>
References: <C7910E9F-CC0D-4E4A-994B-B365A168E4E6@gmail.com>
Message-ID: <4D9C5048.6080407@uni-muenster.de>



On 04/06/2011 01:29 PM, S?bastien Durand wrote:
> Hello to all,
> 
> Since I did not get any answers I will reformulate my question may be I was not clear enough.
> 
> I am try to obtain one estimation variance of a whole area. That area is found within a SpatialPolygons, and it was also krigged using a regular grid.
> 
> I am looking for a similar "var" value as the one obtained for each point of the grid, but a value that would apply to the whole surface surface of interest.
> 
> That question would also apply to block krigging.  
> 
> I guess my question would be when we do block krigging using krige, is the variance error (var) given by the krige function is computed for the whole block krigged or only for the central point of the block (a punctual estimation)....

the block kriging variance is the estimation (prediction) variance for
the average value over the whole block.

> 
>  Thanks to all!
> 
> S?bastien
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From piero.campa at gmail.com  Wed Apr  6 13:59:28 2011
From: piero.campa at gmail.com (Piero Campalani)
Date: Wed, 6 Apr 2011 13:59:28 +0200
Subject: [R-sig-Geo] Cokriging unbiasedness condition
In-Reply-To: <4D9C4835.8010905@uni-muenster.de>
References: <1302083001863-6245350.post@n2.nabble.com>
	<4D9C4835.8010905@uni-muenster.de>
Message-ID: <BANLkTin3XyaGYag-0n0WCb=WgvbQKHGmRQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110406/5e98c917/attachment.pl>

From Roger.Bivand at nhh.no  Wed Apr  6 14:32:29 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 6 Apr 2011 14:32:29 +0200 (CEST)
Subject: [R-sig-Geo] error when converting shapefile to psp object in
 spatstat
In-Reply-To: <20110406121116.raugm36nqcs4ksco@wm.uib.es>
References: <20110406121116.raugm36nqcs4ksco@wm.uib.es>
Message-ID: <alpine.LRH.2.00.1104061431150.12734@reclus.nhh.no>

On Wed, 6 Apr 2011, david.march at uib.es wrote:

> Dear all,
>
> I'm trying to convert a shapefile into psp object of spatstat, but I found an 
> error message. I'm following the guidelness provided here: 
> http://cran.r-project.org/web/packages/spatstat/vignettes/shapefiles.pdf
>
> #script following the instructions with a sample shapefile from maptools:
> library(sp)
> library(spatstat)
> library(gpclib)
> library(maptools)
> spatstat.options(gpclib=TRUE) #here I suspect is only relevant for importing 
> polygon data
>
> setwd(system.file("shapes", package = "maptools"))
> fylk <- readShapeSpatial("fylk-val.shp")
> fdata <- slot(fylk, "data")
> fl <- as(fylk, "SpatialLines")
> fcurves <- slot(fl, "lines")
> fcurves <- lapply(fcurves, function(x) {SpatialLines(list(x))})
> fpatterns <- lapply(fcurves, as.psp)
>
> Here I get the following message error:
> Error en as.data.frame.vector(x, ..., nm = nm) :   el argumento formal "nm" 
> concuerda con m?ltiples argumentos especificados
>
> # also the same problem when trying:
> x <- as.psp(fylk)
>
> # and also here:
> x<-as.psp.SpatialLinesDataFrame(fylk)
>
> I'm running R version 2.12.2  for windows with the following version 
> packages:
> spatstat 1.21-6
> General Polygon Clipper Library for R (version 1.5-1)
> maptools 0.8-6
>
>
> Any about what is this error regarding some "nm" formal argument?

This is fixed thanks to a contribution by Rolf Turner to maptools on 
R-Forge. Please try:

install.packages("maptools", repos="http://R-Forge.R-project.org")

and see if it helps. Thanks for a very clear use case.

Roger

>
> Thanks in advance,
> David
>
> David March Morla
> Doctorando / PhD student
> Instituto Mediterraneo de Estudios Avanzados (UIB-CSIC)
> C/Miquel Marques,21
> 07190 Esporles
> Islas Baleares - Spain
> Tel: +34 971 611 722
> Fax: +34 971 611 761
> Email: david.march at uib.es
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From fisholo at gmail.com  Wed Apr  6 15:49:03 2011
From: fisholo at gmail.com (Fabrizio Diotri)
Date: Wed, 6 Apr 2011 15:49:03 +0200
Subject: [R-sig-Geo] plot raster map
Message-ID: <BANLkTik5pd2cPXe4SCA4rNh1f9Fgb_pEQg@mail.gmail.com>

Hi to everybody,
I'm new to this list.
I load a raster map in R thanks to spgrass6 package: no problem.
I would like to plot it: with image command no problem.

The problem is that I would like to mantain original map colours.
My map is catgorized and in particular:
category     red  green  blue
           1    181     237   239
           2    115      181  231
           3     29      133  223
           4     28        69  183
           5       7         7   144
           6    229     229       0
           7    255     255    255
Is there a way to plot the map with this colours?

Thanks in advance for all reply

fabrizio


From edzer.pebesma at uni-muenster.de  Wed Apr  6 17:00:11 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 06 Apr 2011 17:00:11 +0200
Subject: [R-sig-Geo] Cokriging unbiasedness condition
In-Reply-To: <BANLkTin3XyaGYag-0n0WCb=WgvbQKHGmRQ@mail.gmail.com>
References: <1302083001863-6245350.post@n2.nabble.com>	<4D9C4835.8010905@uni-muenster.de>
	<BANLkTin3XyaGYag-0n0WCb=WgvbQKHGmRQ@mail.gmail.com>
Message-ID: <4D9C7FFB.5050208@uni-muenster.de>

With the usual ones I refered to (what I believe is) ordinary cokriging:
each variable has sum of weights for the variable itself is 1, sum of
the weights for all other variables is 0.

On 04/06/2011 01:59 PM, Piero Campalani wrote:
> Thank you.
> So that means that e.g. with ordinary cokriging, the one condition "sum of
> all coefficients equal 1" is used, and not e.g. the (n+1) nonbias conditions
> by which the coefficients of the target variable sum to 1, whereas the
> coefficients of the n secondary variables sum to 0 ?
> I'm sorry, I'm not aware of what the "usual ones" are unluckily. :)
> 
> Regards.
> 
> On 6 April 2011 13:02, Edzer Pebesma <edzer.pebesma at uni-muenster.de> wrote:
> 
>>
>>
>> On 04/06/2011 11:43 AM, piero campa wrote:
>>> Dear list and dear Edzer,
>>> I was wondering which unbiasedness condition(s) is/are used in the
>>> predict.gstat function when cokriging methods are called.
>>
>> The usual ones; if no trend function is specified (...~1), then an
>> unbiasedness condition for the mean value is used; if some trend
>> function is specified the conditions extend to the mean parameters.
>>
>> Function gstat() has a merge parameter that allows one to join the
>> unbiasedness condition for any two parameters to one.
>>>
>>> Thank you in advance.
>>> /Piero
>>>
>>> --
>>> View this message in context:
>> http://r-sig-geo.2731867.n2.nabble.com/Cokriging-unbiasedness-condition-tp6245350p6245350.html
>>> Sent from the R-sig-geo mailing list archive at Nabble.com.
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> --
>> Edzer Pebesma
>> Institute for Geoinformatics (ifgi), University of M?nster
>> Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
>> 8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
>> http://www.52north.org/geostatistics      e.pebesma at wwu.de
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> 

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From piero.campa at gmail.com  Wed Apr  6 17:09:35 2011
From: piero.campa at gmail.com (Piero Campalani)
Date: Wed, 6 Apr 2011 17:09:35 +0200
Subject: [R-sig-Geo] Cokriging unbiasedness condition
In-Reply-To: <4D9C7FFB.5050208@uni-muenster.de>
References: <1302083001863-6245350.post@n2.nabble.com>
	<4D9C4835.8010905@uni-muenster.de>
	<BANLkTin3XyaGYag-0n0WCb=WgvbQKHGmRQ@mail.gmail.com>
	<4D9C7FFB.5050208@uni-muenster.de>
Message-ID: <BANLkTimgC1t7Of6cvLzB0i23K+DufjnZ2Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110406/c4ddd682/attachment.pl>

From landis at isciences.com  Wed Apr  6 17:45:44 2011
From: landis at isciences.com (Matthew Landis)
Date: Wed, 6 Apr 2011 11:45:44 -0400
Subject: [R-sig-Geo] varying polygon layer with panel in spplot
Message-ID: <4D9C8AA8.2080509@isciences.com>

Dear R-sig-geo followers:

I wonder if anyone can help me with the following problem.  I'd like to 
plot a series of rasters overlaid with polygons (or points), where the 
polygon layer is specific to each panel.  here's an example of my 
nearest attempt using spplot on a Meuse-based dataset (details follow at 
the end). (If someone can do this with ggplot, that's fine with me 
too).  'sgdf' is a SpatialGridDataFrame and pts.list is a list where 
each element is a SpatialPointsDataFrame.

spplot(sgdf, c('cd','cu', 'pb', 'zn'),
        names.attr = c('Cadmium', 'Copper', 'Lead', 'Zinc'),
        as.table = TRUE, main = "Testing",
        panel = function(x, y, z, subscripts, ...){
            panel.gridplot(x, y, z, subscripts, ...)
            sp.points(pts.list[[subscripts]])
        }
)

When I send this, I get SpatialGridDataFrame plotted properly, but the 
points are not.  Instead I get the messages (by panel): Error using 
packet 1\nrecursive indexing failed at level 2; Error using packet 2\nno 
such index at level 1; Error using packet 3\nno such index at level 1; 
Error using packet 4\nno such index at level 1.

If I specify the subscript manually, e.g. "sp.points(pts.list[[1]])", it 
works fine (but of course the points are the same in all panels).

------------------
Here's the code I used to produce the data:

library(sp)
library(lattice) # required for trellis.par.set():
trellis.par.set(sp.theme()) # sets color ramp to bpy.colors()

data(meuse)
coordinates(meuse)=~x+y

library(gstat, pos = match(paste("package", "sp", sep=":"), search()) + 1)
data(meuse.grid)
coordinates(meuse.grid) = ~x+y
gridded(meuse.grid) = TRUE

v.cd = variogram(log(cadmium) ~ 1, meuse)
ok.model.cd = fit.variogram(v.cd, vgm(1, 'Exp', 500, 1))
cd.ok <- krige(log(cadmium) ~ 1, meuse, meuse.grid, model = ok.model.cd)

v.cu <- variogram(log(copper) ~ 1, meuse)
ok.model.cu <- fit.variogram(v.cu, vgm(1, 'Exp', 500, 1))
cu.ok <- krige(log(copper) ~ 1, meuse, meuse.grid, model = ok.model.cu)

v.pb <- variogram(log(lead) ~ 1, meuse)
ok.model.pb <- fit.variogram(v.pb, vgm(1, 'Exp', 500, 1))
pb.ok <- krige(log(lead) ~ 1, meuse, meuse.grid, model = ok.model.pb)

v.zn = variogram(log(zinc)~1, meuse)
ok.model.zn = fit.variogram(v.zn, vgm(1, "Exp", 500, 1))
zn.ok = krige(log(zinc)~1, meuse, meuse.grid, model = ok.model.zn)

sgdf = cd.ok

sgdf[['cd']] <- cd.ok[['var1.pred']]
sgdf[['cu']] <- cu.ok[['var1.pred']]
sgdf[['pb']] <- pb.ok[['var1.pred']]
sgdf[['zn']] <- zn.ok[['var1.pred']]

pts.list <- list(meuse[, 'cadmium'], meuse[, 'copper'], meuse[, 'lead'], 
meuse[, 'zinc'])

spplot(sgdf, c('cd','cu', 'pb', 'zn'),
        names.attr = c('Cadmium', 'Copper', 'Lead', 'Zinc'),
        as.table = TRUE, main = "Testing",
        panel = function(x, y, z, subscripts, ...){
            panel.gridplot(x, y, z, subscripts, ...)
            sp.points(pts.list[[subscripts]])
        }
)

-----------------------
And, FWIW, the session info:

 > sessionInfo()
R version 2.12.2 (2011-02-25)
Platform: x86_64-pc-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United 
States.1252
[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] maptools_0.7-38 foreign_0.8-42  maps_2.1-5      lattice_0.19-17
[5] sp_0.9-78       gstat_0.9-77

loaded via a namespace (and not attached):
[1] grid_2.12.2  tools_2.12.2

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~
Matthew Landis, Ph.D.
Research Scientist
ISciences, LLC
61 Main St. Suite 200
Burlington VT 05401
802.864.2999
~~~~~~~~~~~~~~~~~~~~~~~~~~


From nick at hamm.org  Wed Apr  6 19:52:06 2011
From: nick at hamm.org (Nick Hamm)
Date: Wed, 6 Apr 2011 19:52:06 +0200
Subject: [R-sig-Geo] focalFilter problem raster package
In-Reply-To: <1301980998310-6241121.post@n2.nabble.com>
References: <BANLkTi=TZZg1CnCqw4ZDb3u7NUUPV_TtkQ@mail.gmail.com>
	<1301980998310-6241121.post@n2.nabble.com>
Message-ID: <BANLkTim=Zwo8czGbyNrWXxLd5_qAt7Mrfg@mail.gmail.com>

Dear Robert

I hope the following example is clear.  you should be able to cut and paste...

Nick

library(raster)

tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73,
63, 62, 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79,
55, 55, 56, 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68,
55, 54, 53, 61, 82, 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68,
59, 58, 60, 64, 88, 99, 82, 81, 71, 80, 89, 89, 89, 102, 104, 75, 63,
57, 58, 58, 77, 92, 82, 71, 59, 90, 105, 92, 79, 98, 110, 83, 62, 55,
56, 56, 80, 90, 99, 88, 64, 91, 112, 94, 76, 91, 100, 85, 62, 59, 55,
61, 99, 97, 93, 80, 65, 87, 107, 80, 59, 60, 67, 66, 65, 62, 68, 72,
102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54, 62, 62, 86, 85, 55,
59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61, 56, 41, 40,
43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41, 42,
44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42,
42, 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42,
43, 42, 53, 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43,
43, 49, 59, 62, 53, 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43,
46, 52, 56, 56, 61)

tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
p267 <- tmp
rm(tmp)
p267.r <- raster(p267)

w.ij.2 <- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)

w.ij.2 <- matrix(w.ij.2, 15, 15)

sum(w.ij.2 * p267[-16,-16])
sum(w.ij.2 * p267[-16,-1])
sum(w.ij.2 * p267[-1,-16]) #Cell 136 in the raster
sum(w.ij.2 * p267[-1,-1]) #Cell 137 in the raster

tmp <- focalFilter(p267.r, filter=w.ij.2, fun=sum)

tmp[136]
tmp[137]




On 5 April 2011 07:23, Robert Hijmans <r.hijmans at gmail.com> wrote:
>
>> I am experiencing a strange problem when using the focalFilter
>> function in the raster package.
>
> Nick,
> Could you provide a self-contained example that does not require downloading
> files? Given your description that should not be very hard.
> Robert
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/focalFilter-problem-raster-package-tp6235925p6241121.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From edzer.pebesma at uni-muenster.de  Wed Apr  6 20:47:27 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 06 Apr 2011 20:47:27 +0200
Subject: [R-sig-Geo] varying polygon layer with panel in spplot
In-Reply-To: <4D9C8AA8.2080509@isciences.com>
References: <4D9C8AA8.2080509@isciences.com>
Message-ID: <4D9CB53F.9050705@uni-muenster.de>

Matthew, please try this:

spplot(sgdf, c('cd','cu', 'pb', 'zn'),
       names.attr = c('Cadmium', 'Copper', 'Lead', 'Zinc'),
       as.table = TRUE, main = "Testing",
       panel = function(x, y, z, subscripts, ...){
           panel.gridplot(x, y, z, subscripts, ...)
           sp.points(pts.list[[panel.number()]])
       }
)

subscripts refers to the grid cell numbers, after they're all stacked to
a long table; they do not match your pts.list entries.

On 04/06/2011 05:45 PM, Matthew Landis wrote:
> 
> library(sp)
> library(lattice) # required for trellis.par.set():
> trellis.par.set(sp.theme()) # sets color ramp to bpy.colors()
> 
> data(meuse)
> coordinates(meuse)=~x+y
> 
> library(gstat, pos = match(paste("package", "sp", sep=":"), search()) + 1)
> data(meuse.grid)
> coordinates(meuse.grid) = ~x+y
> gridded(meuse.grid) = TRUE
> 
> v.cd = variogram(log(cadmium) ~ 1, meuse)
> ok.model.cd = fit.variogram(v.cd, vgm(1, 'Exp', 500, 1))
> cd.ok <- krige(log(cadmium) ~ 1, meuse, meuse.grid, model = ok.model.cd)
> 
> v.cu <- variogram(log(copper) ~ 1, meuse)
> ok.model.cu <- fit.variogram(v.cu, vgm(1, 'Exp', 500, 1))
> cu.ok <- krige(log(copper) ~ 1, meuse, meuse.grid, model = ok.model.cu)
> 
> v.pb <- variogram(log(lead) ~ 1, meuse)
> ok.model.pb <- fit.variogram(v.pb, vgm(1, 'Exp', 500, 1))
> pb.ok <- krige(log(lead) ~ 1, meuse, meuse.grid, model = ok.model.pb)
> 
> v.zn = variogram(log(zinc)~1, meuse)
> ok.model.zn = fit.variogram(v.zn, vgm(1, "Exp", 500, 1))
> zn.ok = krige(log(zinc)~1, meuse, meuse.grid, model = ok.model.zn)
> 
> sgdf = cd.ok
> 
> sgdf[['cd']] <- cd.ok[['var1.pred']]
> sgdf[['cu']] <- cu.ok[['var1.pred']]
> sgdf[['pb']] <- pb.ok[['var1.pred']]
> sgdf[['zn']] <- zn.ok[['var1.pred']]
> 
> pts.list <- list(meuse[, 'cadmium'], meuse[, 'copper'], meuse[, 'lead'],
> meuse[, 'zinc'])
> 
> spplot(sgdf, c('cd','cu', 'pb', 'zn'),
>        names.attr = c('Cadmium', 'Copper', 'Lead', 'Zinc'),
>        as.table = TRUE, main = "Testing",
>        panel = function(x, y, z, subscripts, ...){
>            panel.gridplot(x, y, z, subscripts, ...)
>            sp.points(pts.list[[subscripts]])
>        }
> )

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From edzer.pebesma at uni-muenster.de  Wed Apr  6 21:04:57 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 06 Apr 2011 21:04:57 +0200
Subject: [R-sig-Geo] Cokriging unbiasedness condition
In-Reply-To: <BANLkTimgC1t7Of6cvLzB0i23K+DufjnZ2Q@mail.gmail.com>
References: <1302083001863-6245350.post@n2.nabble.com>	<4D9C4835.8010905@uni-muenster.de>	<BANLkTin3XyaGYag-0n0WCb=WgvbQKHGmRQ@mail.gmail.com>	<4D9C7FFB.5050208@uni-muenster.de>
	<BANLkTimgC1t7Of6cvLzB0i23K+DufjnZ2Q@mail.gmail.com>
Message-ID: <4D9CB959.1070006@uni-muenster.de>

That book is very good, but contains some first signs of black magic.

I would recommend Don Myers' "Matrix formulation of cokriging", Jay Ver
Hoef and Noel Cressie's "Multivariable spatial prediction", but in
particular Hans Wackernagel's book on multivariate geostatistics to
learn more.

On 04/06/2011 05:09 PM, Piero Campalani wrote:
> Well, I am asking this because in "An Introduction to Applied
> Geostatistics", ?17, they point out how these that are meant as "usual
> nonbias conditions" are not believed to be the best ones, though they are
> the most commonly used..
> Using just one nonbias condition involving all the cokriging coefficients
> instead, would bring improvements in the estimation process, at the cost of
> a little change in the estimator (that should include estimated means of the
> involved variables to effectively ensure unbiasedness).
> 
> I may have misunderstood however, or these may not be general statements but
> rather case-specific conclusions.. !
> 
> Regards,
> Piero
> 
> On 6 April 2011 17:00, Edzer Pebesma <edzer.pebesma at uni-muenster.de> wrote:
> 
>> With the usual ones I refered to (what I believe is) ordinary cokriging:
>> each variable has sum of weights for the variable itself is 1, sum of
>> the weights for all other variables is 0.
>>
>> On 04/06/2011 01:59 PM, Piero Campalani wrote:
>>> Thank you.
>>> So that means that e.g. with ordinary cokriging, the one condition "sum
>> of
>>> all coefficients equal 1" is used, and not e.g. the (n+1) nonbias
>> conditions
>>> by which the coefficients of the target variable sum to 1, whereas the
>>> coefficients of the n secondary variables sum to 0 ?
>>> I'm sorry, I'm not aware of what the "usual ones" are unluckily. :)
>>>
>>> Regards.
>>>
>>> On 6 April 2011 13:02, Edzer Pebesma <edzer.pebesma at uni-muenster.de>
>> wrote:
>>>
>>>>
>>>>
>>>> On 04/06/2011 11:43 AM, piero campa wrote:
>>>>> Dear list and dear Edzer,
>>>>> I was wondering which unbiasedness condition(s) is/are used in the
>>>>> predict.gstat function when cokriging methods are called.
>>>>
>>>> The usual ones; if no trend function is specified (...~1), then an
>>>> unbiasedness condition for the mean value is used; if some trend
>>>> function is specified the conditions extend to the mean parameters.
>>>>
>>>> Function gstat() has a merge parameter that allows one to join the
>>>> unbiasedness condition for any two parameters to one.
>>>>>
>>>>> Thank you in advance.
>>>>> /Piero
>>>>>
>>>>> --
>>>>> View this message in context:
>>>>
>> http://r-sig-geo.2731867.n2.nabble.com/Cokriging-unbiasedness-condition-tp6245350p6245350.html
>>>>> Sent from the R-sig-geo mailing list archive at Nabble.com.
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>> --
>>>> Edzer Pebesma
>>>> Institute for Geoinformatics (ifgi), University of M?nster
>>>> Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
>>>> 8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
>>>> http://www.52north.org/geostatistics      e.pebesma at wwu.de
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>
>>
>> --
>> Edzer Pebesma
>> Institute for Geoinformatics (ifgi), University of M?nster
>> Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
>> 8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
>> http://www.52north.org/geostatistics      e.pebesma at wwu.de
>>
> 

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From greenberg at ucdavis.edu  Thu Apr  7 00:07:04 2011
From: greenberg at ucdavis.edu (Jonathan Greenberg)
Date: Wed, 6 Apr 2011 15:07:04 -0700
Subject: [R-sig-Geo] Calculating/applying transition matrices from
	classified imagery?
Message-ID: <BANLkTin_9Y9LUEEP+oKbm-O=E7fz2e2V_w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110406/37fbe9ab/attachment.pl>

From v8extra at gmail.com  Thu Apr  7 00:58:06 2011
From: v8extra at gmail.com (=?iso-8859-1?Q?S=E9bastien_Durand?=)
Date: Wed, 6 Apr 2011 18:58:06 -0400
Subject: [R-sig-Geo] Variance estimation of an krigged area
Message-ID: <0E8DD9F3-0220-4461-87CD-DAA25E4BE394@gmail.com>

Gracias!


From roman.lustrik at gmail.com  Thu Apr  7 07:49:54 2011
From: roman.lustrik at gmail.com (=?UTF-8?Q?Roman_Lu=C5=A1trik?=)
Date: Thu, 7 Apr 2011 07:49:54 +0200
Subject: [R-sig-Geo] Calculating/applying transition matrices from
 classified imagery?
In-Reply-To: <BANLkTin_9Y9LUEEP+oKbm-O=E7fz2e2V_w@mail.gmail.com>
References: <BANLkTin_9Y9LUEEP+oKbm-O=E7fz2e2V_w@mail.gmail.com>
Message-ID: <BANLkTi=wZ_5O_Um84Vq_opZfM3-oCRbOfA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110407/d77c12e0/attachment.pl>

From piero.campa at gmail.com  Thu Apr  7 10:13:42 2011
From: piero.campa at gmail.com (Piero Campalani)
Date: Thu, 7 Apr 2011 10:13:42 +0200
Subject: [R-sig-Geo] Cokriging unbiasedness condition
In-Reply-To: <4D9CB959.1070006@uni-muenster.de>
References: <1302083001863-6245350.post@n2.nabble.com>
	<4D9C4835.8010905@uni-muenster.de>
	<BANLkTin3XyaGYag-0n0WCb=WgvbQKHGmRQ@mail.gmail.com>
	<4D9C7FFB.5050208@uni-muenster.de>
	<BANLkTimgC1t7Of6cvLzB0i23K+DufjnZ2Q@mail.gmail.com>
	<4D9CB959.1070006@uni-muenster.de>
Message-ID: <BANLkTi=vkUhbAKXgOUu2EQhhZ3tpCJ78DA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110407/881518f3/attachment.pl>

From luca.candeloro at gmail.com  Thu Apr  7 12:35:54 2011
From: luca.candeloro at gmail.com (luca candeloro)
Date: Thu, 7 Apr 2011 12:35:54 +0200
Subject: [R-sig-Geo] Epidemiological spatio temporal model
Message-ID: <BANLkTinFzu7KmjnETijpkydkMTLKv0-OTA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110407/a45a7377/attachment.pl>

From francesco.pirotti at unipd.it  Thu Apr  7 13:27:05 2011
From: francesco.pirotti at unipd.it (francesco)
Date: Thu, 07 Apr 2011 13:27:05 +0200
Subject: [R-sig-Geo] ppp unique point coordinates?
In-Reply-To: <AANLkTikdjdm+CGv8fpTYVr_GaXB5s33-J6PVqdShQt21@mail.gmail.com>
References: <AANLkTikbF5h1aQjgA1aiNFc4ye2W+FN1OUZ+FHNYjsui@mail.gmail.com>
	<AANLkTikdjdm+CGv8fpTYVr_GaXB5s33-J6PVqdShQt21@mail.gmail.com>
Message-ID: <4D9D9F89.8060504@unipd.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110407/8d2a68b8/attachment.pl>

From b.rowlingson at lancaster.ac.uk  Thu Apr  7 13:30:14 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 7 Apr 2011 12:30:14 +0100
Subject: [R-sig-Geo] ppp unique point coordinates?
In-Reply-To: <4D9D9F89.8060504@unipd.it>
References: <AANLkTikbF5h1aQjgA1aiNFc4ye2W+FN1OUZ+FHNYjsui@mail.gmail.com>
	<AANLkTikdjdm+CGv8fpTYVr_GaXB5s33-J6PVqdShQt21@mail.gmail.com>
	<4D9D9F89.8060504@unipd.it>
Message-ID: <BANLkTimqKTFdz86bcTCPf1TYd-x5Oega1A@mail.gmail.com>

On Thu, Apr 7, 2011 at 12:27 PM, francesco <francesco.pirotti at unipd.it> wrote:
> Hi everyone, I am trying to process a lot of points as ppp points, put I
> see that the ppp object cannot be created if the coordinate matrix has
> duplicated values. I have duplicates because the points are acutally
> from a 3D ?laser scan survey so I will need duplicats. My code to figure
> my error was:
> pts is the x,y matrix and i is the slice that I used to check when my
> error occured.
>
> ppp(pts[1:i,1], pts[1:i,2], range(pts[1:i,1]), range(pts[1:i,2]))
>
> Question is: is there a way to overcome this? My final objective is
> "simply" to use pixellate to aggregate statistics locally (ie average
> point density in a square meter grid or mean reflectance value of each
> point)

 I get a warning, not an error, and the resultant object has all the points:

 > z = ppp(x=c(0,1,2,2)/4,y=c(1,2,3,3)/4)
Warning message:
In ppp(x = c(0, 1, 2, 2)/4, y = c(1, 2, 3, 3)/4) :
  data contain duplicated points
 > z
 planar point pattern: 4 points
window: rectangle = [0, 1] x [0, 1] units

So what's your problem?

Barry


From r.turner at auckland.ac.nz  Thu Apr  7 13:38:09 2011
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 07 Apr 2011 23:38:09 +1200
Subject: [R-sig-Geo] ppp unique point coordinates?
In-Reply-To: <4D9D9F89.8060504@unipd.it>
References: <AANLkTikbF5h1aQjgA1aiNFc4ye2W+FN1OUZ+FHNYjsui@mail.gmail.com>	<AANLkTikdjdm+CGv8fpTYVr_GaXB5s33-J6PVqdShQt21@mail.gmail.com>
	<4D9D9F89.8060504@unipd.it>
Message-ID: <4D9DA221.4020002@auckland.ac.nz>


The ppp() function simply gives you a ***warning*** if there are duplicated
points; duplications are permitted.

However you should be careful.

The theory on which most point process theory rests assumes that the
processes are ***simple***,  so there can be no duplicated points.  Hence
any methodology that invokes the usual theory will be invalid for your
patterns.

If you are not invoking any such methodology, then go for it.

If you don't like getting the warnings, then you could use

     suppressWarnings()

     cheers,

         Rolf Turner

On 07/04/11 23:27, francesco wrote:
> Hi everyone, I am trying to process a lot of points as ppp points, put I
> see that the ppp object cannot be created if the coordinate matrix has
> duplicated values. I have duplicates because the points are acutally
> from a 3D  laser scan survey so I will need duplicats. My code to figure
> my error was:
> pts is the x,y matrix and i is the slice that I used to check when my
> error occured.
>
> ppp(pts[1:i,1], pts[1:i,2], range(pts[1:i,1]), range(pts[1:i,2]))
>
> Question is: is there a way to overcome this? My final objective is
> "simply" to use pixellate to aggregate statistics locally (ie average
> point density in a square meter grid or mean reflectance value of each
> point)
>
> Cheers, and thanks for your time.
> Francesco


From francesco.pirotti at unipd.it  Thu Apr  7 14:17:55 2011
From: francesco.pirotti at unipd.it (francesco)
Date: Thu, 07 Apr 2011 14:17:55 +0200
Subject: [R-sig-Geo] ppp unique point coordinates?
In-Reply-To: <4D9DA221.4020002@auckland.ac.nz>
References: <AANLkTikbF5h1aQjgA1aiNFc4ye2W+FN1OUZ+FHNYjsui@mail.gmail.com>	<AANLkTikdjdm+CGv8fpTYVr_GaXB5s33-J6PVqdShQt21@mail.gmail.com>	<4D9D9F89.8060504@unipd.it>
	<4D9DA221.4020002@auckland.ac.nz>
Message-ID: <4D9DAB73.3020302@unipd.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110407/f44fdb53/attachment.pl>

From landis at isciences.com  Thu Apr  7 15:00:14 2011
From: landis at isciences.com (Matthew Landis)
Date: Thu, 07 Apr 2011 09:00:14 -0400
Subject: [R-sig-Geo] varying polygon layer with panel in spplot
In-Reply-To: <4D9CB53F.9050705@uni-muenster.de>
References: <4D9C8AA8.2080509@isciences.com> <4D9CB53F.9050705@uni-muenster.de>
Message-ID: <4D9DB55E.8020108@isciences.com>

Edzer - thank you VERY much.  That worked great and solved a problem 
that has been driving me NUTS.

I had never heard of the panel.number() function before. It does not 
seem to be mentioned in the usual places in the help files, or else I 
just missed it.

M

On 4/6/2011 2:47 PM, Edzer Pebesma wrote:
> Matthew, please try this:
>
> spplot(sgdf, c('cd','cu', 'pb', 'zn'),
>         names.attr = c('Cadmium', 'Copper', 'Lead', 'Zinc'),
>         as.table = TRUE, main = "Testing",
>         panel = function(x, y, z, subscripts, ...){
>             panel.gridplot(x, y, z, subscripts, ...)
>             sp.points(pts.list[[panel.number()]])
>         }
> )
>
> subscripts refers to the grid cell numbers, after they're all stacked to
> a long table; they do not match your pts.list entries.
>
> On 04/06/2011 05:45 PM, Matthew Landis wrote:
>> library(sp)
>> library(lattice) # required for trellis.par.set():
>> trellis.par.set(sp.theme()) # sets color ramp to bpy.colors()
>>
>> data(meuse)
>> coordinates(meuse)=~x+y
>>
>> library(gstat, pos = match(paste("package", "sp", sep=":"), search()) + 1)
>> data(meuse.grid)
>> coordinates(meuse.grid) = ~x+y
>> gridded(meuse.grid) = TRUE
>>
>> v.cd = variogram(log(cadmium) ~ 1, meuse)
>> ok.model.cd = fit.variogram(v.cd, vgm(1, 'Exp', 500, 1))
>> cd.ok<- krige(log(cadmium) ~ 1, meuse, meuse.grid, model = ok.model.cd)
>>
>> v.cu<- variogram(log(copper) ~ 1, meuse)
>> ok.model.cu<- fit.variogram(v.cu, vgm(1, 'Exp', 500, 1))
>> cu.ok<- krige(log(copper) ~ 1, meuse, meuse.grid, model = ok.model.cu)
>>
>> v.pb<- variogram(log(lead) ~ 1, meuse)
>> ok.model.pb<- fit.variogram(v.pb, vgm(1, 'Exp', 500, 1))
>> pb.ok<- krige(log(lead) ~ 1, meuse, meuse.grid, model = ok.model.pb)
>>
>> v.zn = variogram(log(zinc)~1, meuse)
>> ok.model.zn = fit.variogram(v.zn, vgm(1, "Exp", 500, 1))
>> zn.ok = krige(log(zinc)~1, meuse, meuse.grid, model = ok.model.zn)
>>
>> sgdf = cd.ok
>>
>> sgdf[['cd']]<- cd.ok[['var1.pred']]
>> sgdf[['cu']]<- cu.ok[['var1.pred']]
>> sgdf[['pb']]<- pb.ok[['var1.pred']]
>> sgdf[['zn']]<- zn.ok[['var1.pred']]
>>
>> pts.list<- list(meuse[, 'cadmium'], meuse[, 'copper'], meuse[, 'lead'],
>> meuse[, 'zinc'])
>>
>> spplot(sgdf, c('cd','cu', 'pb', 'zn'),
>>         names.attr = c('Cadmium', 'Copper', 'Lead', 'Zinc'),
>>         as.table = TRUE, main = "Testing",
>>         panel = function(x, y, z, subscripts, ...){
>>             panel.gridplot(x, y, z, subscripts, ...)
>>             sp.points(pts.list[[subscripts]])
>>         }
>> )

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~
Matthew Landis, Ph.D.
Research Scientist
ISciences, LLC
61 Main St. Suite 200
Burlington VT 05401
802.864.2999
~~~~~~~~~~~~~~~~~~~~~~~~~~


From naimi at itc.nl  Thu Apr  7 16:52:17 2011
From: naimi at itc.nl (Babak Naimi)
Date: Thu, 7 Apr 2011 16:52:17 +0200
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
Message-ID: <021F462EDC7F2B42A7038E9F9D4782611F18A4067F@itcnt27.itc.nl>

Dear All,

I'm trying to calculate Local Moran's I for each grid cell in a raster given a distance (d1,d2) within which the neighbour cells are identified. I wrote a function which uses localmoran of spdep package and returns a raster of Moran's I measures. But in case of large dataset, it is a time consuming function. 

I am wondering if there is any efficient solution to calculating local moran's I for raster data. Maybe, using focalFilter is a solution. Therefore, I tried to write a function that can be used in focalFilter of raster package, but the output result is different from what was calculated using spdep for a sample dataset. It might be my mistake in the formula that is used in the function. Following you will find both functions as well as the resultss for a sample raster.

I would appreciate if anyone can help.

Best regards,
Babak


Babak Naimi
PhD Candidate, Natural Resources Department,
Faculty of Geo-Information Science and Earth Observations (ITC), University of Twente, 
P.O. Box 217, 7500 AE Enschede, The Netherlands
Tel: +31 53 4874212


########

locmor<-function(ras,d1=0,d2=6000) { # function to calculate local Moran using spdep
	require(raster)
	require(spdep)
	df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
	df.sub<-subset(df,!is.na(values))
	df.nb <- dnearneigh(as.matrix(df.sub[,2:3]), d1, d2)
	df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list
	lmor <- localmoran(df.sub$values, df.listw)
	ras[df.sub$cells]<-as.data.frame(lmor)[[1]]
	return(ras) 
}


locMor.focal<-function(mtx) { # function to calculate local Moran using focalFilter
  require(raster)
  mtx.v<-as.vector(mtx)
  mtx.v.c<-mtx.v[c(trunc(length(mtx.v)/2)+1)] #value of the center of matrix (Xi)
  mtx.v.Wc<-mtx.v[-c(trunc(length(mtx.v)/2)+1)] # values of the matrix excluding center value (vector of Xj)
  mtx.v<-mtx.v[which(mtx.v != 0)]
  mtx.v.Wc<-mtx.v.Wc[which(mtx.v.Wc != 0)]
  
  z<-function(v) { #to calculate deviation from the mean for a value in the matrix
    return(v-mean(mtx.v,na.rm=T))
    }
  z.sum<-sum(sapply(mtx.v.Wc,z))
  mI<- (z(mtx.v.c)/var(mtx.v,na.rm=T))*z.sum
  return(mI)} 

#---------------

tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73,
63, 62, 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79,
55, 55, 56, 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68,
55, 54, 53, 61, 82, 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68,
59, 58, 60, 64, 88, 99, 82, 81, 71, 80, 89, 89, 89, 102, 104, 75, 63,
57, 58, 58, 77, 92, 82, 71, 59, 90, 105, 92, 79, 98, 110, 83, 62, 55,
56, 56, 80, 90, 99, 88, 64, 91, 112, 94, 76, 91, 100, 85, 62, 59, 55,
61, 99, 97, 93, 80, 65, 87, 107, 80, 59, 60, 67, 66, 65, 62, 68, 72,
102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54, 62, 62, 86, 85, 55,
59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61, 56, 41, 40,
43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41, 42,
44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42,
42, 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42,
43, 42, 53, 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43,
43, 49, 59, 62, 53, 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43,
46, 52, 56, 56, 61)
tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
tmp <- raster(tmp)


tmp.spdep<-locmor(tmp,d1=0,d2=0.08838835)

filter<-matrix(1, nrow=3, ncol=3)
tmp.focal<-focalFilter(tmp,filter,locMor.focal)

plot(tmp.spdep)
plot(tmp.focal)


From leonardo.monasterio at gmail.com  Thu Apr  7 22:31:25 2011
From: leonardo.monasterio at gmail.com (Leonardo Monasterio)
Date: Thu, 7 Apr 2011 17:31:25 -0300
Subject: [R-sig-Geo] Error message in rgeos
Message-ID: <BANLkTi=B6y7f+iEjE0u=TLdoKBELquJd+A@mail.gmail.com>

Dear all,


I am getting the following error message when I try:

> lineborder<-gBoundary(map1)
Erro em TopologyFunc(spgeom, id, byid, "rgeos_boundary") :
  IllegalArgumentException: Operation not supported by GeometryCollection

map1 is a polygon created from a shape file.

Also I get:
> gIsSimple(map1)
Erro em RGEOSUnaryPredFunc(spgeom, byid, "rgeos_issimple") :
  IllegalArgumentException: This method does not support
GeometryCollection arguments
> gIsValid(map_outer_ring)
[1] TRUE

I am running R.2.12.2  under Windows.
Thanks in advance!
Leonardo.


From narayani at ku.edu  Thu Apr  7 23:26:54 2011
From: narayani at ku.edu (Narayani Barve)
Date: Thu, 7 Apr 2011 16:26:54 -0500
Subject: [R-sig-Geo] Raster package
Message-ID: <BANLkTi=GNd_unorLpj22r6h+N_8PkP6bsQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110407/8aaa9873/attachment.pl>

From daniel.nuest at gmail.com  Thu Apr  7 23:28:50 2011
From: daniel.nuest at gmail.com (=?ISO-8859-1?Q?Daniel_N=FCst?=)
Date: Thu, 7 Apr 2011 23:28:50 +0200
Subject: [R-sig-Geo] sos4R: A new package for retrieving spatio-temporal
	data from web services
Message-ID: <BANLkTin9JOxr_OGbA69p8r7P9PQBYJukCQ@mail.gmail.com>

Dear sig-geoers,

I'd like to inform you about a new package on CRAN for downloading
spatio-temporal data in near real-time from standardized web services.
It is called "sos4R" and the package description reads as follows:

sos4R is a client for Sensor Observation Services (SOS) as specified
by the Open Geospatial Consortium (OGC). It allows users to retrieve
metadata from SOS web services and to interactively create requests
for observation data based on the available sensors, phenomena,
observations et cetera using thematic, temporal and spatial filtering.
[1]


If abbreviations like SOS, SWE, OGC, GML, O&M, ... are new to you,
don't worry - the package's goal is to provide users with a simple
interface to the growing amount of data available via Sensor
Observation Services, without you having to dig into the details. And
the quite extensive vignette gives a short introduction into the area
as well.

If you're interested, please also check out the development blog [2],
which also contains a list of some interesting SOS [3] - maybe there
is some data of interest for you already available! I look forward to
comments, questions, bug reports, and feature requests.

Best regards,
Daniel


[1] http://cran.r-project.org/web/packages/sos4R/index.html
[2] http://www.nordholmen.net/sos4r
[3] http://www.nordholmen.net/sos4r/data/


From rundel at gmail.com  Thu Apr  7 23:39:26 2011
From: rundel at gmail.com (Colin Rundel)
Date: Thu, 7 Apr 2011 14:39:26 -0700
Subject: [R-sig-Geo] Error message in rgeos
In-Reply-To: <BANLkTi=B6y7f+iEjE0u=TLdoKBELquJd+A@mail.gmail.com>
References: <BANLkTi=B6y7f+iEjE0u=TLdoKBELquJd+A@mail.gmail.com>
Message-ID: <284B8370-35E4-4A92-9360-72EE16B98F7E@gmail.com>


> I am getting the following error message when I try:
> 
>> lineborder<-gBoundary(map1)
> Erro em TopologyFunc(spgeom, id, byid, "rgeos_boundary") :
>  IllegalArgumentException: Operation not supported by GeometryCollection
> 
> map1 is a polygon created from a shape file.
> 
> Also I get:
>> gIsSimple(map1)
> Erro em RGEOSUnaryPredFunc(spgeom, byid, "rgeos_issimple") :
>  IllegalArgumentException: This method does not support
> GeometryCollection arguments
>> gIsValid(map_outer_ring)
> [1] TRUE

For some reason when Rgeos is attempting to convert your sp object it is using a GeometryCollection type instead of a Polygon or Multipolygon type. Currently the GEOS library does not support GeometryCollection types with either gBoundary or gIsSimple and some of the other functions as well.

You can try both functions with byid=TRUE which will then run the GEOS function on each sub element of your sp object and will either return a single sp object or list of sp objects depending on the compatibility of the returned values.

If you can host the shapefile you are using somewhere and provide a link, I can look into why Rgeos is treating it as a GeometryCollection.

-Colin

From paul.hiemstra at knmi.nl  Fri Apr  8 08:34:42 2011
From: paul.hiemstra at knmi.nl (Paul Hiemstra)
Date: Fri, 08 Apr 2011 08:34:42 +0200
Subject: [R-sig-Geo] Raster package
In-Reply-To: <BANLkTi=GNd_unorLpj22r6h+N_8PkP6bsQ@mail.gmail.com>
References: <BANLkTi=GNd_unorLpj22r6h+N_8PkP6bsQ@mail.gmail.com>
Message-ID: <4D9EAC82.2030700@knmi.nl>

On 04/07/2011 11:26 PM, Narayani Barve wrote:
> Hi All,
>
> I am using raster package for various operations. It is a useful package,
> but manual  / pdf contains lines which are broken and not complete. This
> happens when there is a explanation for a argument in a function. I am not
> able to read explanation for rasterize, raster, and many more function. I
> downloaded the pdf from the following link. Is there a way this could be
> fixed in the raster package project? OR Is there any other link where the
> documentation is complete ?
>
> http://cran.r-project.org/web/packages/raster/raster.pdf
>
> Thanks,
> Narayani
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
Hi,

For example the function rasterize does wrap the text in the details
correctly. In the PDF the text disappears from the page. In the online
help pages (?rasterize) there is wrapping, but it looks quite ugly.
Maybe the .Rd describe environment is not very good at multi-line
descriptions? So, Narayani, you can read the documentation online using
R itself, which does show all the text.

cheers,
Paul

-- 
Paul Hiemstra, MSc
Global Climate Division
Royal Netherlands Meteorological Institute (KNMI)
Wilhelminalaan 10 | 3732 GK | De Bilt | Kamer B 3.39
P.O. Box 201 | 3730 AE | De Bilt
tel: +31 30 2206 494

http://intamap.geo.uu.nl/~paul
http://nl.linkedin.com/pub/paul-hiemstra/20/30b/770


From paul.hiemstra at knmi.nl  Fri Apr  8 08:36:48 2011
From: paul.hiemstra at knmi.nl (Paul Hiemstra)
Date: Fri, 08 Apr 2011 08:36:48 +0200
Subject: [R-sig-Geo] Raster package
In-Reply-To: <4D9EAC82.2030700@knmi.nl>
References: <BANLkTi=GNd_unorLpj22r6h+N_8PkP6bsQ@mail.gmail.com>
	<4D9EAC82.2030700@knmi.nl>
Message-ID: <4D9EAD00.5050203@knmi.nl>

On 04/08/2011 08:34 AM, Paul Hiemstra wrote:
> On 04/07/2011 11:26 PM, Narayani Barve wrote:
>> Hi All,
>>
>> I am using raster package for various operations. It is a useful package,
>> but manual  / pdf contains lines which are broken and not complete. This
>> happens when there is a explanation for a argument in a function. I am not
>> able to read explanation for rasterize, raster, and many more function. I
>> downloaded the pdf from the following link. Is there a way this could be
>> fixed in the raster package project? OR Is there any other link where the
>> documentation is complete ?
>>
>> http://cran.r-project.org/web/packages/raster/raster.pdf
>>
>> Thanks,
>> Narayani
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> Hi,
>

edit: should be 'does not wrap the text correctly'...

> For example the function rasterize does wrap the text in the details
> correctly. In the PDF the text disappears from the page. In the online
> help pages (?rasterize) there is wrapping, but it looks quite ugly.
> Maybe the .Rd describe environment is not very good at multi-line
> descriptions? So, Narayani, you can read the documentation online using
> R itself, which does show all the text.
>
> cheers,
> Paul
>


-- 
Paul Hiemstra, MSc
Global Climate Division
Royal Netherlands Meteorological Institute (KNMI)
Wilhelminalaan 10 | 3732 GK | De Bilt | Kamer B 3.39
P.O. Box 201 | 3730 AE | De Bilt
tel: +31 30 2206 494

http://intamap.geo.uu.nl/~paul
http://nl.linkedin.com/pub/paul-hiemstra/20/30b/770


From jonathan.daeden at gmail.com  Fri Apr  8 10:10:33 2011
From: jonathan.daeden at gmail.com (jonathan daeden)
Date: Fri, 8 Apr 2011 10:10:33 +0200
Subject: [R-sig-Geo] [R-sig-geo] Sum lines when they intersect polygons
Message-ID: <BANLkTi=JAOGkvQPfK84dAQu8zKdbbhUOug@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110408/f69a6b1c/attachment.pl>

From jacobvanetten at yahoo.com  Fri Apr  8 11:41:08 2011
From: jacobvanetten at yahoo.com (Jacob van Etten)
Date: Fri, 8 Apr 2011 10:41:08 +0100 (BST)
Subject: [R-sig-Geo] [R-sig-geo] Sum lines when they intersect polygons
In-Reply-To: <BANLkTi=JAOGkvQPfK84dAQu8zKdbbhUOug@mail.gmail.com>
Message-ID: <536836.71094.qm@web29717.mail.ird.yahoo.com>

Do you want to measure the *length* of the coastline in each grid cell?

Or do you want to measure the *presence* of the coastline in each cell?

Jacob.

--- On Fri, 8/4/11, jonathan daeden <jonathan.daeden at gmail.com> wrote:

> From: jonathan daeden <jonathan.daeden at gmail.com>
> Subject: [R-sig-Geo] [R-sig-geo] Sum lines when they intersect polygons
> To: r-sig-geo at r-project.org
> Date: Friday, 8 April, 2011, 10:10
> Dear list,
> 
> I am looking for a way to measure lines crossing polygones
> or a grid.
> I have a line shapefile representing a coastline and a
> regular grid over it.
> I would like to measure the coastline in each cells of the
> grid and keep it
> in a data.frame.
> Could not make it with Qgis so i would like to try with R.
> 
> 
> cheers,
> 
> Jonathan
> 
> ??? [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From piero.campa at gmail.com  Fri Apr  8 12:40:53 2011
From: piero.campa at gmail.com (piero campa)
Date: Fri, 8 Apr 2011 03:40:53 -0700 (PDT)
Subject: [R-sig-Geo] Support in krige/gstat
Message-ID: <1302259253379-6253214.post@n2.nabble.com>

Dear list,
I have datasets of different variables with different spatio-temporal
supports, and I'd like to [co]krige them together.

What I'd like to ask you is:
- I know that with block kriging one could estimate values over a different
support area wrt the input variables, but is one allowed to have a disparity
in the support areas among the input variables?
- which is the right way to account for different spatio-*temporal* supports
when [co]kriging data?

For what concerns the disparity in the *spatial* support, I'm assuming that
the gstat library recognizes e.g. a SpatialPixelsDataFrame as a dataset with
spatial support equal to the area of the pixels. But I'm not sure about
this.
For what concerns the different *temporal* support instead, I guess a
spatio-temporal model needs to be built up.  (?)

Really thank you for the support.
Regards,
/Piero

--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Support-in-krige-gstat-tp6253214p6253214.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From leonardo.monasterio at gmail.com  Fri Apr  8 13:33:37 2011
From: leonardo.monasterio at gmail.com (Leonardo Monasterio)
Date: Fri, 8 Apr 2011 08:33:37 -0300
Subject: [R-sig-Geo] Error message in rgeos
In-Reply-To: <284B8370-35E4-4A92-9360-72EE16B98F7E@gmail.com>
References: <BANLkTi=B6y7f+iEjE0u=TLdoKBELquJd+A@mail.gmail.com>
	<284B8370-35E4-4A92-9360-72EE16B98F7E@gmail.com>
Message-ID: <BANLkTi=yErXdwqj4YDyzAByG_555zHDT1g@mail.gmail.com>

Dear Colin,

Thank you very much. I've uploaded shapefile to:
http://dl.dropbox.com/u/116353/OD87.zip

Yours,
Leonardo.

On Thu, Apr 7, 2011 at 6:39 PM, Colin Rundel <rundel at gmail.com> wrote:
>
>> I am getting the following error message when I try:
>>
>>> lineborder<-gBoundary(map1)
>> Erro em TopologyFunc(spgeom, id, byid, "rgeos_boundary") :
>> ?IllegalArgumentException: Operation not supported by GeometryCollection
>>
>> map1 is a polygon created from a shape file.
>>
>> Also I get:
>>> gIsSimple(map1)
>> Erro em RGEOSUnaryPredFunc(spgeom, byid, "rgeos_issimple") :
>> ?IllegalArgumentException: This method does not support
>> GeometryCollection arguments
>>> gIsValid(map_outer_ring)
>> [1] TRUE
>
> For some reason when Rgeos is attempting to convert your sp object it is using a GeometryCollection type instead of a Polygon or Multipolygon type. Currently the GEOS library does not support GeometryCollection types with either gBoundary or gIsSimple and some of the other functions as well.
>
> You can try both functions with byid=TRUE which will then run the GEOS function on each sub element of your sp object and will either return a single sp object or list of sp objects depending on the compatibility of the returned values.
>
> If you can host the shapefile you are using somewhere and provide a link, I can look into why Rgeos is treating it as a GeometryCollection.
>
> -Colin


From edzer.pebesma at uni-muenster.de  Fri Apr  8 14:00:49 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Fri, 08 Apr 2011 14:00:49 +0200
Subject: [R-sig-Geo] Support in krige/gstat
In-Reply-To: <1302259253379-6253214.post@n2.nabble.com>
References: <1302259253379-6253214.post@n2.nabble.com>
Message-ID: <4D9EF8F1.7010703@uni-muenster.de>



On 04/08/2011 12:40 PM, piero campa wrote:
> Dear list,
> I have datasets of different variables with different spatio-temporal
> supports, and I'd like to [co]krige them together.
> 
> What I'd like to ask you is:
> - I know that with block kriging one could estimate values over a different
> support area wrt the input variables, but is one allowed to have a disparity
> in the support areas among the input variables?

Theoretically there is no problem in doing this; there have been several
works doing this in the spatial domain, mostly named "area to point
kriging". Implementations seem to be done in matlab (by Phaedon
Kyriakidis), in ArcGIS (maybe not the current release but then probably
in the upcoming version) and in Pierre Goovaerts' software.

I don't know of any open source implementations, I also don't know about
implementations for the heterogeneous spatial AND temporal supports.

> - which is the right way to account for different spatio-*temporal* supports
> when [co]kriging data?

[Co]kriging is about solving systems with covariances between data and
prediction locations/areas, so the way to go is to estimate the
covariogram for the process on the point scale, and from there derive
the st/area-to-area covariances for any observation-observation or
observation-prediction pair by integrating, and put these in the kriging
equations. I believe the equations can be found as far back as in
Journel and Huijbregts' classic "Mining Geostatistics".

> 
> For what concerns the disparity in the *spatial* support, I'm assuming that
> the gstat library recognizes e.g. a SpatialPixelsDataFrame as a dataset with
> spatial support equal to the area of the pixels. But I'm not sure about
> this.

No, it doesn't; it assumes observations are points at pixel centers.

> For what concerns the different *temporal* support instead, I guess a
> spatio-temporal model needs to be built up.  (?)

generally speaking, yes; some people also use 3-D variogram models for
space time (taking the S/T anisotropy issue into account of course).

> 
> Really thank you for the support.

;-)

> Regards,
> /Piero
> 
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Support-in-krige-gstat-tp6253214p6253214.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From b.rowlingson at lancaster.ac.uk  Fri Apr  8 14:06:44 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 8 Apr 2011 13:06:44 +0100
Subject: [R-sig-Geo] [R-sig-geo] Sum lines when they intersect polygons
In-Reply-To: <BANLkTi=JAOGkvQPfK84dAQu8zKdbbhUOug@mail.gmail.com>
References: <BANLkTi=JAOGkvQPfK84dAQu8zKdbbhUOug@mail.gmail.com>
Message-ID: <BANLkTinmxUL5f8xVfS0WTxJMdA6O6XtaeA@mail.gmail.com>

On Fri, Apr 8, 2011 at 9:10 AM, jonathan daeden
<jonathan.daeden at gmail.com> wrote:
> Dear list,
>
> I am looking for a way to measure lines crossing polygones or a grid.
> I have a line shapefile representing a coastline and a regular grid over it.
> I would like to measure the coastline in each cells of the grid and keep it
> in a data.frame.
> Could not make it with Qgis so i would like to try with R.

 Sounds like another job for rgeos!

 If your grid is defined as a set of polygons (and not just some
horizontal and vertical lines) then you'd just be doing overlays of
the polygons on your coastline, and then getting the
SpatialLinesLength (or similar) of the resulting objects.

 Lunchtime, otherwise I'd be doing this now :)

Barry


From b.rowlingson at lancaster.ac.uk  Fri Apr  8 16:32:07 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 8 Apr 2011 15:32:07 +0100
Subject: [R-sig-Geo] [R-sig-geo] Sum lines when they intersect polygons
In-Reply-To: <BANLkTinmxUL5f8xVfS0WTxJMdA6O6XtaeA@mail.gmail.com>
References: <BANLkTi=JAOGkvQPfK84dAQu8zKdbbhUOug@mail.gmail.com>
	<BANLkTinmxUL5f8xVfS0WTxJMdA6O6XtaeA@mail.gmail.com>
Message-ID: <BANLkTikJqJ6mq1LO1tUmYOosi-u0XVVp7g@mail.gmail.com>

Slow and dirty method:

lineLengthIntersects <- function(gridpolys,linegeom){
  ngrids = length(gridpolys)
  ll = rep(0,ngrids)
  for(i in 1:ngrids){
    geomint = gIntersection(linegeom,gridpolys[i])
    if(!is.null(geomint)){
      ll[i] = SpatialLinesLengths(geomint,TRUE)
    }
  }
  return(ll)
}

 this returns the line length in km (TRUE in SpatialLinesLengths means
work in lat-long on a sphere and convert to km) for overlaying
polygons in gridpolys (a SpatialPolygons object) on a line geometry in
linegeom (a SpatialLines object).

 there might be a better way of doing it without the loop...

here's my polygrid function for creating SpatialPolygons of a regular
grid based on x and y - use something like:

grids = polygrid(seq(0,1,len=10),seq(0,1,len=10)) to get a grid
somewhere near the greenwich meridian on the equator...

polygrid <- function(x,y){
  nx = length(x)
  ny = length(y)
  spp = list()
  ip = 1
  for(ix in 1:(nx-1)){
    for(iy in 1:(ny-1)){
      xc=x[c(ix,ix+1,ix+1,ix,ix)]
      yc=y[c(iy,iy,iy+1,iy+1,iy)]
      spp[[ip]] = Polygons(list(Polygon(cbind(xc,yc))),ID=ip)
      ip = ip + 1
    }
  }
  SpatialPolygons(spp)
}


Barry


From landis at isciences.com  Fri Apr  8 17:21:57 2011
From: landis at isciences.com (Matthew Landis)
Date: Fri, 8 Apr 2011 11:21:57 -0400
Subject: [R-sig-Geo] spplot, polygons, and line density fill
Message-ID: <4D9F2815.4090205@isciences.com>

Hello again R-sig-geo-ers,

Does anyone know how to convince spplot to create a figure like 
http://r-spatial.sourceforge.net/gallery/#fig15.R

I would like to overlay some polygons onto a raster, and I need to do it 
with spplot for the lattice functionality.  I've looked at the help and 
the function code for sp.polygon and grid.polygon, as well as searched a 
bit on Google, but I'm not finding anything promising, but then, I'm not 
that familiar with grid graphics.

Since we're talking about R, I have a hard to time believing the answer 
is "it's not possible".

M
<http://r-spatial.sourceforge.net/gallery/#fig15.R>

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~
Matthew Landis, Ph.D.
Research Scientist
ISciences, LLC
61 Main St. Suite 200
Burlington VT 05401
802.864.2999
~~~~~~~~~~~~~~~~~~~~~~~~~~


From landis at isciences.com  Fri Apr  8 17:25:58 2011
From: landis at isciences.com (Matthew Landis)
Date: Fri, 8 Apr 2011 11:25:58 -0400
Subject: [R-sig-Geo] spplot, polygons, and line density fill
In-Reply-To: <4D9F2815.4090205@isciences.com>
References: <4D9F2815.4090205@isciences.com>
Message-ID: <4D9F2906.6050508@isciences.com>

In case my request wasn't really clear, let me rephrase:

I'd like to overlay some polygons onto a raster, by plotting with a line 
density fill instead of a solid fill, as shown here 
http://r-spatial.sourceforge.net/gallery/#fig15.R.  Ideally I would be 
able to modify the angle of the lines.  I know how to do this in base 
graphics, but I really need to use spplot (or willing to use ggplot2 if 
that can do what I want).

Thanks in advance for any ideas!

Matt

On 4/8/2011 11:21 AM, Matthew Landis wrote:
> Hello again R-sig-geo-ers,
>
> Does anyone know how to convince spplot to create a figure like 
> http://r-spatial.sourceforge.net/gallery/#fig15.R
>
> I would like to overlay some polygons onto a raster, and I need to do 
> it with spplot for the lattice functionality.  I've looked at the help 
> and the function code for sp.polygon and grid.polygon, as well as 
> searched a bit on Google, but I'm not finding anything promising, but 
> then, I'm not that familiar with grid graphics.
>
> Since we're talking about R, I have a hard to time believing the 
> answer is "it's not possible".
>
> M
> <http://r-spatial.sourceforge.net/gallery/#fig15.R>
>

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~
Matthew Landis, Ph.D.
Research Scientist
ISciences, LLC
61 Main St. Suite 200
Burlington VT 05401
802.864.2999
~~~~~~~~~~~~~~~~~~~~~~~~~~


From mdsumner at gmail.com  Sat Apr  9 01:25:08 2011
From: mdsumner at gmail.com (Michael Sumner)
Date: Sat, 9 Apr 2011 09:25:08 +1000
Subject: [R-sig-Geo] [R-sig-geo] Sum lines when they intersect polygons
In-Reply-To: <BANLkTikJqJ6mq1LO1tUmYOosi-u0XVVp7g@mail.gmail.com>
References: <BANLkTi=JAOGkvQPfK84dAQu8zKdbbhUOug@mail.gmail.com>
	<BANLkTinmxUL5f8xVfS0WTxJMdA6O6XtaeA@mail.gmail.com>
	<BANLkTikJqJ6mq1LO1tUmYOosi-u0XVVp7g@mail.gmail.com>
Message-ID: <BANLkTikrkuKNPp0EzABxxHxummxrNBj7Yg@mail.gmail.com>

If you really just need a grid then pixellate.psp in spatstat can do
this, pass in the "length" with the weights argument.

On Sat, Apr 9, 2011 at 12:32 AM, Barry Rowlingson
<b.rowlingson at lancaster.ac.uk> wrote:
> Slow and dirty method:
>
> lineLengthIntersects <- function(gridpolys,linegeom){
> ?ngrids = length(gridpolys)
> ?ll = rep(0,ngrids)
> ?for(i in 1:ngrids){
> ? ?geomint = gIntersection(linegeom,gridpolys[i])
> ? ?if(!is.null(geomint)){
> ? ? ?ll[i] = SpatialLinesLengths(geomint,TRUE)
> ? ?}
> ?}
> ?return(ll)
> }
>
> ?this returns the line length in km (TRUE in SpatialLinesLengths means
> work in lat-long on a sphere and convert to km) for overlaying
> polygons in gridpolys (a SpatialPolygons object) on a line geometry in
> linegeom (a SpatialLines object).
>
> ?there might be a better way of doing it without the loop...
>
> here's my polygrid function for creating SpatialPolygons of a regular
> grid based on x and y - use something like:
>
> grids = polygrid(seq(0,1,len=10),seq(0,1,len=10)) to get a grid
> somewhere near the greenwich meridian on the equator...
>
> polygrid <- function(x,y){
> ?nx = length(x)
> ?ny = length(y)
> ?spp = list()
> ?ip = 1
> ?for(ix in 1:(nx-1)){
> ? ?for(iy in 1:(ny-1)){
> ? ? ?xc=x[c(ix,ix+1,ix+1,ix,ix)]
> ? ? ?yc=y[c(iy,iy,iy+1,iy+1,iy)]
> ? ? ?spp[[ip]] = Polygons(list(Polygon(cbind(xc,yc))),ID=ip)
> ? ? ?ip = ip + 1
> ? ?}
> ?}
> ?SpatialPolygons(spp)
> }
>
>
> Barry
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Michael Sumner
Institute for Marine and Antarctic Studies, University of Tasmania
Hobart, Australia
e-mail: mdsumner at gmail.com


From sa.cizmeli at usherbrooke.ca  Sat Apr  9 02:26:54 2011
From: sa.cizmeli at usherbrooke.ca (servet cizmeli)
Date: Fri, 08 Apr 2011 20:26:54 -0400
Subject: [R-sig-Geo] spacetime : the challenge of image time series
	containing holes
Message-ID: <4D9FA7CE.2000108@usherbrooke.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110408/7ebbd004/attachment.pl>

From tom.gottfried at wzw.tum.de  Sat Apr  9 18:40:30 2011
From: tom.gottfried at wzw.tum.de (Tom Gottfried)
Date: Sat, 09 Apr 2011 18:40:30 +0200
Subject: [R-sig-Geo] spacetime : the challenge of image time
 series	containing holes
In-Reply-To: <4D9FA7CE.2000108@usherbrooke.ca>
References: <4D9FA7CE.2000108@usherbrooke.ca>
Message-ID: <4DA08BFE.9070302@wzw.tum.de>

Hi,

see below

Am 09.04.2011 02:26, schrieb servet cizmeli:
> Greetings to all,
>
> I have monthly mean satellite image time series that come in a 4-column
> text file each.  I have dozens of such files. Every line correspond to a
> pixel that is projected onto a regular ISIN grid. The first column is
> the ISIN grid number (same coordinates for a given grid number in every
> file), the second column is the latitude, the third column is the
> longitude and the last column is the value of the satellite measurement.
> Every file contain a varying number of lines (no NAs), although the lat
> and lon associated for a given ISIN grid number is the same in every file.
>
> I read the examples related with the STFDF class and understood it.
> However this is not a good solution for me as I have a lot of holes in
> my data. I would therefore like to place these time series into an
> irregular STSDF spacetime format. I could not find any information on
> how to achieve that. I would appreciate it very much if you could help
> me construct the irregular STSDF (or STIDF?) object. How to instruct the
> STSDF function so that it understands what goes where?

> Below is a sample code that involves 3 image files, each containing
> different pixels. I do not have handy a complete list of a list of all
> grid numbers for which there is at least one pixel. But I could easily
> generate it by reading in every image in a same data frame and producing
> a list of the unique values of the first column. For now I construct a
> SpatialPoints object but eventually I will be working with SpatialPixels.
>
> thanks a lot for your kindly help.
> Best
> Servet
>
> library(sp)
> library(spacetime)
> img200206<- data.frame(rbind(
> c(10849178, -4.979167,      -19.99302,      1.1542897E-03),
> c(10849179, -4.979167,      -19.95120,      1.3418309E-03),
> c(10849180, -4.979167,      -19.90937,      1.3988283E-03)))
>
> img200207<- data.frame(rbind(
> c(10849178,  -4.979167,      -19.99302,      1.2843144E-03),
> c(10849179,  -4.979167,      -19.95120,      1.1260216E-03)))
>
> img200208<- data.frame(rbind(
> c(10849179,  -4.979167,      -19.95120,      1.5001300E-03),
> c(10849180,  -4.979167,      -19.90937,      1.4793734E-03)))
>
> coordinates(img200206)=c("X3","X2")
> coordinates(img200207)=c("X3","X2")
> coordinates(img200208)=c("X3","X2")
>
> time = xts(1:3, as.POSIXct(c("2002-06-01","2002-07-01","2002-08-01")))
>
> #This is where I am stuck at
> mySTSDF = STSDF(  ... )

there's surely a better way for constructing the index (depending on how 
you organise your input data), but for the example this should work:

sp <- SpatialPoints(unique(rbind(coordinates(img200206),
coordinates(img200207), coordinates(img200208))))
data <- rbind(img200206, img200207, img200208)@data
index <- matrix(c(1:3, 1:2, 2:3, rep(1, 3), rep(2,2), rep(3,2)), 	
nrow=nrow(data))
mySTSDF = STSDF(sp, time, data, index)

regards,
Tom

>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From ldecola at comcast.net  Sat Apr  9 19:35:18 2011
From: ldecola at comcast.net (ldecola at comcast.net)
Date: Sat, 9 Apr 2011 17:35:18 +0000 (UTC)
Subject: [R-sig-Geo] graph theory
Message-ID: <2104929106.818270.1302370518979.JavaMail.root@sz0030a.westchester.pa.mail.comcast.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110409/c2627d26/attachment.pl>

From dan.putler at sauder.ubc.ca  Sat Apr  9 21:47:10 2011
From: dan.putler at sauder.ubc.ca (Dan Putler)
Date: Sat, 9 Apr 2011 12:47:10 -0700
Subject: [R-sig-Geo] graph theory
In-Reply-To: <1040_1302370553_1302370553_2104929106.818270.1302370518979.JavaMail.root@sz0030a.westchester.pa.mail.comcast.net>
References: <1040_1302370553_1302370553_2104929106.818270.1302370518979.JavaMail.root@sz0030a.westchester.pa.mail.comcast.net>
Message-ID: <4DA0B7BE.7080002@sauder.ubc.ca>

Hi Lee,

The spdep package has a number of graph theory related tools that may be 
of use.

Dan

On 04/09/2011 10:35 AM, ldecola at comcast.net wrote:
> I'm teaching a GIS data structures using sp and I'd like to give a class on basic graphs. Does anyone have any experience with the various R packages in this area - what I have found so far:
>
> diagram Functions for visualising simple graphs (networks), plotting flow diagrams
> GrapheR A multiplatform GUI for drawing customizable graphs in R
> giRaph The giRaph package for graph representation in R
> graph graph: A package to handle graph data structures
> igraph Network analysis and visualization
> mathgraph Directed and undirected graphs
>
> Lee De Cola, PhD, MCP.
> DATA to Insight
> LDECOLA at COMCAST.NET
> Reston, Virginia
> 571 315 0577 mobile
> 703 709 6972 home&  fax
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From mspinola10 at gmail.com  Sun Apr 10 03:02:39 2011
From: mspinola10 at gmail.com (=?ISO-8859-1?Q?Manuel_Sp=EDnola?=)
Date: Sat, 09 Apr 2011 19:02:39 -0600
Subject: [R-sig-Geo] dissolve in R
Message-ID: <4DA101AF.1050601@gmail.com>

Dear list members,

How can I do a dissolve of a shapefile in R (the shapefile has 3 polygons)?

Thank you very much in advance.

Best,

Manuel


From lists at remoteinformation.com.au  Sun Apr 10 06:45:33 2011
From: lists at remoteinformation.com.au (Ben Madin)
Date: Sun, 10 Apr 2011 12:45:33 +0800
Subject: [R-sig-Geo] graph theory
In-Reply-To: <2104929106.818270.1302370518979.JavaMail.root@sz0030a.westchester.pa.mail.comcast.net>
References: <2104929106.818270.1302370518979.JavaMail.root@sz0030a.westchester.pa.mail.comcast.net>
Message-ID: <B6C58375-309C-47E5-B88E-8D772A422241@remoteinformation.com.au>

You could also look at network & sna in the Social Sciences Task View - they have some good examples.

cheers

Ben



On 10/04/2011, at 1:35 AM, ldecola at comcast.net wrote:

> I'm teaching a GIS data structures using sp and I'd like to give a class on basic graphs. Does anyone have any experience with the various R packages in this area - what I have found so far: 
> 
> diagram Functions for visualising simple graphs (networks), plotting flow diagrams 
> GrapheR A multiplatform GUI for drawing customizable graphs in R 
> giRaph The giRaph package for graph representation in R 
> graph graph: A package to handle graph data structures 
> igraph Network analysis and visualization 
> mathgraph Directed and undirected graphs 
> 
> Lee De Cola, PhD, MCP. 
> DATA to Insight 
> LDECOLA at COMCAST.NET 
> Reston, Virginia 
> 571 315 0577 mobile 
> 703 709 6972 home & fax 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From carson.farmer at gmail.com  Sun Apr 10 11:26:48 2011
From: carson.farmer at gmail.com (Carson J Q Farmer)
Date: Sun, 10 Apr 2011 10:26:48 +0100
Subject: [R-sig-Geo] dissolve in R
In-Reply-To: <4DA101AF.1050601@gmail.com>
References: <4DA101AF.1050601@gmail.com>
Message-ID: <7C5F480B-A9C5-4269-BAC0-55597EF98936@gmail.com>

Have a look at the rgeos package, and the gUnion function.

Carson

--
Carson J. Q. Farmer
ISSP Doctoral Fellow
National Centre for Geocomputation
National University of Ireland, Maynooth
www.carsonfarmer.com

On 10 Apr 2011, at 02:02 a.m., Manuel Sp?nola <mspinola10 at gmail.com> wrote:

> Dear list members,
> 
> How can I do a dissolve of a shapefile in R (the shapefile has 3 polygons)?
> 
> Thank you very much in advance.
> 
> Best,
> 
> Manuel
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From mspinola10 at gmail.com  Sun Apr 10 18:54:20 2011
From: mspinola10 at gmail.com (=?ISO-8859-1?Q?Manuel_Sp=EDnola?=)
Date: Sun, 10 Apr 2011 10:54:20 -0600
Subject: [R-sig-Geo] Problems reading shapefiles in R
Message-ID: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110410/a8cdf05d/attachment.pl>

From mazatlanmexico at yahoo.com  Sun Apr 10 19:26:45 2011
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Sun, 10 Apr 2011 10:26:45 -0700 (PDT)
Subject: [R-sig-Geo] Problems reading shapefiles in R
In-Reply-To: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>
References: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>
Message-ID: <916681.29785.qm@web56606.mail.re3.yahoo.com>

Manuel:
As a test, delete?the rgeos?package from your library and run?one of the 
commands again. You can always?install the package back if you need it.
?
Felipe D. Carrillo
Supervisory Fishery Biologist
Department of the Interior
US Fish & Wildlife Service
California, USA
http://www.fws.gov/redbluff/rbdd_jsmp.aspx




----- Original Message ----
> From: Manuel Sp?nola <mspinola10 at gmail.com>
> To: r-sig-geo at r-project.org
> Sent: Sun, April 10, 2011 9:54:20 AM
> Subject: [R-sig-Geo] Problems reading shapefiles in R
> 
> Dear list members,
> 
> I am trying to read a shapefile in R using 3 different methods and all of
> them failed:
> 
> > ai_biotica <- readOGR(dsn="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
> layer="AI_BIOTICA_010411_CRTM05")
> Error in ogrInfo(dsn = dsn, layer = layer, input_field_name_encoding =
> input_field_name_encoding) :
> 
> ? ? ? ? GDAL Error 4: Unable to open
> C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.shx
> or
> 
C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.SHX.
> 
> > nc1 <-
>readShapePoly("C:/ProyectosRespacial/ICE/Shapefiles/AI_BIOTICA_010411_CRTM05.shp",
>,
> proj4string=CRS("+proj=tmerc +lon_0=-84 +lat_0=0 +x_0=500000 +k=0.9999
> +datum=WGS84"))
> Error in read.dbf(filen1) : unable to open DBF file
> 
> 
> > ai_biotica <- readShapeSpatial("AI_BIOTICA_010411_CRTM05")
> Error in read.dbf(filen1) : unable to open DBF file
> 
> I will appreciate any input on the problem.
> 
> Best,
> 
> Manuel Sp?nola
> 
> 
> 
> -- 
> *Manuel Sp?nola, Ph.D.*
> Instituto Internacional en Conservaci?n y Manejo de Vida Silvestre
> Universidad Nacional
> Apartado 1350-3000
> Heredia
> COSTA RICA
> mspinola at una.ac.cr
> mspinola10 at gmail.com
> Tel?fono: (506) 2277-3598
> Fax: (506) 2237-7036
> Personal website: Lobito de r?o <https://sites.google.com/site/lobitoderio/>
> Institutional website: ICOMVIS <http://www.icomvis.una.ac.cr/>
> 
> ??? [[alternative HTML version deleted]]
> 
>


From Roger.Bivand at nhh.no  Sun Apr 10 21:15:51 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 10 Apr 2011 21:15:51 +0200 (CEST)
Subject: [R-sig-Geo] Problems reading shapefiles in R
In-Reply-To: <916681.29785.qm@web56606.mail.re3.yahoo.com>
References: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>
	<916681.29785.qm@web56606.mail.re3.yahoo.com>
Message-ID: <alpine.LRH.2.00.1104102108430.2438@reclus.nhh.no>

On Sun, 10 Apr 2011, Felipe Carrillo wrote:

> Manuel:
> As a test, delete?the rgeos?package from your library and run?one of the
> commands again. You can always?install the package back if you need it.

Please do not give unmotivated advice on the list, as it will mislead. 
There is no interaction between rgeos and functions for reading vector 
objects - if you believe otherwise, please report this with a reproducible 
case. Bad advice is worse than no advice.

The obvious place to check is:

list.files(path="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
   pattern="AI_BIOTICA_010411_CRTM05")

which must show at least:

"AI_BIOTICA_010411_CRTM05.dbf" "AI_BIOTICA_010411_CRTM05.shp" 
"AI_BIOTICA_010411_CRTM05.shx"

for "AI_BIOTICA_010411_CRTM05" to be a shapefile. The error messages show 
that "AI_BIOTICA_010411_CRTM05.dbf" is missing, so please check that it is 
present first.

Hope this clarifies,

Roger

> ?
> Felipe D. Carrillo
> Supervisory Fishery Biologist
> Department of the Interior
> US Fish & Wildlife Service
> California, USA
> http://www.fws.gov/redbluff/rbdd_jsmp.aspx
>
>
>
>
> ----- Original Message ----
>> From: Manuel Sp?nola <mspinola10 at gmail.com>
>> To: r-sig-geo at r-project.org
>> Sent: Sun, April 10, 2011 9:54:20 AM
>> Subject: [R-sig-Geo] Problems reading shapefiles in R
>>
>> Dear list members,
>>
>> I am trying to read a shapefile in R using 3 different methods and all of
>> them failed:
>>
>>> ai_biotica <- readOGR(dsn="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
>> layer="AI_BIOTICA_010411_CRTM05")
>> Error in ogrInfo(dsn = dsn, layer = layer, input_field_name_encoding =
>> input_field_name_encoding) :
>>
>> ? ? ? ? GDAL Error 4: Unable to open
>> C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.shx
>> or
>>
> C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.SHX.
>>
>>> nc1 <-
>> readShapePoly("C:/ProyectosRespacial/ICE/Shapefiles/AI_BIOTICA_010411_CRTM05.shp",
>> ,
>> proj4string=CRS("+proj=tmerc +lon_0=-84 +lat_0=0 +x_0=500000 +k=0.9999
>> +datum=WGS84"))
>> Error in read.dbf(filen1) : unable to open DBF file
>>
>>
>>> ai_biotica <- readShapeSpatial("AI_BIOTICA_010411_CRTM05")
>> Error in read.dbf(filen1) : unable to open DBF file
>>
>> I will appreciate any input on the problem.
>>
>> Best,
>>
>> Manuel Sp?nola
>>
>>
>>
>> --
>> *Manuel Sp?nola, Ph.D.*
>> Instituto Internacional en Conservaci?n y Manejo de Vida Silvestre
>> Universidad Nacional
>> Apartado 1350-3000
>> Heredia
>> COSTA RICA
>> mspinola at una.ac.cr
>> mspinola10 at gmail.com
>> Tel?fono: (506) 2277-3598
>> Fax: (506) 2237-7036
>> Personal website: Lobito de r?o <https://sites.google.com/site/lobitoderio/>
>> Institutional website: ICOMVIS <http://www.icomvis.una.ac.cr/>
>>
>> ??? [[alternative HTML version deleted]]
>>
>>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From mazatlanmexico at yahoo.com  Sun Apr 10 22:00:38 2011
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Sun, 10 Apr 2011 13:00:38 -0700 (PDT)
Subject: [R-sig-Geo] Problems reading shapefiles in R
In-Reply-To: <alpine.LRH.2.00.1104102108430.2438@reclus.nhh.no>
References: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>
	<916681.29785.qm@web56606.mail.re3.yahoo.com>
	<alpine.LRH.2.00.1104102108430.2438@reclus.nhh.no>
Message-ID: <144223.64029.qm@web56607.mail.re3.yahoo.com>

Mr Bivand, that's not unmotivated advice and I am not?trying to mislead anybody.
That exact thing that Manuel is describing happened to me and it got?fixed by
removing the rgeos package from my library.
?
Felipe D. Carrillo
Supervisory Fishery Biologist
Department of the Interior
US Fish & Wildlife Service
California, USA
http://www.fws.gov/redbluff/rbdd_jsmp.aspx




----- Original Message ----
> From: Roger Bivand <Roger.Bivand at nhh.no>
> To: Felipe Carrillo <mazatlanmexico at yahoo.com>
> Cc: Manuel Sp?nola <mspinola10 at gmail.com>; r-sig-geo at r-project.org
> Sent: Sun, April 10, 2011 12:15:51 PM
> Subject: Re: [R-sig-Geo] Problems reading shapefiles in R
> 
> On Sun, 10 Apr 2011, Felipe Carrillo wrote:
> 
> > Manuel:
> > As a test, delete?the rgeos?package from your library and run?one of the
> > commands again. You can always?install the package back if you need it.
> 
> Please do not give unmotivated advice on the list, as it will mislead. 
> There is no interaction between rgeos and functions for reading vector 
> objects - if you believe otherwise, please report this with a reproducible 
> case. Bad advice is worse than no advice.
> 
> The obvious place to check is:
> 
> list.files(path="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
> ? pattern="AI_BIOTICA_010411_CRTM05")
> 
> which must show at least:
> 
> "AI_BIOTICA_010411_CRTM05.dbf" "AI_BIOTICA_010411_CRTM05.shp" 
> "AI_BIOTICA_010411_CRTM05.shx"
> 
> for "AI_BIOTICA_010411_CRTM05" to be a shapefile. The error messages show 
> that "AI_BIOTICA_010411_CRTM05.dbf" is missing, so please check that it is 
> present first.
> 
> Hope this clarifies,
> 
> Roger
> 
> > ?
> > Felipe D. Carrillo
> > Supervisory Fishery Biologist
> > Department of the Interior
> > US Fish & Wildlife Service
> > California, USA
> > http://www.fws.gov/redbluff/rbdd_jsmp.aspx
> >
> >
> >
> >
> > ----- Original Message ----
> >> From: Manuel Sp?nola <mspinola10 at gmail.com>
> >> To: r-sig-geo at r-project.org
> >> Sent: Sun, April 10, 2011 9:54:20 AM
> >> Subject: [R-sig-Geo] Problems reading shapefiles in R
> >>
> >> Dear list members,
> >>
> >> I am trying to read a shapefile in R using 3 different methods and all of
> >> them failed:
> >>
> >>> ai_biotica <- readOGR(dsn="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
> >> layer="AI_BIOTICA_010411_CRTM05")
> >> Error in ogrInfo(dsn = dsn, layer = layer, input_field_name_encoding =
> >> input_field_name_encoding) :
> >>
> >> ? ? ? ? GDAL Error 4: Unable to open
> >> 
>C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.shx
> >> or
> >>
> > 
>C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.SHX.
> >>
> >>> nc1 <-
> >> 
>readShapePoly("C:/ProyectosRespacial/ICE/Shapefiles/AI_BIOTICA_010411_CRTM05.shp",
>
> >> ,
> >> proj4string=CRS("+proj=tmerc +lon_0=-84 +lat_0=0 +x_0=500000 +k=0.9999
> >> +datum=WGS84"))
> >> Error in read.dbf(filen1) : unable to open DBF file
> >>
> >>
> >>> ai_biotica <- readShapeSpatial("AI_BIOTICA_010411_CRTM05")
> >> Error in read.dbf(filen1) : unable to open DBF file
> >>
> >> I will appreciate any input on the problem.
> >>
> >> Best,
> >>
> >> Manuel Sp?nola
> >>
> >>
> >>
> >> --
> >> *Manuel Sp?nola, Ph.D.*
> >> Instituto Internacional en Conservaci?n y Manejo de Vida Silvestre
> >> Universidad Nacional
> >> Apartado 1350-3000
> >> Heredia
> >> COSTA RICA
> >> mspinola at una.ac.cr
> >> mspinola10 at gmail.com
> >> Tel?fono: (506) 2277-3598
> >> Fax: (506) 2237-7036
> >> Personal website: Lobito de r?o 
<https://sites.google.com/site/lobitoderio/>
> >> Institutional website: ICOMVIS <http://www.icomvis.una.ac.cr/>
> >>
> >> ??? [[alternative HTML version deleted]]
> >>
> >>
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
> 
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> e-mail: Roger.Bivand at nhh.no


From Roger.Bivand at nhh.no  Mon Apr 11 09:54:43 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 11 Apr 2011 09:54:43 +0200 (CEST)
Subject: [R-sig-Geo] Problems reading shapefiles in R
In-Reply-To: <144223.64029.qm@web56607.mail.re3.yahoo.com>
References: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>
	<916681.29785.qm@web56606.mail.re3.yahoo.com>
	<alpine.LRH.2.00.1104102108430.2438@reclus.nhh.no>
	<144223.64029.qm@web56607.mail.re3.yahoo.com>
Message-ID: <alpine.LRH.2.00.1104110936400.4509@reclus.nhh.no>

On Sun, 10 Apr 2011, Felipe Carrillo wrote:

> Mr Bivand, that's not unmotivated advice and I am not?trying to mislead 
> anybody. That exact thing that Manuel is describing happened to me and 
> it got?fixed by removing the rgeos package from my library.

Then please respect the implicit contract in all use of open source 
software! Please inform the maintainer of the packages (me) with a fully 
reproducible case, so that we may benefit from your discovery by 
identifying and removing the cause.

As I stated, both rgeos and maptools are thoroughly tested on multiple 
platforms, so your diagnosis seems unlikely, but the way you repay those 
who write and maintain the software you benefit from is by documenting and 
reporting irregularities.

Until I receive your reproducible case, I will assert that your problem is 
caused by something else, specific to your platform and installation, and 
that your advice is inappropriate with repect to the problem reported by 
the original poster:

library(maptools)
dir <- system.file("shapes", package="maptools")
file.copy(paste(dir, "sids.shp", sep="/"), td)
file.copy(paste(dir, "sids.shx", sep="/"), td)
sids <- readShapeSpatial(paste(td, "sids.shp", sep="/"))
sids <- readShapeSpatial(paste(td, "sids", sep="/"))
sids <- readShapePoly(paste(td, "sids.shp", sep="/"))
library(rgdal)
sids <- readOGR(td, "sids")

(missing DBF file) reproduce the error messages correctly (maptools 
0.8-6). I get a different error message from rgdal (0.6-36) with GDAL 
1.8.0, but as we have neither the output of sessionInfo() from the 
original poster, not the version information printed by rgdal on load, the 
difference may be from a different version of GDAL. Would the original 
poster see different error messages after running update.packages()?

Roger

> ?
> Felipe D. Carrillo
> Supervisory Fishery Biologist
> Department of the Interior
> US Fish & Wildlife Service
> California, USA
> http://www.fws.gov/redbluff/rbdd_jsmp.aspx
>
>
>
>
> ----- Original Message ----
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> To: Felipe Carrillo <mazatlanmexico at yahoo.com>
>> Cc: Manuel Sp?nola <mspinola10 at gmail.com>; r-sig-geo at r-project.org
>> Sent: Sun, April 10, 2011 12:15:51 PM
>> Subject: Re: [R-sig-Geo] Problems reading shapefiles in R
>> 
>> On Sun, 10 Apr 2011, Felipe Carrillo wrote:
>> 
>> > Manuel:
>> > As a test, delete?the rgeos?package from your library and run?one of the
>> > commands again. You can always?install the package back if you need it.
>> 
>> Please do not give unmotivated advice on the list, as it will mislead. 
>> There is no interaction between rgeos and functions for reading vector 
>> objects - if you believe otherwise, please report this with a reproducible 
>> case. Bad advice is worse than no advice.
>> 
>> The obvious place to check is:
>> 
>> list.files(path="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
>> ? pattern="AI_BIOTICA_010411_CRTM05")
>> 
>> which must show at least:
>> 
>> "AI_BIOTICA_010411_CRTM05.dbf" "AI_BIOTICA_010411_CRTM05.shp" 
>> "AI_BIOTICA_010411_CRTM05.shx"
>> 
>> for "AI_BIOTICA_010411_CRTM05" to be a shapefile. The error messages show 
>> that "AI_BIOTICA_010411_CRTM05.dbf" is missing, so please check that it is 
>> present first.
>> 
>> Hope this clarifies,
>> 
>> Roger
>> 
>> > ?
>> > Felipe D. Carrillo
>> > Supervisory Fishery Biologist
>> > Department of the Interior
>> > US Fish & Wildlife Service
>> > California, USA
>> > http://www.fws.gov/redbluff/rbdd_jsmp.aspx
>> >
>> >
>> >
>> >
>> > ----- Original Message ----
>> >> From: Manuel Sp?nola <mspinola10 at gmail.com>
>> >> To: r-sig-geo at r-project.org
>> >> Sent: Sun, April 10, 2011 9:54:20 AM
>> >> Subject: [R-sig-Geo] Problems reading shapefiles in R
>> >>
>> >> Dear list members,
>> >>
>> >> I am trying to read a shapefile in R using 3 different methods and all of
>> >> them failed:
>> >>
>> >>> ai_biotica <- readOGR(dsn="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
>> >> layer="AI_BIOTICA_010411_CRTM05")
>> >> Error in ogrInfo(dsn = dsn, layer = layer, input_field_name_encoding =
>> >> input_field_name_encoding) :
>> >>
>> >> ? ? ? ? GDAL Error 4: Unable to open
>> >> 
>> C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.shx
>> >> or
>> >>
>> > 
>> C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.SHX.
>> >>
>> >>> nc1 <-
>> >> 
>> readShapePoly("C:/ProyectosRespacial/ICE/Shapefiles/AI_BIOTICA_010411_CRTM05.shp",
>>
>> >> ,
>> >> proj4string=CRS("+proj=tmerc +lon_0=-84 +lat_0=0 +x_0=500000 +k=0.9999
>> >> +datum=WGS84"))
>> >> Error in read.dbf(filen1) : unable to open DBF file
>> >>
>> >>
>> >>> ai_biotica <- readShapeSpatial("AI_BIOTICA_010411_CRTM05")
>> >> Error in read.dbf(filen1) : unable to open DBF file
>> >>
>> >> I will appreciate any input on the problem.
>> >>
>> >> Best,
>> >>
>> >> Manuel Sp?nola
>> >>
>> >>
>> >>
>> >> --
>> >> *Manuel Sp?nola, Ph.D.*
>> >> Instituto Internacional en Conservaci?n y Manejo de Vida Silvestre
>> >> Universidad Nacional
>> >> Apartado 1350-3000
>> >> Heredia
>> >> COSTA RICA
>> >> mspinola at una.ac.cr
>> >> mspinola10 at gmail.com
>> >> Tel?fono: (506) 2277-3598
>> >> Fax: (506) 2237-7036
>> >> Personal website: Lobito de r?o 
> <https://sites.google.com/site/lobitoderio/>
>> >> Institutional website: ICOMVIS <http://www.icomvis.una.ac.cr/>
>> >>
>> >> ??? [[alternative HTML version deleted]]
>> >>
>> >>
>> >
>> > _______________________________________________
>> > R-sig-Geo mailing list
>> > R-sig-Geo at r-project.org
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >
>> 
>> -- 
>> Roger Bivand
>> Economic Geography Section, Department of Economics, Norwegian School of
>> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
>> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
>> e-mail: Roger.Bivand at nhh.no
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From Gema.FAviles at uclm.es  Mon Apr 11 12:39:40 2011
From: Gema.FAviles at uclm.es (GEMA FERNANDEZ-AVILES CALDERON)
Date: Mon, 11 Apr 2011 12:39:40 +0200
Subject: [R-sig-Geo] Logit model in R commander
Message-ID: <BFE1525242E7DA4B9CBBDE2CF4DDBBFB9E1F07@EVSAB01.uclm.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110411/86487b01/attachment.pl>

From paul.hiemstra at knmi.nl  Mon Apr 11 16:19:23 2011
From: paul.hiemstra at knmi.nl (Paul Hiemstra)
Date: Mon, 11 Apr 2011 16:19:23 +0200
Subject: [R-sig-Geo] Logit model in R commander
In-Reply-To: <BFE1525242E7DA4B9CBBDE2CF4DDBBFB9E1F07@EVSAB01.uclm.es>
References: <BFE1525242E7DA4B9CBBDE2CF4DDBBFB9E1F07@EVSAB01.uclm.es>
Message-ID: <4DA30DEB.7060303@knmi.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110411/841bd377/attachment.pl>

From anna.cord at gmx.de  Mon Apr 11 19:05:08 2011
From: anna.cord at gmx.de (Anna Cord)
Date: Mon, 11 Apr 2011 19:05:08 +0200
Subject: [R-sig-Geo] resample{raster}: Error "invalid 'y' type in 'x || y'"
Message-ID: <20110411170508.96420@gmx.net>

Dear list,

I am trying to resample a raster layer to another raster layer (dieffernt resolution, different extent) using the "resample" function as implemented in the "raster" package.

----------------------------

1. The raster layer to be resampled:
> test
class       : RasterLayer 
dimensions  : 7272, 12660, 92063520  (nrow, ncol, ncell)
resolution  : 0.0025, 0.0025  (x, y)
extent      : -118.3616, -86.71156, 14.53515, 32.71515  (xmin, xmax, ymin, ymax)
projection  : +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0 
values      : C:/DOKUME~1/Anna/LOKALE~1/Temp/R_raster_tmp/raster_tmp_4863583632.grd 
min value   : 0 
max value   : 1

------------------

2. The reference raster layer:
> land.cover
class       : RasterLayer 
dimensions  : 7059, 12289, 86748051  (nrow, ncol, ncell)
resolution  : 0.002575715, 0.002575715  (x, y)
extent      : -118.3631, -86.71017, 14.53329, 32.71526  (xmin, xmax, ymin, ymax)
projection  : +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0 
values      : F:/R/land_cover/NALCMS_land_cover_2005.tif 
min value   : 0 
max value   : 255

-----------------

The following error occurs:

out <- resample(test, land.cover, method='bilinear')
"Error in resample(test, land.cover, method = "bilinear") : 
  unused argument(s) (method = "bilinear")"

out <- resample(test, land.cover)
Error in missing(size) || size == 1 : invalid 'y' type in 'x || y'

---------------

Interestingly, the same happens when using the example as provided in the package description:

r <- raster(nrow=3, ncol=3)
r[] <- 1:ncell(r)
s <- raster(nrow=10, ncol=10)
s <- resample(r, s, method='bilinear')
Error in resample(r, s, method = "bilinear") : 
  unused argument(s) (method = "bilinear")

s <- resample(r, s)
Error in missing(size) || size == 1 : invalid 'y' type in 'x || y'

Any ideas? I couldn't found any similar thread...

Thanks a lot in advance,
Anna




--


From andy.a.wilson at gmail.com  Mon Apr 11 19:16:12 2011
From: andy.a.wilson at gmail.com (Andy Wilson)
Date: Mon, 11 Apr 2011 18:16:12 +0100
Subject: [R-sig-Geo] focalFilter function in raster
Message-ID: <4DA3375C.3000308@googlemail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110411/d3ef576f/attachment.pl>

From mspinola10 at gmail.com  Mon Apr 11 22:33:00 2011
From: mspinola10 at gmail.com (=?ISO-8859-1?Q?Manuel_Sp=EDnola?=)
Date: Mon, 11 Apr 2011 14:33:00 -0600
Subject: [R-sig-Geo] Shapefile to raster and crop a raster (error)
Message-ID: <BANLkTi=g8mwU+7vTkcH1B7wCFPvFEwxJjw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110411/283e5be9/attachment.pl>

From mazatlanmexico at yahoo.com  Tue Apr 12 00:19:18 2011
From: mazatlanmexico at yahoo.com (Felipe Carrillo)
Date: Mon, 11 Apr 2011 15:19:18 -0700 (PDT)
Subject: [R-sig-Geo] Problems reading shapefiles in R
In-Reply-To: <alpine.LRH.2.00.1104110936400.4509@reclus.nhh.no>
References: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>
	<916681.29785.qm@web56606.mail.re3.yahoo.com>
	<alpine.LRH.2.00.1104102108430.2438@reclus.nhh.no>
	<144223.64029.qm@web56607.mail.re3.yahoo.com>
	<alpine.LRH.2.00.1104110936400.4509@reclus.nhh.no>
Message-ID: <912977.91719.qm@web56604.mail.re3.yahoo.com>

So far I have only used your package(s)?once or twice to help somebody with a 
question.?My type of work doesn't require the use of that software but if 
sometime I am required to use it, I will take the time to report 
bugs?if?found.?Right now, I am too busy to make a reproducible example for you 
but again, coincidence or not the error messages dissapeared when I removed?that 
package from my library.?This list might be diffrent than the R list because I 
see this kind of advice there without complaints.?I will try to watch what I 
suggest from now on..Good job with you packages though.
?
Felipe D. Carrillo
Supervisory Fishery Biologist
Department of the Interior
US Fish & Wildlife Service
California, USA
http://www.fws.gov/redbluff/rbdd_jsmp.aspx




----- Original Message ----
> From: Roger Bivand <Roger.Bivand at nhh.no>
> To: Felipe Carrillo <mazatlanmexico at yahoo.com>
> Cc: Manuel Sp?nola <mspinola10 at gmail.com>; r-sig-geo at r-project.org
> Sent: Mon, April 11, 2011 12:54:43 AM
> Subject: Re: [R-sig-Geo] Problems reading shapefiles in R
> 
> On Sun, 10 Apr 2011, Felipe Carrillo wrote:
> 
> > Mr Bivand, that's not unmotivated advice and I am not?trying to mislead 
>anybody. That exact thing that Manuel is describing happened to me and it 
>got?fixed by removing the rgeos package from my library.
> 
> Then please respect the implicit contract in all use of open source software! 
>Please inform the maintainer of the packages (me) with a fully reproducible 
>case, so that we may benefit from your discovery by identifying and removing the 
>cause.
> 
> As I stated, both rgeos and maptools are thoroughly tested on multiple 
>platforms, so your diagnosis seems unlikely, but the way you repay those who 
>write and maintain the software you benefit from is by documenting and reporting 
>irregularities.
> 
> Until I receive your reproducible case, I will assert that your problem is 
>caused by something else, specific to your platform and installation, and that 
>your advice is inappropriate with repect to the problem reported by the original 
>poster:
> 
> library(maptools)
> dir <- system.file("shapes", package="maptools")
> file.copy(paste(dir, "sids.shp", sep="/"), td)
> file.copy(paste(dir, "sids.shx", sep="/"), td)
> sids <- readShapeSpatial(paste(td, "sids.shp", sep="/"))
> sids <- readShapeSpatial(paste(td, "sids", sep="/"))
> sids <- readShapePoly(paste(td, "sids.shp", sep="/"))
> library(rgdal)
> sids <- readOGR(td, "sids")
> 
> (missing DBF file) reproduce the error messages correctly (maptools 0.8-6). I 
>get a different error message from rgdal (0.6-36) with GDAL 1.8.0, but as we 
>have neither the output of sessionInfo() from the original poster, not the 
>version information printed by rgdal on load, the difference may be from a 
>different version of GDAL. Would the original poster see different error 
>messages after running update.packages()?
> 
> Roger
> 
> > ?
> > Felipe D. Carrillo
> > Supervisory Fishery Biologist
> > Department of the Interior
> > US Fish & Wildlife Service
> > California, USA
> > http://www.fws.gov/redbluff/rbdd_jsmp.aspx
> > 
> > 
> > 
> > 
> > ----- Original Message ----
> >> From: Roger Bivand <Roger.Bivand at nhh.no>
> >> To: Felipe Carrillo <mazatlanmexico at yahoo.com>
> >> Cc: Manuel Sp?nola <mspinola10 at gmail.com>; r-sig-geo at r-project.org
> >> Sent: Sun, April 10, 2011 12:15:51 PM
> >> Subject: Re: [R-sig-Geo] Problems reading shapefiles in R
> >> 
> >> On Sun, 10 Apr 2011, Felipe Carrillo wrote:
> >> 
> >> > Manuel:
> >> > As a test, delete?the rgeos?package from your library and run?one of the
> >> > commands again. You can always?install the package back if you need it.
> >> 
> >> Please do not give unmotivated advice on the list, as it will mislead. There 
>is no interaction between rgeos and functions for reading vector objects - if 
>you believe otherwise, please report this with a reproducible case. Bad advice 
>is worse than no advice.
> >> 
> >> The obvious place to check is:
> >> 
> >> list.files(path="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
> >> ? pattern="AI_BIOTICA_010411_CRTM05")
> >> 
> >> which must show at least:
> >> 
> >> "AI_BIOTICA_010411_CRTM05.dbf" "AI_BIOTICA_010411_CRTM05.shp" 
>"AI_BIOTICA_010411_CRTM05.shx"
> >> 
> >> for "AI_BIOTICA_010411_CRTM05" to be a shapefile. The error messages show 
>that "AI_BIOTICA_010411_CRTM05.dbf" is missing, so please check that it is 
>present first.
> >> 
> >> Hope this clarifies,
> >> 
> >> Roger
> >> 
> >> > ?
> >> > Felipe D. Carrillo
> >> > Supervisory Fishery Biologist
> >> > Department of the Interior
> >> > US Fish & Wildlife Service
> >> > California, USA
> >> > http://www.fws.gov/redbluff/rbdd_jsmp.aspx
> >> >
> >> >
> >> >
> >> >
> >> > ----- Original Message ----
> >> >> From: Manuel Sp?nola <mspinola10 at gmail.com>
> >> >> To: r-sig-geo at r-project.org
> >> >> Sent: Sun, April 10, 2011 9:54:20 AM
> >> >> Subject: [R-sig-Geo] Problems reading shapefiles in R
> >> >>
> >> >> Dear list members,
> >> >>
> >> >> I am trying to read a shapefile in R using 3 different methods and all 
of
> >> >> them failed:
> >> >>
> >> >>> ai_biotica <- readOGR(dsn="C:/ProyectosRespacial/ICE/SIG_Biotica_PHED",
> >> >> layer="AI_BIOTICA_010411_CRTM05")
> >> >> Error in ogrInfo(dsn = dsn, layer = layer, input_field_name_encoding =
> >> >> input_field_name_encoding) :
> >> >>
> >> >> ? ? ? ? GDAL Error 4: Unable to open
> >> >> 
>C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.shx
> >> >> or
> >> >>
> >> > 
>C:/ProyectosRespacial/ICE/SIG_Biotica_PHED\carretera_interamericana_CRTM05.SHX.
> >> >>
> >> >>> nc1 <-
> >> >> 
>readShapePoly("C:/ProyectosRespacial/ICE/Shapefiles/AI_BIOTICA_010411_CRTM05.shp",
>
> >> 
> >> >> ,
> >> >> proj4string=CRS("+proj=tmerc +lon_0=-84 +lat_0=0 +x_0=500000 +k=0.9999
> >> >> +datum=WGS84"))
> >> >> Error in read.dbf(filen1) : unable to open DBF file
> >> >>
> >> >>
> >> >>> ai_biotica <- readShapeSpatial("AI_BIOTICA_010411_CRTM05")
> >> >> Error in read.dbf(filen1) : unable to open DBF file
> >> >>
> >> >> I will appreciate any input on the problem.
> >> >>
> >> >> Best,
> >> >>
> >> >> Manuel Sp?nola
> >> >>
> >> >>
> >> >>
> >> >> --
> >> >> *Manuel Sp?nola, Ph.D.*
> >> >> Instituto Internacional en Conservaci?n y Manejo de Vida Silvestre
> >> >> Universidad Nacional
> >> >> Apartado 1350-3000
> >> >> Heredia
> >> >> COSTA RICA
> >> >> mspinola at una.ac.cr
> >> >> mspinola10 at gmail.com
> >> >> Tel?fono: (506) 2277-3598
> >> >> Fax: (506) 2237-7036
> >> >> Personal website: Lobito de r?o 
> > <https://sites.google.com/site/lobitoderio/>
> >> >> Institutional website: ICOMVIS <http://www.icomvis.una.ac.cr/>
> >> >>
> >> >> ??? [[alternative HTML version deleted]]
> >> >>
> >> >>
> >> >
> >> > _______________________________________________
> >> > R-sig-Geo mailing list
> >> > R-sig-Geo at r-project.org
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >> >
> >> 
> >> -- Roger Bivand
> >> Economic Geography Section, Department of Economics, Norwegian School of
> >> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> >> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> >> e-mail: Roger.Bivand at nhh.no
> > 
> 
> -- Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> e-mail: Roger.Bivand at nhh.no
>


From Adrian.Baddeley at csiro.au  Tue Apr 12 03:41:17 2011
From: Adrian.Baddeley at csiro.au (Adrian.Baddeley at csiro.au)
Date: Tue, 12 Apr 2011 09:41:17 +0800
Subject: [R-sig-Geo] nndist R vs. ArcGIS
In-Reply-To: <4DA39F71.4040204@xtra.co.nz>
References: <1302536998488-3442375.post@n4.nabble.com>
	<BANLkTikgNdbQPsed-fp7XyFaQcJJ8OtBbw@mail.gmail.com>
	<4DA39F71.4040204@xtra.co.nz>
Message-ID: <57DC18C299094D4299F837570C5DF1C536504ABDBD@EXWA-MBX01.nexus.csiro.au>

This topic arose on R-help:

smoluka<smoluka at geo.oregonstate.edu>  writes:
>> Can anyone tell me why I would get different average nearest neighbor values
>> for the same set of coordinates between ArcGIS 10 and R? Sometimes the
>> difference in distance is over 1.3 km.

There are several possible explanations including the treatment of zero distances and duplicated points (i.e. two points at the same location), the handling of edge effects, numerical effects due to the scale of the data, and numerical rounding when the data were transferred between packages.

You don't say which functions/libraries/packages you are using in R. An example would help.

Since you mention 'nndist' I guess you were using the 'spatstat' library. The spatstat function 'nndist' calculates nearest-neighbour distances WITHOUT edge correction. If there are two (x,y) points at the same location, they are deemed to have distance zero, and will be counted as nearest neighbours of each other by nndist. You could check whether this is true for ArcGIS.

[the spatstat function 'Gest' calculates an edge-corrected cumulative distribution function of the nearest neighbour distances]. 

You say that the discrepancies in mean n.n. distance were as large as 1.3 km. What was the overall scale of the data? Hundreds of km? And were the coordinates recorded in metres? If so, then the discrepancy could be due to numerical effects. Firstly the coordinates could have been rounded, either when transferring the data between packages, or at some point in the calculation. Secondly, when distances are calculated in C code, depending on how they are handled, there may or may not be rounding effects (this is "GCC bug #323" although it's not really a bug). They would usually be quite small but could be magnified when you take the mean of a large number of values. 

Since 1.3 is of the same order as sqrt(2), I'll bet $10 that the data were rounded to the nearest 1000 metres. (But if I lose, you have to visit Australia to collect :-)

An example would help.

Adrian Baddeley


From r.hijmans at gmail.com  Tue Apr 12 05:13:17 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Mon, 11 Apr 2011 20:13:17 -0700 (PDT)
Subject: [R-sig-Geo] focalFilter problem raster package
In-Reply-To: <BANLkTim=Zwo8czGbyNrWXxLd5_qAt7Mrfg@mail.gmail.com>
References: <BANLkTi=TZZg1CnCqw4ZDb3u7NUUPV_TtkQ@mail.gmail.com>
	<1301980998310-6241121.post@n2.nabble.com>
	<BANLkTim=Zwo8czGbyNrWXxLd5_qAt7Mrfg@mail.gmail.com>
Message-ID: <1302577997688-6263699.post@n2.nabble.com>

> Dear Robert 
>
> I hope the following example is clear.  you should be able to cut and
> paste... 
>
> Nick 

Dear Nick, 

Thank you for the example. It reveals a serious error in focalFilter.
Results with that function in raster versions lower than 1.8-11 are wrong in
the first rows (half the number of rows of the filter), when using a filter
larger than 3x3 (because I forgot a "byrow=TRUE" argument in a call to
matrix). Version 1.8-11 is on its way to CRAN.

Robert



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/focalFilter-problem-raster-package-tp6235925p6263699.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From r.hijmans at gmail.com  Tue Apr 12 05:19:13 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Mon, 11 Apr 2011 20:19:13 -0700 (PDT)
Subject: [R-sig-Geo] focalFilter function in raster
In-Reply-To: <4DA3375C.3000308@googlemail.com>
References: <4DA3375C.3000308@googlemail.com>
Message-ID: <1302578353089-6263710.post@n2.nabble.com>


Dear Andy, 

Not your mistake, but mine. Like in Nick's case a few days ago, this reveals
a serious error in focalFilter. Results with that function in raster
versions lower than 1.8-12 (I said 1.8-11 in a previous message) are wrong
in the first rows (half the number of rows of the filter), when using a
filter larger than 3x3. Version 1.8-12 is on its way to CRAN. 

Robert 


> Hi all, 
> I'm using the focalFilter function in the raster package to 
> process a raster DEM. I've cropped the data back to a tiny area to make 
> it easier to see. I am expecting focalFilter to apply a matrix of 
> weights (in this case 1s in a 5x5 grid) to the cells surrounding each 
> cell, and then evaluate the supplied function for the result i.e in the 
> first case below simply the max value of all the cells in a 5x5 grid 
> centred on each cell in turn. This works as expected. 
>
> However the "min" function doesnt - the value 63.4079 is in the extreme 
> top-right corner of the raster, so only falls within the 5x5 grid around 
> the 16th cell (top-right of the 4 evaluated) yet this value is also 
> evaluated as the result for the other 3 cells as well, despite not 
> falling within the 5x5 matrix around these cells. Min works with 3x3 but 
> not 5x5 and larger. This is only for illustration not something I'm 
> actually want to do - I want to apply "median" with a 27x27 grid, and I 
> also get the same problem here but the data for that's not so obvious ;-) 
>
> Can someone please point out my obvious mistake? 
> Cheers 
> Andy 



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/focalFilter-function-in-raster-tp6262205p6263710.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From r.hijmans at gmail.com  Tue Apr 12 06:46:43 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Mon, 11 Apr 2011 21:46:43 -0700 (PDT)
Subject: [R-sig-Geo] Shapefile to raster and crop a raster (error)
In-Reply-To: <BANLkTi=g8mwU+7vTkcH1B7wCFPvFEwxJjw@mail.gmail.com>
References: <BANLkTi=g8mwU+7vTkcH1B7wCFPvFEwxJjw@mail.gmail.com>
Message-ID: <1302583603381-6263823.post@n2.nabble.com>


Manuel, 

> bio1ai <- crop(bio1, ai) 
> Error in intersectExtent(x, y) : Invalid extent 

This somewhat cryptic error message means that the extent of y does not
overlap with the extent of x. Hence y cannot be used to crop x. 

extent(bio1)
extent(ai)

Perhaps this happens because bio1 is in lon/lat while ai is in UTM or
another projection. 

Robert



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Shapefile-to-raster-and-crop-a-raster-error-tp6262811p6263823.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From pierre.roudier at gmail.com  Tue Apr 12 09:21:22 2011
From: pierre.roudier at gmail.com (Pierre Roudier)
Date: Tue, 12 Apr 2011 17:21:22 +1000
Subject: [R-sig-Geo] resample{raster}: Error "invalid 'y' type in 'x ||
	y'"
In-Reply-To: <20110411170508.96420@gmx.net>
References: <20110411170508.96420@gmx.net>
Message-ID: <BANLkTikejYkU2edzdQSaKMCaUp_=Wkvtkg@mail.gmail.com>

Anna,

What is the version of raster you are using?

Best way to figure out is to post the output of the sessionInfo() command.

Pierre

2011/4/12 Anna Cord <anna.cord at gmx.de>:
> Dear list,
>
> I am trying to resample a raster layer to another raster layer (dieffernt resolution, different extent) using the "resample" function as implemented in the "raster" package.
>
> ----------------------------
>
> 1. The raster layer to be resampled:
>> test
> class ? ? ? : RasterLayer
> dimensions ?: 7272, 12660, 92063520 ?(nrow, ncol, ncell)
> resolution ?: 0.0025, 0.0025 ?(x, y)
> extent ? ? ?: -118.3616, -86.71156, 14.53515, 32.71515 ?(xmin, xmax, ymin, ymax)
> projection ?: +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0
> values ? ? ?: C:/DOKUME~1/Anna/LOKALE~1/Temp/R_raster_tmp/raster_tmp_4863583632.grd
> min value ? : 0
> max value ? : 1
>
> ------------------
>
> 2. The reference raster layer:
>> land.cover
> class ? ? ? : RasterLayer
> dimensions ?: 7059, 12289, 86748051 ?(nrow, ncol, ncell)
> resolution ?: 0.002575715, 0.002575715 ?(x, y)
> extent ? ? ?: -118.3631, -86.71017, 14.53329, 32.71526 ?(xmin, xmax, ymin, ymax)
> projection ?: +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0
> values ? ? ?: F:/R/land_cover/NALCMS_land_cover_2005.tif
> min value ? : 0
> max value ? : 255
>
> -----------------
>
> The following error occurs:
>
> out <- resample(test, land.cover, method='bilinear')
> "Error in resample(test, land.cover, method = "bilinear") :
> ?unused argument(s) (method = "bilinear")"
>
> out <- resample(test, land.cover)
> Error in missing(size) || size == 1 : invalid 'y' type in 'x || y'
>
> ---------------
>
> Interestingly, the same happens when using the example as provided in the package description:
>
> r <- raster(nrow=3, ncol=3)
> r[] <- 1:ncell(r)
> s <- raster(nrow=10, ncol=10)
> s <- resample(r, s, method='bilinear')
> Error in resample(r, s, method = "bilinear") :
> ?unused argument(s) (method = "bilinear")
>
> s <- resample(r, s)
> Error in missing(size) || size == 1 : invalid 'y' type in 'x || y'
>
> Any ideas? I couldn't found any similar thread...
>
> Thanks a lot in advance,
> Anna
>
>
>
>
> --
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Scientist
Landcare Research, New Zealand


From r.hijmans at gmail.com  Tue Apr 12 09:22:45 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Tue, 12 Apr 2011 00:22:45 -0700 (PDT)
Subject: [R-sig-Geo] resample{raster}: Error "invalid 'y' type in 'x ||
	y'"
In-Reply-To: <20110411170508.96420@gmx.net>
References: <20110411170508.96420@gmx.net>
Message-ID: <1302592965167-6264101.post@n2.nabble.com>

>I am trying to resample a raster layer to another raster layer (dieffernt
resolution, different extent) using >the "resample" function as implemented
in the "raster" package. 
> (...)
>Interestingly, the same happens when using the example as provided in the
package description: 
> (...)
> s <- resample(r, s, method='bilinear') 
> Error in resample(r, s, method = "bilinear") : 
>   unused argument(s) (method = "bilinear") 

Anna,

This suggests that you have loaded another package that also has a function
called resample and that hides the function in 'raster'. 

This will probably work:

s <- raster::resample(r, s, method='bilinear') 

Robert



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/resample-raster-Error-invalid-y-type-in-x-y-tp6262164p6264101.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From anna.cord at gmx.de  Tue Apr 12 09:56:02 2011
From: anna.cord at gmx.de (Anna Cord)
Date: Tue, 12 Apr 2011 09:56:02 +0200
Subject: [R-sig-Geo] resample{raster}: Error "invalid 'y' type in 'x ||
 y'"
In-Reply-To: <1302592965167-6264101.post@n2.nabble.com>
References: <20110411170508.96420@gmx.net>
	<1302592965167-6264101.post@n2.nabble.com>
Message-ID: <4DA40592.2060708@gmx.de>

Many thanks for the hint, Robert!

s<- raster::resample(r, s, method='bilinear')
works absolutely fine.

Anna



Am 4/12/2011 9:22 AM, schrieb Robert Hijmans:
>> I am trying to resample a raster layer to another raster layer (dieffernt
> resolution, different extent) using>the "resample" function as implemented
> in the "raster" package.
>> (...)
>> Interestingly, the same happens when using the example as provided in the
> package description:
>> (...)
>> s<- resample(r, s, method='bilinear')
>> Error in resample(r, s, method = "bilinear") :
>>    unused argument(s) (method = "bilinear")
> Anna,
>
> This suggests that you have loaded another package that also has a function
> called resample and that hides the function in 'raster'.
>
> This will probably work:
>
> s<- raster::resample(r, s, method='bilinear')
>
> Robert
>
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/resample-raster-Error-invalid-y-type-in-x-y-tp6262164p6264101.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From nkkroy3 at gmail.com  Tue Apr 12 12:41:18 2011
From: nkkroy3 at gmail.com (Nikki roy)
Date: Tue, 12 Apr 2011 12:41:18 +0200
Subject: [R-sig-Geo] how to complete this equation (interpolation)
Message-ID: <BANLkTi=sjuGOriDDLgsrV-FurTEeEJQ4yQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110412/89134c26/attachment.pl>

From tom.gottfried at wzw.tum.de  Tue Apr 12 13:32:18 2011
From: tom.gottfried at wzw.tum.de (Tom Gottfried)
Date: Tue, 12 Apr 2011 13:32:18 +0200
Subject: [R-sig-Geo] memory allocation problem with small PostGIS-layer in
	readOGR()
Message-ID: <4DA43842.7000701@wzw.tum.de>

Dear list,

since some upgrades of R-packages and GDAL (I'm sorry I can not exactly reproduce which) I got 
memory allocation problems reading a relatively small PostGIS layer (type POLYGON with 21 polygons):

 > library(rgdal)
Loading required package: sp
Geospatial Data Abstraction Library extensions to R successfully loaded
Loaded GDAL runtime: GDAL 1.8.0, released 2011/01/12
Path to GDAL shared files:
Loaded PROJ.4 runtime: Rel. 4.7.1, 23 September 2009
Path to PROJ.4 shared files: (autodetected)

 > readOGR("PG:dbname=myDB", "layer(the_geom)")
Error: cannot allocate vector of size 13.9 Gb

 > sessionInfo()
R version 2.12.2 Patched (2011-03-18 r55401)
Platform: x86_64-unknown-linux-gnu (64-bit)

locale:
  [1] LC_CTYPE=en_US       LC_NUMERIC=C         LC_TIME=en_US
  [4] LC_COLLATE=en_US     LC_MONETARY=C        LC_MESSAGES=en_US
  [7] LC_PAPER=en_US       LC_NAME=C            LC_ADDRESS=C
[10] LC_TELEPHONE=C       LC_MEASUREMENT=en_US LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     methods   base

other attached packages:
[1] rgdal_0.6-33 sp_0.9-79    RODBC_1.3-2

loaded via a namespace (and not attached):
[1] grid_2.12.2     lattice_0.19-17

Any help is appreciated!
Thanks,
Tom

-- 
Technische Universit?t M?nchen
Department f?r Pflanzenwissenschaften
Lehrstuhl f?r Gr?nlandlehre
Alte Akademie 12
85350 Freising / Germany
Phone: ++49 (0)8161 715324
Fax:   ++49 (0)8161 713243
email: tom.gottfried at wzw.tum.de
http://www.wzw.tum.de/gruenland


From piero.campa at gmail.com  Tue Apr 12 15:04:58 2011
From: piero.campa at gmail.com (piero campa)
Date: Tue, 12 Apr 2011 06:04:58 -0700 (PDT)
Subject: [R-sig-Geo] sp::overlay problem
Message-ID: <1302613497958-6265018.post@n2.nabble.com>

Dear list,
I have a SpatialGridDataFrame, whose SRS is EPSG:32632:
"+init=epsg:32632 +proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units=m
+no_defs +towgs84=0,0,0"

I am willing to overlay these grid values over point locations defined in a
SpatialPixelsDataFrame with a different SRS using the overlay method
(overlaying grids over points in the same SRS gave me no problem until now).

Now instead I need to transform the point locations to EPSG:32632 and then
overlay: 
> my.ov <- overlay(my.grid, spTransform(mySpatialPixels,
> CRS=CRS(proj4string(my.grid))))
[looking at methods?sp::overlay, transforming the grid to the points SRS
would coerce the grid itself into points, and then the overlay method would
not be useful anymore]

But then the output "my.ov" is filled with NAs only.

I checked the bbox of the overlay inputs, and they overlap:
> bbox(my.grid)
              min     max
coords.x1  513000  911000
coords.x2 4844000 5002000
> bbox(mySpatialPixels)
        min       max
x  499880.9  822436.4
y 4816908.3 5045694.8

Any hint is appreciated.
Thank you guys.
/Piero

http://r-sig-geo.2731867.n2.nabble.com/file/n6265018/overlay.png 

--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/sp-overlay-problem-tp6265018p6265018.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From mspinola10 at gmail.com  Tue Apr 12 15:16:59 2011
From: mspinola10 at gmail.com (=?ISO-8859-1?Q?Manuel_Sp=EDnola?=)
Date: Tue, 12 Apr 2011 07:16:59 -0600
Subject: [R-sig-Geo] Shapefile to raster and crop a raster (error)
In-Reply-To: <1302583603381-6263823.post@n2.nabble.com>
References: <BANLkTi=g8mwU+7vTkcH1B7wCFPvFEwxJjw@mail.gmail.com>
	<1302583603381-6263823.post@n2.nabble.com>
Message-ID: <BANLkTimL3v7bQm7D65wyakUKOENXTMG+MQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110412/1fa41f31/attachment.pl>

From Roger.Bivand at nhh.no  Tue Apr 12 15:39:57 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 12 Apr 2011 15:39:57 +0200
Subject: [R-sig-Geo] memory allocation problem with small PostGIS-layer
 in readOGR()
In-Reply-To: <4DA43842.7000701@wzw.tum.de>
References: <4DA43842.7000701@wzw.tum.de>
Message-ID: <alpine.LRH.2.00.1104121530050.12743@reclus.nhh.no>

On Tue, 12 Apr 2011, Tom Gottfried wrote:

> Dear list,
>
> since some upgrades of R-packages and GDAL (I'm sorry I can not exactly 
> reproduce which) I got memory allocation problems reading a relatively small 
> PostGIS layer (type POLYGON with 21 polygons):
>
>> library(rgdal)
> Loading required package: sp
> Geospatial Data Abstraction Library extensions to R successfully loaded
> Loaded GDAL runtime: GDAL 1.8.0, released 2011/01/12
> Path to GDAL shared files:
> Loaded PROJ.4 runtime: Rel. 4.7.1, 23 September 2009
> Path to PROJ.4 shared files: (autodetected)
>
>> readOGR("PG:dbname=myDB", "layer(the_geom)")
> Error: cannot allocate vector of size 13.9 Gb

Since this has to be debugged on your system, please first use ogr2ogr to 
convert it to another format - this will test the PostGIS driver. You are 
on Linux, so should have ogr2ogr installed. If (unlikely) there are 
problems with the driver, you may need to check how PostgreSQL, PostGIS, 
and their dependencies were installed, then GDAL and its dependencies.

Otherwise, this must be in readOGR() somewhere, so do debug(readOGR) and 
run the same command step-by-step. Start by doing debug(ogrInfo) and 
ogrInfo("PG:dbname=myDB", "layer(the_geom)"), because unless you have set 
verbose=FALSE and not reported it (unlikely), ogrInfo() called within 
readOGR() should have reported the DSN and layer names, the driver name, 
etc., before the error if the error occurred later Also look at 
traceback() after the error before trying debug() - this feels like 
ogrInfo() having trouble.

Roger

>
>> sessionInfo()
> R version 2.12.2 Patched (2011-03-18 r55401)
> Platform: x86_64-unknown-linux-gnu (64-bit)
>
> locale:
> [1] LC_CTYPE=en_US       LC_NUMERIC=C         LC_TIME=en_US
> [4] LC_COLLATE=en_US     LC_MONETARY=C        LC_MESSAGES=en_US
> [7] LC_PAPER=en_US       LC_NAME=C            LC_ADDRESS=C
> [10] LC_TELEPHONE=C       LC_MEASUREMENT=en_US LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     methods   base
>
> other attached packages:
> [1] rgdal_0.6-33 sp_0.9-79    RODBC_1.3-2
>
> loaded via a namespace (and not attached):
> [1] grid_2.12.2     lattice_0.19-17
>
> Any help is appreciated!
> Thanks,
> Tom
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From momadou at yahoo.fr  Tue Apr 12 16:18:01 2011
From: momadou at yahoo.fr (Komine)
Date: Tue, 12 Apr 2011 07:18:01 -0700 (PDT)
Subject: [R-sig-Geo] =?utf-8?b?VGVzdMKgZGXCoEhvc21lcsKgZXTCoExlbWVzaG93?=
	=?utf-8?b?wqA=?=
Message-ID: <1302617881535-6265318.post@n2.nabble.com>

 Dear, 
Please, I want an advice.
1-	I want to do Hosmer and Lemeshow test with R software.  My code is the
follow:
HLgof.test (fit = fitted (logisticmodel), obs=Fire$Ignition)
2-	My result is: 
$C
        Hosmer-Lemeshow C statistic

data:  fitted(logisticmodel) and Fire$Ignition 
X-squared = 2.7026, df = 8, p-value = 0.9516
$H
        Hosmer-Lemeshow H statistic

data:  fitted(logisticmodel) and Fire$Ignition 
X-squared = 11.6102, df = 8, p-value = 0.1695 

I don?t understand, why I have two  P.values  and what p.value I will use in
my text? One or both.
Thanks very much 



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Test-de-Hosmer-et-Lemeshow-tp6265318p6265318.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From saldanha.plangeo at gmail.com  Tue Apr 12 16:21:38 2011
From: saldanha.plangeo at gmail.com (Raphael Saldanha)
Date: Tue, 12 Apr 2011 11:21:38 -0300
Subject: [R-sig-Geo] Test de Hosmer et Lemeshow
Message-ID: <BANLkTinwM+zSPYSNgMH+E7O5UN1mRu=nSA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110412/93b45435/attachment.pl>

From tom.gottfried at wzw.tum.de  Tue Apr 12 16:54:30 2011
From: tom.gottfried at wzw.tum.de (Tom Gottfried)
Date: Tue, 12 Apr 2011 16:54:30 +0200
Subject: [R-sig-Geo] memory allocation problem with small PostGIS-layer
 in readOGR()
Message-ID: <4DA467A6.5030208@wzw.tum.de>

Hi Roger,

Am 12.04.2011 15:39, schrieb Roger Bivand:
> On Tue, 12 Apr 2011, Tom Gottfried wrote:
>
>> Dear list,
>>
>> since some upgrades of R-packages and GDAL (I'm sorry I can not exactly reproduce which) I got
>> memory allocation problems reading a relatively small PostGIS layer (type POLYGON with 21 polygons):
>>
>>> library(rgdal)
>> Loading required package: sp
>> Geospatial Data Abstraction Library extensions to R successfully loaded
>> Loaded GDAL runtime: GDAL 1.8.0, released 2011/01/12
>> Path to GDAL shared files:
>> Loaded PROJ.4 runtime: Rel. 4.7.1, 23 September 2009
>> Path to PROJ.4 shared files: (autodetected)
>>
>>> readOGR("PG:dbname=myDB", "layer(the_geom)")
>> Error: cannot allocate vector of size 13.9 Gb
>
> Since this has to be debugged on your system, please first use ogr2ogr to convert it to another
> format - this will test the PostGIS driver. You are on Linux, so should have ogr2ogr installed. If
> (unlikely) there are problems with the driver, you may need to check how PostgreSQL, PostGIS, and
> their dependencies were installed, then GDAL and its dependencies.

converting to ESRI-Shapefile with ogr2ogr worked.

> Otherwise, this must be in readOGR() somewhere, so do debug(readOGR) and run the same command
> step-by-step. Start by doing debug(ogrInfo) and ogrInfo("PG:dbname=myDB", "layer(the_geom)"),
> because unless you have set verbose=FALSE and not reported it (unlikely), ogrInfo() called within
> readOGR() should have reported the DSN and layer names, the driver name, etc., before the error if
> the error occurred later Also look at traceback() after the error before trying debug() - this feels
> like ogrInfo() having trouble.

> ogrInfo("PG:dbname=gs_gk_workspace", "weidengs(the_geom)")
Error: cannot allocate vector of size 13.9 Gb
> traceback()
2: .Call("ogrInfo", as.character(dsn), as.character(layer), PACKAGE = "rgdal")
1: ogrInfo("PG:dbname=gs_gk_workspace", "weidengs(the_geom)")

Now entering C-level (?). I think I first have to read this
http://cran.r-project.org/doc/manuals/R-exts.html#Debugging
before continuing.

Thanks!
Tom

>
> Roger
>
>>
>>> sessionInfo()
>> R version 2.12.2 Patched (2011-03-18 r55401)
>> Platform: x86_64-unknown-linux-gnu (64-bit)
>>
>> locale:
>> [1] LC_CTYPE=en_US LC_NUMERIC=C LC_TIME=en_US
>> [4] LC_COLLATE=en_US LC_MONETARY=C LC_MESSAGES=en_US
>> [7] LC_PAPER=en_US LC_NAME=C LC_ADDRESS=C
>> [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] stats graphics grDevices utils methods base
>>
>> other attached packages:
>> [1] rgdal_0.6-33 sp_0.9-79 RODBC_1.3-2
>>
>> loaded via a namespace (and not attached):
>> [1] grid_2.12.2 lattice_0.19-17
>>
>> Any help is appreciated!
>> Thanks,
>> Tom
>>
>>
>

-- 
Technische Universit?t M?nchen
Department f?r Pflanzenwissenschaften
Lehrstuhl f?r Gr?nlandlehre
Alte Akademie 12
85350 Freising / Germany
Phone: ++49 (0)8161 715324
Fax:   ++49 (0)8161 713243
email: tom.gottfried at wzw.tum.de
http://www.wzw.tum.de/gruenland


From sa.cizmeli at usherbrooke.ca  Tue Apr 12 17:20:01 2011
From: sa.cizmeli at usherbrooke.ca (scizmeli)
Date: Tue, 12 Apr 2011 08:20:01 -0700 (PDT)
Subject: [R-sig-Geo] spacetime : the challenge of image time
	series	containing holes
In-Reply-To: <4DA08BFE.9070302@wzw.tum.de>
References: <4D9FA7CE.2000108@usherbrooke.ca> <4DA08BFE.9070302@wzw.tum.de>
Message-ID: <1302621601781-6265557.post@n2.nabble.com>

Hi Tom,

Thanks a million for your fast, accurate reply. I did not apply that to my
imagery yet but the test I did show that it seems to work...

thank you!
Servet

--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/spacetime-the-challenge-of-image-time-series-containing-holes-tp6255766p6265557.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From jh.eco.cas at gmail.com  Tue Apr 12 18:11:29 2011
From: jh.eco.cas at gmail.com (Jianhua Huang)
Date: Tue, 12 Apr 2011 12:11:29 -0400
Subject: [R-sig-Geo] get the centroids of the polygones
Message-ID: <4da479ad.23aa340a.7f12.330d@mx.google.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110412/805f9019/attachment.pl>

From yud at mail.montclair.edu  Tue Apr 12 18:35:41 2011
From: yud at mail.montclair.edu (Danlin Yu)
Date: Tue, 12 Apr 2011 12:35:41 -0400
Subject: [R-sig-Geo] get the centroids of the polygones
In-Reply-To: <4da479ad.23aa340a.7f12.330d@mx.google.com>
References: <4da479ad.23aa340a.7f12.330d@mx.google.com>
Message-ID: <4DA47F5D.50107@mail.montclair.edu>

Jianhua:

Looks like get.Pcent was legacy now based on the error.

But since you've already read the shapefile into a spatial polygon 
dataframe, why not just use coordinates() to get the centroids? Such as:

....
cte<- readShapePoly("cte2007.shp", IDvar=Null)
centroids<-coordinates (cte)

Hope this helps.

Cheers,
Dr. Danlin Yu

On 2011-4-12 12:11, Jianhua Huang wrote:
> Hi Everyone:
>
>
>
> I am trying to get the centroids of all the polygons in the shape file.
>
>
>
> I use the following code:
>
>
>
> library(spdep)
>
> library(maptools)
>
> cte<- readShapePoly("cte2007.shp",IDvar=NULL)
>
> get.Pcent(cte)
>
>
>
> But there is an error when I run get.Pcent (Error: could not find function
> "get.Pcent")
>
>
>
> My R version is 2.12.2
>
>
>
> Anybody knows why the function doesn't work?
>
>
>
> Thanks
>
>
>
> Robert
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
___________________________________________
Danlin Yu, Ph.D.
Associate Professor of GIS and Urban Geography
Department of Earth&  Environmental Studies
Montclair State University
Montclair, NJ, 07043
Tel: 973-655-4313
Fax: 973-655-4072
email: yud at mail.montclair.edu
webpage: csam.montclair.edu/~yu


From jh.eco.cas at gmail.com  Tue Apr 12 18:51:25 2011
From: jh.eco.cas at gmail.com (Jianhua Huang)
Date: Tue, 12 Apr 2011 12:51:25 -0400
Subject: [R-sig-Geo] get the centroids of the polygones
In-Reply-To: <4DA47F5D.50107@mail.montclair.edu>
References: <4da479ad.23aa340a.7f12.330d@mx.google.com>
	<4DA47F5D.50107@mail.montclair.edu>
Message-ID: <4da4830a.9247e60a.3dcb.ffffa9de@mx.google.com>

Hi Danlin:

Thanks much for your help. This is really a very useful function. Does the
coordinates() function returns the coordinate value of the polygon's
centroids, or other value within or on the polygon?

I have check the function, but the introduction is not detailed enough for
me to tell whether the returned value is exactly the one I need. Here is the
introduction: http://127.0.0.1:27343/library/sp/html/coordinates.html  

Thanks again for your help. 

Jianhua

-----Original Message-----
From: Danlin Yu [mailto:yud at mail.montclair.edu] 
Sent: 2011?4?12? 12:36
To: Jianhua Huang
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] get the centroids of the polygones

Jianhua:

Looks like get.Pcent was legacy now based on the error.

But since you've already read the shapefile into a spatial polygon 
dataframe, why not just use coordinates() to get the centroids? Such as:

....
cte<- readShapePoly("cte2007.shp", IDvar=Null)
centroids<-coordinates (cte)

Hope this helps.

Cheers,
Dr. Danlin Yu

On 2011-4-12 12:11, Jianhua Huang wrote:
> Hi Everyone:
>
>
>
> I am trying to get the centroids of all the polygons in the shape file.
>
>
>
> I use the following code:
>
>
>
> library(spdep)
>
> library(maptools)
>
> cte<- readShapePoly("cte2007.shp",IDvar=NULL)
>
> get.Pcent(cte)
>
>
>
> But there is an error when I run get.Pcent (Error: could not find function
> "get.Pcent")
>
>
>
> My R version is 2.12.2
>
>
>
> Anybody knows why the function doesn't work?
>
>
>
> Thanks
>
>
>
> Robert
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
___________________________________________
Danlin Yu, Ph.D.
Associate Professor of GIS and Urban Geography
Department of Earth&  Environmental Studies
Montclair State University
Montclair, NJ, 07043
Tel: 973-655-4313
Fax: 973-655-4072
email: yud at mail.montclair.edu
webpage: csam.montclair.edu/~yu


From yud at mail.montclair.edu  Tue Apr 12 19:09:40 2011
From: yud at mail.montclair.edu (Danlin Yu)
Date: Tue, 12 Apr 2011 13:09:40 -0400
Subject: [R-sig-Geo] get the centroids of the polygones
In-Reply-To: <4da4830a.9247e60a.3dcb.ffffa9de@mx.google.com>
References: <4da479ad.23aa340a.7f12.330d@mx.google.com>
	<4DA47F5D.50107@mail.montclair.edu>
	<4da4830a.9247e60a.3dcb.ffffa9de@mx.google.com>
Message-ID: <4DA48754.3080202@mail.montclair.edu>

Jianhua:

Well, I happen to have ArcGIS as well, so I did the feature to point and
add xy coordinates routine and compared the obtained coordinates with
what R coordinates() function returns. They match. So I would say
coordinates() certainly returns the centroids of the polygons (it makes
more sense if it is the centroids returned, right? otherwise, the
algorithm for coordinates() would be rather complex and tedious).

Hope this helps.

Cheers,
Danlin

On 2011-4-12 12:51, Jianhua Huang wrote:
> Hi Danlin:
>
> Thanks much for your help. This is really a very useful function. Does the
> coordinates() function returns the coordinate value of the polygon's
> centroids, or other value within or on the polygon?
>
> I have check the function, but the introduction is not detailed enough for
> me to tell whether the returned value is exactly the one I need. Here is the
> introduction: http://127.0.0.1:27343/library/sp/html/coordinates.html  
>
> Thanks again for your help. 
>
> Jianhua
>
> -----Original Message-----
> From: Danlin Yu [mailto:yud at mail.montclair.edu] 
> Sent: 2011?4?12? 12:36
> To: Jianhua Huang
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] get the centroids of the polygones
>
> Jianhua:
>
> Looks like get.Pcent was legacy now based on the error.
>
> But since you've already read the shapefile into a spatial polygon 
> dataframe, why not just use coordinates() to get the centroids? Such as:
>
> ....
> cte<- readShapePoly("cte2007.shp", IDvar=Null)
> centroids<-coordinates (cte)
>
> Hope this helps.
>
> Cheers,
> Dr. Danlin Yu
>
> On 2011-4-12 12:11, Jianhua Huang wrote:
>> Hi Everyone:
>>
>>
>>
>> I am trying to get the centroids of all the polygons in the shape file.
>>
>>
>>
>> I use the following code:
>>
>>
>>
>> library(spdep)
>>
>> library(maptools)
>>
>> cte<- readShapePoly("cte2007.shp",IDvar=NULL)
>>
>> get.Pcent(cte)
>>
>>
>>
>> But there is an error when I run get.Pcent (Error: could not find function
>> "get.Pcent")
>>
>>
>>
>> My R version is 2.12.2
>>
>>
>>
>> Anybody knows why the function doesn't work?
>>
>>
>>
>> Thanks
>>
>>
>>
>> Robert
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
___________________________________________
Danlin Yu, Ph.D.
Associate Professor of GIS and Urban Geography
Department of Earth & Environmental Studies
Montclair State University
Montclair, NJ, 07043
Tel: 973-655-4313
Fax: 973-655-4072
email: yud at mail.montclair.edu
webpage: csam.montclair.edu/~yu


From jh.eco.cas at gmail.com  Tue Apr 12 19:30:48 2011
From: jh.eco.cas at gmail.com (Jianhua Huang)
Date: Tue, 12 Apr 2011 13:30:48 -0400
Subject: [R-sig-Geo] get the centroids of the polygones
In-Reply-To: <4DA48754.3080202@mail.montclair.edu>
References: <4da479ad.23aa340a.7f12.330d@mx.google.com>
	<4DA47F5D.50107@mail.montclair.edu>
	<4da4830a.9247e60a.3dcb.ffffa9de@mx.google.com>
	<4DA48754.3080202@mail.montclair.edu>
Message-ID: <4da48c44.ef00340a.179b.7b02@mx.google.com>

Hi Danlin:

Thanks. It is very helpful.

Jianhua

-----Original Message-----
From: Danlin Yu [mailto:yud at mail.montclair.edu] 
Sent: 2011?4?12? 13:10
To: Jianhua Huang
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] get the centroids of the polygones

Jianhua:

Well, I happen to have ArcGIS as well, so I did the feature to point and
add xy coordinates routine and compared the obtained coordinates with
what R coordinates() function returns. They match. So I would say
coordinates() certainly returns the centroids of the polygons (it makes
more sense if it is the centroids returned, right? otherwise, the
algorithm for coordinates() would be rather complex and tedious).

Hope this helps.

Cheers,
Danlin

On 2011-4-12 12:51, Jianhua Huang wrote:
> Hi Danlin:
>
> Thanks much for your help. This is really a very useful function. Does the
> coordinates() function returns the coordinate value of the polygon's
> centroids, or other value within or on the polygon?
>
> I have check the function, but the introduction is not detailed enough for
> me to tell whether the returned value is exactly the one I need. Here is
the
> introduction: http://127.0.0.1:27343/library/sp/html/coordinates.html  
>
> Thanks again for your help. 
>
> Jianhua
>
> -----Original Message-----
> From: Danlin Yu [mailto:yud at mail.montclair.edu] 
> Sent: 2011?4?12? 12:36
> To: Jianhua Huang
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] get the centroids of the polygones
>
> Jianhua:
>
> Looks like get.Pcent was legacy now based on the error.
>
> But since you've already read the shapefile into a spatial polygon 
> dataframe, why not just use coordinates() to get the centroids? Such as:
>
> ....
> cte<- readShapePoly("cte2007.shp", IDvar=Null)
> centroids<-coordinates (cte)
>
> Hope this helps.
>
> Cheers,
> Dr. Danlin Yu
>
> On 2011-4-12 12:11, Jianhua Huang wrote:
>> Hi Everyone:
>>
>>
>>
>> I am trying to get the centroids of all the polygons in the shape file.
>>
>>
>>
>> I use the following code:
>>
>>
>>
>> library(spdep)
>>
>> library(maptools)
>>
>> cte<- readShapePoly("cte2007.shp",IDvar=NULL)
>>
>> get.Pcent(cte)
>>
>>
>>
>> But there is an error when I run get.Pcent (Error: could not find
function
>> "get.Pcent")
>>
>>
>>
>> My R version is 2.12.2
>>
>>
>>
>> Anybody knows why the function doesn't work?
>>
>>
>>
>> Thanks
>>
>>
>>
>> Robert
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
___________________________________________
Danlin Yu, Ph.D.
Associate Professor of GIS and Urban Geography
Department of Earth & Environmental Studies
Montclair State University
Montclair, NJ, 07043
Tel: 973-655-4313
Fax: 973-655-4072
email: yud at mail.montclair.edu
webpage: csam.montclair.edu/~yu


From r.hijmans at gmail.com  Tue Apr 12 22:07:52 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Tue, 12 Apr 2011 13:07:52 -0700 (PDT)
Subject: [R-sig-Geo] Calculating/applying transition matrices from
 classified imagery?
In-Reply-To: <BANLkTin_9Y9LUEEP+oKbm-O=E7fz2e2V_w@mail.gmail.com>
References: <BANLkTin_9Y9LUEEP+oKbm-O=E7fz2e2V_w@mail.gmail.com>
Message-ID: <1302638872665-6266539.post@n2.nabble.com>


> Does anyone know of a package (or a suggestion on how to implement) to 
> calculate, for two classified raster images of the same location but 
> different times, the relative probability of transitioning from one class
> to 
> the other?  Additionally, once this is figured out, how to apply this 
> transition matrix the one of the rasters to predict future per-class 
> probabilities?  Thanks!  Ideally this should support arbitrarily large 
> files. 
> 
> --j 

Jonathan, 

Is the below what you are after? Perhaps there are some Markov chain
implementations that could provide additional things.

Robert


library(raster)
# creating two classified rasters
s <- r <- raster(nrow=10, ncol=10)
r[] <- rep(1:5, each=20)
s[] = round(r[] + rnorm(ncell(r), sd=0.5))
s[s<1] <- 1
s[s>5] <- 5

# cross tabulate to get probabilities of transition
crt <- crosstab(r, s)
crt <- crt / apply(crt, 1, sum) # probabilities

# function to apply the cross tab probs
fun <- function(x) {
	classes <- as.numeric(rownames(crt))
	res <- matrix(nrow=length(x), ncol=ncol(crt))
	for (i in 1:length(classes)) {
		tr <- t(res[x==classes[i], ])
		tr[] <- crt[i,]
		res[x==classes[i], ] <- t(tr)
	}	
	res
}

x <- calc(s,fun=fun)
layerNames(x) <- paste('p( x =', 1:5, ')')

par(mfrow=c(1,2))
plot(r, main='first')
plot(s, main='second')
x11()
plot(x)
 


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Calculating-applying-transition-matrices-from-classified-imagery-tp6247825p6266539.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From baptiste.auguie at googlemail.com  Tue Apr 12 23:24:17 2011
From: baptiste.auguie at googlemail.com (baptiste auguie)
Date: Wed, 13 Apr 2011 09:24:17 +1200
Subject: [R-sig-Geo] colour mapping tesselated irregular xy data
Message-ID: <BANLkTikT1gufRwFKav_n-hp7jX5D9P0vXw@mail.gmail.com>

Dear list,

I have x,y,z data originating from an electromagnetic simulation on an
irregular mesh. I wish to visualise a 2D map of these (N~1e6) data
points with colour-coded z intensity.

On way is to perform an interpolation onto a regular grid. This works
(using akima's interp for example), but is not what I'm after. The xy
mesh is very fine at places, and rough at others, and interpolation
loses this spatial information. Thus, I wish to display coloured tiles
of irregular size. Voronoi and delaunay tesselations seem quite suited
to the task; however I am stuck with two technical difficulties.

1- while voronoi/dirichlet gives me N polygons, delaunay obviously
returns more triangular tiles than there are points initially. How
could I perform the interpolation for the assignment of colours?

2- spatstat and maptools seem to work with special|spatial| classes,
from which I have been unable to efficiently extract the polygon
outlines in a data.frame. This is necessary because I wish to use
lattice or ggplot2 for plotting, rather than the default plot method.

Below's a minimal illustration,

N <- 100
set.seed(123)
d <- data.frame(x=rnorm(N, 15, 4), y=rnorm(N, 15, 4))
## a circular intensity pattern
d$z = (d$x - 15)^2 + (d$y - 15)^2

library(spatstat)
library(maptools)

W <- ripras(df, shape="rectangle")
W <- owin(c(0, 30), c(0, 30))
X <- as.ppp(d, W=W)
Y <- dirichlet(X)
Z <- as(Y, "SpatialPolygons")
## default plot
## plot(Z, col=grey(d$z/max(d$z)))

## awfully inefficient extraction of the polygons...

extract.tiles <- function(x){
 data.frame(x$bdry[[1]][1:2])
}

all <- lapply(Y$tiles, extract.tiles )

library(ggplot2)
## convert list to long format data.frame with ids
names(all) <- seq_along(all)
m <- melt(all, id=1:2)
m$fill <- d$z[as.integer(m$L1)]

ggplot(m)+
 geom_polygon(aes(x,y,fill=fill, group=fill))+
 scale_fill_gradient(low="black", high="white")

library(latticeExtra)
## result similar (except for the edges) to latticeExtra's
panel.voronoi, as intended
levelplot(z~x*y, data=d,
         panel = function(...) panel.voronoi(..., points=FALSE),
         col.regions = colorRampPalette(grey(0:1))(1e3), cut=1e3)


## now trying the same with delaunay tiles
Y1 <- delaunay(X)
Z1 <- as(Y1, "SpatialPolygons")
## no link between original z-values and tiles
plot(Z1, col=grey(d$z/max(d$z)))


Best regards,

baptiste


From edzer.pebesma at uni-muenster.de  Tue Apr 12 23:27:02 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Tue, 12 Apr 2011 23:27:02 +0200
Subject: [R-sig-Geo] sp::overlay problem
In-Reply-To: <1302613497958-6265018.post@n2.nabble.com>
References: <1302613497958-6265018.post@n2.nabble.com>
Message-ID: <4DA4C3A6.9060702@uni-muenster.de>

Piero, why don't you spTransform your points to the CRS of the grid?

On 04/12/2011 03:04 PM, piero campa wrote:
> Dear list,
> I have a SpatialGridDataFrame, whose SRS is EPSG:32632:
> "+init=epsg:32632 +proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units=m
> +no_defs +towgs84=0,0,0"
> 
> I am willing to overlay these grid values over point locations defined in a
> SpatialPixelsDataFrame with a different SRS using the overlay method
> (overlaying grids over points in the same SRS gave me no problem until now).
> 
> Now instead I need to transform the point locations to EPSG:32632 and then
> overlay: 
>> my.ov <- overlay(my.grid, spTransform(mySpatialPixels,
>> CRS=CRS(proj4string(my.grid))))
> [looking at methods?sp::overlay, transforming the grid to the points SRS
> would coerce the grid itself into points, and then the overlay method would
> not be useful anymore]
> 
> But then the output "my.ov" is filled with NAs only.
> 
> I checked the bbox of the overlay inputs, and they overlap:
>> bbox(my.grid)
>               min     max
> coords.x1  513000  911000
> coords.x2 4844000 5002000
>> bbox(mySpatialPixels)
>         min       max
> x  499880.9  822436.4
> y 4816908.3 5045694.8
> 
> Any hint is appreciated.
> Thank you guys.
> /Piero
> 
> http://r-sig-geo.2731867.n2.nabble.com/file/n6265018/overlay.png 
> 
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/sp-overlay-problem-tp6265018p6265018.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From jacobvanetten at yahoo.com  Tue Apr 12 23:32:10 2011
From: jacobvanetten at yahoo.com (Jacob van Etten)
Date: Tue, 12 Apr 2011 22:32:10 +0100 (BST)
Subject: [R-sig-Geo] Calculating/applying transition matrices from
	classified imagery?
In-Reply-To: <1302638872665-6266539.post@n2.nabble.com>
Message-ID: <650299.46940.qm@web29708.mail.ird.yahoo.com>

This is not an area I am very familiar at all, but I have also seen the use of *adjacent* cells to determine probabilities. Certain types of land use "expand", e.g. new pixels appear at the margin rather than in isolated spots (think about city expansion). 

Perhaps gdistance (or Matrix) comes in handy to work with transition matrices...

Jacob.

--- On Tue, 12/4/11, Robert Hijmans <r.hijmans at gmail.com> wrote:

> From: Robert Hijmans <r.hijmans at gmail.com>
> Subject: Re: [R-sig-Geo] Calculating/applying transition matrices from classified imagery?
> To: r-sig-geo at r-project.org
> Date: Tuesday, 12 April, 2011, 22:07
> 
> > Does anyone know of a package (or a suggestion on how
> to implement) to 
> > calculate, for two classified raster images of the
> same location but 
> > different times, the relative probability of
> transitioning from one class
> > to 
> > the other?? Additionally, once this is figured
> out, how to apply this 
> > transition matrix the one of the rasters to predict
> future per-class 
> > probabilities?? Thanks!? Ideally this should
> support arbitrarily large 
> > files. 
> > 
> > --j 
> 
> Jonathan, 
> 
> Is the below what you are after? Perhaps there are some
> Markov chain
> implementations that could provide additional things.
> 
> Robert
> 
> 
> library(raster)
> # creating two classified rasters
> s <- r <- raster(nrow=10, ncol=10)
> r[] <- rep(1:5, each=20)
> s[] = round(r[] + rnorm(ncell(r), sd=0.5))
> s[s<1] <- 1
> s[s>5] <- 5
> 
> # cross tabulate to get probabilities of transition
> crt <- crosstab(r, s)
> crt <- crt / apply(crt, 1, sum) # probabilities
> 
> # function to apply the cross tab probs
> fun <- function(x) {
> ??? classes <- as.numeric(rownames(crt))
> ??? res <- matrix(nrow=length(x),
> ncol=ncol(crt))
> ??? for (i in 1:length(classes)) {
> ??? ??? tr <-
> t(res[x==classes[i], ])
> ??? ??? tr[] <- crt[i,]
> ??? ??? res[x==classes[i], ]
> <- t(tr)
> ??? }??? 
> ??? res
> }
> 
> x <- calc(s,fun=fun)
> layerNames(x) <- paste('p( x =', 1:5, ')')
> 
> par(mfrow=c(1,2))
> plot(r, main='first')
> plot(s, main='second')
> x11()
> plot(x)
>  
> 
> 
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Calculating-applying-transition-matrices-from-classified-imagery-tp6247825p6266539.html
> Sent from the R-sig-geo mailing list archive at
> Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From r.hijmans at gmail.com  Wed Apr 13 00:14:05 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Tue, 12 Apr 2011 15:14:05 -0700 (PDT)
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
In-Reply-To: <021F462EDC7F2B42A7038E9F9D4782611F18A4067F@itcnt27.itc.nl>
References: <021F462EDC7F2B42A7038E9F9D4782611F18A4067F@itcnt27.itc.nl>
Message-ID: <1302646445119-6266969.post@n2.nabble.com>

> Dear All, 
> 
> I tried to write a function that can be used in focalFilter of raster
> package, but the output
> result is different from what was calculated using spdep for a sample
> dataset. 
> It might be my mistake in the formula that is used in the function.
> Following you will
> find both functions as well as the resultss for a sample raster. 
>
> I would appreciate if anyone can help. 
> 
> Best regards, 
> Babak 


Babak, 
I think the mistake you made is that the local moran uses the _global_ mean
and the _global_ variance, whereas you used the _local_ mean and the _local_
variance. I have changed your function and now the result with focalFilter
is the same as with spdep.
Robert





locmor<-function(ras,d1=0,d2=6000) { # function to calculate local Moran
using spdep 
        require(raster) 
        require(spdep) 
       
df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)])) 
        df.sub<-subset(df,!is.na(values)) 
        df.nb <- dnearneigh(as.matrix(df.sub[,2:3]), d1, d2) 
        df.listw <- nb2listw(df.nb) #turns neighbourhood object into a
weighted list 
        lmor <- localmoran(df.sub$values, df.listw) 
        ras[df.sub$cells]<-as.data.frame(lmor)[[1]] 
        return(ras) 
} 

#--------------- 

tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61) 
tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T) 
tmp <- raster(tmp) 


tmp.spdep<-locmor(tmp,d1=0,d2=0.08838835) 

# new function for use with focalFilter
# first compute global mean and global variance
globmean <- cellStats(tmp, mean)
globvar <-  cellStats(tmp, sd)^2
# adjust variance denominator from n-1 to n
globvar <- (globvar * (ncell(tmp)-1)) / ncell(tmp)

lomo <- function (x, na.rm=TRUE, xbar=globmean, s2=globvar) {
	i <- trunc(length(x)/2)+1
    z <- x - globmean
	lz <- mean(z[-i], na.rm=na.rm)
    (z[i]/s2) * lz
}

filter<-matrix(1, nrow=3, ncol=3) 
tmp.focal<-focalFilter(tmp,filter,lomo) 

par(mfrow=c(1,2))
plot(tmp.spdep) 
plot(tmp.focal) 

# note that you could, but should not, do this:
tmp.focal<-focal(tmp,fun=lomo) 
# because  the trick to get the focal cell
#     i <- trunc(length(x)/2)+1
# does not work for the border cells. 
# To get the border cells:

tmp2 <- raster::expand(tmp, extent(tmp)+2*res(tmp))
tmpfoc2 <- focalFilter(tmp2,filter,lomo) 
tmpfoc2 <- crop(tmpfoc2, tmp)

par(mfrow=c(1,2))
plot(tmp.spdep) 
plot(tmpfoc2) 



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Local-Moran-I-for-raster-focalFilter-tp6250323p6266969.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From edzer.pebesma at uni-muenster.de  Wed Apr 13 00:52:19 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 13 Apr 2011 00:52:19 +0200
Subject: [R-sig-Geo] spacetime : the challenge of image
 time	series	containing holes
In-Reply-To: <1302621601781-6265557.post@n2.nabble.com>
References: <4D9FA7CE.2000108@usherbrooke.ca> <4DA08BFE.9070302@wzw.tum.de>
	<1302621601781-6265557.post@n2.nabble.com>
Message-ID: <4DA4D7A3.2040803@uni-muenster.de>

Servet,

For the solution below to work, you'll need spacetime package version
0.3-1, which I just submitted to CRAN (misnamed as
spacetime_0.3-2.tar.gz; I had to use passive ftp for some reason)

I added a solution to your problem that does not require the manual
create of an index to construct a sparse space time lattice (STSDF), as
was required in Tom's solution. Combined script below.

library(sp)
library(spacetime)
img200206 <- data.frame(rbind(
c(10849178, -4.979167,      -19.99302,      1.1542897E-03),
c(10849179, -4.979167,      -19.95120,      1.3418309E-03),
c(10849180, -4.979167,      -19.90937,      1.3988283E-03)))

img200207 <- data.frame(rbind(
c(10849178,  -4.979167,      -19.99302,      1.2843144E-03),
c(10849179,  -4.979167,      -19.95120,      1.1260216E-03)))

img200208 <- data.frame(rbind(
c(10849179,  -4.979167,      -19.95120,      1.5001300E-03),
c(10849180,  -4.979167,      -19.90937,      1.4793734E-03)))

coordinates(img200206)=c("X3","X2")
coordinates(img200207)=c("X3","X2")
coordinates(img200208)=c("X3","X2")

time = xts(1:3, as.POSIXct(c("2002-06-01","2002-07-01","2002-08-01")))

# solution one: spacetime creates index:
sp.list = list(img200206, img200207, img200208)
times = time[rep(1:length(sp.list), unlist(lapply(sp.list, length)))]
sp.all = do.call(rbind, sp.list)
stidf = STIDF(sp.all, times, sp.all at data)
stsdf = as(stidf, "STSDF")

# solution two: Tom creates index:
sp <- SpatialPoints(unique(rbind(coordinates(img200206),
coordinates(img200207), coordinates(img200208))))
data <- rbind(img200206, img200207, img200208)@data
index <- matrix(c(1:3, 1:2, 2:3, rep(1, 3), rep(2,2), rep(3,2)),
nrow=nrow(data))
mySTSDF = STSDF(sp, time, data, index)

# check they're equal:
all.equal(mySTSDF, stsdf, check.attributes)

On 04/12/2011 05:20 PM, scizmeli wrote:
> Hi Tom,
> 
> Thanks a million for your fast, accurate reply. I did not apply that to my
> imagery yet but the test I did show that it seems to work...
> 
> thank you!
> Servet
> 
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/spacetime-the-challenge-of-image-time-series-containing-holes-tp6255766p6265557.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From mrr at eloquentix.com  Wed Apr 13 04:12:55 2011
From: mrr at eloquentix.com (Mihail Rosu)
Date: Tue, 12 Apr 2011 22:12:55 -0400
Subject: [R-sig-Geo] How to compute the response variable from the
	GMerrorsar output?
Message-ID: <BANLkTim8p2WPQyvq1JKE13DhQwK1_6H24A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110412/557f62f8/attachment.pl>

From jon.skoien at jrc.ec.europa.eu  Wed Apr 13 09:59:16 2011
From: jon.skoien at jrc.ec.europa.eu (Jon Olav Skoien)
Date: Wed, 13 Apr 2011 09:59:16 +0200
Subject: [R-sig-Geo] Find nearest downstream value of a river network
Message-ID: <4DA557D4.7060008@jrc.ec.europa.eu>

Hi,

I have a SpatialLinesDataFrame with predictions on different locations 
of a river network that I would like to plot. However, I have many more 
line segments in the network than I have predictions, so most of the 
data.frame has NA-values. Does anyone know a simple way of finding the 
nearest downstream value for a line segment with an NA-value, so that I 
can get a continuous river network with predictions?

Below is a simple example of what I want to do, based on a shapefile 
("nres") that can be downloaded from:
http://intamap.geo.uu.nl/~jon/sarp/tempaacf/

nres = readOGR(".","nres")
nin = nres
nin$pred[!(nin$OBJECTID %in% c(1015, 1020, 1366, 1369, 4981))] = NA

"nres" is the SpatialLinesDataFrame that I would like to have as result 
after associating all NA-values to the nearest downstream value. "nin" 
is a simplified version of the SpatialLinesDataFrame that I have after 
predicting, and that I would like to use for creating "nres".  "pred" is 
the prediction column in these SLDFs. The following plot shows the 
results (with colors) and thick black lines for the segments where I 
have predictions.

spplot(nres, "pred", col.regions = bpy.colors(),
    panel = function(x,y, ...) {
     panel.polygonsplot(x,y, ...)
     sp.lines(nin[!is.na(nin$pred),], col = "black", lwd = 2)
    })

So far I have got the result in this plot from an iterative procedure 
using the FROMJCT and TOJCT (from and to junction) IDs of the data.frame:

ichange = 1
while (ichange > 0) {
   ichange = 0
   for (i in 1:dim(nin)[1]) {
     if (!is.na(nin$pred[i])) {
       tt = which(nin$TOJCT == nin$FROMJCT[i])
       if (length(tt) > 0 && is.na(nin$pred[tt])) {
         nin$pred[tt] = nin$pred[i]
         ichange = ichange + 1
       }
     }
   }
}

But I think it should be either possible to simplify this (quite slow 
for a large river network), or preferably take advantage of topology of 
SpatialLines objects? Any clues?

Thanks,
Jon


From Roger.Bivand at nhh.no  Wed Apr 13 10:44:17 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 13 Apr 2011 10:44:17 +0200 (CEST)
Subject: [R-sig-Geo] How to compute the response variable from the
 GMerrorsar output?
In-Reply-To: <BANLkTim8p2WPQyvq1JKE13DhQwK1_6H24A@mail.gmail.com>
References: <BANLkTim8p2WPQyvq1JKE13DhQwK1_6H24A@mail.gmail.com>
Message-ID: <alpine.LRH.2.00.1104131034460.15967@reclus.nhh.no>

On Tue, 12 Apr 2011, Mihail Rosu wrote:

> Dear list,
>
> I'm using a 3rd party code to (spatially) analyse the dependence of crops
> yields (YLD) on soil types (MUSYM).  Consider the model
>
> model<- YLD ~ MUSYM -1
>
> The lm() function ouputs as coefficients the average YLD for the various 
> soils (see below). I'm confused about the interpretation of coefficients 
> outputed by GMerrorsar(). They are kind of twice smaller than the 
> average YLD !?!?

Use GM methods with spatial data with great care! Note that the spatial 
coefficient estimate is outside its range (for your row standardised 
sptial weights, it should be strictly less than 1). You can try to tune 
the optimizer used, but in general maximum likelihood is to be prefered. 
If you use spautolm() or errorsarlm() with method="Matrix", you should get 
the exact results you need, or try method="MC" or method="Chebyshev" for 
approximations.

Hope this helps,

Roger


>
> Please help on "how to compute the predicted YLD from the GMerrorsar() 
> output". Should I use the "fitted.values" instead of the coefficients?
>
> much thanks,
>
> Radu
>
>> diagnostics<-lm(model, data)
>> summary(diagnostics)
>
> Call:
> lm(formula = model, data = data)
>
> Residuals:
>    Min      1Q  Median      3Q     Max
> -44.006  -2.489   2.948   7.258  32.591
>
> Coefficients:
>        Estimate Std. Error t value Pr(>|t|)
> MUSYMBa  42.1410     0.2279  184.90   <2e-16 ***
> MUSYMBe  39.1673     0.3420  114.52   <2e-16 ***
> MUSYMBf  19.5921     0.5783   33.88   <2e-16 ***
> MUSYMCa  33.1261     0.2935  112.88   <2e-16 ***
> MUSYMCh  43.6497     0.1580  276.21   <2e-16 ***
> MUSYMCn  41.7622     0.1309  318.98   <2e-16 ***
> MUSYMDa  37.1995     0.5189   71.69   <2e-16 ***
> MUSYMSb  38.3553     0.2168  176.93   <2e-16 ***
> MUSYMTa  44.0064     0.3164  139.10   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>
> Residual standard error: 12.32 on 26679 degrees of freedom
> Multiple R-squared: 0.9171,     Adjusted R-squared: 0.917
> F-statistic: 3.278e+04 on 9 and 26679 DF,  p-value: < 2.2e-16
>
>
> dW <- dnearneigh(coords, 0, dist)
> dlist <- nbdists(dW, coords)
> idlist <- lapply(dlist, function(x) 1/x)
> W <- nb2listw(dW, glist=idlist, style="W")
>
> #Performs spatial error process model with empirically determined spatial
> weights matrix
>
> SEM<-GMerrorsar(model,data=data, W, na.action=na.exclude, zero.policy=TRUE)
>
>> summary(SEM)
>
> Call:GMerrorsar(formula = model, data = data, listw = W, na.action =
> na.exclude,     zero.policy = TRUE)
>
> Residuals:
>       Min         1Q     Median         3Q        Max
> -46.788453  -2.508823   0.024350   2.486553  37.375018
>
> Type: GM SAR estimator
> Coefficients: (GM standard errors)
>        Estimate Std. Error z value  Pr(>|z|)
> MUSYMBa  17.7399     2.3552  7.5322 4.996e-14
> MUSYMBe  21.8829     2.3987  9.1229 < 2.2e-16
> MUSYMBf  16.4898     2.4502  6.7299 1.698e-11
> MUSYMCa  21.3378     2.4094  8.8561 < 2.2e-16
> MUSYMCh  18.8470     2.3216  8.1182 4.441e-16
> MUSYMCn  18.8399     2.3164  8.1332 4.441e-16
> MUSYMDa  19.5054     2.4220  8.0533 8.882e-16
> MUSYMSb  19.0423     2.3655  8.0501 8.882e-16
> MUSYMTa  19.2016     2.3662  8.1150 4.441e-16
>
> Lambda: 1.0157
> Number of observations: 26688
> Number of parameters estimated: 11
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From karl at huftis.org  Wed Apr 13 13:42:24 2011
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Wed, 13 Apr 2011 13:42:24 +0200
Subject: [R-sig-Geo] Diameter of a polygon
Message-ID: <io426v$u4i$1@dough.gmane.org>

Is it possible to easily calculate the diameter of a (large) polygon, 
i.e. the longest distance between two points of the polygon? For a 
rectangle, this would be the length of a diagonal.

The two points need not be actual vertices of the polygons (or must they 
necessarily be so?).

I?m also interested in the shortest length. For a rectangle, this would be 
the length of the shortest side.

And perhaps also the longest line that can be placed inside the polygon. 
Note that this may easily be shorter than the longest diameter, as the 
corresponding diameter line may pass outside the polygon. (Though I guess 
for the *shortest* line, it would be identical to the smallest diameter.)

A quick rseek.org search didn?t turn up anything relevant.

-- 
Karl Ove Hufthammer


From mrr at eloquentix.com  Wed Apr 13 13:58:05 2011
From: mrr at eloquentix.com (Mihail Rosu)
Date: Wed, 13 Apr 2011 07:58:05 -0400
Subject: [R-sig-Geo] How to compute the response variable from the
 GMerrorsar output?
In-Reply-To: <alpine.LRH.2.00.1104131034460.15967@reclus.nhh.no>
References: <BANLkTim8p2WPQyvq1JKE13DhQwK1_6H24A@mail.gmail.com>
	<alpine.LRH.2.00.1104131034460.15967@reclus.nhh.no>
Message-ID: <BANLkTike7QNYYBpJpSwig5Ys93r6Z--uyQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110413/d483227f/attachment.pl>

From nick at hamm.org  Wed Apr 13 14:16:15 2011
From: nick at hamm.org (Nick Hamm)
Date: Wed, 13 Apr 2011 14:16:15 +0200
Subject: [R-sig-Geo] focalFilter problem raster package
In-Reply-To: <1302577997688-6263699.post@n2.nabble.com>
References: <BANLkTi=TZZg1CnCqw4ZDb3u7NUUPV_TtkQ@mail.gmail.com>
	<1301980998310-6241121.post@n2.nabble.com>
	<BANLkTim=Zwo8czGbyNrWXxLd5_qAt7Mrfg@mail.gmail.com>
	<1302577997688-6263699.post@n2.nabble.com>
Message-ID: <BANLkTinqyR9ExS-NJVxsE6LoNUZ4hg=RKA@mail.gmail.com>

Dear Robert

Thanks for this!  Great!

I have now installed version 1.8-12 (11-april-2011).

best wishes

Nick


On 12 April 2011 05:13, Robert Hijmans <r.hijmans at gmail.com> wrote:
>> Dear Robert
>>
>> I hope the following example is clear. ?you should be able to cut and
>> paste...
>>
>> Nick
>
> Dear Nick,
>
> Thank you for the example. It reveals a serious error in focalFilter.
> Results with that function in raster versions lower than 1.8-11 are wrong in
> the first rows (half the number of rows of the filter), when using a filter
> larger than 3x3 (because I forgot a "byrow=TRUE" argument in a call to
> matrix). Version 1.8-11 is on its way to CRAN.
>
> Robert
>
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/focalFilter-problem-raster-package-tp6235925p6263699.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From b.rowlingson at lancaster.ac.uk  Wed Apr 13 15:51:21 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 13 Apr 2011 14:51:21 +0100
Subject: [R-sig-Geo] Diameter of a polygon
In-Reply-To: <io426v$u4i$1@dough.gmane.org>
References: <io426v$u4i$1@dough.gmane.org>
Message-ID: <BANLkTin=dwNhP5BSoHYPG-hpaFZPBKBg2A@mail.gmail.com>

On Wed, Apr 13, 2011 at 12:42 PM, Karl Ove Hufthammer <karl at huftis.org> wrote:
> Is it possible to easily calculate the diameter of a (large) polygon,
> i.e. the longest distance between two points of the polygon? For a
> rectangle, this would be the length of a diagonal.
>
> The two points need not be actual vertices of the polygons (or must they
> necessarily be so?).

 For a convex polygon it must be vertices of the polygon
[http://cgm.cs.mcgill.ca/~orm/diam.html] and for a non-convex polygon
I can't see how it could be anything other than points on the convex
hull [conjecture]. That web site gives an algorithm in pseudo code,
but there's the dumb approach of doing:

max(dist(cbind(xc,yc)))

 where xc and yc are the coords of your polygon.

> I?m also interested in the shortest length. For a rectangle, this would be
> the length of the shortest side.

 Sounds like the 'width' - although again here its for convex
polygons, possibly the convex hull of a non-convex polygon can be
used:

http://cgm.cs.mcgill.ca/~orm/width.html

> And perhaps also the longest line that can be placed inside the polygon.
> Note that this may easily be shorter than the longest diameter, as the
> corresponding diameter line may pass outside the polygon. (Though I guess
> for the *shortest* line, it would be identical to the smallest diameter.)

 For non-convex polygons that sounds like a very hard problem, since
you could have all sorts of little bays and inlets that would mess up
your long line. This is definitely a case where the end points wouldnt
need to be vertices of the polygon. The shortest line that can be
placed inside a polygon is easy - it has length zero!

Barry


From momadou at yahoo.fr  Wed Apr 13 17:26:31 2011
From: momadou at yahoo.fr (Komine)
Date: Wed, 13 Apr 2011 08:26:31 -0700 (PDT)
Subject: [R-sig-Geo] Test de Hosmer et Lemeshow
In-Reply-To: <BANLkTinwM+zSPYSNgMH+E7O5UN1mRu=nSA@mail.gmail.com>
References: <BANLkTinwM+zSPYSNgMH+E7O5UN1mRu=nSA@mail.gmail.com>
Message-ID: <1302708391961-6269454.post@n2.nabble.com>

Thanks very much Raphael Saldanha, 

I used this method also to verify the first. I find the same result for the
first P.Value in my first message. So I think I can interpret the first
p.value only! 

> cut.score<-cut(score,breaks=quantile(score,probs=seq(0,1,0.1)),include.lowest=T) 
> obs<-xtabs(cbind(1-Fire$Ignition,Fire$Ignition)~cut.score) 
> expect<-xtabs(cbind(1-score,score)~cut.score) 
> chisq<-sum((obs-expect)^2/expect) 
> P<-1-pchisq(chisq,10-2) 
> print(c("X^2"=chisq,DF=10-2,"p(<Chi)"=P)) 
     X^2       DF  p(<Chi) 
2.702574 8.000000 0.951615 



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Re-Test-de-Hosmer-et-Lemeshow-tp6265333p6269454.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From naimi at itc.nl  Wed Apr 13 19:46:42 2011
From: naimi at itc.nl (Babak Naimi)
Date: Wed, 13 Apr 2011 19:46:42 +0200
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
Message-ID: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>

Dear Robert,

Thanks for your reply. Now, the result is the same as with localmoran in spdep package. But I checked this with Getis & Ord (1996), seems the formula, as you implement in the script, is not the same as what has been explained in the Getis & Ord. First difference is that, Xi is excluded in calculation of the global mean and variance. The second difference is that, in the line "lz <- mean(z[-i], na.rm=na.rm)", 'mean' should be replaced with 'sum'. I re-implemented this function. The example we used here came from that reference (p.266). The calculation for the cell 136 has also been provided in the book. The result that this new function returns is the same as with the book, now. But the strange thing to me is that it is not the same as spdep. Any comment?

Babak


Getis,A. and Ord, J. K. 1996 Local spatial statistics: an overview. In P. Longley and M. Batty (eds) Spatial
analysis: modelling in a GIS environment (Cambridge: Geoinformation International), 261-277.

#----------------------------------------------
focalMoran<-function(ras,filter) {
	globsum <- cellStats(ras, sum)
	glob.sqsum <- cellStats(ras^2, sum)
	n<-ncell(tmp)
	lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
		i <- trunc(length(x)/2)+1
	     	xbar=(sigmaX-x[i])/(N-1)
		s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
		z <- x - xbar
		lz <- sum(z[-i], na.rm=na.rm)
	    	return((z[i]/s2) * lz)
		}
	ras2 <- raster::expand(ras, extent(ras)+2*res(ras))
	ras2 <- focalFilter(ras2,filter,lomo) 
	ras2 <- crop(ras2, ras)
	return(ras2) 
}
#--------
locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep 
        require(raster) 
        require(spdep) 
        df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)])) 
        df<-na.omit(df)
        df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2) 
        df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list 
        lmor <- localmoran(df$values, df.listw) 
        ras[df$cells]<-as.data.frame(lmor)[[1]] 
        return(ras) 
} 

#####--------------- 

tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61) 
tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T) 
tmp <- raster(tmp) 


filter<-c(NA,1,NA,1,1,1,NA,1,NA)
filter<-matrix(f,nc=3,nr=3,byrow=T)

tmp.focal<-focalMoran(tmp,filter) 
tmp.spdep<-locmor(tmp,d1=0,d2=0.0625)

tmp.focal[136] # the calculation for this cell has been provided in Getis and Ord(1996, p.268)
tmp.spdep[136]


par(mfrow=c(1,2))
plot(tmp.focal)	
plot(tmp.spdep)

#------------------------------------------
#-----------------------------------------



Date: Tue, 12 Apr 2011 15:14:05 -0700 (PDT)
From: Robert Hijmans <r.hijmans at gmail.com>
To: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter
Message-ID: <1302646445119-6266969.post at n2.nabble.com>
Content-Type: text/plain; charset=us-ascii

> Dear All, 
> 
> I tried to write a function that can be used in focalFilter of raster
> package, but the output
> result is different from what was calculated using spdep for a sample
> dataset. 
> It might be my mistake in the formula that is used in the function.
> Following you will
> find both functions as well as the resultss for a sample raster. 
>
> I would appreciate if anyone can help. 
> 
> Best regards, 
> Babak 


Babak, 
I think the mistake you made is that the local moran uses the _global_ mean
and the _global_ variance, whereas you used the _local_ mean and the _local_
variance. I have changed your function and now the result with focalFilter
is the same as with spdep.
Robert





locmor<-function(ras,d1=0,d2=6000) { # function to calculate local Moran
using spdep 
        require(raster) 
        require(spdep) 
       
df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)])) 
        df.sub<-subset(df,!is.na(values)) 
        df.nb <- dnearneigh(as.matrix(df.sub[,2:3]), d1, d2) 
        df.listw <- nb2listw(df.nb) #turns neighbourhood object into a
weighted list 
        lmor <- localmoran(df.sub$values, df.listw) 
        ras[df.sub$cells]<-as.data.frame(lmor)[[1]] 
        return(ras) 
} 

#--------------- 

tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61) 
tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T) 
tmp <- raster(tmp) 


tmp.spdep<-locmor(tmp,d1=0,d2=0.08838835) 

# new function for use with focalFilter
# first compute global mean and global variance
globmean <- cellStats(tmp, mean)
globvar <-  cellStats(tmp, sd)^2
# adjust variance denominator from n-1 to n
globvar <- (globvar * (ncell(tmp)-1)) / ncell(tmp)

lomo <- function (x, na.rm=TRUE, xbar=globmean, s2=globvar) {
	i <- trunc(length(x)/2)+1
    z <- x - globmean
	lz <- mean(z[-i], na.rm=na.rm)
    (z[i]/s2) * lz
}

filter<-matrix(1, nrow=3, ncol=3) 
tmp.focal<-focalFilter(tmp,filter,lomo) 

par(mfrow=c(1,2))
plot(tmp.spdep) 
plot(tmp.focal) 

# note that you could, but should not, do this:
tmp.focal<-focal(tmp,fun=lomo) 
# because  the trick to get the focal cell
#     i <- trunc(length(x)/2)+1
# does not work for the border cells. 
# To get the border cells:

tmp2 <- raster::expand(tmp, extent(tmp)+2*res(tmp))
tmpfoc2 <- focalFilter(tmp2,filter,lomo) 
tmpfoc2 <- crop(tmpfoc2, tmp)

par(mfrow=c(1,2))
plot(tmp.spdep) 
plot(tmpfoc2) 


From momadou at yahoo.fr  Wed Apr 13 20:07:39 2011
From: momadou at yahoo.fr (Komine)
Date: Wed, 13 Apr 2011 11:07:39 -0700 (PDT)
Subject: [R-sig-Geo] Analyse residuals for logistic regression
Message-ID: <1302718059151-6270042.post@n2.nabble.com>

Dear, 

I did a logistic regression and a Hosmer-Lemeshow Test. In order to verify
residuals, I used this code: 

plot(rstudent(reglog),type="p",cex=0.5,ylab="R?sidus studentis?s par VC") 
abline(h=c(-2,2))

I want to verify if it is an adequate code to analyze residuals for a
logistic regression?   

Thanks for your advices 
Komine

--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Analyse-residuals-for-logistic-regression-tp6270042p6270042.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From r.hijmans at gmail.com  Wed Apr 13 21:34:09 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Wed, 13 Apr 2011 12:34:09 -0700
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
In-Reply-To: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>
References: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>
Message-ID: <BANLkTi=XjRcU51twwV=jcw6G7Eqh2HPcHQ@mail.gmail.com>

Babak,

I looked at the original paper,
http://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.1995.tb00338.x/abstract,
and the spdep manual, and other places, but they all seemed to be
saying somewhat different on the local moran statistic. I followed the
logic of the spdep::localmoran function, assuming that this is
"correct" and I should have phrased my answer more carefully: "based
onthe spdep::localmoran function, mean and variance should be
global...".  I do note that the two results are different but that
their correlation is very high R2 = 0.97; so perhaps they are both
right. Perhaps Roger can weigh in on this.

To make it run, in your code I replaced this:
filter<-c(NA,1,NA,1,1,1,NA,1,NA)
filter<-matrix(f,nc=3,nr=3,byrow=T)

with:
filter<-matrix(1 ,nc=3,nr=3)

Robert


On Wed, Apr 13, 2011 at 10:46 AM, Babak Naimi <naimi at itc.nl> wrote:
> Dear Robert,
>
> Thanks for your reply. Now, the result is the same as with localmoran in spdep package. But I checked this with Getis & Ord (1996), seems the formula, as you implement in the script, is not the same as what has been explained in the Getis & Ord. First difference is that, Xi is excluded in calculation of the global mean and variance. The second difference is that, in the line "lz <- mean(z[-i], na.rm=na.rm)", 'mean' should be replaced with 'sum'. I re-implemented this function. The example we used here came from that reference (p.266). The calculation for the cell 136 has also been provided in the book. The result that this new function returns is the same as with the book, now. But the strange thing to me is that it is not the same as spdep. Any comment?
>
> Babak
>
>
> Getis,A. and Ord, J. K. 1996 Local spatial statistics: an overview. In P. Longley and M. Batty (eds) Spatial
> analysis: modelling in a GIS environment (Cambridge: Geoinformation International), 261-277.
>
> #----------------------------------------------
> focalMoran<-function(ras,filter) {
> ? ? ? ?globsum <- cellStats(ras, sum)
> ? ? ? ?glob.sqsum <- cellStats(ras^2, sum)
> ? ? ? ?n<-ncell(tmp)
> ? ? ? ?lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
> ? ? ? ? ? ? ? ?i <- trunc(length(x)/2)+1
> ? ? ? ? ? ? ? ?xbar=(sigmaX-x[i])/(N-1)
> ? ? ? ? ? ? ? ?s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
> ? ? ? ? ? ? ? ?z <- x - xbar
> ? ? ? ? ? ? ? ?lz <- sum(z[-i], na.rm=na.rm)
> ? ? ? ? ? ? ? ?return((z[i]/s2) * lz)
> ? ? ? ? ? ? ? ?}
> ? ? ? ?ras2 <- raster::expand(ras, extent(ras)+2*res(ras))
> ? ? ? ?ras2 <- focalFilter(ras2,filter,lomo)
> ? ? ? ?ras2 <- crop(ras2, ras)
> ? ? ? ?return(ras2)
> }
> #--------
> locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep
> ? ? ? ?require(raster)
> ? ? ? ?require(spdep)
> ? ? ? ?df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
> ? ? ? ?df<-na.omit(df)
> ? ? ? ?df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2)
> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list
> ? ? ? ?lmor <- localmoran(df$values, df.listw)
> ? ? ? ?ras[df$cells]<-as.data.frame(lmor)[[1]]
> ? ? ? ?return(ras)
> }
>
> #####---------------
>
> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
> tmp <- raster(tmp)
>
>
> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
> filter<-matrix(f,nc=3,nr=3,byrow=T)
>
> tmp.focal<-focalMoran(tmp,filter)
> tmp.spdep<-locmor(tmp,d1=0,d2=0.0625)
>
> tmp.focal[136] # the calculation for this cell has been provided in Getis and Ord(1996, p.268)
> tmp.spdep[136]
>
>
> par(mfrow=c(1,2))
> plot(tmp.focal)
> plot(tmp.spdep)
>
> #------------------------------------------
> #-----------------------------------------
>
>
>
> Date: Tue, 12 Apr 2011 15:14:05 -0700 (PDT)
> From: Robert Hijmans <r.hijmans at gmail.com>
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter
> Message-ID: <1302646445119-6266969.post at n2.nabble.com>
> Content-Type: text/plain; charset=us-ascii
>
>> Dear All,
>>
>> I tried to write a function that can be used in focalFilter of raster
>> package, but the output
>> result is different from what was calculated using spdep for a sample
>> dataset.
>> It might be my mistake in the formula that is used in the function.
>> Following you will
>> find both functions as well as the resultss for a sample raster.
>>
>> I would appreciate if anyone can help.
>>
>> Best regards,
>> Babak
>
>
> Babak,
> I think the mistake you made is that the local moran uses the _global_ mean
> and the _global_ variance, whereas you used the _local_ mean and the _local_
> variance. I have changed your function and now the result with focalFilter
> is the same as with spdep.
> Robert
>
>
>
>
>
> locmor<-function(ras,d1=0,d2=6000) { # function to calculate local Moran
> using spdep
> ? ? ? ?require(raster)
> ? ? ? ?require(spdep)
>
> df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
> ? ? ? ?df.sub<-subset(df,!is.na(values))
> ? ? ? ?df.nb <- dnearneigh(as.matrix(df.sub[,2:3]), d1, d2)
> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a
> weighted list
> ? ? ? ?lmor <- localmoran(df.sub$values, df.listw)
> ? ? ? ?ras[df.sub$cells]<-as.data.frame(lmor)[[1]]
> ? ? ? ?return(ras)
> }
>
> #---------------
>
> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
> tmp <- raster(tmp)
>
>
> tmp.spdep<-locmor(tmp,d1=0,d2=0.08838835)
>
> # new function for use with focalFilter
> # first compute global mean and global variance
> globmean <- cellStats(tmp, mean)
> globvar <- ?cellStats(tmp, sd)^2
> # adjust variance denominator from n-1 to n
> globvar <- (globvar * (ncell(tmp)-1)) / ncell(tmp)
>
> lomo <- function (x, na.rm=TRUE, xbar=globmean, s2=globvar) {
> ? ? ? ?i <- trunc(length(x)/2)+1
> ? ?z <- x - globmean
> ? ? ? ?lz <- mean(z[-i], na.rm=na.rm)
> ? ?(z[i]/s2) * lz
> }
>
> filter<-matrix(1, nrow=3, ncol=3)
> tmp.focal<-focalFilter(tmp,filter,lomo)
>
> par(mfrow=c(1,2))
> plot(tmp.spdep)
> plot(tmp.focal)
>
> # note that you could, but should not, do this:
> tmp.focal<-focal(tmp,fun=lomo)
> # because ?the trick to get the focal cell
> # ? ? i <- trunc(length(x)/2)+1
> # does not work for the border cells.
> # To get the border cells:
>
> tmp2 <- raster::expand(tmp, extent(tmp)+2*res(tmp))
> tmpfoc2 <- focalFilter(tmp2,filter,lomo)
> tmpfoc2 <- crop(tmpfoc2, tmp)
>
> par(mfrow=c(1,2))
> plot(tmp.spdep)
> plot(tmpfoc2)
>
>
>


From baptiste.auguie at googlemail.com  Wed Apr 13 22:50:27 2011
From: baptiste.auguie at googlemail.com (baptiste auguie)
Date: Thu, 14 Apr 2011 08:50:27 +1200
Subject: [R-sig-Geo] Diameter of a polygon
In-Reply-To: <BANLkTin=dwNhP5BSoHYPG-hpaFZPBKBg2A@mail.gmail.com>
References: <io426v$u4i$1@dough.gmane.org>
	<BANLkTin=dwNhP5BSoHYPG-hpaFZPBKBg2A@mail.gmail.com>
Message-ID: <BANLkTi=FLsTWE0h9qZ7eXAFs5bnjNKDoow@mail.gmail.com>

On 14 April 2011 01:51, Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
> On Wed, Apr 13, 2011 at 12:42 PM, Karl Ove Hufthammer <karl at huftis.org> wrote:
>> Is it possible to easily calculate the diameter of a (large) polygon,
>> i.e. the longest distance between two points of the polygon? For a
>> rectangle, this would be the length of a diagonal.
>>
>> The two points need not be actual vertices of the polygons (or must they
>> necessarily be so?).
>
> ?For a convex polygon it must be vertices of the polygon
> [http://cgm.cs.mcgill.ca/~orm/diam.html] and for a non-convex polygon
> I can't see how it could be anything other than points on the convex
> hull [conjecture]. That web site gives an algorithm in pseudo code,
> but there's the dumb approach of doing:
>
> max(dist(cbind(xc,yc)))
>
> ?where xc and yc are the coords of your polygon.
>
>> I?m also interested in the shortest length. For a rectangle, this would be
>> the length of the shortest side.
>
> ?Sounds like the 'width' - although again here its for convex
> polygons, possibly the convex hull of a non-convex polygon can be
> used:
>
> http://cgm.cs.mcgill.ca/~orm/width.html
>
>> And perhaps also the longest line that can be placed inside the polygon.
>> Note that this may easily be shorter than the longest diameter, as the
>> corresponding diameter line may pass outside the polygon. (Though I guess
>> for the *shortest* line, it would be identical to the smallest diameter.)
>
> ?For non-convex polygons that sounds like a very hard problem, since
> you could have all sorts of little bays and inlets that would mess up
> your long line. This is definitely a case where the end points wouldnt
> need to be vertices of the polygon. The shortest line that can be
> placed inside a polygon is easy - it has length zero!

I guess by "shortest line" he meant "shortest chord", right?

baptiste

>
> Barry
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From Roger.Bivand at nhh.no  Wed Apr 13 23:15:22 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 13 Apr 2011 23:15:22 +0200 (CEST)
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
In-Reply-To: <BANLkTi=XjRcU51twwV=jcw6G7Eqh2HPcHQ@mail.gmail.com>
References: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>
	<BANLkTi=XjRcU51twwV=jcw6G7Eqh2HPcHQ@mail.gmail.com>
Message-ID: <alpine.LRH.2.00.1104132306140.19364@reclus.nhh.no>

On Wed, 13 Apr 2011, Robert J. Hijmans wrote:

> Babak,
>
> I looked at the original paper,
> http://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.1995.tb00338.x/abstract,
> and the spdep manual, and other places, but they all seemed to be
> saying somewhat different on the local moran statistic. I followed the
> logic of the spdep::localmoran function, assuming that this is
> "correct" and I should have phrased my answer more carefully: "based
> onthe spdep::localmoran function, mean and variance should be
> global...".  I do note that the two results are different but that
> their correlation is very high R2 = 0.97; so perhaps they are both
> right. Perhaps Roger can weigh in on this.

I don't have access to my library now. My understanding is that you are 
short-circuiting the construction of the spatial weights (or the n 
separate star matrices, one for each i, in Michael Tiefeldorf's terms). In 
general, Moran's I is not often used with non-zero values on the diagonal 
of the weights matrix. Non-zero values are seen in the local Getis-Ord 
Gi_star, but not in local Gi. My guess (without references) would be that 
the weight for i itself should be zero (the centre cell of the raster). 
The difference between mean() and sum() is the difference between 
row-standardised (summing to 1) and binary (all unity) weights. The 
function as it is doesn't permit other weighting schemes (such as IDW in 
either row-standardised or binary form).

Hope this helps,

Roger

>
> To make it run, in your code I replaced this:
> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
> filter<-matrix(f,nc=3,nr=3,byrow=T)
>
> with:
> filter<-matrix(1 ,nc=3,nr=3)
>
> Robert
>
>
> On Wed, Apr 13, 2011 at 10:46 AM, Babak Naimi <naimi at itc.nl> wrote:
>> Dear Robert,
>>
>> Thanks for your reply. Now, the result is the same as with localmoran in spdep package. But I checked this with Getis & Ord (1996), seems the formula, as you implement in the script, is not the same as what has been explained in the Getis & Ord. First difference is that, Xi is excluded in calculation of the global mean and variance. The second difference is that, in the line "lz <- mean(z[-i], na.rm=na.rm)", 'mean' should be replaced with 'sum'. I re-implemented this function. The example we used here came from that reference (p.266). The calculation for the cell 136 has also been provided in the book. The result that this new function returns is the same as with the book, now. But the strange thing to me is that it is not the same as spdep. Any comment?
>>
>> Babak
>>
>>
>> Getis,A. and Ord, J. K. 1996 Local spatial statistics: an overview. In P. Longley and M. Batty (eds) Spatial
>> analysis: modelling in a GIS environment (Cambridge: Geoinformation International), 261-277.
>>
>> #----------------------------------------------
>> focalMoran<-function(ras,filter) {
>> ? ? ? ?globsum <- cellStats(ras, sum)
>> ? ? ? ?glob.sqsum <- cellStats(ras^2, sum)
>> ? ? ? ?n<-ncell(tmp)
>> ? ? ? ?lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
>> ? ? ? ? ? ? ? ?i <- trunc(length(x)/2)+1
>> ? ? ? ? ? ? ? ?xbar=(sigmaX-x[i])/(N-1)
>> ? ? ? ? ? ? ? ?s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
>> ? ? ? ? ? ? ? ?z <- x - xbar
>> ? ? ? ? ? ? ? ?lz <- sum(z[-i], na.rm=na.rm)
>> ? ? ? ? ? ? ? ?return((z[i]/s2) * lz)
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ?ras2 <- raster::expand(ras, extent(ras)+2*res(ras))
>> ? ? ? ?ras2 <- focalFilter(ras2,filter,lomo)
>> ? ? ? ?ras2 <- crop(ras2, ras)
>> ? ? ? ?return(ras2)
>> }
>> #--------
>> locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep
>> ? ? ? ?require(raster)
>> ? ? ? ?require(spdep)
>> ? ? ? ?df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>> ? ? ? ?df<-na.omit(df)
>> ? ? ? ?df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2)
>> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list
>> ? ? ? ?lmor <- localmoran(df$values, df.listw)
>> ? ? ? ?ras[df$cells]<-as.data.frame(lmor)[[1]]
>> ? ? ? ?return(ras)
>> }
>>
>> #####---------------
>>
>> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
>> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
>> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
>> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
>> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
>> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
>> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
>> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
>> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
>> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
>> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
>> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
>> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
>> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
>> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
>> tmp <- raster(tmp)
>>
>>
>> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
>> filter<-matrix(f,nc=3,nr=3,byrow=T)
>>
>> tmp.focal<-focalMoran(tmp,filter)
>> tmp.spdep<-locmor(tmp,d1=0,d2=0.0625)
>>
>> tmp.focal[136] # the calculation for this cell has been provided in Getis and Ord(1996, p.268)
>> tmp.spdep[136]
>>
>>
>> par(mfrow=c(1,2))
>> plot(tmp.focal)
>> plot(tmp.spdep)
>>
>> #------------------------------------------
>> #-----------------------------------------
>>
>>
>>
>> Date: Tue, 12 Apr 2011 15:14:05 -0700 (PDT)
>> From: Robert Hijmans <r.hijmans at gmail.com>
>> To: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter
>> Message-ID: <1302646445119-6266969.post at n2.nabble.com>
>> Content-Type: text/plain; charset=us-ascii
>>
>>> Dear All,
>>>
>>> I tried to write a function that can be used in focalFilter of raster
>>> package, but the output
>>> result is different from what was calculated using spdep for a sample
>>> dataset.
>>> It might be my mistake in the formula that is used in the function.
>>> Following you will
>>> find both functions as well as the resultss for a sample raster.
>>>
>>> I would appreciate if anyone can help.
>>>
>>> Best regards,
>>> Babak
>>
>>
>> Babak,
>> I think the mistake you made is that the local moran uses the _global_ mean
>> and the _global_ variance, whereas you used the _local_ mean and the _local_
>> variance. I have changed your function and now the result with focalFilter
>> is the same as with spdep.
>> Robert
>>
>>
>>
>>
>>
>> locmor<-function(ras,d1=0,d2=6000) { # function to calculate local Moran
>> using spdep
>> ? ? ? ?require(raster)
>> ? ? ? ?require(spdep)
>>
>> df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>> ? ? ? ?df.sub<-subset(df,!is.na(values))
>> ? ? ? ?df.nb <- dnearneigh(as.matrix(df.sub[,2:3]), d1, d2)
>> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a
>> weighted list
>> ? ? ? ?lmor <- localmoran(df.sub$values, df.listw)
>> ? ? ? ?ras[df.sub$cells]<-as.data.frame(lmor)[[1]]
>> ? ? ? ?return(ras)
>> }
>>
>> #---------------
>>
>> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
>> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
>> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
>> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
>> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
>> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
>> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
>> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
>> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
>> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
>> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
>> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
>> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
>> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
>> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
>> tmp <- raster(tmp)
>>
>>
>> tmp.spdep<-locmor(tmp,d1=0,d2=0.08838835)
>>
>> # new function for use with focalFilter
>> # first compute global mean and global variance
>> globmean <- cellStats(tmp, mean)
>> globvar <- ?cellStats(tmp, sd)^2
>> # adjust variance denominator from n-1 to n
>> globvar <- (globvar * (ncell(tmp)-1)) / ncell(tmp)
>>
>> lomo <- function (x, na.rm=TRUE, xbar=globmean, s2=globvar) {
>> ? ? ? ?i <- trunc(length(x)/2)+1
>> ? ?z <- x - globmean
>> ? ? ? ?lz <- mean(z[-i], na.rm=na.rm)
>> ? ?(z[i]/s2) * lz
>> }
>>
>> filter<-matrix(1, nrow=3, ncol=3)
>> tmp.focal<-focalFilter(tmp,filter,lomo)
>>
>> par(mfrow=c(1,2))
>> plot(tmp.spdep)
>> plot(tmp.focal)
>>
>> # note that you could, but should not, do this:
>> tmp.focal<-focal(tmp,fun=lomo)
>> # because ?the trick to get the focal cell
>> # ? ? i <- trunc(length(x)/2)+1
>> # does not work for the border cells.
>> # To get the border cells:
>>
>> tmp2 <- raster::expand(tmp, extent(tmp)+2*res(tmp))
>> tmpfoc2 <- focalFilter(tmp2,filter,lomo)
>> tmpfoc2 <- crop(tmpfoc2, tmp)
>>
>> par(mfrow=c(1,2))
>> plot(tmp.spdep)
>> plot(tmpfoc2)
>>
>>
>>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

From Roger.Bivand at nhh.no  Wed Apr 13 23:21:05 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 13 Apr 2011 23:21:05 +0200 (CEST)
Subject: [R-sig-Geo] How to compute the response variable from the
 GMerrorsar output?
In-Reply-To: <BANLkTike7QNYYBpJpSwig5Ys93r6Z--uyQ@mail.gmail.com>
References: <BANLkTim8p2WPQyvq1JKE13DhQwK1_6H24A@mail.gmail.com>
	<alpine.LRH.2.00.1104131034460.15967@reclus.nhh.no>
	<BANLkTike7QNYYBpJpSwig5Ys93r6Z--uyQ@mail.gmail.com>
Message-ID: <alpine.LRH.2.00.1104132316150.19364@reclus.nhh.no>

On Wed, 13 Apr 2011, Mihail Rosu wrote:

> Thanks Roger for your quick answer.
>
> I'll follow your advise.
>
> Now, assuming that lambda is << 1, could I use
> fitted.values from GMErrorsar output to compute the (average)response
> variable?  I think of:
>
> mean(fitted.value) = mean(response -noise) ~= mean(response)
>
> provided that the mean(noise) ~= zero if the mean is computed over more than
> 30 points.
>
> Please be as kind as to advise on that,

For an error model, provided that it is correctly specified, and the 
regression coefficients take values that are very similar to the OLS 
coefficients (the standard errors may differ), you may do as you would 
with OLS. If, however, the model is not well specified, the error SAR 
coefficients differ from the OLS coefficients (see the optional Hausman 
test in the summary method), indicating that there are missing variables 
(or wrong functional forms) correlated with the spatially autocorrelated 
error. In that case, you'd need to correct the specification.

Hope this helps,

Roger

>
> Thanks,
>
> Radu
>
> On Wed, Apr 13, 2011 at 4:44 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Tue, 12 Apr 2011, Mihail Rosu wrote:
>>
>> Dear list,
>>>
>>> I'm using a 3rd party code to (spatially) analyse the dependence of crops
>>> yields (YLD) on soil types (MUSYM).  Consider the model
>>>
>>> model<- YLD ~ MUSYM -1
>>>
>>> The lm() function ouputs as coefficients the average YLD for the various
>>> soils (see below). I'm confused about the interpretation of coefficients
>>> outputed by GMerrorsar(). They are kind of twice smaller than the average
>>> YLD !?!?
>>>
>>
>> Use GM methods with spatial data with great care! Note that the spatial
>> coefficient estimate is outside its range (for your row standardised sptial
>> weights, it should be strictly less than 1). You can try to tune the
>> optimizer used, but in general maximum likelihood is to be prefered. If you
>> use spautolm() or errorsarlm() with method="Matrix", you should get the
>> exact results you need, or try method="MC" or method="Chebyshev" for
>> approximations.
>>
>> Hope this helps,
>>
>> Roger
>>
>>
>>
>>> Please help on "how to compute the predicted YLD from the GMerrorsar()
>>> output". Should I use the "fitted.values" instead of the coefficients?
>>>
>>> much thanks,
>>>
>>> Radu
>>>
>>> diagnostics<-lm(model, data)
>>>> summary(diagnostics)
>>>>
>>>
>>> Call:
>>> lm(formula = model, data = data)
>>>
>>> Residuals:
>>>   Min      1Q  Median      3Q     Max
>>> -44.006  -2.489   2.948   7.258  32.591
>>>
>>> Coefficients:
>>>       Estimate Std. Error t value Pr(>|t|)
>>> MUSYMBa  42.1410     0.2279  184.90   <2e-16 ***
>>> MUSYMBe  39.1673     0.3420  114.52   <2e-16 ***
>>> MUSYMBf  19.5921     0.5783   33.88   <2e-16 ***
>>> MUSYMCa  33.1261     0.2935  112.88   <2e-16 ***
>>> MUSYMCh  43.6497     0.1580  276.21   <2e-16 ***
>>> MUSYMCn  41.7622     0.1309  318.98   <2e-16 ***
>>> MUSYMDa  37.1995     0.5189   71.69   <2e-16 ***
>>> MUSYMSb  38.3553     0.2168  176.93   <2e-16 ***
>>> MUSYMTa  44.0064     0.3164  139.10   <2e-16 ***
>>> ---
>>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>>
>>> Residual standard error: 12.32 on 26679 degrees of freedom
>>> Multiple R-squared: 0.9171,     Adjusted R-squared: 0.917
>>> F-statistic: 3.278e+04 on 9 and 26679 DF,  p-value: < 2.2e-16
>>>
>>>
>>> dW <- dnearneigh(coords, 0, dist)
>>> dlist <- nbdists(dW, coords)
>>> idlist <- lapply(dlist, function(x) 1/x)
>>> W <- nb2listw(dW, glist=idlist, style="W")
>>>
>>> #Performs spatial error process model with empirically determined spatial
>>> weights matrix
>>>
>>> SEM<-GMerrorsar(model,data=data, W, na.action=na.exclude,
>>> zero.policy=TRUE)
>>>
>>> summary(SEM)
>>>>
>>>
>>> Call:GMerrorsar(formula = model, data = data, listw = W, na.action =
>>> na.exclude,     zero.policy = TRUE)
>>>
>>> Residuals:
>>>      Min         1Q     Median         3Q        Max
>>> -46.788453  -2.508823   0.024350   2.486553  37.375018
>>>
>>> Type: GM SAR estimator
>>> Coefficients: (GM standard errors)
>>>       Estimate Std. Error z value  Pr(>|z|)
>>> MUSYMBa  17.7399     2.3552  7.5322 4.996e-14
>>> MUSYMBe  21.8829     2.3987  9.1229 < 2.2e-16
>>> MUSYMBf  16.4898     2.4502  6.7299 1.698e-11
>>> MUSYMCa  21.3378     2.4094  8.8561 < 2.2e-16
>>> MUSYMCh  18.8470     2.3216  8.1182 4.441e-16
>>> MUSYMCn  18.8399     2.3164  8.1332 4.441e-16
>>> MUSYMDa  19.5054     2.4220  8.0533 8.882e-16
>>> MUSYMSb  19.0423     2.3655  8.0501 8.882e-16
>>> MUSYMTa  19.2016     2.3662  8.1150 4.441e-16
>>>
>>> Lambda: 1.0157
>>> Number of observations: 26688
>>> Number of parameters estimated: 11
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>>
>>>
>> --
>> Roger Bivand
>> Economic Geography Section, Department of Economics, Norwegian School of
>> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
>> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
>> e-mail: Roger.Bivand at nhh.no
>>
>>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From baptiste.auguie at googlemail.com  Wed Apr 13 23:48:59 2011
From: baptiste.auguie at googlemail.com (baptiste auguie)
Date: Thu, 14 Apr 2011 09:48:59 +1200
Subject: [R-sig-Geo] colour mapping tesselated irregular xy data
In-Reply-To: <BANLkTinHPjXN9K6QgTn7vLCdCTU+fmCc_w@mail.gmail.com>
References: <BANLkTikT1gufRwFKav_n-hp7jX5D9P0vXw@mail.gmail.com>
	<BANLkTinHPjXN9K6QgTn7vLCdCTU+fmCc_w@mail.gmail.com>
Message-ID: <BANLkTikxF7zj_ouzfX2VrZn=o7t6o8PwoA@mail.gmail.com>

Thanks, it is cleaner indeed. However, it's still painfully slow on
large data sets (typically a couple hundred thousand points). Looking
at the source of various functions, it looks like the place to look at
would be deldir::tile.list(). To my naive eyes it doesn't look
particularly efficient, given that the "meat" of the calculation that
I expected more costly was performed by deldir() in much less time,

N <- 1000
set.seed(123)
d <- data.frame(x=rnorm(N, 15, 4), y=rnorm(N, 15, 4))

library(spatstat)
library(maptools)

W <- owin(c(0, 30), c(0, 30))
X <- as.ppp(d, W=W)

system.time(dd <- deldir(d$x, d$y) )
   user  system elapsed
  0.079   0.009   0.088

system.time(Y <- dirichlet(X))
   user  system elapsed
  5.494   0.074   5.756

tile.list() would probably benefit greatly from being ported to C++,
and currently returns a list where I really want a data.frame to avoid
subsequent looping. Further coercion steps that I attempted earlier
are unnecessary as I wouldn't use the tess or SpatialPolygons classes.

Suggestions and comments most welcome.

baptiste

On 13 April 2011 14:36, Hadley Wickham <hadley at rice.edu> wrote:
> Did you try fortify on the SpatialPolygons?
> Hadley
>
> On Tue, Apr 12, 2011 at 4:24 PM, baptiste auguie
> <baptiste.auguie at googlemail.com> wrote:
>> Dear list,
>>
>> I have x,y,z data originating from an electromagnetic simulation on an
>> irregular mesh. I wish to visualise a 2D map of these (N~1e6) data
>> points with colour-coded z intensity.
>>
>> On way is to perform an interpolation onto a regular grid. This works
>> (using akima's interp for example), but is not what I'm after. The xy
>> mesh is very fine at places, and rough at others, and interpolation
>> loses this spatial information. Thus, I wish to display coloured tiles
>> of irregular size. Voronoi and delaunay tesselations seem quite suited
>> to the task; however I am stuck with two technical difficulties.
>>
>> 1- while voronoi/dirichlet gives me N polygons, delaunay obviously
>> returns more triangular tiles than there are points initially. How
>> could I perform the interpolation for the assignment of colours?
>>
>> 2- spatstat and maptools seem to work with special|spatial| classes,
>> from which I have been unable to efficiently extract the polygon
>> outlines in a data.frame. This is necessary because I wish to use
>> lattice or ggplot2 for plotting, rather than the default plot method.
>>
>> Below's a minimal illustration,
>>
>> N <- 100
>> set.seed(123)
>> d <- data.frame(x=rnorm(N, 15, 4), y=rnorm(N, 15, 4))
>> ## a circular intensity pattern
>> d$z = (d$x - 15)^2 + (d$y - 15)^2
>>
>> library(spatstat)
>> library(maptools)
>>
>> W <- ripras(df, shape="rectangle")
>> W <- owin(c(0, 30), c(0, 30))
>> X <- as.ppp(d, W=W)
>> Y <- dirichlet(X)
>> Z <- as(Y, "SpatialPolygons")
>> ## default plot
>> ## plot(Z, col=grey(d$z/max(d$z)))
>>
>> ## awfully inefficient extraction of the polygons...
>>
>> extract.tiles <- function(x){
>> ?data.frame(x$bdry[[1]][1:2])
>> }
>>
>> all <- lapply(Y$tiles, extract.tiles )
>>
>> library(ggplot2)
>> ## convert list to long format data.frame with ids
>> names(all) <- seq_along(all)
>> m <- melt(all, id=1:2)
>> m$fill <- d$z[as.integer(m$L1)]
>>
>> ggplot(m)+
>> ?geom_polygon(aes(x,y,fill=fill, group=fill))+
>> ?scale_fill_gradient(low="black", high="white")
>>
>> library(latticeExtra)
>> ## result similar (except for the edges) to latticeExtra's
>> panel.voronoi, as intended
>> levelplot(z~x*y, data=d,
>> ? ? ? ? panel = function(...) panel.voronoi(..., points=FALSE),
>> ? ? ? ? col.regions = colorRampPalette(grey(0:1))(1e3), cut=1e3)
>>
>>
>> ## now trying the same with delaunay tiles
>> Y1 <- delaunay(X)
>> Z1 <- as(Y1, "SpatialPolygons")
>> ## no link between original z-values and tiles
>> plot(Z1, col=grey(d$z/max(d$z)))
>>
>>
>> Best regards,
>>
>> baptiste
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
>
> --
> Assistant Professor / Dobelman Family Junior Chair
> Department of Statistics / Rice University
> http://had.co.nz/
>


From r.turner at auckland.ac.nz  Thu Apr 14 00:08:37 2011
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 14 Apr 2011 10:08:37 +1200
Subject: [R-sig-Geo] colour mapping tesselated irregular xy data
In-Reply-To: <BANLkTikxF7zj_ouzfX2VrZn=o7t6o8PwoA@mail.gmail.com>
References: <BANLkTikT1gufRwFKav_n-hp7jX5D9P0vXw@mail.gmail.com>	<BANLkTinHPjXN9K6QgTn7vLCdCTU+fmCc_w@mail.gmail.com>
	<BANLkTikxF7zj_ouzfX2VrZn=o7t6o8PwoA@mail.gmail.com>
Message-ID: <4DA61EE5.8080607@auckland.ac.nz>

On 14/04/11 09:48, baptiste auguie wrote:
> Thanks, it is cleaner indeed. However, it's still painfully slow on
> large data sets (typically a couple hundred thousand points). Looking
> at the source of various functions, it looks like the place to look at
> would be deldir::tile.list(). To my naive eyes it doesn't look
> particularly efficient, given that the "meat" of the calculation that
> I expected more costly was performed by deldir() in much less time.

You are very likely correct; tile.list() was an add-on, which was appended
to the deldir package in a rather cobbled-together manner at the request
of a user.  Its functionality probably should be incorporated into the 
output
process of the basic deldir structure, in the Fortran code.

I've been vaguely thinking about doing something along these lines for a
while.  But this would require that I revive my understanding of the Fortran
code which I wrote lo, these many years ago.  (Like about thirty (!) for the
original pass at the code.)  So it would be a non-trivial undertaking.  
I don't
know when I'll be able to get at it, but it certainly won't be less than 
3 or 4
months from now.

Feel free to nudge me about it, from time to time! :-)

     cheers,

         Rolf Turner


From lestes at princeton.edu  Thu Apr 14 16:02:41 2011
From: lestes at princeton.edu (Lyndon Estes)
Date: Thu, 14 Apr 2011 10:02:41 -0400
Subject: [R-sig-Geo] subs (raster) on large dataset
Message-ID: <BANLkTims5qZE1XBcyrU9Y=tYmzaBC0+UPQ@mail.gmail.com>

Dear List,

I am using a function to assign the results of a crop model simulation
to a raster, which I then aggregate, and finally resample to the
resolution of another grid. This involves the subs function of raster,
and the raster whose values I am replacing is very large (ncells =
263411880).  Despite this, I have successfully run subs on this same
dataset in the past.  This time, however, it is running for over 9
hours without completing (because I kill R in order get other work
done), and I am on a MacBook Pro with a 2.4 Ghz Core 2 Duo processor
and 4 GB of ram, with R 2.12.2 and raster v 1.8-9. I am thus trying to
figure out if there is something wrong with my coding.  First some
code/results, followed by my specific questions:

Here's the function:

spatCSM <- function(spatialmaster, resamplegrid, res.factor, CSMtable,
LUfield, outfields, outnames, type) {
# Creates spatial output grids from results produced by runCSM. Path
needs to be set in advance
# Args:
#   spatialmaster: A grid defining the location of each unique land
unit (climate-soil combination)
#   resamplegrid: An optional grid defining the resolution to which
results should be resampled
#   resamplefactor: Factor by which to aggregate (e.g. 10 times
current pixel size, the default if not
#     specified)
#   CSMtable: The output table of CSM statistics generated by runCSM
#   LUfield: The field in CSMtable containing the land unit codes that
match values in spatialmaster grid
#   outfields: A vector of column names in CSMtable for which gridded
outputs are wanted, e.g. yield, CV yield
#   outnames: A vector of names for writing output grids, one for each
specifed grid
#   type: Output datatype for grid, e.g. INT2S
# Returns: R format raster grids, saved to disk, of mean yield and
other optional grids

 library(raster)
 if(missing(spatialmaster) | missing(CSMtable) | missing(LUfield) |
    missing(outfields) | missing(type)) {
    stop("Missing parameter, check function list.")
 }
 if(length(outfields) != length(outnames)) {
    stop("Number of names for output grids does not match number of
specified output grids")
 }

 for(i in 1:length(outfields)) {

   #CSMtable[, outfields[i]] <- as.numeric(CSMtable[, outfields[i]])
# Convert to numeric

   out.nm <- paste(outnames[i], ".gri", sep = "")
   subs(spatialmaster, CSMtable, by = LUfield, which = outfields[i],
subsWithNA = TRUE, filename =
                  out.nm, datatype = type, overwrite = TRUE)
   if(!missing(resamplegrid)) {
     cat("Aggregating")
     if(missing(res.factor)) {
       res.factor <- 10
     }
     subs.g <- raster(out.nm)
     out.nm2 <- paste("agg.", out.nm, sep = "")
     aggregate(subs.g, res.factor, fun = mean, na.rm = TRUE,
                             filename = out.nm2, datatype = type,
overwrite = TRUE)
     subs.g.agg <- raster(out.nm2)
     out.nm3 <- paste(outnames[i], ".",
as.integer(res(resamplegrid)[1]), ".gri", sep = "")
     resample(subs.g.agg, resamplegrid, method = "bilinear", filename
= out.nm3, datatype = type,
              overwrite = TRUE)
   }
 }
}

# Function call
spatCSM(spatialmaster = ltq.g, resamplegrid = base.g2, res.factor =
10, CSMtable = sum.tab.c,
       LUfield = "FID", outfields = c("y.mn"), outnames = c("f.yld"),
type = "INT2S")

Inputs

ltq.g
class       : RasterLayer
dimensions  : 15170, 17364, 263411880  (nrow, ncol, ncell)
resolution  : 92.73961, 92.73961  (x, y)
extent      : -733488.1, 876842.5, -3806988, -2400128  (xmin, xmax, ymin, ymax)
projection  : +proj=aea +lat_1=-18 +lat_2=-32 +lat_0=0 +lon_0=24
+x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs
+towgs84=0,0,0
values      : lt.quin.3f.grd
min value   : 1
max value   : 136050

base.g2
class       : RasterLayer
dimensions  : 1651, 1937, 3197987  (nrow, ncol, ncell)
resolution  : 926.6254, 926.6254  (x, y)
extent      : -869378.3, 925495.2, -3887679, -2357820  (xmin, xmax, ymin, ymax)
projection  : +proj=aea +lat_1=-18 +lat_2=-32 +lat_0=0 +lon_0=24
+x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs
+towgs84=0,0,0
values      : in memory
min value   : 1
max value   : 1

sum.tab.c
    FID CR WSTA.... SOIL_ID...     YEARS pre.mn y.mn y.med y.cv y.10
1  16276 MZ AAB_7921 LTAE003301 1980-1998    439 1275  1265   69    0
2  16135 MZ AAB_7921 LTAE003303 1980-1998    422 2071  2295   67  111
3  16820 MZ AAA_7921 LTAE003303 1980-1998    427 2083  2374   66  123
4  16948 MZ AAA_7921 LTAE003304 1980-1998    458 3320  3828   35 1198
5  16149 MZ AAB_7921 LTAE003304 1980-1998    453 3277  3780   37 1086
6  16247 MZ AAB_7921 LTAE003305 1980-1998    423 1420  1645   74    0
7  17414 MZ AAC_7921 LTAE003305 1980-1998    419 1467  1637   68   79
8  17017 MZ AAA_7921 LTAH001401 1980-1998    452 3378  3598   29 1808
9  17016 MZ AAA_7921 LTAH001403 1980-1998    453 3250  3579   32 1507
10 17015 MZ AAA_7921 LTAH001404 1980-1998    425 1607  1918   75    0


Note that I have been testing this function with only a very small
input table providing the values to be substituted (sum.tab.c).  This
is because the crop model itself takes a long while to run for each
location provided in ltq.g.

My first question is whether the small table is causing a problem?  I
figured it wouldn't, since subsWithNA is set to TRUE.  But perhaps it
does.

If that is unlikely, then have I somehow coded this in a way that R is
trying to perform the subs entirely with memory (e.g. because I have
placed subs within a function)?

If neither of these are the case, then I will appreciate any pointers
for how I can make this work more quickly.

I apologize for not providing a fully reproducible example, but the
grid in question is 1 GB, and since the size of it seems to be the
main issue (a version of this grid cut down to the area covered by
sum.tab.c works fine), I don't want to burden anyone with such a large
file.

Thanks in advance for any advice you can provide.

Cheers, Lyndon



-- 
Lyndon Estes
Research Associate
Woodrow Wilson School
Princeton University
+1-609-258-2392 (o)
+1-609-258-6082 (f)
+1-202-431-0496 (m)
lestes at princeton.edu


From naimi at itc.nl  Thu Apr 14 16:34:59 2011
From: naimi at itc.nl (Babak Naimi)
Date: Thu, 14 Apr 2011 16:34:59 +0200
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
In-Reply-To: <alpine.LRH.2.00.1104132306140.19364@reclus.nhh.no>
References: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>
	<BANLkTi=XjRcU51twwV=jcw6G7Eqh2HPcHQ@mail.gmail.com>
	<alpine.LRH.2.00.1104132306140.19364@reclus.nhh.no>
Message-ID: <021F462EDC7F2B42A7038E9F9D4782611F18A40694@itcnt27.itc.nl>

Dear Robert and Roger,

Thank both of you. After all, I came up with the following function (focalMoran) that efficiently works. I tested it for a raster layer with 122345 not-NA cells, and compared with locmor (based on localmoran in spdep package). The results are almost identical (cor = 0.99987). I think the only difference between two function is that in the focalMoran, the center pixel is omitted in calculation of global mean and variance. However, focalMoran was so quicker than locmor. Following you will find the difference in computation as well as the functions.

Many thanks,
Babak


b1<-raster(----) # a raster with dimension 355, 617, 219035  (nrow, ncol, ncell) and resolution of 843.8446 m
filter<-matrix(1,nc=9,nr=9)

system.time(
b.foc<-focalMoran(b1,filter)
)
   user  system elapsed 
  44.63    0.45   48.91 

system.time(
b.spdep<-locmor(b1,d1=0,d2=4863.92)
)
   user  system elapsed 
5738.56    6.02 5832.11

###########################

focalMoran<-function(ras,filter) {
	globsum <- cellStats(ras, sum)
	glob.sqsum <- cellStats(ras^2, sum)
	n<-length(na.omit(ras[]))
	lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
		i <- trunc(length(x)/2)+1
	     	xbar=(sigmaX-x[i])/(N-1)
		s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
		z <- x - xbar
		lz <- mean(z[-i], na.rm=na.rm)
	    	return((z[i]/s2) * lz)
		}
	ras2 <- raster::expand(ras, extent(ras)+dim(filter)[1]*res(ras))
	ras2 <- focalFilter(ras2,filter,lomo) 
	ras2 <- crop(ras2, ras)
	return(ras2) 
}
#--------
locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep 
        df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)])) 
        df<-na.omit(df)
        df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2) 
        df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list 
        lmor <- localmoran(df$values, df.listw) 
        ras[df$cells]<-as.data.frame(lmor)[[1]] 
        return(ras) 
}




-----Original Message-----
From: Roger Bivand [mailto:Roger.Bivand at nhh.no] 
Sent: Wednesday, April 13, 2011 11:15 PM
To: Robert J. Hijmans
Cc: Babak Naimi; r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter

On Wed, 13 Apr 2011, Robert J. Hijmans wrote:

> Babak,
>
> I looked at the original paper,
> http://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.1995.tb00338.x/abstract,
> and the spdep manual, and other places, but they all seemed to be
> saying somewhat different on the local moran statistic. I followed the
> logic of the spdep::localmoran function, assuming that this is
> "correct" and I should have phrased my answer more carefully: "based
> onthe spdep::localmoran function, mean and variance should be
> global...".  I do note that the two results are different but that
> their correlation is very high R2 = 0.97; so perhaps they are both
> right. Perhaps Roger can weigh in on this.

I don't have access to my library now. My understanding is that you are 
short-circuiting the construction of the spatial weights (or the n 
separate star matrices, one for each i, in Michael Tiefeldorf's terms). In 
general, Moran's I is not often used with non-zero values on the diagonal 
of the weights matrix. Non-zero values are seen in the local Getis-Ord 
Gi_star, but not in local Gi. My guess (without references) would be that 
the weight for i itself should be zero (the centre cell of the raster). 
The difference between mean() and sum() is the difference between 
row-standardised (summing to 1) and binary (all unity) weights. The 
function as it is doesn't permit other weighting schemes (such as IDW in 
either row-standardised or binary form).

Hope this helps,

Roger

>
> To make it run, in your code I replaced this:
> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
> filter<-matrix(f,nc=3,nr=3,byrow=T)
>
> with:
> filter<-matrix(1 ,nc=3,nr=3)
>
> Robert
>
>
> On Wed, Apr 13, 2011 at 10:46 AM, Babak Naimi <naimi at itc.nl> wrote:
>> Dear Robert,
>>
>> Thanks for your reply. Now, the result is the same as with localmoran in spdep package. But I checked this with Getis & Ord (1996), seems the formula, as you implement in the script, is not the same as what has been explained in the Getis & Ord. First difference is that, Xi is excluded in calculation of the global mean and variance. The second difference is that, in the line "lz <- mean(z[-i], na.rm=na.rm)", 'mean' should be replaced with 'sum'. I re-implemented this function. The example we used here came from that reference (p.266). The calculation for the cell 136 has also been provided in the book. The result that this new function returns is the same as with the book, now. But the strange thing to me is that it is not the same as spdep. Any comment?

>>
>> Babak
>>
>>
>> Getis,A. and Ord, J. K. 1996 Local spatial statistics: an overview. In P. Longley and M. Batty (eds) Spatial
>> analysis: modelling in a GIS environment (Cambridge: Geoinformation International), 261-277.
>>
>> #----------------------------------------------
>> focalMoran<-function(ras,filter) {
>> ? ? ? ?globsum <- cellStats(ras, sum)
>> ? ? ? ?glob.sqsum <- cellStats(ras^2, sum)
>> ? ? ? ?n<-ncell(tmp)
>> ? ? ? ?lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
>> ? ? ? ? ? ? ? ?i <- trunc(length(x)/2)+1
>> ? ? ? ? ? ? ? ?xbar=(sigmaX-x[i])/(N-1)
>> ? ? ? ? ? ? ? ?s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
>> ? ? ? ? ? ? ? ?z <- x - xbar
>> ? ? ? ? ? ? ? ?lz <- sum(z[-i], na.rm=na.rm)
>> ? ? ? ? ? ? ? ?return((z[i]/s2) * lz)
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ?ras2 <- raster::expand(ras, extent(ras)+2*res(ras))
>> ? ? ? ?ras2 <- focalFilter(ras2,filter,lomo)
>> ? ? ? ?ras2 <- crop(ras2, ras)
>> ? ? ? ?return(ras2)
>> }
>> #--------
>> locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep
>> ? ? ? ?require(raster)
>> ? ? ? ?require(spdep)
>> ? ? ? ?df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>> ? ? ? ?df<-na.omit(df)
>> ? ? ? ?df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2)
>> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list
>> ? ? ? ?lmor <- localmoran(df$values, df.listw)
>> ? ? ? ?ras[df$cells]<-as.data.frame(lmor)[[1]]
>> ? ? ? ?return(ras)
>> }
>>
>> #####---------------
>>
>> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
>> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
>> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
>> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
>> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
>> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
>> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
>> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
>> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
>> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
>> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
>> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
>> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
>> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
>> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
>> tmp <- raster(tmp)
>>
>>
>> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
>> filter<-matrix(f,nc=3,nr=3,byrow=T)
>>
>> tmp.focal<-focalMoran(tmp,filter)
>> tmp.spdep<-locmor(tmp,d1=0,d2=0.0625)
>>
>> tmp.focal[136] # the calculation for this cell has been provided in Getis and Ord(1996, p.268)
>> tmp.spdep[136]
>>
>>
>> par(mfrow=c(1,2))
>> plot(tmp.focal)
>> plot(tmp.spdep)
>>
>> #------------------------------------------
>> #-----------------------------------------
>>
>>
>>
>> Date: Tue, 12 Apr 2011 15:14:05 -0700 (PDT)
>> From: Robert Hijmans <r.hijmans at gmail.com>
>> To: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter
>> Message-ID: <1302646445119-6266969.post at n2.nabble.com>
>> Content-Type: text/plain; charset=us-ascii
>>
>>> Dear All,
>>>
>>> I tried to write a function that can be used in focalFilter of raster
>>> package, but the output
>>> result is different from what was calculated using spdep for a sample
>>> dataset.
>>> It might be my mistake in the formula that is used in the function.
>>> Following you will
>>> find both functions as well as the resultss for a sample raster.
>>>
>>> I would appreciate if anyone can help.
>>>
>>> Best regards,
>>> Babak
>>
>>
>> Babak,
>> I think the mistake you made is that the local moran uses the _global_ mean
>> and the _global_ variance, whereas you used the _local_ mean and the _local_
>> variance. I have changed your function and now the result with focalFilter
>> is the same as with spdep.
>> Robert
>>
>>
>>
>>
>>
>> locmor<-function(ras,d1=0,d2=6000) { # function to calculate local Moran
>> using spdep
>> ? ? ? ?require(raster)
>> ? ? ? ?require(spdep)
>>
>> df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>> ? ? ? ?df.sub<-subset(df,!is.na(values))
>> ? ? ? ?df.nb <- dnearneigh(as.matrix(df.sub[,2:3]), d1, d2)
>> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a
>> weighted list
>> ? ? ? ?lmor <- localmoran(df.sub$values, df.listw)
>> ? ? ? ?ras[df.sub$cells]<-as.data.frame(lmor)[[1]]
>> ? ? ? ?return(ras)
>> }
>>
>> #---------------
>>
>> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
>> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
>> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
>> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
>> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
>> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
>> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
>> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
>> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
>> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
>> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
>> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
>> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
>> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
>> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
>> tmp <- raster(tmp)
>>
>>
>> tmp.spdep<-locmor(tmp,d1=0,d2=0.08838835)
>>
>> # new function for use with focalFilter
>> # first compute global mean and global variance
>> globmean <- cellStats(tmp, mean)
>> globvar <- ?cellStats(tmp, sd)^2
>> # adjust variance denominator from n-1 to n
>> globvar <- (globvar * (ncell(tmp)-1)) / ncell(tmp)
>>
>> lomo <- function (x, na.rm=TRUE, xbar=globmean, s2=globvar) {
>> ? ? ? ?i <- trunc(length(x)/2)+1
>> ? ?z <- x - globmean
>> ? ? ? ?lz <- mean(z[-i], na.rm=na.rm)
>> ? ?(z[i]/s2) * lz
>> }
>>
>> filter<-matrix(1, nrow=3, ncol=3)
>> tmp.focal<-focalFilter(tmp,filter,lomo)
>>
>> par(mfrow=c(1,2))
>> plot(tmp.spdep)
>> plot(tmp.focal)
>>
>> # note that you could, but should not, do this:
>> tmp.focal<-focal(tmp,fun=lomo)
>> # because ?the trick to get the focal cell
>> # ? ? i <- trunc(length(x)/2)+1
>> # does not work for the border cells.
>> # To get the border cells:
>>
>> tmp2 <- raster::expand(tmp, extent(tmp)+2*res(tmp))
>> tmpfoc2 <- focalFilter(tmp2,filter,lomo)
>> tmpfoc2 <- crop(tmpfoc2, tmp)
>>
>> par(mfrow=c(1,2))
>> plot(tmp.spdep)
>> plot(tmpfoc2)
>>
>>
>>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From tom.gottfried at wzw.tum.de  Thu Apr 14 17:30:44 2011
From: tom.gottfried at wzw.tum.de (Tom Gottfried)
Date: Thu, 14 Apr 2011 17:30:44 +0200
Subject: [R-sig-Geo] memory allocation problem with small PostGIS-layer
 in readOGR()
In-Reply-To: <alpine.LRH.2.00.1104121530050.12743@reclus.nhh.no>
References: <4DA43842.7000701@wzw.tum.de>
	<alpine.LRH.2.00.1104121530050.12743@reclus.nhh.no>
Message-ID: <4DA71324.9010205@wzw.tum.de>

Solved with Rogers help. It turned out that I had updated GDAL and did not rebuild rgdal, so 
rebuilding rgdal did it.

Tom

Am 12.04.2011 15:39, schrieb Roger Bivand:
> On Tue, 12 Apr 2011, Tom Gottfried wrote:
>
>> Dear list,
>>
>> since some upgrades of R-packages and GDAL (I'm sorry I can not exactly reproduce which) I got
>> memory allocation problems reading a relatively small PostGIS layer (type POLYGON with 21 polygons):
>>
>>> library(rgdal)
>> Loading required package: sp
>> Geospatial Data Abstraction Library extensions to R successfully loaded
>> Loaded GDAL runtime: GDAL 1.8.0, released 2011/01/12
>> Path to GDAL shared files:
>> Loaded PROJ.4 runtime: Rel. 4.7.1, 23 September 2009
>> Path to PROJ.4 shared files: (autodetected)
>>
>>> readOGR("PG:dbname=myDB", "layer(the_geom)")
>> Error: cannot allocate vector of size 13.9 Gb
>
> Since this has to be debugged on your system, please first use ogr2ogr to convert it to another
> format - this will test the PostGIS driver. You are on Linux, so should have ogr2ogr installed. If
> (unlikely) there are problems with the driver, you may need to check how PostgreSQL, PostGIS, and
> their dependencies were installed, then GDAL and its dependencies.
>
> Otherwise, this must be in readOGR() somewhere, so do debug(readOGR) and run the same command
> step-by-step. Start by doing debug(ogrInfo) and ogrInfo("PG:dbname=myDB", "layer(the_geom)"),
> because unless you have set verbose=FALSE and not reported it (unlikely), ogrInfo() called within
> readOGR() should have reported the DSN and layer names, the driver name, etc., before the error if
> the error occurred later Also look at traceback() after the error before trying debug() - this feels
> like ogrInfo() having trouble.
>
> Roger
>
>>
>>> sessionInfo()
>> R version 2.12.2 Patched (2011-03-18 r55401)
>> Platform: x86_64-unknown-linux-gnu (64-bit)
>>
>> locale:
>> [1] LC_CTYPE=en_US LC_NUMERIC=C LC_TIME=en_US
>> [4] LC_COLLATE=en_US LC_MONETARY=C LC_MESSAGES=en_US
>> [7] LC_PAPER=en_US LC_NAME=C LC_ADDRESS=C
>> [10] LC_TELEPHONE=C LC_MEASUREMENT=en_US LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] stats graphics grDevices utils methods base
>>
>> other attached packages:
>> [1] rgdal_0.6-33 sp_0.9-79 RODBC_1.3-2
>>
>> loaded via a namespace (and not attached):
>> [1] grid_2.12.2 lattice_0.19-17
>>
>> Any help is appreciated!
>> Thanks,
>> Tom
>>
>>
>

-- 
Technische Universit?t M?nchen
Department f?r Pflanzenwissenschaften
Lehrstuhl f?r Gr?nlandlehre
Alte Akademie 12
85350 Freising / Germany
Phone: ++49 (0)8161 715324
Fax:   ++49 (0)8161 713243
email: tom.gottfried at wzw.tum.de
http://www.wzw.tum.de/gruenland


From r.hijmans at gmail.com  Thu Apr 14 19:19:48 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Thu, 14 Apr 2011 10:19:48 -0700
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
In-Reply-To: <021F462EDC7F2B42A7038E9F9D4782611F18A40694@itcnt27.itc.nl>
References: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>
	<BANLkTi=XjRcU51twwV=jcw6G7Eqh2HPcHQ@mail.gmail.com>
	<alpine.LRH.2.00.1104132306140.19364@reclus.nhh.no>
	<021F462EDC7F2B42A7038E9F9D4782611F18A40694@itcnt27.itc.nl>
Message-ID: <BANLkTinCFz21m8-Vc8Z+f5VXCEUeXkBb9w@mail.gmail.com>

Hi Babak,

I would propose the below for the local Moran with a RasterLayer
because it deals with NA values in a memory safe manner (and will be a
bit slower because of that)

library(raster)

MoranLocal <- function(x) {
	n <- ncell(x) - cellStats(x, 'countNA')
	s2 <-  cellStats(x, sd)^2
	# adjust variance denominator from n-1 to n
	s2 <- (s2 * (n-1)) / n
	z  <- x - cellStats(x, mean)
	#weights
	w  <- focal( z, fun=function(x, ...){ sum(!is.na(x))-1 } )
	lz <- (focal(z, fun='sum', na.rm=TRUE) - z) / w
	(z / s2) * lz
}


I looked up the Getis and Ord paper you mention (I happen to have the
book), and they do indeed subtract the focal cell from the global mean
& variance. That seems intuitive. But I do not think it was is
suggested in Anselin's LISA paper (Formula 12)  (and implemented in
spdep).

Also, it should be true that  globalMoran == mean(localMoran)

I think the below produces the global Moran (for the queen case
neighboring grid cells)

Moran <- function(x) {
	z <- x - cellStats(x, mean)
	wZiZj <- focal(z, fun='sum', na.rm=TRUE)
	wZiZj <- overlay(wZiZj, z, fun=function(x,y){ (x-y) * y })
	wZiZj <- cellStats(wZiZj, sum)
	z2 <- cellStats(z*z, sum)
	n <- ncell(z) - cellStats(z, 'countNA')
	# weights
	w <- focal( z, fun=function(x, ...){ max(0, sum(!is.na(x))-1) } )
	NS0 <- n / cellStats(w, sum)
	mI <- NS0 * wZiZj / z2
	return(mI)
}

With your 'tmp' data, I get:

> Moran(tmp)
[1] 0.8526906

> cellStats(MoranLocal(tmp), mean)
[1] 0.8192615

Close, but not quite the same.

I'll put these functions in the raster package.

Robert


On Thu, Apr 14, 2011 at 7:34 AM, Babak Naimi <naimi at itc.nl> wrote:
> Dear Robert and Roger,
>
> Thank both of you. After all, I came up with the following function (focalMoran) that efficiently works. I tested it for a raster layer with 122345 not-NA cells, and compared with locmor (based on localmoran in spdep package). The results are almost identical (cor = 0.99987). I think the only difference between two function is that in the focalMoran, the center pixel is omitted in calculation of global mean and variance. However, focalMoran was so quicker than locmor. Following you will find the difference in computation as well as the functions.
>
> Many thanks,
> Babak
>
>
> b1<-raster(----) # a raster with dimension 355, 617, 219035 ?(nrow, ncol, ncell) and resolution of 843.8446 m
> filter<-matrix(1,nc=9,nr=9)
>
> system.time(
> b.foc<-focalMoran(b1,filter)
> )
> ? user ?system elapsed
> ?44.63 ? ?0.45 ? 48.91
>
> system.time(
> b.spdep<-locmor(b1,d1=0,d2=4863.92)
> )
> ? user ?system elapsed
> 5738.56 ? ?6.02 5832.11
>
> ###########################
>
> focalMoran<-function(ras,filter) {
> ? ? ? ?globsum <- cellStats(ras, sum)
> ? ? ? ?glob.sqsum <- cellStats(ras^2, sum)
> ? ? ? ?n<-length(na.omit(ras[]))
> ? ? ? ?lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
> ? ? ? ? ? ? ? ?i <- trunc(length(x)/2)+1
> ? ? ? ? ? ? ? ?xbar=(sigmaX-x[i])/(N-1)
> ? ? ? ? ? ? ? ?s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
> ? ? ? ? ? ? ? ?z <- x - xbar
> ? ? ? ? ? ? ? ?lz <- mean(z[-i], na.rm=na.rm)
> ? ? ? ? ? ? ? ?return((z[i]/s2) * lz)
> ? ? ? ? ? ? ? ?}
> ? ? ? ?ras2 <- raster::expand(ras, extent(ras)+dim(filter)[1]*res(ras))
> ? ? ? ?ras2 <- focalFilter(ras2,filter,lomo)
> ? ? ? ?ras2 <- crop(ras2, ras)
> ? ? ? ?return(ras2)
> }
> #--------
> locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep
> ? ? ? ?df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
> ? ? ? ?df<-na.omit(df)
> ? ? ? ?df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2)
> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list
> ? ? ? ?lmor <- localmoran(df$values, df.listw)
> ? ? ? ?ras[df$cells]<-as.data.frame(lmor)[[1]]
> ? ? ? ?return(ras)
> }
>
>
>
>
> -----Original Message-----
> From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> Sent: Wednesday, April 13, 2011 11:15 PM
> To: Robert J. Hijmans
> Cc: Babak Naimi; r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter
>
> On Wed, 13 Apr 2011, Robert J. Hijmans wrote:
>
>> Babak,
>>
>> I looked at the original paper,
>> http://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.1995.tb00338.x/abstract,
>> and the spdep manual, and other places, but they all seemed to be
>> saying somewhat different on the local moran statistic. I followed the
>> logic of the spdep::localmoran function, assuming that this is
>> "correct" and I should have phrased my answer more carefully: "based
>> onthe spdep::localmoran function, mean and variance should be
>> global...". ?I do note that the two results are different but that
>> their correlation is very high R2 = 0.97; so perhaps they are both
>> right. Perhaps Roger can weigh in on this.
>
> I don't have access to my library now. My understanding is that you are
> short-circuiting the construction of the spatial weights (or the n
> separate star matrices, one for each i, in Michael Tiefeldorf's terms). In
> general, Moran's I is not often used with non-zero values on the diagonal
> of the weights matrix. Non-zero values are seen in the local Getis-Ord
> Gi_star, but not in local Gi. My guess (without references) would be that
> the weight for i itself should be zero (the centre cell of the raster).
> The difference between mean() and sum() is the difference between
> row-standardised (summing to 1) and binary (all unity) weights. The
> function as it is doesn't permit other weighting schemes (such as IDW in
> either row-standardised or binary form).
>
> Hope this helps,
>
> Roger
>
>>
>> To make it run, in your code I replaced this:
>> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
>> filter<-matrix(f,nc=3,nr=3,byrow=T)
>>
>> with:
>> filter<-matrix(1 ,nc=3,nr=3)
>>
>> Robert
>>
>>
>> On Wed, Apr 13, 2011 at 10:46 AM, Babak Naimi <naimi at itc.nl> wrote:
>>> Dear Robert,
>>>
>>> Thanks for your reply. Now, the result is the same as with localmoran in spdep package. But I checked this with Getis & Ord (1996), seems the formula, as you implement in the script, is not the same as what has been explained in the Getis & Ord. First difference is that, Xi is excluded in calculation of the global mean and variance. The second difference is that, in the line "lz <- mean(z[-i], na.rm=na.rm)", 'mean' should be replaced with 'sum'. I re-implemented this function. The example we used here came from that reference (p.266). The calculation for the cell 136 has also been provided in the book. The result that this new function returns is the same as with the book, now. But the strange thing to me is that it is not the same as spdep. Any comment?
>
>>>
>>> Babak
>>>
>>>
>>> Getis,A. and Ord, J. K. 1996 Local spatial statistics: an overview. In P. Longley and M. Batty (eds) Spatial
>>> analysis: modelling in a GIS environment (Cambridge: Geoinformation International), 261-277.
>>>
>>> #----------------------------------------------
>>> focalMoran<-function(ras,filter) {
>>> ? ? ? ?globsum <- cellStats(ras, sum)
>>> ? ? ? ?glob.sqsum <- cellStats(ras^2, sum)
>>> ? ? ? ?n<-ncell(tmp)
>>> ? ? ? ?lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
>>> ? ? ? ? ? ? ? ?i <- trunc(length(x)/2)+1
>>> ? ? ? ? ? ? ? ?xbar=(sigmaX-x[i])/(N-1)
>>> ? ? ? ? ? ? ? ?s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
>>> ? ? ? ? ? ? ? ?z <- x - xbar
>>> ? ? ? ? ? ? ? ?lz <- sum(z[-i], na.rm=na.rm)
>>> ? ? ? ? ? ? ? ?return((z[i]/s2) * lz)
>>> ? ? ? ? ? ? ? ?}
>>> ? ? ? ?ras2 <- raster::expand(ras, extent(ras)+2*res(ras))
>>> ? ? ? ?ras2 <- focalFilter(ras2,filter,lomo)
>>> ? ? ? ?ras2 <- crop(ras2, ras)
>>> ? ? ? ?return(ras2)
>>> }
>>> #--------
>>> locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep
>>> ? ? ? ?require(raster)
>>> ? ? ? ?require(spdep)
>>> ? ? ? ?df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>>> ? ? ? ?df<-na.omit(df)
>>> ? ? ? ?df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2)
>>> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list
>>> ? ? ? ?lmor <- localmoran(df$values, df.listw)
>>> ? ? ? ?ras[df$cells]<-as.data.frame(lmor)[[1]]
>>> ? ? ? ?return(ras)
>>> }
>>>
>>> #####---------------
>>>
>>> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
>>> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
>>> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
>>> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
>>> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
>>> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
>>> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
>>> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
>>> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
>>> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
>>> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
>>> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
>>> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
>>> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
>>> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
>>> tmp <- raster(tmp)
>>>
>>>
>>> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
>>> filter<-matrix(f,nc=3,nr=3,byrow=T)
>>>
>>> tmp.focal<-focalMoran(tmp,filter)
>>> tmp.spdep<-locmor(tmp,d1=0,d2=0.0625)
>>>
>>> tmp.focal[136] # the calculation for this cell has been provided in Getis and Ord(1996, p.268)
>>> tmp.spdep[136]
>>>
>>>
>>> par(mfrow=c(1,2))
>>> plot(tmp.focal)
>>> plot(tmp.spdep)
>>>
>>> #------------------------------------------
>>> #-----------------------------------------
>>>
>>>
>>>
>>> Date: Tue, 12 Apr 2011 15:14:05 -0700 (PDT)
>>> From: Robert Hijmans <r.hijmans at gmail.com>
>>> To: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter
>>> Message-ID: <1302646445119-6266969.post at n2.nabble.com>
>>> Content-Type: text/plain; charset=us-ascii
>>>
>>>> Dear All,
>>>>
>>>> I tried to write a function that can be used in focalFilter of raster
>>>> package, but the output
>>>> result is different from what was calculated using spdep for a sample
>>>> dataset.
>>>> It might be my mistake in the formula that is used in the function.
>>>> Following you will
>>>> find both functions as well as the resultss for a sample raster.
>>>>
>>>> I would appreciate if anyone can help.
>>>>
>>>> Best regards,
>>>> Babak
>>>
>>>
>>> Babak,
>>> I think the mistake you made is that the local moran uses the _global_ mean
>>> and the _global_ variance, whereas you used the _local_ mean and the _local_
>>> variance. I have changed your function and now the result with focalFilter
>>> is the same as with spdep.
>>> Robert
>>>
>>>
>>>
>>>
>>>
>>> locmor<-function(ras,d1=0,d2=6000) { # function to calculate local Moran
>>> using spdep
>>> ? ? ? ?require(raster)
>>> ? ? ? ?require(spdep)
>>>
>>> df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>>> ? ? ? ?df.sub<-subset(df,!is.na(values))
>>> ? ? ? ?df.nb <- dnearneigh(as.matrix(df.sub[,2:3]), d1, d2)
>>> ? ? ? ?df.listw <- nb2listw(df.nb) #turns neighbourhood object into a
>>> weighted list
>>> ? ? ? ?lmor <- localmoran(df.sub$values, df.listw)
>>> ? ? ? ?ras[df.sub$cells]<-as.data.frame(lmor)[[1]]
>>> ? ? ? ?return(ras)
>>> }
>>>
>>> #---------------
>>>
>>> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
>>> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
>>> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
>>> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
>>> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
>>> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
>>> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
>>> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
>>> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
>>> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
>>> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
>>> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
>>> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
>>> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
>>> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
>>> tmp <- raster(tmp)
>>>
>>>
>>> tmp.spdep<-locmor(tmp,d1=0,d2=0.08838835)
>>>
>>> # new function for use with focalFilter
>>> # first compute global mean and global variance
>>> globmean <- cellStats(tmp, mean)
>>> globvar <- ?cellStats(tmp, sd)^2
>>> # adjust variance denominator from n-1 to n
>>> globvar <- (globvar * (ncell(tmp)-1)) / ncell(tmp)
>>>
>>> lomo <- function (x, na.rm=TRUE, xbar=globmean, s2=globvar) {
>>> ? ? ? ?i <- trunc(length(x)/2)+1
>>> ? ?z <- x - globmean
>>> ? ? ? ?lz <- mean(z[-i], na.rm=na.rm)
>>> ? ?(z[i]/s2) * lz
>>> }
>>>
>>> filter<-matrix(1, nrow=3, ncol=3)
>>> tmp.focal<-focalFilter(tmp,filter,lomo)
>>>
>>> par(mfrow=c(1,2))
>>> plot(tmp.spdep)
>>> plot(tmp.focal)
>>>
>>> # note that you could, but should not, do this:
>>> tmp.focal<-focal(tmp,fun=lomo)
>>> # because ?the trick to get the focal cell
>>> # ? ? i <- trunc(length(x)/2)+1
>>> # does not work for the border cells.
>>> # To get the border cells:
>>>
>>> tmp2 <- raster::expand(tmp, extent(tmp)+2*res(tmp))
>>> tmpfoc2 <- focalFilter(tmp2,filter,lomo)
>>> tmpfoc2 <- crop(tmpfoc2, tmp)
>>>
>>> par(mfrow=c(1,2))
>>> plot(tmp.spdep)
>>> plot(tmpfoc2)
>>>
>>>
>>>
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> e-mail: Roger.Bivand at nhh.no
>


From r.hijmans at gmail.com  Thu Apr 14 19:43:52 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Thu, 14 Apr 2011 10:43:52 -0700 (PDT)
Subject: [R-sig-Geo] subs (raster) on large dataset
In-Reply-To: <BANLkTims5qZE1XBcyrU9Y=tYmzaBC0+UPQ@mail.gmail.com>
References: <BANLkTims5qZE1XBcyrU9Y=tYmzaBC0+UPQ@mail.gmail.com>
Message-ID: <1302803032937-6273741.post@n2.nabble.com>

Lyndon,

I have no idea what might be causing this. I would suggest to track down
where exactly this happens, by going, line by line, through the code of
'subs' 

getMethod("subs", c("RasterLayer", "data.frame"))

Presumably, something is not moving here:

for (i in 1:tr$n) {
     v <- getValues(x, row = tr$row[i], nrows = tr$size)
     r <- writeValues(r, localmerge(v, y, subsWithNA),  tr$row[i])
     pbStep(pb)
}

so you can try

 i <- 1
 v <- getValues(x, row = tr$row[i], nrows = tr$size)
 x <- localmerge(v, y, subsWithNA)

and if that last step is indeed very slow, have a step by step look at
"localmerge". 

You can then also see of it matters if you make changes to the "chunksize"
and "maxmemory" options ?setOptions. These will affect the size of "v" in
the code above, and perhaps there is a non-linear effect on the performance
of merge. 

Robert


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/subs-raster-on-large-dataset-tp6272926p6273741.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From naimi at itc.nl  Thu Apr 14 20:58:55 2011
From: naimi at itc.nl (Babak Naimi)
Date: Thu, 14 Apr 2011 20:58:55 +0200
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
In-Reply-To: <BANLkTinCFz21m8-Vc8Z+f5VXCEUeXkBb9w@mail.gmail.com>
References: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>
	<BANLkTi=XjRcU51twwV=jcw6G7Eqh2HPcHQ@mail.gmail.com>
	<alpine.LRH.2.00.1104132306140.19364@reclus.nhh.no>
	<021F462EDC7F2B42A7038E9F9D4782611F18A40694@itcnt27.itc.nl>
	<BANLkTinCFz21m8-Vc8Z+f5VXCEUeXkBb9w@mail.gmail.com>
Message-ID: <021F462EDC7F2B42A7038E9F9D4782611F18A40699@itcnt27.itc.nl>

Dear Robert,

Thanks. It is great. The only thing you may like to include in the functions is the neighbourhood size. So they can be like the below:


MoranLocal <- function(x,ngb=3) {
        n <- ncell(x) - cellStats(x, 'countNA')
        s2 <-  cellStats(x, sd)^2
        # adjust variance denominator from n-1 to n
        s2 <- (s2 * (n-1)) / n
        z  <- x - cellStats(x, mean)
        #weights
        w  <- focal( z, ngb=ngb,fun=function(x, ...){ sum(!is.na(x))-1 } )
        lz <- (focal(z, ngb=ngb,fun='sum', na.rm=TRUE) - z) / w
        (z / s2) * lz
}

Moran <- function(x,ngb=3) {
        z <- x - cellStats(x, mean)
        wZiZj <- focal(z,ngb=ngb, fun='sum', na.rm=TRUE)
        wZiZj <- overlay(wZiZj, z, fun=function(x,y){ (x-y) * y })
        wZiZj <- cellStats(wZiZj, sum)
        z2 <- cellStats(z*z, sum)
        n <- ncell(z) - cellStats(z, 'countNA')
        # weights
        w <- focal( z, ngb=ngb, fun=function(x, ...){ max(0, sum(!is.na(x))-1) } )
        NS0 <- n / cellStats(w, sum)
        mI <- NS0 * wZiZj / z2
        return(mI)
}


Babak



-----Original Message-----
From: Robert J. Hijmans [mailto:r.hijmans at gmail.com]
Sent: Thursday, April 14, 2011 7:20 PM
To: Babak Naimi
Cc: Roger.Bivand at nhh.no; r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter

Hi Babak,

I would propose the below for the local Moran with a RasterLayer
because it deals with NA values in a memory safe manner (and will be a
bit slower because of that)

library(raster)

MoranLocal <- function(x) {
        n <- ncell(x) - cellStats(x, 'countNA')
        s2 <-  cellStats(x, sd)^2
        # adjust variance denominator from n-1 to n
        s2 <- (s2 * (n-1)) / n
        z  <- x - cellStats(x, mean)
        #weights
        w  <- focal( z, fun=function(x, ...){ sum(!is.na(x))-1 } )
        lz <- (focal(z, fun='sum', na.rm=TRUE) - z) / w
        (z / s2) * lz
}


I looked up the Getis and Ord paper you mention (I happen to have the
book), and they do indeed subtract the focal cell from the global mean
& variance. That seems intuitive. But I do not think it was is
suggested in Anselin's LISA paper (Formula 12)  (and implemented in
spdep).

Also, it should be true that  globalMoran == mean(localMoran)

I think the below produces the global Moran (for the queen case
neighboring grid cells)

Moran <- function(x) {
        z <- x - cellStats(x, mean)
        wZiZj <- focal(z, fun='sum', na.rm=TRUE)
        wZiZj <- overlay(wZiZj, z, fun=function(x,y){ (x-y) * y })
        wZiZj <- cellStats(wZiZj, sum)
        z2 <- cellStats(z*z, sum)
        n <- ncell(z) - cellStats(z, 'countNA')
        # weights
        w <- focal( z, fun=function(x, ...){ max(0, sum(!is.na(x))-1) } )
        NS0 <- n / cellStats(w, sum)
        mI <- NS0 * wZiZj / z2
        return(mI)
}

With your 'tmp' data, I get:

> Moran(tmp)
[1] 0.8526906

> cellStats(MoranLocal(tmp), mean)
[1] 0.8192615

Close, but not quite the same.

I'll put these functions in the raster package.

Robert


On Thu, Apr 14, 2011 at 7:34 AM, Babak Naimi <naimi at itc.nl> wrote:
> Dear Robert and Roger,
>
> Thank both of you. After all, I came up with the following function (focalMoran) that efficiently works. I tested it for a raster layer with 122345 not-NA cells, and compared with locmor (based on localmoran in spdep package). The results are almost identical (cor = 0.99987). I think the only difference between two function is that in the focalMoran, the center pixel is omitted in calculation of global mean and variance. However, focalMoran was so quicker than locmor. Following you will find the difference in computation as well as the functions.
>
> Many thanks,
> Babak
>
>
> b1<-raster(----) # a raster with dimension 355, 617, 219035  (nrow, ncol, ncell) and resolution of 843.8446 m
> filter<-matrix(1,nc=9,nr=9)
>
> system.time(
> b.foc<-focalMoran(b1,filter)
> )
>   user  system elapsed
>  44.63    0.45   48.91
>
> system.time(
> b.spdep<-locmor(b1,d1=0,d2=4863.92)
> )
>   user  system elapsed
> 5738.56    6.02 5832.11
>
> ###########################
>
> focalMoran<-function(ras,filter) {
>        globsum <- cellStats(ras, sum)
>        glob.sqsum <- cellStats(ras^2, sum)
>        n<-length(na.omit(ras[]))
>        lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
>                i <- trunc(length(x)/2)+1
>                xbar=(sigmaX-x[i])/(N-1)
>                s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
>                z <- x - xbar
>                lz <- mean(z[-i], na.rm=na.rm)
>                return((z[i]/s2) * lz)
>                }
>        ras2 <- raster::expand(ras, extent(ras)+dim(filter)[1]*res(ras))
>        ras2 <- focalFilter(ras2,filter,lomo)
>        ras2 <- crop(ras2, ras)
>        return(ras2)
> }
> #--------
> locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep
>        df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>        df<-na.omit(df)
>        df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2)
>        df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list
>        lmor <- localmoran(df$values, df.listw)
>        ras[df$cells]<-as.data.frame(lmor)[[1]]
>        return(ras)
> }
>
>
>
>
> -----Original Message-----
> From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> Sent: Wednesday, April 13, 2011 11:15 PM
> To: Robert J. Hijmans
> Cc: Babak Naimi; r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter
>
> On Wed, 13 Apr 2011, Robert J. Hijmans wrote:
>
>> Babak,
>>
>> I looked at the original paper,
>> http://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.1995.tb00338.x/abstract,
>> and the spdep manual, and other places, but they all seemed to be
>> saying somewhat different on the local moran statistic. I followed the
>> logic of the spdep::localmoran function, assuming that this is
>> "correct" and I should have phrased my answer more carefully: "based
>> onthe spdep::localmoran function, mean and variance should be
>> global...".  I do note that the two results are different but that
>> their correlation is very high R2 = 0.97; so perhaps they are both
>> right. Perhaps Roger can weigh in on this.
>
> I don't have access to my library now. My understanding is that you are
> short-circuiting the construction of the spatial weights (or the n
> separate star matrices, one for each i, in Michael Tiefeldorf's terms). In
> general, Moran's I is not often used with non-zero values on the diagonal
> of the weights matrix. Non-zero values are seen in the local Getis-Ord
> Gi_star, but not in local Gi. My guess (without references) would be that
> the weight for i itself should be zero (the centre cell of the raster).
> The difference between mean() and sum() is the difference between
> row-standardised (summing to 1) and binary (all unity) weights. The
> function as it is doesn't permit other weighting schemes (such as IDW in
> either row-standardised or binary form).
>
> Hope this helps,
>
> Roger
>
>>
>> To make it run, in your code I replaced this:
>> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
>> filter<-matrix(f,nc=3,nr=3,byrow=T)
>>
>> with:
>> filter<-matrix(1 ,nc=3,nr=3)
>>
>> Robert
>>
>>
>> On Wed, Apr 13, 2011 at 10:46 AM, Babak Naimi <naimi at itc.nl> wrote:
>>> Dear Robert,
>>>
>>> Thanks for your reply. Now, the result is the same as with localmoran in spdep package. But I checked this with Getis & Ord (1996), seems the formula, as you implement in the script, is not the same as what has been explained in the Getis & Ord. First difference is that, Xi is excluded in calculation of the global mean and variance. The second difference is that, in the line "lz <- mean(z[-i], na.rm=na.rm)", 'mean' should be replaced with 'sum'. I re-implemented this function. The example we used here came from that reference (p.266). The calculation for the cell 136 has also been provided in the book. The result that this new function returns is the same as with the book, now. But the strange thing to me is that it is not the same as spdep. Any comment?
>
>>>
>>> Babak
>>>
>>>
>>> Getis,A. and Ord, J. K. 1996 Local spatial statistics: an overview. In P. Longley and M. Batty (eds) Spatial
>>> analysis: modelling in a GIS environment (Cambridge: Geoinformation International), 261-277.
>>>
>>> #----------------------------------------------
>>> focalMoran<-function(ras,filter) {
>>>        globsum <- cellStats(ras, sum)
>>>        glob.sqsum <- cellStats(ras^2, sum)
>>>        n<-ncell(tmp)
>>>        lomo <- function (x, na.rm=TRUE, sigmaX=globsum, sigmaX2=glob.sqsum,N=n) {
>>>                i <- trunc(length(x)/2)+1
>>>                xbar=(sigmaX-x[i])/(N-1)
>>>                s2<-((sigmaX2-x[i]^2)-((sigmaX -x[i])^2/(N-1)))/(N-1)
>>>                z <- x - xbar
>>>                lz <- sum(z[-i], na.rm=na.rm)
>>>                return((z[i]/s2) * lz)
>>>                }
>>>        ras2 <- raster::expand(ras, extent(ras)+2*res(ras))
>>>        ras2 <- focalFilter(ras2,filter,lomo)
>>>        ras2 <- crop(ras2, ras)
>>>        return(ras2)
>>> }
>>> #--------
>>> locmor<-function(ras,d1,d2) { # function to calculate local Moran using spdep
>>>        require(raster)
>>>        require(spdep)
>>>        df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>>>        df<-na.omit(df)
>>>        df.nb <- dnearneigh(as.matrix(df[,2:3]), d1, d2)
>>>        df.listw <- nb2listw(df.nb) #turns neighbourhood object into a weighted list
>>>        lmor <- localmoran(df$values, df.listw)
>>>        ras[df$cells]<-as.data.frame(lmor)[[1]]
>>>        return(ras)
>>> }
>>>
>>> #####---------------
>>>
>>> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
>>> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
>>> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
>>> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
>>> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
>>> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
>>> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
>>> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
>>> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
>>> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
>>> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
>>> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
>>> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
>>> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
>>> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
>>> tmp <- raster(tmp)
>>>
>>>
>>> filter<-c(NA,1,NA,1,1,1,NA,1,NA)
>>> filter<-matrix(f,nc=3,nr=3,byrow=T)
>>>
>>> tmp.focal<-focalMoran(tmp,filter)
>>> tmp.spdep<-locmor(tmp,d1=0,d2=0.0625)
>>>
>>> tmp.focal[136] # the calculation for this cell has been provided in Getis and Ord(1996, p.268)
>>> tmp.spdep[136]
>>>
>>>
>>> par(mfrow=c(1,2))
>>> plot(tmp.focal)
>>> plot(tmp.spdep)
>>>
>>> #------------------------------------------
>>> #-----------------------------------------
>>>
>>>
>>>
>>> Date: Tue, 12 Apr 2011 15:14:05 -0700 (PDT)
>>> From: Robert Hijmans <r.hijmans at gmail.com>
>>> To: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Local Moran'I for raster & focalFilter
>>> Message-ID: <1302646445119-6266969.post at n2.nabble.com>
>>> Content-Type: text/plain; charset=us-ascii
>>>
>>>> Dear All,
>>>>
>>>> I tried to write a function that can be used in focalFilter of raster
>>>> package, but the output
>>>> result is different from what was calculated using spdep for a sample
>>>> dataset.
>>>> It might be my mistake in the formula that is used in the function.
>>>> Following you will
>>>> find both functions as well as the resultss for a sample raster.
>>>>
>>>> I would appreciate if anyone can help.
>>>>
>>>> Best regards,
>>>> Babak
>>>
>>>
>>> Babak,
>>> I think the mistake you made is that the local moran uses the _global_ mean
>>> and the _global_ variance, whereas you used the _local_ mean and the _local_
>>> variance. I have changed your function and now the result with focalFilter
>>> is the same as with spdep.
>>> Robert
>>>
>>>
>>>
>>>
>>>
>>> locmor<-function(ras,d1=0,d2=6000) { # function to calculate local Moran
>>> using spdep
>>>        require(raster)
>>>        require(spdep)
>>>
>>> df<-data.frame(cbind(cells=1:ncell(ras),xyFromCell(ras,1:ncell(ras)),values=ras[1:ncell(ras)]))
>>>        df.sub<-subset(df,!is.na(values))
>>>        df.nb <- dnearneigh(as.matrix(df.sub[,2:3]), d1, d2)
>>>        df.listw <- nb2listw(df.nb) #turns neighbourhood object into a
>>> weighted list
>>>        lmor <- localmoran(df.sub$values, df.listw)
>>>        ras[df.sub$cells]<-as.data.frame(lmor)[[1]]
>>>        return(ras)
>>> }
>>>
>>> #---------------
>>>
>>> tmp <- c(55, 56, 54, 58, 65, 75, 82, 77, 74, 74, 69, 61, 62, 71, 73, 63, 62,
>>> 63, 64, 59, 85, 88, 95, 106, 110, 99, 89, 82, 79, 84, 97, 79, 55, 55, 56,
>>> 60, 91, 95, 86, 98, 115, 105, 110, 107, 101, 89, 85, 68, 55, 54, 53, 61, 82,
>>> 102, 88, 93, 96, 94, 110, 114, 109, 103, 92, 68, 59, 58, 60, 64, 88, 99, 82,
>>> 81, 71, 80, 89, 89, 89, 102, 104, 75, 63, 57, 58, 58, 77, 92, 82, 71, 59,
>>> 90, 105, 92, 79, 98, 110, 83, 62, 55, 56, 56, 80, 90, 99, 88, 64, 91, 112,
>>> 94, 76, 91, 100, 85, 62, 59, 55, 61, 99, 97, 93, 80, 65, 87, 107, 80, 59,
>>> 60, 67, 66, 65, 62, 68, 72, 102, 94, 90, 83, 74, 81, 96, 69, 52, 50, 51, 54,
>>> 62, 62, 86, 85, 55, 59, 64, 72, 75, 70, 70, 62, 66, 61, 55, 57, 52, 59, 61,
>>> 56, 41, 40, 43, 44, 46, 48, 50, 52, 68, 69, 60, 61, 42, 43, 44, 43, 42, 41,
>>> 42, 44, 43, 43, 44, 47, 58, 59, 55, 61, 44, 41, 39, 42, 44, 43, 42, 42, 42,
>>> 43, 43, 49, 56, 53, 53, 61, 43, 42, 40, 42, 42, 42, 41, 42, 42, 43, 42, 53,
>>> 66, 61, 51, 62, 40, 42, 41, 40, 43, 49, 46, 42, 42, 43, 43, 49, 59, 62, 53,
>>> 62, 40, 41, 42, 43, 49, 54, 47, 44, 42, 44, 43, 46, 52, 56, 56, 61)
>>> tmp <- matrix(tmp, ncol=16, nrow=16, byrow=T)
>>> tmp <- raster(tmp)
>>>
>>>
>>> tmp.spdep<-locmor(tmp,d1=0,d2=0.08838835)
>>>
>>> # new function for use with focalFilter
>>> # first compute global mean and global variance
>>> globmean <- cellStats(tmp, mean)
>>> globvar <-  cellStats(tmp, sd)^2
>>> # adjust variance denominator from n-1 to n
>>> globvar <- (globvar * (ncell(tmp)-1)) / ncell(tmp)
>>>
>>> lomo <- function (x, na.rm=TRUE, xbar=globmean, s2=globvar) {
>>>        i <- trunc(length(x)/2)+1
>>>    z <- x - globmean
>>>        lz <- mean(z[-i], na.rm=na.rm)
>>>    (z[i]/s2) * lz
>>> }
>>>
>>> filter<-matrix(1, nrow=3, ncol=3)
>>> tmp.focal<-focalFilter(tmp,filter,lomo)
>>>
>>> par(mfrow=c(1,2))
>>> plot(tmp.spdep)
>>> plot(tmp.focal)
>>>
>>> # note that you could, but should not, do this:
>>> tmp.focal<-focal(tmp,fun=lomo)
>>> # because  the trick to get the focal cell
>>> #     i <- trunc(length(x)/2)+1
>>> # does not work for the border cells.
>>> # To get the border cells:
>>>
>>> tmp2 <- raster::expand(tmp, extent(tmp)+2*res(tmp))
>>> tmpfoc2 <- focalFilter(tmp2,filter,lomo)
>>> tmpfoc2 <- crop(tmpfoc2, tmp)
>>>
>>> par(mfrow=c(1,2))
>>> plot(tmp.spdep)
>>> plot(tmpfoc2)
>>>
>>>
>>>
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Helleveien 30, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
> e-mail: Roger.Bivand at nhh.no
>


From nick at hamm.org  Thu Apr 14 21:27:06 2011
From: nick at hamm.org (Nick Hamm)
Date: Thu, 14 Apr 2011 21:27:06 +0200
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
In-Reply-To: <BANLkTinCFz21m8-Vc8Z+f5VXCEUeXkBb9w@mail.gmail.com>
References: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>
	<BANLkTi=XjRcU51twwV=jcw6G7Eqh2HPcHQ@mail.gmail.com>
	<alpine.LRH.2.00.1104132306140.19364@reclus.nhh.no>
	<021F462EDC7F2B42A7038E9F9D4782611F18A40694@itcnt27.itc.nl>
	<BANLkTinCFz21m8-Vc8Z+f5VXCEUeXkBb9w@mail.gmail.com>
Message-ID: <BANLkTinTM=Cepbcxnqd8gZqgZFU8AOLb3A@mail.gmail.com>

Dear all

Replies below

On 14 April 2011 19:19, Robert J. Hijmans <r.hijmans at gmail.com> wrote:
> Hi Babak,
>
> I would propose the below for the local Moran with a RasterLayer
> because it deals with NA values in a memory safe manner (and will be a
> bit slower because of that)
>
> library(raster)
>
> MoranLocal <- function(x) {
> ? ? ? ?n <- ncell(x) - cellStats(x, 'countNA')
> ? ? ? ?s2 <- ?cellStats(x, sd)^2
> ? ? ? ?# adjust variance denominator from n-1 to n
> ? ? ? ?s2 <- (s2 * (n-1)) / n
> ? ? ? ?z ?<- x - cellStats(x, mean)
> ? ? ? ?#weights
> ? ? ? ?w ?<- focal( z, fun=function(x, ...){ sum(!is.na(x))-1 } )
> ? ? ? ?lz <- (focal(z, fun='sum', na.rm=TRUE) - z) / w
> ? ? ? ?(z / s2) * lz
> }
>
>
> I looked up the Getis and Ord paper you mention (I happen to have the
> book), and they do indeed subtract the focal cell from the global mean
> & variance. That seems intuitive. But I do not think it was is
> suggested in Anselin's LISA paper (Formula 12) ?(and implemented in
> spdep).

According to my reading of Anselin (1995) wii should be excluded.  See
page 98 in the text between equations 8 and 9 "...,and by convention
wii = 0".

Roger's post yesterday also suggested it should be excluded.  But what
is the situation with spdep?


>
> Also, it should be true that ?globalMoran == mean(localMoran)
>
> I think the below produces the global Moran (for the queen case
> neighboring grid cells)
>
> Moran <- function(x) {
> ? ? ? ?z <- x - cellStats(x, mean)
> ? ? ? ?wZiZj <- focal(z, fun='sum', na.rm=TRUE)
> ? ? ? ?wZiZj <- overlay(wZiZj, z, fun=function(x,y){ (x-y) * y })
> ? ? ? ?wZiZj <- cellStats(wZiZj, sum)
> ? ? ? ?z2 <- cellStats(z*z, sum)
> ? ? ? ?n <- ncell(z) - cellStats(z, 'countNA')
> ? ? ? ?# weights
> ? ? ? ?w <- focal( z, fun=function(x, ...){ max(0, sum(!is.na(x))-1) } )
> ? ? ? ?NS0 <- n / cellStats(w, sum)
> ? ? ? ?mI <- NS0 * wZiZj / z2
> ? ? ? ?return(mI)
> }
>
> With your 'tmp' data, I get:
>
>> Moran(tmp)
> [1] 0.8526906
>
>> cellStats(MoranLocal(tmp), mean)
> [1] 0.8192615
>
> Close, but not quite the same.
>
> I'll put these functions in the raster package.
>

This is a good idea.  There is also interest in using other local
statistics (e.g., Getis) in remote sensing.  This is what I was on my
way to calculating when I found the problem with focalFilter.  Raster
is MUCH faster than using spdep for images - because it is not
necessary to recompute the neighbours for every pixel - as Babak has
shown.

> Robert
>
>


From r.hijmans at gmail.com  Thu Apr 14 21:40:28 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Thu, 14 Apr 2011 12:40:28 -0700
Subject: [R-sig-Geo] Local Moran'I for raster & focalFilter
In-Reply-To: <BANLkTinTM=Cepbcxnqd8gZqgZFU8AOLb3A@mail.gmail.com>
References: <021F462EDC7F2B42A7038E9F9D4782611F18A4068E@itcnt27.itc.nl>
	<BANLkTi=XjRcU51twwV=jcw6G7Eqh2HPcHQ@mail.gmail.com>
	<alpine.LRH.2.00.1104132306140.19364@reclus.nhh.no>
	<021F462EDC7F2B42A7038E9F9D4782611F18A40694@itcnt27.itc.nl>
	<BANLkTinCFz21m8-Vc8Z+f5VXCEUeXkBb9w@mail.gmail.com>
	<BANLkTinTM=Cepbcxnqd8gZqgZFU8AOLb3A@mail.gmail.com>
Message-ID: <BANLkTikgAFcOjc98sce8mA69ULLHvgsfsQ@mail.gmail.com>

> According to my reading of Anselin (1995) wii should be excluded. ?See
> page 98 in the text between equations 8 and 9 "...,and by convention
> wii = 0".
>

Nick,

I believe that Wii is zero in these functions because it uses:

focal(z, fun='sum', na.rm=TRUE) - z)

The above first sums all focal values but then it subtracts the
central cell value. This alleviates us from the problem of finding the
central value, which is difficult around the borders of the raster
(for which the work-around is to use additional rows/columns with NA
values).

I look forward to seeing your implementation of the G statistics :)

Robert



Robert



On Thu, Apr 14, 2011 at 12:27 PM, Nick Hamm <nick at hamm.org> wrote:
> Dear all
>
> Replies below
>
> On 14 April 2011 19:19, Robert J. Hijmans <r.hijmans at gmail.com> wrote:
>> Hi Babak,
>>
>> I would propose the below for the local Moran with a RasterLayer
>> because it deals with NA values in a memory safe manner (and will be a
>> bit slower because of that)
>>
>> library(raster)
>>
>> MoranLocal <- function(x) {
>> ? ? ? ?n <- ncell(x) - cellStats(x, 'countNA')
>> ? ? ? ?s2 <- ?cellStats(x, sd)^2
>> ? ? ? ?# adjust variance denominator from n-1 to n
>> ? ? ? ?s2 <- (s2 * (n-1)) / n
>> ? ? ? ?z ?<- x - cellStats(x, mean)
>> ? ? ? ?#weights
>> ? ? ? ?w ?<- focal( z, fun=function(x, ...){ sum(!is.na(x))-1 } )
>> ? ? ? ?lz <- (focal(z, fun='sum', na.rm=TRUE) - z) / w
>> ? ? ? ?(z / s2) * lz
>> }
>>
>>
>> I looked up the Getis and Ord paper you mention (I happen to have the
>> book), and they do indeed subtract the focal cell from the global mean
>> & variance. That seems intuitive. But I do not think it was is
>> suggested in Anselin's LISA paper (Formula 12) ?(and implemented in
>> spdep).
>
> Roger's post yesterday also suggested it should be excluded. ?But what
> is the situation with spdep?
>
>
>>
>> Also, it should be true that ?globalMoran == mean(localMoran)
>>
>> I think the below produces the global Moran (for the queen case
>> neighboring grid cells)
>>
>> Moran <- function(x) {
>> ? ? ? ?z <- x - cellStats(x, mean)
>> ? ? ? ?wZiZj <- focal(z, fun='sum', na.rm=TRUE)
>> ? ? ? ?wZiZj <- overlay(wZiZj, z, fun=function(x,y){ (x-y) * y })
>> ? ? ? ?wZiZj <- cellStats(wZiZj, sum)
>> ? ? ? ?z2 <- cellStats(z*z, sum)
>> ? ? ? ?n <- ncell(z) - cellStats(z, 'countNA')
>> ? ? ? ?# weights
>> ? ? ? ?w <- focal( z, fun=function(x, ...){ max(0, sum(!is.na(x))-1) } )
>> ? ? ? ?NS0 <- n / cellStats(w, sum)
>> ? ? ? ?mI <- NS0 * wZiZj / z2
>> ? ? ? ?return(mI)
>> }
>>
>> With your 'tmp' data, I get:
>>
>>> Moran(tmp)
>> [1] 0.8526906
>>
>>> cellStats(MoranLocal(tmp), mean)
>> [1] 0.8192615
>>
>> Close, but not quite the same.
>>
>> I'll put these functions in the raster package.
>>
>
> This is a good idea. ?There is also interest in using other local
> statistics (e.g., Getis) in remote sensing. ?This is what I was on my
> way to calculating when I found the problem with focalFilter. ?Raster
> is MUCH faster than using spdep for images - because it is not
> necessary to recompute the neighbours for every pixel - as Babak has
> shown.
>
>> Robert
>>
>>
>


From roman.lustrik at gmail.com  Fri Apr 15 14:10:40 2011
From: roman.lustrik at gmail.com (=?UTF-8?Q?Roman_Lu=C5=A1trik?=)
Date: Fri, 15 Apr 2011 14:10:40 +0200
Subject: [R-sig-Geo] rgeos::gBuffer quadsegs argument in respect to
	SpatialPoints -> SpatialPolygons
Message-ID: <BANLkTikUWDNs5y3c-BN5OJh6ZHCiYk7WEA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110415/955fcfd7/attachment.pl>

From rundel at gmail.com  Fri Apr 15 18:39:34 2011
From: rundel at gmail.com (Colin Rundel)
Date: Fri, 15 Apr 2011 09:39:34 -0700
Subject: [R-sig-Geo] rgeos::gBuffer quadsegs argument in respect to
	SpatialPoints -> SpatialPolygons
In-Reply-To: <BANLkTikUWDNs5y3c-BN5OJh6ZHCiYk7WEA@mail.gmail.com>
References: <BANLkTikUWDNs5y3c-BN5OJh6ZHCiYk7WEA@mail.gmail.com>
Message-ID: <303E2AB0-030D-45C9-813E-E0DB3ACA46A3@gmail.com>

> Which sp objects does quadsegs argument in gBuffer affect (or rather, which
> it does not)? Based on my experiments, SpatialPoints -> SpatialPolygons is
> not affected. I use SpatialPoints to draw circles (see below) and I figured
> that specifying the quadsegs would alter the "roughness" of the circle
> generated (which is now SpatialPolygons). This doesn't seem to be the case.
> Can someone expand on this?
> 
> sap <- gBuffer(spgeom = sap, width = 500, quadsegs = 5)
> nrow(sap at polygons[[1]]@Polygons[[1]]@coords) #always the same, irrespective
> of quadsegs


This should not be happening and when I run similar code I do get a difference in the number of coordinates as expected using the most current version of rgeos 0.1-4:

> x=SpatialPoints(cbind(x=1,y=1))
> z1=gBuffer(x,quadsegs=10)
> z2=gBuffer(x,quadsegs=5)
> nrow(z1 at polygons[[1]]@Polygons[[1]]@coords)
[1] 41
> nrow(z2 at polygons[[1]]@Polygons[[1]]@coords)
[1] 21

There may be some weirdness with your initial SpatialPoints object that is causing this but I don't have a plausible theory as to why. Try the above code and see if you still have the problem and double check that your rgeos is up to date.

-Colin

From roman.lustrik at gmail.com  Fri Apr 15 19:09:41 2011
From: roman.lustrik at gmail.com (=?UTF-8?Q?Roman_Lu=C5=A1trik?=)
Date: Fri, 15 Apr 2011 19:09:41 +0200
Subject: [R-sig-Geo] rgeos::gBuffer quadsegs argument in respect to
 SpatialPoints -> SpatialPolygons
In-Reply-To: <303E2AB0-030D-45C9-813E-E0DB3ACA46A3@gmail.com>
References: <BANLkTikUWDNs5y3c-BN5OJh6ZHCiYk7WEA@mail.gmail.com>
	<303E2AB0-030D-45C9-813E-E0DB3ACA46A3@gmail.com>
Message-ID: <BANLkTi=4C9MMWo+Bgs7C6RTq=EBUE4Tdfg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110415/04182c42/attachment.pl>

From emptican at gmail.com  Fri Apr 15 23:27:15 2011
From: emptican at gmail.com (Steve Hong)
Date: Fri, 15 Apr 2011 16:27:15 -0500
Subject: [R-sig-Geo] comparing spatial distribution of two species in a same
 area with different coordinates
Message-ID: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110415/ab3ab3a2/attachment.pl>

From momadou at yahoo.fr  Sat Apr 16 23:44:05 2011
From: momadou at yahoo.fr (Komine)
Date: Sat, 16 Apr 2011 14:44:05 -0700 (PDT)
Subject: [R-sig-Geo] Hosmer-Lemeshow and Le Cessie-van Houwelingen
Message-ID: <1302990245402-6279901.post@n2.nabble.com>

Hi
I would like your help to understand these results of Hosmer-Lemeshow and Le
Cessie-van Houwelingen test. The first invalidate my model and the second
validate it.

reg<-glm(Ignition~FMC,data=Fire,family=binomial)
summary(reg)

Call:
glm(formula = Ignition ~ FMC, family = binomial, data = Fire)
Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.20116  -0.03811   0.09321   0.31675   2.06383  
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  7.29351    1.07182   6.805 1.01e-11 ***
FMC         -0.06973    0.01113  -6.266 3.72e-10 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 313.618  on 230  degrees of freedom
Residual deviance:  94.877  on 229  degrees of freedom
AIC: 98.877

Number of Fisher Scoring iterations: 8

The result of the test is: 

 library(MKmisc)
 HLgof.test(fit = fitted(reg), obs = Ignition,X= cbind(FMC))
$C
        Hosmer-Lemeshow C statistic
data:  fitted(reg) and Ignition 
X-squared = 231, df = 8, p-value < 2.2e-16
$H
        Hosmer-Lemeshow H statistic
data:  fitted(reg) and Ignition 
X-squared = 231, df = 8, p-value < 2.2e-16
$gof
        le Cessie-van Houwelingen-Copas-Hosmer global goodness of fit test
data:  fitted(reg) and Ignition 
z = 1.8747, p-value = 0.06084
Messages d'avis :
1: In Ops.factor(1, obs) : - not meaningful for factors
2: In Ops.factor(1, obs) : - not meaningful for factors

1-	In case these codes are correct, what is the reason of contradiction
between Hosmer-Lemeshow and Le Cessie-van Houwelingen test?  
2-	In this case, what is the conclusion: logistic model is validated or
invalidated? 
Thanks for your help. 


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Hosmer-Lemeshow-and-Le-Cessie-van-Houwelingen-tp6279901p6279901.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From ostrikas at post.harvard.edu  Sun Apr 17 00:50:06 2011
From: ostrikas at post.harvard.edu (Ona Strikas)
Date: Sat, 16 Apr 2011 15:50:06 -0700
Subject: [R-sig-Geo] Automation of csv/dbf data importation and plotting the
	allstats function
Message-ID: <BANLkTin--Lt6+vBWc4m2iN9hcjCs5AiUmg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110416/41499c76/attachment.pl>

From mspinola10 at gmail.com  Sun Apr 17 15:16:16 2011
From: mspinola10 at gmail.com (=?ISO-8859-1?Q?Manuel_Sp=EDnola?=)
Date: Sun, 17 Apr 2011 07:16:16 -0600
Subject: [R-sig-Geo] Problems reading shapefiles in R
In-Reply-To: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>
References: <BANLkTikEW+JHz_-q0MwoE-NYWv+vdTkfNw@mail.gmail.com>
Message-ID: <BANLkTinX=8VvJU0UUVmhk4jz=UxsfJASYQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110417/7bdb5a71/attachment.pl>

From geraldjtb at googlemail.com  Mon Apr 18 10:22:02 2011
From: geraldjtb at googlemail.com (=?ISO-8859-1?Q?Gerald_Johann_to_B=FCren?=)
Date: Mon, 18 Apr 2011 10:22:02 +0200
Subject: [R-sig-Geo] comparing spatial distribution of two species in a
 same area with different coordinates
In-Reply-To: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>
References: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>
Message-ID: <BANLkTimaYqTugOLLBtusj20zeUX=qVfDTA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110418/08f5edc0/attachment.pl>

From mdsumner at gmail.com  Mon Apr 18 11:13:30 2011
From: mdsumner at gmail.com (Michael Sumner)
Date: Mon, 18 Apr 2011 19:13:30 +1000
Subject: [R-sig-Geo] comparing spatial distribution of two species in a
 same area with different coordinates
In-Reply-To: <BANLkTimaYqTugOLLBtusj20zeUX=qVfDTA@mail.gmail.com>
References: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>
	<BANLkTimaYqTugOLLBtusj20zeUX=qVfDTA@mail.gmail.com>
Message-ID: <BANLkTi=DD68x=AWSNK4UfZuL-39D06-iRw@mail.gmail.com>

Apparently not: http://finzi.psych.upenn.edu/R/Rhelp02/archive/98714.html



On Mon, Apr 18, 2011 at 6:22 PM, Gerald Johann to B?ren
<geraldjtb at googlemail.com> wrote:
> Hi,
> I would suggest the
> SADIE-Method<http://www.rothamsted.ac.uk/pie/sadie/SADIE_home_page_1.htm>.
> As far as I know it ist not yet implemented for R, but there is a free
> Standalone-Tool available.
>
> with best regards
> Gerald
>
> PS: Does anyone know if it is there is an R-package for that?
>
>
> On Fri, Apr 15, 2011 at 11:27 PM, Steve Hong <emptican at gmail.com> wrote:
>> Dear List,
>>
>> I hope this is not a redundant question. ?I tried to find the issue from
>> the previous lists but could not find although there are several comments
>> about comparing two species distribution in the same area and coordinates.
>>
>> I have a data set with species counts in a study area, however, the GPS
>> location is slightly different from species to species. ?The sampling was
>> conducted in a grid pattern covering uniformly the area.
>>
>> My question are:
>> Is there any package(s) and/or way(s) to compare spatial distribution of
> two
>> species with different coordinate readings in the same study area?
>> If so, is there any (potential) problem(s) in doing that?
>>
>> Thanks for your any comment and suggestion in advance!
>>
>> Steve
>>
>> ? ? ? ?[[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Michael Sumner
Institute for Marine and Antarctic Studies, University of Tasmania
Hobart, Australia
e-mail: mdsumner at gmail.com


From corey.sparks at UTSA.EDU  Mon Apr 18 13:58:17 2011
From: corey.sparks at UTSA.EDU (Corey Sparks)
Date: Mon, 18 Apr 2011 04:58:17 -0700 (PDT)
Subject: [R-sig-Geo] comparing spatial distribution of two species in a
 same area with different coordinates
In-Reply-To: <BANLkTi=DD68x=AWSNK4UfZuL-39D06-iRw@mail.gmail.com>
References: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>
	<BANLkTimaYqTugOLLBtusj20zeUX=qVfDTA@mail.gmail.com>
	<BANLkTi=DD68x=AWSNK4UfZuL-39D06-iRw@mail.gmail.com>
Message-ID: <1303127897008-6283365.post@n2.nabble.com>

Would the cross L or K function not give you a descriptive tool for what you
want? they measure clustering in two or more point patterns. See 
?Lcross
or 
?Kcross
in spatstat.  Also there are inhomogeneous Poisson process models as well,
Lcross.inhom

Cheers,
Corey

-----
Corey Sparks, PhD
Assistant Professor
Department of Demography and Organization Studies
University of Texas at San Antonio
501 West Durango Blvd
Monterey Building 2.270C
San Antonio, TX 78207
210-458-3166
corey.sparks 'at' utsa.edu
https://rowdyspace.utsa.edu/users/ozd504/www/index.htm
--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/comparing-spatial-distribution-of-two-species-in-a-same-area-with-different-coordinates-tp6277728p6283365.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From virgilio.gomez at uclm.es  Mon Apr 18 14:44:34 2011
From: virgilio.gomez at uclm.es (Virgilio =?ISO-8859-1?Q?G=F3mez-Rubio?=)
Date: Mon, 18 Apr 2011 14:44:34 +0200
Subject: [R-sig-Geo] comparing spatial distribution of two species in a
 same area with different coordinates
In-Reply-To: <1303127897008-6283365.post@n2.nabble.com>
References: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>
	<BANLkTimaYqTugOLLBtusj20zeUX=qVfDTA@mail.gmail.com>
	<BANLkTi=DD68x=AWSNK4UfZuL-39D06-iRw@mail.gmail.com>
	<1303127897008-6283365.post@n2.nabble.com>
Message-ID: <1303130674.1804.110.camel@virgil-HP-Compaq-8000-Elite-USDT-PC>

Hi,

El lun, 18-04-2011 a las 04:58 -0700, Corey Sparks escribi?:
> Would the cross L or K function not give you a descriptive tool for what you
> want? they measure clustering in two or more point patterns. See 
> ?Lcross
> or 
> ?Kcross
> in spatstat.  Also there are inhomogeneous Poisson process models as well,
> Lcross.inhom

As I have understood this problem, there are aggregate data at different
locations. So the point pattern approach does not seem feasible.

A possible alternative could be using a geostatistical model with a
Poisson response, so that you have a spatially correlated effect
underneath. You may fit the same model to your data sets and then
compare the  spatially correlated effects.

You could use geoRglm. In particular, functions pois.krige and
pois.krige.bayes seem to be relevant. And you can also check the book on
model-based geostatistics:

http://www.leg.ufpr.br/mbgbook/


Could other people comment on this approach?

I can think of other interesting (Bayesian) models. If you are happy to
go this way, feel free to contact me off-list.


Best,


Virgilio


From emptican at gmail.com  Mon Apr 18 15:57:43 2011
From: emptican at gmail.com (Steve Hong)
Date: Mon, 18 Apr 2011 08:57:43 -0500
Subject: [R-sig-Geo] comparing spatial distribution of two species in a
 same area with different coordinates
In-Reply-To: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>
References: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>
Message-ID: <BANLkTi=H7QWsDTJ0eEVanLd4ZHTcwqDN6w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110418/e601ff1b/attachment.pl>

From tom.kurkowski at alaska.edu  Mon Apr 18 18:26:14 2011
From: tom.kurkowski at alaska.edu (Tom Kurkowski)
Date: Mon, 18 Apr 2011 08:26:14 -0800
Subject: [R-sig-Geo] get.var.ncdf and embedded scale factors
Message-ID: <BANLkTim=Wbk272uUpdOuuwe3yct0b9oubQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110418/b244aa39/attachment.pl>

From r.hijmans at gmail.com  Mon Apr 18 21:47:54 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Mon, 18 Apr 2011 12:47:54 -0700 (PDT)
Subject: [R-sig-Geo] get.var.ncdf and embedded scale factors
In-Reply-To: <BANLkTim=Wbk272uUpdOuuwe3yct0b9oubQ@mail.gmail.com>
References: <BANLkTim=Wbk272uUpdOuuwe3yct0b9oubQ@mail.gmail.com>
Message-ID: <1303156074708-6284894.post@n2.nabble.com>

> I would like to confirm that the get.var.ncdf function within the ncdf 
> package automatically scales output data values if there is an embedded 
> scale factor within the ncdf file.  

It does and that is easy to infer from the code frament below that you get
when typing "get.var.ncdf". As you can see, you can use the option
"verbose=TRUE" to see what's going on when you use get.var.ncdf.

> library(ncdf)
> get.var.ncdf

(...) 

 if (!isdimvar) {
        if (verbose) 
            print(paste("get.var.ncdf: implementing add_offset (", 
                nc$var[[nc$varid2Rindex[varid]]]$hasAddOffset, 
                ") and scale_factor (",
nc$var[[nc$varid2Rindex[varid]]]$hasScaleFact, 
                ")"))
        if (nc$var[[nc$varid2Rindex[varid]]]$hasAddOffset && 
            nc$var[[nc$varid2Rindex[varid]]]$hasScaleFact) {
            if (verbose) 
                print(paste("var has BOTH add_offset (",
nc$var[[nc$varid2Rindex[varid]]]$addOffset, 
                  ") and scale_fact (",
nc$var[[nc$varid2Rindex[varid]]]$scaleFact, 
                  ")"))
            rv$data <- rv$data * nc$var[[nc$varid2Rindex[varid]]]$scaleFact
+ 
                nc$var[[nc$varid2Rindex[varid]]]$addOffset
        }
        else if (nc$var[[nc$varid2Rindex[varid]]]$hasAddOffset) {
            if (verbose) 
                print(paste("var has add_offset (only):",
nc$var[[nc$varid2Rindex[varid]]]$addOffset))
            rv$data <- rv$data + nc$var[[nc$varid2Rindex[varid]]]$addOffset
        }
        else if (nc$var[[nc$varid2Rindex[varid]]]$hasScaleFact) {
            if (verbose) 
                print(paste("var has scale_factor (only):",
nc$var[[nc$varid2Rindex[varid]]]$scaleFact))
            rv$data <- rv$data * nc$var[[nc$varid2Rindex[varid]]]$scaleFact
        }
        else {
            if (verbose) 
                print("var has NEITHER add_offset nor scale_factor")
        }
    }
    return(rv$data)



--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/get-var-ncdf-and-embedded-scale-factors-tp6284280p6284894.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From lestes at princeton.edu  Tue Apr 19 21:19:31 2011
From: lestes at princeton.edu (Lyndon Estes)
Date: Tue, 19 Apr 2011 15:19:31 -0400
Subject: [R-sig-Geo] subs (raster) on large dataset
In-Reply-To: <1302803032937-6273741.post@n2.nabble.com>
References: <BANLkTims5qZE1XBcyrU9Y=tYmzaBC0+UPQ@mail.gmail.com>
	<1302803032937-6273741.post@n2.nabble.com>
Message-ID: <BANLkTi=kO4NnxJMXWfCRo1es76BHRztLQw@mail.gmail.com>

Dear Robert,

Thanks for your response on this (sorry for my slow feedback, I had
another project intervene).

I have gone through the subs code now:

Using my raster ltq.g:

class       : RasterLayer
dimensions  : 15170, 17364, 263411880  (nrow, ncol, ncell)
resolution  : 92.73961, 92.73961  (x, y)
extent      : -733488.1, 876842.5, -3806988, -2400128  (xmin, xmax, ymin, ymax)
projection  : +proj=aea +lat_1=-18 +lat_2=-32 +lat_0=0 +lon_0=24
+x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs
+towgs84=0,0,0
values      : lt.quin.3f.grd
min value   : 1
max value   : 136050

r <- raster(ltq.g)
tr <- blockSize(r)  # n = 267, nrows = 57, size = 57

>From this test, I get quite variable speeds:

i <- 1
dong <- Sys.time()
v <- getValues(ltq.g, row = tr$row[i], nrows = tr$size)
x <- localmerge(v, y, TRUE)
length(x)  # 989748 = ncells in 57 (nrow) *17364(ncol)
ding <- Sys.time()
ding-dong
# Times:
# 2.374584 mins, 49.45746 seconds, 44.11 seconds
# After computer restart: 10.97272 after reboot, 14.53 seconds

The last two times are much better, so perhaps I was having some
performance bottlenecks on my computer.  Do these seem reasonable to
you, given my computer (2.4 GHz Core 2, 4 GB RAM Macbook Pro, 64-bit
R-2.12.2)?

I did catch something potentially important here:

if (canProcessInMemory(x, 3)) {
            v <- localmerge(getValues(x), y, subsWithNA)
            r <- setValues(r, v)
            if (filename != "") {
                r <- writeRaster(r, filename = filename, ...)
            }
            return(r)

Testing this on its own:

canProcessInMemory(ltq.g, 3)

Takes anywhere from 7.6 minutes (after computer restart) to 13.2
minutes (last night before restart), and returns "TRUE". Breaking out
the relevant bits of code for canProcessInMemory:

n <- 3 + (nlayers(ltq.g) - 1)
cells <- round(1.1 * ncell(ltq.g))
#if ((cells * n) > .maxmemory()) {
#        return(FALSE)
#    }
# Note: I couldn't figure out how to access .maxmemory()--it keeps telling me
# "Error: could not find function ".maxmemory"", so presumably this is
something
# internal to raster that I am not smart enough to figure out how to
access, so I used this:

((cells * n) > as.integer(1e+09))  # 1e+09 is my memory limit.
 > TRUE

So it seems my raster might be getting processed entirely in memory.
The slowness of canProcessInMemory then
comes from this line:

r <- try(matrix(0.1, ncol = n, nrow = cells), silent = TRUE)

There is this line at the beginning of canProcessInMemory that seems
to me important, but I get the same problem as with .maxmemory(), in
that I can't figure out how to see its code or what it does:

 if (.toDisk()) {
        return(FALSE)
    }

Based on its name, I assume it checks whether the parent function is
being directed to process onto the disk, i.e. a filename is supplied.
Is this correct?  And, if the answer is yes, is this further
assumption also correct?

A raster function (e.g. subs) run with an output filename and without
assignment to an object must necessarily write to disk.  e.g.

subs(x, y, by = "foo, which = "bar", subsWithNA = TRUE, filename =
out, datatype = type, overwrite = TRUE)

If this last assumption is correct, then it seems that something is
going wrong (or perhaps something I have mis-specified in my code)
such that this large file is being handled in memory when it should be
processed in chunks to disk.

I hope this makes some sense, but I'll appreciate any points of advice on this.

Thanks, Lyndon

p.s. the progress option doesn't work for me on subs, either "text" or
"window". Is this because I am mac, or is it indicative of something
wrong regarding the installation?










format    : raster
datatype  : FLT8S
overwrite : FALSE
progress  : none
timer     : FALSE
chunksize : 1e+06
maxmemory : 1e+09
tmpdir    : /var/folders/Xc/XcJpMYIWFJm5ANTLUN-U2U+++TM/-Tmp-/R_raster_tmp/
setfileext: TRUE



On Thu, Apr 14, 2011 at 1:43 PM, Robert Hijmans <r.hijmans at gmail.com> wrote:
> Lyndon,
>
> I have no idea what might be causing this. I would suggest to track down
> where exactly this happens, by going, line by line, through the code of
> 'subs'
>
> getMethod("subs", c("RasterLayer", "data.frame"))
>
> Presumably, something is not moving here:
>
> for (i in 1:tr$n) {
> ? ? v <- getValues(x, row = tr$row[i], nrows = tr$size)
> ? ? r <- writeValues(r, localmerge(v, y, subsWithNA), ?tr$row[i])
> ? ? pbStep(pb)
> }
>
> so you can try
>
> ?i <- 1
> ?v <- getValues(x, row = tr$row[i], nrows = tr$size)
> ?x <- localmerge(v, y, subsWithNA)
>
> and if that last step is indeed very slow, have a step by step look at
> "localmerge".
>
> You can then also see of it matters if you make changes to the "chunksize"
> and "maxmemory" options ?setOptions. These will affect the size of "v" in
> the code above, and perhaps there is a non-linear effect on the performance
> of merge.
>
> Robert
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/subs-raster-on-large-dataset-tp6272926p6273741.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Lyndon Estes
Research Associate
Woodrow Wilson School
Princeton University
+1-609-258-2392 (o)
+1-609-258-6082 (f)
+1-202-431-0496 (m)
lestes at princeton.edu


From wul1984 at gmail.com  Wed Apr 20 10:03:14 2011
From: wul1984 at gmail.com (Marco)
Date: Wed, 20 Apr 2011 16:03:14 +0800
Subject: [R-sig-Geo] Cut polygons layer using contour layer underneath
Message-ID: <BANLkTin+RFX+=oy4fMbV6YACTf3SO=EWVw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110420/99191041/attachment.pl>

From stefano at casalegno.net  Thu Apr 21 11:07:06 2011
From: stefano at casalegno.net (stefano)
Date: Thu, 21 Apr 2011 11:07:06 +0200
Subject: [R-sig-Geo] Ecological modelling training using R and OS tools
Message-ID: <1303376826.1674.191.camel@ste>

We are pleased to invite you to the 2011 summer school on "Ecological
modelling using R, Grass, Bash, awk and more Open sources tools".

The training is organized by the University of Copenhagen, Faculty of
Life Science - Forest and Landscape Inst. 4-15 July 2011. 

more info and application form : 
http://en.sl.life.ku.dk/upload/use_of_open_source_tools_for_spatial__ecological_modelling.doc

Stefano Casalegno 
Giuseppe Amatulli


From momadou at yahoo.fr  Thu Apr 21 13:50:04 2011
From: momadou at yahoo.fr (Komine)
Date: Thu, 21 Apr 2011 04:50:04 -0700 (PDT)
Subject: [R-sig-Geo] To validate logistic regression
Message-ID: <1303386604920-6293834.post@n2.nabble.com>

Hi, 
I would like your help to validate my logistic regression. I know how to do
logistic regression. 

rlog<-glm(Y~X,family=binomial,data=tab)
summary(rlog)
HLgof.test(fit = fitted(rlog), obs=Y)

However, I would like to validate my model. For example to divise my data in
a sample for training (66%) and a sample for validation (34%).
e.g for ma table 
Area   Y     X 
1       1     135
1       0     200
1       1      97
1       1     160
1       0     201
1       1     144
1       0     100

But I don't know how to validate it. 
1- My first problem: How to create my 2 samples from my variables Y and X
using pourcentage 66 ang 34 %?

- How to have the pourcentage of good prediction and bad prediction?

Thanks for your Help 
Komine 


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/To-validate-logistic-regression-tp6293834p6293834.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From momadou at yahoo.fr  Thu Apr 21 16:26:27 2011
From: momadou at yahoo.fr (Komine)
Date: Thu, 21 Apr 2011 07:26:27 -0700 (PDT)
Subject: [R-sig-Geo] To validate logistic regression
Message-ID: <1303395987492-6294437.post@n2.nabble.com>

Hi, 
I would have your help to validate my logistic regression. I know how to do
it. 

rlog<-glm(Y~X,family=binomial,data=tab)
summary(rlog)
HLgof.test(fit = fitted(rlog), obs=Y)

However, I would like to validate my model. For example to divise my data in
a sample for training (66%) and a sample for validation (34%).
e.g for ma table 
Area   Y     X 
1      1     135
1      0     200
1      1      97
1      1     160
1      0     201
1      1     144
1       0     100

But I don't know how to do it. 
1- My first problem: How to create my 2 samples from my variables Y and X
using pourcentage 66 ang 34 %?

- How to have the pourcentage of good prediction and bad prediction?

Thanks for your Help 
Komine 


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/To-validate-logistic-regression-tp6294437p6294437.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From HodgessE at uhd.edu  Fri Apr 22 13:01:05 2011
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Fri, 22 Apr 2011 06:01:05 -0500
Subject: [R-sig-Geo] question about putting KML files on Google My Maps
Message-ID: <586A4828D7AAA04CA9B258C50A28DA206DB6FE@BALI.uhd.campus>

Dear R Sig Geo People:
 
I have generated the attached KML file via R.  It works great in Google Earth.
 
But when I put it into Google My Maps and then try to look at it, the placemarks are there, but the info in the ballons has vanished.
 
Has anyone had a problem like this, please?
 
Thanks,
Erin
 
 
Erin M. Hodgess, PhD
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgesse at uhd.edu
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110422/68dcee5b/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: oza1.kml
Type: application/vnd.google-earth.kml+xml
Size: 16815 bytes
Desc: oza1.kml
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110422/68dcee5b/attachment.bin>

From luca.candeloro at gmail.com  Fri Apr 22 13:51:33 2011
From: luca.candeloro at gmail.com (luca candeloro)
Date: Fri, 22 Apr 2011 13:51:33 +0200
Subject: [R-sig-Geo] Adding points to Map objects in RgoogleMaps
Message-ID: <BANLkTimRxD7zgfvHnL4OnxpnhbSGVieumQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110422/6ed4ea84/attachment.pl>

From risemary48 at yahoo.com  Fri Apr 22 15:37:22 2011
From: risemary48 at yahoo.com (Mary Rise)
Date: Fri, 22 Apr 2011 06:37:22 -0700 (PDT)
Subject: [R-sig-Geo] Delete levels fo a factor
Message-ID: <535334.18512.qm@web120810.mail.ne1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110422/496b03f5/attachment.pl>

From roman.lustrik at gmail.com  Fri Apr 22 15:45:19 2011
From: roman.lustrik at gmail.com (=?UTF-8?Q?Roman_Lu=C5=A1trik?=)
Date: Fri, 22 Apr 2011 15:45:19 +0200
Subject: [R-sig-Geo] Delete levels fo a factor
In-Reply-To: <535334.18512.qm@web120810.mail.ne1.yahoo.com>
References: <535334.18512.qm@web120810.mail.ne1.yahoo.com>
Message-ID: <BANLkTi=sQ95GiFBJA6XASQp+64JW5+sWBw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110422/48ad1013/attachment.pl>

From b.rowlingson at lancaster.ac.uk  Fri Apr 22 16:33:25 2011
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 22 Apr 2011 15:33:25 +0100
Subject: [R-sig-Geo] question about putting KML files on Google My Maps
In-Reply-To: <586A4828D7AAA04CA9B258C50A28DA206DB6FE@BALI.uhd.campus>
References: <586A4828D7AAA04CA9B258C50A28DA206DB6FE@BALI.uhd.campus>
Message-ID: <BANLkTi=cVHsq2KQtmG0r__rhsnYAdNWLmw@mail.gmail.com>

On Fri, Apr 22, 2011 at 12:01 PM, Hodgess, Erin <HodgessE at uhd.edu> wrote:
> Dear R Sig Geo People:
>
> I have generated the attached KML file via R.? It works great in Google
> Earth.
>
> But when I put it into Google My Maps and then try to look at it, the
> placemarks are there, but the info in the ballons has vanished.
>
> Has anyone had a problem like this, please?
>

 I suspect the KML for My Maps doesn't support the ExtendedData block
that is in your original KML....

http://code.google.com/apis/kml/documentation/kmlelementsinmaps.html

says "untyped <Data> only, no <SimpleData> or <Schema>, and entity
replacements of the form $[dataName] are unsupported. "

Your KML uses ExtendedData with SimpleData from a Schema. I'm not sure
how you'd replace this with simple untyped <Data>, or how this would
look on My Maps....

Barry


From Greg.Snow at imail.org  Fri Apr 22 18:58:18 2011
From: Greg.Snow at imail.org (Greg Snow)
Date: Fri, 22 Apr 2011 10:58:18 -0600
Subject: [R-sig-Geo] Adding points to Map objects in RgoogleMaps
In-Reply-To: <BANLkTimRxD7zgfvHnL4OnxpnhbSGVieumQ@mail.gmail.com>
References: <BANLkTimRxD7zgfvHnL4OnxpnhbSGVieumQ@mail.gmail.com>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC63459F485A@LP-EXMBVS10.CO.IHC.COM>

Does the PlotArrowsOnStaticMap in the RgoogleMaps package help?

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111


> -----Original Message-----
> From: r-sig-geo-bounces at r-project.org [mailto:r-sig-geo-bounces at r-
> project.org] On Behalf Of luca candeloro
> Sent: Friday, April 22, 2011 5:52 AM
> To: r-sig-geo at r-project.org
> Subject: [R-sig-Geo] Adding points to Map objects in RgoogleMaps
> 
> Hi all,
> I'd like to add points and arrows to visualize connections between
> points in
> a map created with PlotOnStaticMap function.
> Here is my code:
> 
> library(RgoogleMaps)
> library(maptools)
> 
> LatMin=36.5
> LatMax=38.5
> LongMin=12
> LongMax=15.9
> CenterLat=(LatMin+LatMax)/2
> CenterLong=(LongMin+LongMax)/2
> 
> mzoom <- MaxZoom(latrange=c(LatMin,LatMax),
> lonrange=c(LongMin,LongMax),
> size=c(1280, 1280))[[1]]
> 
> Sicilia <- GetMap.bbox(center=c(CenterLat,CenterLong), zoom=8,
> destfile="C:/Sicilia.png", maptype="satellite")
> PlotOnStaticMap(Sicilia)
> 
> POINT1=c(14.6502685546875, 36.923547681089296)
> POINT2=c(14.7052001953125, 37.65338320128765)
> POINT3=c(13.5955810546875, 37.64903402157866)
> POINT4=c(14.3316650390625, 37.4530574713902)
> POINT5=c(13.7054443359375, 37.39634613318923)
> POINT6=c(15.1995849609375, 37.965854128749434)
> #here I'd insert something like arrows() or points() or plot() of a
> graph
> objects from igraph package.
> 
> How may I adjust points coordinates or projections?
> Thanks,
> Luca.
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From znmeb at borasky-research.net  Fri Apr 22 20:05:01 2011
From: znmeb at borasky-research.net (M. Edward (Ed) Borasky)
Date: Fri, 22 Apr 2011 11:05:01 -0700
Subject: [R-sig-Geo] Fwd: [CrisisMappers] GIS analysis of the sizes of
 population living near each of the world's nuclear power plants
In-Reply-To: <c527da94-ed41-44fd-9169-28d0d49b3d64@l2g2000prg.googlegroups.com>
References: <c527da94-ed41-44fd-9169-28d0d49b3d64@l2g2000prg.googlegroups.com>
Message-ID: <BANLkTimHX3hEk6YF-EaJjXur8FuHtD9qrw@mail.gmail.com>

---------- Forwarded message ----------
From: DB <declan.m.butler at gmail.com>
Date: Thu, Apr 21, 2011 at 2:32 PM
Subject: [CrisisMappers] GIS analysis of the sizes of population
living near each of the world's nuclear power plants
To: CrisisMappers <crisismappers at googlegroups.com>


Hi
Fyi; I've just published an analysis I did with the NASA Socioeconomic
Data and Applications Center operated by Columbia University?s Center
for International Earth Science Information Network (CIESIN), looking
at how many people live within certain distances of each of all the
world's nuclear power plants.
It shows, for example, that two-thirds of the world?s power plants
have more people living within a 30-kilometre radius than the 172,000
people living within 30 kilometres of the Fukushima Daiichi plante.
Some 21 plants have populations larger than 1 million within that
radius, and six have populations larger than 3 million. One hundred
and fifty-two nuclear power plants have more than 1 million people
living within 75 kilometres.

I've published the full results in the form of a Google Earth map here
-- http://www.declanbutler.info/nucpop/nucpop.kmz ?(this was a quick
map; I'll add a scale etc soon).

My article in Nature on this is here;
http://www.nature.com/news/2011/110421/full/472400a.html
When I get a moment, I'll publish the full dataset that we created.
Others have published similar analyses and maps for the US, and Die
Zeit did one for Germany, but as far as I know this is the first
published for populations near every nuclear power plant worldwide.
Best
Declan

--
You received this message because you are subscribed to the Google
Groups "CrisisMappers" group.
To post to this group, send email to crisismappers at googlegroups.com.
To unsubscribe from this group, send email to
crisismappers+unsubscribe at googlegroups.com.
For more options, visit this group at
http://groups.google.com/group/crisismappers?hl=en.




-- 
http://twitter.com/znmeb?http://borasky-research.net

"A mathematician is a device for turning coffee into theorems." -- Paul Erd?s


From rhunne at mac.com  Fri Apr 22 20:56:29 2011
From: rhunne at mac.com (Robin W Hunnewell)
Date: Fri, 22 Apr 2011 18:56:29 +0000 (GMT)
Subject: [R-sig-Geo] help with multivariate pt pattern analysis
Message-ID: <ea0816c0-c861-e1ca-deff-9779bce36279@me.com>

Dear list,

Is there a way to perform kernel density estimation on a density surface of spatial polygon objects, obtaining an estimate of the # of polygons contributing to the intensity of a spatial process? The polygons would be narrow zig-zag strips that -- taken together -- overlap within and constitute a composite search region; each one the realization of an aerial line transect survey that followed an idealized zig-zag transect layout.

Further to this query, I've hit a wall with thinking about how to handle some?potentially unusual pt pattern data; hope it's okay to post a spatial stats methodology question here as well -- need help!?

I have multivariate pt pattern data consisting of seabird flock locations observed during repeat aerial surveys. The data consist of flock locations (w/ marks) from ~30 line transect flights that used the same zig-zag transect layout. The flights followed the layout each time, but mapped together they do vary spatially about the idealized path -- so I have a set of polylines from which I created spatial polygon objects that correspond to the region of sea overflown on each flight. The flightpaths therefore are interwoven and occur (with varying intensity) in a composite search window, which is zig-zag shaped and constitutes a sizable buffer to the idealized transect layout. Hope this is clear.

I'm interested to investigate flock pt pattern data with the type of 'further methods for point patterns' outlined in Bailey & Gatrell (1995), i.e., analysis of multiple types of events and correcting for spatial variation in distribution of a 'background population.'

I'd like to examine flock pt patterns taken together, through time, and explore potential interactions/relationships between different types of flock events in the overall pattern. I'm interested to see if flock patterns from one survey to next exhibit independence: I'm reasonably confident they do, but want to test for this ?-- following methods outlined in Bailey&Gatrell.
I have a few hypotheses of clustering that are of interest, but comparison with CSR is not meaningful as the intensity of flocks is expected to vary with a 'background population' -- varying density surface of aerial coverage itself; the flightpaths from which my flock sightings can be said to 'arise' on any given survey occasion. I think I need is to adjust for this heterogeneity in order to detect any additional spatial structure that flock events might display over and above non-homogeneous distribution of aerial coverage itself.

Bailey& Gatrell discuss ways to adjust for an obvious covariate which is known to affect the rate at which events of interest occur, since otherwise - any patterns in event distribution may be obscured/eclipsed by spatial variation in the underlying covariate itself. One method they?suggest is taking a ratio of kernel estimates for the intensity of events and population density respectively, thus obtaining an adjusted estimate of the intensity of events on that basis. Hence my kernel estimation question above relating to polygon objects.
This way of correcting for a 'population at risk' applies in epidemiology, but Please can anyone comment as to the validity of using this approach in a flock event/flightpaths setting??

Thanks very much in advance,
Robin

------------------------------------------
Robin Hunnewell
University of New Brunswick
Fredericton, NB  
Canada 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110422/645fa777/attachment.html>

From jm3389 at columbia.edu  Sun Apr 24 16:55:41 2011
From: jm3389 at columbia.edu (jm3389)
Date: Sun, 24 Apr 2011 07:55:41 -0700 (PDT)
Subject: [R-sig-Geo] Manipulate extract output
Message-ID: <1303656941037-6301073.post@n2.nabble.com>

Dear All,

I have been struggling to extract individual pixel values from within a set
of polygons.

Mbola.shp <- readOGR(dsn="C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola",
"MBSubplot1LULC")
SMA <- raster("C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola/SMA-clipped.img")

X <- extract(SMA, Mbola.shp)

then when I display X I obtain a list:

>X
[[1]]
 [1] 0.08505697 0.11521957 0.07027940 0.07027940 0.10559745 0.10363697
0.09353689
 [8] 0.09353689 0.09673717 0.12536806 0.15696177 0.09306118 0.09353689
0.09673717
[15] 0.09306118 0.09306118 0.09669495

[[2]]
 [1]  0.02609529  0.02609529  0.02996226 -0.00057072  0.02159369  0.02159369
 [7]  0.04489277  0.02804214  0.02835380  0.06462935  0.06462935  0.02266412
[13]  0.03684234  0.02083779  0.07917891  0.05502048  0.03577228

[[3]]
 [1] 0.08705948 0.08705948 0.08622552 0.05902963 0.05902963 0.03294581
0.08919024
 [8] 0.10308155 0.10308155 0.04357137 0.06129387 0.18882106 0.15271443
0.15271443
[15] 0.07749587 0.23532785 0.23532785

[[4]]
 [1] 0.09554891 0.09314694 0.13511878 0.12017184 0.12056647 0.10497463
0.19552167
 [8] 0.19496135 0.10160323 0.09736327 0.12032449 0.18401594 0.13698930
0.09173016
[15] 0.11627619 0.13992140

[[5]]
 [1] 0.08130399 0.12848020 0.15129283 0.06018241 0.06018241 0.12764616
0.15831620
 [8] 0.16025396 0.07245398 0.07245398 0.08861573 0.15831620 0.16025396
0.07245398
[15] 0.08861573 0.10879661

[[6]]
 [1] 0.01403566 0.05509807 0.06573696 0.07086041 0.05806389 0.08060271
0.04586258
 [8] 0.04270630 0.07691538 0.06859619 0.08897498 0.06756449 0.04492125
0.04651234
[15] 0.04652581 0.08224432 0.04266668 0.05377325

[[7]]
 [1] 0.19860968 0.13489768 0.11530623 0.11530623 0.12832202 0.18902805
0.09427368
 [8] 0.08301217 0.08301217 0.11991798 0.18725793 0.19014162 0.14259814
0.14259814
[15] 0.08376002 0.18725793 0.14259814 0.14259814 0.08376002

[[8]]
 [1]  0.06091007  0.06091007  0.05323072  0.03962127  0.04709431  0.04709431
 [7]  0.05578892  0.06078413  0.03223819  0.02318165  0.02318165  0.04193906
[13]  0.04568461 -0.01365644  0.01866432  0.01866432  0.04833658  0.04675072
[19]  0.04113418  0.04113418  0.03635887


I would like to reshape the list like that:

Pixel value  | Polygon
0.08505697     [[1]]
0.11521957     [[1]]
0.07027940     [[1]]
0.07027940     [[1]]
0.10559745     [[1]]
...

I have tried stack, and the reshape package but haven't been able to
manipulate the list.
I am sure it's a very simple function but I've been searching the web for
days now and nothing really works.

Any suggestion?
Thanks
Joseph


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From gustavo.bio+R at gmail.com  Sun Apr 24 17:13:50 2011
From: gustavo.bio+R at gmail.com (Gustavo Carvalho)
Date: Sun, 24 Apr 2011 12:13:50 -0300
Subject: [R-sig-Geo] Manipulate extract output
In-Reply-To: <1303656941037-6301073.post@n2.nabble.com>
References: <1303656941037-6301073.post@n2.nabble.com>
Message-ID: <BANLkTikAKzY2fP=rbOnK8FLwmvoGY7fLSA@mail.gmail.com>

Dear Joseph,

Could you please try this?

names(X) <- 1:length(X)
stack(X)

Gustavo.

On Sun, Apr 24, 2011 at 11:55 AM, jm3389 <jm3389 at columbia.edu> wrote:
> Dear All,
>
> I have been struggling to extract individual pixel values from within a set
> of polygons.
>
> Mbola.shp <- readOGR(dsn="C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola",
> "MBSubplot1LULC")
> SMA <- raster("C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola/SMA-clipped.img")
>
> X <- extract(SMA, Mbola.shp)
>
> then when I display X I obtain a list:
>
>>X
> [[1]]
> ?[1] 0.08505697 0.11521957 0.07027940 0.07027940 0.10559745 0.10363697
> 0.09353689
> ?[8] 0.09353689 0.09673717 0.12536806 0.15696177 0.09306118 0.09353689
> 0.09673717
> [15] 0.09306118 0.09306118 0.09669495
>
> [[2]]
> ?[1] ?0.02609529 ?0.02609529 ?0.02996226 -0.00057072 ?0.02159369 ?0.02159369
> ?[7] ?0.04489277 ?0.02804214 ?0.02835380 ?0.06462935 ?0.06462935 ?0.02266412
> [13] ?0.03684234 ?0.02083779 ?0.07917891 ?0.05502048 ?0.03577228
>
> [[3]]
> ?[1] 0.08705948 0.08705948 0.08622552 0.05902963 0.05902963 0.03294581
> 0.08919024
> ?[8] 0.10308155 0.10308155 0.04357137 0.06129387 0.18882106 0.15271443
> 0.15271443
> [15] 0.07749587 0.23532785 0.23532785
>
> [[4]]
> ?[1] 0.09554891 0.09314694 0.13511878 0.12017184 0.12056647 0.10497463
> 0.19552167
> ?[8] 0.19496135 0.10160323 0.09736327 0.12032449 0.18401594 0.13698930
> 0.09173016
> [15] 0.11627619 0.13992140
>
> [[5]]
> ?[1] 0.08130399 0.12848020 0.15129283 0.06018241 0.06018241 0.12764616
> 0.15831620
> ?[8] 0.16025396 0.07245398 0.07245398 0.08861573 0.15831620 0.16025396
> 0.07245398
> [15] 0.08861573 0.10879661
>
> [[6]]
> ?[1] 0.01403566 0.05509807 0.06573696 0.07086041 0.05806389 0.08060271
> 0.04586258
> ?[8] 0.04270630 0.07691538 0.06859619 0.08897498 0.06756449 0.04492125
> 0.04651234
> [15] 0.04652581 0.08224432 0.04266668 0.05377325
>
> [[7]]
> ?[1] 0.19860968 0.13489768 0.11530623 0.11530623 0.12832202 0.18902805
> 0.09427368
> ?[8] 0.08301217 0.08301217 0.11991798 0.18725793 0.19014162 0.14259814
> 0.14259814
> [15] 0.08376002 0.18725793 0.14259814 0.14259814 0.08376002
>
> [[8]]
> ?[1] ?0.06091007 ?0.06091007 ?0.05323072 ?0.03962127 ?0.04709431 ?0.04709431
> ?[7] ?0.05578892 ?0.06078413 ?0.03223819 ?0.02318165 ?0.02318165 ?0.04193906
> [13] ?0.04568461 -0.01365644 ?0.01866432 ?0.01866432 ?0.04833658 ?0.04675072
> [19] ?0.04113418 ?0.04113418 ?0.03635887
>
>
> I would like to reshape the list like that:
>
> Pixel value ?| Polygon
> 0.08505697 ? ? [[1]]
> 0.11521957 ? ? [[1]]
> 0.07027940 ? ? [[1]]
> 0.07027940 ? ? [[1]]
> 0.10559745 ? ? [[1]]
> ...
>
> I have tried stack, and the reshape package but haven't been able to
> manipulate the list.
> I am sure it's a very simple function but I've been searching the web for
> days now and nothing really works.
>
> Any suggestion?
> Thanks
> Joseph
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From jm3389 at columbia.edu  Sun Apr 24 23:21:47 2011
From: jm3389 at columbia.edu (jm3389)
Date: Sun, 24 Apr 2011 14:21:47 -0700 (PDT)
Subject: [R-sig-Geo] Manipulate extract output
In-Reply-To: <BANLkTikAKzY2fP=rbOnK8FLwmvoGY7fLSA@mail.gmail.com>
References: <1303656941037-6301073.post@n2.nabble.com>
	<BANLkTikAKzY2fP=rbOnK8FLwmvoGY7fLSA@mail.gmail.com>
Message-ID: <BANLkTikGFZTb7GLJmLxWDUnbtv1e7QTLEQ@mail.gmail.com>

Thanks for the quick response Gustavo.

after names(X) <- 1:length(X)

I get this:

>X
$`1`
 [1] 0.08505697 0.11521957 0.07027940 0.07027940 0.10559745 0.10363697
0.09353689
 [8] 0.09353689 0.09673717 0.12536806 0.15696177 0.09306118 0.09353689
0.09673717
[15] 0.09306118 0.09306118 0.09669495

$`2`
 [1]  0.02609529  0.02609529  0.02996226 -0.00057072  0.02159369  0.02159369
 [7]  0.04489277  0.02804214  0.02835380  0.06462935  0.06462935  0.02266412
[13]  0.03684234  0.02083779  0.07917891  0.05502048  0.03577228

$`3`
 [1] 0.08705948 0.08705948 0.08622552 0.05902963 0.05902963 0.03294581
0.08919024
 [8] 0.10308155 0.10308155 0.04357137 0.06129387 0.18882106 0.15271443
0.15271443
[15] 0.07749587 0.23532785 0.23532785

And I am able to call individual vectors using this command X$'1', so that's
an improvement.

Yet the stack(X) still doesn't work

> stack(X)
Error in .local(x, ...) :
  Arguments should be Raster* objects or filenames


On Sun, Apr 24, 2011 at 11:15 AM, Gustavo Carvalho [via R-sig-geo] <
ml-node+6301092-942006797-332023 at n2.nabble.com> wrote:

> Dear Joseph,
>
> Could you please try this?
>
> names(X) <- 1:length(X)
> stack(X)
>
> Gustavo.
>
> On Sun, Apr 24, 2011 at 11:55 AM, jm3389 <[hidden email]<http://user/SendEmail.jtp?type=node&node=6301092&i=0&by-user=t>>
> wrote:
>
> > Dear All,
> >
> > I have been struggling to extract individual pixel values from within a
> set
> > of polygons.
> >
> > Mbola.shp <- readOGR(dsn="C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola",
> > "MBSubplot1LULC")
> > SMA <-
> raster("C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola/SMA-clipped.img")
> >
> > X <- extract(SMA, Mbola.shp)
> >
> > then when I display X I obtain a list:
> >
> >>X
> > [[1]]
> >  [1] 0.08505697 0.11521957 0.07027940 0.07027940 0.10559745 0.10363697
> > 0.09353689
> >  [8] 0.09353689 0.09673717 0.12536806 0.15696177 0.09306118 0.09353689
> > 0.09673717
> > [15] 0.09306118 0.09306118 0.09669495
> >
> > [[2]]
> >  [1]  0.02609529  0.02609529  0.02996226 -0.00057072  0.02159369
>  0.02159369
> >  [7]  0.04489277  0.02804214  0.02835380  0.06462935  0.06462935
>  0.02266412
> > [13]  0.03684234  0.02083779  0.07917891  0.05502048  0.03577228
> >
> > [[3]]
> >  [1] 0.08705948 0.08705948 0.08622552 0.05902963 0.05902963 0.03294581
> > 0.08919024
> >  [8] 0.10308155 0.10308155 0.04357137 0.06129387 0.18882106 0.15271443
> > 0.15271443
> > [15] 0.07749587 0.23532785 0.23532785
> >
> > [[4]]
> >  [1] 0.09554891 0.09314694 0.13511878 0.12017184 0.12056647 0.10497463
> > 0.19552167
> >  [8] 0.19496135 0.10160323 0.09736327 0.12032449 0.18401594 0.13698930
> > 0.09173016
> > [15] 0.11627619 0.13992140
> >
> > [[5]]
> >  [1] 0.08130399 0.12848020 0.15129283 0.06018241 0.06018241 0.12764616
> > 0.15831620
> >  [8] 0.16025396 0.07245398 0.07245398 0.08861573 0.15831620 0.16025396
> > 0.07245398
> > [15] 0.08861573 0.10879661
> >
> > [[6]]
> >  [1] 0.01403566 0.05509807 0.06573696 0.07086041 0.05806389 0.08060271
> > 0.04586258
> >  [8] 0.04270630 0.07691538 0.06859619 0.08897498 0.06756449 0.04492125
> > 0.04651234
> > [15] 0.04652581 0.08224432 0.04266668 0.05377325
> >
> > [[7]]
> >  [1] 0.19860968 0.13489768 0.11530623 0.11530623 0.12832202 0.18902805
> > 0.09427368
> >  [8] 0.08301217 0.08301217 0.11991798 0.18725793 0.19014162 0.14259814
> > 0.14259814
> > [15] 0.08376002 0.18725793 0.14259814 0.14259814 0.08376002
> >
> > [[8]]
> >  [1]  0.06091007  0.06091007  0.05323072  0.03962127  0.04709431
>  0.04709431
> >  [7]  0.05578892  0.06078413  0.03223819  0.02318165  0.02318165
>  0.04193906
> > [13]  0.04568461 -0.01365644  0.01866432  0.01866432  0.04833658
>  0.04675072
> > [19]  0.04113418  0.04113418  0.03635887
> >
> >
> > I would like to reshape the list like that:
> >
> > Pixel value  | Polygon
> > 0.08505697     [[1]]
> > 0.11521957     [[1]]
> > 0.07027940     [[1]]
> > 0.07027940     [[1]]
> > 0.10559745     [[1]]
> > ...
> >
> > I have tried stack, and the reshape package but haven't been able to
> > manipulate the list.
> > I am sure it's a very simple function but I've been searching the web for
>
> > days now and nothing really works.
> >
> > Any suggestion?
> > Thanks
> > Joseph
> >
> >
> > --
> > View this message in context:
> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html<http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html?by-user=t>
> > Sent from the R-sig-geo mailing list archive at Nabble.com.
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > [hidden email]<http://user/SendEmail.jtp?type=node&node=6301092&i=1&by-user=t>
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> _______________________________________________
> R-sig-Geo mailing list
> [hidden email]<http://user/SendEmail.jtp?type=node&node=6301092&i=2&by-user=t>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301092.html
>  To unsubscribe from Manipulate extract output, click here<http://r-sig-geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=6301073&code=am0zMzg5QGNvbHVtYmlhLmVkdXw2MzAxMDczfDkxNTAzOTkyNg==>.
>
>


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301561.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From gustavo.bio+R at gmail.com  Sun Apr 24 23:53:35 2011
From: gustavo.bio+R at gmail.com (Gustavo Carvalho)
Date: Sun, 24 Apr 2011 18:53:35 -0300
Subject: [R-sig-Geo] Manipulate extract output
In-Reply-To: <BANLkTikGFZTb7GLJmLxWDUnbtv1e7QTLEQ@mail.gmail.com>
References: <1303656941037-6301073.post@n2.nabble.com>
	<BANLkTikAKzY2fP=rbOnK8FLwmvoGY7fLSA@mail.gmail.com>
	<BANLkTikGFZTb7GLJmLxWDUnbtv1e7QTLEQ@mail.gmail.com>
Message-ID: <BANLkTikj=mhNQYWy3c5w-rmseDVjK2pv4A@mail.gmail.com>

utils::stack(X)

On Sun, Apr 24, 2011 at 6:21 PM, jm3389 <jm3389 at columbia.edu> wrote:
> Thanks for the quick response Gustavo.
>
> after names(X) <- 1:length(X)
>
> I get this:
>
>>X
> $`1`
> ?[1] 0.08505697 0.11521957 0.07027940 0.07027940 0.10559745 0.10363697
> 0.09353689
> ?[8] 0.09353689 0.09673717 0.12536806 0.15696177 0.09306118 0.09353689
> 0.09673717
> [15] 0.09306118 0.09306118 0.09669495
>
> $`2`
> ?[1] ?0.02609529 ?0.02609529 ?0.02996226 -0.00057072 ?0.02159369 ?0.02159369
> ?[7] ?0.04489277 ?0.02804214 ?0.02835380 ?0.06462935 ?0.06462935 ?0.02266412
> [13] ?0.03684234 ?0.02083779 ?0.07917891 ?0.05502048 ?0.03577228
>
> $`3`
> ?[1] 0.08705948 0.08705948 0.08622552 0.05902963 0.05902963 0.03294581
> 0.08919024
> ?[8] 0.10308155 0.10308155 0.04357137 0.06129387 0.18882106 0.15271443
> 0.15271443
> [15] 0.07749587 0.23532785 0.23532785
>
> And I am able to call individual vectors using this command X$'1', so that's
> an improvement.
>
> Yet the stack(X) still doesn't work
>
>> stack(X)
> Error in .local(x, ...) :
> ?Arguments should be Raster* objects or filenames
>
>
> On Sun, Apr 24, 2011 at 11:15 AM, Gustavo Carvalho [via R-sig-geo] <
> ml-node+6301092-942006797-332023 at n2.nabble.com> wrote:
>
>> Dear Joseph,
>>
>> Could you please try this?
>>
>> names(X) <- 1:length(X)
>> stack(X)
>>
>> Gustavo.
>>
>> On Sun, Apr 24, 2011 at 11:55 AM, jm3389 <[hidden email]<http://user/SendEmail.jtp?type=node&node=6301092&i=0&by-user=t>>
>> wrote:
>>
>> > Dear All,
>> >
>> > I have been struggling to extract individual pixel values from within a
>> set
>> > of polygons.
>> >
>> > Mbola.shp <- readOGR(dsn="C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola",
>> > "MBSubplot1LULC")
>> > SMA <-
>> raster("C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola/SMA-clipped.img")
>> >
>> > X <- extract(SMA, Mbola.shp)
>> >
>> > then when I display X I obtain a list:
>> >
>> >>X
>> > [[1]]
>> > ?[1] 0.08505697 0.11521957 0.07027940 0.07027940 0.10559745 0.10363697
>> > 0.09353689
>> > ?[8] 0.09353689 0.09673717 0.12536806 0.15696177 0.09306118 0.09353689
>> > 0.09673717
>> > [15] 0.09306118 0.09306118 0.09669495
>> >
>> > [[2]]
>> > ?[1] ?0.02609529 ?0.02609529 ?0.02996226 -0.00057072 ?0.02159369
>> ?0.02159369
>> > ?[7] ?0.04489277 ?0.02804214 ?0.02835380 ?0.06462935 ?0.06462935
>> ?0.02266412
>> > [13] ?0.03684234 ?0.02083779 ?0.07917891 ?0.05502048 ?0.03577228
>> >
>> > [[3]]
>> > ?[1] 0.08705948 0.08705948 0.08622552 0.05902963 0.05902963 0.03294581
>> > 0.08919024
>> > ?[8] 0.10308155 0.10308155 0.04357137 0.06129387 0.18882106 0.15271443
>> > 0.15271443
>> > [15] 0.07749587 0.23532785 0.23532785
>> >
>> > [[4]]
>> > ?[1] 0.09554891 0.09314694 0.13511878 0.12017184 0.12056647 0.10497463
>> > 0.19552167
>> > ?[8] 0.19496135 0.10160323 0.09736327 0.12032449 0.18401594 0.13698930
>> > 0.09173016
>> > [15] 0.11627619 0.13992140
>> >
>> > [[5]]
>> > ?[1] 0.08130399 0.12848020 0.15129283 0.06018241 0.06018241 0.12764616
>> > 0.15831620
>> > ?[8] 0.16025396 0.07245398 0.07245398 0.08861573 0.15831620 0.16025396
>> > 0.07245398
>> > [15] 0.08861573 0.10879661
>> >
>> > [[6]]
>> > ?[1] 0.01403566 0.05509807 0.06573696 0.07086041 0.05806389 0.08060271
>> > 0.04586258
>> > ?[8] 0.04270630 0.07691538 0.06859619 0.08897498 0.06756449 0.04492125
>> > 0.04651234
>> > [15] 0.04652581 0.08224432 0.04266668 0.05377325
>> >
>> > [[7]]
>> > ?[1] 0.19860968 0.13489768 0.11530623 0.11530623 0.12832202 0.18902805
>> > 0.09427368
>> > ?[8] 0.08301217 0.08301217 0.11991798 0.18725793 0.19014162 0.14259814
>> > 0.14259814
>> > [15] 0.08376002 0.18725793 0.14259814 0.14259814 0.08376002
>> >
>> > [[8]]
>> > ?[1] ?0.06091007 ?0.06091007 ?0.05323072 ?0.03962127 ?0.04709431
>> ?0.04709431
>> > ?[7] ?0.05578892 ?0.06078413 ?0.03223819 ?0.02318165 ?0.02318165
>> ?0.04193906
>> > [13] ?0.04568461 -0.01365644 ?0.01866432 ?0.01866432 ?0.04833658
>> ?0.04675072
>> > [19] ?0.04113418 ?0.04113418 ?0.03635887
>> >
>> >
>> > I would like to reshape the list like that:
>> >
>> > Pixel value ?| Polygon
>> > 0.08505697 ? ? [[1]]
>> > 0.11521957 ? ? [[1]]
>> > 0.07027940 ? ? [[1]]
>> > 0.07027940 ? ? [[1]]
>> > 0.10559745 ? ? [[1]]
>> > ...
>> >
>> > I have tried stack, and the reshape package but haven't been able to
>> > manipulate the list.
>> > I am sure it's a very simple function but I've been searching the web for
>>
>> > days now and nothing really works.
>> >
>> > Any suggestion?
>> > Thanks
>> > Joseph
>> >
>> >
>> > --
>> > View this message in context:
>> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html<http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html?by-user=t>
>> > Sent from the R-sig-geo mailing list archive at Nabble.com.
>> >
>> > _______________________________________________
>> > R-sig-Geo mailing list
>> > [hidden email]<http://user/SendEmail.jtp?type=node&node=6301092&i=1&by-user=t>
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> [hidden email]<http://user/SendEmail.jtp?type=node&node=6301092&i=2&by-user=t>
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
>> ------------------------------
>> ?If you reply to this email, your message will be added to the discussion
>> below:
>>
>> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301092.html
>> ?To unsubscribe from Manipulate extract output, click here<http://r-sig-geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=6301073&code=am0zMzg5QGNvbHVtYmlhLmVkdXw2MzAxMDczfDkxNTAzOTkyNg==>.
>>
>>
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301561.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From jm3389 at columbia.edu  Mon Apr 25 01:33:32 2011
From: jm3389 at columbia.edu (jm3389)
Date: Sun, 24 Apr 2011 16:33:32 -0700 (PDT)
Subject: [R-sig-Geo] Manipulate extract output
In-Reply-To: <BANLkTikj=mhNQYWy3c5w-rmseDVjK2pv4A@mail.gmail.com>
References: <1303656941037-6301073.post@n2.nabble.com>
	<BANLkTikAKzY2fP=rbOnK8FLwmvoGY7fLSA@mail.gmail.com>
	<BANLkTikGFZTb7GLJmLxWDUnbtv1e7QTLEQ@mail.gmail.com>
	<BANLkTikj=mhNQYWy3c5w-rmseDVjK2pv4A@mail.gmail.com>
Message-ID: <BANLkTikgmcCkRvU5dqKDFFVA-201sxH=yA@mail.gmail.com>

My bad forgot to add utils::

it's working ! Thanks a lot Gustavo

>utils::stack(X)
             values       ind
1     0.0850569680   1
2     0.1152195707   1
3     0.0702793971   1
4     0.0702793971   1
5     0.1055974513   1
6     0.1036369652   1
7     0.0935368910   1
8     0.0935368910   1
9     0.0967371687   1
10    0.1253680587   1
11    0.1569617689   1
12    0.0930611789   1
13    0.0935368910   1
14    0.0967371687   1
15    0.0930611789   1
16    0.0930611789   1
17    0.0966949463   1
18    0.0260952916   2
19    0.0260952916   2
20    0.0299622640   2
21   -0.0005707200   2
22    0.0215936881   2

---------------------------------------------------------
Joseph Muhlhausen
Project development researcher | CIESIN
61 Route 9W, PO Box 1000
Palisades, NY 10964 USA | 845-365-8935




On Sun, Apr 24, 2011 at 5:54 PM, Gustavo Carvalho [via R-sig-geo] <
ml-node+6301600-211949547-332023 at n2.nabble.com> wrote:

> utils::stack(X)
>
> On Sun, Apr 24, 2011 at 6:21 PM, jm3389 <[hidden email]<http://user/SendEmail.jtp?type=node&node=6301600&i=0&by-user=t>>
> wrote:
>
> > Thanks for the quick response Gustavo.
> >
> > after names(X) <- 1:length(X)
> >
> > I get this:
> >
> >>X
> > $`1`
> >  [1] 0.08505697 0.11521957 0.07027940 0.07027940 0.10559745 0.10363697
> > 0.09353689
> >  [8] 0.09353689 0.09673717 0.12536806 0.15696177 0.09306118 0.09353689
> > 0.09673717
> > [15] 0.09306118 0.09306118 0.09669495
> >
> > $`2`
> >  [1]  0.02609529  0.02609529  0.02996226 -0.00057072  0.02159369
>  0.02159369
> >  [7]  0.04489277  0.02804214  0.02835380  0.06462935  0.06462935
>  0.02266412
> > [13]  0.03684234  0.02083779  0.07917891  0.05502048  0.03577228
> >
> > $`3`
> >  [1] 0.08705948 0.08705948 0.08622552 0.05902963 0.05902963 0.03294581
> > 0.08919024
> >  [8] 0.10308155 0.10308155 0.04357137 0.06129387 0.18882106 0.15271443
> > 0.15271443
> > [15] 0.07749587 0.23532785 0.23532785
> >
> > And I am able to call individual vectors using this command X$'1', so
> that's
> > an improvement.
> >
> > Yet the stack(X) still doesn't work
> >
> >> stack(X)
> > Error in .local(x, ...) :
> >  Arguments should be Raster* objects or filenames
> >
> >
> > On Sun, Apr 24, 2011 at 11:15 AM, Gustavo Carvalho [via R-sig-geo] <
> > [hidden email]<http://user/SendEmail.jtp?type=node&node=6301600&i=1&by-user=t>>
> wrote:
> >
> >> Dear Joseph,
> >>
> >> Could you please try this?
> >>
> >> names(X) <- 1:length(X)
> >> stack(X)
> >>
> >> Gustavo.
> >>
> >> On Sun, Apr 24, 2011 at 11:55 AM, jm3389 <[hidden email]<
> http://user/SendEmail.jtp?type=node&node=6301092&i=0&by-user=t>>
> >> wrote:
> >>
> >> > Dear All,
> >> >
> >> > I have been struggling to extract individual pixel values from within
> a
> >> set
> >> > of polygons.
> >> >
> >> > Mbola.shp <-
> readOGR(dsn="C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola",
> >> > "MBSubplot1LULC")
> >> > SMA <-
> >> raster("C:/Users/JOX201S/Desktop/Dropbox/LDSF/Mbola/SMA-clipped.img")
> >> >
> >> > X <- extract(SMA, Mbola.shp)
> >> >
> >> > then when I display X I obtain a list:
> >> >
> >> >>X
> >> > [[1]]
> >> >  [1] 0.08505697 0.11521957 0.07027940 0.07027940 0.10559745 0.10363697
>
> >> > 0.09353689
> >> >  [8] 0.09353689 0.09673717 0.12536806 0.15696177 0.09306118
> 0.09353689
> >> > 0.09673717
> >> > [15] 0.09306118 0.09306118 0.09669495
> >> >
> >> > [[2]]
> >> >  [1]  0.02609529  0.02609529  0.02996226 -0.00057072  0.02159369
> >>  0.02159369
> >> >  [7]  0.04489277  0.02804214  0.02835380  0.06462935  0.06462935
> >>  0.02266412
> >> > [13]  0.03684234  0.02083779  0.07917891  0.05502048  0.03577228
> >> >
> >> > [[3]]
> >> >  [1] 0.08705948 0.08705948 0.08622552 0.05902963 0.05902963 0.03294581
>
> >> > 0.08919024
> >> >  [8] 0.10308155 0.10308155 0.04357137 0.06129387 0.18882106 0.15271443
>
> >> > 0.15271443
> >> > [15] 0.07749587 0.23532785 0.23532785
> >> >
> >> > [[4]]
> >> >  [1] 0.09554891 0.09314694 0.13511878 0.12017184 0.12056647 0.10497463
>
> >> > 0.19552167
> >> >  [8] 0.19496135 0.10160323 0.09736327 0.12032449 0.18401594
> 0.13698930
> >> > 0.09173016
> >> > [15] 0.11627619 0.13992140
> >> >
> >> > [[5]]
> >> >  [1] 0.08130399 0.12848020 0.15129283 0.06018241 0.06018241 0.12764616
>
> >> > 0.15831620
> >> >  [8] 0.16025396 0.07245398 0.07245398 0.08861573 0.15831620
> 0.16025396
> >> > 0.07245398
> >> > [15] 0.08861573 0.10879661
> >> >
> >> > [[6]]
> >> >  [1] 0.01403566 0.05509807 0.06573696 0.07086041 0.05806389 0.08060271
>
> >> > 0.04586258
> >> >  [8] 0.04270630 0.07691538 0.06859619 0.08897498 0.06756449
> 0.04492125
> >> > 0.04651234
> >> > [15] 0.04652581 0.08224432 0.04266668 0.05377325
> >> >
> >> > [[7]]
> >> >  [1] 0.19860968 0.13489768 0.11530623 0.11530623 0.12832202 0.18902805
>
> >> > 0.09427368
> >> >  [8] 0.08301217 0.08301217 0.11991798 0.18725793 0.19014162
> 0.14259814
> >> > 0.14259814
> >> > [15] 0.08376002 0.18725793 0.14259814 0.14259814 0.08376002
> >> >
> >> > [[8]]
> >> >  [1]  0.06091007  0.06091007  0.05323072  0.03962127  0.04709431
> >>  0.04709431
> >> >  [7]  0.05578892  0.06078413  0.03223819  0.02318165  0.02318165
> >>  0.04193906
> >> > [13]  0.04568461 -0.01365644  0.01866432  0.01866432  0.04833658
> >>  0.04675072
> >> > [19]  0.04113418  0.04113418  0.03635887
> >> >
> >> >
> >> > I would like to reshape the list like that:
> >> >
> >> > Pixel value  | Polygon
> >> > 0.08505697     [[1]]
> >> > 0.11521957     [[1]]
> >> > 0.07027940     [[1]]
> >> > 0.07027940     [[1]]
> >> > 0.10559745     [[1]]
> >> > ...
> >> >
> >> > I have tried stack, and the reshape package but haven't been able to
> >> > manipulate the list.
> >> > I am sure it's a very simple function but I've been searching the web
> for
> >>
> >> > days now and nothing really works.
> >> >
> >> > Any suggestion?
> >> > Thanks
> >> > Joseph
> >> >
> >> >
> >> > --
> >> > View this message in context:
> >>
> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html<http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html?by-user=t><http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html%3Chttp://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301073.html?by-user=t%3E&by-user=t>
>
> >> > Sent from the R-sig-geo mailing list archive at Nabble.com.
> >> >
> >> > _______________________________________________
> >> > R-sig-Geo mailing list
> >> > [hidden email]<
> http://user/SendEmail.jtp?type=node&node=6301092&i=1&by-user=t>
> >> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >> >
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> [hidden email]<
> http://user/SendEmail.jtp?type=node&node=6301092&i=2&by-user=t>
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >>
> >> ------------------------------
> >>  If you reply to this email, your message will be added to the
> discussion
> >> below:
> >>
> >>
>
> >>
> >>
> >
> >
> > --
> > View this message in context:
> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301561.html<http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301561.html?by-user=t>
>
> > Sent from the R-sig-geo mailing list archive at Nabble.com.
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > [hidden email]<http://user/SendEmail.jtp?type=node&node=6301600&i=2&by-user=t>
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> _______________________________________________
> R-sig-Geo mailing list
> [hidden email]<http://user/SendEmail.jtp?type=node&node=6301600&i=3&by-user=t>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301600.html
>  To unsubscribe from Manipulate extract output, click here<http://r-sig-geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=6301073&code=am0zMzg5QGNvbHVtYmlhLmVkdXw2MzAxMDczfDkxNTAzOTkyNg==>.
>
>


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6301690.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From alp97 at hotmail.com  Mon Apr 25 14:11:03 2011
From: alp97 at hotmail.com (Alper ALTINOK)
Date: Mon, 25 Apr 2011 12:11:03 +0000
Subject: [R-sig-Geo] How to handle data non-uniform in time and space
Message-ID: <BLU161-w9C334C6C2CA4FF7BE9E99BC960@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110425/ea1aab29/attachment.pl>

From hadley at rice.edu  Mon Apr 25 18:07:40 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Mon, 25 Apr 2011 11:07:40 -0500
Subject: [R-sig-Geo] spplot, polygons, and line density fill
In-Reply-To: <4D9F2906.6050508@isciences.com>
References: <4D9F2815.4090205@isciences.com> <4D9F2906.6050508@isciences.com>
Message-ID: <BANLkTinrN0n_kRvHOmWgXrmdPCp+Uz=zxA@mail.gmail.com>

Unfortunately this type of fill is not currently easy with grid
graphics, which is what both spplot and ggplot2 use.

Hadley

On Fri, Apr 8, 2011 at 10:25 AM, Matthew Landis <landis at isciences.com> wrote:
> In case my request wasn't really clear, let me rephrase:
>
> I'd like to overlay some polygons onto a raster, by plotting with a line
> density fill instead of a solid fill, as shown here
> http://r-spatial.sourceforge.net/gallery/#fig15.R. ?Ideally I would be able
> to modify the angle of the lines. ?I know how to do this in base graphics,
> but I really need to use spplot (or willing to use ggplot2 if that can do
> what I want).
>
> Thanks in advance for any ideas!
>
> Matt
>
> On 4/8/2011 11:21 AM, Matthew Landis wrote:
>>
>> Hello again R-sig-geo-ers,
>>
>> Does anyone know how to convince spplot to create a figure like
>> http://r-spatial.sourceforge.net/gallery/#fig15.R
>>
>> I would like to overlay some polygons onto a raster, and I need to do it
>> with spplot for the lattice functionality. ?I've looked at the help and the
>> function code for sp.polygon and grid.polygon, as well as searched a bit on
>> Google, but I'm not finding anything promising, but then, I'm not that
>> familiar with grid graphics.
>>
>> Since we're talking about R, I have a hard to time believing the answer is
>> "it's not possible".
>>
>> M
>> <http://r-spatial.sourceforge.net/gallery/#fig15.R>
>>
>
> --
> ~~~~~~~~~~~~~~~~~~~~~~~~~~
> Matthew Landis, Ph.D.
> Research Scientist
> ISciences, LLC
> 61 Main St. Suite 200
> Burlington VT 05401
> 802.864.2999
> ~~~~~~~~~~~~~~~~~~~~~~~~~~
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From landis at isciences.com  Mon Apr 25 18:38:06 2011
From: landis at isciences.com (Matthew Landis)
Date: Mon, 25 Apr 2011 12:38:06 -0400
Subject: [R-sig-Geo] spplot, polygons, and line density fill
In-Reply-To: <BANLkTinrN0n_kRvHOmWgXrmdPCp+Uz=zxA@mail.gmail.com>
References: <4D9F2815.4090205@isciences.com> <4D9F2906.6050508@isciences.com>
	<BANLkTinrN0n_kRvHOmWgXrmdPCp+Uz=zxA@mail.gmail.com>
Message-ID: <4DB5A36E.4090004@isciences.com>

Hi Hadley,

Can you elaborate on what you mean by "not...easy"?  Does that mean 
"impossible", or  "not currently possible without writing a lot of low 
level code",  or something else?  Are there any resources you know that 
might help to implement it?

Thanks,

Matt

On 4/25/2011 12:07 PM, Hadley Wickham wrote:
> Unfortunately this type of fill is not currently easy with grid
> graphics, which is what both spplot and ggplot2 use.
>
> Hadley

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~
Matthew Landis, Ph.D.
Research Scientist
ISciences, LLC
61 Main St. Suite 200
Burlington VT 05401
802.864.2999
~~~~~~~~~~~~~~~~~~~~~~~~~~


From hadley at rice.edu  Mon Apr 25 20:27:26 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Mon, 25 Apr 2011 13:27:26 -0500
Subject: [R-sig-Geo] spplot, polygons, and line density fill
In-Reply-To: <4DB5A36E.4090004@isciences.com>
References: <4D9F2815.4090205@isciences.com> <4D9F2906.6050508@isciences.com>
	<BANLkTinrN0n_kRvHOmWgXrmdPCp+Uz=zxA@mail.gmail.com>
	<4DB5A36E.4090004@isciences.com>
Message-ID: <BANLkTimjgJd13647O-_ZmD_TdUjwJjYGxg@mail.gmail.com>

Hi Matthew,

The basic problem is that there is no low-level support for pattern
fills in grid. To resolve this, you need:

 * some way of parameterising pattern fills using other grobs.  A good
start would be to think about filling a region with regularly spaced
lines of different angles.

 * some way of clipping that filling pattern to arbitrary polygons
(not sure how hard that is to do)

 * some way of making it efficient, maybe using rasterGrob.

I haven't looked it into enough to know how hard or each each of these
steps would be.  You might try also asking on the ggplot2 mailing
list, because other's may have a better idea than me.

Hadley

On Mon, Apr 25, 2011 at 11:38 AM, Matthew Landis <landis at isciences.com> wrote:
> Hi Hadley,
>
> Can you elaborate on what you mean by "not...easy"? ?Does that mean
> "impossible", or ?"not currently possible without writing a lot of low level
> code", ?or something else? ?Are there any resources you know that might help
> to implement it?
>
> Thanks,
>
> Matt
>
> On 4/25/2011 12:07 PM, Hadley Wickham wrote:
>>
>> Unfortunately this type of fill is not currently easy with grid
>> graphics, which is what both spplot and ggplot2 use.
>>
>> Hadley
>
> --
> ~~~~~~~~~~~~~~~~~~~~~~~~~~
> Matthew Landis, Ph.D.
> Research Scientist
> ISciences, LLC
> 61 Main St. Suite 200
> Burlington VT 05401
> 802.864.2999
> ~~~~~~~~~~~~~~~~~~~~~~~~~~
>
>
>



-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From jm3389 at columbia.edu  Mon Apr 25 20:45:13 2011
From: jm3389 at columbia.edu (jm3389)
Date: Mon, 25 Apr 2011 11:45:13 -0700 (PDT)
Subject: [R-sig-Geo] Manipulate extract output
In-Reply-To: <BANLkTikgmcCkRvU5dqKDFFVA-201sxH=yA@mail.gmail.com>
References: <1303656941037-6301073.post@n2.nabble.com>
	<BANLkTikAKzY2fP=rbOnK8FLwmvoGY7fLSA@mail.gmail.com>
	<BANLkTikGFZTb7GLJmLxWDUnbtv1e7QTLEQ@mail.gmail.com>
	<BANLkTikj=mhNQYWy3c5w-rmseDVjK2pv4A@mail.gmail.com>
	<BANLkTikgmcCkRvU5dqKDFFVA-201sxH=yA@mail.gmail.com>
Message-ID: <1303757113092-6303557.post@n2.nabble.com>

I have an additional question.

I know have extracted multiple bands and my list layout has changed.

[[50]]
      SMA-clipped_1 SMA-clipped_2 SMA-clipped_3
 [1,]  0.0632362440    0.14176662     0.8255592
 [2,]  0.1722273529    0.10599104     0.7607671
 [3,]  0.1722273529    0.10599104     0.7607671
 [4,]  0.2838407159    0.05971635     0.6902863
 [5,]  0.0257746875    0.15945776     0.8517801
 [6,]  0.2064613551    0.06632594     0.7841757
 [7,]  0.2064613551    0.06632594     0.7841757
 [8,]  0.2621570826    0.02389294     0.7842789
 [9,]  0.2310775816    0.04211631     0.7626599
[10,]  0.0287268255    0.18798502     0.8102853
[11,]  0.0848897994    0.09991057     0.8653935
[12,]  0.0848897994    0.09991057     0.8653935
[13,]  0.1673026830    0.01770168     0.8713655
[14,]  0.1527486742    0.03026910     0.8653567
[15,]  0.0003926796    0.14994104     0.8714772
[16,]  0.0003926796    0.14994104     0.8714772
[17,]  0.0026585322    0.10952766     0.9157249

[[51]]
      SMA-clipped_1 SMA-clipped_2 SMA-clipped_3
 [1,]    0.15911901     0.3241347     0.5089146
 [2,]    0.15911150     0.3826110     0.4281845
 [3,]    0.15911150     0.3826110     0.4281845
 [4,]    0.10494564     0.2808739     0.5892844
 [5,]    0.14055550     0.2827841     0.5563493
 [6,]    0.13600963     0.3378267     0.4836067
 


How can I get the same output but with this time 3 bands?

      SMA-clipped_1 | SMA-clipped_2 | SMA-clipped_3 | Plot_ID
[1,]  0.0632362440    0.14176662     0.8255592             [[50]]
 [2,]  0.1722273529    0.10599104     0.7607671            [[50]]
 [3,]  0.1722273529    0.10599104     0.7607671            [[50]]
 [4,]  0.2838407159    0.05971635     0.6902863            [[50]]
 [5,]  0.0257746875    0.15945776     0.8517801            [[50]]
 [6,]  0.2064613551    0.06632594     0.7841757            [[50]]
 [7,]  0.2064613551    0.06632594     0.7841757            [[50]]
 [8,]  0.2621570826    0.02389294     0.7842789            [[50]]
 [9,]  0.2310775816    0.04211631     0.7626599            [[50]]
[10,]  0.0287268255    0.18798502     0.8102853           [[50]]
[11,]  0.0848897994    0.09991057     0.8653935           [[50]]
[12,]  0.0848897994    0.09991057     0.8653935           [[50]]
[13,]  0.1673026830    0.01770168     0.8713655           [[50]]
[14,]  0.1527486742    0.03026910     0.8653567           [[50]]
[15,]  0.0003926796    0.14994104     0.8714772           [[50]]
[16,]  0.0003926796    0.14994104     0.8714772           [[50]]
[17,]  0.0026585322    0.10952766     0.9157249           [[50]]


jm3389 wrote:
> 
> My bad forgot to add utils::
> 
> it's working ! Thanks a lot Gustavo
> 
>>utils::stack(X)
>              values       ind
> 1     0.0850569680   1
> 2     0.1152195707   1
> 3     0.0702793971   1
> 4     0.0702793971   1
> 5     0.1055974513   1
> 6     0.1036369652   1
> 7     0.0935368910   1
> 8     0.0935368910   1
> 9     0.0967371687   1
> 10    0.1253680587   1
> 11    0.1569617689   1
> 12    0.0930611789   1
> 
> 
>> 
>>  On Sun, Apr 24, 2011 at 11:15 AM, Gustavo Carvalho [via R-sig-geo] <
>>  [hidden
>> email]&lt;http://user/SendEmail.jtp?type=node&amp;node=6301600&amp;i=1&amp;by-user=t&gt;>
>> wrote:
>> >
>> >> Dear Joseph,
>> >>
>> >> Could you please try this?
>> >>
>> >> names(X) <- 1:length(X)
>> >> stack(X)
> 
> 


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6303557.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From landis at isciences.com  Mon Apr 25 20:53:53 2011
From: landis at isciences.com (Matthew Landis)
Date: Mon, 25 Apr 2011 14:53:53 -0400
Subject: [R-sig-Geo] spplot, polygons, and line density fill
In-Reply-To: <BANLkTimjgJd13647O-_ZmD_TdUjwJjYGxg@mail.gmail.com>
References: <4D9F2815.4090205@isciences.com> <4D9F2906.6050508@isciences.com>
	<BANLkTinrN0n_kRvHOmWgXrmdPCp+Uz=zxA@mail.gmail.com>
	<4DB5A36E.4090004@isciences.com>
	<BANLkTimjgJd13647O-_ZmD_TdUjwJjYGxg@mail.gmail.com>
Message-ID: <4DB5C341.3030702@isciences.com>

Many thanks for these tips Hadley.  It's unlikely that I'll be able to 
dive into this soon, but at least now I have some starting points.

Best,

Matt

On 4/25/2011 2:27 PM, Hadley Wickham wrote:
> Hi Matthew,
>
> The basic problem is that there is no low-level support for pattern
> fills in grid. To resolve this, you need:
>
>   * some way of parameterising pattern fills using other grobs.  A good
> start would be to think about filling a region with regularly spaced
> lines of different angles.
>
>   * some way of clipping that filling pattern to arbitrary polygons
> (not sure how hard that is to do)
>
>   * some way of making it efficient, maybe using rasterGrob.
>
> I haven't looked it into enough to know how hard or each each of these
> steps would be.  You might try also asking on the ggplot2 mailing
> list, because other's may have a better idea than me.
>
> Hadley

-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~
Matthew Landis, Ph.D.
Research Scientist
ISciences, LLC
61 Main St. Suite 200
Burlington VT 05401
802.864.2999
~~~~~~~~~~~~~~~~~~~~~~~~~~


From gustavo.bio+R at gmail.com  Mon Apr 25 23:21:22 2011
From: gustavo.bio+R at gmail.com (Gustavo Carvalho)
Date: Mon, 25 Apr 2011 18:21:22 -0300
Subject: [R-sig-Geo] Manipulate extract output
In-Reply-To: <1303757113092-6303557.post@n2.nabble.com>
References: <1303656941037-6301073.post@n2.nabble.com>
	<BANLkTikAKzY2fP=rbOnK8FLwmvoGY7fLSA@mail.gmail.com>
	<BANLkTikGFZTb7GLJmLxWDUnbtv1e7QTLEQ@mail.gmail.com>
	<BANLkTikj=mhNQYWy3c5w-rmseDVjK2pv4A@mail.gmail.com>
	<BANLkTikgmcCkRvU5dqKDFFVA-201sxH=yA@mail.gmail.com>
	<1303757113092-6303557.post@n2.nabble.com>
Message-ID: <BANLkTikPM5frfXRU=CMxe5aWuuaxZSVaqg@mail.gmail.com>

Something like this should work:

bleh <- structure(list(`1` = structure(list(a = 1:10, b = 21:30),
.Names = c("a",
"b"), row.names = c(NA, -10L), class = "data.frame"), `2` = structure(list(
    a = 31:40, b = 41:50), .Names = c("a", "b"), row.names = c(NA,
-10L), class = "data.frame"), `3` = structure(list(a = 51:60,
    b = 61:70), .Names = c("a", "b"), row.names = c(NA, -10L), class =
"data.frame")), .Names = c("1",
"2", "3"))

bleh

cbind(Reduce(rbind, bleh), plot = rep(names(bleh), times = sapply(bleh, NROW)))

Don't forget to set the names attribute of your list.

Gustavo.

On Mon, Apr 25, 2011 at 3:45 PM, jm3389 <jm3389 at columbia.edu> wrote:
> I have an additional question.
>
> I know have extracted multiple bands and my list layout has changed.
>
> [[50]]
> ? ? ?SMA-clipped_1 SMA-clipped_2 SMA-clipped_3
> ?[1,] ?0.0632362440 ? ?0.14176662 ? ? 0.8255592
> ?[2,] ?0.1722273529 ? ?0.10599104 ? ? 0.7607671
> ?[3,] ?0.1722273529 ? ?0.10599104 ? ? 0.7607671
> ?[4,] ?0.2838407159 ? ?0.05971635 ? ? 0.6902863
> ?[5,] ?0.0257746875 ? ?0.15945776 ? ? 0.8517801
> ?[6,] ?0.2064613551 ? ?0.06632594 ? ? 0.7841757
> ?[7,] ?0.2064613551 ? ?0.06632594 ? ? 0.7841757
> ?[8,] ?0.2621570826 ? ?0.02389294 ? ? 0.7842789
> ?[9,] ?0.2310775816 ? ?0.04211631 ? ? 0.7626599
> [10,] ?0.0287268255 ? ?0.18798502 ? ? 0.8102853
> [11,] ?0.0848897994 ? ?0.09991057 ? ? 0.8653935
> [12,] ?0.0848897994 ? ?0.09991057 ? ? 0.8653935
> [13,] ?0.1673026830 ? ?0.01770168 ? ? 0.8713655
> [14,] ?0.1527486742 ? ?0.03026910 ? ? 0.8653567
> [15,] ?0.0003926796 ? ?0.14994104 ? ? 0.8714772
> [16,] ?0.0003926796 ? ?0.14994104 ? ? 0.8714772
> [17,] ?0.0026585322 ? ?0.10952766 ? ? 0.9157249
>
> [[51]]
> ? ? ?SMA-clipped_1 SMA-clipped_2 SMA-clipped_3
> ?[1,] ? ?0.15911901 ? ? 0.3241347 ? ? 0.5089146
> ?[2,] ? ?0.15911150 ? ? 0.3826110 ? ? 0.4281845
> ?[3,] ? ?0.15911150 ? ? 0.3826110 ? ? 0.4281845
> ?[4,] ? ?0.10494564 ? ? 0.2808739 ? ? 0.5892844
> ?[5,] ? ?0.14055550 ? ? 0.2827841 ? ? 0.5563493
> ?[6,] ? ?0.13600963 ? ? 0.3378267 ? ? 0.4836067
>
>
>
> How can I get the same output but with this time 3 bands?
>
> ? ? ?SMA-clipped_1 | SMA-clipped_2 | SMA-clipped_3 | Plot_ID
> [1,] ?0.0632362440 ? ?0.14176662 ? ? 0.8255592 ? ? ? ? ? ? [[50]]
> ?[2,] ?0.1722273529 ? ?0.10599104 ? ? 0.7607671 ? ? ? ? ? ?[[50]]
> ?[3,] ?0.1722273529 ? ?0.10599104 ? ? 0.7607671 ? ? ? ? ? ?[[50]]
> ?[4,] ?0.2838407159 ? ?0.05971635 ? ? 0.6902863 ? ? ? ? ? ?[[50]]
> ?[5,] ?0.0257746875 ? ?0.15945776 ? ? 0.8517801 ? ? ? ? ? ?[[50]]
> ?[6,] ?0.2064613551 ? ?0.06632594 ? ? 0.7841757 ? ? ? ? ? ?[[50]]
> ?[7,] ?0.2064613551 ? ?0.06632594 ? ? 0.7841757 ? ? ? ? ? ?[[50]]
> ?[8,] ?0.2621570826 ? ?0.02389294 ? ? 0.7842789 ? ? ? ? ? ?[[50]]
> ?[9,] ?0.2310775816 ? ?0.04211631 ? ? 0.7626599 ? ? ? ? ? ?[[50]]
> [10,] ?0.0287268255 ? ?0.18798502 ? ? 0.8102853 ? ? ? ? ? [[50]]
> [11,] ?0.0848897994 ? ?0.09991057 ? ? 0.8653935 ? ? ? ? ? [[50]]
> [12,] ?0.0848897994 ? ?0.09991057 ? ? 0.8653935 ? ? ? ? ? [[50]]
> [13,] ?0.1673026830 ? ?0.01770168 ? ? 0.8713655 ? ? ? ? ? [[50]]
> [14,] ?0.1527486742 ? ?0.03026910 ? ? 0.8653567 ? ? ? ? ? [[50]]
> [15,] ?0.0003926796 ? ?0.14994104 ? ? 0.8714772 ? ? ? ? ? [[50]]
> [16,] ?0.0003926796 ? ?0.14994104 ? ? 0.8714772 ? ? ? ? ? [[50]]
> [17,] ?0.0026585322 ? ?0.10952766 ? ? 0.9157249 ? ? ? ? ? [[50]]
>
>
> jm3389 wrote:
>>
>> My bad forgot to add utils::
>>
>> it's working ! Thanks a lot Gustavo
>>
>>>utils::stack(X)
>> ? ? ? ? ? ? ?values ? ? ? ind
>> 1 ? ? 0.0850569680 ? 1
>> 2 ? ? 0.1152195707 ? 1
>> 3 ? ? 0.0702793971 ? 1
>> 4 ? ? 0.0702793971 ? 1
>> 5 ? ? 0.1055974513 ? 1
>> 6 ? ? 0.1036369652 ? 1
>> 7 ? ? 0.0935368910 ? 1
>> 8 ? ? 0.0935368910 ? 1
>> 9 ? ? 0.0967371687 ? 1
>> 10 ? ?0.1253680587 ? 1
>> 11 ? ?0.1569617689 ? 1
>> 12 ? ?0.0930611789 ? 1
>>
>>
>>>
>>> ?On Sun, Apr 24, 2011 at 11:15 AM, Gustavo Carvalho [via R-sig-geo] <
>>> ?[hidden
>>> email]&lt;http://user/SendEmail.jtp?type=node&amp;node=6301600&amp;i=1&amp;by-user=t&gt;>
>>> wrote:
>>> >
>>> >> Dear Joseph,
>>> >>
>>> >> Could you please try this?
>>> >>
>>> >> names(X) <- 1:length(X)
>>> >> stack(X)
>>
>>
>
>
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6303557.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From jm3389 at columbia.edu  Tue Apr 26 05:10:36 2011
From: jm3389 at columbia.edu (jm3389)
Date: Mon, 25 Apr 2011 20:10:36 -0700 (PDT)
Subject: [R-sig-Geo] Manipulate extract output
In-Reply-To: <BANLkTikPM5frfXRU=CMxe5aWuuaxZSVaqg@mail.gmail.com>
References: <1303656941037-6301073.post@n2.nabble.com>
	<BANLkTikAKzY2fP=rbOnK8FLwmvoGY7fLSA@mail.gmail.com>
	<BANLkTikGFZTb7GLJmLxWDUnbtv1e7QTLEQ@mail.gmail.com>
	<BANLkTikj=mhNQYWy3c5w-rmseDVjK2pv4A@mail.gmail.com>
	<BANLkTikgmcCkRvU5dqKDFFVA-201sxH=yA@mail.gmail.com>
	<1303757113092-6303557.post@n2.nabble.com>
	<BANLkTikPM5frfXRU=CMxe5aWuuaxZSVaqg@mail.gmail.com>
Message-ID: <BANLkTikTmSUc946rBbe1izwF19BRc3KEpw@mail.gmail.com>

I works, I dont know why there is quote marks. But I can merge with other
data frames, so it's all good for me.

Thank you again for all your help Gustavo

> names(X) <- 1:length(X)
> X.stack <- cbind(Reduce(rbind, X), plot = rep(names(X), times = sapply(X,
NROW)))
> X.stack
        SMA-clipped_1           SMA-clipped_2           SMA-clipped_3
  plot
   [1,] "0.0850569680333138"    "0.525055825710297"     "0.391901522874832"
  "1"
   [2,] "0.115219570696354"     "0.433272808790207"     "0.48068830370903"
 "1"
   [3,] "0.0702793970704079"    "0.451244980096817"     "0.480365782976151"
  "1"
   [4,] "0.0702793970704079"    "0.451244980096817"     "0.480365782976151"
  "1"
   [5,] "0.105597451329231"     "0.470949500799179"     "0.450028508901596"
  "1"
   [6,] "0.103636965155602"     "0.436819821596146"     "0.457322180271149"
  "1"
   [7,] "0.0935368910431862"    "0.454399734735489"     "0.45674803853035"
 "1"
   [8,] "0.0935368910431862"    "0.454399734735489"     "0.45674803853035"
 "1"


---------------------------------------------------------
Joseph Muhlhausen
Project development researcher | CIESIN
61 Route 9W, PO Box 1000
Palisades, NY 10964 USA | 845-365-8935




On Mon, Apr 25, 2011 at 5:30 PM, Gustavo Carvalho [via R-sig-geo] <
ml-node+6303936-889868159-332023 at n2.nabble.com> wrote:

> Something like this should work:
>
> bleh <- structure(list(`1` = structure(list(a = 1:10, b = 21:30),
> .Names = c("a",
> "b"), row.names = c(NA, -10L), class = "data.frame"), `2` = structure(list(
>
>     a = 31:40, b = 41:50), .Names = c("a", "b"), row.names = c(NA,
> -10L), class = "data.frame"), `3` = structure(list(a = 51:60,
>     b = 61:70), .Names = c("a", "b"), row.names = c(NA, -10L), class =
> "data.frame")), .Names = c("1",
> "2", "3"))
>
> bleh
>
> cbind(Reduce(rbind, bleh), plot = rep(names(bleh), times = sapply(bleh,
> NROW)))
>
> Don't forget to set the names attribute of your list.
>
> Gustavo.
>
> On Mon, Apr 25, 2011 at 3:45 PM, jm3389 <[hidden email]<http://user/SendEmail.jtp?type=node&node=6303936&i=0&by-user=t>>
> wrote:
>
> > I have an additional question.
> >
> > I know have extracted multiple bands and my list layout has changed.
> >
> > [[50]]
> >      SMA-clipped_1 SMA-clipped_2 SMA-clipped_3
> >  [1,]  0.0632362440    0.14176662     0.8255592
> >  [2,]  0.1722273529    0.10599104     0.7607671
> >  [3,]  0.1722273529    0.10599104     0.7607671
> >  [4,]  0.2838407159    0.05971635     0.6902863
> >  [5,]  0.0257746875    0.15945776     0.8517801
> >  [6,]  0.2064613551    0.06632594     0.7841757
> >  [7,]  0.2064613551    0.06632594     0.7841757
> >  [8,]  0.2621570826    0.02389294     0.7842789
> >  [9,]  0.2310775816    0.04211631     0.7626599
> > [10,]  0.0287268255    0.18798502     0.8102853
> > [11,]  0.0848897994    0.09991057     0.8653935
> > [12,]  0.0848897994    0.09991057     0.8653935
> > [13,]  0.1673026830    0.01770168     0.8713655
> > [14,]  0.1527486742    0.03026910     0.8653567
> > [15,]  0.0003926796    0.14994104     0.8714772
> > [16,]  0.0003926796    0.14994104     0.8714772
> > [17,]  0.0026585322    0.10952766     0.9157249
> >
> > [[51]]
> >      SMA-clipped_1 SMA-clipped_2 SMA-clipped_3
> >  [1,]    0.15911901     0.3241347     0.5089146
> >  [2,]    0.15911150     0.3826110     0.4281845
> >  [3,]    0.15911150     0.3826110     0.4281845
> >  [4,]    0.10494564     0.2808739     0.5892844
> >  [5,]    0.14055550     0.2827841     0.5563493
> >  [6,]    0.13600963     0.3378267     0.4836067
> >
> >
> >
> > How can I get the same output but with this time 3 bands?
> >
> >      SMA-clipped_1 | SMA-clipped_2 | SMA-clipped_3 | Plot_ID
> > [1,]  0.0632362440    0.14176662     0.8255592             [[50]]
> >  [2,]  0.1722273529    0.10599104     0.7607671            [[50]]
> >  [3,]  0.1722273529    0.10599104     0.7607671            [[50]]
> >  [4,]  0.2838407159    0.05971635     0.6902863            [[50]]
> >  [5,]  0.0257746875    0.15945776     0.8517801            [[50]]
> >  [6,]  0.2064613551    0.06632594     0.7841757            [[50]]
> >  [7,]  0.2064613551    0.06632594     0.7841757            [[50]]
> >  [8,]  0.2621570826    0.02389294     0.7842789            [[50]]
> >  [9,]  0.2310775816    0.04211631     0.7626599            [[50]]
> > [10,]  0.0287268255    0.18798502     0.8102853           [[50]]
> > [11,]  0.0848897994    0.09991057     0.8653935           [[50]]
> > [12,]  0.0848897994    0.09991057     0.8653935           [[50]]
> > [13,]  0.1673026830    0.01770168     0.8713655           [[50]]
> > [14,]  0.1527486742    0.03026910     0.8653567           [[50]]
> > [15,]  0.0003926796    0.14994104     0.8714772           [[50]]
> > [16,]  0.0003926796    0.14994104     0.8714772           [[50]]
> > [17,]  0.0026585322    0.10952766     0.9157249           [[50]]
> >
> >
> > jm3389 wrote:
> >>
> >> My bad forgot to add utils::
> >>
> >> it's working ! Thanks a lot Gustavo
> >>
> >>>utils::stack(X)
> >>              values       ind
> >> 1     0.0850569680   1
> >> 2     0.1152195707   1
> >> 3     0.0702793971   1
> >> 4     0.0702793971   1
> >> 5     0.1055974513   1
> >> 6     0.1036369652   1
> >> 7     0.0935368910   1
> >> 8     0.0935368910   1
> >> 9     0.0967371687   1
> >> 10    0.1253680587   1
> >> 11    0.1569617689   1
> >> 12    0.0930611789   1
> >>
> >>
> >>>
> >>>  On Sun, Apr 24, 2011 at 11:15 AM, Gustavo Carvalho [via R-sig-geo] <
> >>>  [hidden
> >>> email]&lt;
> http://user/SendEmail.jtp?type=node&amp;node=6301600&amp;i=1&amp;by-user=t&gt<http://user/SendEmail.jtp?type=node&node=6301600&i=1&by-user=t&gt>
> ;>
> >>> wrote:
> >>> >
> >>> >> Dear Joseph,
> >>> >>
> >>> >> Could you please try this?
> >>> >>
> >>> >> names(X) <- 1:length(X)
> >>> >> stack(X)
> >>
> >>
> >
> >
> > --
> > View this message in context:
> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6303557.html<http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6303557.html?by-user=t>
>
> > Sent from the R-sig-geo mailing list archive at Nabble.com.
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > [hidden email]<http://user/SendEmail.jtp?type=node&node=6303936&i=1&by-user=t>
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> _______________________________________________
> R-sig-Geo mailing list
> [hidden email]<http://user/SendEmail.jtp?type=node&node=6303936&i=2&by-user=t>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6303936.html
>  To unsubscribe from Manipulate extract output, click here<http://r-sig-geo.2731867.n2.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=6301073&code=am0zMzg5QGNvbHVtYmlhLmVkdXw2MzAxMDczfDkxNTAzOTkyNg==>.
>
>


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/Manipulate-extract-output-tp6301073p6304478.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From marcelino.delacruz at upm.es  Tue Apr 26 10:58:18 2011
From: marcelino.delacruz at upm.es (Marcelino de la Cruz)
Date: Tue, 26 Apr 2011 10:58:18 +0200
Subject: [R-sig-Geo] comparing spatial distribution of two species in a
 same area with different coordinates
In-Reply-To: <BANLkTi=H7QWsDTJ0eEVanLd4ZHTcwqDN6w@mail.gmail.com>
References: <BANLkTinxBv1_y3o74Vbcve2tQ2Y382zCnw@mail.gmail.com>
	<BANLkTi=H7QWsDTJ0eEVanLd4ZHTcwqDN6w@mail.gmail.com>
Message-ID: <201104260858.p3Q8wKk3000642@smtp.upm.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110426/da78a2b5/attachment.pl>

From bram.van.moorter at gmail.com  Tue Apr 26 12:24:00 2011
From: bram.van.moorter at gmail.com (Bram Van Moorter)
Date: Tue, 26 Apr 2011 12:24:00 +0200
Subject: [R-sig-Geo] To validate logistic regression
Message-ID: <BANLkTi=2EovS_uRTrcrTWFQFeN9HmrKCxw@mail.gmail.com>

Dear Komine,
Not sure whether this is the easiest way, but it has worked for me:

set.seed(0)
head(tab <- data.frame(Y=as.numeric(runif(100)>0.5), X=rnorm(100)))
subs <- sample(c(1:nrow(tab)), round(nrow(tab)*0.66), replace=F)  #the
66% of data you want in one sample
tab1 <- tab[subs, ] #the one sample
tab2 <- tab[!c(1:nrow(tab)) %in% subs, ] #the other sample, which are
the data that do not fall in the first sample

rlog1 <- glm(Y~X,family=binomial,data=tab1)
summary(rlog1)
tab2$pred <-predict(rlog1, newdata=tab2, type="response")
hist(tab2$pred)

library(ROCR) #allows you to make easily ROC's which allows the
assessment of your prediction
pred <- prediction(tab2$pred, tab2$Y)
perf <- performance(pred,"tpr","fpr")
plot(perf); abline(0, 1, col="red")  #the proportional line shows that
the prediction is as good as random, which you would expect in this
example

Best,
Bram


> Hi,
> I would like your help to validate my logistic regression. I know how to do
> logistic regression.
>
> rlog<-glm(Y~X,family=binomial,data=tab)
> summary(rlog)
> HLgof.test(fit = fitted(rlog), obs=Y)
>
> However, I would like to validate my model. For example to divise my data in
> a sample for training (66%) and a sample for validation (34%).
> e.g for ma table
> Area ? Y ? ? X
> 1 ? ? ? 1 ? ? 135
> 1 ? ? ? 0 ? ? 200
> 1 ? ? ? 1 ? ? ?97
> 1 ? ? ? 1 ? ? 160
> 1 ? ? ? 0 ? ? 201
> 1 ? ? ? 1 ? ? 144
> 1 ? ? ? 0 ? ? 100
>
> But I don't know how to validate it.
> 1- My first problem: How to create my 2 samples from my variables Y and X
> using pourcentage 66 ang 34 %?
>
> - How to have the pourcentage of good prediction and bad prediction?
>
> Thanks for your Help
> Komine
>


-- 
Bram Van Moorter
Centre for Conservation Biology (NTNU),
Norwegian Institute for Nature Research (NINA)
Trondheim (Norway)
email: ?Bram.Van.Moorter at gmail.com
website: http://ase-research.org/moorter
phone: +47 73596060


From debeaudette at ucdavis.edu  Tue Apr 26 15:58:56 2011
From: debeaudette at ucdavis.edu (Dylan Beaudette)
Date: Tue, 26 Apr 2011 06:58:56 -0700
Subject: [R-sig-Geo] To validate logistic regression
In-Reply-To: <BANLkTi=2EovS_uRTrcrTWFQFeN9HmrKCxw@mail.gmail.com>
References: <BANLkTi=2EovS_uRTrcrTWFQFeN9HmrKCxw@mail.gmail.com>
Message-ID: <201104260658.56091.dylan.beaudette@gmail.com>

Another approach:

See ?lrm, ?validate, and ?calibrate from the rms package. 

Dylan

On Tuesday, April 26, 2011, Bram Van Moorter wrote:
> Dear Komine,
> Not sure whether this is the easiest way, but it has worked for me:
> 
> set.seed(0)
> head(tab <- data.frame(Y=as.numeric(runif(100)>0.5), X=rnorm(100)))
> subs <- sample(c(1:nrow(tab)), round(nrow(tab)*0.66), replace=F)  #the
> 66% of data you want in one sample
> tab1 <- tab[subs, ] #the one sample
> tab2 <- tab[!c(1:nrow(tab)) %in% subs, ] #the other sample, which are
> the data that do not fall in the first sample
> 
> rlog1 <- glm(Y~X,family=binomial,data=tab1)
> summary(rlog1)
> tab2$pred <-predict(rlog1, newdata=tab2, type="response")
> hist(tab2$pred)
> 
> library(ROCR) #allows you to make easily ROC's which allows the
> assessment of your prediction
> pred <- prediction(tab2$pred, tab2$Y)
> perf <- performance(pred,"tpr","fpr")
> plot(perf); abline(0, 1, col="red")  #the proportional line shows that
> the prediction is as good as random, which you would expect in this
> example
> 
> Best,
> Bram
> 
> 
> > Hi,
> > I would like your help to validate my logistic regression. I know how to 
do
> > logistic regression.
> >
> > rlog<-glm(Y~X,family=binomial,data=tab)
> > summary(rlog)
> > HLgof.test(fit = fitted(rlog), obs=Y)
> >
> > However, I would like to validate my model. For example to divise my data 
in
> > a sample for training (66%) and a sample for validation (34%).
> > e.g for ma table
> > Area   Y     X
> > 1       1     135
> > 1       0     200
> > 1       1      97
> > 1       1     160
> > 1       0     201
> > 1       1     144
> > 1       0     100
> >
> > But I don't know how to validate it.
> > 1- My first problem: How to create my 2 samples from my variables Y and X
> > using pourcentage 66 ang 34 %?
> >
> > - How to have the pourcentage of good prediction and bad prediction?
> >
> > Thanks for your Help
> > Komine
> >
> 
> 
> -- 
> Bram Van Moorter
> Centre for Conservation Biology (NTNU),
> Norwegian Institute for Nature Research (NINA)
> Trondheim (Norway)
> email:  Bram.Van.Moorter at gmail.com
> website: http://ase-research.org/moorter
> phone: +47 73596060
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 


-- 
Dylan E. Beaudette
USDA-NRCS Soil Scientist
California Soil Resource Lab
http://casoilresource.lawr.ucdavis.edu/


From Tim.Haering at lwf.bayern.de  Tue Apr 26 18:09:13 2011
From: Tim.Haering at lwf.bayern.de (=?iso-8859-1?Q?H=E4ring=2C_Tim_=28LWF=29?=)
Date: Tue, 26 Apr 2011 18:09:13 +0200
Subject: [R-sig-Geo] hide legend in plot raster
Message-ID: <70FC67C4A585D1489E66225A4E0238BAECA996@RZS-EXC-CL06.rz-sued.bayern.de>

Dear list,

I would like to overlay two raster objects to produce a nice map. The underlying raster is a hillshade, the second is transparent.
plot(hillshade, col=grey(1:20/20))
plot(soilraster, alpha = 0.7, add = TRUE)

Does anybody know, how to hide the legend of the hillshade?

Thank you.
TIM

------------------------------------------- 
Tim H?ring
Bavarian State Institute of Forest Research 
Department of Soil and Climate
Hans-Carl-von-Carlowitz-Platz 1
D-85354 Freising
E-Mail: tim.haering at lwf.bayern.de
http://www.lwf.bayern.de


From sa.cizmeli at usherbrooke.ca  Tue Apr 26 18:52:17 2011
From: sa.cizmeli at usherbrooke.ca (scizmeli)
Date: Tue, 26 Apr 2011 09:52:17 -0700 (PDT)
Subject: [R-sig-Geo] spacetime : the challenge of image
	time	series	containing holes
In-Reply-To: <4DA4D7A3.2040803@uni-muenster.de>
References: <4D9FA7CE.2000108@usherbrooke.ca> <4DA08BFE.9070302@wzw.tum.de>
	<1302621601781-6265557.post@n2.nabble.com>
	<4DA4D7A3.2040803@uni-muenster.de>
Message-ID: <1303836737225-6306523.post@n2.nabble.com>

dear Edzer,

Thanks a lot for the wonderful improvement. It certainly makes it easier.

Servet

--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/spacetime-the-challenge-of-image-time-series-containing-holes-tp6255766p6306523.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From sa.cizmeli at usherbrooke.ca  Tue Apr 26 19:15:48 2011
From: sa.cizmeli at usherbrooke.ca (scizmeli)
Date: Tue, 26 Apr 2011 10:15:48 -0700 (PDT)
Subject: [R-sig-Geo] spacetime : adding a fourth (spectral) dimension
Message-ID: <1303838148993-6306599.post@n2.nabble.com>

I have recently been introduced to the promising package called spacetime
which helps solve some of the data management problems we regularly
encounter with time-series of spatial data. I would like to congratulate the
developers of this wonderful package.

My question : Is there a way to incorporate in a spacetime object additional
dimensions? For example, in remote sensing, we regularly deal with time
series of spectral imagery. I can easily think of a 5-D model output
(timeseries of a 3-d model of a spectral parameter). In such a case, it
would also be useful to be able to specify auxiliary data for each band i.e.
wavelength information and its units (412 nm, 515 nm etc). It would of
course be better if these auxiliary data can be defined in a custom-manner.

If this is not possible for now, can it be considered in the near future?
Spatial data nowadays can easily have more than 3 dimensions and it would be
very useful seeing this addressed by the spacetime package.

Servet

--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/spacetime-adding-a-fourth-spectral-dimension-tp6306599p6306599.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From aritra.sourav at gmail.com  Tue Apr 26 20:53:35 2011
From: aritra.sourav at gmail.com (sourav sengupta)
Date: Tue, 26 Apr 2011 14:53:35 -0400
Subject: [R-sig-Geo] Plotting Data the globe
Message-ID: <BANLkTik-U_EOn3R0UbQ6xpTmwSCg_QrMfQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110426/2c448c95/attachment.pl>

From emorway at engr.colostate.edu  Wed Apr 27 00:36:02 2011
From: emorway at engr.colostate.edu (emorway)
Date: Tue, 26 Apr 2011 15:36:02 -0700 (PDT)
Subject: [R-sig-Geo] summing a variable number of rasters
Message-ID: <1303857362606-6307555.post@n2.nabble.com>

Hello,

I'm working with precipitation data as estimated by the reflectivity of a
radar 'sweep' (1 complete radar rotation).  Depending upon whether or not
there is a storm in the area, the NOAA radar will automatically speed up or
slow down.  This leads to a variable number of 'sweeps' for any given day. 
The problem I wish to solve is this:  I would like to sum all the sweeps for
each day of the year, keeping in mind that some days may have 140 sweeps and
others may have as many as 350 sweeps).  My initial approach has been to
create a text file containing the names of all the files in the directory
where the ascii grids reside.  I accomplished this from the command line
using the command: "DIR /B /O:n > filenames.txt"  

Each line of the file "filenames.txt" contains a name of a file in the
directory, for example:

KRGX_DPA_20090218_000100_native.asc
KRGX_DPA_20090218_000100_native.prj
KRGX_DPA_20090218_000700_native.asc
KRGX_DPA_20090218_000700_native.prj
KRGX_DPA_20090218_001300_native.asc
KRGX_DPA_20090218_001300_native.prj
KRGX_DPA_20090218_001900_native.asc
KRGX_DPA_20090218_001900_native.prj
KRGX_DPA_20090218_002400_native.asc
KRGX_DPA_20090218_002400_native.prj
.
.

The naming convention is uniform: KRGX_DPA_YYYYMMDD_HHMMSS_native.asc.  So,
to get the daily precipitation, I just need to sum up all of the files that
fall on February 18th, 2009, or February 19th, 2009, etc.  My problem is I
don't know how to code the loop in R to go through each day (and sometimes
there are days with missing/no data) and read in the appropriate rasters and
finally sum them and write out the result, preserving the header
information.  Each raster has the same header information, for example:

NCOLS 131
NROWS 131
XLLCORNER -1704975.0000000
YLLCORNER -5710237.5000000
CELLSIZE 4762.5000000
NODATA_VALUE -999.0000000
-999.0000000 -999.0000000 -999.0000000 -999.0000000 ...
-999.0000000 -999.0000000 -999.0000000 -999.0000000 ...
-999.0000000 -999.0000000 -999.0000000 -999.0000000 ...
-999.0000000 -999.0000000 -999.0000000 -999.0000000 ...
.
.
.

An example of a complete raster file is attached (I haven't checked if there
is
precip on this particular day or not) (the projection file is also
attached).  

Suggestions to help me get started are very much appreciated.
Eric

http://r-sig-geo.2731867.n2.nabble.com/file/n6307555/KRGX_DPA_20090605_173400_native.asc
KRGX_DPA_20090605_173400_native.asc 
http://r-sig-geo.2731867.n2.nabble.com/file/n6307555/KRGX_DPA_20090605_173400_native.prj
KRGX_DPA_20090605_173400_native.prj 


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/summing-a-variable-number-of-rasters-tp6307555p6307555.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From reeves at nceas.ucsb.edu  Wed Apr 27 02:49:09 2011
From: reeves at nceas.ucsb.edu (Rick Reeves)
Date: Tue, 26 Apr 2011 17:49:09 -0700
Subject: [R-sig-Geo] From NCEAS: Notice discrepancy in raster subimages
 clipped with same extent -
Message-ID: <4DB76805.3070107@nceas.ucsb.edu>

Hi Robert:

Apologize for not posting this yet on rSigGeo, but I am under
a bit of time pressure:

When clipping two images with the same extent object:

Browse[2]> eTestAreaExtent
class       : Extent
xmin        : -135.2
xmax        : -100.2
ymin        : 59.998
ymax        : 60.002

The resulting images have different extents:

rEdgeRegionCDEM <- crop(inputCDEMRaster, eTestAreaExtent)
rEdgeRegionAsterCgiar <- crop(inputAsterCgiarRaster, eTestAreaExtent)

Browse[2]> extent(rEdgeRegionCDEM)
class       : Extent
xmin        : -135.1996
xmax        : -100.2002
ymin        : 59.99808
ymax        : 60.00224
Browse[2]> extent(rEdgeRegionAsterCgiar)
class       : Extent
xmin        : -135.1998
xmax        : -100.2003
ymin        : 59.99772
ymax        : 60.00189

..As a result, the command:

rDelta <- rEdgeRegionAsterCgiar - rEdgeRegionCDEM

results in:

Error in compare(e1, e2, extent = FALSE, rowcol = FALSE, prj = TRUE, res 
= TRUE,  :
   different origin

if I add the 'hack':

extent(rEdgeRegionCDEM) <- extent(rEdgeRegionAsterCgiar)

Then the subtraction operation works. But I doubt the true
absolute geographic alignment of the two images.

Any insights?

thanks,
Rick Reeves
NCEAS



Using raster package on:

platform       x86_64-pc-linux-gnu
arch           x86_64
os             linux-gnu
system         x86_64, linux-gnu
status
major          2
minor          13.0
year           2011
month          04
day            13
svn rev        55427
language       R
version.string R version 2.13.0 (2011-04-13)


From adamhsparks at gmail.com  Wed Apr 27 02:54:31 2011
From: adamhsparks at gmail.com (Adam Sparks)
Date: Wed, 27 Apr 2011 08:54:31 +0800
Subject: [R-sig-Geo] hide legend in plot raster
In-Reply-To: <70FC67C4A585D1489E66225A4E0238BAECA996@RZS-EXC-CL06.rz-sued.bayern.de>
References: <70FC67C4A585D1489E66225A4E0238BAECA996@RZS-EXC-CL06.rz-sued.bayern.de>
Message-ID: <BANLkTimWZTwuwNMieBpVu-=4VUkhWkSAtA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110427/ba1bfdfd/attachment.pl>

From r.hijmans at gmail.com  Wed Apr 27 03:00:59 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Tue, 26 Apr 2011 18:00:59 -0700
Subject: [R-sig-Geo] From NCEAS: Notice discrepancy in raster subimages
 clipped with same extent -
In-Reply-To: <4DB76805.3070107@nceas.ucsb.edu>
References: <4DB76805.3070107@nceas.ucsb.edu>
Message-ID: <BANLkTikYU0+hnxAcEL7q2zK=j6S8U1ufmw@mail.gmail.com>

Rick,

This suggests that  inputCDEMRaster and inputAsterCgiarRaster do not
have exactly the same origin (the raster cells do not exactly align --
within the tolerance that raster uses).

While you clip with the same extent, the clipped result is always in
"rounded to the nearest whole cells".

The question is whether this is wrong (caused by rounding or another
error) or not, and whether the difference is big enough to care. If in
fact they do align, or if you do not care, you can use your hack. If
not, resample.

This is the difference. It is small, but you it would be about a
quarter cell if the resolution is 0.008333333
> ymin ? ? ? ?: 59.99808
> ymin ? ? ? ?: 59.99772

Robert

On Tue, Apr 26, 2011 at 5:49 PM, Rick Reeves <reeves at nceas.ucsb.edu> wrote:
> Hi Robert:
>
> Apologize for not posting this yet on rSigGeo, but I am under
> a bit of time pressure:
>
> When clipping two images with the same extent object:
>
> Browse[2]> eTestAreaExtent
> class ? ? ? : Extent
> xmin ? ? ? ?: -135.2
> xmax ? ? ? ?: -100.2
> ymin ? ? ? ?: 59.998
> ymax ? ? ? ?: 60.002
>
> The resulting images have different extents:
>
> rEdgeRegionCDEM <- crop(inputCDEMRaster, eTestAreaExtent)
> rEdgeRegionAsterCgiar <- crop(inputAsterCgiarRaster, eTestAreaExtent)
>
> Browse[2]> extent(rEdgeRegionCDEM)
> class ? ? ? : Extent
> xmin ? ? ? ?: -135.1996
> xmax ? ? ? ?: -100.2002
> ymin ? ? ? ?: 59.99808
> ymax ? ? ? ?: 60.00224
> Browse[2]> extent(rEdgeRegionAsterCgiar)
> class ? ? ? : Extent
> xmin ? ? ? ?: -135.1998
> xmax ? ? ? ?: -100.2003
> ymin ? ? ? ?: 59.99772
> ymax ? ? ? ?: 60.00189
>
> ..As a result, the command:
>
> rDelta <- rEdgeRegionAsterCgiar - rEdgeRegionCDEM
>
> results in:
>
> Error in compare(e1, e2, extent = FALSE, rowcol = FALSE, prj = TRUE, res =
> TRUE, ?:
> ?different origin
>
> if I add the 'hack':
>
> extent(rEdgeRegionCDEM) <- extent(rEdgeRegionAsterCgiar)
>
> Then the subtraction operation works. But I doubt the true
> absolute geographic alignment of the two images.
>
> Any insights?
>
> thanks,
> Rick Reeves
> NCEAS
>
>
>
> Using raster package on:
>
> platform ? ? ? x86_64-pc-linux-gnu
> arch ? ? ? ? ? x86_64
> os ? ? ? ? ? ? linux-gnu
> system ? ? ? ? x86_64, linux-gnu
> status
> major ? ? ? ? ?2
> minor ? ? ? ? ?13.0
> year ? ? ? ? ? 2011
> month ? ? ? ? ?04
> day ? ? ? ? ? ?13
> svn rev ? ? ? ?55427
> language ? ? ? R
> version.string R version 2.13.0 (2011-04-13)
>
>
>


From pierre.roudier at gmail.com  Wed Apr 27 06:17:13 2011
From: pierre.roudier at gmail.com (Pierre Roudier)
Date: Wed, 27 Apr 2011 16:17:13 +1200
Subject: [R-sig-Geo] Plotting Data the globe
In-Reply-To: <BANLkTik-U_EOn3R0UbQ6xpTmwSCg_QrMfQ@mail.gmail.com>
References: <BANLkTik-U_EOn3R0UbQ6xpTmwSCg_QrMfQ@mail.gmail.com>
Message-ID: <BANLkTimqXkpGNnPBjNBoU8QwJGzrteKcXQ@mail.gmail.com>

Hi,

It is not clear to me why you do not make use of the sp package. It is
not clear what projection your data is in, I assume it is geographic
coordinates/WGS84. Let's say you have your data in a data.frame called
df, with columns lat, lon and z:

set.seed(0)
npts <- 100
lon <- runif(n=npts, min=-180, max=180)
lat <- runif(n=npts, min=-90, max=90)
z <- rnorm(n=npts, mean=10, sd=5)
df <- data.frame(lon, lat, z)

library(sp)
coordinates(df) <- ~lon+lat

# you can plot it already:
spplot(df, zcol='z')

# let's add some information about the data projection:
proj4string(df) <- CRS("+init=epsg:4326")

# if you want to use the Mollveide projection, we have to change projection:
library(rgdal)
df.proj <- spTransform(df, CRS("+proj=moll +lon_0=0 +x_0=0 +y_0=0
+ellps=WGS84 +datum=WGS84 +units=m +no_defs"))

# Plot it again
spplot(df.proj, zcol='z')

# change the color scheme
spplot(df.proj, zcol='z', col.regions=rainbow(64))

Hope this help,

Pierre

2011/4/27 sourav sengupta <aritra.sourav at gmail.com>:
> Hello everyone,
>
> I have a big data set with latitude, laongitude and the values (Z). I want
> to plot the data on a map using a color scheme, so that the color at a
> particular lat-long value depends on the Z-value.
>
> First I used a projection to get the x-y coordinates. Then defined a color
> scheme, and made a scatter plot.
>
> rescale(as.numeric((data$z))) -> z
> coords <- mapproject(data$lon, data$lat, projection="mollweide",
> parameters=NULL, orientation=NULL)
> plot(coords$x, coords$y,col=gray(seq(0,10,,11)/10)[abs(10*(1-z))])
> map("world", projection="",add=TRUE)
>
> This is creating the plot for me, but the quality is very bad. I am not able
> to make it look really smooth. I even used pch="." but that made it very
> grainy. Can anyone help me in this regard? Thanks.
>
> -Aritra
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Scientist
Landcare Research, New Zealand


From r.hijmans at gmail.com  Wed Apr 27 06:30:49 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Tue, 26 Apr 2011 21:30:49 -0700
Subject: [R-sig-Geo] subs (raster) on large dataset
In-Reply-To: <BANLkTi=kO4NnxJMXWfCRo1es76BHRztLQw@mail.gmail.com>
References: <BANLkTims5qZE1XBcyrU9Y=tYmzaBC0+UPQ@mail.gmail.com>
	<1302803032937-6273741.post@n2.nabble.com>
	<BANLkTi=kO4NnxJMXWfCRo1es76BHRztLQw@mail.gmail.com>
Message-ID: <BANLkTikOEoXXALW0x1s5G+M63Dsrzjq6XA@mail.gmail.com>

On Tue, Apr 19, 2011 at 12:19 PM, Lyndon Estes <lestes at princeton.edu> wrote:
> Dear Robert,
>
> Thanks for your response on this (sorry for my slow feedback, I had
> another project intervene).

Hi Lyndon,

likewise :); sorry about that.

>
> I have gone through the subs code now:
>
> Using my raster ltq.g:
>
> class ? ? ? : RasterLayer
> dimensions ?: 15170, 17364, 263411880 ?(nrow, ncol, ncell)
> resolution ?: 92.73961, 92.73961 ?(x, y)
> extent ? ? ?: -733488.1, 876842.5, -3806988, -2400128 ?(xmin, xmax, ymin, ymax)
> projection ?: +proj=aea +lat_1=-18 +lat_2=-32 +lat_0=0 +lon_0=24
> +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs
> +towgs84=0,0,0
> values ? ? ?: lt.quin.3f.grd
> min value ? : 1
> max value ? : 136050
>
> r <- raster(ltq.g)
> tr <- blockSize(r) ?# n = 267, nrows = 57, size = 57
>
> From this test, I get quite variable speeds:
>
> i <- 1
> dong <- Sys.time()
> v <- getValues(ltq.g, row = tr$row[i], nrows = tr$size)
> x <- localmerge(v, y, TRUE)
> length(x) ?# 989748 = ncells in 57 (nrow) *17364(ncol)
> ding <- Sys.time()
> ding-dong
> # Times:
> # 2.374584 mins, 49.45746 seconds, 44.11 seconds
> # After computer restart: 10.97272 after reboot, 14.53 seconds
>
> The last two times are much better, so perhaps I was having some
> performance bottlenecks on my computer. ?Do these seem reasonable to
> you, given my computer (2.4 GHz Core 2, 4 GB RAM Macbook Pro, 64-bit
> R-2.12.2)?
>
> I did catch something potentially important here:
>
> if (canProcessInMemory(x, 3)) {
> ? ? ? ? ? ?v <- localmerge(getValues(x), y, subsWithNA)
> ? ? ? ? ? ?r <- setValues(r, v)
> ? ? ? ? ? ?if (filename != "") {
> ? ? ? ? ? ? ? ?r <- writeRaster(r, filename = filename, ...)
> ? ? ? ? ? ?}
> ? ? ? ? ? ?return(r)
>
> Testing this on its own:
>
> canProcessInMemory(ltq.g, 3)
>
> Takes anywhere from 7.6 minutes (after computer restart) to 13.2
> minutes (last night before restart), and returns "TRUE".

That is *horrible*. I am not sure what is going on here. This is my
rather empirical test to see if you have enough RAM for a given
computation (and to work with chunks if not). Although it is somewhat
inefficient, I have never seen anything crazy like this. I wonder if
you have insufficient RAM and that virtual RAM is created on-disk as
the object grows. I may lower the default maxmemory setting because of
this. You can also set it with setOptions. You can also do
setOptions(todisk=TRUE)  before using this function (and set it back
afterwards, to force the function to use chunks (canProcessInMemory
will return FALSE before doing the memory consuming test).


> Breaking out
> the relevant bits of code for canProcessInMemory:
>
> n <- 3 + (nlayers(ltq.g) - 1)
> cells <- round(1.1 * ncell(ltq.g))
> #if ((cells * n) > .maxmemory()) {
> # ? ? ? ?return(FALSE)
> # ? ?}
> # Note: I couldn't figure out how to access .maxmemory()--it keeps telling me
> # "Error: could not find function ".maxmemory"", so presumably this is
> something
> # internal to raster that I am not smart enough to figure out how to
> access, so I used this:

all raster functions that start with an "." are hidden but are
accessible via raster:::
e.g.
raster:::.maxmemory


>
> ((cells * n) > as.integer(1e+09)) ?# 1e+09 is my memory limit.
> ?> TRUE
>
> So it seems my raster might be getting processed entirely in memory.
> The slowness of canProcessInMemory then
> comes from this line:
>
> r <- try(matrix(0.1, ncol = n, nrow = cells), silent = TRUE)
>
> There is this line at the beginning of canProcessInMemory that seems
> to me important, but I get the same problem as with .maxmemory(), in
> that I can't figure out how to see its code or what it does:
>
> ?if (.toDisk()) {
> ? ? ? ?return(FALSE)
> ? ?}
>
> Based on its name, I assume it checks whether the parent function is
> being directed to process onto the disk, i.e. a filename is supplied.
> Is this correct?

No, it checks of the option "todisk" is set to TRUE or FALSE. Does not
depend on having supplied a filename or not (a temp filename will be
assigned if not).

> And, if the answer is yes, is this further
> assumption also correct?
>
> A raster function (e.g. subs) run with an output filename and without
> assignment to an object must necessarily write to disk. ?e.g.
>
> subs(x, y, by = "foo, which = "bar", subsWithNA = TRUE, filename =
> out, datatype = type, overwrite = TRUE)
>

It does not matter whether you assign or not (and you should normally assign).

> If this last assumption is correct, then it seems that something is
> going wrong (or perhaps something I have mis-specified in my code)
> such that this large file is being handled in memory when it should be
> processed in chunks to disk.

In some functions that is true, but not in this one. Even if you
supply a filename, the whole thing will be done in memory if possible,
and the only at the end will the resulting raster data be written to
disk. Perhaps it would be safer if it would always work as you
expected.


>
> I hope this makes some sense, but I'll appreciate any points of advice on this.
>
> Thanks, Lyndon
>
> p.s. the progress option doesn't work for me on subs, either "text" or
> "window". Is this because I am mac, or is it indicative of something
> wrong regarding the installation?
>

In the majority of functions, the progress bar only works when
processing is done on chunks (otherwise there really is only 1 step).


>
>
>
>
>
>
>
>
>
> format ? ?: raster
> datatype ?: FLT8S
> overwrite : FALSE
> progress ?: none
> timer ? ? : FALSE
> chunksize : 1e+06
> maxmemory : 1e+09
> tmpdir ? ?: /var/folders/Xc/XcJpMYIWFJm5ANTLUN-U2U+++TM/-Tmp-/R_raster_tmp/
> setfileext: TRUE
>
>
>
> On Thu, Apr 14, 2011 at 1:43 PM, Robert Hijmans <r.hijmans at gmail.com> wrote:
>> Lyndon,
>>
>> I have no idea what might be causing this. I would suggest to track down
>> where exactly this happens, by going, line by line, through the code of
>> 'subs'
>>
>> getMethod("subs", c("RasterLayer", "data.frame"))
>>
>> Presumably, something is not moving here:
>>
>> for (i in 1:tr$n) {
>> ? ? v <- getValues(x, row = tr$row[i], nrows = tr$size)
>> ? ? r <- writeValues(r, localmerge(v, y, subsWithNA), ?tr$row[i])
>> ? ? pbStep(pb)
>> }
>>
>> so you can try
>>
>> ?i <- 1
>> ?v <- getValues(x, row = tr$row[i], nrows = tr$size)
>> ?x <- localmerge(v, y, subsWithNA)
>>
>> and if that last step is indeed very slow, have a step by step look at
>> "localmerge".
>>
>> You can then also see of it matters if you make changes to the "chunksize"
>> and "maxmemory" options ?setOptions. These will affect the size of "v" in
>> the code above, and perhaps there is a non-linear effect on the performance
>> of merge.
>>
>> Robert
>>
>>
>> --
>> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/subs-raster-on-large-dataset-tp6272926p6273741.html
>> Sent from the R-sig-geo mailing list archive at Nabble.com.
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
>
> --
> Lyndon Estes
> Research Associate
> Woodrow Wilson School
> Princeton University
> +1-609-258-2392?(o)
> +1-609-258-6082?(f)
> +1-202-431-0496?(m)
> lestes at princeton.edu
>


From r.hijmans at gmail.com  Wed Apr 27 06:34:26 2011
From: r.hijmans at gmail.com (Robert Hijmans)
Date: Tue, 26 Apr 2011 21:34:26 -0700 (PDT)
Subject: [R-sig-Geo] hide legend in plot raster
In-Reply-To: <BANLkTimWZTwuwNMieBpVu-=4VUkhWkSAtA@mail.gmail.com>
References: <70FC67C4A585D1489E66225A4E0238BAECA996@RZS-EXC-CL06.rz-sued.bayern.de>
	<BANLkTimWZTwuwNMieBpVu-=4VUkhWkSAtA@mail.gmail.com>
Message-ID: <1303878866573-6308140.post@n2.nabble.com>

> I would like to overlay two raster objects to produce a nice map. The 
> underlying raster is a hillshade, the second is transparent. 
> plot(hillshade, col=grey(1:20/20)) 
> plot(soilraster, alpha = 0.7, add = TRUE) 
> Does anybody know, how to hide the legend of the hillshade? 

In addition to using Adam's suggestions, you can also have a look at the
example in the raster::hillShade function:

dem <- getData('alt', country='CHE')
slas <- slopeAspect(dem)
hill <- hillShade(slas[[1]], slas[[2]], 40, 270)
plot(hill, col=grey(0:100/100), legend=FALSE, main='Switzerland')
plot(dem, col=rainbow(25, alpha=0.35), add=TRUE)


--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/hide-legend-in-plot-raster-tp6306428p6308140.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From Tim.Haering at lwf.bayern.de  Wed Apr 27 08:49:02 2011
From: Tim.Haering at lwf.bayern.de (=?iso-8859-1?Q?H=E4ring=2C_Tim_=28LWF=29?=)
Date: Wed, 27 Apr 2011 08:49:02 +0200
Subject: [R-sig-Geo] hide legend in plot raster
In-Reply-To: <5209_1303878976_p3R4aEiZ009640_1303878866573-6308140.post@n2.nabble.com>
References: <70FC67C4A585D1489E66225A4E0238BAECA996@RZS-EXC-CL06.rz-sued.bayern.de><BANLkTimWZTwuwNMieBpVu-=4VUkhWkSAtA@mail.gmail.com>
	<5209_1303878976_p3R4aEiZ009640_1303878866573-6308140.post@n2.nabble.com>
Message-ID: <70FC67C4A585D1489E66225A4E0238BAECA9E7@RZS-EXC-CL06.rz-sued.bayern.de>

Thank you both. It works fine.
TIM

> -----Original Message-----
> From: r-sig-geo-bounces at r-project.org [mailto:r-sig-geo-bounces at r-
> project.org] On Behalf Of Robert Hijmans
> Sent: Wednesday, April 27, 2011 6:34 AM
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] hide legend in plot raster
> 
> > I would like to overlay two raster objects to produce a nice map. The
> > underlying raster is a hillshade, the second is transparent.
> > plot(hillshade, col=grey(1:20/20))
> > plot(soilraster, alpha = 0.7, add = TRUE)
> > Does anybody know, how to hide the legend of the hillshade?
> 
> In addition to using Adam's suggestions, you can also have a look at
> the
> example in the raster::hillShade function:
> 
> dem <- getData('alt', country='CHE')
> slas <- slopeAspect(dem)
> hill <- hillShade(slas[[1]], slas[[2]], 40, 270)
> plot(hill, col=grey(0:100/100), legend=FALSE, main='Switzerland')
> plot(dem, col=rainbow(25, alpha=0.35), add=TRUE)
> 
> 
> --
> View this message in context: http://r-sig-
> geo.2731867.n2.nabble.com/hide-legend-in-plot-raster-
> tp6306428p6308140.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From Tobias.Reiners at bio.uni-giessen.de  Wed Apr 27 09:01:09 2011
From: Tobias.Reiners at bio.uni-giessen.de (Tobias Erik Reiners)
Date: Wed, 27 Apr 2011 09:01:09 +0200
Subject: [R-sig-Geo] To validate logistic regression
In-Reply-To: <201104260658.56091.dylan.beaudette@gmail.com>
References: <BANLkTi=2EovS_uRTrcrTWFQFeN9HmrKCxw@mail.gmail.com>
	<201104260658.56091.dylan.beaudette@gmail.com>
Message-ID: <20110427090109.10565bq1u3tzldkw@webmail.stud.uni-giessen.de>

Dear Komine,

I have another more sophisticated approach for you.
If you really want to validate your logistic model
with x-fold internal croxxvalidation you should not
only perform your data partitioning once. I recomment
to do it 100 to 999 times to really get an estimate of
your data and model quality stability.

####5 fold Crossvalidation with 100 Permutations
k		<- 20 			##20% of the dataset as testdata
N		<- 100			##100 Permutations
permu		<- paste("Permut_",1:N,sep="")
AUC_Results 	<- matrix(NA, 1, N, dimnames=list("AUC",permu))
n 		<- ncol(Dataset)
numrows 	<- nrow(Dataset)
learnDataSize	<- round(numrows*(1-0.01*k))
testDataSize	<- numrows-learnDataSize
##loop
for (j in 1:N){
		cat("calculating",((j/N)*100),"% \n")
		learnIndex	<-sample(nrow(Dataset))[1:learnDataSize]
		learnData	<-Dataset[learnIndex,]
		testData	<-Dataset[-learnIndex,]
		mg		<-glm(formula =yourFormula,
				family = binomial(link = "logit"),data=learnData)
		bestmod_cv	<-step(mg,direction="backward",trace=0)
		predicted_cv	<-predict(bestmod_cv, newdata=testData, type="response")
		observed_cv		<-testData[,"Y"]
		AUC_result		<-roc.auc(observed_k, predicted_k)
		AUC_Results[1,j]	<-AUC_result$A
				}

Cheers,
Tobias Erik Reiners
Mammalian Ecology Group





Zitat von Dylan Beaudette <debeaudette at ucdavis.edu>:

> Another approach:
>
> See ?lrm, ?validate, and ?calibrate from the rms package.
>
> Dylan
>
> On Tuesday, April 26, 2011, Bram Van Moorter wrote:
>> Dear Komine,
>> Not sure whether this is the easiest way, but it has worked for me:
>>
>> set.seed(0)
>> head(tab <- data.frame(Y=as.numeric(runif(100)>0.5), X=rnorm(100)))
>> subs <- sample(c(1:nrow(tab)), round(nrow(tab)*0.66), replace=F)  #the
>> 66% of data you want in one sample
>> tab1 <- tab[subs, ] #the one sample
>> tab2 <- tab[!c(1:nrow(tab)) %in% subs, ] #the other sample, which are
>> the data that do not fall in the first sample
>>
>> rlog1 <- glm(Y~X,family=binomial,data=tab1)
>> summary(rlog1)
>> tab2$pred <-predict(rlog1, newdata=tab2, type="response")
>> hist(tab2$pred)
>>
>> library(ROCR) #allows you to make easily ROC's which allows the
>> assessment of your prediction
>> pred <- prediction(tab2$pred, tab2$Y)
>> perf <- performance(pred,"tpr","fpr")
>> plot(perf); abline(0, 1, col="red")  #the proportional line shows that
>> the prediction is as good as random, which you would expect in this
>> example
>>
>> Best,
>> Bram
>>
>>
>> > Hi,
>> > I would like your help to validate my logistic regression. I know how to
> do
>> > logistic regression.
>> >
>> > rlog<-glm(Y~X,family=binomial,data=tab)
>> > summary(rlog)
>> > HLgof.test(fit = fitted(rlog), obs=Y)
>> >
>> > However, I would like to validate my model. For example to divise my data
> in
>> > a sample for training (66%) and a sample for validation (34%).
>> > e.g for ma table
>> > Area   Y     X
>> > 1       1     135
>> > 1       0     200
>> > 1       1      97
>> > 1       1     160
>> > 1       0     201
>> > 1       1     144
>> > 1       0     100
>> >
>> > But I don't know how to validate it.
>> > 1- My first problem: How to create my 2 samples from my variables Y and X
>> > using pourcentage 66 ang 34 %?
>> >
>> > - How to have the pourcentage of good prediction and bad prediction?
>> >
>> > Thanks for your Help
>> > Komine
>> >
>>
>>
>> --
>> Bram Van Moorter
>> Centre for Conservation Biology (NTNU),
>> Norwegian Institute for Nature Research (NINA)
>> Trondheim (Norway)
>> email:  Bram.Van.Moorter at gmail.com
>> website: http://ase-research.org/moorter
>> phone: +47 73596060
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
> --
> Dylan E. Beaudette
> USDA-NRCS Soil Scientist
> California Soil Resource Lab
> http://casoilresource.lawr.ucdavis.edu/
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From peterfrancis at me.com  Wed Apr 27 11:56:22 2011
From: peterfrancis at me.com (Peter Francis)
Date: Wed, 27 Apr 2011 10:56:22 +0100
Subject: [R-sig-Geo] Beginner help with SAR model
Message-ID: <4A050896-4946-4321-8F17-E287BBC4F0BF@me.com>

Dear List,

I feel i must say i have very little experience with GIS systems and modelling geographical data so i apologise in advance. I have looked through the literature and mailing list archives with little success.

I want to model the correlates of three diversity indices - functional diversity (FD), phylogenetic diversity (PD) and species richness (SR).

I have built communities - or neighbourhoods - on a presence absence basis i.e does species occur in region 1, does species 2 occur in region 2 etc. The spatial data comes from WWF ecoregions (http://www.worldwildlife.org/science/data/item6373.html)  and the environmental predictors from the same place. Of the 600 odd ecoregions i  have 458 which are occupied. 

To date i have been using a GLS with the corExp function to model the spatial autocorrelation present in the dataset i.e the latitude and longitude of each ecoregion was included as a smooth factor in all-statistical models. However i have a feeling this may not be sufficient ( Kissling & Carl, 2008) so i wanted to use the package sdep to form a SAR.

Now to the problem - the data i have are .dbf file, .shp file and .shx file  - i can get more if needed. 

So to do a SAR i needed to construct a neighbourhood? and i called the following

ccShape=readShapePoly("/WWFBinaryitem6603/wwf_terr_ecos.shp")

ccNb=poly2nb(ccShape)

ccMat=nb2mat(ccNb,style='B')

and got Error in nb2listw(neighbours, glist = glist, style = style, zero.policy = zero.policy) : 
  Empty neighbour sets found

ccNb
Neighbour list object:
Number of regions: 14458 
Number of nonzero links: 16076 
Percentage nonzero weights: 0.007690624 
Average number of links: 1.111910 
8601 regions with no links:

Any help or advice would be greatly appreciated.

Peter


From alobolistas at gmail.com  Wed Apr 27 12:14:59 2011
From: alobolistas at gmail.com (Agustin Lobo)
Date: Wed, 27 Apr 2011 12:14:59 +0200
Subject: [R-sig-Geo] Problem with raster::extract
Message-ID: <BANLkTi=y5-0UZ7b9cH5X5HpcNS6uF3+8uQ@mail.gmail.com>

Hi!

I'm doing

SGRGBF40 = brick("/media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL/SGRGB/SGRGBWBPS125F40.tif")
calibf2 = readOGR(dsn="/media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL",layer="calibf2",stringsAsFactors=F)
projection(SGRGBF40) = projection(calibf2)
#I make sure vector calibf2 and raster SGRGBF40 do overlap:
plot(subset(SGRGBF40,1))
plot(calibf2,add=T)

#Calculate mean values for each polygon:
v <- extract(SGRGBF40, calibf2,fun=mean,na.rm=T,nl=3)
summary(v)
SGRGBF40data = cbind(calibf2 at data,v)

But then, despite na.rm=T:
> SGRGBF40data[7,]
  ID code codetxt SGRGBWBPS125F40_1 SGRGBWBPS125F40_2 SGRGBWBPS125F40_3
6  7    7      C7               NaN          39781.02                 3

> summary(v)
 SGRGBWBPS125F40_1 SGRGBWBPS125F40_2 SGRGBWBPS125F40_3
 Min.   :18444     Min.   :  119.4   Min.   :    3
 1st Qu.:34098     1st Qu.:25875.7   1st Qu.:15633
 Median :37776     Median :36668.2   Median :22374
 Mean   :41314     Mean   :36882.2   Mean   :29737
 3rd Qu.:52236     3rd Qu.:49452.6   3rd Qu.:48116
 Max.   :65396     Max.   :65417.5   Max.   :63826
 NA's   :    3

While there are no NA in the raster brick:
> summary(SGRGBF40)
Cells:  4646400
NAs  :  0 0 0

            1     2     3
Min.     1777     3     3
1st Qu. 34910 22460 18640
Median  42660 27540 24420
Mean    41070 28570 25170
3rd Qu. 48470 33320 29060
Max.    65520 65530 64220


A possibility would be a division by 0 (weird, because all polygons
are large), so I've tried:
> vl <- extract(SGRGBF40, calibf2,fun=length,nl=3)
but...
Error in t(sapply(res[!i], function(x) apply(x, 2, FUN = fun, na.rm =
na.rm))) :
  error in evaluating the argument 'x' in selecting a method for
function 't': Error in FUN(newX[, i], ...) :
  2 arguments passed to 'length' which requires 1
Calls: sapply -> lapply -> FUN -> apply
Calls: extract -> extract -> .polygonValues -> t

Any hint?

Data:
http://dl.dropbox.com/u/3180464/SGRGBWBPS125F40.tif
http://dl.dropbox.com/u/3180464/SGRGBWBPS125F40.tfw
http://dl.dropbox.com/u/3180464/calibf2.zip

> sessionInfo()
R version 2.13.0 (2011-04-13)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
 [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
[11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8

attached base packages:
[1] grid      stats     graphics  grDevices utils     datasets  methods
[8] base

other attached packages:
[1] gridExtra_0.7 ggplot2_0.8.9 rgdal_0.6-33  raster_1.7-29 sp_0.9-76
[6] reshape_0.8.4 plyr_1.4      proto_0.3-8   rkward_0.5.6

loaded via a namespace (and not attached):
[1] lattice_0.19-26 tools_2.13.0
>

Agus


From gianni.lavaredo at gmail.com  Wed Apr 27 17:16:24 2011
From: gianni.lavaredo at gmail.com (gianni lavaredo)
Date: Wed, 27 Apr 2011 17:16:24 +0200
Subject: [R-sig-Geo] help to select polygos inside ad area
Message-ID: <BANLkTimm6=6ULq15ooRbynZCzi0e70inDA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110427/7aa30c99/attachment.pl>

From r.hijmans at gmail.com  Wed Apr 27 17:52:19 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Wed, 27 Apr 2011 08:52:19 -0700
Subject: [R-sig-Geo] Problem with raster::extract
In-Reply-To: <BANLkTi=y5-0UZ7b9cH5X5HpcNS6uF3+8uQ@mail.gmail.com>
References: <BANLkTi=y5-0UZ7b9cH5X5HpcNS6uF3+8uQ@mail.gmail.com>
Message-ID: <BANLkTinkhdSDhzpT3nqBgysY5RSXrwrDEQ@mail.gmail.com>

Hello Agus,

The NaN suggests that there are only NA values for that polygon/layer.
To investigate this I would do

v <- extract(SGRGBF40, calibf2, nl=3)

# i.e. do not use a function and then inspect

v[[7]]


All functions must accept an na.rm argument (even if they choose to
ignore it. So for length you can do something like:

vl <- extract(SGRGBF40, calibf2, fun=function(x,...)length(x), nl=3)


Hope this helps, Robert

> v
[1] 387.8158 329.3913
> v <- extract(r, polys, length)
Error in FUN(X[[1L]], ...) :
  2 arguments passed to 'length' which requires 1
> v <- extract(r, polys, function(x,...)length(x))
> v
[1] 38 23
>


On Wed, Apr 27, 2011 at 3:14 AM, Agustin Lobo <alobolistas at gmail.com> wrote:
> Hi!
>
> I'm doing
>
> SGRGBF40 = brick("/media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL/SGRGB/SGRGBWBPS125F40.tif")
> calibf2 = readOGR(dsn="/media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL",layer="calibf2",stringsAsFactors=F)
> projection(SGRGBF40) = projection(calibf2)
> #I make sure vector calibf2 and raster SGRGBF40 do overlap:
> plot(subset(SGRGBF40,1))
> plot(calibf2,add=T)
>
> #Calculate mean values for each polygon:
> v <- extract(SGRGBF40, calibf2,fun=mean,na.rm=T,nl=3)
> summary(v)
> SGRGBF40data = cbind(calibf2 at data,v)
>
> But then, despite na.rm=T:
>> SGRGBF40data[7,]
> ?ID code codetxt SGRGBWBPS125F40_1 SGRGBWBPS125F40_2 SGRGBWBPS125F40_3
> 6 ?7 ? ?7 ? ? ?C7 ? ? ? ? ? ? ? NaN ? ? ? ? ?39781.02 ? ? ? ? ? ? ? ? 3
>
>> summary(v)
> ?SGRGBWBPS125F40_1 SGRGBWBPS125F40_2 SGRGBWBPS125F40_3
> ?Min. ? :18444 ? ? Min. ? : ?119.4 ? Min. ? : ? ?3
> ?1st Qu.:34098 ? ? 1st Qu.:25875.7 ? 1st Qu.:15633
> ?Median :37776 ? ? Median :36668.2 ? Median :22374
> ?Mean ? :41314 ? ? Mean ? :36882.2 ? Mean ? :29737
> ?3rd Qu.:52236 ? ? 3rd Qu.:49452.6 ? 3rd Qu.:48116
> ?Max. ? :65396 ? ? Max. ? :65417.5 ? Max. ? :63826
> ?NA's ? : ? ?3
>
> While there are no NA in the raster brick:
>> summary(SGRGBF40)
> Cells: ?4646400
> NAs ?: ?0 0 0
>
> ? ? ? ? ? ?1 ? ? 2 ? ? 3
> Min. ? ? 1777 ? ? 3 ? ? 3
> 1st Qu. 34910 22460 18640
> Median ?42660 27540 24420
> Mean ? ?41070 28570 25170
> 3rd Qu. 48470 33320 29060
> Max. ? ?65520 65530 64220
>
>
> A possibility would be a division by 0 (weird, because all polygons
> are large), so I've tried:
>> vl <- extract(SGRGBF40, calibf2,fun=length,nl=3)
> but...
> Error in t(sapply(res[!i], function(x) apply(x, 2, FUN = fun, na.rm =
> na.rm))) :
> ?error in evaluating the argument 'x' in selecting a method for
> function 't': Error in FUN(newX[, i], ...) :
> ?2 arguments passed to 'length' which requires 1
> Calls: sapply -> lapply -> FUN -> apply
> Calls: extract -> extract -> .polygonValues -> t
>
> Any hint?
>
> Data:
> http://dl.dropbox.com/u/3180464/SGRGBWBPS125F40.tif
> http://dl.dropbox.com/u/3180464/SGRGBWBPS125F40.tfw
> http://dl.dropbox.com/u/3180464/calibf2.zip
>
>> sessionInfo()
> R version 2.13.0 (2011-04-13)
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> locale:
> ?[1] LC_CTYPE=en_US.UTF-8 ? ? ? ? ?LC_NUMERIC=C
> ?[3] LC_TIME=en_US.UTF-8 ? ? ? ? ? LC_COLLATE=en_US.UTF-8
> ?[5] LC_MONETARY=en_US.UTF-8 ? ? ? LC_MESSAGES=en_US.UTF-8
> ?[7] LC_PAPER=en_US.UTF-8 ? ? ? ? ?LC_NAME=en_US.UTF-8
> ?[9] LC_ADDRESS=en_US.UTF-8 ? ? ? ?LC_TELEPHONE=en_US.UTF-8
> [11] LC_MEASUREMENT=en_US.UTF-8 ? ?LC_IDENTIFICATION=en_US.UTF-8
>
> attached base packages:
> [1] grid ? ? ?stats ? ? graphics ?grDevices utils ? ? datasets ?methods
> [8] base
>
> other attached packages:
> [1] gridExtra_0.7 ggplot2_0.8.9 rgdal_0.6-33 ?raster_1.7-29 sp_0.9-76
> [6] reshape_0.8.4 plyr_1.4 ? ? ?proto_0.3-8 ? rkward_0.5.6
>
> loaded via a namespace (and not attached):
> [1] lattice_0.19-26 tools_2.13.0
>>
>
> Agus
>


From gianni.lavaredo at gmail.com  Wed Apr 27 18:27:24 2011
From: gianni.lavaredo at gmail.com (gianni lavaredo)
Date: Wed, 27 Apr 2011 18:27:24 +0200
Subject: [R-sig-Geo] convert "SpatialPolygonsDataFrame" in PBSmapping object
 ("SpatialPolygons2PolySet") and import data.frame ID of
 "SpatialPolygonsDataFrame"
Message-ID: <BANLkTik4LnQ8CFCdVTtA7gA6mSTDz_GUWA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110427/153cb858/attachment.pl>

From gianni.lavaredo at gmail.com  Wed Apr 27 20:50:35 2011
From: gianni.lavaredo at gmail.com (gianni lavaredo)
Date: Wed, 27 Apr 2011 20:50:35 +0200
Subject: [R-sig-Geo] help to convert a data.frame in SPATIALPOLYGONDATAFRAME
Message-ID: <BANLkTinfwjbPitsoQ75pGUOvqntz8Ca-Ww@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110427/f8c9d1fa/attachment.pl>

From roman.lustrik at gmail.com  Wed Apr 27 21:16:09 2011
From: roman.lustrik at gmail.com (=?UTF-8?Q?Roman_Lu=C5=A1trik?=)
Date: Wed, 27 Apr 2011 21:16:09 +0200
Subject: [R-sig-Geo] help to convert a data.frame in
	SPATIALPOLYGONDATAFRAME
In-Reply-To: <BANLkTinfwjbPitsoQ75pGUOvqntz8Ca-Ww@mail.gmail.com>
References: <BANLkTinfwjbPitsoQ75pGUOvqntz8Ca-Ww@mail.gmail.com>
Message-ID: <BANLkTin=hhfwLtp_r6_pxu7eaVs6ezA9qQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110427/7f463feb/attachment.pl>

From chrismcowen at gmail.com  Wed Apr 27 21:34:45 2011
From: chrismcowen at gmail.com (Chris Mcowen)
Date: Wed, 27 Apr 2011 20:34:45 +0100
Subject: [R-sig-Geo] dnearneigh,
	knearneigh and SAR - not removing spatial autocorrelation
Message-ID: <F10E6F24-AAAE-4ED7-B0E0-48A41CC80D39@gmail.com>

Dear list,

I was wondering if somebody could possibly offer me some advice?

I have a dataset where there is spatial autocorrelation present - visible from the correlogram. So i tried to remove it using a SAR. My data is for 458 regions of differing size for which i have long - lat co-ordinates. 


coords<-cbind(data$X,data$Y)
coords<-as.matrix(coords)

The first approach was to use dnearneigh to set up the neighbourhood. I am very new to this and was having problems as regions would often appear with no links ( see below) so i upped dmax until this no longer occurred - this maybe a incorrect method?

> dnearneigh(coords, 0, 1500, row.names = NULL, longlat = TRUE)
Neighbour list object:
Number of regions: 458 
Number of nonzero links: 8990 
Percentage nonzero weights: 4.285769 
Average number of links: 19.62882 
2 regions with no links:
235 236

Results in - Empty neighbour sets found

> dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
Neighbour list object:
Number of regions: 458 
Number of nonzero links: 14200 
Percentage nonzero weights: 6.769512 
Average number of links: 31.00437 

I then converted this to a weight matrix and used in my SAR.

nb1.5 <- dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
nb1.5.w<-nb2listw(nb1.5, glist=NULL, style="W", zero.policy=FALSE)

However, looking at the correlogram and the AIC ( below) it seems to not have made a huge difference

AIC: -2581.8, (AIC for lm: -2574)

So i tried defining my neighbourhood using the knearneigh function but that made very little difference

test <- knearneigh(coords, k=1, longlat = NULL, RANN=TRUE)
knn2nb(test, row.names = NULL, sym = FALSE)
k1 <- knn2nb(knearneigh(coords))
all.linked <- max(unlist(nbdists(k1, coords)))
col.nb.0.all <- dnearneigh(coords, 0, all.linked, row.names=rn)

AIC: -2581.8, (AIC for lm: -2574)

IS there something i am doing wrong or is there a step i am not doing?

Any help would be gratefully received.

Chris


From Roger.Bivand at nhh.no  Wed Apr 27 22:16:13 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 27 Apr 2011 22:16:13 +0200 (CEST)
Subject: [R-sig-Geo] dnearneigh,
 knearneigh and SAR - not removing spatial autocorrelation
In-Reply-To: <F10E6F24-AAAE-4ED7-B0E0-48A41CC80D39@gmail.com>
References: <F10E6F24-AAAE-4ED7-B0E0-48A41CC80D39@gmail.com>
Message-ID: <alpine.LRH.2.00.1104272209330.3935@reclus.nhh.no>

On Wed, 27 Apr 2011, Chris Mcowen wrote:

> Dear list,
>
> I was wondering if somebody could possibly offer me some advice?
>
> I have a dataset where there is spatial autocorrelation present - 
> visible from the correlogram. So i tried to remove it using a SAR. My 
> data is for 458 regions of differing size for which i have long - lat 
> co-ordinates.
>
>
> coords<-cbind(data$X,data$Y)
> coords<-as.matrix(coords)
>
> The first approach was to use dnearneigh to set up the neighbourhood. I 
> am very new to this and was having problems as regions would often 
> appear with no links ( see below) so i upped dmax until this no longer 
> occurred - this maybe a incorrect method?
>
>> dnearneigh(coords, 0, 1500, row.names = NULL, longlat = TRUE)
> Neighbour list object:
> Number of regions: 458
> Number of nonzero links: 8990
> Percentage nonzero weights: 4.285769
> Average number of links: 19.62882
> 2 regions with no links:
> 235 236
>
> Results in - Empty neighbour sets found
>
>> dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
> Neighbour list object:
> Number of regions: 458
> Number of nonzero links: 14200
> Percentage nonzero weights: 6.769512
> Average number of links: 31.00437
>
> I then converted this to a weight matrix and used in my SAR.
>
> nb1.5 <- dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
> nb1.5.w<-nb2listw(nb1.5, glist=NULL, style="W", zero.policy=FALSE)
>
> However, looking at the correlogram and the AIC ( below) it seems to not 
> have made a huge difference
>
> AIC: -2581.8, (AIC for lm: -2574)

With relatively large numbers of neighbours, you smooth more. What 
definition of neighbour are you using in the correlogram? Why not just use 
the same? The correlogram obviously doesn't care about no-neighbour 
observations, so perhaps do the same? If you want to link every 
observation in, why not then down-weight distant neighbours using inverse 
distance weights - see ?nbdists.

The test neighbour definition and the weights used for model fitting do 
not match. Further, you don't say whether your correlogram is for the OLS 
model residuals or just the response variable. If the latter, the 
explanatory variables may co-vary in space with the response, so the 
residuals are in fact not spatially autocorrelated.

Hope this helps,

Roger

>
> So i tried defining my neighbourhood using the knearneigh function but that made very little difference
>
> test <- knearneigh(coords, k=1, longlat = NULL, RANN=TRUE)
> knn2nb(test, row.names = NULL, sym = FALSE)
> k1 <- knn2nb(knearneigh(coords))
> all.linked <- max(unlist(nbdists(k1, coords)))
> col.nb.0.all <- dnearneigh(coords, 0, all.linked, row.names=rn)
>
> AIC: -2581.8, (AIC for lm: -2574)
>
> IS there something i am doing wrong or is there a step i am not doing?
>
> Any help would be gratefully received.
>
> Chris
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From chrismcowen at gmail.com  Wed Apr 27 22:39:10 2011
From: chrismcowen at gmail.com (Chris Mcowen)
Date: Wed, 27 Apr 2011 21:39:10 +0100
Subject: [R-sig-Geo] dnearneigh,
	knearneigh and SAR - not removing spatial autocorrelation
In-Reply-To: <alpine.LRH.2.00.1104272209330.3935@reclus.nhh.no>
References: <F10E6F24-AAAE-4ED7-B0E0-48A41CC80D39@gmail.com>
	<alpine.LRH.2.00.1104272209330.3935@reclus.nhh.no>
Message-ID: <65B692BA-57B5-4383-9101-C26303AA4941@gmail.com>

Hi Roger,

Thanks for this, as previously mentioned i am relatively new to this, i started with a GLS method but then realised it may not be the most effective so i decided to try SAR methods.

> What definition of neighbour are you using in the correlogram?


I call the SAR as follows :- sem.nb1.5.w<-errorsarlm(ols2, listw=nb1.5.w, zero.policy=F, na.action=na.omit)

Then extract the residuals: - res.sem.nb1.5.w <- residuals(sem.nb1.5.w)

Then use these to construct the correlogram: - cor.sem.nb1.5.w<-correlog(data$X, data$Y, z=residuals(sem.nb1.5.w), na.rm=T, increment=1, resamp=1)


> The correlogram obviously doesn't care about no-neighbour observations, so perhaps do the same?

How do i do this? If i set the dmax too low i get "Empty neighbour sets found" so it wont run the model. I feel i am artificially setting the dmax based on the fact it works rather than any scientific rationale. 

I tried a method used in your example:

> coords <- coordinates(columbus)
> rn <- sapply(slot(columbus, "polygons"), function(x) slot(x, "ID"))
> k1 <- knn2nb(knearneigh(coords))
> all.linked <- max(unlist(nbdists(k1, coords)))
> col.nb.0.all <- dnearneigh(coords, 0, all.linked, row.names=rn)

To join everything on a NN basis. But that resulted in little difference in the correlgram and AIC.

So my two questions are:

Do i have to link every neighbour and if not how to i run the model without getting the error and how do i decide which neighbours not to link?

Will it make a large difference if the correlgram doesn't change much? Does this mean the model is not "dealing" with the correlation effectively and how do i improve this?

Thanks again and sorry for my relatively naive questions!

Chris


On 27 Apr 2011, at 21:16, Roger Bivand wrote:

On Wed, 27 Apr 2011, Chris Mcowen wrote:

> Dear list,
> 
> I was wondering if somebody could possibly offer me some advice?
> 
> I have a dataset where there is spatial autocorrelation present - visible from the correlogram. So i tried to remove it using a SAR. My data is for 458 regions of differing size for which i have long - lat co-ordinates.
> 
> 
> coords<-cbind(data$X,data$Y)
> coords<-as.matrix(coords)
> 
> The first approach was to use dnearneigh to set up the neighbourhood. I am very new to this and was having problems as regions would often appear with no links ( see below) so i upped dmax until this no longer occurred - this maybe a incorrect method?
> 
>> dnearneigh(coords, 0, 1500, row.names = NULL, longlat = TRUE)
> Neighbour list object:
> Number of regions: 458
> Number of nonzero links: 8990
> Percentage nonzero weights: 4.285769
> Average number of links: 19.62882
> 2 regions with no links:
> 235 236
> 
> Results in - Empty neighbour sets found
> 
>> dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
> Neighbour list object:
> Number of regions: 458
> Number of nonzero links: 14200
> Percentage nonzero weights: 6.769512
> Average number of links: 31.00437
> 
> I then converted this to a weight matrix and used in my SAR.
> 
> nb1.5 <- dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
> nb1.5.w<-nb2listw(nb1.5, glist=NULL, style="W", zero.policy=FALSE)
> 
> However, looking at the correlogram and the AIC ( below) it seems to not have made a huge difference
> 
> AIC: -2581.8, (AIC for lm: -2574)

With relatively large numbers of neighbours, you smooth more. What definition of neighbour are you using in the correlogram? Why not just use the same? The correlogram obviously doesn't care about no-neighbour observations, so perhaps do the same? If you want to link every observation in, why not then down-weight distant neighbours using inverse distance weights - see ?nbdists.

The test neighbour definition and the weights used for model fitting do not match. Further, you don't say whether your correlogram is for the OLS model residuals or just the response variable. If the latter, the explanatory variables may co-vary in space with the response, so the residuals are in fact not spatially autocorrelated.

Hope this helps,

Roger

> 
> So i tried defining my neighbourhood using the knearneigh function but that made very little difference
> 
> test <- knearneigh(coords, k=1, longlat = NULL, RANN=TRUE)
> knn2nb(test, row.names = NULL, sym = FALSE)
> k1 <- knn2nb(knearneigh(coords))
> all.linked <- max(unlist(nbdists(k1, coords)))
> col.nb.0.all <- dnearneigh(coords, 0, all.linked, row.names=rn)
> 
> AIC: -2581.8, (AIC for lm: -2574)
> 
> IS there something i am doing wrong or is there a step i am not doing?
> 
> Any help would be gratefully received.
> 
> Chris
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From r.hijmans at gmail.com  Thu Apr 28 05:52:22 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Wed, 27 Apr 2011 20:52:22 -0700
Subject: [R-sig-Geo] Problem with raster::extract
In-Reply-To: <BANLkTinkhdSDhzpT3nqBgysY5RSXrwrDEQ@mail.gmail.com>
References: <BANLkTi=y5-0UZ7b9cH5X5HpcNS6uF3+8uQ@mail.gmail.com>
	<BANLkTinkhdSDhzpT3nqBgysY5RSXrwrDEQ@mail.gmail.com>
Message-ID: <BANLkTimj6UMj1JQDdW7+P+msKt3PgE40xg@mail.gmail.com>

This was caused by raster incorrectly interpreting the highest
possible INT2U value (65530) as NA. Now fixed (version 1.8-16), at
least for this case. Thanks for reporting, Robert

On Wed, Apr 27, 2011 at 8:52 AM, Robert J. Hijmans <r.hijmans at gmail.com> wrote:
> Hello Agus,
>
> The NaN suggests that there are only NA values for that polygon/layer.
> To investigate this I would do
>
> v <- extract(SGRGBF40, calibf2, nl=3)
>
> # i.e. do not use a function and then inspect
>
> v[[7]]
>
>
> All functions must accept an na.rm argument (even if they choose to
> ignore it. So for length you can do something like:
>
> vl <- extract(SGRGBF40, calibf2, fun=function(x,...)length(x), nl=3)
>
>
> Hope this helps, Robert
>
>> v
> [1] 387.8158 329.3913
>> v <- extract(r, polys, length)
> Error in FUN(X[[1L]], ...) :
> ?2 arguments passed to 'length' which requires 1
>> v <- extract(r, polys, function(x,...)length(x))
>> v
> [1] 38 23
>>
>
>
> On Wed, Apr 27, 2011 at 3:14 AM, Agustin Lobo <alobolistas at gmail.com> wrote:
>> Hi!
>>
>> I'm doing
>>
>> SGRGBF40 = brick("/media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL/SGRGB/SGRGBWBPS125F40.tif")
>> calibf2 = readOGR(dsn="/media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL",layer="calibf2",stringsAsFactors=F)
>> projection(SGRGBF40) = projection(calibf2)
>> #I make sure vector calibf2 and raster SGRGBF40 do overlap:
>> plot(subset(SGRGBF40,1))
>> plot(calibf2,add=T)
>>
>> #Calculate mean values for each polygon:
>> v <- extract(SGRGBF40, calibf2,fun=mean,na.rm=T,nl=3)
>> summary(v)
>> SGRGBF40data = cbind(calibf2 at data,v)
>>
>> But then, despite na.rm=T:
>>> SGRGBF40data[7,]
>> ?ID code codetxt SGRGBWBPS125F40_1 SGRGBWBPS125F40_2 SGRGBWBPS125F40_3
>> 6 ?7 ? ?7 ? ? ?C7 ? ? ? ? ? ? ? NaN ? ? ? ? ?39781.02 ? ? ? ? ? ? ? ? 3
>>
>>> summary(v)
>> ?SGRGBWBPS125F40_1 SGRGBWBPS125F40_2 SGRGBWBPS125F40_3
>> ?Min. ? :18444 ? ? Min. ? : ?119.4 ? Min. ? : ? ?3
>> ?1st Qu.:34098 ? ? 1st Qu.:25875.7 ? 1st Qu.:15633
>> ?Median :37776 ? ? Median :36668.2 ? Median :22374
>> ?Mean ? :41314 ? ? Mean ? :36882.2 ? Mean ? :29737
>> ?3rd Qu.:52236 ? ? 3rd Qu.:49452.6 ? 3rd Qu.:48116
>> ?Max. ? :65396 ? ? Max. ? :65417.5 ? Max. ? :63826
>> ?NA's ? : ? ?3
>>
>> While there are no NA in the raster brick:
>>> summary(SGRGBF40)
>> Cells: ?4646400
>> NAs ?: ?0 0 0
>>
>> ? ? ? ? ? ?1 ? ? 2 ? ? 3
>> Min. ? ? 1777 ? ? 3 ? ? 3
>> 1st Qu. 34910 22460 18640
>> Median ?42660 27540 24420
>> Mean ? ?41070 28570 25170
>> 3rd Qu. 48470 33320 29060
>> Max. ? ?65520 65530 64220
>>
>>
>> A possibility would be a division by 0 (weird, because all polygons
>> are large), so I've tried:
>>> vl <- extract(SGRGBF40, calibf2,fun=length,nl=3)
>> but...
>> Error in t(sapply(res[!i], function(x) apply(x, 2, FUN = fun, na.rm =
>> na.rm))) :
>> ?error in evaluating the argument 'x' in selecting a method for
>> function 't': Error in FUN(newX[, i], ...) :
>> ?2 arguments passed to 'length' which requires 1
>> Calls: sapply -> lapply -> FUN -> apply
>> Calls: extract -> extract -> .polygonValues -> t
>>
>> Any hint?
>>
>> Data:
>> http://dl.dropbox.com/u/3180464/SGRGBWBPS125F40.tif
>> http://dl.dropbox.com/u/3180464/SGRGBWBPS125F40.tfw
>> http://dl.dropbox.com/u/3180464/calibf2.zip
>>
>>> sessionInfo()
>> R version 2.13.0 (2011-04-13)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>>
>> locale:
>> ?[1] LC_CTYPE=en_US.UTF-8 ? ? ? ? ?LC_NUMERIC=C
>> ?[3] LC_TIME=en_US.UTF-8 ? ? ? ? ? LC_COLLATE=en_US.UTF-8
>> ?[5] LC_MONETARY=en_US.UTF-8 ? ? ? LC_MESSAGES=en_US.UTF-8
>> ?[7] LC_PAPER=en_US.UTF-8 ? ? ? ? ?LC_NAME=en_US.UTF-8
>> ?[9] LC_ADDRESS=en_US.UTF-8 ? ? ? ?LC_TELEPHONE=en_US.UTF-8
>> [11] LC_MEASUREMENT=en_US.UTF-8 ? ?LC_IDENTIFICATION=en_US.UTF-8
>>
>> attached base packages:
>> [1] grid ? ? ?stats ? ? graphics ?grDevices utils ? ? datasets ?methods
>> [8] base
>>
>> other attached packages:
>> [1] gridExtra_0.7 ggplot2_0.8.9 rgdal_0.6-33 ?raster_1.7-29 sp_0.9-76
>> [6] reshape_0.8.4 plyr_1.4 ? ? ?proto_0.3-8 ? rkward_0.5.6
>>
>> loaded via a namespace (and not attached):
>> [1] lattice_0.19-26 tools_2.13.0
>>>
>>
>> Agus
>>
>


From pierre.roudier at gmail.com  Thu Apr 28 05:56:52 2011
From: pierre.roudier at gmail.com (Pierre Roudier)
Date: Thu, 28 Apr 2011 15:56:52 +1200
Subject: [R-sig-Geo] Reading a WMS layer as a sp object
Message-ID: <BANLkTim9qY06rW4C4Rcqc6hGXsMxMCRt5w@mail.gmail.com>

Hi list,

Recently I had a lot of fun with the gmap() function in Robert's dismo
package. It is retrieving a Google Map layer (roadmap, satellite, or
terrain) that can be used as a background for plotting other sp
objects. The extent of the downloaded layer is given by a name
(geocode) or a Spatial* object. If you haven't tried it, you should -
it is fun and easy to use!

Following that nice experience, I was wondering if anybody has been
working on a similar process, but for a WMS layer (instead of using
Google's data... with the associated constraints). I got some
home-built WMS layers that would be more appropriate for use as a
background in my plots.

Cheers,

Pierre

-- 
Scientist
Landcare Research, New Zealand


From mdsumner at gmail.com  Thu Apr 28 06:27:12 2011
From: mdsumner at gmail.com (Michael Sumner)
Date: Thu, 28 Apr 2011 14:27:12 +1000
Subject: [R-sig-Geo] Reading a WMS layer as a sp object
In-Reply-To: <BANLkTim9qY06rW4C4Rcqc6hGXsMxMCRt5w@mail.gmail.com>
References: <BANLkTim9qY06rW4C4Rcqc6hGXsMxMCRt5w@mail.gmail.com>
Message-ID: <BANLkTikHOhsSJB1Y=S7kO3S0_w8dXXx0sQ@mail.gmail.com>

If you build rgdal against a GDAL with WMS support, then it should
work fine with readGDAL (rgdal) or raster.

Building GDAL is easy enough, and there are nightly builds that can be
used here:  http://vbkto.dyndns.org/sdk/

There are notes in the rgdal source for building on various platforms,
I haven't done it for ages though, on any platform.

If that's not feasible, then you can invoke the gdal_translate app
with from R system() to do the download for you, then read that from
GeoTIFF or whatever. There are instructions and examples here:

http://www.gdal.org/frmt_wms.html

Cheers, Mike.

On Thu, Apr 28, 2011 at 1:56 PM, Pierre Roudier
<pierre.roudier at gmail.com> wrote:
> Hi list,
>
> Recently I had a lot of fun with the gmap() function in Robert's dismo
> package. It is retrieving a Google Map layer (roadmap, satellite, or
> terrain) that can be used as a background for plotting other sp
> objects. The extent of the downloaded layer is given by a name
> (geocode) or a Spatial* object. If you haven't tried it, you should -
> it is fun and easy to use!
>
> Following that nice experience, I was wondering if anybody has been
> working on a similar process, but for a WMS layer (instead of using
> Google's data... with the associated constraints). I got some
> home-built WMS layers that would be more appropriate for use as a
> background in my plots.
>
> Cheers,
>
> Pierre
>
> --
> Scientist
> Landcare Research, New Zealand
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>



-- 
Michael Sumner
Institute for Marine and Antarctic Studies, University of Tasmania
Hobart, Australia
e-mail: mdsumner at gmail.com


From edzer.pebesma at uni-muenster.de  Thu Apr 28 10:41:28 2011
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Thu, 28 Apr 2011 10:41:28 +0200
Subject: [R-sig-Geo] spacetime : adding a fourth (spectral) dimension
In-Reply-To: <1303838148993-6306599.post@n2.nabble.com>
References: <1303838148993-6306599.post@n2.nabble.com>
Message-ID: <4DB92838.4000901@uni-muenster.de>



On 04/26/2011 07:15 PM, scizmeli wrote:
> I have recently been introduced to the promising package called spacetime
> which helps solve some of the data management problems we regularly
> encounter with time-series of spatial data. I would like to congratulate the
> developers of this wonderful package.

Thanks for the encouragement!
> 
> My question : Is there a way to incorporate in a spacetime object additional
> dimensions? For example, in remote sensing, we regularly deal with time
> series of spectral imagery. I can easily think of a 5-D model output
> (timeseries of a 3-d model of a spectral parameter). In such a case, it
> would also be useful to be able to specify auxiliary data for each band i.e.
> wavelength information and its units (412 nm, 515 nm etc). It would of
> course be better if these auxiliary data can be defined in a custom-manner.
> 
> If this is not possible for now, can it be considered in the near future?
> Spatial data nowadays can easily have more than 3 dimensions and it would be
> very useful seeing this addressed by the spacetime package.
> 

Yes and no. In a spacetime object st,

st[i,j,k]

has index k to refer to the attribute, and this can indicate the band
number (1:n) or name (B412,B515, etc). So yes, there is room in these
objects for multiple bands, but not as generic as array() does --
although that has its limits too.

Also, the wavelength information is "lost", or needs encoding in
attribute names, which is annoying when you need to decode. I can see a
relatively simple option of subclassing one of the available classes to
a class for multispectral data that holds an attribute (or slot) with
the wavelength information (wavelength values, or intervals per
attribute) and allow selection options like

st[,,"412::515"]

to select all bands in this particular range.

Which use cases, or particular demands, did you have in mind to
manipulate objects? What is the typical thing you do with these data
that you'd like to do with R / spacetime?

> Servet
> 
> --
> View this message in context: http://r-sig-geo.2731867.n2.nabble.com/spacetime-adding-a-fourth-spectral-dimension-tp6306599p6306599.html
> Sent from the R-sig-geo mailing list archive at Nabble.com.
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Weseler Stra?e 253, 48151 M?nster, Germany. Phone: +49 251
8333081, Fax: +49 251 8339763  http://ifgi.uni-muenster.de
http://www.52north.org/geostatistics      e.pebesma at wwu.de


From aritra.sourav at gmail.com  Thu Apr 28 16:43:53 2011
From: aritra.sourav at gmail.com (sourav sengupta)
Date: Thu, 28 Apr 2011 10:43:53 -0400
Subject: [R-sig-Geo] Plotting Data the globe
In-Reply-To: <BANLkTimqXkpGNnPBjNBoU8QwJGzrteKcXQ@mail.gmail.com>
References: <BANLkTik-U_EOn3R0UbQ6xpTmwSCg_QrMfQ@mail.gmail.com>
	<BANLkTimqXkpGNnPBjNBoU8QwJGzrteKcXQ@mail.gmail.com>
Message-ID: <BANLkTin+LkRwZqzx+obAHnVCsZBXJssc7A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110428/48d7dec2/attachment.pl>

From Roger.Bivand at nhh.no  Thu Apr 28 16:55:30 2011
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 28 Apr 2011 16:55:30 +0200 (CEST)
Subject: [R-sig-Geo] dnearneigh,
 knearneigh and SAR - not removing spatial autocorrelation
In-Reply-To: <65B692BA-57B5-4383-9101-C26303AA4941@gmail.com>
References: <F10E6F24-AAAE-4ED7-B0E0-48A41CC80D39@gmail.com>
	<alpine.LRH.2.00.1104272209330.3935@reclus.nhh.no>
	<65B692BA-57B5-4383-9101-C26303AA4941@gmail.com>
Message-ID: <alpine.LRH.2.00.1104281645380.3623@reclus.nhh.no>

On Wed, 27 Apr 2011, Chris Mcowen wrote:

> Hi Roger,
>
> Thanks for this, as previously mentioned i am relatively new to this, i 
> started with a GLS method but then realised it may not be the most 
> effective so i decided to try SAR methods.
>
>> What definition of neighbour are you using in the correlogram?
>

You have not said where the correlogram is coming from. Please say whether 
it is from pgirmess, or (probably) from ncf? Once you establish that, 
you'll know more about the spatial process, which obviously isn't the one 
you are modelling.

>
> I call the SAR as follows :- sem.nb1.5.w<-errorsarlm(ols2, 
> listw=nb1.5.w, zero.policy=F, na.action=na.omit)
>
> Then extract the residuals: - res.sem.nb1.5.w <- residuals(sem.nb1.5.w)
>
> Then use these to construct the correlogram: - 
> cor.sem.nb1.5.w<-correlog(data$X, data$Y, z=residuals(sem.nb1.5.w), 
> na.rm=T, increment=1, resamp=1)
>
>
>> The correlogram obviously doesn't care about no-neighbour observations, 
>> so perhaps do the same?
>
> How do i do this? If i set the dmax too low i get "Empty neighbour sets 
> found" so it wont run the model. I feel i am artificially setting the 
> dmax based on the fact it works rather than any scientific rationale.
>
> I tried a method used in your example:
>
>> coords <- coordinates(columbus)
>> rn <- sapply(slot(columbus, "polygons"), function(x) slot(x, "ID"))
>> k1 <- knn2nb(knearneigh(coords))
>> all.linked <- max(unlist(nbdists(k1, coords)))
>> col.nb.0.all <- dnearneigh(coords, 0, all.linked, row.names=rn)
>
> To join everything on a NN basis. But that resulted in little difference 
> in the correlgram and AIC.
>
> So my two questions are:
>
> Do i have to link every neighbour and if not how to i run the model 
> without getting the error and how do i decide which neighbours not to 
> link?

You do not have to, and if you do not want to, use the zero.policy= 
argument to permit analysis with weights objects including no-neighbour 
observations. It all depends on your model of the spatial process. If say 
biology tells you that interaction beyond 100m is unlikely, use 100m. 
Alternatively, choose a graph-based definition of neighbours, or possibly 
k-nearest, if you have a reason for choosing k.

>
> Will it make a large difference if the correlgram doesn't change much? 
> Does this mean the model is not "dealing" with the correlation 
> effectively and how do i improve this?

By thinking about what the correlogram function is doing in expressing the 
spatial process it thinks it sees - it isn't magic, and both 
representations are reasonable (autocorrelation present with the 
representation in the correlogram function, and autocorrelation absent 
with your distance based representation and row-standardised weights).

Roger

>
> Thanks again and sorry for my relatively naive questions!
>
> Chris
>
>
> On 27 Apr 2011, at 21:16, Roger Bivand wrote:
>
> On Wed, 27 Apr 2011, Chris Mcowen wrote:
>
>> Dear list,
>>
>> I was wondering if somebody could possibly offer me some advice?
>>
>> I have a dataset where there is spatial autocorrelation present - visible from the correlogram. So i tried to remove it using a SAR. My data is for 458 regions of differing size for which i have long - lat co-ordinates.
>>
>>
>> coords<-cbind(data$X,data$Y)
>> coords<-as.matrix(coords)
>>
>> The first approach was to use dnearneigh to set up the neighbourhood. I am very new to this and was having problems as regions would often appear with no links ( see below) so i upped dmax until this no longer occurred - this maybe a incorrect method?
>>
>>> dnearneigh(coords, 0, 1500, row.names = NULL, longlat = TRUE)
>> Neighbour list object:
>> Number of regions: 458
>> Number of nonzero links: 8990
>> Percentage nonzero weights: 4.285769
>> Average number of links: 19.62882
>> 2 regions with no links:
>> 235 236
>>
>> Results in - Empty neighbour sets found
>>
>>> dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
>> Neighbour list object:
>> Number of regions: 458
>> Number of nonzero links: 14200
>> Percentage nonzero weights: 6.769512
>> Average number of links: 31.00437
>>
>> I then converted this to a weight matrix and used in my SAR.
>>
>> nb1.5 <- dnearneigh(coords, 0, 2000, row.names = NULL, longlat = TRUE)
>> nb1.5.w<-nb2listw(nb1.5, glist=NULL, style="W", zero.policy=FALSE)
>>
>> However, looking at the correlogram and the AIC ( below) it seems to not have made a huge difference
>>
>> AIC: -2581.8, (AIC for lm: -2574)
>
> With relatively large numbers of neighbours, you smooth more. What definition of neighbour are you using in the correlogram? Why not just use the same? The correlogram obviously doesn't care about no-neighbour observations, so perhaps do the same? If you want to link every observation in, why not then down-weight distant neighbours using inverse distance weights - see ?nbdists.
>
> The test neighbour definition and the weights used for model fitting do not match. Further, you don't say whether your correlogram is for the OLS model residuals or just the response variable. If the latter, the explanatory variables may co-vary in space with the response, so the residuals are in fact not spatially autocorrelated.
>
> Hope this helps,
>
> Roger
>
>>
>> So i tried defining my neighbourhood using the knearneigh function but that made very little difference
>>
>> test <- knearneigh(coords, k=1, longlat = NULL, RANN=TRUE)
>> knn2nb(test, row.names = NULL, sym = FALSE)
>> k1 <- knn2nb(knearneigh(coords))
>> all.linked <- max(unlist(nbdists(k1, coords)))
>> col.nb.0.all <- dnearneigh(coords, 0, all.linked, row.names=rn)
>>
>> AIC: -2581.8, (AIC for lm: -2574)
>>
>> IS there something i am doing wrong or is there a step i am not doing?
>>
>> Any help would be gratefully received.
>>
>> Chris
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no


From clement.calenge at oncfs.gouv.fr  Thu Apr 28 18:39:57 2011
From: clement.calenge at oncfs.gouv.fr (=?windows-1252?Q?Cl=E9ment_Calenge?=)
Date: Thu, 28 Apr 2011 18:39:57 +0200
Subject: [R-sig-Geo] New version of adehabitatLT and adehabitatHR
Message-ID: <4DB9985D.3040504@oncfs.gouv.fr>

Dear all,

I have just uploaded to CRAN a new version of adehabitatLT and 
adehabitatHR.
Significant changes in adehabitatLT are listed below:

* The residence time method of Barraquand and Benhamou (2008) is now 
implemented in the function residenceTime.

* The segmentation method of Lavielle (2000) (used for the residence 
time) is now implemented in the function lavielle.

* The function redisltraj now takes an additional argument 
type=c("space","time"). When type == "space", a rediscretization of the 
trajectory in steps of equal length is carried out (as previously). When 
type == "time", a rediscretization of the trajectory in steps of equal 
duration is carried out.

* a new function na.omit.ltraj can be used to remove missing values from 
a trajectory

* Several functions are now available to build null models for 
trajectory analysis (see the help page of the function testNM, as well 
as the vignette for additional information).

Significant changes in adehabitatHR are listed below:

* There is a major change in the function BRB: in the previous version 
of adehabitatHR, there was a slight inconsistency in the package design: 
whereas all the parameters characterizing the steps in an object of 
class "ltraj" (e.g. ?dist, dx, dy?) describe the step between 
relocations i and i+1, it was required for ?BRB? that the activity 
described the proportion of activity time between relocation i-1 and i. 
This inconsistency has been corrected in the new version: the activity 
measured at relocation i should correspond to the activity of the animal 
between relocation i and i+1.

* The dependencies of adehabitatHR on the packages tripack and gpclib 
have been replaced by dependencies on the packages deldir and rgeos to 
circumvent the serious licence issues of the former.

All major changes are documented in the vignettes of the packages.
Happy testing,


Cl?ment Calenge

-- 
Cl?ment CALENGE
Cellule d'appui ? l'analyse de donn?es
Direction des Etudes et de la Recherche
Office national de la chasse et de la faune sauvage
Saint Benoist - 78610 Auffargis
tel. (33) 01.30.46.54.14


From emorway at usgs.gov  Fri Apr 29 05:12:11 2011
From: emorway at usgs.gov (emorway)
Date: Thu, 28 Apr 2011 20:12:11 -0700 (PDT)
Subject: [R-sig-Geo] read projection file throws error
Message-ID: <1304046731429-6315537.post@n2.nabble.com>

Hello,

Having searched around on some related posts I have yet to figure this one
out.  While trying to grab the spatial information of a raster I turned to
the readOGR function.  

myprj<-readOGR(dsn="c:/temp/",layer="RasterDat")

but got an error:

#Error in ogrInfo(dsn = dsn, layer = layer, input_field_name_encoding =
input_field_name_encoding) : 
#  Cannot open file

Next, I used ArcCatalog to create an empty shapefile and imported the
spatial information from the raster to the shapefile and tried using readOGR
and readShapeSpatial on it (in case there was some issue with the raster):

myprj<-readOGR(dsn="c:/temp/",layer="dummy_ShpFile")
#Error in ogrInfo(dsn = dsn, layer = layer, input_field_name_encoding =
input_field_name_encoding) : 
#  Cannot open file
myprj<-readShapeSpatial(system.file("c:/temp/Dummy_ShpFile.shp"))
#Error in getinfo.shape(fn) : Error opening SHP file

but I know the file is there using this command:

getinfo.shape("c:/temp/Dummy_ShpFile.shp")
#Shapefile type: PolyLine, (3), # of Shapes: 0

I have attached the 3 files associated with the raster and put the contents
of the *.prj here:

PROJCS["User_Defined_Stereographic_North_Pole",GEOGCS["GCS_User_Defined",DATUM["D_User_Defined",SPHEROID["User_Defined_Spheroid",6371200.0,0.0]],PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]],PROJECTION["Stereographic_North_Pole"],PARAMETER["False_Easting",0.0],PARAMETER["False_Northing",0.0],PARAMETER["Central_Meridian",-105.0],PARAMETER["Standard_Parallel_1",60.0],UNIT["Meter",1.0]]

http://r-sig-geo.2731867.n2.nabble.com/file/n6315537/RadarDat.asc
RadarDat.asc 
http://r-sig-geo.2731867.n2.nabble.com/file/n6315537/RadarDat.prj
RadarDat.prj 
http://r-sig-geo.2731867.n2.nabble.com/file/n6315537/RadarDat.asc.xml
RadarDat.asc.xml 

The post that looked most relevant appears to have had a bit of private back
and forth and so the final solution wasn't available: 
http://r.789695.n4.nabble.com/Problem-with-uploading-library-td3170455.html#a3170670
--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/read-projection-file-throws-error-tp6315537p6315537.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From mdsumner at gmail.com  Fri Apr 29 05:25:40 2011
From: mdsumner at gmail.com (Michael Sumner)
Date: Fri, 29 Apr 2011 13:25:40 +1000
Subject: [R-sig-Geo] read projection file throws error
In-Reply-To: <1304046731429-6315537.post@n2.nabble.com>
References: <1304046731429-6315537.post@n2.nabble.com>
Message-ID: <BANLkTi=3CB9am20iHu6jCvHmmaj-n2v+jw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110429/f410ed24/attachment.pl>

From emorway at usgs.gov  Fri Apr 29 06:08:25 2011
From: emorway at usgs.gov (emorway)
Date: Thu, 28 Apr 2011 21:08:25 -0700 (PDT)
Subject: [R-sig-Geo] read projection file throws error
In-Reply-To: <BANLkTi=3CB9am20iHu6jCvHmmaj-n2v+jw@mail.gmail.com>
References: <1304046731429-6315537.post@n2.nabble.com>
	<BANLkTi=3CB9am20iHu6jCvHmmaj-n2v+jw@mail.gmail.com>
Message-ID: <1304050105218-6315605.post@n2.nabble.com>

Hi Michael,

Thanks for pointing out my blunders, it never pays to work late as evidenced
by such simple mistakes.  That is not to say I didn't learn anything from
your response, I certainly did.  Part of why I was trying to get at the
spatial reference information in my first post was because it doesn't appear
to get written in the code below (using the 'raster' library), which is
unfortunate because I would like to read in the resultant raster
(intermediate calcs omitted for now) to an ArcMap project?

ex1<-raster("c:/temp/RadarDat.asc")

ex1
#class       : RasterLayer 
#dimensions  : 131, 131, 17161  (nrow, ncol, ncell)
#resolution  : 4762.5, 4762.5  (x, y)
#extent      : -1704975, -1081088, -5710238, -5086350  (xmin, xmax, ymin,
ymax)
#projection  : +proj=stere +lat_0=90 +lat_ts=60 +lon_0=-105 +k=1 +x_0=0
+y_0=0 +a=6371200 +b=6371200 +units=m +no_defs 
#values      :
M:/NEXRAD_RenoRadar/2009_AsciiGridFiles/KRGX_DPA_20090605_172100_native.asc 
#min         : ? 
#max         : ? 

appears to have projection info

writeRaster(ex1,"c:/temp/whatnow",format="ascii",overwrite=TRUE,NAflag=-999)  
#class       : RasterLayer                                                     
#dimensions  : 131, 131, 17161  (nrow, ncol, ncell)                            
#resolution  : 4762.5, 4762.5  (x, y)                                          
#extent      : -1704975, -1081088, -5710238, -5086350  (xmin, xmax, ymin,
ymax)
#projection  : NA                                                              
#values      : c:/temp/whatnow.asc                                             
#min         : ?                                                               
#max         : ?                                                               

projection  : NA  ???  If I'm missing an option or specifying a certain
variable, I haven't been able to find it in the documentation.

Eric
--
View this message in context: http://r-sig-geo.2731867.n2.nabble.com/read-projection-file-throws-error-tp6315537p6315605.html
Sent from the R-sig-geo mailing list archive at Nabble.com.


From federicotomasetto at hotmail.it  Fri Apr 29 06:14:44 2011
From: federicotomasetto at hotmail.it (Federico Tomasetto)
Date: Fri, 29 Apr 2011 04:14:44 +0000
Subject: [R-sig-Geo] Spatial autocorrelation in GLM/GAM/CART binomial family
In-Reply-To: <DUB107-w3678F68AC64BDB30EA78B5C89A0@phx.gbl>
References: <DUB107-w3678F68AC64BDB30EA78B5C89A0@phx.gbl>
Message-ID: <DUB107-w4222F08EB0EE24BDE52719C89A0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110429/f65bdfcd/attachment.pl>

From alobolistas at gmail.com  Fri Apr 29 11:57:40 2011
From: alobolistas at gmail.com (Agustin Lobo)
Date: Fri, 29 Apr 2011 11:57:40 +0200
Subject: [R-sig-Geo] Problem with raster::calc
Message-ID: <BANLkTi=axHVCKg+jibAdzK6Ef4ycuSrWBw@mail.gmail.com>

I've calculated a linear model using a subset of data from raster files
> sgcirRlm = lm(SGRGBF40data[,4] ~ SGCIRF56data[,4]+SGCIRF56data[,5]+SGCIRF56data[,6]
> coefficients(sgcirRlm)
      (Intercept) SGCIRF56data[, 4] SGCIRF56data[, 5] SGCIRF56data[, 6]
    -2075.0432230         0.9229117         0.9963710        -0.8310794

and want to apply the coefficients to the multiband raster
> show(SGCIRF56)
class       : RasterBrick
dimensions  : 1760, 2640, 3  (nrow, ncol, nlayers)
resolution  : 1, 1  (x, y)
extent      : 0, 2640, -1759, 1  (xmin, xmax, ymin, ymax)
projection  : +proj=utm +zone=31 +ellps=intl +units=m +no_defs
values      : /media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL/SGCIR/SGCIRWBPS125F56v2.tif
min values  : 0 0 0
max values  : 65535 65535 65535

to calculate the predicted raster, for which I'm using subset() within calc():
> sgcirRpred = calc(SGCIRF56,fun=function(x){coefficients(sgcirRlm)[1]+coefficients(sgcirRlm)[2]*subset(x,1) + coefficients(sgcirRlm)[3]*subset(x,2) + coefficients(sgcirRlm)[4]*subset(x,3)})

but this results into an error:
Error in .local(x, fun, ...) : cannot use this function
Calls: calc -> calc -> .local

is this because subset() cannot be used within the function to be
provided to calc()?

Thanks
> sessionInfo()
R version 2.13.0 (2011-04-13)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
 [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
[11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8

attached base packages:
[1] grid      stats     graphics  grDevices utils     datasets  methods
[8] base

other attached packages:
[1] gridExtra_0.7 ggplot2_0.8.9 rgdal_0.6-33  raster_1.8-16 sp_0.9-76
[6] reshape_0.8.4 plyr_1.4      proto_0.3-8   rkward_0.5.6

loaded via a namespace (and not attached):
[1] lattice_0.19-26 tools_2.13.0

Agus


From alobolistas at gmail.com  Fri Apr 29 12:15:47 2011
From: alobolistas at gmail.com (Agustin Lobo)
Date: Fri, 29 Apr 2011 12:15:47 +0200
Subject: [R-sig-Geo] Problem with raster::extract
In-Reply-To: <BANLkTimj6UMj1JQDdW7+P+msKt3PgE40xg@mail.gmail.com>
References: <BANLkTi=y5-0UZ7b9cH5X5HpcNS6uF3+8uQ@mail.gmail.com>
	<BANLkTinkhdSDhzpT3nqBgysY5RSXrwrDEQ@mail.gmail.com>
	<BANLkTimj6UMj1JQDdW7+P+msKt3PgE40xg@mail.gmail.com>
Message-ID: <BANLkTikkt917H8CO4HHNkcjCC4w2NNc1ug@mail.gmail.com>

I confirm the problem is solved in  raster_1.8-16

Thanks!

Agus

2011/4/28 Robert J. Hijmans <r.hijmans at gmail.com>:
> This was caused by raster incorrectly interpreting the highest
> possible INT2U value (65530) as NA. Now fixed (version 1.8-16), at
> least for this case. Thanks for reporting, Robert
>
> On Wed, Apr 27, 2011 at 8:52 AM, Robert J. Hijmans <r.hijmans at gmail.com> wrote:
>> Hello Agus,
>>
>> The NaN suggests that there are only NA values for that polygon/layer.
>> To investigate this I would do
>>
>> v <- extract(SGRGBF40, calibf2, nl=3)
>>
>> # i.e. do not use a function and then inspect
>>
>> v[[7]]
>>
>>
>> All functions must accept an na.rm argument (even if they choose to
>> ignore it. So for length you can do something like:
>>
>> vl <- extract(SGRGBF40, calibf2, fun=function(x,...)length(x), nl=3)
>>
>>
>> Hope this helps, Robert
>>
>>> v
>> [1] 387.8158 329.3913
>>> v <- extract(r, polys, length)
>> Error in FUN(X[[1L]], ...) :
>> ?2 arguments passed to 'length' which requires 1
>>> v <- extract(r, polys, function(x,...)length(x))
>>> v
>> [1] 38 23
>>>
>>
>>
>> On Wed, Apr 27, 2011 at 3:14 AM, Agustin Lobo <alobolistas at gmail.com> wrote:
>>> Hi!
>>>
>>> I'm doing
>>>
>>> SGRGBF40 = brick("/media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL/SGRGB/SGRGBWBPS125F40.tif")
>>> calibf2 = readOGR(dsn="/media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL",layer="calibf2",stringsAsFactors=F)
>>> projection(SGRGBF40) = projection(calibf2)
>>> #I make sure vector calibf2 and raster SGRGBF40 do overlap:
>>> plot(subset(SGRGBF40,1))
>>> plot(calibf2,add=T)
>>>
>>> #Calculate mean values for each polygon:
>>> v <- extract(SGRGBF40, calibf2,fun=mean,na.rm=T,nl=3)
>>> summary(v)
>>> SGRGBF40data = cbind(calibf2 at data,v)
>>>
>>> But then, despite na.rm=T:
>>>> SGRGBF40data[7,]
>>> ?ID code codetxt SGRGBWBPS125F40_1 SGRGBWBPS125F40_2 SGRGBWBPS125F40_3
>>> 6 ?7 ? ?7 ? ? ?C7 ? ? ? ? ? ? ? NaN ? ? ? ? ?39781.02 ? ? ? ? ? ? ? ? 3
>>>
>>>> summary(v)
>>> ?SGRGBWBPS125F40_1 SGRGBWBPS125F40_2 SGRGBWBPS125F40_3
>>> ?Min. ? :18444 ? ? Min. ? : ?119.4 ? Min. ? : ? ?3
>>> ?1st Qu.:34098 ? ? 1st Qu.:25875.7 ? 1st Qu.:15633
>>> ?Median :37776 ? ? Median :36668.2 ? Median :22374
>>> ?Mean ? :41314 ? ? Mean ? :36882.2 ? Mean ? :29737
>>> ?3rd Qu.:52236 ? ? 3rd Qu.:49452.6 ? 3rd Qu.:48116
>>> ?Max. ? :65396 ? ? Max. ? :65417.5 ? Max. ? :63826
>>> ?NA's ? : ? ?3
>>>
>>> While there are no NA in the raster brick:
>>>> summary(SGRGBF40)
>>> Cells: ?4646400
>>> NAs ?: ?0 0 0
>>>
>>> ? ? ? ? ? ?1 ? ? 2 ? ? 3
>>> Min. ? ? 1777 ? ? 3 ? ? 3
>>> 1st Qu. 34910 22460 18640
>>> Median ?42660 27540 24420
>>> Mean ? ?41070 28570 25170
>>> 3rd Qu. 48470 33320 29060
>>> Max. ? ?65520 65530 64220
>>>
>>>
>>> A possibility would be a division by 0 (weird, because all polygons
>>> are large), so I've tried:
>>>> vl <- extract(SGRGBF40, calibf2,fun=length,nl=3)
>>> but...
>>> Error in t(sapply(res[!i], function(x) apply(x, 2, FUN = fun, na.rm =
>>> na.rm))) :
>>> ?error in evaluating the argument 'x' in selecting a method for
>>> function 't': Error in FUN(newX[, i], ...) :
>>> ?2 arguments passed to 'length' which requires 1
>>> Calls: sapply -> lapply -> FUN -> apply
>>> Calls: extract -> extract -> .polygonValues -> t
>>>
>>> Any hint?
>>>
>>> Data:
>>> http://dl.dropbox.com/u/3180464/SGRGBWBPS125F40.tif
>>> http://dl.dropbox.com/u/3180464/SGRGBWBPS125F40.tfw
>>> http://dl.dropbox.com/u/3180464/calibf2.zip
>>>
>>>> sessionInfo()
>>> R version 2.13.0 (2011-04-13)
>>> Platform: x86_64-pc-linux-gnu (64-bit)
>>>
>>> locale:
>>> ?[1] LC_CTYPE=en_US.UTF-8 ? ? ? ? ?LC_NUMERIC=C
>>> ?[3] LC_TIME=en_US.UTF-8 ? ? ? ? ? LC_COLLATE=en_US.UTF-8
>>> ?[5] LC_MONETARY=en_US.UTF-8 ? ? ? LC_MESSAGES=en_US.UTF-8
>>> ?[7] LC_PAPER=en_US.UTF-8 ? ? ? ? ?LC_NAME=en_US.UTF-8
>>> ?[9] LC_ADDRESS=en_US.UTF-8 ? ? ? ?LC_TELEPHONE=en_US.UTF-8
>>> [11] LC_MEASUREMENT=en_US.UTF-8 ? ?LC_IDENTIFICATION=en_US.UTF-8
>>>
>>> attached base packages:
>>> [1] grid ? ? ?stats ? ? graphics ?grDevices utils ? ? datasets ?methods
>>> [8] base
>>>
>>> other attached packages:
>>> [1] gridExtra_0.7 ggplot2_0.8.9 rgdal_0.6-33 ?raster_1.7-29 sp_0.9-76
>>> [6] reshape_0.8.4 plyr_1.4 ? ? ?proto_0.3-8 ? rkward_0.5.6
>>>
>>> loaded via a namespace (and not attached):
>>> [1] lattice_0.19-26 tools_2.13.0
>>>>
>>>
>>> Agus
>>>
>>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From a.mosnier at gmail.com  Fri Apr 29 15:10:16 2011
From: a.mosnier at gmail.com (Arnaud Mosnier)
Date: Fri, 29 Apr 2011 09:10:16 -0400
Subject: [R-sig-Geo] Spatial autocorrelation in GLM/GAM/CART binomial
	family
Message-ID: <BANLkTikCSWyqgko44F2uoX3q=k10MHW82g@mail.gmail.com>

Federico,

I recommend reading the following paper :

Methods to account for spatial autocorrelation in the analysis of
species distributional data: a review
Carsten F. Dormann, Jana M. McPherson, Miguel B. Ara?jo, Roger Bivand,
Janine Bolliger, Gudrun Carl, Richard G. Davies, Alexandre Hirzel,
Walter Jetz, W. Daniel Kissling, Ingolf K?hn, Ralf Ohlem?ller, Pedro
R. Peres-Neto, Bj?rn Reineking, Boris Schr?der, Frank M. Schurr,
Robert Wilson
Ecography Volume 30, Issue 5, pages 609?628, October 2007

Hope this help,

Arnaud


Date: Fri, 29 Apr 2011 04:14:44 +0000
From: Federico Tomasetto <federicotomasetto at hotmail.it>
To: <r-sig-geo at r-project.org>
Subject: [R-sig-Geo] Spatial autocorrelation in GLM/GAM/CART binomial
       family
Message-ID: <DUB107-w4222F08EB0EE24BDE52719C89A0 at phx.gbl>
Content-Type: text/plain


Hi List,


I have 1227 data plots regularly sampled in a grid
of 1 x 1 Km in which species richness and abundance of 682 vascular plant
species have been recorded.

I constructed also a database of environmental
variables with GIS. My first goal was to compute a GLS model to explain species
richness with the environment. To do so I used GLS + spatial structure as it is
explained in Pinheiro-Bates, Mixed-Effects Models in S and S-PLUS (pag
260-267). The model spatial vs non spatial have worked ok, showing lower AIC
values in GLS + spatial model than GLS + no spatial. So my data is not
spatially independent and show spatial autocorrelation.

Now, I intend to compute GLM/GAM/CART models with
binomial data family to compute the abundance of species in each plot and the
environment taking into account the spatial autocorrelation. The final
objective would be to compare those models (spatial and non spatial and check
which one is the "best").

I read that a way to do a spatial GLM is using the
glmmPQL function of MASS and putting all the observations in the same group for
the random effect. If this is the way to do it,
how about GAM and CART?

Do you have any idea how to deal with 682 species and model
fitting and if there is any other methods to do what I want (comparison of
spatial and non-spatial)?
Many thanks for any kind of help
Federico


From r.hijmans at gmail.com  Fri Apr 29 17:49:29 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Fri, 29 Apr 2011 08:49:29 -0700
Subject: [R-sig-Geo] Problem with raster::calc
In-Reply-To: <BANLkTi=axHVCKg+jibAdzK6Ef4ycuSrWBw@mail.gmail.com>
References: <BANLkTi=axHVCKg+jibAdzK6Ef4ycuSrWBw@mail.gmail.com>
Message-ID: <BANLkTimKRSpzn_-W6WB6xi3Qz3Gg0CrWXw@mail.gmail.com>

Agus,

I think what you want to accomplish can be done like this:

x <- predict(SGCIRF56, sgcirRpred)

but if you want to use calc, I think it should be something like

sgcirRpred = calc(SGCIRF56, fun=function(x){coefficients(sgcirRlm)[1]
+ coefficients(sgcirRlm)[2:4] * x })

But in this case you need to make sure that the order of the layers in
SGCIRF56 is the same as in the coefficients.

subset(x,3)
is, it seems to me, an invalid statement
but following your approach, this might work:

sgcirRpred = calc(SGCIRF56,fun=function(x){coefficients(sgcirRlm)[1]+coefficients(sgcirRlm)[2]*x[1]
+ coefficients(sgcirRlm)[3]*x[2] + coefficients(sgcirRlm)[4]*x[3] } )

Best, Robert


On Fri, Apr 29, 2011 at 2:57 AM, Agustin Lobo <alobolistas at gmail.com> wrote:
> I've calculated a linear model using a subset of data from raster files
>> sgcirRlm = lm(SGRGBF40data[,4] ~ SGCIRF56data[,4]+SGCIRF56data[,5]+SGCIRF56data[,6]
>> coefficients(sgcirRlm)
> ? ? ?(Intercept) SGCIRF56data[, 4] SGCIRF56data[, 5] SGCIRF56data[, 6]
> ? ?-2075.0432230 ? ? ? ? 0.9229117 ? ? ? ? 0.9963710 ? ? ? ?-0.8310794
>
> and want to apply the coefficients to the multiband raster
>> show(SGCIRF56)
> class ? ? ? : RasterBrick
> dimensions ?: 1760, 2640, 3 ?(nrow, ncol, nlayers)
> resolution ?: 1, 1 ?(x, y)
> extent ? ? ?: 0, 2640, -1759, 1 ?(xmin, xmax, ymin, ymax)
> projection ?: +proj=utm +zone=31 +ellps=intl +units=m +no_defs
> values ? ? ?: /media/Iomega_HDD/UAVetal/CALIBRACIONRADIOM/TESTCASA/CALSEL/SGCIR/SGCIRWBPS125F56v2.tif
> min values ?: 0 0 0
> max values ?: 65535 65535 65535
>
> to calculate the predicted raster, for which I'm using subset() within calc():
>> sgcirRpred = calc(SGCIRF56,fun=function(x){coefficients(sgcirRlm)[1]+coefficients(sgcirRlm)[2]*subset(x,1) + coefficients(sgcirRlm)[3]*subset(x,2) + coefficients(sgcirRlm)[4]*subset(x,3)})
>
> but this results into an error:
> Error in .local(x, fun, ...) : cannot use this function
> Calls: calc -> calc -> .local
>
> is this because subset() cannot be used within the function to be
> provided to calc()?
>
> Thanks
>> sessionInfo()
> R version 2.13.0 (2011-04-13)
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> locale:
> ?[1] LC_CTYPE=en_US.UTF-8 ? ? ? ? ?LC_NUMERIC=C
> ?[3] LC_TIME=en_US.UTF-8 ? ? ? ? ? LC_COLLATE=en_US.UTF-8
> ?[5] LC_MONETARY=en_US.UTF-8 ? ? ? LC_MESSAGES=en_US.UTF-8
> ?[7] LC_PAPER=en_US.UTF-8 ? ? ? ? ?LC_NAME=en_US.UTF-8
> ?[9] LC_ADDRESS=en_US.UTF-8 ? ? ? ?LC_TELEPHONE=en_US.UTF-8
> [11] LC_MEASUREMENT=en_US.UTF-8 ? ?LC_IDENTIFICATION=en_US.UTF-8
>
> attached base packages:
> [1] grid ? ? ?stats ? ? graphics ?grDevices utils ? ? datasets ?methods
> [8] base
>
> other attached packages:
> [1] gridExtra_0.7 ggplot2_0.8.9 rgdal_0.6-33 ?raster_1.8-16 sp_0.9-76
> [6] reshape_0.8.4 plyr_1.4 ? ? ?proto_0.3-8 ? rkward_0.5.6
>
> loaded via a namespace (and not attached):
> [1] lattice_0.19-26 tools_2.13.0
>
> Agus
>


From kerryr at sccwrp.org  Fri Apr 29 20:37:45 2011
From: kerryr at sccwrp.org (Kerry Ritter)
Date: Fri, 29 Apr 2011 11:37:45 -0700
Subject: [R-sig-Geo] kriging variance vs sample spacing
Message-ID: <4DBB0579.5080000@sccwrp.org>

Hi I want to compute the kriging variance as a function of grid spacing 
so that I can measure cost vs. kriging variance in deciding on an 
appropriate grid spacing for an ocean sediment surrounding an outfall.  
 From an earlier study I have a response with an anisotropic variogram 
given by model=vgm(0.37108, "Gau", 4500, anis = c(0, 0.75)).  The 
sampling area is a polygon.  To compute this curve, I have created a 
series of shapefiles containing the polygon with different grid 
spacings.  I want to compute the kriging variance for each grid spacing 
so I can plot it on the y-axis and grid spacing on the x-axis.

   I have tried the following code:  I have been using the library gstat 
and geoR

names(SD1000)                      # shape file that contains a 1000 x 
750 sampling grid (in therms of UTM coordinates) - this is what I wan to 
find the associated kriging variance
g.dummy <- gstat(formula = z~1, locations= ~lon_UTM+lat_UTM, dummy = 
TRUE, beta = 0,
     model = vgm(0.37108, "Gau", 4500, anis = c(0, 
0.75)))                            #variance model from earlier survey
BRI1000 <- predict(g.dummy,newdata=SD1000, nsim = 1)
BRI1000                                                                
                   #sampling grid of 1000 x 750 simulated values


SD100.spdf=SD100                                                             
# shape file that contains a dense grid resolution (153 x 100m) polygon
coordinates(SD100.spdf)=~lon_UTM+lat_UTM
class(SD100.spdf)
head(SD100.spdf)


vgm.aniso7=variogram(sim1~1,BRI1000.spdf, 
alpha=c(0,90,180,270),boundaries= c(0,300,900,1300,1800,1900,
     2300,2800,3000,3500,4000,4500,6000,10000,15000),cressie=TRUE)

vgmaniso.fit7=fit.variogram(vgm.aniso7,vgm(0.37108,model="Gau",range= 
4500,anis=c(0,0.75)),fit.sills=c(FALSE,FALSE),fit.range=c(FALSE,FALSE),fit.method=7) 


vgmaniso.fit1V.pr=krige(sim1~1,BRI1000.spdf,SD100.spdf,vgmaniso.fit7)
summary(vgmaniso.fit1V.pr)

Coordinates:
             min     max
lon_UTM  464500  476500
lat_UTM 3606804 3625196
Is projected: NA
proj4string : [NA]
Number of points: 9539
Data attributes:
    var1.pred          var1.var
  Min.   :-1.6710   Min.   :3.270e-11
  1st Qu.:-0.7611   1st Qu.:1.736e-08
  Median :-0.3351   Median :1.002e-07
  Mean   :-0.2340   Mean   :2.711e-05 <------- Kriging variance????
  3rd Qu.: 0.3641   3rd Qu.:1.098e-06
  Max.   : 1.4195   Max.   :7.022e-03


Thank you in advance for your help.
-Kerry

-- 
**********************
Kerry Ritter, Ph.D.
statistician
Southern California Coastal Water Research Project
3535 Harbor Blvd., Suite 110

work: 714-755-3210
cell: 714-420-3346
fax:  714-755-3299

email: kerryr at sccwrp.org


From kerryr at sccwrp.org  Fri Apr 29 23:35:11 2011
From: kerryr at sccwrp.org (Kerry Ritter)
Date: Fri, 29 Apr 2011 14:35:11 -0700
Subject: [R-sig-Geo] gstat - data formats- krige and kriging variance in
 dependent of data
Message-ID: <4DBB2F0F.3070305@sccwrp.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20110429/e720d578/attachment.pl>

From horning at amnh.org  Sat Apr 30 02:50:25 2011
From: horning at amnh.org (Ned Horning)
Date: Fri, 29 Apr 2011 20:50:25 -0400
Subject: [R-sig-Geo] spsample taking excessive memory
Message-ID: <4DBB5CD1.4040609@amnh.org>

Hi -

I am trying to select 1000 random samples from each set of ESRI 
Shapefile polygons with the same attribute value (attName). This has 
worked well in the past with other shapefiles. The file I'm using now 
has 69 relatively simple polygons with 9 unique values for the attribute 
I'm using to group the polygons. When I run spsample it takes several 
minutes to collect the samples for some of the unique attribute values 
and it is using well over 12GB of memory. By the time it start 
collecting samples from the last set of polygons my swap space is nearly 
exhausted and the spsample process goes on for hours until I kill it. Do 
these symptoms sound familiar to anyone? Any ideas about what the 
problem might be? I pasted my code below.

Thanks in advance for any help.

Ned

--
shapefile <- '/home/nedhorning/AMNH/Galapagos/quickbird_train_4.shp'
numsamps <- 1000
attName <- 'type_id'

vec <- readShapePoly(shapefile)
#
# Create vector of unique land cover attribute values
allAtt <- slot(vec, "data")
tabAtt <-table(allAtt[[attName]])
uniqueAtt <-as.numeric(names(tabAtt))
# Create input data from a Shapefile with training data and the image
for (x in 1:length(uniqueAtt)) {
   # Create a vector of all date for attName=x
   class_data <- vec[vec[[attName]]==uniqueAtt[x],]
   # Get random points for the class
   classpts <- spsample(class_data, type="random", n=numsamps)
   #Append data for all but the first run
   if (x == 1) {
     xy <- classpts
   } else {
     xy <- rbind(xy, classpts)
   }
}


