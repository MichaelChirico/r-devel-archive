From m_konk01 @end|ng |rom un|-muen@ter@de  Mon Aug  3 09:51:40 2020
From: m_konk01 @end|ng |rom un|-muen@ter@de (Markus Konkol)
Date: Mon, 3 Aug 2020 09:51:40 +0200
Subject: [R-sig-Geo] Online study in the context of open reproducible
 research
Message-ID: <3aa6b621-380b-52bf-193e-da58c5f4c006@uni-muenster.de>

Dear R-geo community,

I am Markus and a member of the project Opening Reproducible Research 
(https://o2r.info/) at the Institute for Geoinformatics under the 
supervision of Prof. Dr. Christian Kray and Prof. Dr. Edzer Pebesma. I 
am currently running an online study to investigate the understanding of 
open reproducible research. For this reason, I am looking for 
researchers from the geo-domain who have some familiarity with R and are 
interested in participating in my study. Substantial R expertise is not 
required. Being able to open, run, read, and write a bit of R code is 
already enough. The study will take no longer than one hour. With the 
help of a system we developed, the participants will solve some tasks 
(we do not test the participants, so they cannot do anything wrong!) and 
then we will have a little interview to talk a bit about their opinions 
and experiences while using the system. The exact dates will be arranged 
individually. You can contact me via mail: m.konkol at uni-muenster.de 
<mailto:m.konkol at uni-muenster.de>. Don?t hesitate to forward this mail 
to anyone who might be interested.

Thanks in advance and kind regards,

Markus Konkol


	[[alternative HTML version deleted]]


From P|etro_te|@to @end|ng |rom hotm@||@com  Mon Aug  3 18:28:24 2020
From: P|etro_te|@to @end|ng |rom hotm@||@com (Pietro Andre Telatin Paschoalino)
Date: Mon, 3 Aug 2020 16:28:24 +0000
Subject: [R-sig-Geo] Doubt
Message-ID: <BN8PR17MB2980A9900EBB6DFAFAF367A5824D0@BN8PR17MB2980.namprd17.prod.outlook.com>

Hello everyone, follow my doubts about the sphet package, related to direct and indirect impacts, and simulation of standard errors.

I will be very grateful if anyone can help me.

follow my code using built-in columbus data.

library(rgdal)
library(sp)
library(raster)
library(rgeos)
library(maptools)
library(spdep)
library (splm)
library(sphet)

data(columbus, package="spdep")
listw <- spdep::nb2listw(col.gal.nb)

attach(columbus)

#i'm going direct to regression by sphet

#SAC (SARAR MODEL)

SAC.robust <- spreg(INC ~ HOVAL+DISCBD+CP, data=columbus, listw, model="sarar", het=TRUE)

#LAG MODEL

SARrobust <- spreg(INC ~ HOVAL+DISCBD+CP, data=columbus, listw, model="lag", het=TRUE)

#going to my questions about the impacts function

#FOR SAC MODEL

> W <- as(as_dgRMatrix_listw(listw), "CsparseMatrix")
Warning message:

Function as_dgRMatrix_listw moved to the spatialreg package

> trMatc <- trW(W, type="mult")

Warning message:
Function trW moved to the spatialreg package

> effects<- summary(impacts(SAC.robust, tr=trMatc, R=10000), zstats=TRUE, short=TRUE)
> effects

========================================================
Simulation results (GSTSLS variance matrix):
========================================================

 #FOR SAR MODEL
>
> effects<- summary(impacts(SARrobust, tr=trMatc, R=10000), zstats=TRUE, short=TRUE)

that gives me:

Error in UseMethod("impacts", obj) :
  no applicable method for 'impacts' applied to an object of class "c('sphet', 'stsls_sphet')"

#how i said the impacts is not computable.

#One way to compute the impacts is by:

effects<- summary(spatialreg::impacts(SARrobust, tr=trMatc, R=10000), zstats=TRUE, short=TRUE)
effects

Impact measures (lag, trace):
========================================================
Simulation results (IV HAC variance matrix):
========================================================

 the simulation of standard errors and p-values is done by IV HAC. (which left me confused because I did not specify such an
#attribute in my regression, leaving me in doubt if the standard errors of the regression had also been corrected with HAC.)

#another option is:

effects<- summary(spatialreg::impacts.stsls(SARrobust, tr=trMatc, R=10000), zstats=TRUE, short=TRUE)
effects

#again simulated through the HAC variance matrix.
#Note: In this case, generally, all p-values were not significant, but using my database, by spatialreg :: impacts, the impacts are
#statistically significant and using spatialreg :: impacts. stsls, they become insignificant.

#follow my sessionInfo (), how i Said, i'm using the version of sphet by R-forge.
>
> sessionInfo ()

R version 3.6.2 (2019-12-12)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 18363)

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
 [1] sphet_1.7-6    splm_1.4-11    spdep_1.1-5    sf_0.9-5       spData_0.3.8   maptools_1.0-1 rgeos_0.5-3    raster_3.3-7   rgdal_1.5-10
[10] sp_1.4-2

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.5         bdsmatrix_1.3-4    lattice_0.20-41    deldir_0.1-25      class_7.3-15       zoo_1.8-8          gtools_3.8.2
 [8] digest_0.6.25      lmtest_0.9-37      spDataLarge_0.4.1  R6_2.4.1           coda_0.19-3        e1071_1.7-3        spam_2.5-1
[15] pillar_1.4.4       Rdpack_0.11-1      miscTools_0.6-26   rlang_0.4.7        rstudioapi_0.11    gdata_2.18.0       gmodels_2.18.1
[22] Matrix_1.2-18      splines_3.6.2      stringr_1.4.0      foreign_0.8-72     compiler_3.6.2     pkgconfig_2.0.3    maxLik_1.3-8
[29] tidyselect_1.1.0   tibble_3.0.1       expm_0.999-4       codetools_0.2-16   crayon_1.3.4       dplyr_1.0.0        MASS_7.3-51.4
[36] spatialreg_1.1-5   grid_3.6.2         nlme_3.1-142       lifecycle_0.2.0    DBI_1.1.0          magrittr_1.5       units_0.6-7
[43] ibdreg_0.2.5       bibtex_0.4.2.2     KernSmooth_2.23-16 stringi_1.4.6      plm_2.2-3          LearnBayes_2.15.1  ellipsis_0.3.1
[50] generics_0.0.2     vctrs_0.3.1        boot_1.3-23        sandwich_2.5-1     Formula_1.2-3      tools_3.6.2        glue_1.4.1
[57] purrr_0.3.4        parallel_3.6.2     classInt_0.4-3     gbRd_0.4-11        dotCall64_1.0-0

Thank you.

Pietro Andre Telatin Paschoalino
Doutorando em Ci?ncias Econ?micas da Universidade Estadual de Maring? - PCE.

[https://ipmcdn.avast.com/images/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=icon>    Virus-free. www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=link>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Aug  3 21:56:38 2020
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 3 Aug 2020 21:56:38 +0200
Subject: [R-sig-Geo] Doubt
In-Reply-To: <BN8PR17MB2980A9900EBB6DFAFAF367A5824D0@BN8PR17MB2980.namprd17.prod.outlook.com>
References: <BN8PR17MB2980A9900EBB6DFAFAF367A5824D0@BN8PR17MB2980.namprd17.prod.outlook.com>
Message-ID: <alpine.LFD.2.23.451.2008032145010.963098@reclus.nhh.no>

On Mon, 3 Aug 2020, Pietro Andre Telatin Paschoalino wrote:

> Hello everyone, follow my doubts about the sphet package, related to direct and indirect impacts, and simulation of standard errors.
>
> I will be very grateful if anyone can help me.
>
> follow my code using built-in columbus data.
>
> library(rgdal)
> library(sp)
> library(raster)
> library(rgeos)
> library(maptools)
> library(spdep)
> library (splm)
> library(sphet)
>
> data(columbus, package="spdep")
> listw <- spdep::nb2listw(col.gal.nb)
>
> attach(columbus)
>
> #i'm going direct to regression by sphet
>
> #SAC (SARAR MODEL)
>
> SAC.robust <- spreg(INC ~ HOVAL+DISCBD+CP, data=columbus, listw, model="sarar", het=TRUE)
>
> #LAG MODEL
>
> SARrobust <- spreg(INC ~ HOVAL+DISCBD+CP, data=columbus, listw, model="lag", het=TRUE)
>
> #going to my questions about the impacts function
>
> #FOR SAC MODEL
>
>> W <- as(as_dgRMatrix_listw(listw), "CsparseMatrix")
> Warning message:
>
> Function as_dgRMatrix_listw moved to the spatialreg package
>
>> trMatc <- trW(W, type="mult")
>
> Warning message:
> Function trW moved to the spatialreg package
>
>> effects<- summary(impacts(SAC.robust, tr=trMatc, R=10000), zstats=TRUE, short=TRUE)
>> effects
>

With remotes::install_github("gpiras/sphet"), you need rather:

effects<- impacts(SAC.robust, tr=trMatc, n_mvn=10000, inference=TRUE)
summary(effects)
effects <- impacts(SARrobust, tr=trMatc, n_mvn=10000, inference=TRUE)
summary(effects)

However, these are work-in-progress, and would benefit from raising an 
issue on https://github.com/gpiras/sphet/issues. The values are similar 
to:

library(spatialreg)
SARrobust1 <- stsls(INC ~ HOVAL+DISCBD+CP, data=columbus, listw,
   robust=TRUE)
ii <- impacts(SARrobust1, tr=trMatc, R=10000)
summary(ii, zstats=TRUE, short=TRUE)

Raising an issue would help, and this example should be included there.

Roger

> ========================================================
> Simulation results (GSTSLS variance matrix):
> ========================================================
>
> #FOR SAR MODEL
>>
>> effects<- summary(impacts(SARrobust, tr=trMatc, R=10000), zstats=TRUE, short=TRUE)
>
> that gives me:
>
> Error in UseMethod("impacts", obj) :
>  no applicable method for 'impacts' applied to an object of class "c('sphet', 'stsls_sphet')"
>
> #how i said the impacts is not computable.
>
> #One way to compute the impacts is by:
>
> effects<- summary(spatialreg::impacts(SARrobust, tr=trMatc, R=10000), zstats=TRUE, short=TRUE)
> effects
>
> Impact measures (lag, trace):
> ========================================================
> Simulation results (IV HAC variance matrix):
> ========================================================
>
> the simulation of standard errors and p-values is done by IV HAC. (which left me confused because I did not specify such an
> #attribute in my regression, leaving me in doubt if the standard errors of the regression had also been corrected with HAC.)
>
> #another option is:
>
> effects<- summary(spatialreg::impacts.stsls(SARrobust, tr=trMatc, R=10000), zstats=TRUE, short=TRUE)
> effects
>
> #again simulated through the HAC variance matrix.
> #Note: In this case, generally, all p-values were not significant, but using my database, by spatialreg :: impacts, the impacts are
> #statistically significant and using spatialreg :: impacts. stsls, they become insignificant.
>
> #follow my sessionInfo (), how i Said, i'm using the version of sphet by R-forge.
>>
>> sessionInfo ()
>
> R version 3.6.2 (2019-12-12)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 10 x64 (build 18363)
>
> Matrix products: default
>
> locale:
> [1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C                           LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] sphet_1.7-6    splm_1.4-11    spdep_1.1-5    sf_0.9-5       spData_0.3.8   maptools_1.0-1 rgeos_0.5-3    raster_3.3-7   rgdal_1.5-10
> [10] sp_1.4-2
>
> loaded via a namespace (and not attached):
> [1] Rcpp_1.0.5         bdsmatrix_1.3-4    lattice_0.20-41    deldir_0.1-25      class_7.3-15       zoo_1.8-8          gtools_3.8.2
> [8] digest_0.6.25      lmtest_0.9-37      spDataLarge_0.4.1  R6_2.4.1           coda_0.19-3        e1071_1.7-3        spam_2.5-1
> [15] pillar_1.4.4       Rdpack_0.11-1      miscTools_0.6-26   rlang_0.4.7        rstudioapi_0.11    gdata_2.18.0       gmodels_2.18.1
> [22] Matrix_1.2-18      splines_3.6.2      stringr_1.4.0      foreign_0.8-72     compiler_3.6.2     pkgconfig_2.0.3    maxLik_1.3-8
> [29] tidyselect_1.1.0   tibble_3.0.1       expm_0.999-4       codetools_0.2-16   crayon_1.3.4       dplyr_1.0.0        MASS_7.3-51.4
> [36] spatialreg_1.1-5   grid_3.6.2         nlme_3.1-142       lifecycle_0.2.0    DBI_1.1.0          magrittr_1.5       units_0.6-7
> [43] ibdreg_0.2.5       bibtex_0.4.2.2     KernSmooth_2.23-16 stringi_1.4.6      plm_2.2-3          LearnBayes_2.15.1  ellipsis_0.3.1
> [50] generics_0.0.2     vctrs_0.3.1        boot_1.3-23        sandwich_2.5-1     Formula_1.2-3      tools_3.6.2        glue_1.4.1
> [57] purrr_0.3.4        parallel_3.6.2     classInt_0.4-3     gbRd_0.4-11        dotCall64_1.0-0
>
> Thank you.
>
> Pietro Andre Telatin Paschoalino
> Doutorando em Ci?ncias Econ?micas da Universidade Estadual de Maring? - PCE.
>
> [https://ipmcdn.avast.com/images/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=icon>    Virus-free. www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=link>
>
> 	[[alternative HTML version deleted]]
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From pietro_tei@to m@iii@g oii hotm@ii@com  Mon Aug  3 23:56:59 2020
From: pietro_tei@to m@iii@g oii hotm@ii@com (pietro_tei@to m@iii@g oii hotm@ii@com)
Date: Mon, 03 Aug 2020 18:56:59 -0300
Subject: [R-sig-Geo] Doubt
In-Reply-To: <alpine.LFD.2.23.451.2008032145010.963098@reclus.nhh.no>
Message-ID: <BN8PR17MB298021D7A0D1FE27671B4F35824D0@BN8PR17MB2980.namprd17.prod.outlook.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20200803/7c1a5ec3/attachment.html>

From m@@@|mo@bre@@@n @end|ng |rom @rp@@veneto@|t  Thu Aug  6 11:19:29 2020
From: m@@@|mo@bre@@@n @end|ng |rom @rp@@veneto@|t (massimo bressan)
Date: Thu, 6 Aug 2020 11:19:29 +0200
Subject: [R-sig-Geo] library rgdal 1.5-15 installation failed
Message-ID: <CAFZj849HNad8WWg8ihWQYpDH1f5Ks9UzJC3u6-TtikHDd5achw@mail.gmail.com>

I've got this error by updating rgdal packages from 1.5-12 to 1.5-15

###################################

projectit.cpp:159:95: error: expected ?,? or ?...? before ?SEXP?
 SEXP transform_ng(SEXP fromargs, SEXP toargs, SEXP coordOp, SEXP npts,
SEXP x, SEXP y, SEXP z SEXP aoi) {

                   ^~~~
projectit.cpp: In function ?SEXPREC* transform_ng(SEXP, SEXP, SEXP, SEXP,
SEXP, SEXP, SEXP)?:
projectit.cpp:159:6: error: conflicting declaration of C function ?SEXPREC*
transform_ng(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP)?
 SEXP transform_ng(SEXP fromargs, SEXP toargs, SEXP coordOp, SEXP npts,
SEXP x, SEXP y, SEXP z SEXP aoi) {
      ^~~~~~~~~~~~
In file included from projectit.cpp:11:0:
rgdal.h:132:6: note: previous declaration ?SEXPREC* transform_ng(SEXP,
SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP)?
 SEXP transform_ng(SEXP fromargs, SEXP toargs, SEXP coordOp, SEXP npts,
SEXP x, SEXP y, SEXP z, SEXP aoi); // both
      ^~~~~~~~~~~~
/usr/lib/R/etc/Makeconf:177: recipe for target 'projectit.o' failed
make: *** [projectit.o] Error 1
ERROR: compilation failed for package ?rgdal?
* removing ?/home/max/R/x86_64-pc-linux-gnu-library/3.5/rgdal?
* restoring previous ?/home/max/R/x86_64-pc-linux-gnu-library/3.5/rgdal?
####################################
my system is:

Debian 9 stretch, Debian 4.9.228-1 (2020-07-05)

R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

any help for that?
thanks

	[[alternative HTML version deleted]]


From tom@heng| @end|ng |rom gm@||@com  Thu Aug  6 12:11:30 2020
From: tom@heng| @end|ng |rom gm@||@com (Tomislav Hengl)
Date: Thu, 6 Aug 2020 12:11:30 +0200
Subject: [R-sig-Geo] Postdoctoral position (OpenGeoHub): integrated proximal
 and remote sensing of soil
Message-ID: <40456100-ebba-c7ce-9caa-5f5d206d3335@gmail.com>


Application Deadline: 20th of September 2020
Possible start date: 15th of November 2020
Duration: 2+ years,
Job location: Agro Business Park 10, Wageningen, the Netherlands
Bruto monthly salary: The gross monthly salary starts at ? 2.720,- (with 
possible bonuses)
Job type: Post-doctoral researcher
Fields: pedometrics, soil spectroscopy, remote sensing, predictive 
modeling, soil organic carbon, data science
Employer: OpenGeoHub foundation

OpenGeoHub foundation is looking for a Post-doctoral researcher in the 
field of proximal and remote soil sensing, with a proven experience of 
fitting soil-spectroscopy calibration models, harmonizing and managing 
laboratory soil measurements, estimating soil organic carbon stocks 
using combination of remote sensing and soil spectroscopy, and has 
experience in programming in R and/or Python. We especially welcome 
candidates with interest in developing and implementing novel modeling 
techniques that help bridge the gaps between field-based proximal 
sensing, and satellite-based remote sensing of soil. The candidate would 
work for OpenGeoHub on current and future international projects 
including the ?Soil Spectroscopy for global good? (global data sets; 
project lead by WHRC) and the H2020 project ?AgriCapture ? Developing 
EO-powered services to promote soil carbon sequestration through 
regenerative agriculture? (EU-wide data sets). The candidate would also 
actively contribute to the www.OpenLandMap.org data portal and similar 
global datasets produced and maintained by OpenGeoHub.

"Help us make better data and better software for a better world!"

To apply for this position visit: 
https://opengeohub.org/jobs/2020/postdoctoral-position-integrated-proximal-and-remote-sensing-soil


-- 
T. (Tom) Hengl
Technical support / Vice Chair
The OpenGeoHub Foundation
Visiting address: Agro Business Park 10, 6708PW Wageningen, NL
Tel: +31 (0)317 427537
Url: https://opengeohub.org/about
Twitter: https://twitter.com/tom_hengl
https://www.youtube.com/c/OpenGeoHubFoundation
skype:tom.hengl?chat
Publications: http://scholar.google.com/citations?user=2oYU7S8AAAAJ


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Aug  6 12:17:58 2020
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 6 Aug 2020 12:17:58 +0200
Subject: [R-sig-Geo] library rgdal 1.5-15 installation failed
In-Reply-To: <CAFZj849HNad8WWg8ihWQYpDH1f5Ks9UzJC3u6-TtikHDd5achw@mail.gmail.com>
References: <CAFZj849HNad8WWg8ihWQYpDH1f5Ks9UzJC3u6-TtikHDd5achw@mail.gmail.com>
Message-ID: <alpine.LFD.2.23.451.2008061206220.1945157@reclus.nhh.no>

A known problem for users of outdated PROJ and GDAL; work-around

install.packages("rgdal", repos="http://R-Forge.R-project.org")

until next release (submission when 
https://cran.r-project.org/web/checks/check_results_rgdal.html updates to 
1.5-15 for all platforms).

Error initially trapped by the Solaris platform in CRAN package checks, 
thanks to Brian Ripley for alerting me that I had omitted to check the 
package with outdated PROJ/GDAL before submission; error not trapped by 
the CRAN submission system because those platforms use recent PROJ/GDAL.

As package maintainer, I recommend (strongly) staying as close to current 
system requirements as possible, because those version fix often serious 
bugs and offer much enhanced processing. In the case of PROJ/GDAL, from 
PROJ >= 6 and GDAL >= 3, the data models and APIs change radically, and we 
cannot be sure that we can test for outdated versions (this happened in 
this case because of a rushed release to fix a different problem).

I believe that things should still avoid failing, but will be more and 
more outdated. The gate for outdated PROJ/GDAL has to be kept open for 
really bad choices (legacy OS versions without available builds of more 
recent versions of GDAL and PROJ). However, as in this case, it may not be 
possible to test for dozens of external system requirements combinations, 
including reverse dependency checks for almost 900 packages using rgdal 
and rgdal through raster. Revdeps run yesterday with the development 
version of rgdal and PROJ 5.2.0 / GDAL 2.2.4 show no remaining problems 
caused by rgdal.

Users are strongly encouraged to move to as recent PROJ and GDAL releases 
as possible; CRAN Windows binaries (and shortly OSX binaries) are fairly 
recent. A maintainer's life is rarely a happy one ...

Roger


On Thu, 6 Aug 2020, massimo bressan wrote:

> I've got this error by updating rgdal packages from 1.5-12 to 1.5-15
>
> ###################################
>
> projectit.cpp:159:95: error: expected ?,? or ?...? before ?SEXP?
> SEXP transform_ng(SEXP fromargs, SEXP toargs, SEXP coordOp, SEXP npts,
> SEXP x, SEXP y, SEXP z SEXP aoi) {
>
>                   ^~~~
> projectit.cpp: In function ?SEXPREC* transform_ng(SEXP, SEXP, SEXP, SEXP,
> SEXP, SEXP, SEXP)?:
> projectit.cpp:159:6: error: conflicting declaration of C function ?SEXPREC*
> transform_ng(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP)?
> SEXP transform_ng(SEXP fromargs, SEXP toargs, SEXP coordOp, SEXP npts,
> SEXP x, SEXP y, SEXP z SEXP aoi) {
>      ^~~~~~~~~~~~
> In file included from projectit.cpp:11:0:
> rgdal.h:132:6: note: previous declaration ?SEXPREC* transform_ng(SEXP,
> SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP)?
> SEXP transform_ng(SEXP fromargs, SEXP toargs, SEXP coordOp, SEXP npts,
> SEXP x, SEXP y, SEXP z, SEXP aoi); // both
>      ^~~~~~~~~~~~
> /usr/lib/R/etc/Makeconf:177: recipe for target 'projectit.o' failed
> make: *** [projectit.o] Error 1
> ERROR: compilation failed for package ?rgdal?
> * removing ?/home/max/R/x86_64-pc-linux-gnu-library/3.5/rgdal?
> * restoring previous ?/home/max/R/x86_64-pc-linux-gnu-library/3.5/rgdal?
> ####################################
> my system is:
>
> Debian 9 stretch, Debian 4.9.228-1 (2020-07-05)
>
> R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
> Copyright (C) 2020 The R Foundation for Statistical Computing
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> any help for that?
> thanks
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From te@3rd @end|ng |rom gm@||@com  Mon Aug 10 22:01:33 2020
From: te@3rd @end|ng |rom gm@||@com (Thomas Adams)
Date: Mon, 10 Aug 2020 16:01:33 -0400
Subject: [R-sig-Geo] How to change scale labeling in spplot
Message-ID: <CAGxgkWifnuNxsW+gJPQ4efEb6eWwwOFwfu5g15nUM0JQzk9jNw@mail.gmail.com>

Hi all,

I have searched for examples to change the labeling for raster data (two
panels) consisting of integer values (1, 2, and 3). In the scale I want to
label these as "Af", "Am", Aw".

My code looks like:

colp <- colorRampPalette(c("#960000", "#FF0000", "#FFCCCC"))
brks<-seq(0,3,1)
spplot(KG_haiti,xlim=c(-8313027,-7959668),ylim=c(2017552,2297053),at=brks,col.regions=colp(length(brks)-1),ylab="Climate
Class",main="Koppen-Geiger Climate Classification for\nClimate Years
1981-2010 and El Nino Years",names.attr=c("Climate Normal","El
Nino"),colorkey=list(labels = list(labels = c("Af","Am","Aw"))))

I'm getting just the 3 colors, but the labels "Af", "Am", Aw" are repeated
across each color; I need to have each color associated with just one of
the labels. Obviously, I'm doing something wrong...

I just cannot figure out how to do this.

Regards,
Tom

	[[alternative HTML version deleted]]


From t|m@@ppe|h@n@ @end|ng |rom gm@||@com  Mon Aug 10 22:34:36 2020
From: t|m@@ppe|h@n@ @end|ng |rom gm@||@com (Tim Salabim)
Date: Mon, 10 Aug 2020 22:34:36 +0200
Subject: [R-sig-Geo] How to change scale labeling in spplot
In-Reply-To: <CAGxgkWifnuNxsW+gJPQ4efEb6eWwwOFwfu5g15nUM0JQzk9jNw@mail.gmail.com>
References: <CAGxgkWifnuNxsW+gJPQ4efEb6eWwwOFwfu5g15nUM0JQzk9jNw@mail.gmail.com>
Message-ID: <CAMbH+HjGGaHArdm5jvmM0YNhEYctsvDTrPNV=23mmvyAYY0GxQ@mail.gmail.com>

Have you tried
colorkey=list(labels = list(labels = list("Af","Am","Aw")))
?



On Mon, Aug 10, 2020, 22:01 Thomas Adams <tea3rd at gmail.com> wrote:

> Hi all,
>
> I have searched for examples to change the labeling for raster data (two
> panels) consisting of integer values (1, 2, and 3). In the scale I want to
> label these as "Af", "Am", Aw".
>
> My code looks like:
>
> colp <- colorRampPalette(c("#960000", "#FF0000", "#FFCCCC"))
> brks<-seq(0,3,1)
>
> spplot(KG_haiti,xlim=c(-8313027,-7959668),ylim=c(2017552,2297053),at=brks,col.regions=colp(length(brks)-1),ylab="Climate
> Class",main="Koppen-Geiger Climate Classification for\nClimate Years
> 1981-2010 and El Nino Years",names.attr=c("Climate Normal","El
> Nino"),colorkey=list(labels = list(labels = c("Af","Am","Aw"))))
>
> I'm getting just the 3 colors, but the labels "Af", "Am", Aw" are repeated
> across each color; I need to have each color associated with just one of
> the labels. Obviously, I'm doing something wrong...
>
> I just cannot figure out how to do this.
>
> Regards,
> Tom
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From te@3rd @end|ng |rom gm@||@com  Mon Aug 10 23:05:35 2020
From: te@3rd @end|ng |rom gm@||@com (Thomas Adams)
Date: Mon, 10 Aug 2020 17:05:35 -0400
Subject: [R-sig-Geo] How to change scale labeling in spplot
In-Reply-To: <CAMbH+HjGGaHArdm5jvmM0YNhEYctsvDTrPNV=23mmvyAYY0GxQ@mail.gmail.com>
References: <CAGxgkWifnuNxsW+gJPQ4efEb6eWwwOFwfu5g15nUM0JQzk9jNw@mail.gmail.com>
 <CAMbH+HjGGaHArdm5jvmM0YNhEYctsvDTrPNV=23mmvyAYY0GxQ@mail.gmail.com>
Message-ID: <CAGxgkWjPvLh2ZznxbYs916P7iY364KeNBQPo7v99BZz=JGTZkg@mail.gmail.com>

Hi Tim,

Yes, basically, I have and just did again, try this -- same result...

Tom

On Mon, Aug 10, 2020 at 4:34 PM Tim Salabim <tim.appelhans at gmail.com> wrote:

> Have you tried
> colorkey=list(labels = list(labels = list("Af","Am","Aw")))
> ?
>
>
>
> On Mon, Aug 10, 2020, 22:01 Thomas Adams <tea3rd at gmail.com> wrote:
>
>> Hi all,
>>
>> I have searched for examples to change the labeling for raster data (two
>> panels) consisting of integer values (1, 2, and 3). In the scale I want to
>> label these as "Af", "Am", Aw".
>>
>> My code looks like:
>>
>> colp <- colorRampPalette(c("#960000", "#FF0000", "#FFCCCC"))
>> brks<-seq(0,3,1)
>>
>> spplot(KG_haiti,xlim=c(-8313027,-7959668),ylim=c(2017552,2297053),at=brks,col.regions=colp(length(brks)-1),ylab="Climate
>> Class",main="Koppen-Geiger Climate Classification for\nClimate Years
>> 1981-2010 and El Nino Years",names.attr=c("Climate Normal","El
>> Nino"),colorkey=list(labels = list(labels = c("Af","Am","Aw"))))
>>
>> I'm getting just the 3 colors, but the labels "Af", "Am", Aw" are repeated
>> across each color; I need to have each color associated with just one of
>> the labels. Obviously, I'm doing something wrong...
>>
>> I just cannot figure out how to do this.
>>
>> Regards,
>> Tom
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>

-- 
Thomas E Adams, III
1724 Sage Lane
Blacksburg, VA 24060
tea3rd at gmail.com (personal)
tea at terrapredictions.org (work)

1 (513) 739-9512 (cell)

	[[alternative HTML version deleted]]


From me| @end|ng |rom mb@cou@com  Tue Aug 11 00:14:36 2020
From: me| @end|ng |rom mb@cou@com (Bacou, Melanie)
Date: Mon, 10 Aug 2020 23:14:36 +0100
Subject: [R-sig-Geo] How to change scale labeling in spplot
In-Reply-To: <CAGxgkWjPvLh2ZznxbYs916P7iY364KeNBQPo7v99BZz=JGTZkg@mail.gmail.com>
References: <CAGxgkWifnuNxsW+gJPQ4efEb6eWwwOFwfu5g15nUM0JQzk9jNw@mail.gmail.com>
 <CAMbH+HjGGaHArdm5jvmM0YNhEYctsvDTrPNV=23mmvyAYY0GxQ@mail.gmail.com>
 <CAGxgkWjPvLh2ZznxbYs916P7iY364KeNBQPo7v99BZz=JGTZkg@mail.gmail.com>
Message-ID: <d9570088-fc2b-2781-e6ce-880ad7d2e12c@mbacou.com>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20200810/60068873/attachment.html>

From cch|@en| @end|ng |rom gm@||@com  Tue Aug 11 19:20:23 2020
From: cch|@en| @end|ng |rom gm@||@com (Chanda Chiseni)
Date: Tue, 11 Aug 2020 19:20:23 +0200
Subject: [R-sig-Geo] Moran's I with Objects of Different Length
Message-ID: <CAA5N_eai9wEB-Lf0ik4xfsHbwq6kZKU4tUCBy8mENC5iS3fqwg@mail.gmail.com>

I am trying to perform moran's I test on residuals from by probit model
using k-nearest neighbour weights, however i run into an error which i
cant seem to find the solution anywhere online the error is

> moran.test(residuals.glm(svyprobitest),knear2weight)
Error in moran.test(residuals.glm(svyprobitest), knear2weight) :
  objects of different length


The codes I use to come up to this are below

> library(foreign)
> dhsanalysis2= read.dta("DHSSpatialreg11.dta")
##removing NAs in PSU
> dhsanalysis3=subset(dhsanalysis2, !is.na(psu))
##generating a survey design
> mydesighdhs= svydesign(ids =~psu, data =dhsanalysis3, weight = ~wgt,
strata = ~v023,nest=TRUE)
##Defining coordinate colums
> coordinates(dhsanalysis3)= c("longnum","latnum")
#Defining Projection
> proj4string(dhsanalysis3) <- CRS("+init=epsg:4326")
##binding longiude and latitude
> lon2<- dhsanalysis3$longnum
> lat2<- dhsanalysis3$latnum
> coords<- cbind(lon2,lat2)
##Creating spatial weights based on the nearest neighbour
> knear2= knearneigh(coords,k=2,longlat=T)
Warning message:
In knearneigh(coords, k = 2, longlat = T) :
  knearneigh: identical points found ### I get a warning message on
identical points found, is this a problem and how would i deal with this
> knear2.nb= knn2nb(knear2)
> knear2weight= nb2listw(knear2.nb, style="W",zero.policy=T)

##declaring categorical variables as factor variables
> dhsanalysis3$hivpositive.f <- factor(dhsanalysis3$hivpositive)
> dhsanalysis3$protestant.f <- factor(dhsanalysis3$protestant2)
> dhsanalysis3$Married.f <- factor(dhsanalysis3$Married)
> dhsanalysis3$female.f <- factor(dhsanalysis3$female)
> dhsanalysis3$urban1.f <- factor(dhsanalysis3$urban1)
> dhsanalysis3$river10kmdum.f <- factor(dhsanalysis3$river10kmdum)
> dhsanalysis3$Explorer50kmdum.f <- factor(dhsanalysis3$Explorer50kmdum)
> dhsanalysis3$Rail50kmdum.f <- factor(dhsanalysis3$Rail50kmdum)
> dhsanalysis3$Province1.f <- factor(dhsanalysis3$Province1)
> dhsanalysis3$WealthIndex.f <- factor(dhsanalysis3$WealthIndex)
> dhsanalysis3$occupation2.f <- factor(dhsanalysis3$occupation2)
> dhsanalysis3$highested.f <- factor(dhsanalysis3$highested)
> protestant.f= dhsanalysis3$protestant.f
> hivpositive.f=dhsanalysi3$hivpositive.f
> Married.f=dhsanalysis3$Married.f
> female.f=dhsanalysis3$female.f
> urban1.f=dhsanalysis3$urban1.f
> river10kmdum.f=dhsanalysis3$river10kmdum.f
> Explorer50kmdum.f=dhsanalysis2$Explorer50kmdum.f
> Province1.f=dhsanalysis3$Province1.f
> WealthIndex.f=dhsanalysis3$WealthIndex.f
> occupation2.f=dhsanalysis3$occupation2.f
> highested.f=dhsanalysis3$highested.f
> Age=dhsanalysis3$Age
> Age2=dhsanalysis3$Age2
> HIVKnowledge=dhsanalysis3$HIVKnowledge
> churchkm=dhsanalysis3$churchkm
> lnhospitalkm=dhsanalysis3$lnhospitalkm
> lnElevationMean=dhsanalysis3$lnElevationMean

> ###VARIABLES IN SURVEY DESIGN
> mydesighdhs$hivpositive.f <- factor(dhsanalysis3$hivpositive)
> mydesighdhs$protestant.f <- factor(dhsanalysis3$protestant2)
> mydesighdhs$Married.f <- factor(dhsanalysis3$Married)
> mydesighdhs$female.f <- factor(dhsanalysis3$female)
> mydesighdhs$urban1.f <- factor(dhsanalysis3$urban1)
> mydesighdhs$river10kmdum.f <- factor(dhsanalysis3$river10kmdum)
> mydesighdhs$Explorer50kmdum.f <- factor(dhsanalysis3$Explorer50kmdum)
> mydesighdhs$Rail50kmdum.f <- factor(dhsanalysis3$Rail50kmdum)
> mydesighdhs$Province1.f <- factor(dhsanalysis3$Province1)
> mydesighdhs$WealthIndex.f <- factor(dhsanalysis3$WealthIndex)
> mydesighdhs$occupation2.f <- factor(dhsanalysis3$occupation2)
> mydesighdhs$highested.f <- factor(dhsanalysis3$highested)
> mydesighdhs$Age=Age
> mydesighdhs$Age2=Age2
> mydesighdhs$HIVKnowledge=HIVKnowledge
> mydesighdhs$churchkm=churchkm
> mydesighdhs$lnhospitalkm=lnhospitalkm
> mydesighdhs$lnElevationMean=lnElevationMean

> svyprobitest= svyglm(hivpositive.f~ churchkm +lnhospitalkm +protestant.f+
Age+
Age2+Married.f+female.f+urban1.f+river10kmdum.f+Explorer50kmdum.f+Rail50kmdum.f+lnElevationMean+Province1.f
+ WealthIndex.f + HIVKnowledge+ occupation2.f+
highested.f,design=mydesighdhs,family = quasibinomial(link =
"probit"),data=dhsanalysis3)
Error in .subset2(x, i, exact = exact) : subscript out of bounds

## Iran my probit model without implementing the survey design in the model
just to see whether Moran test is working
> svyprobitest2= glm(hivpositive.f~ churchkm +lnhospitalkm +protestant.f+
Age+
Age2+Married.f+female.f+urban1.f+river10kmdum.f+Explorer50kmdum.f+Rail50kmdum.f+lnElevationMean+Province1.f
+ WealthIndex.f + HIVKnowledge+ occupation2.f+ highested.f,family =
quasibinomial(link = "probit"),data=dhsanalysis3)
#Moran test
> moran.test(residuals.glm(svyprobitest),knear2weight)
Error in moran.test(residuals.glm(svyprobitest), knear2weight) :
  objects of different length


Kind Regards,

Michael Chanda Chiseni

Phd Candidate

Department of Economic History

Lund University

Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund



*Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *

	[[alternative HTML version deleted]]


From |v|@|e|ro @end|ng |rom gm@||@com  Tue Aug 11 22:43:13 2020
From: |v|@|e|ro @end|ng |rom gm@||@com (Frederico Faleiro)
Date: Tue, 11 Aug 2020 17:43:13 -0300
Subject: [R-sig-Geo] Mask from raster with less RAM
Message-ID: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>

Dear all,

I would like to generate a mask from a raster, but my workflow needs a lot
of RAM to process big rasters. I need this polygon mask to use in another's
rasters.
Do you know another approach that needs less RAM?

# reproducible example
library(raster)
# read data to create mask
r <- raster(system.file("external/test.grd", package="raster"))
r[!is.na(r)] <- 1
pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the data
# apply the mask in another raster
 r2 <- raster(extent(r), res(r))
r2[ ] <- 1
r2.mask <- mask(r2, pol)

Cheers!

-- 
Frederico Faleiro
Postdoctoral Researcher in the INCT-EECBio (https://www.eecbio.ufg.br/)
Department of Ecology | Institute of Biological Science | Federal
University of Goi?s | Brazil
RG: https://www.researchgate.net/profile/Frederico_Faleiro
CV: http://lattes.cnpq.br/4926404840659003

	[[alternative HTML version deleted]]


From @tephen@@tew@rt85 @end|ng |rom gm@||@com  Wed Aug 12 00:25:46 2020
From: @tephen@@tew@rt85 @end|ng |rom gm@||@com (Stephen Stewart)
Date: Wed, 12 Aug 2020 08:25:46 +1000
Subject: [R-sig-Geo] Mask from raster with less RAM
In-Reply-To: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
References: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
Message-ID: <CAJefdj6YEOJQfpx28U_wKVXx7hVsm7CbyREXGVUqsuYpjUNKAw@mail.gmail.com>

Hi Frederico,

It may not solve all of your RAM issues, but in this situation I would skip
the rasterToPolygons (which is also usually very slow) and use raster math
to propagate NAs.

r <- raster(system.file("external/test.grd", package="raster"))
r[!is.na(r)] <- 1
# Can also be faster to do r = r / r, but add an offset (that cannot result
in 0) if you have valid 0s.
 r2 <- raster(extent(r), res(r))
r2[ ] <- 1
r2.mask <- r * r2

If you have a polygon to use as a mask, burn it in using the fasterize
package and then apply the above.

Hope that helps.

Cheers,

Steve

On Wed., 12 Aug. 2020, 6:43 am Frederico Faleiro, <fvfaleiro at gmail.com>
wrote:

> Dear all,
>
> I would like to generate a mask from a raster, but my workflow needs a lot
> of RAM to process big rasters. I need this polygon mask to use in another's
> rasters.
> Do you know another approach that needs less RAM?
>
> # reproducible example
> library(raster)
> # read data to create mask
> r <- raster(system.file("external/test.grd", package="raster"))
> r[!is.na(r)] <- 1
> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
> data
> # apply the mask in another raster
>  r2 <- raster(extent(r), res(r))
> r2[ ] <- 1
> r2.mask <- mask(r2, pol)
>
> Cheers!
>
> --
> Frederico Faleiro
> Postdoctoral Researcher in the INCT-EECBio (https://www.eecbio.ufg.br/)
> Department of Ecology | Institute of Biological Science | Federal
> University of Goi?s | Brazil
> RG: https://www.researchgate.net/profile/Frederico_Faleiro
> CV: http://lattes.cnpq.br/4926404840659003
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Aug 12 12:55:21 2020
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 12 Aug 2020 12:55:21 +0200
Subject: [R-sig-Geo] Moran's I with Objects of Different Length
In-Reply-To: <CAA5N_eai9wEB-Lf0ik4xfsHbwq6kZKU4tUCBy8mENC5iS3fqwg@mail.gmail.com>
References: <CAA5N_eai9wEB-Lf0ik4xfsHbwq6kZKU4tUCBy8mENC5iS3fqwg@mail.gmail.com>
Message-ID: <alpine.LFD.2.23.451.2008121244250.2887369@reclus.nhh.no>

On Tue, 11 Aug 2020, Chanda Chiseni wrote:

> I am trying to perform moran's I test on residuals from by probit model
> using k-nearest neighbour weights, however i run into an error which i
> cant seem to find the solution anywhere online the error is
>
>> moran.test(residuals.glm(svyprobitest),knear2weight)
> Error in moran.test(residuals.glm(svyprobitest), knear2weight) :
>  objects of different length
>
>
> The codes I use to come up to this are below
>
>> library(foreign)
>> dhsanalysis2= read.dta("DHSSpatialreg11.dta")
> ##removing NAs in PSU
>> dhsanalysis3=subset(dhsanalysis2, !is.na(psu))
> ##generating a survey design
>> mydesighdhs= svydesign(ids =~psu, data =dhsanalysis3, weight = ~wgt,
> strata = ~v023,nest=TRUE)
> ##Defining coordinate colums
>> coordinates(dhsanalysis3)= c("longnum","latnum")
> #Defining Projection
>> proj4string(dhsanalysis3) <- CRS("+init=epsg:4326")
> ##binding longiude and latitude
>> lon2<- dhsanalysis3$longnum
>> lat2<- dhsanalysis3$latnum
>> coords<- cbind(lon2,lat2)
> ##Creating spatial weights based on the nearest neighbour
>> knear2= knearneigh(coords,k=2,longlat=T)
> Warning message:
> In knearneigh(coords, k = 2, longlat = T) :
>  knearneigh: identical points found ### I get a warning message on
> identical points found, is this a problem and how would i deal with this

It usually indicates muddled thinking. You have more than?one observation 
at the same point, which could be an unintended duplicate (copy of the 
same observation), or it could indicate that any spatial process has 
multiple values at that point. In geostatistics, one would jitter the 
points to introduce distance. In your case, this probably isn't households 
living at one address point. All the observations at the same point will 
get the same sets of neighbours. If there are many co-located 
observations, k in k-nearest may be too small to include them all.

>> knear2.nb= knn2nb(knear2)
>> knear2weight= nb2listw(knear2.nb, style="W",zero.policy=T)
>
> ##declaring categorical variables as factor variables
>> dhsanalysis3$hivpositive.f <- factor(dhsanalysis3$hivpositive)
>> dhsanalysis3$protestant.f <- factor(dhsanalysis3$protestant2)
>> dhsanalysis3$Married.f <- factor(dhsanalysis3$Married)
>> dhsanalysis3$female.f <- factor(dhsanalysis3$female)
>> dhsanalysis3$urban1.f <- factor(dhsanalysis3$urban1)
>> dhsanalysis3$river10kmdum.f <- factor(dhsanalysis3$river10kmdum)
>> dhsanalysis3$Explorer50kmdum.f <- factor(dhsanalysis3$Explorer50kmdum)
>> dhsanalysis3$Rail50kmdum.f <- factor(dhsanalysis3$Rail50kmdum)
>> dhsanalysis3$Province1.f <- factor(dhsanalysis3$Province1)
>> dhsanalysis3$WealthIndex.f <- factor(dhsanalysis3$WealthIndex)
>> dhsanalysis3$occupation2.f <- factor(dhsanalysis3$occupation2)
>> dhsanalysis3$highested.f <- factor(dhsanalysis3$highested)
>> protestant.f= dhsanalysis3$protestant.f
>> hivpositive.f=dhsanalysi3$hivpositive.f
>> Married.f=dhsanalysis3$Married.f
>> female.f=dhsanalysis3$female.f
>> urban1.f=dhsanalysis3$urban1.f
>> river10kmdum.f=dhsanalysis3$river10kmdum.f
>> Explorer50kmdum.f=dhsanalysis2$Explorer50kmdum.f
>> Province1.f=dhsanalysis3$Province1.f
>> WealthIndex.f=dhsanalysis3$WealthIndex.f
>> occupation2.f=dhsanalysis3$occupation2.f
>> highested.f=dhsanalysis3$highested.f
>> Age=dhsanalysis3$Age
>> Age2=dhsanalysis3$Age2
>> HIVKnowledge=dhsanalysis3$HIVKnowledge
>> churchkm=dhsanalysis3$churchkm
>> lnhospitalkm=dhsanalysis3$lnhospitalkm
>> lnElevationMean=dhsanalysis3$lnElevationMean
>
>> ###VARIABLES IN SURVEY DESIGN
>> mydesighdhs$hivpositive.f <- factor(dhsanalysis3$hivpositive)
>> mydesighdhs$protestant.f <- factor(dhsanalysis3$protestant2)
>> mydesighdhs$Married.f <- factor(dhsanalysis3$Married)
>> mydesighdhs$female.f <- factor(dhsanalysis3$female)
>> mydesighdhs$urban1.f <- factor(dhsanalysis3$urban1)
>> mydesighdhs$river10kmdum.f <- factor(dhsanalysis3$river10kmdum)
>> mydesighdhs$Explorer50kmdum.f <- factor(dhsanalysis3$Explorer50kmdum)
>> mydesighdhs$Rail50kmdum.f <- factor(dhsanalysis3$Rail50kmdum)
>> mydesighdhs$Province1.f <- factor(dhsanalysis3$Province1)
>> mydesighdhs$WealthIndex.f <- factor(dhsanalysis3$WealthIndex)
>> mydesighdhs$occupation2.f <- factor(dhsanalysis3$occupation2)
>> mydesighdhs$highested.f <- factor(dhsanalysis3$highested)
>> mydesighdhs$Age=Age
>> mydesighdhs$Age2=Age2
>> mydesighdhs$HIVKnowledge=HIVKnowledge
>> mydesighdhs$churchkm=churchkm
>> mydesighdhs$lnhospitalkm=lnhospitalkm
>> mydesighdhs$lnElevationMean=lnElevationMean
>
>> svyprobitest= svyglm(hivpositive.f~ churchkm +lnhospitalkm +protestant.f+
> Age+
> Age2+Married.f+female.f+urban1.f+river10kmdum.f+Explorer50kmdum.f+Rail50kmdum.f+lnElevationMean+Province1.f
> + WealthIndex.f + HIVKnowledge+ occupation2.f+
> highested.f,design=mydesighdhs,family = quasibinomial(link =
> "probit"),data=dhsanalysis3)
> Error in .subset2(x, i, exact = exact) : subscript out of bounds

So you've an error anyway.

>
> ## Iran my probit model without implementing the survey design in the model
> just to see whether Moran test is working
>> svyprobitest2= glm(hivpositive.f~ churchkm +lnhospitalkm +protestant.f+
> Age+
> Age2+Married.f+female.f+urban1.f+river10kmdum.f+Explorer50kmdum.f+Rail50kmdum.f+lnElevationMean+Province1.f
> + WealthIndex.f + HIVKnowledge+ occupation2.f+ highested.f,family =
> quasibinomial(link = "probit"),data=dhsanalysis3)
> #Moran test
>> moran.test(residuals.glm(svyprobitest),knear2weight)
> Error in moran.test(residuals.glm(svyprobitest), knear2weight) :
>  objects of different length
>

Testing residuals using moran.test() is usually wrong, as the expectation 
and variance of the statistic are based on a null model (intercept only), 
not a model with covariates.

What are length(residuals.glm(svyprobitest)) and length(knear2.nb)? Did 
glm drop observations with missing values? If so, you should subset both 
the data submitted to glm() and the neighbour object so that they match.

Hope this helps,

Roger

>
> Kind Regards,
>
> Michael Chanda Chiseni
>
> Phd Candidate
>
> Department of Economic History
>
> Lund University
>
> Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund
>
>
>
> *Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From cch|@en| @end|ng |rom gm@||@com  Wed Aug 12 14:30:46 2020
From: cch|@en| @end|ng |rom gm@||@com (Chanda Chiseni)
Date: Wed, 12 Aug 2020 14:30:46 +0200
Subject: [R-sig-Geo] Moran's I with Objects of Different Length
In-Reply-To: <alpine.LFD.2.23.451.2008121244250.2887369@reclus.nhh.no>
References: <CAA5N_eai9wEB-Lf0ik4xfsHbwq6kZKU4tUCBy8mENC5iS3fqwg@mail.gmail.com>
 <alpine.LFD.2.23.451.2008121244250.2887369@reclus.nhh.no>
Message-ID: <CAA5N_eZm9+JRFw2gtHE=kyAEhLND6EH4SJHe_0ALLNdXTqsFoQ@mail.gmail.com>

Hi Roger

Thank you for your response.

I am using DHS individual level data , they cluster individuals into about
25-30 people and assign them coordinates depending on where they were
surveyed. So about 30 individuals have similar coordinates. Yes I do have
more than one observation at one point as you have stated, this could be
the source of my problem indeed, My question is how could i go about
creating nearest neighbors and weights by clustering individuals that share
the same point? I sought for a solution online but could not find any.
Sorry I am a novice at spatial econometrics, this may be obvious to some ,
so instead of testing for spatial correlation in my residuals, I should in
essence test for spatial autocorrelation in my outcome variable, for
instance in my analysis I am using a dummy variable of whether an
individual is hiv positive or negative. And is there a specific way of
using a moran.test() on a dummy variable?


Kind Regards,

Michael Chanda Chiseni

Phd Candidate

Department of Economic History

Lund University

Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund



*Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *






On Wed, Aug 12, 2020 at 12:55 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Tue, 11 Aug 2020, Chanda Chiseni wrote:
>
> > I am trying to perform moran's I test on residuals from by probit model
> > using k-nearest neighbour weights, however i run into an error which i
> > cant seem to find the solution anywhere online the error is
> >
> >> moran.test(residuals.glm(svyprobitest),knear2weight)
> > Error in moran.test(residuals.glm(svyprobitest), knear2weight) :
> >  objects of different length
> >
> >
> > The codes I use to come up to this are below
> >
> >> library(foreign)
> >> dhsanalysis2= read.dta("DHSSpatialreg11.dta")
> > ##removing NAs in PSU
> >> dhsanalysis3=subset(dhsanalysis2, !is.na(psu))
> > ##generating a survey design
> >> mydesighdhs= svydesign(ids =~psu, data =dhsanalysis3, weight = ~wgt,
> > strata = ~v023,nest=TRUE)
> > ##Defining coordinate colums
> >> coordinates(dhsanalysis3)= c("longnum","latnum")
> > #Defining Projection
> >> proj4string(dhsanalysis3) <- CRS("+init=epsg:4326")
> > ##binding longiude and latitude
> >> lon2<- dhsanalysis3$longnum
> >> lat2<- dhsanalysis3$latnum
> >> coords<- cbind(lon2,lat2)
> > ##Creating spatial weights based on the nearest neighbour
> >> knear2= knearneigh(coords,k=2,longlat=T)
> > Warning message:
> > In knearneigh(coords, k = 2, longlat = T) :
> >  knearneigh: identical points found ### I get a warning message on
> > identical points found, is this a problem and how would i deal with this
>
> It usually indicates muddled thinking. You have more than one observation
> at the same point, which could be an unintended duplicate (copy of the
> same observation), or it could indicate that any spatial process has
> multiple values at that point. In geostatistics, one would jitter the
> points to introduce distance. In your case, this probably isn't households
> living at one address point. All the observations at the same point will
> get the same sets of neighbours. If there are many co-located
> observations, k in k-nearest may be too small to include them all.
>
> >> knear2.nb= knn2nb(knear2)
> >> knear2weight= nb2listw(knear2.nb, style="W",zero.policy=T)
> >
> > ##declaring categorical variables as factor variables
> >> dhsanalysis3$hivpositive.f <- factor(dhsanalysis3$hivpositive)
> >> dhsanalysis3$protestant.f <- factor(dhsanalysis3$protestant2)
> >> dhsanalysis3$Married.f <- factor(dhsanalysis3$Married)
> >> dhsanalysis3$female.f <- factor(dhsanalysis3$female)
> >> dhsanalysis3$urban1.f <- factor(dhsanalysis3$urban1)
> >> dhsanalysis3$river10kmdum.f <- factor(dhsanalysis3$river10kmdum)
> >> dhsanalysis3$Explorer50kmdum.f <- factor(dhsanalysis3$Explorer50kmdum)
> >> dhsanalysis3$Rail50kmdum.f <- factor(dhsanalysis3$Rail50kmdum)
> >> dhsanalysis3$Province1.f <- factor(dhsanalysis3$Province1)
> >> dhsanalysis3$WealthIndex.f <- factor(dhsanalysis3$WealthIndex)
> >> dhsanalysis3$occupation2.f <- factor(dhsanalysis3$occupation2)
> >> dhsanalysis3$highested.f <- factor(dhsanalysis3$highested)
> >> protestant.f= dhsanalysis3$protestant.f
> >> hivpositive.f=dhsanalysi3$hivpositive.f
> >> Married.f=dhsanalysis3$Married.f
> >> female.f=dhsanalysis3$female.f
> >> urban1.f=dhsanalysis3$urban1.f
> >> river10kmdum.f=dhsanalysis3$river10kmdum.f
> >> Explorer50kmdum.f=dhsanalysis2$Explorer50kmdum.f
> >> Province1.f=dhsanalysis3$Province1.f
> >> WealthIndex.f=dhsanalysis3$WealthIndex.f
> >> occupation2.f=dhsanalysis3$occupation2.f
> >> highested.f=dhsanalysis3$highested.f
> >> Age=dhsanalysis3$Age
> >> Age2=dhsanalysis3$Age2
> >> HIVKnowledge=dhsanalysis3$HIVKnowledge
> >> churchkm=dhsanalysis3$churchkm
> >> lnhospitalkm=dhsanalysis3$lnhospitalkm
> >> lnElevationMean=dhsanalysis3$lnElevationMean
> >
> >> ###VARIABLES IN SURVEY DESIGN
> >> mydesighdhs$hivpositive.f <- factor(dhsanalysis3$hivpositive)
> >> mydesighdhs$protestant.f <- factor(dhsanalysis3$protestant2)
> >> mydesighdhs$Married.f <- factor(dhsanalysis3$Married)
> >> mydesighdhs$female.f <- factor(dhsanalysis3$female)
> >> mydesighdhs$urban1.f <- factor(dhsanalysis3$urban1)
> >> mydesighdhs$river10kmdum.f <- factor(dhsanalysis3$river10kmdum)
> >> mydesighdhs$Explorer50kmdum.f <- factor(dhsanalysis3$Explorer50kmdum)
> >> mydesighdhs$Rail50kmdum.f <- factor(dhsanalysis3$Rail50kmdum)
> >> mydesighdhs$Province1.f <- factor(dhsanalysis3$Province1)
> >> mydesighdhs$WealthIndex.f <- factor(dhsanalysis3$WealthIndex)
> >> mydesighdhs$occupation2.f <- factor(dhsanalysis3$occupation2)
> >> mydesighdhs$highested.f <- factor(dhsanalysis3$highested)
> >> mydesighdhs$Age=Age
> >> mydesighdhs$Age2=Age2
> >> mydesighdhs$HIVKnowledge=HIVKnowledge
> >> mydesighdhs$churchkm=churchkm
> >> mydesighdhs$lnhospitalkm=lnhospitalkm
> >> mydesighdhs$lnElevationMean=lnElevationMean
> >
> >> svyprobitest= svyglm(hivpositive.f~ churchkm +lnhospitalkm
> +protestant.f+
> > Age+
> >
> Age2+Married.f+female.f+urban1.f+river10kmdum.f+Explorer50kmdum.f+Rail50kmdum.f+lnElevationMean+Province1.f
> > + WealthIndex.f + HIVKnowledge+ occupation2.f+
> > highested.f,design=mydesighdhs,family = quasibinomial(link =
> > "probit"),data=dhsanalysis3)
> > Error in .subset2(x, i, exact = exact) : subscript out of bounds
>
> So you've an error anyway.
>
> >
> > ## Iran my probit model without implementing the survey design in the
> model
> > just to see whether Moran test is working
> >> svyprobitest2= glm(hivpositive.f~ churchkm +lnhospitalkm +protestant.f+
> > Age+
> >
> Age2+Married.f+female.f+urban1.f+river10kmdum.f+Explorer50kmdum.f+Rail50kmdum.f+lnElevationMean+Province1.f
> > + WealthIndex.f + HIVKnowledge+ occupation2.f+ highested.f,family =
> > quasibinomial(link = "probit"),data=dhsanalysis3)
> > #Moran test
> >> moran.test(residuals.glm(svyprobitest),knear2weight)
> > Error in moran.test(residuals.glm(svyprobitest), knear2weight) :
> >  objects of different length
> >
>
> Testing residuals using moran.test() is usually wrong, as the expectation
> and variance of the statistic are based on a null model (intercept only),
> not a model with covariates.
>
> What are length(residuals.glm(svyprobitest)) and length(knear2.nb)? Did
> glm drop observations with missing values? If so, you should subset both
> the data submitted to glm() and the neighbour object so that they match.
>
> Hope this helps,
>
> Roger
>
> >
> > Kind Regards,
> >
> > Michael Chanda Chiseni
> >
> > Phd Candidate
> >
> > Department of Economic History
> >
> > Lund University
> >
> > Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund
> >
> >
> >
> > *Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

	[[alternative HTML version deleted]]


From |v|@|e|ro @end|ng |rom gm@||@com  Wed Aug 12 15:55:21 2020
From: |v|@|e|ro @end|ng |rom gm@||@com (Frederico Faleiro)
Date: Wed, 12 Aug 2020 10:55:21 -0300
Subject: [R-sig-Geo] Mask from raster with less RAM
In-Reply-To: <CAJefdj6YEOJQfpx28U_wKVXx7hVsm7CbyREXGVUqsuYpjUNKAw@mail.gmail.com>
References: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
 <CAJefdj6YEOJQfpx28U_wKVXx7hVsm7CbyREXGVUqsuYpjUNKAw@mail.gmail.com>
Message-ID: <CAL+ycWmwbpGiUNnBa6Kx7V1tTgF9DWwqa6wq0DTrvFqQ-N649Q@mail.gmail.com>

Hi guys, thank you for your reply.

Jaime, I have tried, but I can't install rgdal needed to use raster and
apparently other people too (
https://stackoverflow.com/questions/57617895/how-to-install-rgdal-and-or-upload-raster-on-google-collaboration).
Do you have a solution?
Hugo, I need the mask in vector format because the rasters have different
resolutions, so I can't use raster as a mask. I have modified the reprex to
be more precise about it (see below).
Steve, I think this approach has the same issue of Hugo Costa. I don't have
the polygon of the mask, so I am trying to create one to apply in the other
rasters of different resolutions.

# reproducible example
library(raster)
# read data to create mask
r <- raster(system.file("external/test.grd", package="raster"))
r[!is.na(r)] <- 1
pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the data
# raster of different resolution
res2 <- res(r) + 10
r2 <- raster(extent(r), resolution = res2)
r2[ ] <- 1
# apply the mask
r2.mask <- mask(r2, pol)
# plot
par(mfrow = c(1, 3))
plot(r)
plot(r2)
plot(r2.mask)

Cheers!

Em ter., 11 de ago. de 2020 ?s 19:26, Stephen Stewart <
stephen.stewart85 at gmail.com> escreveu:

> Hi Frederico,
>
> It may not solve all of your RAM issues, but in this situation I would
> skip the rasterToPolygons (which is also usually very slow) and use raster
> math to propagate NAs.
>
> r <- raster(system.file("external/test.grd", package="raster"))
> r[!is.na(r)] <- 1
> # Can also be faster to do r = r / r, but add an offset (that cannot
> result in 0) if you have valid 0s.
>  r2 <- raster(extent(r), res(r))
> r2[ ] <- 1
> r2.mask <- r * r2
>
> If you have a polygon to use as a mask, burn it in using the fasterize
> package and then apply the above.
>
> Hope that helps.
>
> Cheers,
>
> Steve
>
> On Wed., 12 Aug. 2020, 6:43 am Frederico Faleiro, <fvfaleiro at gmail.com>
> wrote:
>
>> Dear all,
>>
>> I would like to generate a mask from a raster, but my workflow needs a lot
>> of RAM to process big rasters. I need this polygon mask to use in
>> another's
>> rasters.
>> Do you know another approach that needs less RAM?
>>
>> # reproducible example
>> library(raster)
>> # read data to create mask
>> r <- raster(system.file("external/test.grd", package="raster"))
>> r[!is.na(r)] <- 1
>> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
>> data
>> # apply the mask in another raster
>>  r2 <- raster(extent(r), res(r))
>> r2[ ] <- 1
>> r2.mask <- mask(r2, pol)
>>
>> Cheers!
>>
>> --
>> Frederico Faleiro
>> Postdoctoral Researcher in the INCT-EECBio (https://www.eecbio.ufg.br/)
>> Department of Ecology | Institute of Biological Science | Federal
>> University of Goi?s | Brazil
>> RG: https://www.researchgate.net/profile/Frederico_Faleiro
>> CV: http://lattes.cnpq.br/4926404840659003
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>

	[[alternative HTML version deleted]]


From hugo@gco@t@ @end|ng |rom gm@||@com  Wed Aug 12 17:09:10 2020
From: hugo@gco@t@ @end|ng |rom gm@||@com (Hugo Costa)
Date: Wed, 12 Aug 2020 16:09:10 +0100
Subject: [R-sig-Geo] Mask from raster with less RAM
In-Reply-To: <CAL+ycWmwbpGiUNnBa6Kx7V1tTgF9DWwqa6wq0DTrvFqQ-N649Q@mail.gmail.com>
References: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
 <CAJefdj6YEOJQfpx28U_wKVXx7hVsm7CbyREXGVUqsuYpjUNKAw@mail.gmail.com>
 <CAL+ycWmwbpGiUNnBa6Kx7V1tTgF9DWwqa6wq0DTrvFqQ-N649Q@mail.gmail.com>
Message-ID: <CADvFi5pSYY7nB12dB__J_ALHLRJkK2NO9GFRpMrszP9rHkDkCQ@mail.gmail.com>

Hi Frederico,
my first reaction is to project the raster object to be used as a mask to
the same resolution of the raster to be masked. Certainly it's faster then
and converting raster to vector. If the raster is big, gdalUtils::gdalwarp
can help in projecting your raster data.
Hope this helps
Hugo

Frederico Faleiro <fvfaleiro at gmail.com> escreveu no dia quarta, 12/08/2020
?(s) 14:55:

> Hi guys, thank you for your reply.
>
> Jaime, I have tried, but I can't install rgdal needed to use raster and
> apparently other people too (
>
> https://stackoverflow.com/questions/57617895/how-to-install-rgdal-and-or-upload-raster-on-google-collaboration
> ).
> Do you have a solution?
> Hugo, I need the mask in vector format because the rasters have different
> resolutions, so I can't use raster as a mask. I have modified the reprex to
> be more precise about it (see below).
> Steve, I think this approach has the same issue of Hugo Costa. I don't have
> the polygon of the mask, so I am trying to create one to apply in the other
> rasters of different resolutions.
>
> # reproducible example
> library(raster)
> # read data to create mask
> r <- raster(system.file("external/test.grd", package="raster"))
> r[!is.na(r)] <- 1
> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
> data
> # raster of different resolution
> res2 <- res(r) + 10
> r2 <- raster(extent(r), resolution = res2)
> r2[ ] <- 1
> # apply the mask
> r2.mask <- mask(r2, pol)
> # plot
> par(mfrow = c(1, 3))
> plot(r)
> plot(r2)
> plot(r2.mask)
>
> Cheers!
>
> Em ter., 11 de ago. de 2020 ?s 19:26, Stephen Stewart <
> stephen.stewart85 at gmail.com> escreveu:
>
> > Hi Frederico,
> >
> > It may not solve all of your RAM issues, but in this situation I would
> > skip the rasterToPolygons (which is also usually very slow) and use
> raster
> > math to propagate NAs.
> >
> > r <- raster(system.file("external/test.grd", package="raster"))
> > r[!is.na(r)] <- 1
> > # Can also be faster to do r = r / r, but add an offset (that cannot
> > result in 0) if you have valid 0s.
> >  r2 <- raster(extent(r), res(r))
> > r2[ ] <- 1
> > r2.mask <- r * r2
> >
> > If you have a polygon to use as a mask, burn it in using the fasterize
> > package and then apply the above.
> >
> > Hope that helps.
> >
> > Cheers,
> >
> > Steve
> >
> > On Wed., 12 Aug. 2020, 6:43 am Frederico Faleiro, <fvfaleiro at gmail.com>
> > wrote:
> >
> >> Dear all,
> >>
> >> I would like to generate a mask from a raster, but my workflow needs a
> lot
> >> of RAM to process big rasters. I need this polygon mask to use in
> >> another's
> >> rasters.
> >> Do you know another approach that needs less RAM?
> >>
> >> # reproducible example
> >> library(raster)
> >> # read data to create mask
> >> r <- raster(system.file("external/test.grd", package="raster"))
> >> r[!is.na(r)] <- 1
> >> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
> >> data
> >> # apply the mask in another raster
> >>  r2 <- raster(extent(r), res(r))
> >> r2[ ] <- 1
> >> r2.mask <- mask(r2, pol)
> >>
> >> Cheers!
> >>
> >> --
> >> Frederico Faleiro
> >> Postdoctoral Researcher in the INCT-EECBio (https://www.eecbio.ufg.br/)
> >> Department of Ecology | Institute of Biological Science | Federal
> >> University of Goi?s | Brazil
> >> RG: https://www.researchgate.net/profile/Frederico_Faleiro
> >> CV: http://lattes.cnpq.br/4926404840659003
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Aug 12 18:34:32 2020
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 12 Aug 2020 18:34:32 +0200
Subject: [R-sig-Geo] Moran's I with Objects of Different Length
In-Reply-To: <CAA5N_eZm9+JRFw2gtHE=kyAEhLND6EH4SJHe_0ALLNdXTqsFoQ@mail.gmail.com>
References: <CAA5N_eai9wEB-Lf0ik4xfsHbwq6kZKU4tUCBy8mENC5iS3fqwg@mail.gmail.com>
 <alpine.LFD.2.23.451.2008121244250.2887369@reclus.nhh.no>
 <CAA5N_eZm9+JRFw2gtHE=kyAEhLND6EH4SJHe_0ALLNdXTqsFoQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.23.451.2008121824590.2892079@reclus.nhh.no>

On Wed, 12 Aug 2020, Chanda Chiseni wrote:

> Hi Roger
>
> Thank you for your response.
>
> I am using DHS individual level data , they cluster individuals into about
> 25-30 people and assign them coordinates depending on where they were
> surveyed. So about 30 individuals have similar coordinates. Yes I do have
> more than one observation at one point as you have stated, this could be
> the source of my problem indeed, My question is how could i go about
> creating nearest neighbors and weights by clustering individuals that share
> the same point? I sought for a solution online but could not find any.

Searching online is seldom informative, there is a poor signal-to-noise 
ratio.

See for example the HSAR package or other multi-level approaches, review 
in https://doi.org/10.1016/j.spasta.2017.01.002 and in references in HSAR. 
You have a lower layer of observations in an upper layer of known survey 
locations, so a multi-level approach respects that structure, I think. 
Probably an upper level MRF in mgcv::gam() or try the hgml package, or 
INLA/BayesX for Bayesian approaches.

In these you fit a model with spatially structured random effects at the 
upper level (the weights probably need to be symmetric), which then show 
whether including this term improves the fit.

Hope this helps,

Roger

> Sorry I am a novice at spatial econometrics, this may be obvious to some ,
> so instead of testing for spatial correlation in my residuals, I should in
> essence test for spatial autocorrelation in my outcome variable, for
> instance in my analysis I am using a dummy variable of whether an
> individual is hiv positive or negative. And is there a specific way of
> using a moran.test() on a dummy variable?
>
>
> Kind Regards,
>
> Michael Chanda Chiseni
>
> Phd Candidate
>
> Department of Economic History
>
> Lund University
>
> Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund
>
>
>
> *Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *
>
>
>
>
>
>
> On Wed, Aug 12, 2020 at 12:55 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Tue, 11 Aug 2020, Chanda Chiseni wrote:
>>
>>> I am trying to perform moran's I test on residuals from by probit model
>>> using k-nearest neighbour weights, however i run into an error which i
>>> cant seem to find the solution anywhere online the error is
>>>
>>>> moran.test(residuals.glm(svyprobitest),knear2weight)
>>> Error in moran.test(residuals.glm(svyprobitest), knear2weight) :
>>>  objects of different length
>>>
>>>
>>> The codes I use to come up to this are below
>>>
>>>> library(foreign)
>>>> dhsanalysis2= read.dta("DHSSpatialreg11.dta")
>>> ##removing NAs in PSU
>>>> dhsanalysis3=subset(dhsanalysis2, !is.na(psu))
>>> ##generating a survey design
>>>> mydesighdhs= svydesign(ids =~psu, data =dhsanalysis3, weight = ~wgt,
>>> strata = ~v023,nest=TRUE)
>>> ##Defining coordinate colums
>>>> coordinates(dhsanalysis3)= c("longnum","latnum")
>>> #Defining Projection
>>>> proj4string(dhsanalysis3) <- CRS("+init=epsg:4326")
>>> ##binding longiude and latitude
>>>> lon2<- dhsanalysis3$longnum
>>>> lat2<- dhsanalysis3$latnum
>>>> coords<- cbind(lon2,lat2)
>>> ##Creating spatial weights based on the nearest neighbour
>>>> knear2= knearneigh(coords,k=2,longlat=T)
>>> Warning message:
>>> In knearneigh(coords, k = 2, longlat = T) :
>>>  knearneigh: identical points found ### I get a warning message on
>>> identical points found, is this a problem and how would i deal with this
>>
>> It usually indicates muddled thinking. You have more than one observation
>> at the same point, which could be an unintended duplicate (copy of the
>> same observation), or it could indicate that any spatial process has
>> multiple values at that point. In geostatistics, one would jitter the
>> points to introduce distance. In your case, this probably isn't households
>> living at one address point. All the observations at the same point will
>> get the same sets of neighbours. If there are many co-located
>> observations, k in k-nearest may be too small to include them all.
>>
>>>> knear2.nb= knn2nb(knear2)
>>>> knear2weight= nb2listw(knear2.nb, style="W",zero.policy=T)
>>>
>>> ##declaring categorical variables as factor variables
>>>> dhsanalysis3$hivpositive.f <- factor(dhsanalysis3$hivpositive)
>>>> dhsanalysis3$protestant.f <- factor(dhsanalysis3$protestant2)
>>>> dhsanalysis3$Married.f <- factor(dhsanalysis3$Married)
>>>> dhsanalysis3$female.f <- factor(dhsanalysis3$female)
>>>> dhsanalysis3$urban1.f <- factor(dhsanalysis3$urban1)
>>>> dhsanalysis3$river10kmdum.f <- factor(dhsanalysis3$river10kmdum)
>>>> dhsanalysis3$Explorer50kmdum.f <- factor(dhsanalysis3$Explorer50kmdum)
>>>> dhsanalysis3$Rail50kmdum.f <- factor(dhsanalysis3$Rail50kmdum)
>>>> dhsanalysis3$Province1.f <- factor(dhsanalysis3$Province1)
>>>> dhsanalysis3$WealthIndex.f <- factor(dhsanalysis3$WealthIndex)
>>>> dhsanalysis3$occupation2.f <- factor(dhsanalysis3$occupation2)
>>>> dhsanalysis3$highested.f <- factor(dhsanalysis3$highested)
>>>> protestant.f= dhsanalysis3$protestant.f
>>>> hivpositive.f=dhsanalysi3$hivpositive.f
>>>> Married.f=dhsanalysis3$Married.f
>>>> female.f=dhsanalysis3$female.f
>>>> urban1.f=dhsanalysis3$urban1.f
>>>> river10kmdum.f=dhsanalysis3$river10kmdum.f
>>>> Explorer50kmdum.f=dhsanalysis2$Explorer50kmdum.f
>>>> Province1.f=dhsanalysis3$Province1.f
>>>> WealthIndex.f=dhsanalysis3$WealthIndex.f
>>>> occupation2.f=dhsanalysis3$occupation2.f
>>>> highested.f=dhsanalysis3$highested.f
>>>> Age=dhsanalysis3$Age
>>>> Age2=dhsanalysis3$Age2
>>>> HIVKnowledge=dhsanalysis3$HIVKnowledge
>>>> churchkm=dhsanalysis3$churchkm
>>>> lnhospitalkm=dhsanalysis3$lnhospitalkm
>>>> lnElevationMean=dhsanalysis3$lnElevationMean
>>>
>>>> ###VARIABLES IN SURVEY DESIGN
>>>> mydesighdhs$hivpositive.f <- factor(dhsanalysis3$hivpositive)
>>>> mydesighdhs$protestant.f <- factor(dhsanalysis3$protestant2)
>>>> mydesighdhs$Married.f <- factor(dhsanalysis3$Married)
>>>> mydesighdhs$female.f <- factor(dhsanalysis3$female)
>>>> mydesighdhs$urban1.f <- factor(dhsanalysis3$urban1)
>>>> mydesighdhs$river10kmdum.f <- factor(dhsanalysis3$river10kmdum)
>>>> mydesighdhs$Explorer50kmdum.f <- factor(dhsanalysis3$Explorer50kmdum)
>>>> mydesighdhs$Rail50kmdum.f <- factor(dhsanalysis3$Rail50kmdum)
>>>> mydesighdhs$Province1.f <- factor(dhsanalysis3$Province1)
>>>> mydesighdhs$WealthIndex.f <- factor(dhsanalysis3$WealthIndex)
>>>> mydesighdhs$occupation2.f <- factor(dhsanalysis3$occupation2)
>>>> mydesighdhs$highested.f <- factor(dhsanalysis3$highested)
>>>> mydesighdhs$Age=Age
>>>> mydesighdhs$Age2=Age2
>>>> mydesighdhs$HIVKnowledge=HIVKnowledge
>>>> mydesighdhs$churchkm=churchkm
>>>> mydesighdhs$lnhospitalkm=lnhospitalkm
>>>> mydesighdhs$lnElevationMean=lnElevationMean
>>>
>>>> svyprobitest= svyglm(hivpositive.f~ churchkm +lnhospitalkm
>> +protestant.f+
>>> Age+
>>>
>> Age2+Married.f+female.f+urban1.f+river10kmdum.f+Explorer50kmdum.f+Rail50kmdum.f+lnElevationMean+Province1.f
>>> + WealthIndex.f + HIVKnowledge+ occupation2.f+
>>> highested.f,design=mydesighdhs,family = quasibinomial(link =
>>> "probit"),data=dhsanalysis3)
>>> Error in .subset2(x, i, exact = exact) : subscript out of bounds
>>
>> So you've an error anyway.
>>
>>>
>>> ## Iran my probit model without implementing the survey design in the
>> model
>>> just to see whether Moran test is working
>>>> svyprobitest2= glm(hivpositive.f~ churchkm +lnhospitalkm +protestant.f+
>>> Age+
>>>
>> Age2+Married.f+female.f+urban1.f+river10kmdum.f+Explorer50kmdum.f+Rail50kmdum.f+lnElevationMean+Province1.f
>>> + WealthIndex.f + HIVKnowledge+ occupation2.f+ highested.f,family =
>>> quasibinomial(link = "probit"),data=dhsanalysis3)
>>> #Moran test
>>>> moran.test(residuals.glm(svyprobitest),knear2weight)
>>> Error in moran.test(residuals.glm(svyprobitest), knear2weight) :
>>>  objects of different length
>>>
>>
>> Testing residuals using moran.test() is usually wrong, as the expectation
>> and variance of the statistic are based on a null model (intercept only),
>> not a model with covariates.
>>
>> What are length(residuals.glm(svyprobitest)) and length(knear2.nb)? Did
>> glm drop observations with missing values? If so, you should subset both
>> the data submitted to glm() and the neighbour object so that they match.
>>
>> Hope this helps,
>>
>> Roger
>>
>>>
>>> Kind Regards,
>>>
>>> Michael Chanda Chiseni
>>>
>>> Phd Candidate
>>>
>>> Department of Economic History
>>>
>>> Lund University
>>>
>>> Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund
>>>
>>>
>>>
>>> *Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From Roger@B|v@nd @end|ng |rom nhh@no  Wed Aug 12 18:45:29 2020
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 12 Aug 2020 18:45:29 +0200
Subject: [R-sig-Geo] Mask from raster with less RAM
In-Reply-To: <CAL+ycWmwbpGiUNnBa6Kx7V1tTgF9DWwqa6wq0DTrvFqQ-N649Q@mail.gmail.com>
References: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
 <CAJefdj6YEOJQfpx28U_wKVXx7hVsm7CbyREXGVUqsuYpjUNKAw@mail.gmail.com>
 <CAL+ycWmwbpGiUNnBa6Kx7V1tTgF9DWwqa6wq0DTrvFqQ-N649Q@mail.gmail.com>
Message-ID: <alpine.LFD.2.23.451.2008121836110.2892079@reclus.nhh.no>

On Wed, 12 Aug 2020, Frederico Faleiro wrote:

> Hi guys, thank you for your reply.
>
> Jaime, I have tried, but I can't install rgdal needed to use raster and
> apparently other people too (
> https://stackoverflow.com/questions/57617895/how-to-install-rgdal-and-or-upload-raster-on-google-collaboration).
> Do you have a solution?

Use CRAN Windows or MacOS binaries, or install the system requirements. 
Unless you can do that, for example because you do not control the 
platform you are using (are you working on a cloud instance?), do not 
install packages needing external software 
from source.

> Hugo, I need the mask in vector format because the rasters have different
> resolutions, so I can't use raster as a mask. I have modified the reprex to
> be more precise about it (see below).

Please use GRASS. What you are trying to do is something that has "just 
worked" in GRASS since it was first created. GRASS can be run from the 
shell, from Python and from R (rgrass7). Or use SAGA, another fast raster 
processor. Both mask from raster directly. Write a shell script for GRASS 
to resample your rasters, mask them, and complete. You can also use GRASS 
and SAGA from QGIS. You might use Python or R to make file name handling 
"easier" than in a script. Neither GRASS nor SAGA use much memory unless 
rasters are huge, and then they are lean.

Roger

> Steve, I think this approach has the same issue of Hugo Costa. I don't have
> the polygon of the mask, so I am trying to create one to apply in the other
> rasters of different resolutions.
>
> # reproducible example
> library(raster)
> # read data to create mask
> r <- raster(system.file("external/test.grd", package="raster"))
> r[!is.na(r)] <- 1
> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the data
> # raster of different resolution
> res2 <- res(r) + 10
> r2 <- raster(extent(r), resolution = res2)
> r2[ ] <- 1
> # apply the mask
> r2.mask <- mask(r2, pol)
> # plot
> par(mfrow = c(1, 3))
> plot(r)
> plot(r2)
> plot(r2.mask)
>
> Cheers!
>
> Em ter., 11 de ago. de 2020 ?s 19:26, Stephen Stewart <
> stephen.stewart85 at gmail.com> escreveu:
>
>> Hi Frederico,
>>
>> It may not solve all of your RAM issues, but in this situation I would
>> skip the rasterToPolygons (which is also usually very slow) and use raster
>> math to propagate NAs.
>>
>> r <- raster(system.file("external/test.grd", package="raster"))
>> r[!is.na(r)] <- 1
>> # Can also be faster to do r = r / r, but add an offset (that cannot
>> result in 0) if you have valid 0s.
>>  r2 <- raster(extent(r), res(r))
>> r2[ ] <- 1
>> r2.mask <- r * r2
>>
>> If you have a polygon to use as a mask, burn it in using the fasterize
>> package and then apply the above.
>>
>> Hope that helps.
>>
>> Cheers,
>>
>> Steve
>>
>> On Wed., 12 Aug. 2020, 6:43 am Frederico Faleiro, <fvfaleiro at gmail.com>
>> wrote:
>>
>>> Dear all,
>>>
>>> I would like to generate a mask from a raster, but my workflow needs a lot
>>> of RAM to process big rasters. I need this polygon mask to use in
>>> another's
>>> rasters.
>>> Do you know another approach that needs less RAM?
>>>
>>> # reproducible example
>>> library(raster)
>>> # read data to create mask
>>> r <- raster(system.file("external/test.grd", package="raster"))
>>> r[!is.na(r)] <- 1
>>> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
>>> data
>>> # apply the mask in another raster
>>>  r2 <- raster(extent(r), res(r))
>>> r2[ ] <- 1
>>> r2.mask <- mask(r2, pol)
>>>
>>> Cheers!
>>>
>>> --
>>> Frederico Faleiro
>>> Postdoctoral Researcher in the INCT-EECBio (https://www.eecbio.ufg.br/)
>>> Department of Ecology | Institute of Biological Science | Federal
>>> University of Goi?s | Brazil
>>> RG: https://www.researchgate.net/profile/Frederico_Faleiro
>>> CV: http://lattes.cnpq.br/4926404840659003
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From |v|@|e|ro @end|ng |rom gm@||@com  Wed Aug 12 19:58:30 2020
From: |v|@|e|ro @end|ng |rom gm@||@com (Frederico Faleiro)
Date: Wed, 12 Aug 2020 14:58:30 -0300
Subject: [R-sig-Geo] Mask from raster with less RAM
In-Reply-To: <alpine.LFD.2.23.451.2008121836110.2892079@reclus.nhh.no>
References: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
 <CAJefdj6YEOJQfpx28U_wKVXx7hVsm7CbyREXGVUqsuYpjUNKAw@mail.gmail.com>
 <CAL+ycWmwbpGiUNnBa6Kx7V1tTgF9DWwqa6wq0DTrvFqQ-N649Q@mail.gmail.com>
 <alpine.LFD.2.23.451.2008121836110.2892079@reclus.nhh.no>
Message-ID: <CAL+ycWmdRJM=nG1kkSvDy0VORUWV4q395z86uUX4OmA3kmdghg@mail.gmail.com>

Hi guys,

I really need to use the mask without modifying the original resolution of
the rasters, so vectorizing was my first try. I know I can resample first
before applying a raster mask, but this will not work for my objectives
that involve creating some visualisations to the same area with the
original resolution.
Roger, I have tried use the google colab in the cloud (e.g.
https://colab.research.google.com/drive/1BYnnbqeyZAlYnxR9IHC8tpW07EpDeyKR#scrollTo=uaE0kZ0vkqms)
and I think you are right about the restriction about the use of external
softwares like GDAL in this specific case.
Do you know if is it possible to do it in GRASS or QGIS? I only find the
function polygonize (as in raster::rasterToContour) that generates contours
around equal values from the raster.

Cheers,


Em qua., 12 de ago. de 2020 ?s 13:45, Roger Bivand <Roger.Bivand at nhh.no>
escreveu:

> On Wed, 12 Aug 2020, Frederico Faleiro wrote:
>
> > Hi guys, thank you for your reply.
> >
> > Jaime, I have tried, but I can't install rgdal needed to use raster and
> > apparently other people too (
> >
> https://stackoverflow.com/questions/57617895/how-to-install-rgdal-and-or-upload-raster-on-google-collaboration
> ).
> > Do you have a solution?
>
> Use CRAN Windows or MacOS binaries, or install the system requirements.
> Unless you can do that, for example because you do not control the
> platform you are using (are you working on a cloud instance?), do not
> install packages needing external software
> from source.
>
> > Hugo, I need the mask in vector format because the rasters have different
> > resolutions, so I can't use raster as a mask. I have modified the reprex
> to
> > be more precise about it (see below).
>
> Please use GRASS. What you are trying to do is something that has "just
> worked" in GRASS since it was first created. GRASS can be run from the
> shell, from Python and from R (rgrass7). Or use SAGA, another fast raster
> processor. Both mask from raster directly. Write a shell script for GRASS
> to resample your rasters, mask them, and complete. You can also use GRASS
> and SAGA from QGIS. You might use Python or R to make file name handling
> "easier" than in a script. Neither GRASS nor SAGA use much memory unless
> rasters are huge, and then they are lean.
>
> Roger
>
> > Steve, I think this approach has the same issue of Hugo Costa. I don't
> have
> > the polygon of the mask, so I am trying to create one to apply in the
> other
> > rasters of different resolutions.
> >
> > # reproducible example
> > library(raster)
> > # read data to create mask
> > r <- raster(system.file("external/test.grd", package="raster"))
> > r[!is.na(r)] <- 1
> > pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
> data
> > # raster of different resolution
> > res2 <- res(r) + 10
> > r2 <- raster(extent(r), resolution = res2)
> > r2[ ] <- 1
> > # apply the mask
> > r2.mask <- mask(r2, pol)
> > # plot
> > par(mfrow = c(1, 3))
> > plot(r)
> > plot(r2)
> > plot(r2.mask)
> >
> > Cheers!
> >
> > Em ter., 11 de ago. de 2020 ?s 19:26, Stephen Stewart <
> > stephen.stewart85 at gmail.com> escreveu:
> >
> >> Hi Frederico,
> >>
> >> It may not solve all of your RAM issues, but in this situation I would
> >> skip the rasterToPolygons (which is also usually very slow) and use
> raster
> >> math to propagate NAs.
> >>
> >> r <- raster(system.file("external/test.grd", package="raster"))
> >> r[!is.na(r)] <- 1
> >> # Can also be faster to do r = r / r, but add an offset (that cannot
> >> result in 0) if you have valid 0s.
> >>  r2 <- raster(extent(r), res(r))
> >> r2[ ] <- 1
> >> r2.mask <- r * r2
> >>
> >> If you have a polygon to use as a mask, burn it in using the fasterize
> >> package and then apply the above.
> >>
> >> Hope that helps.
> >>
> >> Cheers,
> >>
> >> Steve
> >>
> >> On Wed., 12 Aug. 2020, 6:43 am Frederico Faleiro, <fvfaleiro at gmail.com>
> >> wrote:
> >>
> >>> Dear all,
> >>>
> >>> I would like to generate a mask from a raster, but my workflow needs a
> lot
> >>> of RAM to process big rasters. I need this polygon mask to use in
> >>> another's
> >>> rasters.
> >>> Do you know another approach that needs less RAM?
> >>>
> >>> # reproducible example
> >>> library(raster)
> >>> # read data to create mask
> >>> r <- raster(system.file("external/test.grd", package="raster"))
> >>> r[!is.na(r)] <- 1
> >>> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
> >>> data
> >>> # apply the mask in another raster
> >>>  r2 <- raster(extent(r), res(r))
> >>> r2[ ] <- 1
> >>> r2.mask <- mask(r2, pol)
> >>>
> >>> Cheers!
> >>>
> >>> --
> >>> Frederico Faleiro
> >>> Postdoctoral Researcher in the INCT-EECBio (https://www.eecbio.ufg.br/
> )
> >>> Department of Ecology | Institute of Biological Science | Federal
> >>> University of Goi?s | Brazil
> >>> RG: https://www.researchgate.net/profile/Frederico_Faleiro
> >>> CV: http://lattes.cnpq.br/4926404840659003
> >>>
> >>>         [[alternative HTML version deleted]]
> >>>
> >>> _______________________________________________
> >>> R-sig-Geo mailing list
> >>> R-sig-Geo at r-project.org
> >>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

	[[alternative HTML version deleted]]


From @our@v@@@rk@r @end|ng |rom @hdun|@edu@|n  Wed Aug 12 20:20:37 2020
From: @our@v@@@rk@r @end|ng |rom @hdun|@edu@|n (Sourav Sarkar)
Date: Wed, 12 Aug 2020 23:50:37 +0530
Subject: [R-sig-Geo] Mask from raster with less RAM
In-Reply-To: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
References: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
Message-ID: <CAFBgidpWpCaZV7COwh6HMcRQ9mhXnJ_UajO8SfSJGjYaD3G7ag@mail.gmail.com>

Dear Frederico,
With the very little experience that I have with raster data, it seems to
me (maybe I am wrong) that `crop' is computationally easier than `mask'. If
the crop command serves your purpose, maybe you can try:
r2.mask <- crop(r2, pol)

Kind regards,

???, ?? ?????, ???? ?????? ?:?? AM ??? ? Frederico Faleiro <
fvfaleiro at gmail.com> ???????:

> Dear all,
>
> I would like to generate a mask from a raster, but my workflow needs a lot
> of RAM to process big rasters. I need this polygon mask to use in another's
> rasters.
> Do you know another approach that needs less RAM?
>
> # reproducible example
> library(raster)
> # read data to create mask
> r <- raster(system.file("external/test.grd", package="raster"))
> r[!is.na(r)] <- 1
> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
> data
> # apply the mask in another raster
>  r2 <- raster(extent(r), res(r))
> r2[ ] <- 1
> r2.mask <- mask(r2, pol)
>
> Cheers!
>
> --
> Frederico Faleiro
> Postdoctoral Researcher in the INCT-EECBio (https://www.eecbio.ufg.br/)
> Department of Ecology | Institute of Biological Science | Federal
> University of Goi?s | Brazil
> RG: https://www.researchgate.net/profile/Frederico_Faleiro
> CV: http://lattes.cnpq.br/4926404840659003
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


-- 
Sourav Sarkar,
Assistant Professor,
School of Arts and Sciences,
Ahmedabad University

	[[alternative HTML version deleted]]


From n|co|@ @end|ng |rom g@mb@ro@co@uk  Thu Aug 13 20:11:38 2020
From: n|co|@ @end|ng |rom g@mb@ro@co@uk (Nicola Gambaro)
Date: Thu, 13 Aug 2020 20:11:38 +0200
Subject: [R-sig-Geo] Spatial correlation between two 'sf' kriging objects
Message-ID: <E1C26BA8-67CE-4FB4-BD57-A649D7FDCA21@gambaro.co.uk>

I have created two ?sf? kriging objects (point vectors), one for temperature and another for agricultural yields. To make the grid and carry out the point interpolation, I have remained within the ?sf? package.

I would now like to create a spatial local correlation ?raster? between these two variables, as shown on this webpage https://statnmap.com/2018-01-27-spatial-correlation-between-rasters/ <https://statnmap.com/2018-01-27-spatial-correlation-between-rasters/>. However, in that example, they use the ?raster? package and the ?focal? function. I was wondering if there was a way of doing this within ?sf?, i.e. without having to change classes? If not, what is the best way to convert those objects into raster classes?

Here is an excerpt of my kriging code for reference:
library(sf)
sf_data <- st_as_sf(x = data, coords = c("longitude", "latitude"), crs = 4326)
library(gstat)
vgm_utci <- variogram(UTCI~1, sf_data)
utci_fit <- fit.variogram(vgm_utci, vgm("Gau"), fit.kappa = TRUE)
plot(vgm_utci, utci_fit)
istria <- read_sf(?./Istria_Boundary.shp")
istria <- istria$geometry
istria.grid <- istria %>% 
  st_make_grid(cellsize = 0.05, what = "centers") %>% 
  st_intersection(istria) 
library(ggplot2)
ggplot() + geom_sf(data = istria) + geom_sf(data = istria.grid)
library(stars)
utci_krig <- krige(formula = sf_data$UTCI ~ 1, locations = sf_data, 
                   newdata = istria.grid, model = utci_fit)


Thank you very much in advance,

Nicola
	[[alternative HTML version deleted]]


From b|@|ev||@t @end|ng |rom gm@||@com  Fri Aug 14 14:01:43 2020
From: b|@|ev||@t @end|ng |rom gm@||@com (=?UTF-8?Q?Bede-Fazekas_=c3=81kos?=)
Date: Fri, 14 Aug 2020 14:01:43 +0200
Subject: [R-sig-Geo] 
 Spatial correlation between two 'sf' kriging objects
In-Reply-To: <E1C26BA8-67CE-4FB4-BD57-A649D7FDCA21@gambaro.co.uk>
References: <E1C26BA8-67CE-4FB4-BD57-A649D7FDCA21@gambaro.co.uk>
Message-ID: <043f13cc-20e1-a768-3419-03301f8d8f7b@gmail.com>

Dear Nicola,

Instead of raster::focal(), you can apply the cor() function in 
combination with st_distance() (or st_buffer() and st_within()). E.g. 
let's say that 'grid' is a POINT type sf object containing columns 
'temperature' and 'yield':
distance_threshold <- 40
distance_matrix <- st_distance(grid)
grid$correlation <- vapply(X = 1:nrow(grid), FUN.VALUE = numeric(1), FUN 
= function (point_number) {cor(x = st_set_geometry(grid, 
NULL)[distance_matrix[point_number, distance_matrix[point_number, , drop 
= TRUE] < distance_threshold , drop = TRUE], "temperature"], y = 
st_set_geometry(grid, NULL)[distance_matrix[point_number, 
distance_matrix[point_number, , drop = TRUE] < distance_threshold , drop 
= TRUE], "yield"])})

Or something like this... I have not tested this code, and am sure that 
it is not the most efficient solution.

For large, square grids, raster might be faster than sf. You can convert 
your grid to RasterLayer with function rasterFromXYZ() combined with 
st_coordinates(), st_set_geometry(, NULL) and cbind.data.frame(). There 
might be more straightforward solutions for the conversion...

HTH,
?kos Bede-Fazekas
Hungarian Academy of Sciences


2020.08.13. 20:11 keltez?ssel, Nicola Gambaro ?rta:
> I have created two ?sf? kriging objects (point vectors), one for temperature and another for agricultural yields. To make the grid and carry out the point interpolation, I have remained within the ?sf? package.
>
> I would now like to create a spatial local correlation ?raster? between these two variables, as shown on this webpage https://statnmap.com/2018-01-27-spatial-correlation-between-rasters/ <https://statnmap.com/2018-01-27-spatial-correlation-between-rasters/>. However, in that example, they use the ?raster? package and the ?focal? function. I was wondering if there was a way of doing this within ?sf?, i.e. without having to change classes? If not, what is the best way to convert those objects into raster classes?
>
> Here is an excerpt of my kriging code for reference:
> library(sf)
> sf_data <- st_as_sf(x = data, coords = c("longitude", "latitude"), crs = 4326)
> library(gstat)
> vgm_utci <- variogram(UTCI~1, sf_data)
> utci_fit <- fit.variogram(vgm_utci, vgm("Gau"), fit.kappa = TRUE)
> plot(vgm_utci, utci_fit)
> istria <- read_sf(?./Istria_Boundary.shp")
> istria <- istria$geometry
> istria.grid <- istria %>%
>    st_make_grid(cellsize = 0.05, what = "centers") %>%
>    st_intersection(istria)
> library(ggplot2)
> ggplot() + geom_sf(data = istria) + geom_sf(data = istria.grid)
> library(stars)
> utci_krig <- krige(formula = sf_data$UTCI ~ 1, locations = sf_data,
>                     newdata = istria.grid, model = utci_fit)
>
>
> Thank you very much in advance,
>
> Nicola
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From |v|@|e|ro @end|ng |rom gm@||@com  Sat Aug 15 00:08:43 2020
From: |v|@|e|ro @end|ng |rom gm@||@com (Frederico Faleiro)
Date: Fri, 14 Aug 2020 19:08:43 -0300
Subject: [R-sig-Geo] Mask from raster with less RAM
In-Reply-To: <CAFBgidpWpCaZV7COwh6HMcRQ9mhXnJ_UajO8SfSJGjYaD3G7ag@mail.gmail.com>
References: <CAL+ycW=O2Mpw2Y+HJWczvreVjSSTvd5MuxvODhW9UPCf2XAFJA@mail.gmail.com>
 <CAFBgidpWpCaZV7COwh6HMcRQ9mhXnJ_UajO8SfSJGjYaD3G7ag@mail.gmail.com>
Message-ID: <CAL+ycWnx0QdYV0pCuAxgcr9jEawBAAo3qyFXkJfeaUNbbvd02w@mail.gmail.com>

Hi guys,

thank you for your help. Sometimes we really need more RAM. I got access to
a cluster with more RAM and solved the problem.
Dear Sourav, you are right, but the crop only cuts the extent of the
rasters, so I really need to use mask. In addition, my main limitation was
in generating the polygon from raster (i.e. rasterToPolygons).

Cheers!

Em qua., 12 de ago. de 2020 ?s 15:20, Sourav Sarkar <
sourav.sarkar at ahduni.edu.in> escreveu:

> Dear Frederico,
> With the very little experience that I have with raster data, it seems to
> me (maybe I am wrong) that `crop' is computationally easier than `mask'. If
> the crop command serves your purpose, maybe you can try:
> r2.mask <- crop(r2, pol)
>
> Kind regards,
>
> ???, ?? ?????, ???? ?????? ?:?? AM ??? ? Frederico Faleiro <
> fvfaleiro at gmail.com> ???????:
>
>> Dear all,
>>
>> I would like to generate a mask from a raster, but my workflow needs a lot
>> of RAM to process big rasters. I need this polygon mask to use in
>> another's
>> rasters.
>> Do you know another approach that needs less RAM?
>>
>> # reproducible example
>> library(raster)
>> # read data to create mask
>> r <- raster(system.file("external/test.grd", package="raster"))
>> r[!is.na(r)] <- 1
>> pol <- rasterToPolygons(r, dissolve = T) #  a lot of RAM to process the
>> data
>> # apply the mask in another raster
>>  r2 <- raster(extent(r), res(r))
>> r2[ ] <- 1
>> r2.mask <- mask(r2, pol)
>>
>> Cheers!
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
> --
> Sourav Sarkar,
> Assistant Professor,
> School of Arts and Sciences,
> Ahmedabad University
>


-- 
Frederico Faleiro
Postdoctoral Researcher in the INCT-EECBio (https://www.eecbio.ufg.br/)
Department of Ecology | Institute of Biological Science | Federal
University of Goi?s | Brazil
RG: https://www.researchgate.net/profile/Frederico_Faleiro
CV: http://lattes.cnpq.br/4926404840659003

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat Aug 15 06:29:20 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 15 Aug 2020 16:29:20 +1200
Subject: [R-sig-Geo] Testing.
Message-ID: <20200815162920.12d78260@rolf-Latitude-E7470>


My apologies for the noise.  Please ignore this message.
I am just trying to test out message filters in a new mail client that
I am learning to use.

Again, sorry for the noise.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From n|co|@ @end|ng |rom g@mb@ro@co@uk  Sat Aug 15 23:15:52 2020
From: n|co|@ @end|ng |rom g@mb@ro@co@uk (Nicola Gambaro)
Date: Sat, 15 Aug 2020 23:15:52 +0200
Subject: [R-sig-Geo] 
 Spatial correlation between two 'sf' kriging objects
In-Reply-To: <mailman.28368.3.1597485601.62583.r-sig-geo@r-project.org>
References: <mailman.28368.3.1597485601.62583.r-sig-geo@r-project.org>
Message-ID: <BC3DF338-570E-4CBD-B713-6EB511B704D4@gambaro.co.uk>

Dear ?kos,

Thank you very much for your help. 

I ran your code but unfortunately it returns the error:

Error in Ops.units(distance_matrix[as_units(point_number), , drop = TRUE],  : 
  		both operands of the expression should be "units" objects 

Any ideas how to fix this?

Cheers!

Nicola Gambaro
BSc Environmental Geoscience, First Class
Durham University 

> Message: 1
> Date: Fri, 14 Aug 2020 14:01:43 +0200
> From: =?UTF-8?Q?Bede-Fazekas_=c3=81kos?= <bfalevlist at gmail.com>
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo]  Spatial correlation between two 'sf' kriging
> 	objects
> Message-ID: <043f13cc-20e1-a768-3419-03301f8d8f7b at gmail.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
> 
> Dear Nicola,
> 
> Instead of raster::focal(), you can apply the cor() function in 
> combination with st_distance() (or st_buffer() and st_within()). E.g. 
> let's say that 'grid' is a POINT type sf object containing columns 
> 'temperature' and 'yield':
> distance_threshold <- 40
> distance_matrix <- st_distance(grid)
> grid$correlation <- vapply(X = 1:nrow(grid), FUN.VALUE = numeric(1), FUN 
> = function (point_number) {cor(x = st_set_geometry(grid, 
> NULL)[distance_matrix[point_number, distance_matrix[point_number, , drop 
> = TRUE] < distance_threshold , drop = TRUE], "temperature"], y = 
> st_set_geometry(grid, NULL)[distance_matrix[point_number, 
> distance_matrix[point_number, , drop = TRUE] < distance_threshold , drop 
> = TRUE], "yield"])})
> 
> Or something like this... I have not tested this code, and am sure that 
> it is not the most efficient solution.
> 
> For large, square grids, raster might be faster than sf. You can convert 
> your grid to RasterLayer with function rasterFromXYZ() combined with 
> st_coordinates(), st_set_geometry(, NULL) and cbind.data.frame(). There 
> might be more straightforward solutions for the conversion...
> 
> HTH,
> ?kos Bede-Fazekas
> Hungarian Academy of Sciences
> 
> 
> 2020.08.13. 20:11 keltez?ssel, Nicola Gambaro ?rta:
>> I have created two ?sf? kriging objects (point vectors), one for temperature and another for agricultural yields. To make the grid and carry out the point interpolation, I have remained within the ?sf? package.
>> 
>> I would now like to create a spatial local correlation ?raster? between these two variables, as shown on this webpage https://statnmap.com/2018-01-27-spatial-correlation-between-rasters/ <https://statnmap.com/2018-01-27-spatial-correlation-between-rasters/>. However, in that example, they use the ?raster? package and the ?focal? function. I was wondering if there was a way of doing this within ?sf?, i.e. without having to change classes? If not, what is the best way to convert those objects into raster classes?
>> 
>> Here is an excerpt of my kriging code for reference:
>> library(sf)
>> sf_data <- st_as_sf(x = data, coords = c("longitude", "latitude"), crs = 4326)
>> library(gstat)
>> vgm_utci <- variogram(UTCI~1, sf_data)
>> utci_fit <- fit.variogram(vgm_utci, vgm("Gau"), fit.kappa = TRUE)
>> plot(vgm_utci, utci_fit)
>> istria <- read_sf(?./Istria_Boundary.shp")
>> istria <- istria$geometry
>> istria.grid <- istria %>%
>>   st_make_grid(cellsize = 0.05, what = "centers") %>%
>>   st_intersection(istria)
>> library(ggplot2)
>> ggplot() + geom_sf(data = istria) + geom_sf(data = istria.grid)
>> library(stars)
>> utci_krig <- krige(formula = sf_data$UTCI ~ 1, locations = sf_data,
>>                    newdata = istria.grid, model = utci_fit)
>> 
>> 
>> Thank you very much in advance,
>> 
>> Nicola
>> 	[[alternative HTML version deleted]]
>> 
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From b|@|ev||@t @end|ng |rom gm@||@com  Mon Aug 17 09:00:44 2020
From: b|@|ev||@t @end|ng |rom gm@||@com (=?UTF-8?Q?Bede-Fazekas_=c3=81kos?=)
Date: Mon, 17 Aug 2020 09:00:44 +0200
Subject: [R-sig-Geo] 
 Spatial correlation between two 'sf' kriging objects
In-Reply-To: <BC3DF338-570E-4CBD-B713-6EB511B704D4@gambaro.co.uk>
References: <mailman.28368.3.1597485601.62583.r-sig-geo@r-project.org>
 <BC3DF338-570E-4CBD-B713-6EB511B704D4@gambaro.co.uk>
Message-ID: <43b238f9-6ddd-2e01-6eef-113d74156ae1@gmail.com>

Dear Nicola,
you are right, the class of the result of st_distance is not numeric but 
units (if CRS != NA). You may convert it to numeric with as.numeric().
HTH,
?kos

2020.08.15. 23:15 keltez?ssel, Nicola Gambaro ?rta:
> Dear ?kos,
>
> Thank you very much for your help.
>
> I ran your code but unfortunately it returns the error:
>
> Error in Ops.units(distance_matrix[as_units(point_number), , drop = TRUE],  :
>    		both operands of the expression should be "units" objects
>
> Any ideas how to fix this?
>
> Cheers!
>
> Nicola Gambaro
> BSc Environmental Geoscience, First Class
> Durham University
>
>> Message: 1
>> Date: Fri, 14 Aug 2020 14:01:43 +0200
>> From: =?UTF-8?Q?Bede-Fazekas_=c3=81kos?= <bfalevlist at gmail.com>
>> To: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo]  Spatial correlation between two 'sf' kriging
>> 	objects
>> Message-ID: <043f13cc-20e1-a768-3419-03301f8d8f7b at gmail.com>
>> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>>
>> Dear Nicola,
>>
>> Instead of raster::focal(), you can apply the cor() function in
>> combination with st_distance() (or st_buffer() and st_within()). E.g.
>> let's say that 'grid' is a POINT type sf object containing columns
>> 'temperature' and 'yield':
>> distance_threshold <- 40
>> distance_matrix <- st_distance(grid)
>> grid$correlation <- vapply(X = 1:nrow(grid), FUN.VALUE = numeric(1), FUN
>> = function (point_number) {cor(x = st_set_geometry(grid,
>> NULL)[distance_matrix[point_number, distance_matrix[point_number, , drop
>> = TRUE] < distance_threshold , drop = TRUE], "temperature"], y =
>> st_set_geometry(grid, NULL)[distance_matrix[point_number,
>> distance_matrix[point_number, , drop = TRUE] < distance_threshold , drop
>> = TRUE], "yield"])})
>>
>> Or something like this... I have not tested this code, and am sure that
>> it is not the most efficient solution.
>>
>> For large, square grids, raster might be faster than sf. You can convert
>> your grid to RasterLayer with function rasterFromXYZ() combined with
>> st_coordinates(), st_set_geometry(, NULL) and cbind.data.frame(). There
>> might be more straightforward solutions for the conversion...
>>
>> HTH,
>> ?kos Bede-Fazekas
>> Hungarian Academy of Sciences
>>
>>
>> 2020.08.13. 20:11 keltez?ssel, Nicola Gambaro ?rta:
>>> I have created two ?sf? kriging objects (point vectors), one for temperature and another for agricultural yields. To make the grid and carry out the point interpolation, I have remained within the ?sf? package.
>>>
>>> I would now like to create a spatial local correlation ?raster? between these two variables, as shown on this webpage https://statnmap.com/2018-01-27-spatial-correlation-between-rasters/ <https://statnmap.com/2018-01-27-spatial-correlation-between-rasters/>. However, in that example, they use the ?raster? package and the ?focal? function. I was wondering if there was a way of doing this within ?sf?, i.e. without having to change classes? If not, what is the best way to convert those objects into raster classes?
>>>
>>> Here is an excerpt of my kriging code for reference:
>>> library(sf)
>>> sf_data <- st_as_sf(x = data, coords = c("longitude", "latitude"), crs = 4326)
>>> library(gstat)
>>> vgm_utci <- variogram(UTCI~1, sf_data)
>>> utci_fit <- fit.variogram(vgm_utci, vgm("Gau"), fit.kappa = TRUE)
>>> plot(vgm_utci, utci_fit)
>>> istria <- read_sf(?./Istria_Boundary.shp")
>>> istria <- istria$geometry
>>> istria.grid <- istria %>%
>>>    st_make_grid(cellsize = 0.05, what = "centers") %>%
>>>    st_intersection(istria)
>>> library(ggplot2)
>>> ggplot() + geom_sf(data = istria) + geom_sf(data = istria.grid)
>>> library(stars)
>>> utci_krig <- krige(formula = sf_data$UTCI ~ 1, locations = sf_data,
>>>                     newdata = istria.grid, model = utci_fit)
>>>
>>>
>>> Thank you very much in advance,
>>>
>>> Nicola
>>> 	[[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Wed Aug 19 22:21:31 2020
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Wed, 19 Aug 2020 16:21:31 -0400
Subject: [R-sig-Geo] Count occurrences less memory expensive than
 superimpose function in several spatial objects
References: <e814793f-2d3c-5bd5-1a40-43c8093f2081.ref@yahoo.com.br>
Message-ID: <e814793f-2d3c-5bd5-1a40-43c8093f2081@yahoo.com.br>

Dear r-sig-geo Members,

 ??? I'll like to read several shapefiles, count occurrences in the same 
coordinate and create a final shapefile with a threshold number of 
occurrences. I try to convert the shapefiles in ppp object (because I 
have some part of my data set in shapefile and another in ppp objects) 
and applied superimpose function without success. In my synthetic example :

#Packages
library(spatstat)
library(dplyr)
library(sp)
library(rgdal)
library(raster)


#Point process example
data(ants)
ants.df<-as.data.frame(ants) #Convert to data frame

# Sample 75% in original dataset, repeat this 9 times and create a 
shapefile in each loop

for(i in 1:9){
s.ants.df<-sample_frac(ants.df, 0.75)
s.ants<-ppp(x=s.ants.df[,1],y=s.ants.df[,2],window=ants$window)#Create 
new ppp object
sample.pts<-cbind(s.ants$x,s.ants$y)
pts.sampling = SpatialPoints(sample.pts)
UTMcoor.df <- SpatialPointsDataFrame(pts.sampling, 
data.frame(id=1:length(pts.sampling)))
writeOGR(UTMcoor.df, ".",paste0('sample.shape',i), driver="ESRI 
Shapefile",overwrite=TRUE)
}

#Read all the 9 shapefiles created
all_shape <- list.files(pattern="\\.shp$", full.names=TRUE)
all_shape_list <- lapply(all_shape, shapefile)

#Convert shapefile to ppp statstat
target <- vector("list", length(all_shape_list))
for(i in 1:length(all_shape_list)){
target[[i]] <- ppp(x=all_shape_list[[i]]@coords[,1],
y=all_shape_list[[i]]@coords[,2],window=ants$window)}

#Join all ppp objects using multiplicity
target_sub<-do.call(superimpose,target)
res<-multiplicity(target_sub)

#Occurrences in the same coordinate > 5
res.xy<-as.data.frame(target_sub$x,target_sub$y,res)
res_F<-res.xy[res.xy$res>5,]

#Final shapefile
final.pts<-cbind(res_F[,1],res_F[,2])
pts.final = SpatialPoints(final.pts)
UTMcoor.df <- SpatialPointsDataFrame(pts.final, 
data.frame(id=1:length(pts.final)))
UTMcoor.df2 <-remove.duplicates(UTMcoor.df)
writeOGR(UTMcoor.df2, ".", paste0('final.ants'), driver="ESRI 
Shapefile",overwrite=TRUE)


This approach works very well in this synthetic example!!! But in my 
real data set a have the 99 shapefiles with 10^7 coordinates and when I 
try to use the do.call(superimpose,target) function my 32GB RAM memory 
crashed.

Please any ideas for how I can create a new shapefile with a criteria 
occurrences exposed but less memory expensive than superimpose all the 
objects created?

Thanks in advanced,
Alexandre

-- 
Alexandre dos Santos
Geotechnologies and Spatial Statistics applied to Forest Entomology
Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
Caixa Postal 244 (PO Box)
Avenida dos Ramires, s/n - Vila Real
Caceres - MT - CEP 78201-380 (ZIP code)
Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
Lattes CV: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
ResearchGate: www.researchgate.net/profile/Alexandre_Santos10
Publons: https://publons.com/researcher/3085587/alexandre-dos-santos/
--


	[[alternative HTML version deleted]]


From v|j@y|u||@ @end|ng |rom gm@||@com  Thu Aug 20 02:49:52 2020
From: v|j@y|u||@ @end|ng |rom gm@||@com (Vijay Lulla)
Date: Wed, 19 Aug 2020 20:49:52 -0400
Subject: [R-sig-Geo] Count occurrences less memory expensive than
 superimpose function in several spatial objects
In-Reply-To: <e814793f-2d3c-5bd5-1a40-43c8093f2081@yahoo.com.br>
References: <e814793f-2d3c-5bd5-1a40-43c8093f2081.ref@yahoo.com.br>
 <e814793f-2d3c-5bd5-1a40-43c8093f2081@yahoo.com.br>
Message-ID: <CAKkiGbvfcw9HVPDGUnCy6+mNmtPbyZDfE3XEDDzLbkam81P=qg@mail.gmail.com>

Hi Alexandre,
As far as I can tell (mostly from reading the docs...no prior experience of
using multiplicity or superimpose myself) it appears that they are just
calculating the number of unique values for a combination of x,y coordinate
pairs. So, you can do this by using the group by semantics of either
tidyverse or SQL to generate the res.xy data.frame. Below is an example of
generating res.xy alternatively using data.table (I'm not as familiar with
tidyverse):

target_sub1 <- rbindlist(lapply(target, as.data.table))
res1 <- target_sub1[, .(res=.N), by=.(x,y)]
res.xy1 = res1[target_sub1, on=c("x","y")]

all.equal(res.xy, res.xy1, check.attributes=FALSE) # should return TRUE

If you're using SQL then you just join the raw table with the grouped table
and you should get the table coordinates and occurrences. And, considering
the number of coordinates you have I recommend either data.table or SQL to
generate the final output.
HTH,
Vijay.


On Wed, Aug 19, 2020 at 4:22 PM ASANTOS via R-sig-Geo <
r-sig-geo at r-project.org> wrote:

> Dear r-sig-geo Members,
>
>  ??? I'll like to read several shapefiles, count occurrences in the same
> coordinate and create a final shapefile with a threshold number of
> occurrences. I try to convert the shapefiles in ppp object (because I
> have some part of my data set in shapefile and another in ppp objects)
> and applied superimpose function without success. In my synthetic example :
>
> #Packages
> library(spatstat)
> library(dplyr)
> library(sp)
> library(rgdal)
> library(raster)
>
>
> #Point process example
> data(ants)
> ants.df<-as.data.frame(ants) #Convert to data frame
>
> # Sample 75% in original dataset, repeat this 9 times and create a
> shapefile in each loop
>
> for(i in 1:9){
> s.ants.df<-sample_frac(ants.df, 0.75)
> s.ants<-ppp(x=s.ants.df[,1],y=s.ants.df[,2],window=ants$window)#Create
> new ppp object
> sample.pts<-cbind(s.ants$x,s.ants$y)
> pts.sampling = SpatialPoints(sample.pts)
> UTMcoor.df <- SpatialPointsDataFrame(pts.sampling,
> data.frame(id=1:length(pts.sampling)))
> writeOGR(UTMcoor.df, ".",paste0('sample.shape',i), driver="ESRI
> Shapefile",overwrite=TRUE)
> }
>
> #Read all the 9 shapefiles created
> all_shape <- list.files(pattern="\\.shp$", full.names=TRUE)
> all_shape_list <- lapply(all_shape, shapefile)
>
> #Convert shapefile to ppp statstat
> target <- vector("list", length(all_shape_list))
> for(i in 1:length(all_shape_list)){
> target[[i]] <- ppp(x=all_shape_list[[i]]@coords[,1],
> y=all_shape_list[[i]]@coords[,2],window=ants$window)}
>
> #Join all ppp objects using multiplicity
> target_sub<-do.call(superimpose,target)
> res<-multiplicity(target_sub)
>
> #Occurrences in the same coordinate > 5
> res.xy<-as.data.frame(target_sub$x,target_sub$y,res)
> res_F<-res.xy[res.xy$res>5,]
>
> #Final shapefile
> final.pts<-cbind(res_F[,1],res_F[,2])
> pts.final = SpatialPoints(final.pts)
> UTMcoor.df <- SpatialPointsDataFrame(pts.final,
> data.frame(id=1:length(pts.final)))
> UTMcoor.df2 <-remove.duplicates(UTMcoor.df)
> writeOGR(UTMcoor.df2, ".", paste0('final.ants'), driver="ESRI
> Shapefile",overwrite=TRUE)
>
>
> This approach works very well in this synthetic example!!! But in my
> real data set a have the 99 shapefiles with 10^7 coordinates and when I
> try to use the do.call(superimpose,target) function my 32GB RAM memory
> crashed.
>
> Please any ideas for how I can create a new shapefile with a criteria
> occurrences exposed but less memory expensive than superimpose all the
> objects created?
>
> Thanks in advanced,
> Alexandre
>
> --
> Alexandre dos Santos
> Geotechnologies and Spatial Statistics applied to Forest Entomology
> Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
> Caixa Postal 244 (PO Box)
> Avenida dos Ramires, s/n - Vila Real
> Caceres - MT - CEP 78201-380 (ZIP code)
> Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
> Lattes CV: http://lattes.cnpq.br/1360403201088680
> OrcID: orcid.org/0000-0001-8232-6722
> ResearchGate: www.researchgate.net/profile/Alexandre_Santos10
> Publons: https://publons.com/researcher/3085587/alexandre-dos-santos/
> --
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


-- 
Vijay Lulla, PhD
ORCID | <https://orcid.org/0000-0002-0823-2522> Homepage
<http://vlulla.github.io> | Google Scholar
<https://scholar.google.com/citations?user=VjhJWOgAAAAJ&hl=en> | Github
<https://github.com/vlulla>

	[[alternative HTML version deleted]]


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Thu Aug 20 17:18:19 2020
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Thu, 20 Aug 2020 11:18:19 -0400
Subject: [R-sig-Geo] Count occurrences less memory expensive than
 superimpose function in several spatial objects [SOLVED]
In-Reply-To: <CAKkiGbvfcw9HVPDGUnCy6+mNmtPbyZDfE3XEDDzLbkam81P=qg@mail.gmail.com>
References: <e814793f-2d3c-5bd5-1a40-43c8093f2081.ref@yahoo.com.br>
 <e814793f-2d3c-5bd5-1a40-43c8093f2081@yahoo.com.br>
 <CAKkiGbvfcw9HVPDGUnCy6+mNmtPbyZDfE3XEDDzLbkam81P=qg@mail.gmail.com>
Message-ID: <36f98583-ad35-3aa9-af8b-684a4a2d80b7@yahoo.com.br>

Thanks Vijay,

Now works and I don't to need pass my shapefiles objects to ppp for 
using the superimpose function. The solution now is:

#Packages library(spatstat) library(dplyr) library(sp) library(rgdal) 
library(raster) library(data.table) #Point process example data(ants) 
ants.df<-as.data.frame(ants) #Convert to data frame # Sample 75% in 
original dataset, repeat this 9 times and create a shapefile in each 
loop for(i in 1:9){ s.ants.df<-sample_frac(ants.df, 0.75) 
s.ants<-ppp(x=s.ants.df[,1],y=s.ants.df[,2],window=ants$window)#Create 
new ppp object sample.pts<-cbind(s.ants$x,s.ants$y) pts.sampling = 
SpatialPoints(sample.pts) UTMcoor.df <- 
SpatialPointsDataFrame(pts.sampling, 
data.frame(id=1:length(pts.sampling))) writeOGR(UTMcoor.df, 
".",paste0('sample.shape',i), driver="ESRI Shapefile",overwrite=TRUE) } 
#Read all the 9 shapefiles created all_shape <- 
list.files(pattern="\\.shp$", full.names=TRUE) all_shape_list <- 
lapply(all_shape, shapefile) # target_sub1 <- 
rbindlist(lapply(all_shape_list, as.data.table)) res1 <- target_sub1[, 
.(res=.N), by=.(coords.x1,coords.x2)] res.xy1 = res1[target_sub1, 
on=c("coords.x1","coords.x2")] all.equal(res.xy1, res.xy1, 
check.attributes=FALSE) # should return TRUE #Occurrences in the same 
coordinate > 5 res_F<-res.xy1[res.xy1$res>5,] 
res_F<-as.data.frame(res_F) #Final shapefile 
final.pts<-cbind(res_F[,1],res_F[,2]) pts.final = 
SpatialPoints(final.pts) UTMcoor.df <- SpatialPointsDataFrame(pts.final, 
data.frame(id=1:length(pts.final))) UTMcoor.df2 
<-remove.duplicates(UTMcoor.df) writeOGR(UTMcoor.df2, ".", 
paste0('final.ants'), driver="ESRI Shapefile", overwrite=TRUE) #<END>

Alexandre

-- 
Alexandre dos Santos
Geotechnologies and Spatial Statistics applied to Forest Entomology
Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
Caixa Postal 244 (PO Box)
Avenida dos Ramires, s/n - Vila Real
Caceres - MT - CEP 78201-380 (ZIP code)
Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
Lattes CV: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
ResearchGate: www.researchgate.net/profile/Alexandre_Santos10
Publons: https://publons.com/researcher/3085587/alexandre-dos-santos/
--

Em 19/08/2020 20:49, Vijay Lulla escreveu:
> Hi Alexandre,
> As far as I can tell (mostly from reading the docs...no prior experience of
> using multiplicity or superimpose myself) it appears that they are just
> calculating the number of unique values for a combination of x,y coordinate
> pairs. So, you can do this by using the group by semantics of either
> tidyverse or SQL to generate the res.xy data.frame. Below is an example of
> generating res.xy alternatively using data.table (I'm not as familiar with
> tidyverse):
>
> target_sub1 <- rbindlist(lapply(target, as.data.table))
> res1 <- target_sub1[, .(res=.N), by=.(x,y)]
> res.xy1 = res1[target_sub1, on=c("x","y")]
>
> all.equal(res.xy, res.xy1, check.attributes=FALSE) # should return TRUE
>
> If you're using SQL then you just join the raw table with the grouped table
> and you should get the table coordinates and occurrences. And, considering
> the number of coordinates you have I recommend either data.table or SQL to
> generate the final output.
> HTH,
> Vijay.
>
>
> On Wed, Aug 19, 2020 at 4:22 PM ASANTOS via R-sig-Geo <
> r-sig-geo at r-project.org> wrote:
>
>> Dear r-sig-geo Members,
>>
>>   ??? I'll like to read several shapefiles, count occurrences in the same
>> coordinate and create a final shapefile with a threshold number of
>> occurrences. I try to convert the shapefiles in ppp object (because I
>> have some part of my data set in shapefile and another in ppp objects)
>> and applied superimpose function without success. In my synthetic example :
>>
>> #Packages
>> library(spatstat)
>> library(dplyr)
>> library(sp)
>> library(rgdal)
>> library(raster)
>>
>>
>> #Point process example
>> data(ants)
>> ants.df<-as.data.frame(ants) #Convert to data frame
>>
>> # Sample 75% in original dataset, repeat this 9 times and create a
>> shapefile in each loop
>>
>> for(i in 1:9){
>> s.ants.df<-sample_frac(ants.df, 0.75)
>> s.ants<-ppp(x=s.ants.df[,1],y=s.ants.df[,2],window=ants$window)#Create
>> new ppp object
>> sample.pts<-cbind(s.ants$x,s.ants$y)
>> pts.sampling = SpatialPoints(sample.pts)
>> UTMcoor.df <- SpatialPointsDataFrame(pts.sampling,
>> data.frame(id=1:length(pts.sampling)))
>> writeOGR(UTMcoor.df, ".",paste0('sample.shape',i), driver="ESRI
>> Shapefile",overwrite=TRUE)
>> }
>>
>> #Read all the 9 shapefiles created
>> all_shape <- list.files(pattern="\\.shp$", full.names=TRUE)
>> all_shape_list <- lapply(all_shape, shapefile)
>>
>> #Convert shapefile to ppp statstat
>> target <- vector("list", length(all_shape_list))
>> for(i in 1:length(all_shape_list)){
>> target[[i]] <- ppp(x=all_shape_list[[i]]@coords[,1],
>> y=all_shape_list[[i]]@coords[,2],window=ants$window)}
>>
>> #Join all ppp objects using multiplicity
>> target_sub<-do.call(superimpose,target)
>> res<-multiplicity(target_sub)
>>
>> #Occurrences in the same coordinate > 5
>> res.xy<-as.data.frame(target_sub$x,target_sub$y,res)
>> res_F<-res.xy[res.xy$res>5,]
>>
>> #Final shapefile
>> final.pts<-cbind(res_F[,1],res_F[,2])
>> pts.final = SpatialPoints(final.pts)
>> UTMcoor.df <- SpatialPointsDataFrame(pts.final,
>> data.frame(id=1:length(pts.final)))
>> UTMcoor.df2 <-remove.duplicates(UTMcoor.df)
>> writeOGR(UTMcoor.df2, ".", paste0('final.ants'), driver="ESRI
>> Shapefile",overwrite=TRUE)
>>
>>
>> This approach works very well in this synthetic example!!! But in my
>> real data set a have the 99 shapefiles with 10^7 coordinates and when I
>> try to use the do.call(superimpose,target) function my 32GB RAM memory
>> crashed.
>>
>> Please any ideas for how I can create a new shapefile with a criteria
>> occurrences exposed but less memory expensive than superimpose all the
>> objects created?
>>
>> Thanks in advanced,
>> Alexandre
>>
>> --
>> Alexandre dos Santos
>> Geotechnologies and Spatial Statistics applied to Forest Entomology
>> Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
>> Caixa Postal 244 (PO Box)
>> Avenida dos Ramires, s/n - Vila Real
>> Caceres - MT - CEP 78201-380 (ZIP code)
>> Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
>> Lattes CV: http://lattes.cnpq.br/1360403201088680
>> OrcID: orcid.org/0000-0001-8232-6722
>> ResearchGate: www.researchgate.net/profile/Alexandre_Santos10
>> Publons: https://publons.com/researcher/3085587/alexandre-dos-santos/
>> --
>>
>>
>>          [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>

	[[alternative HTML version deleted]]


From @dr|@n@b@dde|ey @end|ng |rom curt|n@edu@@u  Sat Aug 22 13:04:13 2020
From: @dr|@n@b@dde|ey @end|ng |rom curt|n@edu@@u (Adrian Baddeley)
Date: Sat, 22 Aug 2020 11:04:13 +0000
Subject: [R-sig-Geo] Count occurrences less memory expensive than
 superimpose function in several spatial objects
Message-ID: <MEAPR01MB2232C1EADCBFB9236DBF7816A4580@MEAPR01MB2232.ausprd01.prod.outlook.com>

Alexandre Santos writes:

 > I'll like to read several shapefiles, count occurrences in the same
 > coordinate and create a final shapefile with a threshold number of
 > occurrences. I try to convert the shapefiles in ppp object (because I
 > have some part of my data set in shapefile and another in ppp objects)
 > and applied superimpose function [.... ]

The function 'superimpose' in the spatstat package is generic, with methods for 'ppp' and 'default'.

Your example code applies 'superimpose' to a list of objects of class 'ppp'.
This uses the method 'superimpose.ppp' which applies to objects of class 'ppp'
and constructs a new object of class 'ppp'. This task includes computing the appropriate "observation window"
(a component of the 'ppp' structure) from the observation windows of the input patterns.
There is an option in 'superimpose.ppp' to specify the observation window of the result.
You didn't use this option, so you're expecting the function 'superimpose.ppp' to calculate the
appropriate window. When you have many objects with complicated windows, this will take a lot of time.

To make this go faster you could simply extract the (x,y) coordinates of the objects using coords() or as.data.frame().
Then call 'superimpose' on these data frames which will invoke superimpose.default which will concatenate the
(x,y) coordinate lists very quickly.

If I understand correctly, your ultimate goal is to have a list of the unique (x,y) points and their multiplicities.

If you have already superimposed (concatenated) the x, y coordinate lists, then you can calculate the multiplicities
with 'table' , or the spatstat function 'uniquemap' (the latter function is extremely fast)

However, you don't need to concatenate all the coordinates of all the point patterns before calculating multiplicities.
In big data applications it would be more efficient to process each point pattern dataset first,
determining the unique (x,y) points and their multiplicities within each point pattern,
and then to merge the results from the different point patterns. Something like this,
if 'Plist' is your list of point patterns:

       # process each point pattern
        Vlist <- lapply(unname(Plist),
        function(P) {
               xy <- as.data.frame(P)[,c("x","y")]
               um <- uniquemap(xy)
               isun <- (um == seq_along(um))
               mul <- table(um)
               return(cbind(xy[isun, , drop=FALSE], m=mul))
       })
       # concatenate results from all patterns
       V <- do.call(rbind, Vlist)
       # find unique points
       um <- uniquemap(V[,c("x","y")])
       isun <- (um == seq_along(um))
       U <- V[isun, c("x", "y")]
       m <- tapply(V$m, factor(um), sum)

Then U contains the unique locations and m is the multiplicities.




Prof Adrian Baddeley HonDSc FAA

John Curtin Distinguished Professor

School of Electrical Engineering, Computing and Mathematical Sciences

Curtin University, Perth, Western Australia


I work Wednesdays and Thursdays only

	[[alternative HTML version deleted]]


From @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br  Fri Aug 28 03:17:58 2020
From: @|ex@ndre@@nto@br @end|ng |rom y@hoo@com@br (ASANTOS)
Date: Thu, 27 Aug 2020 21:17:58 -0400
Subject: [R-sig-Geo] Distance rule that filtering a set of points in "ppp"
 class by minimum or maximum distance
References: <a3951256-697a-bb29-41f6-bf6b434dea51.ref@yahoo.com.br>
Message-ID: <a3951256-697a-bb29-41f6-bf6b434dea51@yahoo.com.br>

Dear R-Sig_Geo Members,

I'd like to find a more simple way to filtering a set of points in a 
"ppp" object by minimum or maximum distance.

In my example using a ants ("ppp" object) in spatstat package:

#Packages
library(spatstat)
library(sp)

#Point process example
data(ants)
str(ants)
#- attr(*, "class")= chr "ppp"

#Computes the matrix of distances between all pairs of ants nests and 
selection of more than 20 m distance neighborhood
ants.df<-as.data.frame(ants)
coordinates(ants.df) <- ~x+y
ants.dmat <- spDists(ants.df) #Calculate a distance matrix using spDists
min.dist <- 20 #distance range
ants.dmat[ants.dmat <= min.dist] <- NA # Na for less than 20 meters

Here yet I need to start a relative long code for identified columns 
with NA in ants.dmat object, create a new data frame and than create a 
new ppp object based in the coordinates and marks etc...

Is there any more easy way for create a new "ppp" object just only 
specified a distance rule that filtering a set of points in a original 
"ppp" object by minimum or maximum distance?

Thanks in advanced,

Alexandre

-- 
Alexandre dos Santos
Geotechnologies and Spatial Statistics applied to Forest Entomology
Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
Caixa Postal 244 (PO Box)
Avenida dos Ramires, s/n - Vila Real
Caceres - MT - CEP 78201-380 (ZIP code)
Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
Lattes CV: http://lattes.cnpq.br/1360403201088680
OrcID: orcid.org/0000-0001-8232-6722
ResearchGate: www.researchgate.net/profile/Alexandre_Santos10
Publons: https://publons.com/researcher/3085587/alexandre-dos-santos/
--


	[[alternative HTML version deleted]]


From @@bev@n @end|ng |rom uc|@@c@uk  Fri Aug 28 03:18:59 2020
From: @@bev@n @end|ng |rom uc|@@c@uk (Bevan, Andrew)
Date: Fri, 28 Aug 2020 01:18:59 +0000
Subject: [R-sig-Geo] c884
In-Reply-To: <a3951256-697a-bb29-41f6-bf6b434dea51@yahoo.com.br>
References: <a3951256-697a-bb29-41f6-bf6b434dea51.ref@yahoo.com.br>
 <a3951256-697a-bb29-41f6-bf6b434dea51@yahoo.com.br>
Message-ID: <F3950BB9-66DF-40A2-895A-37D17D40A11B@ucl.ac.uk>

I am on leave until mid-December 2020 and will only be checking emails very intermittently.

If you are a prospective student and have queries about the MSc in Computational Archaeology, please email Mark Lake (mark.lake at ucl.ac.uk).

On 28 Aug 2020, at 02:17, ASANTOS via R-sig-Geo <r-sig-geo at r-project.org> wrote:

Dear R-Sig_Geo Members,

I'd like to find a more simple way to filtering a set of points in a
"ppp" object by minimum or maximum distance.

In my example using a ants ("ppp" object) in spatstat package:

#Packages
library(spatstat)
library(sp)

#Point process example
data(ants)
str(ants)
#- attr(*, "class")= chr "ppp"

#Computes the matrix of distances between all pairs of ants nests and
selection of more than 20 m distance neighborhood
ants.df<-as.data.frame(ants)
coordinates(ants.df) <- ~x+y
ants.dmat <- spDists(ants.df) #Calculate a distance matrix using spDists
min.dist <- 20 #distance range
ants.dmat[ants.dmat <= min.dist] <- NA # Na for less than 20 meters

Here yet I need to start a relative long code for identified columns
with NA in ants.dmat object, create a new data frame and than create a
new ppp object based in the coordinates and marks etc...

Is there any more easy way for create a new "ppp" object just only
specified a distance rule that filtering a set of points in a original
"ppp" object by minimum or maximum distance?

Thanks in advanced,

Alexandre

--
Alexandre dos Santos
Geotechnologies and Spatial Statistics applied to Forest Entomology
Instituto Federal de Mato Grosso (IFMT) - Campus Caceres
Caixa Postal 244 (PO Box)
Avenida dos Ramires, s/n - Vila Real
Caceres - MT - CEP 78201-380 (ZIP code)
Phone: (+55) 65 99686-6970 / (+55) 65 3221-2674
Lattes CV: https://eur01.safelinks.protection.outlook.com/?url=http%3A%2F%2Flattes.cnpq.br%2F1360403201088680&amp;data=02%7C01%7C%7Ca70f07e55bbb49d60ec008d84af0479d%7C1faf88fea9984c5b93c9210a11d9a5c2%7C0%7C0%7C637341743158107956&amp;sdata=yE4%2Fk76Ya9lntZoXFJsAUHg55rl%2BBVLBHuFk95Q4uc8%3D&amp;reserved=0
OrcID: orcid.org/0000-0001-8232-6722
ResearchGate: https://eur01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAlexandre_Santos10&amp;data=02%7C01%7C%7Ca70f07e55bbb49d60ec008d84af0479d%7C1faf88fea9984c5b93c9210a11d9a5c2%7C0%7C0%7C637341743158107956&amp;sdata=hLw2XYvqjhMn%2FuJOXmYu%2BHUKrLFgrZaxoSj6%2Bl%2BIm6Q%3D&amp;reserved=0
Publons: https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fpublons.com%2Fresearcher%2F3085587%2Falexandre-dos-santos%2F&amp;data=02%7C01%7C%7Ca70f07e55bbb49d60ec008d84af0479d%7C1faf88fea9984c5b93c9210a11d9a5c2%7C0%7C0%7C637341743158107956&amp;sdata=o%2FR75Akr9VE%2BF6GLEWHcEsbRtoRkLCkjsBk%2BXH%2F5yLc%3D&amp;reserved=0
--


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-sig-geo&amp;data=02%7C01%7C%7Ca70f07e55bbb49d60ec008d84af0479d%7C1faf88fea9984c5b93c9210a11d9a5c2%7C0%7C0%7C637341743158107956&amp;sdata=%2Fhi%2Fkda%2FniyJ0GuRmnZvoBPl92%2BwmORhGcZFNShiNRc%3D&amp;reserved=0

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri Aug 28 13:37:41 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Fri, 28 Aug 2020 23:37:41 +1200
Subject: [R-sig-Geo] 
 Distance rule that filtering a set of points in "ppp"
 class by minimum or maximum distance
In-Reply-To: <a3951256-697a-bb29-41f6-bf6b434dea51@yahoo.com.br>
References: <a3951256-697a-bb29-41f6-bf6b434dea51.ref@yahoo.com.br>
 <a3951256-697a-bb29-41f6-bf6b434dea51@yahoo.com.br>
Message-ID: <20200828233741.7946b1a4@rolf-Latitude-E7470>


On Thu, 27 Aug 2020 21:17:58 -0400
ASANTOS via R-sig-Geo <r-sig-geo at r-project.org> wrote:

> Dear R-Sig_Geo Members,
> 
> I'd like to find a more simple way to filtering a set of points in a 
> "ppp" object by minimum or maximum distance.
> 
> In my example using a ants ("ppp" object) in spatstat package:
> 
> #Packages
> library(spatstat)
> library(sp)
> 
> #Point process example
> data(ants)
> str(ants)
> #- attr(*, "class")= chr "ppp"
> 
> #Computes the matrix of distances between all pairs of ants nests and 
> selection of more than 20 m distance neighborhood
> ants.df<-as.data.frame(ants)
> coordinates(ants.df) <- ~x+y
> ants.dmat <- spDists(ants.df) #Calculate a distance matrix using
> spDists min.dist <- 20 #distance range
> ants.dmat[ants.dmat <= min.dist] <- NA # Na for less than 20 meters
> 
> Here yet I need to start a relative long code for identified columns 
> with NA in ants.dmat object, create a new data frame and than create
> a new ppp object based in the coordinates and marks etc...
> 
> Is there any more easy way for create a new "ppp" object just only 
> specified a distance rule that filtering a set of points in a
> original "ppp" object by minimum or maximum distance?
> 
> Thanks in advanced,
> 
> Alexandre
> 

It's very easy! :-)

ddd <- nndist(ants)
farAnts <- ants[ddd >= 20]

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From P|etro_te|@to @end|ng |rom hotm@||@com  Fri Aug 28 20:09:55 2020
From: P|etro_te|@to @end|ng |rom hotm@||@com (Pietro Andre Telatin Paschoalino)
Date: Fri, 28 Aug 2020 18:09:55 +0000
Subject: [R-sig-Geo] Doubt in Extract function (raster)
Message-ID: <BN8PR17MB298010203675B068ED2902BF82520@BN8PR17MB2980.namprd17.prod.outlook.com>

Hello everyone,

I have a simple question regarding extracting climatic data from raster to shape (polygons).

I am extracting climatic data, for example precipitation, from a raster (nc file) following the  structure:

fn <-file.path ("mypath")

ncfile <- nc_open (fn)
rasbrick <- stack (fn)

how it is a multiband and I want just a few of them:

rasbrick <- rasbrick [[70:71]]

finally, I extract to the columns that I want:

for (i in 1: length (rasbrick @ layers)) {
  weath_dt [, 4 + i] = raster :: extract (rasbrick [[i]], shape2, mean)
}

Could you confirm what I understand in the R documentation?

  *   In this case, I am extracting only the simple average of the values ??of the cells that have centroid inside the polygon, correct?


  *   Would you know to inform me why, when I ask for it to be weighted by the area of ??the cell within the polygon (weights = TRUE) I receive more missing values ??than asking just the average? Shouldn't it be the other way around?

Sorry if the question is too basic. If necessary I can try to do some example, but I believe it must be a "general" question, not depending much on my data, but maybe I'm wrong.

Thank you very much.

Pietro Andre Telatin Paschoalino
Doutorando em Ci?ncias Econ?micas da Universidade Estadual de Maring? - PCE.

[https://ipmcdn.avast.com/images/icons/icon-envelope-tick-round-orange-animated-no-repeat-v1.gif]<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=icon>    Virus-free. www.avast.com<https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail&utm_term=link>

	[[alternative HTML version deleted]]


From bouchet@@oph|e @end|ng |rom |ree@|r  Mon Aug 31 14:26:07 2020
From: bouchet@@oph|e @end|ng |rom |ree@|r (Sophie Bouchet)
Date: Mon, 31 Aug 2020 14:26:07 +0200
Subject: [R-sig-Geo] unsuscribe
Message-ID: <CALR7N39bhxyzMtHdOsB77Uao4nc79wNdm_TE1NPJ+70QfJkxBQ@mail.gmail.com>

Hello,

Can you unsubscribe me?

Thanks

	[[alternative HTML version deleted]]


From @cottmh@@g @end|ng |rom gm@||@com  Mon Aug 31 16:07:15 2020
From: @cottmh@@g @end|ng |rom gm@||@com (Scott Haag)
Date: Mon, 31 Aug 2020 10:07:15 -0400
Subject: [R-sig-Geo] Could you please unsubscribe me?
Message-ID: <CAEMP_RurMvt1TyMk8rZQBkTSTG0kaoneiBXJVx6Qz1Zkk+rLrw@mail.gmail.com>



	[[alternative HTML version deleted]]


