From cch|@en| @end|ng |rom gm@||@com  Mon Dec  2 12:00:33 2019
From: cch|@en| @end|ng |rom gm@||@com (Chanda Chiseni)
Date: Mon, 2 Dec 2019 12:00:33 +0100
Subject: [R-sig-Geo] Creating Spatial Weight Matrices with Large Data
Message-ID: <CAA5N_eZBvu4CuzH4L8+XOduE5mMxrjFO+Sr2E6O1KG9aiQzJXw@mail.gmail.com>

I am currently working with a census data that has about 758 000
individuals. I am trying to create a spatial weight matrix using the X-Y
coordinates for their place of birth . However, i am running into problems
when I try to create the nb type weights matrix using the poly2nb, R is
taking super long and after running for a long time it crushes. I have
increased R's memory size to about 80000 but this is still not working.

Is there a way i can get around this problem? If anyone has any ideas on
how i can create a spatial weight matrix for such a large data set please
help.

Kind Regards,


Michael Chanda Chiseni

Phd Candidate

Department of Economic History

Lund University

Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund



*Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Dec  2 13:00:45 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 2 Dec 2019 13:00:45 +0100
Subject: [R-sig-Geo] Creating Spatial Weight Matrices with Large Data
In-Reply-To: <CAA5N_eZBvu4CuzH4L8+XOduE5mMxrjFO+Sr2E6O1KG9aiQzJXw@mail.gmail.com>
References: <CAA5N_eZBvu4CuzH4L8+XOduE5mMxrjFO+Sr2E6O1KG9aiQzJXw@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1912021254050.2897@reclus.nhh.no>

On Mon, 2 Dec 2019, Chanda Chiseni wrote:

> I am currently working with a census data that has about 758 000
> individuals. I am trying to create a spatial weight matrix using the X-Y
> coordinates for their place of birth . However, i am running into problems
> when I try to create the nb type weights matrix using the poly2nb, R is
> taking super long and after running for a long time it crushes. I have
> increased R's memory size to about 80000 but this is still not working.

Please provide the (shortened) code used. poly2nb() is used for polygons, 
not points. If you were using distances between points, you may have used 
a distance threshold such that many observations have many neighbours. 
Also ask yourself whether this is not a multi-level problem, in that 
spatial interactions perhaps occur between aggregates of observations, not 
the observations themselves.

>
> Is there a way i can get around this problem? If anyone has any ideas on
> how i can create a spatial weight matrix for such a large data set please
> help.

An nb object (and listw) are just lists of length n, so a neighbour object 
with 800K observations and 4 neighbours each only takes about 13MB, the 
listw takes 38MB. What you can use them for may be another problem, and 
much of the data may actually simply be noise not signal.

Roger

>
> Kind Regards,
>
>
> Michael Chanda Chiseni
>
> Phd Candidate
>
> Department of Economic History
>
> Lund University
>
> Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund
>
>
>
> *Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From @erd@r|@p|r @end|ng |rom gm@||@com  Mon Dec  2 20:08:56 2019
From: @erd@r|@p|r @end|ng |rom gm@||@com (=?UTF-8?Q?Serdar_=C4=B0spir?=)
Date: Mon, 2 Dec 2019 21:08:56 +0200
Subject: [R-sig-Geo] spdep package
Message-ID: <CAAQ+6ee-jfML=ALvddPvFs-wznv9AhLXu8bgWC9U7tTTSOiSMA@mail.gmail.com>

Hello,
I am Serdar Ispir, from Turkey. I wanted to install spdep package to my
virtual machine with Ubuntu 18.04 OS but I got the following error
messages. If you give any suggestions to me about the problem, that would
be perfect.
Thanks in advance.
Best regards,
Serdar

Warning messages:1: In install.packages("spdep") : installation of package
?units? had non-zero exit status2: In install.packages("spdep") :
installation of package ?sf? had non-zero exit status3: In
install.packages("spdep") : installation of package ?spdep? had non-zero
exit status

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Dec  2 20:14:58 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 2 Dec 2019 20:14:58 +0100
Subject: [R-sig-Geo] spdep package
In-Reply-To: <CAAQ+6ee-jfML=ALvddPvFs-wznv9AhLXu8bgWC9U7tTTSOiSMA@mail.gmail.com>
References: <CAAQ+6ee-jfML=ALvddPvFs-wznv9AhLXu8bgWC9U7tTTSOiSMA@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1912022012100.19766@reclus.nhh.no>

On Mon, 2 Dec 2019, Serdar ?spir wrote:

> Hello,
> I am Serdar Ispir, from Turkey. I wanted to install spdep package to my
> virtual machine with Ubuntu 18.04 OS but I got the following error
> messages. If you give any suggestions to me about the problem, that would
> be perfect.

Refer to installation instructions for the sf package, or rather install 
units first, then sf, and only when that works try spdep. You need all the 
external dependencies listed for units and for sf 
(https://r-spatial.github.io/sf/).

Roger

> Thanks in advance.
> Best regards,
> Serdar
>
> Warning messages:1: In install.packages("spdep") : installation of package
> ?units? had non-zero exit status2: In install.packages("spdep") :
> installation of package ?sf? had non-zero exit status3: In
> install.packages("spdep") : installation of package ?spdep? had non-zero
> exit status
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From cch|@en| @end|ng |rom gm@||@com  Tue Dec  3 11:40:55 2019
From: cch|@en| @end|ng |rom gm@||@com (Chanda Chiseni)
Date: Tue, 3 Dec 2019 11:40:55 +0100
Subject: [R-sig-Geo] Creating Spatial Weight Matrices with Large Data
In-Reply-To: <alpine.LFD.2.21.1912021254050.2897@reclus.nhh.no>
References: <CAA5N_eZBvu4CuzH4L8+XOduE5mMxrjFO+Sr2E6O1KG9aiQzJXw@mail.gmail.com>
 <alpine.LFD.2.21.1912021254050.2897@reclus.nhh.no>
Message-ID: <CAA5N_ebRtPyHu4jo=WacGG667+LcTsC2YpC=-hBFd2pTpE16uQ@mail.gmail.com>

 Hi Roger

Thank you for your very helpful feedback. I was indeed treating my point
data as polygons and did not impose a distance thresh hold.Essentially, as
you stated, many observations had many neighbors. I have since tried to you
K-neighbors and imposed a restriction of k=4. However, this is still taking
a bit long.

 #Increasing the memory capacity
 memory.limit(size = 80000)
 ## defining data
 censusdata= CensusFinal_Analysis_R1

#Creating Matrix of Coordinates
 sp_point <- cbind(censusdata$X, censusdata$Y)

colnames(sp_point)= c("Long","Lat")
head(sp_point)

## Create the K nearest neighbour
censusdata.4nn = knearneigh(sp_point,k=4,longlat = TRUE)

I get stuck at the stage where i try to create the K nearest neighbor, the
operation is quite slow. Am i still doing something wrong?


Kind Regards,

Michael Chanda Chiseni

Phd Candidate

Department of Economic History

Lund University

Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund



*Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *






On Mon, Dec 2, 2019 at 1:00 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Mon, 2 Dec 2019, Chanda Chiseni wrote:
>
> > I am currently working with a census data that has about 758 000
> > individuals. I am trying to create a spatial weight matrix using the X-Y
> > coordinates for their place of birth . However, i am running into
> problems
> > when I try to create the nb type weights matrix using the poly2nb, R is
> > taking super long and after running for a long time it crushes. I have
> > increased R's memory size to about 80000 but this is still not working.
>
> Please provide the (shortened) code used. poly2nb() is used for polygons,
> not points. If you were using distances between points, you may have used
> a distance threshold such that many observations have many neighbours.
> Also ask yourself whether this is not a multi-level problem, in that
> spatial interactions perhaps occur between aggregates of observations, not
> the observations themselves.
>
> >
> > Is there a way i can get around this problem? If anyone has any ideas on
> > how i can create a spatial weight matrix for such a large data set please
> > help.
>
> An nb object (and listw) are just lists of length n, so a neighbour object
> with 800K observations and 4 neighbours each only takes about 13MB, the
> listw takes 38MB. What you can use them for may be another problem, and
> much of the data may actually simply be noise not signal.
>
> Roger
>
> >
> > Kind Regards,
> >
> >
> > Michael Chanda Chiseni
> >
> > Phd Candidate
> >
> > Department of Economic History
> >
> > Lund University
> >
> > Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund
> >
> >
> >
> > *Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Dec  3 11:52:51 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 3 Dec 2019 11:52:51 +0100
Subject: [R-sig-Geo] Creating Spatial Weight Matrices with Large Data
In-Reply-To: <CAA5N_ebRtPyHu4jo=WacGG667+LcTsC2YpC=-hBFd2pTpE16uQ@mail.gmail.com>
References: <CAA5N_eZBvu4CuzH4L8+XOduE5mMxrjFO+Sr2E6O1KG9aiQzJXw@mail.gmail.com>
 <alpine.LFD.2.21.1912021254050.2897@reclus.nhh.no>
 <CAA5N_ebRtPyHu4jo=WacGG667+LcTsC2YpC=-hBFd2pTpE16uQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1912031151050.29869@reclus.nhh.no>

On Tue, 3 Dec 2019, Chanda Chiseni wrote:

> Hi Roger
>
> Thank you for your very helpful feedback. I was indeed treating my point
> data as polygons and did not impose a distance thresh hold.Essentially, as
> you stated, many observations had many neighbors. I have since tried to you
> K-neighbors and imposed a restriction of k=4. However, this is still taking
> a bit long.
>
> #Increasing the memory capacity
> memory.limit(size = 80000)
> ## defining data
> censusdata= CensusFinal_Analysis_R1
>
> #Creating Matrix of Coordinates
> sp_point <- cbind(censusdata$X, censusdata$Y)
>
> colnames(sp_point)= c("Long","Lat")
> head(sp_point)
>
> ## Create the K nearest neighbour
> censusdata.4nn = knearneigh(sp_point,k=4,longlat = TRUE)

Don't use geographical coordinates. Project first, then K-nearest 
neighbours uses RANN, which is fast (Euclidean as against Great Circle 
distances).

Roger

>
> I get stuck at the stage where i try to create the K nearest neighbor, the
> operation is quite slow. Am i still doing something wrong?
>
>
> Kind Regards,
>
> Michael Chanda Chiseni
>
> Phd Candidate
>
> Department of Economic History
>
> Lund University
>
> Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund
>
>
>
> *Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *
>
>
>
>
>
>
> On Mon, Dec 2, 2019 at 1:00 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Mon, 2 Dec 2019, Chanda Chiseni wrote:
>>
>>> I am currently working with a census data that has about 758 000
>>> individuals. I am trying to create a spatial weight matrix using the X-Y
>>> coordinates for their place of birth . However, i am running into
>> problems
>>> when I try to create the nb type weights matrix using the poly2nb, R is
>>> taking super long and after running for a long time it crushes. I have
>>> increased R's memory size to about 80000 but this is still not working.
>>
>> Please provide the (shortened) code used. poly2nb() is used for polygons,
>> not points. If you were using distances between points, you may have used
>> a distance threshold such that many observations have many neighbours.
>> Also ask yourself whether this is not a multi-level problem, in that
>> spatial interactions perhaps occur between aggregates of observations, not
>> the observations themselves.
>>
>>>
>>> Is there a way i can get around this problem? If anyone has any ideas on
>>> how i can create a spatial weight matrix for such a large data set please
>>> help.
>>
>> An nb object (and listw) are just lists of length n, so a neighbour object
>> with 800K observations and 4 neighbours each only takes about 13MB, the
>> listw takes 38MB. What you can use them for may be another problem, and
>> much of the data may actually simply be noise not signal.
>>
>> Roger
>>
>>>
>>> Kind Regards,
>>>
>>>
>>> Michael Chanda Chiseni
>>>
>>> Phd Candidate
>>>
>>> Department of Economic History
>>>
>>> Lund University
>>>
>>> Visiting address: Alfa 1, Scheelev?gen 15 B, 22363 Lund
>>>
>>>
>>>
>>> *Africa is not poor, it is poorly managed (Ellen Johnson-Sirleaf ). *
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From u@erc@tch @end|ng |rom out|ook@com  Wed Dec  4 00:34:03 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Tue, 3 Dec 2019 23:34:03 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <3cd7666ef730416c8e7f6f4110315f0a@nhh.no>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07689051B2D54AD121BE7E63B07E0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911061131380.71006@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
 <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911262040120.583221@reclus.nhh.no>,
 <VI1P190MB07680F075EA1B55A9AE9D56AB0450@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <3cd7666ef730416c8e7f6f4110315f0a@nhh.no>
Message-ID: <VI1P190MB076828E385B2E65EB3CD4093B0420@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Roger,

Again, thank you for your answer. What do you mean by "zip code random effect"? You mean I should use in plm the model "random"?

regression_re <- plm(formula = model, data = listings, model = "random", index = c("id", "date_compiled"))

And any other methodology in dealing with large weight matrices in spatialreg::lagsarlm?

Thank you and best regards,
Robert

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Wednesday, November 27, 2019 13:53
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: SV: [R-sig-Geo] Spatial Autocorrelation Estimation Method

Yes this is expected, since the # neighbours in a single zip code block is a dense matrix, and there will be multiple such matrices. (15000^2)*8 is 1.8e+09 so such a dense matrix will max out your RAM. There is no way to look at block neighbours in that format without subsetting your data (think train/test), use a zip code random effect. I would certainly drop all attempts to examine spatial dependency until you get an aspatial multilevel hedonic model working.

Roger

--
Roger Bivand
Norwegian School of Economics
Helleveien 30, 5045 Bergen, Norway
Roger.Bivand at nhh.no


________________________________________
Fra: Robert R <usercatch at outlook.com>
Sendt: tirsdag 26. november 2019 21.04
Til: Roger Bivand
Kopi: r-sig-geo at r-project.org
Emne: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

Dear Roger,

Thank you for your e-mail. Actually there is less noise that it seems. Rental prices are daily rental prices and I have an extract of all Airbnb listings daily prices once a month for a period of 4 years. Each listings information contains the lat, lon, number of bedrooms, category (entire home/apt, shared room or private room), etc.

One question regarding the spdep::nb2blocknb function: it runs super fast with up to n = 1000, and always crashes my R session with n = 15000 or so. Is there an alternative to solve this problem?

Thank you and best regards,
Robert

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Tuesday, November 26, 2019 20:48
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

Sorry for late reply, am indisposed and unable to help further. I feel
that there is so much noise in your data (differences in offers, rental
lengths, repeats or not, etc.), that you will certainly have to subset
vigorously first to isolate response cases that are comparable. What you
are trying to disentangle are the hedonic components in the bundle where
you just have price as response, but lots of other bundle characteristics
on the right hand side (days, etc.). I feel you'd need to try to get to a
response index of price per day per rental area or some such. I'd
certainly advise examining responses to a specific driver (major concert
or sports event) to get a feel for how the market responds, and return to
spatial hedonic after finding an approach that gives reasonable aspatial
outcomes.

Roger

On Sun, 17 Nov 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your message and sorry for my late answer.
>
> Regarding the number of listings (lettings) for my data set (2.216.642 observations), each listing contains an individual id:
>
> unique ids: 180.004
> time periods: 54 (2015-01 to 2019-09)
> number of ids that appear only once: 28.486 (of 180.004 ids) (15,8%)
> number of ids that appear/repeat 2-10 times: 82.641 (of 180.004 ids) (45,9%)
> number of ids that appear/repeat 11-30 times: 46.465 (of 180.004 ids) (25,8%)
> number of ids that appear/repeat 31-54 times: 22.412 (of 180.004 ids) (12,5%)
>
> Important to notice is that hosts can change the room_category (between entire/home apt, private room and shared room) keeping the same listing id number. In my data, the number of unique ids that in some point changed the room_type is of 7.204 ids.
>
> --
>
> For the OLS model, I was using only a fixed effect model, where each time period (date_compiled) (54 in total) is a time dummy.
>
> plm::plm(formula = model, data = listings, model = "pooling", index = c("id", "date_compiled"))
>
>
> --
> Osland et al. (2016) (https://doi.org/10.1111/jors.12281) use a spatial fixed effects (SFE) hedonic model, where each defined neighborhood zone in the study area is represented by dummy variables.
>
> Dong et al. (2015) (https://doi.org/10.1111/gean.12049) outline four model specifications to accommodate geographically hierarchical data structures: (1) groupwise W and fixed regional effects; (2) groupwise W and random regional effects; (3) proximity-based W and fixed regional effects; and (4) proximity-based W and random regional effects.
> --
>
> I created a new column/variable containing the borough where the zipcode is found (Manhattan, Brooklyn, Queens, Bronx, Staten Island).
>
> If I understood it right, the (two-level) Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) considers the occurrence of spatial relations at the (lower) individual (geographical coordinates - in my case, the listing location) and (higher) group level (territorial units - in my case, zipcodes).
>
> According to Bivand et al. (2017): "(...) W is a spatial weights matrix. The HSAR model may also be estimated without this component.". So, in this case I only estimate the Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) in a "one-level" basis, i.e., at the higher-level.
>
> HSAR::hsar(model, data = listings, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>
> (Where the "model" formula contains the 54 time dummy variables)
>
> Do you think I can proceed with this model? I was able to calculate it.
>
> If I remove all observations/rows with NAs in one of the chosen variables/observations, 884.183 observations remain. If I would create a W matrix for HSAR::hsar, I would have a gigantic 884.183 by 884.183 matrix. This is the reason why I put W = NULL.
>
>
> Thank you and best regards
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Monday, November 11, 2019 11:31
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Sun, 10 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Again, thank you for your answer. I read the material provided and
>> decided that Hierarchical Spatial Autoregressive (HSAR) could be the
>> right model for me.
>>
>> I indeed have the precise latitude and longitude information for all my
>> listings for NYC.
>>
>> I created a stratified sample (group = zipcode) with 22172 (1%) of my
>> observations called listings_sample and tried to replicate the hsar
>> model, please see below.
>>
>> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.
>
> Unless you know definitely that you want to relate the response to its
> lagged value, you do not need this. Do note that the matrix is very
> sparse, so could be fitted without difficulty with ML in a cross-sectional
> model.
>
>>
>> You recommended then to introduce a Markov random field (MRF) random
>> effect (RE) at the zipcode level, but I did not understand it so well.
>> Could you develop a litte more?
>>
>
> Did you read the development in
> https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and
> includes code for fitting the Beijing housing parcels data se from HSAR
> with many other packages (MCMC, INLA, hglm, etc.). I guess that you should
> try to create a model that works on a single borough, sing the zipcodes
> in that borough as a proxy for unobserved neighbourhood effects. Try for
> example using lme4::lmer() with only a zipcode IID random effect, see if
> the hedonic estimates are similar to lm(), and leave adding an MRF RE
> (with for example mgcv::gam() or hglm::hglm()) until you have a working
> testbed. Then advance step-by-step from there.
>
> You still have not said how many repeat lettings you see - it will affect
> the way you specify your model.
>
> Roger
>
>> ##############
>> library(spdep)
>> library(HSAR)
>> library(dplyr)
>> library(splitstackshape)
>>
>>
>> # Stratified sample per zipcode (size = 1%) listings_sample <-
>> splitstackshape::stratified(indt = listings, group = "zipcode", size =
>> 0.01)
>>
>> # Removing zipcodes from polygon_nyc which are not observable in
>> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode
>> %in% c(unique(as.character(listings_sample$zipcode))))
>>
>>
>> ## Random effect matrix (N by J)
>>
>> # N: 22172
>> # J: 154
>>
>> # Arrange listings_sample by zipcode (ascending)
>> listings_sample <- listings_sample %>% arrange(zipcode)
>>
>> # Count number of listings per zipcode
>> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
>> # sum(MM$count)
>>
>> # N by J nulled matrix creation
>> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>>
>> # The total number of neighbourhood
>> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>>
>> for(i in 1:dim(MM)[1]) {
>>  Delta[Uid==i,i] <- 1
>> }
>> rm(i)
>>
>> Delta <- as(Delta,"dgCMatrix")
>>
>>
>> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>>
>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>>
>> # Include neighbour itself as a neighbour
>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>
>> # Spatial weights matrix for nb
>> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
>> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>>
>>
>> ## Fit HSAR SAR upper level random effect
>> model <- as.formula(log_price ~ guests_included + minimum_nights)
>>
>> betas = coef(lm(formula = model, data = listings_sample))
>> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>>
>> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>
>> ##############
>>
>> Thank you and best regards
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Friday, November 8, 2019 13:29
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Fri, 8 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Thank you for your answer.
>>>
>>> I successfully used the function nb2blocknb() for a smaller dataset.
>>>
>>> But for a dataset of over 2 million observations, I get the following
>>> error: "Error: cannot allocate vector of size 840 Kb".
>>
>> I don't think the observations are helpful. If you have repeat lets in the
>> same property in a given month, you need to handle that anyway. I'd go for
>> making the modelling exercise work (we agree that this is not panel data,
>> right?) on a small subset first. I would further argue that you need a
>> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
>> RE. You could very well take (stratified) samples per zipcode to represent
>> your data. Once that works, introduce an MRF RE at the zipcode level,
>> where you do know relative position. Using SARAR is going to be a waste of
>> time unless you can geocode the letting addresses. A multi-level approach
>> will work. Having big data in your case with no useful location
>> information per observation is just adding noise and over-smoothing, I'm
>> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
>> will work, also when you sample the within zipcode lets, given a split
>> into training and test sets, and making CV possible.
>>
>> Roger
>>
>>>
>>> I am expecting that at least 500.000 observations will be dropped due
>>> the lack of values for the chosen variables for the regression model, so
>>> probably I will filter and remove the observations/rows that will not be
>>> used anyway - do you know if there is any package that does this
>>> automatically, given the variables/columns chosed by me?
>>>
>>> Or would you recommend me another approach to avoid the above mentioned
>>> error?
>>>
>>> Thank you and best regards,
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Thursday, November 7, 2019 10:13
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Thu, 7 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Many thanks for your help.
>>>>
>>>> I have an additional question:
>>>>
>>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>>> rownumbers) from my data set? For now, I am taking my data set and
>>>> merging with the sf object polygon_nyc with the function
>>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>>> a huge n x n matrix (depending of the size of my data set).
>>>>
>>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>>> object has only n = 177.
>>>>
>>>> Of course running
>>>>
>>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>>
>>>> does not work ("Input data and weights have different dimensions").
>>>>
>>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>>> zipcode) and then create the weights list lw? Or there another option?
>>>
>>> I think we are getting more clarity. You do not know the location of the
>>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>>> areas, and can create a neighbour object from these boundaries. You then
>>> want to treat all the lettings in a zipcode area i as neighbours, and
>>> additionally lettings in zipcode areas neighbouring i as neighbours of
>>> lettings in i. This is the data structure that motivated the
>>> spdep::nb2blocknb() function:
>>>
>>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>>
>>> Try running the examples to get a feel for what is going on.
>>>
>>> I feel that most of the variability will vanish in the very large numbers
>>> of neighbours, over-smoothing the outcomes. If you do not have locations
>>> for the lettings themselves, I don't think you can make much progress.
>>>
>>> You could try a linear mixed model (or gam with a spatially structured
>>> random effect) with a temporal and a spatial random effect. See the HSAR
>>> package, articles by Dong et al., and maybe
>>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>>> effects at the zipcode level might be a way forward, together with an IID
>>> random effect at the same level (equivalent to sef-neighbours).
>>>
>>> Hope this helps,
>>>
>>> Roger
>>>
>>>>
>>>> Best regards,
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Wednesday, November 6, 2019 15:07
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>
>>>>> Dear Roger,
>>>>>
>>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>>> plain text.
>>>>>
>>>>> I will give a better context for my desired outcome.
>>>>>
>>>>> I am taking Airbnb's listings information for New York City available
>>>>> on: http://insideairbnb.com/get-the-data.html
>>>>>
>>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>>> Listings/ folder. When importing all these 54 files into one single data
>>>>> set, I create a new "date_compiled" variable/column.
>>>>>
>>>>> In total, after the data cleansing process, I have a little more 2
>>>>> million observations.
>>>>
>>>> You have repeat lettings for some, but not all properties. So this is at
>>>> best a very unbalanced panel. For those properties with repeats, you may
>>>> see temporal movement (trend/seasonal).
>>>>
>>>> I suggest (strongly) taking a single borough or even zipcode with some
>>>> hindreds of properties, and working from there. Do not include the
>>>> observation as its own neighbour, perhaps identify repeats and handle them
>>>> specially (create or use a property ID). Unbalanced panels may also create
>>>> a selection bias issue (why are some properties only listed sometimes?).
>>>>
>>>> So this although promising isn't simple, and getting to a hedonic model
>>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>>> necessarily trust OLS output either, partly because of the repeat property
>>>> issue.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> I created 54 timedummy variables for each time period available.
>>>>>
>>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>>> a variety of characteristics which potentially determine the daily rate
>>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>>> of the listing as number of bedrooms, if the host if professional,
>>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>>> from the listing, income per capita, etc.).
>>>>>
>>>>> My dependent variable is price (log price, common in the related
>>>>> literature for hedonic prices).
>>>>>
>>>>> The OLS model is done.
>>>>>
>>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>>> pricing of their listings, take not only into account its structural and
>>>>> location characteristics, but also the prices charged by near listings
>>>>> with similar characteristics - spatial autocorrelation is then present,
>>>>> at least spatial dependence is present in the dependent variable.
>>>>>
>>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>>> itself as a neighbor.
>>>>>
>>>>> Parts of my code can be found below:
>>>>>
>>>>> ########
>>>>>
>>>>> ## packages
>>>>>
>>>>> packages_install <- function(packages){
>>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>>> if (length(new.packages))
>>>>> install.packages(new.packages, dependencies = TRUE)
>>>>> sapply(packages, require, character.only = TRUE)
>>>>> }
>>>>>
>>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>>> packages_install(packages_required)
>>>>>
>>>>> # Working directory
>>>>> setwd("C:/Users/User/R")
>>>>>
>>>>>
>>>>>
>>>>> ## shapefile_us
>>>>>
>>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>>
>>>>> # Columns removal
>>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>>
>>>>> # Column rename: ZCTA5CE10
>>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>>
>>>>> # Column class change: zipcode
>>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>>
>>>>>
>>>>>
>>>>> ## polygon_nyc
>>>>>
>>>>> # Zip code not available in shapefile: 11695
>>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>>
>>>>>
>>>>>
>>>>> ## weight_matrix
>>>>>
>>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>>
>>>>> # Include neighbour itself as a neighbour
>>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>>
>>>>> # Weights to each neighboring polygon
>>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>>
>>>>>
>>>>>
>>>>> ## listings
>>>>>
>>>>> # Data import
>>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>>> listings <- listings %>% bind_rows
>>>>>
>>>>> # Characters removal
>>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>>> listings$price <- gsub("\\$", "", listings$price)
>>>>> listings$price <- gsub(",", "", listings$price)
>>>>>
>>>>>
>>>>>
>>>>> ## timedummy
>>>>>
>>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>>> timedummy <- gsub("-", "_", timedummy)
>>>>>
>>>>>
>>>>>
>>>>> ## OLS regression
>>>>>
>>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>>
>>>>> ########
>>>>>
>>>>> Some of my id's repeat in multiple time periods.
>>>>>
>>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>>
>>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>>
>>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>>
>>>>> Again, thank you very much for the help provided until now.
>>>>>
>>>>> Best regards,
>>>>> Robert
>>>>>
>>>>> ________________________________________
>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>> Sent: Tuesday, November 5, 2019 15:30
>>>>> To: Robert R
>>>>> Cc: r-sig-geo at r-project.org
>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>
>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>
>>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>>> for now that spatial dependence is present in both the dependent
>>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>>> think it would be possible to calculate this.? What I would like to know
>>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>>
>>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>>> of the covariates time-varying? Please provide motivation and use area
>>>>> (preferably with affiliation (your email and user name are not
>>>>> informative) - this feels like a real estate problem, probably wrongly
>>>>> specified. You should use splm if time make sense in your case, but if it
>>>>> really doesn't, simplify your approach, as much of the data will be
>>>>> subject to very large temporal autocorrelation.
>>>>>
>>>>> If this is a continuation of your previous question about using
>>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>>
>>>>> Roger
>>>>>
>>>>>>
>>>>>>       [[alternative HTML version deleted]]
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-Geo mailing list
>>>>>> R-sig-Geo at r-project.org
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>
>>>>> --
>>>>> Roger Bivand
>>>>> Department of Economics, Norwegian School of Economics,
>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>> https://orcid.org/0000-0003-2392-6140
>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Dec  4 09:07:13 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 4 Dec 2019 09:07:13 +0100
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB076828E385B2E65EB3CD4093B0420@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
 <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911262040120.583221@reclus.nhh.no>,
 <VI1P190MB07680F075EA1B55A9AE9D56AB0450@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <3cd7666ef730416c8e7f6f4110315f0a@nhh.no>
 <VI1P190MB076828E385B2E65EB3CD4093B0420@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1912040901070.29869@reclus.nhh.no>

On Wed, 4 Dec 2019, Robert R wrote:

> Dear Roger,
>
> Again, thank you for your answer. What do you mean by "zip code random 
> effect"? You mean I should use in plm the model "random"?
>
> regression_re <- plm(formula = model, data = listings, model = "random", 
> index = c("id", "date_compiled"))

No, obviously not, your data are not a balanced panel. I mean a multilevel 
model, where the <200 zip codes cluster the data, and where a zip code 
level IID RE will almost certainly do a better job than dummies. An 
MRF/ICAR RE might be an extension.

>
> And any other methodology in dealing with large weight matrices in 
> spatialreg::lagsarlm?

Please refer to Bivand et al. (2013) refered to in the package. Probably 
the weights would need to be symmetric and very sparse.

I still think that you should focus on a small subset of the data and to 
improving the signal-noise ratio before trying to scale up.

Roger

>
> Thank you and best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Wednesday, November 27, 2019 13:53
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: SV: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> Yes this is expected, since the # neighbours in a single zip code block is a dense matrix, and there will be multiple such matrices. (15000^2)*8 is 1.8e+09 so such a dense matrix will max out your RAM. There is no way to look at block neighbours in that format without subsetting your data (think train/test), use a zip code random effect. I would certainly drop all attempts to examine spatial dependency until you get an aspatial multilevel hedonic model working.
>
> Roger
>
> --
> Roger Bivand
> Norwegian School of Economics
> Helleveien 30, 5045 Bergen, Norway
> Roger.Bivand at nhh.no
>
>
> ________________________________________
> Fra: Robert R <usercatch at outlook.com>
> Sendt: tirsdag 26. november 2019 21.04
> Til: Roger Bivand
> Kopi: r-sig-geo at r-project.org
> Emne: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> Dear Roger,
>
> Thank you for your e-mail. Actually there is less noise that it seems. Rental prices are daily rental prices and I have an extract of all Airbnb listings daily prices once a month for a period of 4 years. Each listings information contains the lat, lon, number of bedrooms, category (entire home/apt, shared room or private room), etc.
>
> One question regarding the spdep::nb2blocknb function: it runs super fast with up to n = 1000, and always crashes my R session with n = 15000 or so. Is there an alternative to solve this problem?
>
> Thank you and best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Tuesday, November 26, 2019 20:48
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> Sorry for late reply, am indisposed and unable to help further. I feel
> that there is so much noise in your data (differences in offers, rental
> lengths, repeats or not, etc.), that you will certainly have to subset
> vigorously first to isolate response cases that are comparable. What you
> are trying to disentangle are the hedonic components in the bundle where
> you just have price as response, but lots of other bundle characteristics
> on the right hand side (days, etc.). I feel you'd need to try to get to a
> response index of price per day per rental area or some such. I'd
> certainly advise examining responses to a specific driver (major concert
> or sports event) to get a feel for how the market responds, and return to
> spatial hedonic after finding an approach that gives reasonable aspatial
> outcomes.
>
> Roger
>
> On Sun, 17 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Thank you for your message and sorry for my late answer.
>>
>> Regarding the number of listings (lettings) for my data set (2.216.642 observations), each listing contains an individual id:
>>
>> unique ids: 180.004
>> time periods: 54 (2015-01 to 2019-09)
>> number of ids that appear only once: 28.486 (of 180.004 ids) (15,8%)
>> number of ids that appear/repeat 2-10 times: 82.641 (of 180.004 ids) (45,9%)
>> number of ids that appear/repeat 11-30 times: 46.465 (of 180.004 ids) (25,8%)
>> number of ids that appear/repeat 31-54 times: 22.412 (of 180.004 ids) (12,5%)
>>
>> Important to notice is that hosts can change the room_category (between entire/home apt, private room and shared room) keeping the same listing id number. In my data, the number of unique ids that in some point changed the room_type is of 7.204 ids.
>>
>> --
>>
>> For the OLS model, I was using only a fixed effect model, where each time period (date_compiled) (54 in total) is a time dummy.
>>
>> plm::plm(formula = model, data = listings, model = "pooling", index = c("id", "date_compiled"))
>>
>>
>> --
>> Osland et al. (2016) (https://doi.org/10.1111/jors.12281) use a spatial fixed effects (SFE) hedonic model, where each defined neighborhood zone in the study area is represented by dummy variables.
>>
>> Dong et al. (2015) (https://doi.org/10.1111/gean.12049) outline four model specifications to accommodate geographically hierarchical data structures: (1) groupwise W and fixed regional effects; (2) groupwise W and random regional effects; (3) proximity-based W and fixed regional effects; and (4) proximity-based W and random regional effects.
>> --
>>
>> I created a new column/variable containing the borough where the zipcode is found (Manhattan, Brooklyn, Queens, Bronx, Staten Island).
>>
>> If I understood it right, the (two-level) Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) considers the occurrence of spatial relations at the (lower) individual (geographical coordinates - in my case, the listing location) and (higher) group level (territorial units - in my case, zipcodes).
>>
>> According to Bivand et al. (2017): "(...) W is a spatial weights matrix. The HSAR model may also be estimated without this component.". So, in this case I only estimate the Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) in a "one-level" basis, i.e., at the higher-level.
>>
>> HSAR::hsar(model, data = listings, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>
>> (Where the "model" formula contains the 54 time dummy variables)
>>
>> Do you think I can proceed with this model? I was able to calculate it.
>>
>> If I remove all observations/rows with NAs in one of the chosen variables/observations, 884.183 observations remain. If I would create a W matrix for HSAR::hsar, I would have a gigantic 884.183 by 884.183 matrix. This is the reason why I put W = NULL.
>>
>>
>> Thank you and best regards
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Monday, November 11, 2019 11:31
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Sun, 10 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Again, thank you for your answer. I read the material provided and
>>> decided that Hierarchical Spatial Autoregressive (HSAR) could be the
>>> right model for me.
>>>
>>> I indeed have the precise latitude and longitude information for all my
>>> listings for NYC.
>>>
>>> I created a stratified sample (group = zipcode) with 22172 (1%) of my
>>> observations called listings_sample and tried to replicate the hsar
>>> model, please see below.
>>>
>>> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.
>>
>> Unless you know definitely that you want to relate the response to its
>> lagged value, you do not need this. Do note that the matrix is very
>> sparse, so could be fitted without difficulty with ML in a cross-sectional
>> model.
>>
>>>
>>> You recommended then to introduce a Markov random field (MRF) random
>>> effect (RE) at the zipcode level, but I did not understand it so well.
>>> Could you develop a litte more?
>>>
>>
>> Did you read the development in
>> https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and
>> includes code for fitting the Beijing housing parcels data se from HSAR
>> with many other packages (MCMC, INLA, hglm, etc.). I guess that you should
>> try to create a model that works on a single borough, sing the zipcodes
>> in that borough as a proxy for unobserved neighbourhood effects. Try for
>> example using lme4::lmer() with only a zipcode IID random effect, see if
>> the hedonic estimates are similar to lm(), and leave adding an MRF RE
>> (with for example mgcv::gam() or hglm::hglm()) until you have a working
>> testbed. Then advance step-by-step from there.
>>
>> You still have not said how many repeat lettings you see - it will affect
>> the way you specify your model.
>>
>> Roger
>>
>>> ##############
>>> library(spdep)
>>> library(HSAR)
>>> library(dplyr)
>>> library(splitstackshape)
>>>
>>>
>>> # Stratified sample per zipcode (size = 1%) listings_sample <-
>>> splitstackshape::stratified(indt = listings, group = "zipcode", size =
>>> 0.01)
>>>
>>> # Removing zipcodes from polygon_nyc which are not observable in
>>> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode
>>> %in% c(unique(as.character(listings_sample$zipcode))))
>>>
>>>
>>> ## Random effect matrix (N by J)
>>>
>>> # N: 22172
>>> # J: 154
>>>
>>> # Arrange listings_sample by zipcode (ascending)
>>> listings_sample <- listings_sample %>% arrange(zipcode)
>>>
>>> # Count number of listings per zipcode
>>> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
>>> # sum(MM$count)
>>>
>>> # N by J nulled matrix creation
>>> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>>>
>>> # The total number of neighbourhood
>>> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>>>
>>> for(i in 1:dim(MM)[1]) {
>>>  Delta[Uid==i,i] <- 1
>>> }
>>> rm(i)
>>>
>>> Delta <- as(Delta,"dgCMatrix")
>>>
>>>
>>> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>>>
>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>>>
>>> # Include neighbour itself as a neighbour
>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>
>>> # Spatial weights matrix for nb
>>> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
>>> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>>>
>>>
>>> ## Fit HSAR SAR upper level random effect
>>> model <- as.formula(log_price ~ guests_included + minimum_nights)
>>>
>>> betas = coef(lm(formula = model, data = listings_sample))
>>> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>>>
>>> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>>
>>> ##############
>>>
>>> Thank you and best regards
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Friday, November 8, 2019 13:29
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Fri, 8 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Thank you for your answer.
>>>>
>>>> I successfully used the function nb2blocknb() for a smaller dataset.
>>>>
>>>> But for a dataset of over 2 million observations, I get the following
>>>> error: "Error: cannot allocate vector of size 840 Kb".
>>>
>>> I don't think the observations are helpful. If you have repeat lets in the
>>> same property in a given month, you need to handle that anyway. I'd go for
>>> making the modelling exercise work (we agree that this is not panel data,
>>> right?) on a small subset first. I would further argue that you need a
>>> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
>>> RE. You could very well take (stratified) samples per zipcode to represent
>>> your data. Once that works, introduce an MRF RE at the zipcode level,
>>> where you do know relative position. Using SARAR is going to be a waste of
>>> time unless you can geocode the letting addresses. A multi-level approach
>>> will work. Having big data in your case with no useful location
>>> information per observation is just adding noise and over-smoothing, I'm
>>> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
>>> will work, also when you sample the within zipcode lets, given a split
>>> into training and test sets, and making CV possible.
>>>
>>> Roger
>>>
>>>>
>>>> I am expecting that at least 500.000 observations will be dropped due
>>>> the lack of values for the chosen variables for the regression model, so
>>>> probably I will filter and remove the observations/rows that will not be
>>>> used anyway - do you know if there is any package that does this
>>>> automatically, given the variables/columns chosed by me?
>>>>
>>>> Or would you recommend me another approach to avoid the above mentioned
>>>> error?
>>>>
>>>> Thank you and best regards,
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Thursday, November 7, 2019 10:13
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Thu, 7 Nov 2019, Robert R wrote:
>>>>
>>>>> Dear Roger,
>>>>>
>>>>> Many thanks for your help.
>>>>>
>>>>> I have an additional question:
>>>>>
>>>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>>>> rownumbers) from my data set? For now, I am taking my data set and
>>>>> merging with the sf object polygon_nyc with the function
>>>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>>>> a huge n x n matrix (depending of the size of my data set).
>>>>>
>>>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>>>> object has only n = 177.
>>>>>
>>>>> Of course running
>>>>>
>>>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>>>
>>>>> does not work ("Input data and weights have different dimensions").
>>>>>
>>>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>>>> zipcode) and then create the weights list lw? Or there another option?
>>>>
>>>> I think we are getting more clarity. You do not know the location of the
>>>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>>>> areas, and can create a neighbour object from these boundaries. You then
>>>> want to treat all the lettings in a zipcode area i as neighbours, and
>>>> additionally lettings in zipcode areas neighbouring i as neighbours of
>>>> lettings in i. This is the data structure that motivated the
>>>> spdep::nb2blocknb() function:
>>>>
>>>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>>>
>>>> Try running the examples to get a feel for what is going on.
>>>>
>>>> I feel that most of the variability will vanish in the very large numbers
>>>> of neighbours, over-smoothing the outcomes. If you do not have locations
>>>> for the lettings themselves, I don't think you can make much progress.
>>>>
>>>> You could try a linear mixed model (or gam with a spatially structured
>>>> random effect) with a temporal and a spatial random effect. See the HSAR
>>>> package, articles by Dong et al., and maybe
>>>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>>>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>>>> effects at the zipcode level might be a way forward, together with an IID
>>>> random effect at the same level (equivalent to sef-neighbours).
>>>>
>>>> Hope this helps,
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> Best regards,
>>>>> Robert
>>>>>
>>>>> ________________________________________
>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>> Sent: Wednesday, November 6, 2019 15:07
>>>>> To: Robert R
>>>>> Cc: r-sig-geo at r-project.org
>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>
>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>
>>>>>> Dear Roger,
>>>>>>
>>>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>>>> plain text.
>>>>>>
>>>>>> I will give a better context for my desired outcome.
>>>>>>
>>>>>> I am taking Airbnb's listings information for New York City available
>>>>>> on: http://insideairbnb.com/get-the-data.html
>>>>>>
>>>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>>>> Listings/ folder. When importing all these 54 files into one single data
>>>>>> set, I create a new "date_compiled" variable/column.
>>>>>>
>>>>>> In total, after the data cleansing process, I have a little more 2
>>>>>> million observations.
>>>>>
>>>>> You have repeat lettings for some, but not all properties. So this is at
>>>>> best a very unbalanced panel. For those properties with repeats, you may
>>>>> see temporal movement (trend/seasonal).
>>>>>
>>>>> I suggest (strongly) taking a single borough or even zipcode with some
>>>>> hindreds of properties, and working from there. Do not include the
>>>>> observation as its own neighbour, perhaps identify repeats and handle them
>>>>> specially (create or use a property ID). Unbalanced panels may also create
>>>>> a selection bias issue (why are some properties only listed sometimes?).
>>>>>
>>>>> So this although promising isn't simple, and getting to a hedonic model
>>>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>>>> necessarily trust OLS output either, partly because of the repeat property
>>>>> issue.
>>>>>
>>>>> Roger
>>>>>
>>>>>>
>>>>>> I created 54 timedummy variables for each time period available.
>>>>>>
>>>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>>>> a variety of characteristics which potentially determine the daily rate
>>>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>>>> of the listing as number of bedrooms, if the host if professional,
>>>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>>>> from the listing, income per capita, etc.).
>>>>>>
>>>>>> My dependent variable is price (log price, common in the related
>>>>>> literature for hedonic prices).
>>>>>>
>>>>>> The OLS model is done.
>>>>>>
>>>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>>>> pricing of their listings, take not only into account its structural and
>>>>>> location characteristics, but also the prices charged by near listings
>>>>>> with similar characteristics - spatial autocorrelation is then present,
>>>>>> at least spatial dependence is present in the dependent variable.
>>>>>>
>>>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>>>> itself as a neighbor.
>>>>>>
>>>>>> Parts of my code can be found below:
>>>>>>
>>>>>> ########
>>>>>>
>>>>>> ## packages
>>>>>>
>>>>>> packages_install <- function(packages){
>>>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>>>> if (length(new.packages))
>>>>>> install.packages(new.packages, dependencies = TRUE)
>>>>>> sapply(packages, require, character.only = TRUE)
>>>>>> }
>>>>>>
>>>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>>>> packages_install(packages_required)
>>>>>>
>>>>>> # Working directory
>>>>>> setwd("C:/Users/User/R")
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## shapefile_us
>>>>>>
>>>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>>>
>>>>>> # Columns removal
>>>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>>>
>>>>>> # Column rename: ZCTA5CE10
>>>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>>>
>>>>>> # Column class change: zipcode
>>>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## polygon_nyc
>>>>>>
>>>>>> # Zip code not available in shapefile: 11695
>>>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## weight_matrix
>>>>>>
>>>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>>>
>>>>>> # Include neighbour itself as a neighbour
>>>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>>>
>>>>>> # Weights to each neighboring polygon
>>>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## listings
>>>>>>
>>>>>> # Data import
>>>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>>>> listings <- listings %>% bind_rows
>>>>>>
>>>>>> # Characters removal
>>>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>>>> listings$price <- gsub("\\$", "", listings$price)
>>>>>> listings$price <- gsub(",", "", listings$price)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## timedummy
>>>>>>
>>>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>>>> timedummy <- gsub("-", "_", timedummy)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## OLS regression
>>>>>>
>>>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>>>
>>>>>> ########
>>>>>>
>>>>>> Some of my id's repeat in multiple time periods.
>>>>>>
>>>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>>>
>>>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>>>
>>>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>>>
>>>>>> Again, thank you very much for the help provided until now.
>>>>>>
>>>>>> Best regards,
>>>>>> Robert
>>>>>>
>>>>>> ________________________________________
>>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>>> Sent: Tuesday, November 5, 2019 15:30
>>>>>> To: Robert R
>>>>>> Cc: r-sig-geo at r-project.org
>>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>>
>>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>>
>>>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>>>> for now that spatial dependence is present in both the dependent
>>>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>>>> think it would be possible to calculate this.? What I would like to know
>>>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>>>
>>>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>>>> of the covariates time-varying? Please provide motivation and use area
>>>>>> (preferably with affiliation (your email and user name are not
>>>>>> informative) - this feels like a real estate problem, probably wrongly
>>>>>> specified. You should use splm if time make sense in your case, but if it
>>>>>> really doesn't, simplify your approach, as much of the data will be
>>>>>> subject to very large temporal autocorrelation.
>>>>>>
>>>>>> If this is a continuation of your previous question about using
>>>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>>>
>>>>>> Roger
>>>>>>
>>>>>>>
>>>>>>>       [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-Geo mailing list
>>>>>>> R-sig-Geo at r-project.org
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>>
>>>>>> --
>>>>>> Roger Bivand
>>>>>> Department of Economics, Norwegian School of Economics,
>>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>>> https://orcid.org/0000-0003-2392-6140
>>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>>
>>>>>
>>>>> --
>>>>> Roger Bivand
>>>>> Department of Economics, Norwegian School of Economics,
>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>> https://orcid.org/0000-0003-2392-6140
>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From @bdou|@ye@@r @end|ng |rom gm@||@com  Wed Dec  4 18:54:25 2019
From: @bdou|@ye@@r @end|ng |rom gm@||@com (Abdoulaye Sarr)
Date: Wed, 4 Dec 2019 17:54:25 +0000
Subject: [R-sig-Geo] ggmap/rgadl
Message-ID: <CAN=6O0LWaq_BJxuujvQYzO1VXHHb=fcTWm047WMWUzD18C+R1g@mail.gmail.com>

I am using the command below and getting error message I don't understand
exactly what it is.
I hope some will help pass this step.
utmcoor <- SpatialPoints(cbind(OAK$UTM33_X, OAK$UTM33_Y),
+                          proj4string = CRS("+proj=utm +zone=33"))

Error: 'new_proj_and_gdal' is not an exported object from 'namespace:rgdal'

Kind regards,

Abdou

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Dec  4 19:23:51 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 4 Dec 2019 18:23:51 +0000
Subject: [R-sig-Geo] ggmap/rgadl
In-Reply-To: <CAN=6O0LWaq_BJxuujvQYzO1VXHHb=fcTWm047WMWUzD18C+R1g@mail.gmail.com>
References: <CAN=6O0LWaq_BJxuujvQYzO1VXHHb=fcTWm047WMWUzD18C+R1g@mail.gmail.com>
Message-ID: <SN6PR04MB416027664781D5D41D89C9B4A35D0@SN6PR04MB4160.namprd04.prod.outlook.com>

Please provide the output of sessionInfo(). You must have installed an unreleased development version of sp after having installed an unreleased development version of rgdal, then reinstalled a released version of rgdal. For rgdal also provide the output of the startup messages with PROJ and GDAL versions.

Roger Bivand
Norwegian School of Economics
Bergen, Norway
________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Abdoulaye Sarr <abdoulayesar at gmail.com>
Sent: Wednesday, December 4, 2019 6:54:25 PM
To: r-sig-geo <r-sig-geo at r-project.org>
Subject: [R-sig-Geo] ggmap/rgadl

I am using the command below and getting error message I don't understand
exactly what it is.
I hope some will help pass this step.
utmcoor <- SpatialPoints(cbind(OAK$UTM33_X, OAK$UTM33_Y),
+                          proj4string = CRS("+proj=utm +zone=33"))

Error: 'new_proj_and_gdal' is not an exported object from 'namespace:rgdal'

Kind regards,

Abdou

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From @bdou|@ye@@r @end|ng |rom gm@||@com  Wed Dec  4 19:51:36 2019
From: @bdou|@ye@@r @end|ng |rom gm@||@com (Abdoulaye Sarr)
Date: Wed, 4 Dec 2019 18:51:36 +0000
Subject: [R-sig-Geo] ggmap/rgadl
In-Reply-To: <SN6PR04MB416027664781D5D41D89C9B4A35D0@SN6PR04MB4160.namprd04.prod.outlook.com>
References: <CAN=6O0LWaq_BJxuujvQYzO1VXHHb=fcTWm047WMWUzD18C+R1g@mail.gmail.com>
 <SN6PR04MB416027664781D5D41D89C9B4A35D0@SN6PR04MB4160.namprd04.prod.outlook.com>
Message-ID: <CAN=6O0K4GDkZVWQ2dv3NzdyVdz_POw9m+a7bC_REzBNb2NVQtA@mail.gmail.com>

rgdal: version: 1.4-8, (SVN revision 845)
 Geospatial Data Abstraction Library extensions to R successfully loaded
 Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28
 Path to GDAL shared files:
/Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal
 GDAL binary built with GEOS: FALSE
 Loaded PROJ.4 runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION: 520]
 Path to PROJ.4 shared files:
/Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj
 Linking to sp version: 1.3-2

sessioninfo()
R version 3.6.1 (2019-07-05)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.6

Matrix products: default
BLAS:
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK:
/Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] parallel  stats     graphics  grDevices utils     datasets  methods
base

other attached packages:
 [1] rgdal_1.4-8          OpenStreetMap_0.3.4  magrittr_1.5
dplyr_0.8.3
 [5] gstat_2.0-3          ggmap_3.0.0.901      mapdata_2.3.0
 maptools_0.9-8
 [9] maps_3.3.0           MASS_7.3-51.4        climates_0.1-1.6
RANN_2.6.1
[13] rWBclimate_0.1.3     LaplacesDemon_16.1.1 inlabru_2.1.12.999
brinla_0.1.0
[17] INLA_19.09.03        reshape_0.8.8        lme4_1.1-21
 Matrix_1.2-17
[21] GGally_1.4.0         tidyr_1.0.0          plyr_1.8.4
ggplot2_3.2.1
[25] mgcv_1.8-31          nlme_3.1-142         lattice_0.20-38
 SpatialEpiApp_0.5
[29] devtools_2.2.1       usethis_1.5.1        glm2_1.2.1
raster_3.0-7
[33] sp_1.3-3



On Wed, Dec 4, 2019 at 6:23 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> Please provide the output of sessionInfo(). You must have installed an
> unreleased development version of sp after having installed an unreleased
> development version of rgdal, then reinstalled a released version of rgdal.
> For rgdal also provide the output of the startup messages with PROJ and
> GDAL versions.
>
> Roger Bivand
> Norwegian School of Economics
> Bergen, Norway
> ------------------------------
> *From:* R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
> Abdoulaye Sarr <abdoulayesar at gmail.com>
> *Sent:* Wednesday, December 4, 2019 6:54:25 PM
> *To:* r-sig-geo <r-sig-geo at r-project.org>
> *Subject:* [R-sig-Geo] ggmap/rgadl
>
> I am using the command below and getting error message I don't understand
> exactly what it is.
> I hope some will help pass this step.
> utmcoor <- SpatialPoints(cbind(OAK$UTM33_X, OAK$UTM33_Y),
> +                          proj4string = CRS("+proj=utm +zone=33"))
>
> Error: 'new_proj_and_gdal' is not an exported object from 'namespace:rgdal'
>
> Kind regards,
>
> Abdou
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Wed Dec  4 20:18:24 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Wed, 4 Dec 2019 20:18:24 +0100
Subject: [R-sig-Geo] ggmap/rgadl
In-Reply-To: <CAN=6O0K4GDkZVWQ2dv3NzdyVdz_POw9m+a7bC_REzBNb2NVQtA@mail.gmail.com>
References: <CAN=6O0LWaq_BJxuujvQYzO1VXHHb=fcTWm047WMWUzD18C+R1g@mail.gmail.com>
 <SN6PR04MB416027664781D5D41D89C9B4A35D0@SN6PR04MB4160.namprd04.prod.outlook.com>
 <CAN=6O0K4GDkZVWQ2dv3NzdyVdz_POw9m+a7bC_REzBNb2NVQtA@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1912041959530.62064@reclus.nhh.no>

On Wed, 4 Dec 2019, Abdoulaye Sarr wrote:

> rgdal: version: 1.4-8, (SVN revision 845)
> Geospatial Data Abstraction Library extensions to R successfully loaded
> Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28
> Path to GDAL shared files:
> /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal
> GDAL binary built with GEOS: FALSE
> Loaded PROJ.4 runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION: 520]
> Path to PROJ.4 shared files:
> /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj
> Linking to sp version: 1.3-2
>
> sessioninfo()
> R version 3.6.1 (2019-07-05)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS Mojave 10.14.6
>
> Matrix products: default
> BLAS:
> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
> LAPACK:
> /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] parallel  stats     graphics  grDevices utils     datasets  methods
> base
>
> other attached packages:
> [1] rgdal_1.4-8          OpenStreetMap_0.3.4  magrittr_1.5
> dplyr_0.8.3
> [5] gstat_2.0-3          ggmap_3.0.0.901      mapdata_2.3.0
> maptools_0.9-8
> [9] maps_3.3.0           MASS_7.3-51.4        climates_0.1-1.6
> RANN_2.6.1
> [13] rWBclimate_0.1.3     LaplacesDemon_16.1.1 inlabru_2.1.12.999
> brinla_0.1.0
> [17] INLA_19.09.03        reshape_0.8.8        lme4_1.1-21
> Matrix_1.2-17
> [21] GGally_1.4.0         tidyr_1.0.0          plyr_1.8.4
> ggplot2_3.2.1
> [25] mgcv_1.8-31          nlme_3.1-142         lattice_0.20-38
> SpatialEpiApp_0.5
> [29] devtools_2.2.1       usethis_1.5.1        glm2_1.2.1
> raster_3.0-7
> [33] sp_1.3-3
    ^^^^^^^^^^^^

Please install sp 1.3-2 from CRAN. The 1.3-3 version is only available 
from my github fork, and can currently only be installed (from source) if
rgdal >=1.5-1 is installed from R-Forge from source. It would be helpful
if you could say how you managed to install sp 1.3-3 and why and when you 
did so.

When we are ready for GDAL >= 3 and PROJ >= 6, sp, rgdal and sf (with 
stars and lwgeom) will be announced and released. We are not there yet.

Roger


>
>
>
> On Wed, Dec 4, 2019 at 6:23 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> Please provide the output of sessionInfo(). You must have installed an
>> unreleased development version of sp after having installed an unreleased
>> development version of rgdal, then reinstalled a released version of rgdal.
>> For rgdal also provide the output of the startup messages with PROJ and
>> GDAL versions.
>>
>> Roger Bivand
>> Norwegian School of Economics
>> Bergen, Norway
>> ------------------------------
>> *From:* R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
>> Abdoulaye Sarr <abdoulayesar at gmail.com>
>> *Sent:* Wednesday, December 4, 2019 6:54:25 PM
>> *To:* r-sig-geo <r-sig-geo at r-project.org>
>> *Subject:* [R-sig-Geo] ggmap/rgadl
>>
>> I am using the command below and getting error message I don't understand
>> exactly what it is.
>> I hope some will help pass this step.
>> utmcoor <- SpatialPoints(cbind(OAK$UTM33_X, OAK$UTM33_Y),
>> +                          proj4string = CRS("+proj=utm +zone=33"))
>>
>> Error: 'new_proj_and_gdal' is not an exported object from 'namespace:rgdal'
>>
>> Kind regards,
>>
>> Abdou
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From @bdou|@ye@@r @end|ng |rom gm@||@com  Thu Dec  5 06:23:48 2019
From: @bdou|@ye@@r @end|ng |rom gm@||@com (Abdoulaye Sarr)
Date: Thu, 5 Dec 2019 05:23:48 +0000
Subject: [R-sig-Geo] ggmap/rgadl
In-Reply-To: <alpine.LFD.2.21.1912041959530.62064@reclus.nhh.no>
References: <CAN=6O0LWaq_BJxuujvQYzO1VXHHb=fcTWm047WMWUzD18C+R1g@mail.gmail.com>
 <SN6PR04MB416027664781D5D41D89C9B4A35D0@SN6PR04MB4160.namprd04.prod.outlook.com>
 <CAN=6O0K4GDkZVWQ2dv3NzdyVdz_POw9m+a7bC_REzBNb2NVQtA@mail.gmail.com>
 <alpine.LFD.2.21.1912041959530.62064@reclus.nhh.no>
Message-ID: <CAN=6O0KaRTEMCvzCT6FUZcqJAAgJBBrb0PfqhPw2Ks4hGBh7Sw@mail.gmail.com>

Thank you, what I recall is the pack update I did after installing R 3.6.1!
(most recent action.
I now have another issue:
> longlatcoor   <- spTransform(utmcoor, CRS("+proj=longlat"))
> OAK$Long <- longlatcoor at coords[,1]
> OAK$Lat  <- longlatcoor at coords[,2]
>
> range(OAK$Long)
[1]  6.164666 18.653709
> range(OAK$Lat)
[1] 56.79080 60.40477
> glgmap   <- get_map(location = c(5, 55, 20, 62),
+                     #zoom = 10,
+                     source = "google",
+                     maptype= "terrain")
Bounding box given to Google - spatial extent only approximate.
Source :
https://maps.googleapis.com/maps/api/staticmap?center=58.5,12.5&zoom=6&size=640x640&scale=2&maptype=terrain&language=en-EN&key=[RKCJmE1PNJtH1c%20]
Error in aperm.default(map, c(2, 1, 3)) :
  invalid first argument, must be an array
In addition: Warning message:
In get_googlemap(center = location, zoom = zoom, maptype = maptype,  :
  HTTP 400 Bad Request

Abdou

On Wed, Dec 4, 2019 at 7:18 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Wed, 4 Dec 2019, Abdoulaye Sarr wrote:
>
> > rgdal: version: 1.4-8, (SVN revision 845)
> > Geospatial Data Abstraction Library extensions to R successfully loaded
> > Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28
> > Path to GDAL shared files:
> > /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal
> > GDAL binary built with GEOS: FALSE
> > Loaded PROJ.4 runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION:
> 520]
> > Path to PROJ.4 shared files:
> > /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj
> > Linking to sp version: 1.3-2
> >
> > sessioninfo()
> > R version 3.6.1 (2019-07-05)
> > Platform: x86_64-apple-darwin15.6.0 (64-bit)
> > Running under: macOS Mojave 10.14.6
> >
> > Matrix products: default
> > BLAS:
> >
> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
> > LAPACK:
> >
> /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
> >
> > locale:
> > [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
> >
> > attached base packages:
> > [1] parallel  stats     graphics  grDevices utils     datasets  methods
> > base
> >
> > other attached packages:
> > [1] rgdal_1.4-8          OpenStreetMap_0.3.4  magrittr_1.5
> > dplyr_0.8.3
> > [5] gstat_2.0-3          ggmap_3.0.0.901      mapdata_2.3.0
> > maptools_0.9-8
> > [9] maps_3.3.0           MASS_7.3-51.4        climates_0.1-1.6
> > RANN_2.6.1
> > [13] rWBclimate_0.1.3     LaplacesDemon_16.1.1 inlabru_2.1.12.999
> > brinla_0.1.0
> > [17] INLA_19.09.03        reshape_0.8.8        lme4_1.1-21
> > Matrix_1.2-17
> > [21] GGally_1.4.0         tidyr_1.0.0          plyr_1.8.4
> > ggplot2_3.2.1
> > [25] mgcv_1.8-31          nlme_3.1-142         lattice_0.20-38
> > SpatialEpiApp_0.5
> > [29] devtools_2.2.1       usethis_1.5.1        glm2_1.2.1
> > raster_3.0-7
> > [33] sp_1.3-3
>     ^^^^^^^^^^^^
>
> Please install sp 1.3-2 from CRAN. The 1.3-3 version is only available
> from my github fork, and can currently only be installed (from source) if
> rgdal >=1.5-1 is installed from R-Forge from source. It would be helpful
> if you could say how you managed to install sp 1.3-3 and why and when you
> did so.
>
> When we are ready for GDAL >= 3 and PROJ >= 6, sp, rgdal and sf (with
> stars and lwgeom) will be announced and released. We are not there yet.
>
> Roger
>
>
> >
> >
> >
> > On Wed, Dec 4, 2019 at 6:23 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
> >
> >> Please provide the output of sessionInfo(). You must have installed an
> >> unreleased development version of sp after having installed an
> unreleased
> >> development version of rgdal, then reinstalled a released version of
> rgdal.
> >> For rgdal also provide the output of the startup messages with PROJ and
> >> GDAL versions.
> >>
> >> Roger Bivand
> >> Norwegian School of Economics
> >> Bergen, Norway
> >> ------------------------------
> >> *From:* R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
> >> Abdoulaye Sarr <abdoulayesar at gmail.com>
> >> *Sent:* Wednesday, December 4, 2019 6:54:25 PM
> >> *To:* r-sig-geo <r-sig-geo at r-project.org>
> >> *Subject:* [R-sig-Geo] ggmap/rgadl
> >>
> >> I am using the command below and getting error message I don't
> understand
> >> exactly what it is.
> >> I hope some will help pass this step.
> >> utmcoor <- SpatialPoints(cbind(OAK$UTM33_X, OAK$UTM33_Y),
> >> +                          proj4string = CRS("+proj=utm +zone=33"))
> >>
> >> Error: 'new_proj_and_gdal' is not an exported object from
> 'namespace:rgdal'
> >>
> >> Kind regards,
> >>
> >> Abdou
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Thu Dec  5 08:56:02 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Thu, 5 Dec 2019 08:56:02 +0100
Subject: [R-sig-Geo] ggmap/rgadl
In-Reply-To: <CAN=6O0KaRTEMCvzCT6FUZcqJAAgJBBrb0PfqhPw2Ks4hGBh7Sw@mail.gmail.com>
References: <CAN=6O0LWaq_BJxuujvQYzO1VXHHb=fcTWm047WMWUzD18C+R1g@mail.gmail.com>
 <SN6PR04MB416027664781D5D41D89C9B4A35D0@SN6PR04MB4160.namprd04.prod.outlook.com>
 <CAN=6O0K4GDkZVWQ2dv3NzdyVdz_POw9m+a7bC_REzBNb2NVQtA@mail.gmail.com>
 <alpine.LFD.2.21.1912041959530.62064@reclus.nhh.no>
 <CAN=6O0KaRTEMCvzCT6FUZcqJAAgJBBrb0PfqhPw2Ks4hGBh7Sw@mail.gmail.com>
Message-ID: <alpine.LFD.2.21.1912050851290.76407@reclus.nhh.no>

On Thu, 5 Dec 2019, Abdoulaye Sarr wrote:

> Thank you, what I recall is the pack update I did after installing R 3.6.1!
> (most recent action.

There is literally no path from sp 1.3-3 on my github fork to any CRAN 
package update on OSX whatsoever.

> I now have another issue:
>> longlatcoor   <- spTransform(utmcoor, CRS("+proj=longlat"))
>> OAK$Long <- longlatcoor at coords[,1]
>> OAK$Lat  <- longlatcoor at coords[,2]
>>
>> range(OAK$Long)
> [1]  6.164666 18.653709
>> range(OAK$Lat)
> [1] 56.79080 60.40477
>> glgmap   <- get_map(location = c(5, 55, 20, 62),
> +                     #zoom = 10,
> +                     source = "google",
> +                     maptype= "terrain")
> Bounding box given to Google - spatial extent only approximate.
> Source :
> https://maps.googleapis.com/maps/api/staticmap?center=58.5,12.5&zoom=6&size=640x640&scale=2&maptype=terrain&language=en-EN&key=[RKCJmE1PNJtH1c%20]
> Error in aperm.default(map, c(2, 1, 3)) :
>  invalid first argument, must be an array
> In addition: Warning message:
> In get_googlemap(center = location, zoom = zoom, maptype = maptype,  :
>  HTTP 400 Bad Request

HTTP 400 Bad Request seems obvious. The Google API is aggressive and 
largely unhelpful, you did not say which package provides get_map(), but I 
see you using ggmap in the earlier sessionInfo(). Maybe raise an issue 
with ggmap?

Roger

>
> Abdou
>
> On Wed, Dec 4, 2019 at 7:18 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Wed, 4 Dec 2019, Abdoulaye Sarr wrote:
>>
>>> rgdal: version: 1.4-8, (SVN revision 845)
>>> Geospatial Data Abstraction Library extensions to R successfully loaded
>>> Loaded GDAL runtime: GDAL 2.4.2, released 2019/06/28
>>> Path to GDAL shared files:
>>> /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/gdal
>>> GDAL binary built with GEOS: FALSE
>>> Loaded PROJ.4 runtime: Rel. 5.2.0, September 15th, 2018, [PJ_VERSION:
>> 520]
>>> Path to PROJ.4 shared files:
>>> /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rgdal/proj
>>> Linking to sp version: 1.3-2
>>>
>>> sessioninfo()
>>> R version 3.6.1 (2019-07-05)
>>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>>> Running under: macOS Mojave 10.14.6
>>>
>>> Matrix products: default
>>> BLAS:
>>>
>> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
>>> LAPACK:
>>>
>> /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
>>>
>>> locale:
>>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>>
>>> attached base packages:
>>> [1] parallel  stats     graphics  grDevices utils     datasets  methods
>>> base
>>>
>>> other attached packages:
>>> [1] rgdal_1.4-8          OpenStreetMap_0.3.4  magrittr_1.5
>>> dplyr_0.8.3
>>> [5] gstat_2.0-3          ggmap_3.0.0.901      mapdata_2.3.0
>>> maptools_0.9-8
>>> [9] maps_3.3.0           MASS_7.3-51.4        climates_0.1-1.6
>>> RANN_2.6.1
>>> [13] rWBclimate_0.1.3     LaplacesDemon_16.1.1 inlabru_2.1.12.999
>>> brinla_0.1.0
>>> [17] INLA_19.09.03        reshape_0.8.8        lme4_1.1-21
>>> Matrix_1.2-17
>>> [21] GGally_1.4.0         tidyr_1.0.0          plyr_1.8.4
>>> ggplot2_3.2.1
>>> [25] mgcv_1.8-31          nlme_3.1-142         lattice_0.20-38
>>> SpatialEpiApp_0.5
>>> [29] devtools_2.2.1       usethis_1.5.1        glm2_1.2.1
>>> raster_3.0-7
>>> [33] sp_1.3-3
>>     ^^^^^^^^^^^^
>>
>> Please install sp 1.3-2 from CRAN. The 1.3-3 version is only available
>> from my github fork, and can currently only be installed (from source) if
>> rgdal >=1.5-1 is installed from R-Forge from source. It would be helpful
>> if you could say how you managed to install sp 1.3-3 and why and when you
>> did so.
>>
>> When we are ready for GDAL >= 3 and PROJ >= 6, sp, rgdal and sf (with
>> stars and lwgeom) will be announced and released. We are not there yet.
>>
>> Roger
>>
>>
>>>
>>>
>>>
>>> On Wed, Dec 4, 2019 at 6:23 PM Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>>
>>>> Please provide the output of sessionInfo(). You must have installed an
>>>> unreleased development version of sp after having installed an
>> unreleased
>>>> development version of rgdal, then reinstalled a released version of
>> rgdal.
>>>> For rgdal also provide the output of the startup messages with PROJ and
>>>> GDAL versions.
>>>>
>>>> Roger Bivand
>>>> Norwegian School of Economics
>>>> Bergen, Norway
>>>> ------------------------------
>>>> *From:* R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
>>>> Abdoulaye Sarr <abdoulayesar at gmail.com>
>>>> *Sent:* Wednesday, December 4, 2019 6:54:25 PM
>>>> *To:* r-sig-geo <r-sig-geo at r-project.org>
>>>> *Subject:* [R-sig-Geo] ggmap/rgadl
>>>>
>>>> I am using the command below and getting error message I don't
>> understand
>>>> exactly what it is.
>>>> I hope some will help pass this step.
>>>> utmcoor <- SpatialPoints(cbind(OAK$UTM33_X, OAK$UTM33_Y),
>>>> +                          proj4string = CRS("+proj=utm +zone=33"))
>>>>
>>>> Error: 'new_proj_and_gdal' is not an exported object from
>> 'namespace:rgdal'
>>>>
>>>> Kind regards,
>>>>
>>>> Abdou
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From j@ck@onmrodr|gue@ @end|ng |rom gm@||@com  Thu Dec  5 16:20:36 2019
From: j@ck@onmrodr|gue@ @end|ng |rom gm@||@com (Jackson Rodrigues)
Date: Thu, 5 Dec 2019 12:20:36 -0300
Subject: [R-sig-Geo] Error in rgrass7 ... no stars support yet
Message-ID: <CAPL76w_fxWs8m-qttbeX2WvSQsEUUX9uWuaaWHk+VAz8g5cVyQ@mail.gmail.com>

Dear all,

I am trying to work with grass in R but I got the following error:

Error in rgrass7::writeRAST(raster_image, "raster_image", flags =
c("overwrite")) : no stars support yet

Could anyone give me a support on this issue?

I have searched for it elsewhere but no answer found so far.

Reproducible exemple below from Rbloggers

#####################
library(raster)

raster_image <- raster::raster(system.file("external/test.grd", package =
"raster"))
raster_image <- as(raster_image, "SpatialGridDataFrame")
plot(raster_image)

loc <- rgrass7::initGRASS("C:/Program Files/GRASS GIS 7.4.4", home =
tempdir(), override = TRUE)

rgrass7::writeRAST(raster_image, "raster_image", flags = c("overwrite"))
#Error in rgrass7::writeRAST(raster_image, "raster_image", flags =
c("overwrite")) :
#no stars support yet
  #####################

Best regards,

Jackson
--

	[[alternative HTML version deleted]]


From D@n@Ke||ey @end|ng |rom D@|@C@  Fri Dec  6 15:25:45 2019
From: D@n@Ke||ey @end|ng |rom D@|@C@ (Daniel Kelley)
Date: Fri, 6 Dec 2019 14:25:45 +0000
Subject: [R-sig-Geo] PROJ6 rgdal::project/sp::spTransform - issue of points
 that cannot be inverted
Message-ID: <48E8D345-7E4D-4A13-9A04-7645D7B356BD@dal.ca>

>From prior discussions on this thread and elsewhere, I am given to understand that rgdal::project() will not be carried over into PROJ6, and that we are recommendd to use sp::spTransform() instead.

Accordingly, I started work on switching the 'oce' package from rgdal::project() to sp::spTransform().  However, I have a problem with points that cannot be inverted.  With rgdal::project(), we get warning messages if the input contains such points (and the output is Inf for the associated projected points).  However, it seems that with sp::spTransform(), we get an error message and no results, if any of the data contain points that cannot be inverted.  I want to be able to transform a lot of points at the same time (owing to data-set size and speed considerations), so handling the data point-by-point is not an option.

My question is simple: is there a way I can make sp::spTransform() return an equivalent to that of rgdal::project(), with finite data where possible and Inf (or similar) where an inverse cannot be done?

Possibly I am just something.  For anyone who has the patience to look, I have some thoughts at https://github.com/dankelley/oce/issues/1599#issuecomment-562578641 but the gist is as follows: note that 'PROJ' consists of three points, one of which is (of course) Inf, but that 'TRAN' consists only of an error message. Maybe there is an argument to sp::spTransform() that I'm unaware of, that will cause it to return data when it can, and Inf when it cannot?

```
R Under development (unstable) (2019-11-07 r77386) -- "Unsuffered Consequences"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library("rgdal")
> library("sp")
> ll <- cbind(rep(180, 3), c(-89, -90, -89))
> lonlat <- sp::SpatialPoints(ll, sp::CRS("+proj=longlat"))
> xy <- sp::spTransform(lonlat, sp::CRS("+proj=moll"))
> # rgdal::project returns a mixture of results and Inf values
> dput(rgdal::project(sp::coordinates(xy), proj="+proj=moll", inv=TRUE))
structure(c(179.999999999998, Inf, 179.999999999998, -89.0000000000001, 
Inf, -89.0000000000001), .Dim = 3:2, .Dimnames = list(NULL, c("coords.x1", 
"coords.x2")))
> ## but sp::spTransform returns no data at all
> dput(try(sp::spTransform(xy, sp::CRS("+proj=longlat")), silent=TRUE))
non finite transformation detected:
    coords.x1     coords.x2                             
 1.104637e-09 -9.020048e+06           Inf           Inf 
structure("Error in sp::spTransform(xy, sp::CRS(\"+proj=longlat\")) : \n  failure in points 2\n", class = "try-error", condition = structure(list(
    message = "failure in points 2", call = sp::spTransform(xy, 
        sp::CRS("+proj=longlat"))), class = c("simpleError", 
"error", "condition")))
> 
```



Dan Kelley
Department of Oceanography
Dalhousie University
Halifax, NS, Canada


From edzer@pebe@m@ @end|ng |rom un|-muen@ter@de  Fri Dec  6 15:53:37 2019
From: edzer@pebe@m@ @end|ng |rom un|-muen@ter@de (Edzer Pebesma)
Date: Fri, 6 Dec 2019 15:53:37 +0100
Subject: [R-sig-Geo] 
 PROJ6 rgdal::project/sp::spTransform - issue of points
 that cannot be inverted
In-Reply-To: <48E8D345-7E4D-4A13-9A04-7645D7B356BD@dal.ca>
References: <48E8D345-7E4D-4A13-9A04-7645D7B356BD@dal.ca>
Message-ID: <ea15247f-3cf1-0c7c-06cc-91421edf6c4d@uni-muenster.de>

I don't know why Roger wants to deprecate rgdal::project, but for
various reasons I implemented a similar function in sf:

library(sf)
# Linking to GEOS 3.8.0, GDAL 3.0.2, PROJ 6.2.1
ll <- cbind(rep(180, 3), c(-89, -90, -89))
xy = sf_project("+proj=longlat", "+proj=moll", ll)
sf_project("+proj=moll", "+proj=longlat", xy)
#      [,1] [,2]
# [1,]  180  -89
# [2,]  Inf  Inf
# [3,]  180  -89
# Warning message:
# In CPL_proj_direct(as.character(c(from[1], to[1])), as.matrix(pts)) :
#   one or more projected point(s) not finite

As it stands now, I want to keep this function in sf, and would be happy
to add an argument to suppress the warning msg.



On 12/6/19 3:25 PM, Daniel Kelley wrote:
> From prior discussions on this thread and elsewhere, I am given to understand that rgdal::project() will not be carried over into PROJ6, and that we are recommendd to use sp::spTransform() instead.
> 
> Accordingly, I started work on switching the 'oce' package from rgdal::project() to sp::spTransform().  However, I have a problem with points that cannot be inverted.  With rgdal::project(), we get warning messages if the input contains such points (and the output is Inf for the associated projected points).  However, it seems that with sp::spTransform(), we get an error message and no results, if any of the data contain points that cannot be inverted.  I want to be able to transform a lot of points at the same time (owing to data-set size and speed considerations), so handling the data point-by-point is not an option.
> 
> My question is simple: is there a way I can make sp::spTransform() return an equivalent to that of rgdal::project(), with finite data where possible and Inf (or similar) where an inverse cannot be done?
> 
> Possibly I am just something.  For anyone who has the patience to look, I have some thoughts at https://github.com/dankelley/oce/issues/1599#issuecomment-562578641 but the gist is as follows: note that 'PROJ' consists of three points, one of which is (of course) Inf, but that 'TRAN' consists only of an error message. Maybe there is an argument to sp::spTransform() that I'm unaware of, that will cause it to return data when it can, and Inf when it cannot?
> 
> ```
> R Under development (unstable) (2019-11-07 r77386) -- "Unsuffered Consequences"
> Copyright (C) 2019 The R Foundation for Statistical Computing
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
>   Natural language support but running in an English locale
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for an HTML browser interface to help.
> Type 'q()' to quit R.
> 
>> library("rgdal")
>> library("sp")
>> ll <- cbind(rep(180, 3), c(-89, -90, -89))
>> lonlat <- sp::SpatialPoints(ll, sp::CRS("+proj=longlat"))
>> xy <- sp::spTransform(lonlat, sp::CRS("+proj=moll"))
>> # rgdal::project returns a mixture of results and Inf values
>> dput(rgdal::project(sp::coordinates(xy), proj="+proj=moll", inv=TRUE))
> structure(c(179.999999999998, Inf, 179.999999999998, -89.0000000000001, 
> Inf, -89.0000000000001), .Dim = 3:2, .Dimnames = list(NULL, c("coords.x1", 
> "coords.x2")))
>> ## but sp::spTransform returns no data at all
>> dput(try(sp::spTransform(xy, sp::CRS("+proj=longlat")), silent=TRUE))
> non finite transformation detected:
>     coords.x1     coords.x2                             
>  1.104637e-09 -9.020048e+06           Inf           Inf 
> structure("Error in sp::spTransform(xy, sp::CRS(\"+proj=longlat\")) : \n  failure in points 2\n", class = "try-error", condition = structure(list(
>     message = "failure in points 2", call = sp::spTransform(xy, 
>         sp::CRS("+proj=longlat"))), class = c("simpleError", 
> "error", "condition")))
>>
> ```
> 
> 
> 
> Dan Kelley
> Department of Oceanography
> Dalhousie University
> Halifax, NS, Canada
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics
Heisenbergstrasse 2, 48151 Muenster, Germany
Phone: +49 251 8333081

-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 3110 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191206/dcf0e6bd/attachment.bin>

From u@erc@tch @end|ng |rom out|ook@com  Sun Dec  8 21:55:42 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Sun, 8 Dec 2019 20:55:42 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <alpine.LFD.2.21.1912040901070.29869@reclus.nhh.no>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB0768184FA1F1365BA1AF7277B0780@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911070954330.18987@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
 <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911262040120.583221@reclus.nhh.no>,
 <VI1P190MB07680F075EA1B55A9AE9D56AB0450@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <3cd7666ef730416c8e7f6f4110315f0a@nhh.no>
 <VI1P190MB076828E385B2E65EB3CD4093B0420@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1912040901070.29869@reclus.nhh.no>
Message-ID: <VI1P190MB07684BF8B370053F54EB1DD6B0590@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Roger,

Thank you for your answer. Regarding the sparse matrix, you're right - I tested creating one, as follows:

#####

listings_nb <- listings %>% spdep::poly2nb(queen = TRUE) %>% spdep::include.self()
# is.symmetric.nb(listings_nb)

nb_B  <- spdep::nb2listw(neighbours = listings_nb, style="B", zero.policy = FALSE)

B <- as(nb_B , "CsparseMatrix")
all(B == t(B))

nb_B1 <- mat2listw(as(B, "dgTMatrix"))

format(object.size(nb_B), units = "Mb")
# 85.6 Mb

format(object.size(nb_B1), units = "Mb")
# 85.6 Mb

#####

The size for both objects is the same - different from this example: https://cran.r-project.org/web/packages/spdep/vignettes/nb_igraph.html


Summing up the data that I have: a "picture" for Airbnb's listings/ads once a month for NYC, incl. lat, lon, price (per night), id, and some characteristics from the listing/ad as number of rooms, bedrooms, guests included, etc. for a period of 54 months. The data was taken here: http://insideairbnb.com/get-the-data.html (listings.csv.gz)

For the OLS, I used a pooled OLS with time dummy fixed effects (date_compiled, when the "picture" was compiled - how Airbnb listings for NYC were shown) because many of my observations (listings id) do not repeat for many periods. Also, many listings changed the room_type at least once during the whole time period analyzed (3 types of room_type: entire home/apt, private room, shared room).

I am now trying a random intercepts three-level hierarchy multilevel model, where _id_ (level 1) are nested within _zipcode_ (level 2), and the last is nested within _borough_ (level 3). So groups: id:(zipcode:borough).

lme4::lmer(log_price ~ factor(room_type) + bedrooms + bathrooms + guests_included + minimum_nights + distance_downtown + distance_subway + number_of_reviews + review_scores_cleanliness + professional_host + host_is_superhost + is_business_travel_ready + offense_misdemeanor + offense_felony + income_per_capita + factor(date_compiled) + (1 | borough / zipcode / id), data = listings)

Roger, do you think it is okay to factor(room_type) (for the 3 types of room) and factor(date_compiled) for the dates when the NYC Airbnb's listings/ads were extracted?

Thank you and best regards,
Robert

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Wednesday, December 4, 2019 09:07
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

On Wed, 4 Dec 2019, Robert R wrote:

> Dear Roger,
>
> Again, thank you for your answer. What do you mean by "zip code random
> effect"? You mean I should use in plm the model "random"?
>
> regression_re <- plm(formula = model, data = listings, model = "random",
> index = c("id", "date_compiled"))

No, obviously not, your data are not a balanced panel. I mean a multilevel
model, where the <200 zip codes cluster the data, and where a zip code
level IID RE will almost certainly do a better job than dummies. An
MRF/ICAR RE might be an extension.

>
> And any other methodology in dealing with large weight matrices in
> spatialreg::lagsarlm?

Please refer to Bivand et al. (2013) refered to in the package. Probably
the weights would need to be symmetric and very sparse.

I still think that you should focus on a small subset of the data and to
improving the signal-noise ratio before trying to scale up.

Roger

>
> Thank you and best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Wednesday, November 27, 2019 13:53
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: SV: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> Yes this is expected, since the # neighbours in a single zip code block is a dense matrix, and there will be multiple such matrices. (15000^2)*8 is 1.8e+09 so such a dense matrix will max out your RAM. There is no way to look at block neighbours in that format without subsetting your data (think train/test), use a zip code random effect. I would certainly drop all attempts to examine spatial dependency until you get an aspatial multilevel hedonic model working.
>
> Roger
>
> --
> Roger Bivand
> Norwegian School of Economics
> Helleveien 30, 5045 Bergen, Norway
> Roger.Bivand at nhh.no
>
>
> ________________________________________
> Fra: Robert R <usercatch at outlook.com>
> Sendt: tirsdag 26. november 2019 21.04
> Til: Roger Bivand
> Kopi: r-sig-geo at r-project.org
> Emne: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> Dear Roger,
>
> Thank you for your e-mail. Actually there is less noise that it seems. Rental prices are daily rental prices and I have an extract of all Airbnb listings daily prices once a month for a period of 4 years. Each listings information contains the lat, lon, number of bedrooms, category (entire home/apt, shared room or private room), etc.
>
> One question regarding the spdep::nb2blocknb function: it runs super fast with up to n = 1000, and always crashes my R session with n = 15000 or so. Is there an alternative to solve this problem?
>
> Thank you and best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Tuesday, November 26, 2019 20:48
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> Sorry for late reply, am indisposed and unable to help further. I feel
> that there is so much noise in your data (differences in offers, rental
> lengths, repeats or not, etc.), that you will certainly have to subset
> vigorously first to isolate response cases that are comparable. What you
> are trying to disentangle are the hedonic components in the bundle where
> you just have price as response, but lots of other bundle characteristics
> on the right hand side (days, etc.). I feel you'd need to try to get to a
> response index of price per day per rental area or some such. I'd
> certainly advise examining responses to a specific driver (major concert
> or sports event) to get a feel for how the market responds, and return to
> spatial hedonic after finding an approach that gives reasonable aspatial
> outcomes.
>
> Roger
>
> On Sun, 17 Nov 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Thank you for your message and sorry for my late answer.
>>
>> Regarding the number of listings (lettings) for my data set (2.216.642 observations), each listing contains an individual id:
>>
>> unique ids: 180.004
>> time periods: 54 (2015-01 to 2019-09)
>> number of ids that appear only once: 28.486 (of 180.004 ids) (15,8%)
>> number of ids that appear/repeat 2-10 times: 82.641 (of 180.004 ids) (45,9%)
>> number of ids that appear/repeat 11-30 times: 46.465 (of 180.004 ids) (25,8%)
>> number of ids that appear/repeat 31-54 times: 22.412 (of 180.004 ids) (12,5%)
>>
>> Important to notice is that hosts can change the room_category (between entire/home apt, private room and shared room) keeping the same listing id number. In my data, the number of unique ids that in some point changed the room_type is of 7.204 ids.
>>
>> --
>>
>> For the OLS model, I was using only a fixed effect model, where each time period (date_compiled) (54 in total) is a time dummy.
>>
>> plm::plm(formula = model, data = listings, model = "pooling", index = c("id", "date_compiled"))
>>
>>
>> --
>> Osland et al. (2016) (https://doi.org/10.1111/jors.12281) use a spatial fixed effects (SFE) hedonic model, where each defined neighborhood zone in the study area is represented by dummy variables.
>>
>> Dong et al. (2015) (https://doi.org/10.1111/gean.12049) outline four model specifications to accommodate geographically hierarchical data structures: (1) groupwise W and fixed regional effects; (2) groupwise W and random regional effects; (3) proximity-based W and fixed regional effects; and (4) proximity-based W and random regional effects.
>> --
>>
>> I created a new column/variable containing the borough where the zipcode is found (Manhattan, Brooklyn, Queens, Bronx, Staten Island).
>>
>> If I understood it right, the (two-level) Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) considers the occurrence of spatial relations at the (lower) individual (geographical coordinates - in my case, the listing location) and (higher) group level (territorial units - in my case, zipcodes).
>>
>> According to Bivand et al. (2017): "(...) W is a spatial weights matrix. The HSAR model may also be estimated without this component.". So, in this case I only estimate the Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) in a "one-level" basis, i.e., at the higher-level.
>>
>> HSAR::hsar(model, data = listings, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>
>> (Where the "model" formula contains the 54 time dummy variables)
>>
>> Do you think I can proceed with this model? I was able to calculate it.
>>
>> If I remove all observations/rows with NAs in one of the chosen variables/observations, 884.183 observations remain. If I would create a W matrix for HSAR::hsar, I would have a gigantic 884.183 by 884.183 matrix. This is the reason why I put W = NULL.
>>
>>
>> Thank you and best regards
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Monday, November 11, 2019 11:31
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> On Sun, 10 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Again, thank you for your answer. I read the material provided and
>>> decided that Hierarchical Spatial Autoregressive (HSAR) could be the
>>> right model for me.
>>>
>>> I indeed have the precise latitude and longitude information for all my
>>> listings for NYC.
>>>
>>> I created a stratified sample (group = zipcode) with 22172 (1%) of my
>>> observations called listings_sample and tried to replicate the hsar
>>> model, please see below.
>>>
>>> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.
>>
>> Unless you know definitely that you want to relate the response to its
>> lagged value, you do not need this. Do note that the matrix is very
>> sparse, so could be fitted without difficulty with ML in a cross-sectional
>> model.
>>
>>>
>>> You recommended then to introduce a Markov random field (MRF) random
>>> effect (RE) at the zipcode level, but I did not understand it so well.
>>> Could you develop a litte more?
>>>
>>
>> Did you read the development in
>> https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and
>> includes code for fitting the Beijing housing parcels data se from HSAR
>> with many other packages (MCMC, INLA, hglm, etc.). I guess that you should
>> try to create a model that works on a single borough, sing the zipcodes
>> in that borough as a proxy for unobserved neighbourhood effects. Try for
>> example using lme4::lmer() with only a zipcode IID random effect, see if
>> the hedonic estimates are similar to lm(), and leave adding an MRF RE
>> (with for example mgcv::gam() or hglm::hglm()) until you have a working
>> testbed. Then advance step-by-step from there.
>>
>> You still have not said how many repeat lettings you see - it will affect
>> the way you specify your model.
>>
>> Roger
>>
>>> ##############
>>> library(spdep)
>>> library(HSAR)
>>> library(dplyr)
>>> library(splitstackshape)
>>>
>>>
>>> # Stratified sample per zipcode (size = 1%) listings_sample <-
>>> splitstackshape::stratified(indt = listings, group = "zipcode", size =
>>> 0.01)
>>>
>>> # Removing zipcodes from polygon_nyc which are not observable in
>>> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode
>>> %in% c(unique(as.character(listings_sample$zipcode))))
>>>
>>>
>>> ## Random effect matrix (N by J)
>>>
>>> # N: 22172
>>> # J: 154
>>>
>>> # Arrange listings_sample by zipcode (ascending)
>>> listings_sample <- listings_sample %>% arrange(zipcode)
>>>
>>> # Count number of listings per zipcode
>>> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
>>> # sum(MM$count)
>>>
>>> # N by J nulled matrix creation
>>> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>>>
>>> # The total number of neighbourhood
>>> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>>>
>>> for(i in 1:dim(MM)[1]) {
>>>  Delta[Uid==i,i] <- 1
>>> }
>>> rm(i)
>>>
>>> Delta <- as(Delta,"dgCMatrix")
>>>
>>>
>>> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>>>
>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>>>
>>> # Include neighbour itself as a neighbour
>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>
>>> # Spatial weights matrix for nb
>>> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
>>> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>>>
>>>
>>> ## Fit HSAR SAR upper level random effect
>>> model <- as.formula(log_price ~ guests_included + minimum_nights)
>>>
>>> betas = coef(lm(formula = model, data = listings_sample))
>>> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>>>
>>> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>>
>>> ##############
>>>
>>> Thank you and best regards
>>> Robert
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Friday, November 8, 2019 13:29
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Fri, 8 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Thank you for your answer.
>>>>
>>>> I successfully used the function nb2blocknb() for a smaller dataset.
>>>>
>>>> But for a dataset of over 2 million observations, I get the following
>>>> error: "Error: cannot allocate vector of size 840 Kb".
>>>
>>> I don't think the observations are helpful. If you have repeat lets in the
>>> same property in a given month, you need to handle that anyway. I'd go for
>>> making the modelling exercise work (we agree that this is not panel data,
>>> right?) on a small subset first. I would further argue that you need a
>>> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
>>> RE. You could very well take (stratified) samples per zipcode to represent
>>> your data. Once that works, introduce an MRF RE at the zipcode level,
>>> where you do know relative position. Using SARAR is going to be a waste of
>>> time unless you can geocode the letting addresses. A multi-level approach
>>> will work. Having big data in your case with no useful location
>>> information per observation is just adding noise and over-smoothing, I'm
>>> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
>>> will work, also when you sample the within zipcode lets, given a split
>>> into training and test sets, and making CV possible.
>>>
>>> Roger
>>>
>>>>
>>>> I am expecting that at least 500.000 observations will be dropped due
>>>> the lack of values for the chosen variables for the regression model, so
>>>> probably I will filter and remove the observations/rows that will not be
>>>> used anyway - do you know if there is any package that does this
>>>> automatically, given the variables/columns chosed by me?
>>>>
>>>> Or would you recommend me another approach to avoid the above mentioned
>>>> error?
>>>>
>>>> Thank you and best regards,
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Thursday, November 7, 2019 10:13
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Thu, 7 Nov 2019, Robert R wrote:
>>>>
>>>>> Dear Roger,
>>>>>
>>>>> Many thanks for your help.
>>>>>
>>>>> I have an additional question:
>>>>>
>>>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>>>> rownumbers) from my data set? For now, I am taking my data set and
>>>>> merging with the sf object polygon_nyc with the function
>>>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>>>> a huge n x n matrix (depending of the size of my data set).
>>>>>
>>>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>>>> object has only n = 177.
>>>>>
>>>>> Of course running
>>>>>
>>>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>>>
>>>>> does not work ("Input data and weights have different dimensions").
>>>>>
>>>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>>>> zipcode) and then create the weights list lw? Or there another option?
>>>>
>>>> I think we are getting more clarity. You do not know the location of the
>>>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>>>> areas, and can create a neighbour object from these boundaries. You then
>>>> want to treat all the lettings in a zipcode area i as neighbours, and
>>>> additionally lettings in zipcode areas neighbouring i as neighbours of
>>>> lettings in i. This is the data structure that motivated the
>>>> spdep::nb2blocknb() function:
>>>>
>>>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>>>
>>>> Try running the examples to get a feel for what is going on.
>>>>
>>>> I feel that most of the variability will vanish in the very large numbers
>>>> of neighbours, over-smoothing the outcomes. If you do not have locations
>>>> for the lettings themselves, I don't think you can make much progress.
>>>>
>>>> You could try a linear mixed model (or gam with a spatially structured
>>>> random effect) with a temporal and a spatial random effect. See the HSAR
>>>> package, articles by Dong et al., and maybe
>>>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>>>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>>>> effects at the zipcode level might be a way forward, together with an IID
>>>> random effect at the same level (equivalent to sef-neighbours).
>>>>
>>>> Hope this helps,
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> Best regards,
>>>>> Robert
>>>>>
>>>>> ________________________________________
>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>> Sent: Wednesday, November 6, 2019 15:07
>>>>> To: Robert R
>>>>> Cc: r-sig-geo at r-project.org
>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>
>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>
>>>>>> Dear Roger,
>>>>>>
>>>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>>>> plain text.
>>>>>>
>>>>>> I will give a better context for my desired outcome.
>>>>>>
>>>>>> I am taking Airbnb's listings information for New York City available
>>>>>> on: http://insideairbnb.com/get-the-data.html
>>>>>>
>>>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>>>> Listings/ folder. When importing all these 54 files into one single data
>>>>>> set, I create a new "date_compiled" variable/column.
>>>>>>
>>>>>> In total, after the data cleansing process, I have a little more 2
>>>>>> million observations.
>>>>>
>>>>> You have repeat lettings for some, but not all properties. So this is at
>>>>> best a very unbalanced panel. For those properties with repeats, you may
>>>>> see temporal movement (trend/seasonal).
>>>>>
>>>>> I suggest (strongly) taking a single borough or even zipcode with some
>>>>> hindreds of properties, and working from there. Do not include the
>>>>> observation as its own neighbour, perhaps identify repeats and handle them
>>>>> specially (create or use a property ID). Unbalanced panels may also create
>>>>> a selection bias issue (why are some properties only listed sometimes?).
>>>>>
>>>>> So this although promising isn't simple, and getting to a hedonic model
>>>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>>>> necessarily trust OLS output either, partly because of the repeat property
>>>>> issue.
>>>>>
>>>>> Roger
>>>>>
>>>>>>
>>>>>> I created 54 timedummy variables for each time period available.
>>>>>>
>>>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>>>> a variety of characteristics which potentially determine the daily rate
>>>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>>>> of the listing as number of bedrooms, if the host if professional,
>>>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>>>> from the listing, income per capita, etc.).
>>>>>>
>>>>>> My dependent variable is price (log price, common in the related
>>>>>> literature for hedonic prices).
>>>>>>
>>>>>> The OLS model is done.
>>>>>>
>>>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>>>> pricing of their listings, take not only into account its structural and
>>>>>> location characteristics, but also the prices charged by near listings
>>>>>> with similar characteristics - spatial autocorrelation is then present,
>>>>>> at least spatial dependence is present in the dependent variable.
>>>>>>
>>>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>>>> itself as a neighbor.
>>>>>>
>>>>>> Parts of my code can be found below:
>>>>>>
>>>>>> ########
>>>>>>
>>>>>> ## packages
>>>>>>
>>>>>> packages_install <- function(packages){
>>>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>>>> if (length(new.packages))
>>>>>> install.packages(new.packages, dependencies = TRUE)
>>>>>> sapply(packages, require, character.only = TRUE)
>>>>>> }
>>>>>>
>>>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>>>> packages_install(packages_required)
>>>>>>
>>>>>> # Working directory
>>>>>> setwd("C:/Users/User/R")
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## shapefile_us
>>>>>>
>>>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>>>
>>>>>> # Columns removal
>>>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>>>
>>>>>> # Column rename: ZCTA5CE10
>>>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>>>
>>>>>> # Column class change: zipcode
>>>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## polygon_nyc
>>>>>>
>>>>>> # Zip code not available in shapefile: 11695
>>>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## weight_matrix
>>>>>>
>>>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>>>
>>>>>> # Include neighbour itself as a neighbour
>>>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>>>
>>>>>> # Weights to each neighboring polygon
>>>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## listings
>>>>>>
>>>>>> # Data import
>>>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>>>> listings <- listings %>% bind_rows
>>>>>>
>>>>>> # Characters removal
>>>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>>>> listings$price <- gsub("\\$", "", listings$price)
>>>>>> listings$price <- gsub(",", "", listings$price)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## timedummy
>>>>>>
>>>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>>>> timedummy <- gsub("-", "_", timedummy)
>>>>>>
>>>>>>
>>>>>>
>>>>>> ## OLS regression
>>>>>>
>>>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>>>
>>>>>> ########
>>>>>>
>>>>>> Some of my id's repeat in multiple time periods.
>>>>>>
>>>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>>>
>>>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>>>
>>>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>>>
>>>>>> Again, thank you very much for the help provided until now.
>>>>>>
>>>>>> Best regards,
>>>>>> Robert
>>>>>>
>>>>>> ________________________________________
>>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>>> Sent: Tuesday, November 5, 2019 15:30
>>>>>> To: Robert R
>>>>>> Cc: r-sig-geo at r-project.org
>>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>>
>>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>>
>>>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>>>> for now that spatial dependence is present in both the dependent
>>>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>>>> think it would be possible to calculate this.? What I would like to know
>>>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>>>
>>>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>>>> of the covariates time-varying? Please provide motivation and use area
>>>>>> (preferably with affiliation (your email and user name are not
>>>>>> informative) - this feels like a real estate problem, probably wrongly
>>>>>> specified. You should use splm if time make sense in your case, but if it
>>>>>> really doesn't, simplify your approach, as much of the data will be
>>>>>> subject to very large temporal autocorrelation.
>>>>>>
>>>>>> If this is a continuation of your previous question about using
>>>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>>>
>>>>>> Roger
>>>>>>
>>>>>>>
>>>>>>>       [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> R-sig-Geo mailing list
>>>>>>> R-sig-Geo at r-project.org
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>>
>>>>>> --
>>>>>> Roger Bivand
>>>>>> Department of Economics, Norwegian School of Economics,
>>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>>> https://orcid.org/0000-0003-2392-6140
>>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>>
>>>>>
>>>>> --
>>>>> Roger Bivand
>>>>> Department of Economics, Norwegian School of Economics,
>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>> https://orcid.org/0000-0003-2392-6140
>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Dec  9 12:14:41 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 9 Dec 2019 12:14:41 +0100
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <VI1P190MB07684BF8B370053F54EB1DD6B0590@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
 <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911262040120.583221@reclus.nhh.no>,
 <VI1P190MB07680F075EA1B55A9AE9D56AB0450@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <3cd7666ef730416c8e7f6f4110315f0a@nhh.no>
 <VI1P190MB076828E385B2E65EB3CD4093B0420@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1912040901070.29869@reclus.nhh.no>
 <VI1P190MB07684BF8B370053F54EB1DD6B0590@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>
Message-ID: <alpine.LFD.2.21.1912091210130.60452@reclus.nhh.no>

On Sun, 8 Dec 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your answer. Regarding the sparse matrix, you're right - I 
> tested creating one, as follows:
>
> #####
>
> listings_nb <- listings %>% spdep::poly2nb(queen = TRUE) %>% 
> spdep::include.self() # is.symmetric.nb(listings_nb)

Are listings point or polygon support (must be polygon for this function)?

>
> nb_B <- spdep::nb2listw(neighbours = listings_nb, style="B", zero.policy 
> = FALSE)
>
> B <- as(nb_B , "CsparseMatrix")
> all(B == t(B))
>
> nb_B1 <- mat2listw(as(B, "dgTMatrix"))
>
> format(object.size(nb_B), units = "Mb")
> # 85.6 Mb
>
> format(object.size(nb_B1), units = "Mb")
> # 85.6 Mb
>
> #####
>
> The size for both objects is the same - different from this example: 
> https://cran.r-project.org/web/packages/spdep/vignettes/nb_igraph.html
>
>
> Summing up the data that I have: a "picture" for Airbnb's listings/ads 
> once a month for NYC, incl. lat, lon, price (per night), id, and some 
> characteristics from the listing/ad as number of rooms, bedrooms, guests 
> included, etc. for a period of 54 months. The data was taken here: 
> http://insideairbnb.com/get-the-data.html (listings.csv.gz)
>
> For the OLS, I used a pooled OLS with time dummy fixed effects 
> (date_compiled, when the "picture" was compiled - how Airbnb listings 
> for NYC were shown) because many of my observations (listings id) do not 
> repeat for many periods. Also, many listings changed the room_type at 
> least once during the whole time period analyzed (3 types of room_type: 
> entire home/apt, private room, shared room).
>

OK, I suppose.

> I am now trying a random intercepts three-level hierarchy multilevel 
> model, where _id_ (level 1) are nested within _zipcode_ (level 2), and 
> the last is nested within _borough_ (level 3). So groups: 
> id:(zipcode:borough).
>

I'd try and see what happens, and watch machine resources (memory anyway).

> lme4::lmer(log_price ~ factor(room_type) + bedrooms + bathrooms + 
> guests_included + minimum_nights + distance_downtown + distance_subway + 
> number_of_reviews + review_scores_cleanliness + professional_host + 
> host_is_superhost + is_business_travel_ready + offense_misdemeanor + 
> offense_felony + income_per_capita + factor(date_compiled) + (1 | 
> borough / zipcode / id), data = listings)
>
> Roger, do you think it is okay to factor(room_type) (for the 3 types of 
> room) and factor(date_compiled) for the dates when the NYC Airbnb's 
> listings/ads were extracted?

Do you see structured variability coming from these - if so, inclusion 
makes sense.

Roger

>
> Thank you and best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Wednesday, December 4, 2019 09:07
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Wed, 4 Dec 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Again, thank you for your answer. What do you mean by "zip code random
>> effect"? You mean I should use in plm the model "random"?
>>
>> regression_re <- plm(formula = model, data = listings, model = "random",
>> index = c("id", "date_compiled"))
>
> No, obviously not, your data are not a balanced panel. I mean a multilevel
> model, where the <200 zip codes cluster the data, and where a zip code
> level IID RE will almost certainly do a better job than dummies. An
> MRF/ICAR RE might be an extension.
>
>>
>> And any other methodology in dealing with large weight matrices in
>> spatialreg::lagsarlm?
>
> Please refer to Bivand et al. (2013) refered to in the package. Probably
> the weights would need to be symmetric and very sparse.
>
> I still think that you should focus on a small subset of the data and to
> improving the signal-noise ratio before trying to scale up.
>
> Roger
>
>>
>> Thank you and best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Wednesday, November 27, 2019 13:53
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: SV: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> Yes this is expected, since the # neighbours in a single zip code block is a dense matrix, and there will be multiple such matrices. (15000^2)*8 is 1.8e+09 so such a dense matrix will max out your RAM. There is no way to look at block neighbours in that format without subsetting your data (think train/test), use a zip code random effect. I would certainly drop all attempts to examine spatial dependency until you get an aspatial multilevel hedonic model working.
>>
>> Roger
>>
>> --
>> Roger Bivand
>> Norwegian School of Economics
>> Helleveien 30, 5045 Bergen, Norway
>> Roger.Bivand at nhh.no
>>
>>
>> ________________________________________
>> Fra: Robert R <usercatch at outlook.com>
>> Sendt: tirsdag 26. november 2019 21.04
>> Til: Roger Bivand
>> Kopi: r-sig-geo at r-project.org
>> Emne: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> Dear Roger,
>>
>> Thank you for your e-mail. Actually there is less noise that it seems. Rental prices are daily rental prices and I have an extract of all Airbnb listings daily prices once a month for a period of 4 years. Each listings information contains the lat, lon, number of bedrooms, category (entire home/apt, shared room or private room), etc.
>>
>> One question regarding the spdep::nb2blocknb function: it runs super fast with up to n = 1000, and always crashes my R session with n = 15000 or so. Is there an alternative to solve this problem?
>>
>> Thank you and best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Tuesday, November 26, 2019 20:48
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> Sorry for late reply, am indisposed and unable to help further. I feel
>> that there is so much noise in your data (differences in offers, rental
>> lengths, repeats or not, etc.), that you will certainly have to subset
>> vigorously first to isolate response cases that are comparable. What you
>> are trying to disentangle are the hedonic components in the bundle where
>> you just have price as response, but lots of other bundle characteristics
>> on the right hand side (days, etc.). I feel you'd need to try to get to a
>> response index of price per day per rental area or some such. I'd
>> certainly advise examining responses to a specific driver (major concert
>> or sports event) to get a feel for how the market responds, and return to
>> spatial hedonic after finding an approach that gives reasonable aspatial
>> outcomes.
>>
>> Roger
>>
>> On Sun, 17 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Thank you for your message and sorry for my late answer.
>>>
>>> Regarding the number of listings (lettings) for my data set (2.216.642 observations), each listing contains an individual id:
>>>
>>> unique ids: 180.004
>>> time periods: 54 (2015-01 to 2019-09)
>>> number of ids that appear only once: 28.486 (of 180.004 ids) (15,8%)
>>> number of ids that appear/repeat 2-10 times: 82.641 (of 180.004 ids) (45,9%)
>>> number of ids that appear/repeat 11-30 times: 46.465 (of 180.004 ids) (25,8%)
>>> number of ids that appear/repeat 31-54 times: 22.412 (of 180.004 ids) (12,5%)
>>>
>>> Important to notice is that hosts can change the room_category (between entire/home apt, private room and shared room) keeping the same listing id number. In my data, the number of unique ids that in some point changed the room_type is of 7.204 ids.
>>>
>>> --
>>>
>>> For the OLS model, I was using only a fixed effect model, where each time period (date_compiled) (54 in total) is a time dummy.
>>>
>>> plm::plm(formula = model, data = listings, model = "pooling", index = c("id", "date_compiled"))
>>>
>>>
>>> --
>>> Osland et al. (2016) (https://doi.org/10.1111/jors.12281) use a spatial fixed effects (SFE) hedonic model, where each defined neighborhood zone in the study area is represented by dummy variables.
>>>
>>> Dong et al. (2015) (https://doi.org/10.1111/gean.12049) outline four model specifications to accommodate geographically hierarchical data structures: (1) groupwise W and fixed regional effects; (2) groupwise W and random regional effects; (3) proximity-based W and fixed regional effects; and (4) proximity-based W and random regional effects.
>>> --
>>>
>>> I created a new column/variable containing the borough where the zipcode is found (Manhattan, Brooklyn, Queens, Bronx, Staten Island).
>>>
>>> If I understood it right, the (two-level) Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) considers the occurrence of spatial relations at the (lower) individual (geographical coordinates - in my case, the listing location) and (higher) group level (territorial units - in my case, zipcodes).
>>>
>>> According to Bivand et al. (2017): "(...) W is a spatial weights matrix. The HSAR model may also be estimated without this component.". So, in this case I only estimate the Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) in a "one-level" basis, i.e., at the higher-level.
>>>
>>> HSAR::hsar(model, data = listings, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>>
>>> (Where the "model" formula contains the 54 time dummy variables)
>>>
>>> Do you think I can proceed with this model? I was able to calculate it.
>>>
>>> If I remove all observations/rows with NAs in one of the chosen variables/observations, 884.183 observations remain. If I would create a W matrix for HSAR::hsar, I would have a gigantic 884.183 by 884.183 matrix. This is the reason why I put W = NULL.
>>>
>>>
>>> Thank you and best regards
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Monday, November 11, 2019 11:31
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Sun, 10 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Again, thank you for your answer. I read the material provided and
>>>> decided that Hierarchical Spatial Autoregressive (HSAR) could be the
>>>> right model for me.
>>>>
>>>> I indeed have the precise latitude and longitude information for all my
>>>> listings for NYC.
>>>>
>>>> I created a stratified sample (group = zipcode) with 22172 (1%) of my
>>>> observations called listings_sample and tried to replicate the hsar
>>>> model, please see below.
>>>>
>>>> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.
>>>
>>> Unless you know definitely that you want to relate the response to its
>>> lagged value, you do not need this. Do note that the matrix is very
>>> sparse, so could be fitted without difficulty with ML in a cross-sectional
>>> model.
>>>
>>>>
>>>> You recommended then to introduce a Markov random field (MRF) random
>>>> effect (RE) at the zipcode level, but I did not understand it so well.
>>>> Could you develop a litte more?
>>>>
>>>
>>> Did you read the development in
>>> https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and
>>> includes code for fitting the Beijing housing parcels data se from HSAR
>>> with many other packages (MCMC, INLA, hglm, etc.). I guess that you should
>>> try to create a model that works on a single borough, sing the zipcodes
>>> in that borough as a proxy for unobserved neighbourhood effects. Try for
>>> example using lme4::lmer() with only a zipcode IID random effect, see if
>>> the hedonic estimates are similar to lm(), and leave adding an MRF RE
>>> (with for example mgcv::gam() or hglm::hglm()) until you have a working
>>> testbed. Then advance step-by-step from there.
>>>
>>> You still have not said how many repeat lettings you see - it will affect
>>> the way you specify your model.
>>>
>>> Roger
>>>
>>>> ##############
>>>> library(spdep)
>>>> library(HSAR)
>>>> library(dplyr)
>>>> library(splitstackshape)
>>>>
>>>>
>>>> # Stratified sample per zipcode (size = 1%) listings_sample <-
>>>> splitstackshape::stratified(indt = listings, group = "zipcode", size =
>>>> 0.01)
>>>>
>>>> # Removing zipcodes from polygon_nyc which are not observable in
>>>> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode
>>>> %in% c(unique(as.character(listings_sample$zipcode))))
>>>>
>>>>
>>>> ## Random effect matrix (N by J)
>>>>
>>>> # N: 22172
>>>> # J: 154
>>>>
>>>> # Arrange listings_sample by zipcode (ascending)
>>>> listings_sample <- listings_sample %>% arrange(zipcode)
>>>>
>>>> # Count number of listings per zipcode
>>>> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
>>>> # sum(MM$count)
>>>>
>>>> # N by J nulled matrix creation
>>>> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>>>>
>>>> # The total number of neighbourhood
>>>> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>>>>
>>>> for(i in 1:dim(MM)[1]) {
>>>>  Delta[Uid==i,i] <- 1
>>>> }
>>>> rm(i)
>>>>
>>>> Delta <- as(Delta,"dgCMatrix")
>>>>
>>>>
>>>> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>>>>
>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>>>>
>>>> # Include neighbour itself as a neighbour
>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>
>>>> # Spatial weights matrix for nb
>>>> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
>>>> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>>>>
>>>>
>>>> ## Fit HSAR SAR upper level random effect
>>>> model <- as.formula(log_price ~ guests_included + minimum_nights)
>>>>
>>>> betas = coef(lm(formula = model, data = listings_sample))
>>>> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>>>>
>>>> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>>>
>>>> ##############
>>>>
>>>> Thank you and best regards
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Friday, November 8, 2019 13:29
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Fri, 8 Nov 2019, Robert R wrote:
>>>>
>>>>> Dear Roger,
>>>>>
>>>>> Thank you for your answer.
>>>>>
>>>>> I successfully used the function nb2blocknb() for a smaller dataset.
>>>>>
>>>>> But for a dataset of over 2 million observations, I get the following
>>>>> error: "Error: cannot allocate vector of size 840 Kb".
>>>>
>>>> I don't think the observations are helpful. If you have repeat lets in the
>>>> same property in a given month, you need to handle that anyway. I'd go for
>>>> making the modelling exercise work (we agree that this is not panel data,
>>>> right?) on a small subset first. I would further argue that you need a
>>>> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
>>>> RE. You could very well take (stratified) samples per zipcode to represent
>>>> your data. Once that works, introduce an MRF RE at the zipcode level,
>>>> where you do know relative position. Using SARAR is going to be a waste of
>>>> time unless you can geocode the letting addresses. A multi-level approach
>>>> will work. Having big data in your case with no useful location
>>>> information per observation is just adding noise and over-smoothing, I'm
>>>> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
>>>> will work, also when you sample the within zipcode lets, given a split
>>>> into training and test sets, and making CV possible.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> I am expecting that at least 500.000 observations will be dropped due
>>>>> the lack of values for the chosen variables for the regression model, so
>>>>> probably I will filter and remove the observations/rows that will not be
>>>>> used anyway - do you know if there is any package that does this
>>>>> automatically, given the variables/columns chosed by me?
>>>>>
>>>>> Or would you recommend me another approach to avoid the above mentioned
>>>>> error?
>>>>>
>>>>> Thank you and best regards,
>>>>> Robert
>>>>>
>>>>> ________________________________________
>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>> Sent: Thursday, November 7, 2019 10:13
>>>>> To: Robert R
>>>>> Cc: r-sig-geo at r-project.org
>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>
>>>>> On Thu, 7 Nov 2019, Robert R wrote:
>>>>>
>>>>>> Dear Roger,
>>>>>>
>>>>>> Many thanks for your help.
>>>>>>
>>>>>> I have an additional question:
>>>>>>
>>>>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>>>>> rownumbers) from my data set? For now, I am taking my data set and
>>>>>> merging with the sf object polygon_nyc with the function
>>>>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>>>>> a huge n x n matrix (depending of the size of my data set).
>>>>>>
>>>>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>>>>> object has only n = 177.
>>>>>>
>>>>>> Of course running
>>>>>>
>>>>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>>>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>>>>
>>>>>> does not work ("Input data and weights have different dimensions").
>>>>>>
>>>>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>>>>> zipcode) and then create the weights list lw? Or there another option?
>>>>>
>>>>> I think we are getting more clarity. You do not know the location of the
>>>>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>>>>> areas, and can create a neighbour object from these boundaries. You then
>>>>> want to treat all the lettings in a zipcode area i as neighbours, and
>>>>> additionally lettings in zipcode areas neighbouring i as neighbours of
>>>>> lettings in i. This is the data structure that motivated the
>>>>> spdep::nb2blocknb() function:
>>>>>
>>>>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>>>>
>>>>> Try running the examples to get a feel for what is going on.
>>>>>
>>>>> I feel that most of the variability will vanish in the very large numbers
>>>>> of neighbours, over-smoothing the outcomes. If you do not have locations
>>>>> for the lettings themselves, I don't think you can make much progress.
>>>>>
>>>>> You could try a linear mixed model (or gam with a spatially structured
>>>>> random effect) with a temporal and a spatial random effect. See the HSAR
>>>>> package, articles by Dong et al., and maybe
>>>>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>>>>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>>>>> effects at the zipcode level might be a way forward, together with an IID
>>>>> random effect at the same level (equivalent to sef-neighbours).
>>>>>
>>>>> Hope this helps,
>>>>>
>>>>> Roger
>>>>>
>>>>>>
>>>>>> Best regards,
>>>>>> Robert
>>>>>>
>>>>>> ________________________________________
>>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>>> Sent: Wednesday, November 6, 2019 15:07
>>>>>> To: Robert R
>>>>>> Cc: r-sig-geo at r-project.org
>>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>>
>>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>>
>>>>>>> Dear Roger,
>>>>>>>
>>>>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>>>>> plain text.
>>>>>>>
>>>>>>> I will give a better context for my desired outcome.
>>>>>>>
>>>>>>> I am taking Airbnb's listings information for New York City available
>>>>>>> on: http://insideairbnb.com/get-the-data.html
>>>>>>>
>>>>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>>>>> Listings/ folder. When importing all these 54 files into one single data
>>>>>>> set, I create a new "date_compiled" variable/column.
>>>>>>>
>>>>>>> In total, after the data cleansing process, I have a little more 2
>>>>>>> million observations.
>>>>>>
>>>>>> You have repeat lettings for some, but not all properties. So this is at
>>>>>> best a very unbalanced panel. For those properties with repeats, you may
>>>>>> see temporal movement (trend/seasonal).
>>>>>>
>>>>>> I suggest (strongly) taking a single borough or even zipcode with some
>>>>>> hindreds of properties, and working from there. Do not include the
>>>>>> observation as its own neighbour, perhaps identify repeats and handle them
>>>>>> specially (create or use a property ID). Unbalanced panels may also create
>>>>>> a selection bias issue (why are some properties only listed sometimes?).
>>>>>>
>>>>>> So this although promising isn't simple, and getting to a hedonic model
>>>>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>>>>> necessarily trust OLS output either, partly because of the repeat property
>>>>>> issue.
>>>>>>
>>>>>> Roger
>>>>>>
>>>>>>>
>>>>>>> I created 54 timedummy variables for each time period available.
>>>>>>>
>>>>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>>>>> a variety of characteristics which potentially determine the daily rate
>>>>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>>>>> of the listing as number of bedrooms, if the host if professional,
>>>>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>>>>> from the listing, income per capita, etc.).
>>>>>>>
>>>>>>> My dependent variable is price (log price, common in the related
>>>>>>> literature for hedonic prices).
>>>>>>>
>>>>>>> The OLS model is done.
>>>>>>>
>>>>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>>>>> pricing of their listings, take not only into account its structural and
>>>>>>> location characteristics, but also the prices charged by near listings
>>>>>>> with similar characteristics - spatial autocorrelation is then present,
>>>>>>> at least spatial dependence is present in the dependent variable.
>>>>>>>
>>>>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>>>>> itself as a neighbor.
>>>>>>>
>>>>>>> Parts of my code can be found below:
>>>>>>>
>>>>>>> ########
>>>>>>>
>>>>>>> ## packages
>>>>>>>
>>>>>>> packages_install <- function(packages){
>>>>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>>>>> if (length(new.packages))
>>>>>>> install.packages(new.packages, dependencies = TRUE)
>>>>>>> sapply(packages, require, character.only = TRUE)
>>>>>>> }
>>>>>>>
>>>>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>>>>> packages_install(packages_required)
>>>>>>>
>>>>>>> # Working directory
>>>>>>> setwd("C:/Users/User/R")
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## shapefile_us
>>>>>>>
>>>>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>>>>
>>>>>>> # Columns removal
>>>>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>>>>
>>>>>>> # Column rename: ZCTA5CE10
>>>>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>>>>
>>>>>>> # Column class change: zipcode
>>>>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## polygon_nyc
>>>>>>>
>>>>>>> # Zip code not available in shapefile: 11695
>>>>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## weight_matrix
>>>>>>>
>>>>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>>>>
>>>>>>> # Include neighbour itself as a neighbour
>>>>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>>>>
>>>>>>> # Weights to each neighboring polygon
>>>>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## listings
>>>>>>>
>>>>>>> # Data import
>>>>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>>>>> listings <- listings %>% bind_rows
>>>>>>>
>>>>>>> # Characters removal
>>>>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>>>>> listings$price <- gsub("\\$", "", listings$price)
>>>>>>> listings$price <- gsub(",", "", listings$price)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## timedummy
>>>>>>>
>>>>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>>>>> timedummy <- gsub("-", "_", timedummy)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## OLS regression
>>>>>>>
>>>>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>>>>
>>>>>>> ########
>>>>>>>
>>>>>>> Some of my id's repeat in multiple time periods.
>>>>>>>
>>>>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>>>>
>>>>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>>>>
>>>>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>>>>
>>>>>>> Again, thank you very much for the help provided until now.
>>>>>>>
>>>>>>> Best regards,
>>>>>>> Robert
>>>>>>>
>>>>>>> ________________________________________
>>>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>>>> Sent: Tuesday, November 5, 2019 15:30
>>>>>>> To: Robert R
>>>>>>> Cc: r-sig-geo at r-project.org
>>>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>>>
>>>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>>>
>>>>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>>>>> for now that spatial dependence is present in both the dependent
>>>>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>>>>> think it would be possible to calculate this.? What I would like to know
>>>>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>>>>
>>>>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>>>>> of the covariates time-varying? Please provide motivation and use area
>>>>>>> (preferably with affiliation (your email and user name are not
>>>>>>> informative) - this feels like a real estate problem, probably wrongly
>>>>>>> specified. You should use splm if time make sense in your case, but if it
>>>>>>> really doesn't, simplify your approach, as much of the data will be
>>>>>>> subject to very large temporal autocorrelation.
>>>>>>>
>>>>>>> If this is a continuation of your previous question about using
>>>>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>>>>
>>>>>>> Roger
>>>>>>>
>>>>>>>>
>>>>>>>>       [[alternative HTML version deleted]]
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-sig-Geo mailing list
>>>>>>>> R-sig-Geo at r-project.org
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>>>
>>>>>>> --
>>>>>>> Roger Bivand
>>>>>>> Department of Economics, Norwegian School of Economics,
>>>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>>>> https://orcid.org/0000-0003-2392-6140
>>>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>>>
>>>>>>
>>>>>> --
>>>>>> Roger Bivand
>>>>>> Department of Economics, Norwegian School of Economics,
>>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>>> https://orcid.org/0000-0003-2392-6140
>>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>>
>>>>>
>>>>> --
>>>>> Roger Bivand
>>>>> Department of Economics, Norwegian School of Economics,
>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>> https://orcid.org/0000-0003-2392-6140
>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From edou@rd@|egoup|| @end|ng |rom gm@||@com  Thu Dec 12 05:58:01 2019
From: edou@rd@|egoup|| @end|ng |rom gm@||@com (Edouard LEGOUPIL)
Date: Thu, 12 Dec 2019 06:58:01 +0200
Subject: [R-sig-Geo] PPA ubuntugis-unstable update to Proj6/GDAL3 breaks SF
 / rgdal
Message-ID: <CAN8Cwn=64SbLg4bPDc4pHfGOk4hTnug0n4PsmSCJ_nHCpqEWLA@mail.gmail.com>

Hi,

Has anyone found how to get sf/rgdal to work with the recent push on
ubuntugis-unstable PPA of the last version of Proj6 & Gdal3 ?

It seems that sf is looking for pcs.csv that is not here anymore in gdal3 -
configure: error: pcs.csv not found in GDAL data directory.
and rgdal looks for proj_def.dat wich does not exist anymore in proj6

Thanks,
Edouard

	[[alternative HTML version deleted]]


From edzer@pebe@m@ @end|ng |rom un|-muen@ter@de  Thu Dec 12 09:07:54 2019
From: edzer@pebe@m@ @end|ng |rom un|-muen@ter@de (Edzer Pebesma)
Date: Thu, 12 Dec 2019 09:07:54 +0100
Subject: [R-sig-Geo] 
 PPA ubuntugis-unstable update to Proj6/GDAL3 breaks SF / rgdal
In-Reply-To: <CAN8Cwn=64SbLg4bPDc4pHfGOk4hTnug0n4PsmSCJ_nHCpqEWLA@mail.gmail.com>
References: <CAN8Cwn=64SbLg4bPDc4pHfGOk4hTnug0n4PsmSCJ_nHCpqEWLA@mail.gmail.com>
Message-ID: <bcaa52a3-bf1b-c0f7-7bc0-77a14187b5f4@uni-muenster.de>

Yes, I had that too. It turned out that my distribution wouldn't update
gdal all the way due to all kind of dependencies, and kept it back at
some version, resulting in this error. After installing it manually with

sudo apt-get install gdal-dev

gdal3 got installed. gdal3 does not use pcs.csv.

On 12/12/19 5:58 AM, Edouard LEGOUPIL wrote:
> Hi,
> 
> Has anyone found how to get sf/rgdal to work with the recent push on
> ubuntugis-unstable PPA of the last version of Proj6 & Gdal3 ?
> 
> It seems that sf is looking for pcs.csv that is not here anymore in gdal3 -
> configure: error: pcs.csv not found in GDAL data directory.
> and rgdal looks for proj_def.dat wich does not exist anymore in proj6
> 
> Thanks,
> Edouard
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics
Heisenbergstrasse 2, 48151 Muenster, Germany
Phone: +49 251 8333081

-------------- next part --------------
A non-text attachment was scrubbed...
Name: pEpkey.asc
Type: application/pgp-keys
Size: 3110 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20191212/cbdfd01e/attachment.bin>

From |rz@mbr@ @end|ng |rom gm@||@com  Fri Dec 13 13:39:37 2019
From: |rz@mbr@ @end|ng |rom gm@||@com (Francisco Zambrano)
Date: Fri, 13 Dec 2019 09:39:37 -0300
Subject: [R-sig-Geo] Package 'gdalUtils' when using 'gdal_translate' on HDF
 files it gives error "no installation match"
Message-ID: <CANE=RDNNDW-1dkhGiSEz5rUjyB7OMA54Z2wOX2D4KxTdO1xCgQ@mail.gmail.com>

Hi all,

I'm using the package gdalUtils to work with HDF (MODIS data), I've used
for a while without problems, but now (I believe after the update of gdal)
is giving me the following error when I use the function 'gdal_translate'
over HDF files. A time before this used to work fine.

Error in gdal_chooseInstallation(hasDrivers = of) :
  No installations match.


also,

gdal_chooseInstallation(hasDrivers=c("HDF4","HDF5"))Error in
gdal_chooseInstallation(hasDrivers = c("HDF4", "HDF5")) :
  No installations match.

The version and date for gdal are:

> getOption("gdalUtils_gdalPath")[[1]]$versionversion
"3.0.2" > getOption("gdalUtils_gdalPath")[[1]]$date        date
"2019-10-28"


Searching the HDF drivers I get:

> getOption("gdalUtils_gdalPath")[[1]]$drivers[grep('HDF',getOption("gdalUtils_gdalPath")[[1]]$drivers$format_name),]          format_code read write update virtualIO subdatasets  format_name
43  HDF4Image-raster- TRUE  TRUE   TRUE     FALSE       FALSE HDF4 Dataset
114 HDF5Image-raster- TRUE FALSE  FALSE      TRUE       FALSE HDF5 Dataset


My sessionInfo():

R version 3.6.1 (2019-07-05)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 18.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

locale:
 [1] LC_CTYPE=es_CL.UTF-8       LC_NUMERIC=C
LC_TIME=es_CL.UTF-8
 [4] LC_COLLATE=es_CL.UTF-8     LC_MONETARY=es_CL.UTF-8
LC_MESSAGES=es_CL.UTF-8
 [7] LC_PAPER=es_CL.UTF-8       LC_NAME=C
LC_ADDRESS=C
[10] LC_TELEPHONE=C             LC_MEASUREMENT=es_CL.UTF-8
LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] stringr_1.4.0      gdalUtils_2.0.1.14 purrr_0.3.3
raster_3.0-7       sp_1.3-1

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.2        lattice_0.20-38   codetools_0.2-16
foreach_1.4.7     R.methodsS3_1.7.1
 [6] grid_3.6.1        magrittr_1.5      stringi_1.4.3     rlang_0.4.1
      R.oo_1.23.0
[11] R.utils_2.9.0     rgdal_1.4-8       iterators_1.0.12  tools_3.6.1
      compiler_3.6.1


Regards,

Francisco

	[[alternative HTML version deleted]]


From u@erc@tch @end|ng |rom out|ook@com  Sat Dec 14 20:57:06 2019
From: u@erc@tch @end|ng |rom out|ook@com (Robert R)
Date: Sat, 14 Dec 2019 19:57:06 +0000
Subject: [R-sig-Geo] Spatial Autocorrelation Estimation Method
In-Reply-To: <alpine.LFD.2.21.1912091210130.60452@reclus.nhh.no>
References: <VI1P190MB07688185ADC27FB3BEC32E2CB07F0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911051521080.22435@reclus.nhh.no>
 <VI1P190MB07681CC6153D9179C1BAE8BEB07B0@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911081321040.53982@reclus.nhh.no>
 <VI1P190MB0768A15BBFF80C0B7A89B3B8B0750@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911111122260.2810@reclus.nhh.no>
 <VI1P190MB0768BA4B37089E19C9FF2576B0730@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1911262040120.583221@reclus.nhh.no>,
 <VI1P190MB07680F075EA1B55A9AE9D56AB0450@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <3cd7666ef730416c8e7f6f4110315f0a@nhh.no>
 <VI1P190MB076828E385B2E65EB3CD4093B0420@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1912040901070.29869@reclus.nhh.no>
 <VI1P190MB07684BF8B370053F54EB1DD6B0590@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>,
 <alpine.LFD.2.21.1912091210130.60452@reclus.nhh.no>
Message-ID: <VI1P190MB0768F5DB37E88568EC0BE32CB0570@VI1P190MB0768.EURP190.PROD.OUTLOOK.COM>

Dear Roger,

Thank you again for your answer. You're really helping me a lot.

I was wondering how to deal with the scales for lme4::lmer, e.g. I have the variable income_per_capita which has the income_per_capita for the zipcode in which the listing/ad is located. Is it possible to add a income_per_capita_zipcode and income_per_capita_borough for the different scales/level for the multilevel model?

lme4::lmer(log_price ~ factor(room_type) + bedrooms + bathrooms + guests_included + minimum_nights + distance_downtown + distance_subway + number_of_reviews + review_scores_cleanliness + professional_host + host_is_superhost + is_business_travel_ready + offense_misdemeanor + offense_felony + income_per_capita + factor(date_compiled) + (1 | borough / zipcode / id), data = listings)

Thank you and best regards

________________________________________
From: Roger Bivand <Roger.Bivand at nhh.no>
Sent: Monday, December 9, 2019 12:14
To: Robert R
Cc: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method

On Sun, 8 Dec 2019, Robert R wrote:

> Dear Roger,
>
> Thank you for your answer. Regarding the sparse matrix, you're right - I
> tested creating one, as follows:
>
> #####
>
> listings_nb <- listings %>% spdep::poly2nb(queen = TRUE) %>%
> spdep::include.self() # is.symmetric.nb(listings_nb)

Are listings point or polygon support (must be polygon for this function)?

>
> nb_B <- spdep::nb2listw(neighbours = listings_nb, style="B", zero.policy
> = FALSE)
>
> B <- as(nb_B , "CsparseMatrix")
> all(B == t(B))
>
> nb_B1 <- mat2listw(as(B, "dgTMatrix"))
>
> format(object.size(nb_B), units = "Mb")
> # 85.6 Mb
>
> format(object.size(nb_B1), units = "Mb")
> # 85.6 Mb
>
> #####
>
> The size for both objects is the same - different from this example:
> https://cran.r-project.org/web/packages/spdep/vignettes/nb_igraph.html
>
>
> Summing up the data that I have: a "picture" for Airbnb's listings/ads
> once a month for NYC, incl. lat, lon, price (per night), id, and some
> characteristics from the listing/ad as number of rooms, bedrooms, guests
> included, etc. for a period of 54 months. The data was taken here:
> http://insideairbnb.com/get-the-data.html (listings.csv.gz)
>
> For the OLS, I used a pooled OLS with time dummy fixed effects
> (date_compiled, when the "picture" was compiled - how Airbnb listings
> for NYC were shown) because many of my observations (listings id) do not
> repeat for many periods. Also, many listings changed the room_type at
> least once during the whole time period analyzed (3 types of room_type:
> entire home/apt, private room, shared room).
>

OK, I suppose.

> I am now trying a random intercepts three-level hierarchy multilevel
> model, where _id_ (level 1) are nested within _zipcode_ (level 2), and
> the last is nested within _borough_ (level 3). So groups:
> id:(zipcode:borough).
>

I'd try and see what happens, and watch machine resources (memory anyway).

> lme4::lmer(log_price ~ factor(room_type) + bedrooms + bathrooms +
> guests_included + minimum_nights + distance_downtown + distance_subway +
> number_of_reviews + review_scores_cleanliness + professional_host +
> host_is_superhost + is_business_travel_ready + offense_misdemeanor +
> offense_felony + income_per_capita + factor(date_compiled) + (1 |
> borough / zipcode / id), data = listings)
>
> Roger, do you think it is okay to factor(room_type) (for the 3 types of
> room) and factor(date_compiled) for the dates when the NYC Airbnb's
> listings/ads were extracted?

Do you see structured variability coming from these - if so, inclusion
makes sense.

Roger

>
> Thank you and best regards,
> Robert
>
> ________________________________________
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Sent: Wednesday, December 4, 2019 09:07
> To: Robert R
> Cc: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>
> On Wed, 4 Dec 2019, Robert R wrote:
>
>> Dear Roger,
>>
>> Again, thank you for your answer. What do you mean by "zip code random
>> effect"? You mean I should use in plm the model "random"?
>>
>> regression_re <- plm(formula = model, data = listings, model = "random",
>> index = c("id", "date_compiled"))
>
> No, obviously not, your data are not a balanced panel. I mean a multilevel
> model, where the <200 zip codes cluster the data, and where a zip code
> level IID RE will almost certainly do a better job than dummies. An
> MRF/ICAR RE might be an extension.
>
>>
>> And any other methodology in dealing with large weight matrices in
>> spatialreg::lagsarlm?
>
> Please refer to Bivand et al. (2013) refered to in the package. Probably
> the weights would need to be symmetric and very sparse.
>
> I still think that you should focus on a small subset of the data and to
> improving the signal-noise ratio before trying to scale up.
>
> Roger
>
>>
>> Thank you and best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Wednesday, November 27, 2019 13:53
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: SV: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> Yes this is expected, since the # neighbours in a single zip code block is a dense matrix, and there will be multiple such matrices. (15000^2)*8 is 1.8e+09 so such a dense matrix will max out your RAM. There is no way to look at block neighbours in that format without subsetting your data (think train/test), use a zip code random effect. I would certainly drop all attempts to examine spatial dependency until you get an aspatial multilevel hedonic model working.
>>
>> Roger
>>
>> --
>> Roger Bivand
>> Norwegian School of Economics
>> Helleveien 30, 5045 Bergen, Norway
>> Roger.Bivand at nhh.no
>>
>>
>> ________________________________________
>> Fra: Robert R <usercatch at outlook.com>
>> Sendt: tirsdag 26. november 2019 21.04
>> Til: Roger Bivand
>> Kopi: r-sig-geo at r-project.org
>> Emne: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> Dear Roger,
>>
>> Thank you for your e-mail. Actually there is less noise that it seems. Rental prices are daily rental prices and I have an extract of all Airbnb listings daily prices once a month for a period of 4 years. Each listings information contains the lat, lon, number of bedrooms, category (entire home/apt, shared room or private room), etc.
>>
>> One question regarding the spdep::nb2blocknb function: it runs super fast with up to n = 1000, and always crashes my R session with n = 15000 or so. Is there an alternative to solve this problem?
>>
>> Thank you and best regards,
>> Robert
>>
>> ________________________________________
>> From: Roger Bivand <Roger.Bivand at nhh.no>
>> Sent: Tuesday, November 26, 2019 20:48
>> To: Robert R
>> Cc: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>
>> Sorry for late reply, am indisposed and unable to help further. I feel
>> that there is so much noise in your data (differences in offers, rental
>> lengths, repeats or not, etc.), that you will certainly have to subset
>> vigorously first to isolate response cases that are comparable. What you
>> are trying to disentangle are the hedonic components in the bundle where
>> you just have price as response, but lots of other bundle characteristics
>> on the right hand side (days, etc.). I feel you'd need to try to get to a
>> response index of price per day per rental area or some such. I'd
>> certainly advise examining responses to a specific driver (major concert
>> or sports event) to get a feel for how the market responds, and return to
>> spatial hedonic after finding an approach that gives reasonable aspatial
>> outcomes.
>>
>> Roger
>>
>> On Sun, 17 Nov 2019, Robert R wrote:
>>
>>> Dear Roger,
>>>
>>> Thank you for your message and sorry for my late answer.
>>>
>>> Regarding the number of listings (lettings) for my data set (2.216.642 observations), each listing contains an individual id:
>>>
>>> unique ids: 180.004
>>> time periods: 54 (2015-01 to 2019-09)
>>> number of ids that appear only once: 28.486 (of 180.004 ids) (15,8%)
>>> number of ids that appear/repeat 2-10 times: 82.641 (of 180.004 ids) (45,9%)
>>> number of ids that appear/repeat 11-30 times: 46.465 (of 180.004 ids) (25,8%)
>>> number of ids that appear/repeat 31-54 times: 22.412 (of 180.004 ids) (12,5%)
>>>
>>> Important to notice is that hosts can change the room_category (between entire/home apt, private room and shared room) keeping the same listing id number. In my data, the number of unique ids that in some point changed the room_type is of 7.204 ids.
>>>
>>> --
>>>
>>> For the OLS model, I was using only a fixed effect model, where each time period (date_compiled) (54 in total) is a time dummy.
>>>
>>> plm::plm(formula = model, data = listings, model = "pooling", index = c("id", "date_compiled"))
>>>
>>>
>>> --
>>> Osland et al. (2016) (https://doi.org/10.1111/jors.12281) use a spatial fixed effects (SFE) hedonic model, where each defined neighborhood zone in the study area is represented by dummy variables.
>>>
>>> Dong et al. (2015) (https://doi.org/10.1111/gean.12049) outline four model specifications to accommodate geographically hierarchical data structures: (1) groupwise W and fixed regional effects; (2) groupwise W and random regional effects; (3) proximity-based W and fixed regional effects; and (4) proximity-based W and random regional effects.
>>> --
>>>
>>> I created a new column/variable containing the borough where the zipcode is found (Manhattan, Brooklyn, Queens, Bronx, Staten Island).
>>>
>>> If I understood it right, the (two-level) Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) considers the occurrence of spatial relations at the (lower) individual (geographical coordinates - in my case, the listing location) and (higher) group level (territorial units - in my case, zipcodes).
>>>
>>> According to Bivand et al. (2017): "(...) W is a spatial weights matrix. The HSAR model may also be estimated without this component.". So, in this case I only estimate the Hierarchical Spatial Simultaneous Autoregressive Model (HSAR) in a "one-level" basis, i.e., at the higher-level.
>>>
>>> HSAR::hsar(model, data = listings, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>>
>>> (Where the "model" formula contains the 54 time dummy variables)
>>>
>>> Do you think I can proceed with this model? I was able to calculate it.
>>>
>>> If I remove all observations/rows with NAs in one of the chosen variables/observations, 884.183 observations remain. If I would create a W matrix for HSAR::hsar, I would have a gigantic 884.183 by 884.183 matrix. This is the reason why I put W = NULL.
>>>
>>>
>>> Thank you and best regards
>>>
>>> ________________________________________
>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>> Sent: Monday, November 11, 2019 11:31
>>> To: Robert R
>>> Cc: r-sig-geo at r-project.org
>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>
>>> On Sun, 10 Nov 2019, Robert R wrote:
>>>
>>>> Dear Roger,
>>>>
>>>> Again, thank you for your answer. I read the material provided and
>>>> decided that Hierarchical Spatial Autoregressive (HSAR) could be the
>>>> right model for me.
>>>>
>>>> I indeed have the precise latitude and longitude information for all my
>>>> listings for NYC.
>>>>
>>>> I created a stratified sample (group = zipcode) with 22172 (1%) of my
>>>> observations called listings_sample and tried to replicate the hsar
>>>> model, please see below.
>>>>
>>>> For now W = NULL, because otherwise I would have a 22172 x 22172 matrix.
>>>
>>> Unless you know definitely that you want to relate the response to its
>>> lagged value, you do not need this. Do note that the matrix is very
>>> sparse, so could be fitted without difficulty with ML in a cross-sectional
>>> model.
>>>
>>>>
>>>> You recommended then to introduce a Markov random field (MRF) random
>>>> effect (RE) at the zipcode level, but I did not understand it so well.
>>>> Could you develop a litte more?
>>>>
>>>
>>> Did you read the development in
>>> https://doi.org/10.1016/j.spasta.2017.01.002? It is explained there, and
>>> includes code for fitting the Beijing housing parcels data se from HSAR
>>> with many other packages (MCMC, INLA, hglm, etc.). I guess that you should
>>> try to create a model that works on a single borough, sing the zipcodes
>>> in that borough as a proxy for unobserved neighbourhood effects. Try for
>>> example using lme4::lmer() with only a zipcode IID random effect, see if
>>> the hedonic estimates are similar to lm(), and leave adding an MRF RE
>>> (with for example mgcv::gam() or hglm::hglm()) until you have a working
>>> testbed. Then advance step-by-step from there.
>>>
>>> You still have not said how many repeat lettings you see - it will affect
>>> the way you specify your model.
>>>
>>> Roger
>>>
>>>> ##############
>>>> library(spdep)
>>>> library(HSAR)
>>>> library(dplyr)
>>>> library(splitstackshape)
>>>>
>>>>
>>>> # Stratified sample per zipcode (size = 1%) listings_sample <-
>>>> splitstackshape::stratified(indt = listings, group = "zipcode", size =
>>>> 0.01)
>>>>
>>>> # Removing zipcodes from polygon_nyc which are not observable in
>>>> listings_sample polygon_nyc_listings <- polygon_nyc %>% filter(zipcode
>>>> %in% c(unique(as.character(listings_sample$zipcode))))
>>>>
>>>>
>>>> ## Random effect matrix (N by J)
>>>>
>>>> # N: 22172
>>>> # J: 154
>>>>
>>>> # Arrange listings_sample by zipcode (ascending)
>>>> listings_sample <- listings_sample %>% arrange(zipcode)
>>>>
>>>> # Count number of listings per zipcode
>>>> MM <- listings_sample %>% st_drop_geometry() %>% group_by(zipcode) %>% summarise(count = n()) %>% as.data.frame()
>>>> # sum(MM$count)
>>>>
>>>> # N by J nulled matrix creation
>>>> Delta <- matrix(data = 0, nrow = nrow(listings_sample), ncol = dim(MM)[1])
>>>>
>>>> # The total number of neighbourhood
>>>> Uid <- rep(c(1:dim(MM)[1]), MM[,2])
>>>>
>>>> for(i in 1:dim(MM)[1]) {
>>>>  Delta[Uid==i,i] <- 1
>>>> }
>>>> rm(i)
>>>>
>>>> Delta <- as(Delta,"dgCMatrix")
>>>>
>>>>
>>>> ## Higher-level spatial weights matrix or neighbourhood matrix (J by J)
>>>>
>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>> polygon_nyc_nb <- poly2nb(polygon_nyc_listings, row.names = polygon_nyc$zipcode, queen = TRUE)
>>>>
>>>> # Include neighbour itself as a neighbour
>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>
>>>> # Spatial weights matrix for nb
>>>> polygon_nyc_nb_matrix <- nb2mat(neighbours = polygon_nyc_nb, style = "W", zero.policy = NULL)
>>>> M <- as(polygon_nyc_nb_matrix,"dgCMatrix")
>>>>
>>>>
>>>> ## Fit HSAR SAR upper level random effect
>>>> model <- as.formula(log_price ~ guests_included + minimum_nights)
>>>>
>>>> betas = coef(lm(formula = model, data = listings_sample))
>>>> pars = list(rho = 0.5, lambda = 0.5, sigma2e = 2.0, sigma2u = 2.0, betas = betas)
>>>>
>>>> m_hsar <- hsar(model, data = listings_sample, W = NULL, M = M, Delta = Delta, burnin = 5000, Nsim = 10000, thinning = 1, parameters.start = pars)
>>>>
>>>> ##############
>>>>
>>>> Thank you and best regards
>>>> Robert
>>>>
>>>> ________________________________________
>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>> Sent: Friday, November 8, 2019 13:29
>>>> To: Robert R
>>>> Cc: r-sig-geo at r-project.org
>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>
>>>> On Fri, 8 Nov 2019, Robert R wrote:
>>>>
>>>>> Dear Roger,
>>>>>
>>>>> Thank you for your answer.
>>>>>
>>>>> I successfully used the function nb2blocknb() for a smaller dataset.
>>>>>
>>>>> But for a dataset of over 2 million observations, I get the following
>>>>> error: "Error: cannot allocate vector of size 840 Kb".
>>>>
>>>> I don't think the observations are helpful. If you have repeat lets in the
>>>> same property in a given month, you need to handle that anyway. I'd go for
>>>> making the modelling exercise work (we agree that this is not panel data,
>>>> right?) on a small subset first. I would further argue that you need a
>>>> multi-level approach rather than spdep::nb2blocknb(), with a zipcode IID
>>>> RE. You could very well take (stratified) samples per zipcode to represent
>>>> your data. Once that works, introduce an MRF RE at the zipcode level,
>>>> where you do know relative position. Using SARAR is going to be a waste of
>>>> time unless you can geocode the letting addresses. A multi-level approach
>>>> will work. Having big data in your case with no useful location
>>>> information per observation is just adding noise and over-smoothing, I'm
>>>> afraid. The approach used in https://doi.org/10.1016/j.spasta.2017.01.002
>>>> will work, also when you sample the within zipcode lets, given a split
>>>> into training and test sets, and making CV possible.
>>>>
>>>> Roger
>>>>
>>>>>
>>>>> I am expecting that at least 500.000 observations will be dropped due
>>>>> the lack of values for the chosen variables for the regression model, so
>>>>> probably I will filter and remove the observations/rows that will not be
>>>>> used anyway - do you know if there is any package that does this
>>>>> automatically, given the variables/columns chosed by me?
>>>>>
>>>>> Or would you recommend me another approach to avoid the above mentioned
>>>>> error?
>>>>>
>>>>> Thank you and best regards,
>>>>> Robert
>>>>>
>>>>> ________________________________________
>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>> Sent: Thursday, November 7, 2019 10:13
>>>>> To: Robert R
>>>>> Cc: r-sig-geo at r-project.org
>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>
>>>>> On Thu, 7 Nov 2019, Robert R wrote:
>>>>>
>>>>>> Dear Roger,
>>>>>>
>>>>>> Many thanks for your help.
>>>>>>
>>>>>> I have an additional question:
>>>>>>
>>>>>> Is it possible to create a "separate" lw (nb2listw) (with different
>>>>>> rownumbers) from my data set? For now, I am taking my data set and
>>>>>> merging with the sf object polygon_nyc with the function
>>>>>> "merge(polygon_nyc, listings, by=c("zipcode" = "zipcode"))", so I create
>>>>>> a huge n x n matrix (depending of the size of my data set).
>>>>>>
>>>>>> Taking the polygon_nyc alone and turning it to a lw (weights list)
>>>>>> object has only n = 177.
>>>>>>
>>>>>> Of course running
>>>>>>
>>>>>> spatialreg::lagsarlm(formula=model, data = listings_sample,
>>>>>> spatialreg::polygon_nyc_lw, tol.solve=1.0e-10)
>>>>>>
>>>>>> does not work ("Input data and weights have different dimensions").
>>>>>>
>>>>>> The only option is to take my data set, merge it to my polygon_nyc (by
>>>>>> zipcode) and then create the weights list lw? Or there another option?
>>>>>
>>>>> I think we are getting more clarity. You do not know the location of the
>>>>> lettings beyond their zipcode. You do know the boundaries of the zipcode
>>>>> areas, and can create a neighbour object from these boundaries. You then
>>>>> want to treat all the lettings in a zipcode area i as neighbours, and
>>>>> additionally lettings in zipcode areas neighbouring i as neighbours of
>>>>> lettings in i. This is the data structure that motivated the
>>>>> spdep::nb2blocknb() function:
>>>>>
>>>>> https://r-spatial.github.io/spdep/reference/nb2blocknb.html
>>>>>
>>>>> Try running the examples to get a feel for what is going on.
>>>>>
>>>>> I feel that most of the variability will vanish in the very large numbers
>>>>> of neighbours, over-smoothing the outcomes. If you do not have locations
>>>>> for the lettings themselves, I don't think you can make much progress.
>>>>>
>>>>> You could try a linear mixed model (or gam with a spatially structured
>>>>> random effect) with a temporal and a spatial random effect. See the HSAR
>>>>> package, articles by Dong et al., and maybe
>>>>> https://doi.org/10.1016/j.spasta.2017.01.002 for another survey. Neither
>>>>> this nor Dong et al. handle spatio-temporal settings. MRF spatial random
>>>>> effects at the zipcode level might be a way forward, together with an IID
>>>>> random effect at the same level (equivalent to sef-neighbours).
>>>>>
>>>>> Hope this helps,
>>>>>
>>>>> Roger
>>>>>
>>>>>>
>>>>>> Best regards,
>>>>>> Robert
>>>>>>
>>>>>> ________________________________________
>>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>>> Sent: Wednesday, November 6, 2019 15:07
>>>>>> To: Robert R
>>>>>> Cc: r-sig-geo at r-project.org
>>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>>
>>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>>
>>>>>>> Dear Roger,
>>>>>>>
>>>>>>> Thank you for your reply. I disabled HTML; my e-mails should be now in
>>>>>>> plain text.
>>>>>>>
>>>>>>> I will give a better context for my desired outcome.
>>>>>>>
>>>>>>> I am taking Airbnb's listings information for New York City available
>>>>>>> on: http://insideairbnb.com/get-the-data.html
>>>>>>>
>>>>>>> I save every listings.csv.gz file available for NYC (2015-01 to 2019-09)
>>>>>>> - in total, 54 files/time periods - as a YYYY-MM-DD.csv file into a
>>>>>>> Listings/ folder. When importing all these 54 files into one single data
>>>>>>> set, I create a new "date_compiled" variable/column.
>>>>>>>
>>>>>>> In total, after the data cleansing process, I have a little more 2
>>>>>>> million observations.
>>>>>>
>>>>>> You have repeat lettings for some, but not all properties. So this is at
>>>>>> best a very unbalanced panel. For those properties with repeats, you may
>>>>>> see temporal movement (trend/seasonal).
>>>>>>
>>>>>> I suggest (strongly) taking a single borough or even zipcode with some
>>>>>> hindreds of properties, and working from there. Do not include the
>>>>>> observation as its own neighbour, perhaps identify repeats and handle them
>>>>>> specially (create or use a property ID). Unbalanced panels may also create
>>>>>> a selection bias issue (why are some properties only listed sometimes?).
>>>>>>
>>>>>> So this although promising isn't simple, and getting to a hedonic model
>>>>>> may be hard, but not (just) because of spatial autocorrelation. I wouldn't
>>>>>> necessarily trust OLS output either, partly because of the repeat property
>>>>>> issue.
>>>>>>
>>>>>> Roger
>>>>>>
>>>>>>>
>>>>>>> I created 54 timedummy variables for each time period available.
>>>>>>>
>>>>>>> I want to estimate using a hedonic spatial timedummy model the impact of
>>>>>>> a variety of characteristics which potentially determine the daily rate
>>>>>>> on Airbnb listings through time in New York City (e.g. characteristics
>>>>>>> of the listing as number of bedrooms, if the host if professional,
>>>>>>> proximity to downtown (New York City Hall) and nearest subway station
>>>>>>> from the listing, income per capita, etc.).
>>>>>>>
>>>>>>> My dependent variable is price (log price, common in the related
>>>>>>> literature for hedonic prices).
>>>>>>>
>>>>>>> The OLS model is done.
>>>>>>>
>>>>>>> For the spatial model, I am assuming that hosts, when deciding the
>>>>>>> pricing of their listings, take not only into account its structural and
>>>>>>> location characteristics, but also the prices charged by near listings
>>>>>>> with similar characteristics - spatial autocorrelation is then present,
>>>>>>> at least spatial dependence is present in the dependent variable.
>>>>>>>
>>>>>>> As I wrote in my previous post, I was willing to consider the neighbor
>>>>>>> itself as a neighbor.
>>>>>>>
>>>>>>> Parts of my code can be found below:
>>>>>>>
>>>>>>> ########
>>>>>>>
>>>>>>> ## packages
>>>>>>>
>>>>>>> packages_install <- function(packages){
>>>>>>> new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
>>>>>>> if (length(new.packages))
>>>>>>> install.packages(new.packages, dependencies = TRUE)
>>>>>>> sapply(packages, require, character.only = TRUE)
>>>>>>> }
>>>>>>>
>>>>>>> packages_required <- c("bookdown", "cowplot", "data.table", "dplyr", "e1071", "fastDummies", "ggplot2", "ggrepel", "janitor", "kableExtra", "knitr", "lubridate", "nngeo", "plm", "RColorBrewer", "readxl", "scales", "sf", "spdep", "stargazer", "tidyverse")
>>>>>>> packages_install(packages_required)
>>>>>>>
>>>>>>> # Working directory
>>>>>>> setwd("C:/Users/User/R")
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## shapefile_us
>>>>>>>
>>>>>>> # Shapefile zips import and Coordinate Reference System (CRS) transformation
>>>>>>> # Shapefile download: https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip
>>>>>>> shapefile_us <- sf::st_read(dsn = "Shapefile", layer = "cb_2018_us_zcta510_500k")
>>>>>>>
>>>>>>> # Columns removal
>>>>>>> shapefile_us <- shapefile_us %>% select(-c(AFFGEOID10, GEOID10, ALAND10, AWATER10))
>>>>>>>
>>>>>>> # Column rename: ZCTA5CE10
>>>>>>> setnames(shapefile_us, old=c("ZCTA5CE10"), new=c("zipcode"))
>>>>>>>
>>>>>>> # Column class change: zipcode
>>>>>>> shapefile_us$zipcode <- as.character(shapefile_us$zipcode)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## polygon_nyc
>>>>>>>
>>>>>>> # Zip code not available in shapefile: 11695
>>>>>>> polygon_nyc <- shapefile_us %>% filter(zipcode %in% zips_nyc)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## weight_matrix
>>>>>>>
>>>>>>> # Neighboring polygons: list of neighbors for each polygon (queen contiguity neighbors)
>>>>>>> polygon_nyc_nb <- poly2nb((polygon_nyc %>% select(-borough)), queen=TRUE)
>>>>>>>
>>>>>>> # Include neighbour itself as a neighbour
>>>>>>> # for(i in 1:length(polygon_nyc_nb)){polygon_nyc_nb[[i]]=as.integer(c(i,polygon_nyc_nb[[i]]))}
>>>>>>> polygon_nyc_nb <- include.self(polygon_nyc_nb)
>>>>>>>
>>>>>>> # Weights to each neighboring polygon
>>>>>>> lw <- nb2listw(neighbours = polygon_nyc_nb, style="W", zero.policy=TRUE)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## listings
>>>>>>>
>>>>>>> # Data import
>>>>>>> files <- list.files(path="Listings/", pattern=".csv", full.names=TRUE)
>>>>>>> listings <- setNames(lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE, encoding="UTF-8")), files)
>>>>>>> listings <- mapply(cbind, listings, date_compiled = names(listings))
>>>>>>> listings <- listings %>% bind_rows
>>>>>>>
>>>>>>> # Characters removal
>>>>>>> listings$date_compiled <- gsub("Listings/", "", listings$date_compiled)
>>>>>>> listings$date_compiled <- gsub(".csv", "", listings$date_compiled)
>>>>>>> listings$price <- gsub("\\$", "", listings$price)
>>>>>>> listings$price <- gsub(",", "", listings$price)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## timedummy
>>>>>>>
>>>>>>> timedummy <- sapply("date_compiled_", paste, unique(listings$date_compiled), sep="")
>>>>>>> timedummy <- paste(timedummy, sep = "", collapse = " + ")
>>>>>>> timedummy <- gsub("-", "_", timedummy)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ## OLS regression
>>>>>>>
>>>>>>> # Pooled cross-section data - Randomly sampled cross sections of Airbnb listings price at different points in time
>>>>>>> regression <- plm(formula=as.formula(paste("log_price ~ #some variables", timedummy, sep = "", collapse = " + ")), data=listings, model="pooling", index="id")
>>>>>>>
>>>>>>> ########
>>>>>>>
>>>>>>> Some of my id's repeat in multiple time periods.
>>>>>>>
>>>>>>> I use NYC's zip codes to left join my data with the neighborhood zip code specific characteristics, such as income per capita to that specific zip code, etc.
>>>>>>>
>>>>>>> Now I want to apply the hedonic model with the timedummy variables.
>>>>>>>
>>>>>>> Do you know how to proceed? 1) Which package to use (spdep/splm)?; 2) Do I have to join the polygon_nyc (by zip code) to my listings data set, and then calculate the weight matrix "lw"?
>>>>>>>
>>>>>>> Again, thank you very much for the help provided until now.
>>>>>>>
>>>>>>> Best regards,
>>>>>>> Robert
>>>>>>>
>>>>>>> ________________________________________
>>>>>>> From: Roger Bivand <Roger.Bivand at nhh.no>
>>>>>>> Sent: Tuesday, November 5, 2019 15:30
>>>>>>> To: Robert R
>>>>>>> Cc: r-sig-geo at r-project.org
>>>>>>> Subject: Re: [R-sig-Geo] Spatial Autocorrelation Estimation Method
>>>>>>>
>>>>>>> On Tue, 5 Nov 2019, Robert R wrote:
>>>>>>>
>>>>>>>> I have a large pooled cross-section data set. ?I would like to
>>>>>>>> estimate/regress using spatial autocorrelation methods. I am assuming
>>>>>>>> for now that spatial dependence is present in both the dependent
>>>>>>>> variable and the error term.? ?My data set is over a period of 4 years,
>>>>>>>> monthly data (54 periods). For this means, I've created a time dummy
>>>>>>>> variable for each time period.? ?I also created a weight matrix using the
>>>>>>>> functions "poly2nb" and "nb2listw".? ?Now I am trying to figure out a way
>>>>>>>> to estimate my model which contains a really big data set.? ?Basically, my
>>>>>>>> model is as follows: y = ?D + ?W1y + X? + ?W2u + ?? ?My questions are:? ?1)
>>>>>>>> My spatial weight matrix for the whole data set will be probably a
>>>>>>>> enormous matrix with submatrices for each time period itself. I don't
>>>>>>>> think it would be possible to calculate this.? What I would like to know
>>>>>>>> is a way to estimate each time dummy/period separately (to compare
>>>>>>>> different periods alone). How to do it?? ?2) Which package to use: spdep
>>>>>>>> or splm?? ?Thank you and best regards,? Robert?
>>>>>>>
>>>>>>> Please do not post HTML, only plain text. Almost certainly your model
>>>>>>> specification is wrong (SARAR/SAC is always a bad idea if alternatives are
>>>>>>> untried). What is your cross-sectional size? Using sparse kronecker
>>>>>>> products, the "enormous" matrix may not be very big. Does it make any
>>>>>>> sense using time dummies (54 x N x T will be mostly zero anyway)? Are most
>>>>>>> of the covariates time-varying? Please provide motivation and use area
>>>>>>> (preferably with affiliation (your email and user name are not
>>>>>>> informative) - this feels like a real estate problem, probably wrongly
>>>>>>> specified. You should use splm if time make sense in your case, but if it
>>>>>>> really doesn't, simplify your approach, as much of the data will be
>>>>>>> subject to very large temporal autocorrelation.
>>>>>>>
>>>>>>> If this is a continuation of your previous question about using
>>>>>>> self-neighbours, be aware that you should not use self-neighbours in
>>>>>>> modelling, they are only useful for the Getis-Ord local G_i^* measure.
>>>>>>>
>>>>>>> Roger
>>>>>>>
>>>>>>>>
>>>>>>>>       [[alternative HTML version deleted]]
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> R-sig-Geo mailing list
>>>>>>>> R-sig-Geo at r-project.org
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>>>
>>>>>>> --
>>>>>>> Roger Bivand
>>>>>>> Department of Economics, Norwegian School of Economics,
>>>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>>>> https://orcid.org/0000-0003-2392-6140
>>>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>>>
>>>>>>
>>>>>> --
>>>>>> Roger Bivand
>>>>>> Department of Economics, Norwegian School of Economics,
>>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>>> https://orcid.org/0000-0003-2392-6140
>>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>>
>>>>>
>>>>> --
>>>>> Roger Bivand
>>>>> Department of Economics, Norwegian School of Economics,
>>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>>> https://orcid.org/0000-0003-2392-6140
>>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>>
>>>>
>>>> --
>>>> Roger Bivand
>>>> Department of Economics, Norwegian School of Economics,
>>>> Helleveien 30, N-5045 Bergen, Norway.
>>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>>> https://orcid.org/0000-0003-2392-6140
>>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>>
>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> https://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> https://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> https://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

--
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From |bu@ett @end|ng |rom gm@||@com  Sun Dec 15 11:19:50 2019
From: |bu@ett @end|ng |rom gm@||@com (Lorenzo Busetto)
Date: Sun, 15 Dec 2019 11:19:50 +0100
Subject: [R-sig-Geo] 
 Package 'gdalUtils' when using 'gdal_translate' on HDF
 files it gives error "no installation match"
In-Reply-To: <CANE=RDNNDW-1dkhGiSEz5rUjyB7OMA54Z2wOX2D4KxTdO1xCgQ@mail.gmail.com>
References: <CANE=RDNNDW-1dkhGiSEz5rUjyB7OMA54Z2wOX2D4KxTdO1xCgQ@mail.gmail.com>
Message-ID: <CAM1G4SRLoLstoPe_mQHbSTaUGwA7dMNHLHu+0ZgQapPxJ=Q53g@mail.gmail.com>

The `gdal_chooseInstallation ` issue should be indeed related to switching
to GDAL3. The problem should have been already addressed in the github repo
here:

https://github.com/gearslaboratory/gdalUtils/commit/a94bbe422e7fe3bda1f571fc3629c3d052b4f195


Therefore, installing from github should solve the issue and allow you to
keep working with GDAL3 (at least, it did it for me).

Note that on Windows the gdalUtils package has some additionals problems
related with GDAL 3 migration due to issues reported here:

https://github.com/r-spatial/discuss/issues/31

HTH,

 Lorenzo

PS: For Francisco: sorry for the double replies, but I noticed that by
mistake I replied only to you and not to the list.

On Fri, 13 Dec 2019 at 13:40, Francisco Zambrano <frzambra at gmail.com> wrote:

> Hi all,
>
> I'm using the package gdalUtils to work with HDF (MODIS data), I've used
> for a while without problems, but now (I believe after the update of gdal)
> is giving me the following error when I use the function 'gdal_translate'
> over HDF files. A time before this used to work fine.
>
> Error in gdal_chooseInstallation(hasDrivers = of) :
>   No installations match.
>
>
> also,
>
> gdal_chooseInstallation(hasDrivers=c("HDF4","HDF5"))Error in
> gdal_chooseInstallation(hasDrivers = c("HDF4", "HDF5")) :
>   No installations match.
>
> The version and date for gdal are:
>
> > getOption("gdalUtils_gdalPath")[[1]]$versionversion
> "3.0.2" > getOption("gdalUtils_gdalPath")[[1]]$date        date
> "2019-10-28"
>
>
> Searching the HDF drivers I get:
>
> >
> getOption("gdalUtils_gdalPath")[[1]]$drivers[grep('HDF',getOption("gdalUtils_gdalPath")[[1]]$drivers$format_name),]
>         format_code read write update virtualIO subdatasets  format_name
> 43  HDF4Image-raster- TRUE  TRUE   TRUE     FALSE       FALSE HDF4 Dataset
> 114 HDF5Image-raster- TRUE FALSE  FALSE      TRUE       FALSE HDF5 Dataset
>
>
> My sessionInfo():
>
> R version 3.6.1 (2019-07-05)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 18.04.3 LTS
>
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1
>
> locale:
>  [1] LC_CTYPE=es_CL.UTF-8       LC_NUMERIC=C
> LC_TIME=es_CL.UTF-8
>  [4] LC_COLLATE=es_CL.UTF-8     LC_MONETARY=es_CL.UTF-8
> LC_MESSAGES=es_CL.UTF-8
>  [7] LC_PAPER=es_CL.UTF-8       LC_NAME=C
> LC_ADDRESS=C
> [10] LC_TELEPHONE=C             LC_MEASUREMENT=es_CL.UTF-8
> LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] stringr_1.4.0      gdalUtils_2.0.1.14 purrr_0.3.3
> raster_3.0-7       sp_1.3-1
>
> loaded via a namespace (and not attached):
>  [1] Rcpp_1.0.2        lattice_0.20-38   codetools_0.2-16
> foreach_1.4.7     R.methodsS3_1.7.1
>  [6] grid_3.6.1        magrittr_1.5      stringi_1.4.3     rlang_0.4.1
>       R.oo_1.23.0
> [11] R.utils_2.9.0     rgdal_1.4-8       iterators_1.0.12  tools_3.6.1
>       compiler_3.6.1
>
>
> Regards,
>
> Francisco
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr  Mon Dec 16 08:03:22 2019
From: |eon|d@@_||@ko@ @end|ng |rom y@hoo@gr (Leonidas Liakos)
Date: Mon, 16 Dec 2019 07:03:22 +0000 (UTC)
Subject: [R-sig-Geo] raster: stackApply problems..
In-Reply-To: <alpine.LFD.2.21.1911281220530.607873@reclus.nhh.no>
References: <a6e4f1a2-63af-0eaa-6b89-2135a6f72796.ref@yahoo.gr>
 <a6e4f1a2-63af-0eaa-6b89-2135a6f72796@yahoo.gr>
 <CAL+ycWk6UX8ytQRwiUsQWySfCtb73rQeWRoedzuOXORK8Gz18Q@mail.gmail.com>
 <c6edc4c6-8711-d2ca-2011-89c0daaae025@yahoo.gr>
 <CAKkiGbszs3zhUN4=MaN9AvsA9pNYtKQwarAcsi-anyH-bOYiPQ@mail.gmail.com>
 <19fee58f-9d2e-3045-1294-80d92f6f2da1@yahoo.gr>
 <CAKkiGbt8hGBKOcTt=u0E0t-xtxOdOE+MaisohdrwGwd8nKcP3A@mail.gmail.com>
 <b1b84ed1-8f1a-bbaf-872c-9276ec8eb5a8@yahoo.gr>
 <CAKkiGbvZEtrp-gYApMxPj6eNGd4T-ERb3pvHnFniYaq8a8iNhw@mail.gmail.com>
 <0f533804-424c-1fbe-30f5-3abc78788b7d@yahoo.gr>
 <alpine.LFD.2.21.1911281220530.607873@reclus.nhh.no>
Message-ID: <547895470.24608361.1576479802705@mail.yahoo.com>

 Thank you, I did a Pull Request.
    ???? ??????, 28 ????????? 2019, 01:25:38 ?.?. EET, ? ??????? Roger Bivand <roger.bivand at nhh.no> ??????:  
 
 On Wed, 27 Nov 2019, Leonidas Liakos via R-sig-Geo wrote:

> Thank you for your help!
>
> I tried to fix stackApply according to your instructions.
>
> Now the indices of names are the same and consistent with indices
> enumeration (gist for validation and tests:
> https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170).
>
> I've attached a patch file here:
>
> https://gist.github.com/kokkytos/ca2c319134677b19900579665267a7a7

Thanks very much for contributing!

Please consider raising an issue on https://github.com/rspatial/raster 
pointing to this thread and your patch. I had expected response from 
raster developers here, but they may well be on field work, so raising an 
issue on the development site should get their attention when there is 
enough time for them to look. You might even fork raster, apply your patch 
and file a PR in addition to the issue. In that case, a short test would 
be helpful, and maybe edits to the documentation.

Roger


>
>> stackapply_mean
> class????? : RasterBrick
> dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
> resolution : 500, 500? (x, y)
> extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
> crs??????? : NA
> source???? : /tmp/Rtmp9W8UNc/raster/r_tmp_2019-11-27_191205_2929_20324.grd
> names????? :? index_5,? index_6,? index_7,? index_1,? index_2,?
> index_3,? index_4
> min values : 444.6946, 440.2028, 429.6900, 442.7436, 440.0467, 444.9182,
> 437.1589
> max values : 565.8671, 560.1375, 561.7972, 556.2471, 563.8341, 561.7687,
> 560.4509
>
>> ver_mean
> class????? : RasterStack
> dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
> resolution : 500, 500? (x, y)
> extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
> crs??????? : NA
> names????? :? layer.1,? layer.2,? layer.3,? layer.4,? layer.5,?
> layer.6,? layer.7
> min values : 442.7436, 440.0467, 444.9182, 437.1589, 444.6946, 440.2028,
> 429.6900
> max values : 556.2471, 563.8341, 561.7687, 560.4509, 565.8671, 560.1375,
> 561.7972
>
>
>
>
>
> On 11/26/19 10:58 PM, Vijay Lulla wrote:
>> Hmm...it appears that stackApply is using different conditions which
>> might be causing this problem. Below is the snippet of the code which
>> I think might be the problem.
>>
>> ## For canProcessInMemory
>> if (rowcalc) {
>> ? v <- lapply(uin, function(i) fun(x[, ind == i, drop = FALSE], na.rm
>> = na.rm))
>> }
>> else {
>> ? v <- lapply(uin, function(i, ...) apply(x[, ind == i, drop = FALSE],
>> 1, fun, na.rm = na.rm))
>> }
>>
>>
>> ## If canProcessInMemory is not TRUE
>> if (rowcalc) {
>> ? v <- lapply(uin, function(i) fun(a[, ind == uin[i], drop = FALSE],
>> na.rm = na.rm))
>> }
>> else {
>> ? v <- lapply(uin, function(i, ...) apply(a[, ind == uin[i], drop =
>> FALSE], 1, fun, na.rm = na.rm))
>> }
>>
>> I think they should both be same but it appears that they aren't and
>> that's what you've discovered.? Maybe you can try fix(stackApply) to
>> see if it really is the problem and can tell us what you find.?
>> Anyways, good catch...and...sorry for wasting your time.
>> Cordially,
>> Vijay.
>>
>> On Tue, Nov 26, 2019 at 2:53 PM Leonidas Liakos
>> <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>>
>>? ? Thank you!
>>? ? The problem is not with the resulting values but with the index
>>? ? mapping. Values are correct in all three cases.
>>
>>? ? As I wrote in a previous post in the thread
>>? ? (https://stat.ethz.ch/pipermail/r-sig-geo/2019-November/027821.html)
>>? ? , stackApply behaves inconsistently depending on whether the
>>? ? exported stack will remain in memory or it will be stored, due to
>>? ? its large size, on the hard disk.
>>
>>? ? In the first case the indices are identical to my function
>>? ? (ver_mean) and the lubridate::wday indexing system (and are
>>? ? correct) while in the second they are shuffled.
>>
>>? ? So, while Sunday has index 1 and while in the first case (when the
>>? ? result is in memory) stackApply returns the correct index, in the
>>? ? second case (when the result is stored on the hard disk) it
>>? ? returns index_4! So how can one be sure if index_1 corresponds to
>>? ? Sunday or another day using stackApply since it sometimes
>>? ? enumerates it with index_1 and sometimes index_4?
>>
>>
>>? ? Try to run this example (when the resulting stack remains in
>>? ? memory) to see that the indexes are identical (stackApply =
>>? ? ver_median = lubridate::wday)
>>? ? https://gist.github.com/kokkytos/5d554b5a725bb48d2189e2d1fa0e2206
>>
>>? ? Thank you again
>>
>>? ? On 11/26/19 9:00 PM, Vijay Lulla wrote:
>>>? ? I'm sorry for the miscommunication.? What I meant to say is that
>>>? ? the output from stackApply and zApply are the same (because
>>>? ? zApply uses stackApply internally) except the names.? The
>>>? ? behavior of stackApply makes sense because AFAIUI R doesn't
>>>? ? automatically reorder vectors/indices that it receives.? Your
>>>? ? observation about inconsistent result with ver_mean is very valid
>>>? ? though!? And, that's what I meant with my comment that using
>>>? ? sapply with the explicit ordering that you want is the best way
>>>? ? to control what raster package will output.? In R the input order
>>>? ? should be maintained (this is the prime difference between SQL
>>>? ? and R) but packages/tools do not always adhere to this...so it's
>>>? ? never clear how the output will be ordered.? Sorry for the confusion.
>>>
>>>
>>>? ? On Tue, Nov 26, 2019 at 12:22 PM Leonidas Liakos
>>>? ? <leonidas_liakos at yahoo.gr <mailto:leonidas_liakos at yahoo.gr>> wrote:
>>>
>>>? ? ? ? Why do they seem logical since they do not match?
>>>
>>>? ? ? ? Check for example index 1 (Sunday). The results are different
>>>? ? ? ? for the three processes
>>>
>>>? ? ? ? > stackapply_mean
>>>? ? ? ? class????? : RasterBrick
>>>? ? ? ? dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>>? ? ? ? resolution : 500, 500? (x, y)
>>>? ? ? ? extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>>? ? ? ? crs??????? : NA
>>>? ? ? ? source???? :
>>>? ? ? ? /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191359_7710_20324.grd
>>>? ? ? ? names????? :? index_5,? index_6,? index_7,? index_1,?
>>>? ? ? ? index_2,? index_3,? index_4
>>>? ? ? ? min values : 440.0467, 444.9182, 437.1589, 444.6946,
>>>? ? ? ? 440.2028, 429.6900, 442.7436
>>>? ? ? ? max values : 563.8341, 561.7687, 560.4509, 565.8671,
>>>? ? ? ? 560.1375, 561.7972, 556.2471
>>>
>>>
>>>? ? ? ? > ver_mean
>>>? ? ? ? class????? : RasterStack
>>>? ? ? ? dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>>? ? ? ? resolution : 500, 500? (x, y)
>>>? ? ? ? extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>>? ? ? ? crs??????? : NA
>>>? ? ? ? names????? :? layer.1,? layer.2,? layer.3,? layer.4,?
>>>? ? ? ? layer.5,? layer.6,? layer.7
>>>? ? ? ? min values : 442.7436, 440.0467, 444.9182, 437.1589,
>>>? ? ? ? 444.6946, 440.2028, 429.6900
>>>? ? ? ? max values : 556.2471, 563.8341, 561.7687, 560.4509,
>>>? ? ? ? 565.8671, 560.1375, 561.7972
>>>
>>>
>>>? ? ? ? > z
>>>? ? ? ? class????? : RasterBrick
>>>? ? ? ? dimensions : 300, 300, 90000, 7? (nrow, ncol, ncell, nlayers)
>>>? ? ? ? resolution : 500, 500? (x, y)
>>>? ? ? ? extent???? : 0, 150000, 0, 150000? (xmin, xmax, ymin, ymax)
>>>? ? ? ? crs??????? : NA
>>>? ? ? ? source???? :
>>>? ? ? ? /tmp/RtmpkRMXLb/raster/r_tmp_2019-11-26_191439_7710_04780.grd
>>>? ? ? ? names????? :?????? X1,?????? X2,?????? X3,?????? X4,??????
>>>? ? ? ? X5,?????? X6,?????? X7
>>>? ? ? ? min values : 440.0467, 444.9182, 437.1589, 444.6946,
>>>? ? ? ? 440.2028, 429.6900, 442.7436
>>>? ? ? ? max values : 563.8341, 561.7687, 560.4509, 565.8671,
>>>? ? ? ? 560.1375, 561.7972, 556.2471
>>>? ? ? ? ?????????? : 1, 2, 3, 4, 5, 6, 7
>>>
>>>
>>>? ? ? ? On 11/26/19 7:03 PM, Vijay Lulla wrote:
>>>>? ? ? ? If you read the code/help for `stackApply` and `zApply`
>>>>? ? ? ? you'll see that the results that you obtain make sense (at
>>>>? ? ? ? least they seem sensible/reasonable to me).? IMO, if you
>>>>? ? ? ? want to control the ordering of your layers then just use
>>>>? ? ? ? sapply, like how you've used for ver_mean.? IMO, this is the
>>>>? ? ? ? only reliable (safe?), and quite a readable, way to
>>>>? ? ? ? accomplish what you're trying to do.
>>>>? ? ? ? Just my 2 cents.
>>>>? ? ? ? -- Vijay.
>>>>
>>>>? ? ? ? On Tue, Nov 26, 2019 at 11:19 AM Leonidas Liakos via
>>>>? ? ? ? R-sig-Geo <r-sig-geo at r-project.org
>>>>? ? ? ? <mailto:r-sig-geo at r-project.org>> wrote:
>>>>
>>>>? ? ? ? ? ? I added raster::zApply in my tests to validate the
>>>>? ? ? ? ? ? results. However, the
>>>>? ? ? ? ? ? indices of the names of the results are different now.
>>>>? ? ? ? ? ? Recall that the
>>>>? ? ? ? ? ? goal is to calculate from a raster stack time series the
>>>>? ? ? ? ? ? mean per day of
>>>>? ? ? ? ? ? the week. And that problem I have is that stackApply,
>>>>? ? ? ? ? ? zApply and
>>>>? ? ? ? ? ? calc/sapply return different indices in the result
>>>>? ? ? ? ? ? names. New code is
>>>>? ? ? ? ? ? available here:
>>>>? ? ? ? ? ? https://gist.github.com/kokkytos/93f315a5ecf59c0b183f9788754bc170
>>>>? ? ? ? ? ? I'm really curious about missing something.
>>>>
>>>>
>>>>? ? ? ? ? ? On 11/20/19 3:30 AM, Frederico Faleiro wrote:
>>>>? ? ? ? ? ? > Hi Leonidas,
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? > both results are in the same order, but the name is
>>>>? ? ? ? ? ? different.
>>>>? ? ? ? ? ? > You can rename the first as in the second:
>>>>? ? ? ? ? ? > names(res) <- names(res2)
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? > I provided an example to help you understand the logic.
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? > library(raster)
>>>>? ? ? ? ? ? > beginCluster(2)
>>>>? ? ? ? ? ? > r <- raster()
>>>>? ? ? ? ? ? > values(r) <- 1
>>>>? ? ? ? ? ? > # simple sequential stack from 1 to 6 in all cells
>>>>? ? ? ? ? ? > s <- stack(r, r*2, r*3, r*4, r*5, r*6)
>>>>? ? ? ? ? ? > s
>>>>? ? ? ? ? ? > res <- clusterR(s, stackApply, args =
>>>>? ? ? ? ? ? list(indices=c(2,2,3,3,1,1), fun
>>>>? ? ? ? ? ? > = mean))
>>>>? ? ? ? ? ? > res
>>>>? ? ? ? ? ? > res2 <- stackApply(s, c(2,2,3,3,1,1), mean)
>>>>? ? ? ? ? ? > res2
>>>>? ? ? ? ? ? > dif <- res - res2
>>>>? ? ? ? ? ? > # exatly the same order because the difference?is zero
>>>>? ? ? ? ? ? for all layers
>>>>? ? ? ? ? ? > dif
>>>>? ? ? ? ? ? > # rename
>>>>? ? ? ? ? ? > names(res) <- names(res2)
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? > Best regards,
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? > Frederico Faleiro
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? > On Tue, Nov 19, 2019 at 4:15 PM Leonidas Liakos via
>>>>? ? ? ? ? ? R-sig-Geo
>>>>? ? ? ? ? ? > <r-sig-geo at r-project.org
>>>>? ? ? ? ? ? <mailto:r-sig-geo at r-project.org>
>>>>? ? ? ? ? ? <mailto:r-sig-geo at r-project.org
>>>>? ? ? ? ? ? <mailto:r-sig-geo at r-project.org>>> wrote:
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?I run the example with clusterR:
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?no_cores <- parallel::detectCores() -1
>>>>? ? ? ? ? ? >? ? ?raster::beginCluster(no_cores)
>>>>? ? ? ? ? ? >? ? ??????? res <- raster::clusterR(inp,
>>>>? ? ? ? ? ? raster::stackApply, args =
>>>>? ? ? ? ? ? >? ? ?list(indices=c(2,2,3,3,1,1),fun = mean))
>>>>? ? ? ? ? ? >? ? ?raster::endCluster()
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?And the result is:
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?> res
>>>>? ? ? ? ? ? >? ? ?class?????????? : RasterBrick
>>>>? ? ? ? ? ? >? ? ?dimensions : 180, 360, 64800, 3?? (nrow, ncol,
>>>>? ? ? ? ? ? ncell, nlayers)
>>>>? ? ? ? ? ? >? ? ?resolution : 1, 1?? (x, y)
>>>>? ? ? ? ? ? >? ? ?extent???????? : -180, 180, -90, 90?? (xmin, xmax,
>>>>? ? ? ? ? ? ymin, ymax)
>>>>? ? ? ? ? ? >? ? ?crs?????????????? : +proj=longlat +datum=WGS84
>>>>? ? ? ? ? ? +ellps=WGS84
>>>>? ? ? ? ? ? >? ? ?+towgs84=0,0,0
>>>>? ? ? ? ? ? >? ? ?source???????? : memory
>>>>? ? ? ? ? ? >? ? ?names?????????? : layer.1, layer.2, layer.3
>>>>? ? ? ? ? ? >? ? ?min values :???????? 1.5,???????? 3.5,???????? 5.5
>>>>? ? ? ? ? ? >? ? ?max values :???????? 1.5,???????? 3.5,???????? 5.5??
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?layer.1, layer.2, layer.3 (?)
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?So what corrensponds to what?
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?If I run:
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?res2 <- stackApply(inp,c(2,2,3,3,1,1),mean)
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?The result is:
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?> res2
>>>>? ? ? ? ? ? >? ? ?class? ? ? : RasterBrick
>>>>? ? ? ? ? ? >? ? ?dimensions : 180, 360, 64800, 3? (nrow, ncol,
>>>>? ? ? ? ? ? ncell, nlayers)
>>>>? ? ? ? ? ? >? ? ?resolution : 1, 1? (x, y)
>>>>? ? ? ? ? ? >? ? ?extent? ? ?: -180, 180, -90, 90? (xmin, xmax,
>>>>? ? ? ? ? ? ymin, ymax)
>>>>? ? ? ? ? ? >? ? ?crs? ? ? ? : +proj=longlat +datum=WGS84
>>>>? ? ? ? ? ? +ellps=WGS84 +towgs84=0,0,0
>>>>? ? ? ? ? ? >? ? ?source? ? ?: memory
>>>>? ? ? ? ? ? >? ? ?names? ? ? : index_2, index_3, index_1
>>>>? ? ? ? ? ? >? ? ?min values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>>>? ? ? ? ? ? >? ? ?max values :? ? ?1.5,? ? ?3.5,? ? ?5.5
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?There is no consistency with the names of the
>>>>? ? ? ? ? ? output and obscure
>>>>? ? ? ? ? ? >? ? ?correspondence with the indices in the case of
>>>>? ? ? ? ? ? clusterR
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?? ? ? ? [[alternative HTML version deleted]]
>>>>? ? ? ? ? ? >
>>>>? ? ? ? ? ? >? ? ?_______________________________________________
>>>>? ? ? ? ? ? >? ? ?R-sig-Geo mailing list
>>>>? ? ? ? ? ? >? ? ?R-sig-Geo at r-project.org
>>>>? ? ? ? ? ? <mailto:R-sig-Geo at r-project.org>
>>>>? ? ? ? ? ? <mailto:R-sig-Geo at r-project.org
>>>>? ? ? ? ? ? <mailto:R-sig-Geo at r-project.org>>
>>>>? ? ? ? ? ? >? ? ?https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>? ? ? ? ? ? >
>>>>
>>>>? ? ? ? ? ? ? ? ? ? [[alternative HTML version deleted]]
>>>>
>>>>? ? ? ? ? ? _______________________________________________
>>>>? ? ? ? ? ? R-sig-Geo mailing list
>>>>? ? ? ? ? ? R-sig-Geo at r-project.org <mailto:R-sig-Geo at r-project.org>
>>>>? ? ? ? ? ? https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>
>>>>
>>>
>>
>
> ??? [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en  
	[[alternative HTML version deleted]]


From eg@mp@du @end|ng |rom gm@||@com  Thu Dec 19 16:22:02 2019
From: eg@mp@du @end|ng |rom gm@||@com (Enoch Gyamfi Ampadu)
Date: Thu, 19 Dec 2019 17:22:02 +0200
Subject: [R-sig-Geo] Isolation of cover classes
Message-ID: <CAC4+n+yMhn3Zg=gcaNz7jQtY-dLE3_+626hd+Y7PfGx+TYsuAQ@mail.gmail.com>

Dear List,

Please I have done a land cover classification with SVM using the caret
package. I will like to isolate each of the land cover classes and have a
map of each of each of them but have not been successful. I will be happy
if I could get some help on this.

Thank you.

Best regards,

Enoch

-- 
*Enoch Gyamfi - Ampadu*

*Geography & Environmental Sciences*

*College of Agriculture, Engineering & Science*

*University of KwaZulu-Natal, Westville Campus*

*Private Bag X54001*
*Durban, South Africa **? 4000**.*
*Phone: +27 835 828255*

*email: egampadu at gmail.com <egampadu at gmail.com>*


*skype: enoch.ampadu*
*The highest evidence of nobility is self-control*.

*A simple act of kindness creates an endless ripple*.

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Thu Dec 19 19:04:32 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Thu, 19 Dec 2019 11:04:32 -0700
Subject: [R-sig-Geo] Kriging question, please
Message-ID: <CACxE24n8fDPTOsq2DzXbFt4_pfDmUV4MhvSYEWwpBMFrRMu_6w@mail.gmail.com>

Hello!

Is there a method for Kriging for irregular data, please?  I?m about 99.9%
sure that there is not, but just thought I would double check.

Thanks so much for your help!

Happy Holidays!

Sincerely,
Erin

-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From @@r@h@go@|ee @end|ng |rom gm@||@com  Thu Dec 19 19:35:38 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Thu, 19 Dec 2019 13:35:38 -0500
Subject: [R-sig-Geo] Kriging question, please
In-Reply-To: <CACxE24n8fDPTOsq2DzXbFt4_pfDmUV4MhvSYEWwpBMFrRMu_6w@mail.gmail.com>
References: <CACxE24n8fDPTOsq2DzXbFt4_pfDmUV4MhvSYEWwpBMFrRMu_6w@mail.gmail.com>
Message-ID: <CAM_vjukOecPLAA_WPT7LS=sH7gV8rSUa58k_6+H2-TUt5jgScw@mail.gmail.com>

Hi Erin,

I'm not sure what you mean by "irregular data." There's no requirement
that points be on a grid, and that's the only thing I can think of,
but I'm not sure how you'd get the idea that that was required given
the existence of the basic tutorials using the meuse dataset.

https://rpubs.com/nabilabd/118172

So, please expand on your question.

Sarah

On Thu, Dec 19, 2019 at 1:04 PM Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> Hello!
>
> Is there a method for Kriging for irregular data, please?  I?m about 99.9%
> sure that there is not, but just thought I would double check.
>
> Thanks so much for your help!
>
> Happy Holidays!
>
> Sincerely,
> Erin
>
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From j@r|@ok@@nen @end|ng |rom ou|u@||  Thu Dec 19 20:11:34 2019
From: j@r|@ok@@nen @end|ng |rom ou|u@|| (Jari Oksanen)
Date: Thu, 19 Dec 2019 19:11:34 +0000
Subject: [R-sig-Geo] Kriging question, please
In-Reply-To: <CACxE24n8fDPTOsq2DzXbFt4_pfDmUV4MhvSYEWwpBMFrRMu_6w@mail.gmail.com>
References: <CACxE24n8fDPTOsq2DzXbFt4_pfDmUV4MhvSYEWwpBMFrRMu_6w@mail.gmail.com>
Message-ID: <51253374-5066-4B49-9088-BED2CFD7524F@oulu.fi>

Howdy,

Danie Kriege  [d?ni? ?kri??]<https://en.wikipedia.org/wiki/Help:IPA/Afrikaans> was a mining engineer. I find it very hard to believe there is a practicing engineer who thinks that you can find a gold mine only if you have collected data in a regular grid (if that is what you mean with ?regular data"). Indeed, his methods can be used for usual prospecting data ? which typically is irregular or not in a regular grid.

Cheers, J.

On 19 Dec 2019, at 20:04, Erin Hodgess <erinm.hodgess at gmail.com<mailto:erinm.hodgess at gmail.com>> wrote:

Hello!

Is there a method for Kriging for irregular data, please?  I?m about 99.9%
sure that there is not, but just thought I would double check.

Thanks so much for your help!

Happy Holidays!

Sincerely,
Erin

--
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com<mailto:erinm.hodgess at gmail.com>

[[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org<mailto:R-sig-Geo at r-project.org>
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Thu Dec 19 20:36:45 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Thu, 19 Dec 2019 12:36:45 -0700
Subject: [R-sig-Geo] Kriging question, please
In-Reply-To: <51253374-5066-4B49-9088-BED2CFD7524F@oulu.fi>
References: <CACxE24n8fDPTOsq2DzXbFt4_pfDmUV4MhvSYEWwpBMFrRMu_6w@mail.gmail.com>
 <51253374-5066-4B49-9088-BED2CFD7524F@oulu.fi>
Message-ID: <CACxE24nDjtfncBcMvvx-8ocpcEQseUArYYmM+5k0ZqvD=XvVXQ@mail.gmail.com>

Hi again everyone:

Sorry for the unclear question.

Typically, I have data that is at say, 8 locations and collected every
hour.  So spatial and/or spatial-temporal kriging works fine.

In this case, I am dealing with traffic accident data.  So it can be at
"any" location and at any time.  I had the (probably incorrect) impression
that kriging may not work in that setting.

Any info much appreciated.

Thanks for your understanding.

Sincerely,
Erin



Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Thu, Dec 19, 2019 at 12:15 PM Jari Oksanen <jari.oksanen at oulu.fi> wrote:

> Howdy,
>
> Danie Kriege  [d?ni? ?kri??]
> <https://en.wikipedia.org/wiki/Help:IPA/Afrikaans> was a mining engineer.
> I find it very hard to believe there is a practicing engineer who thinks
> that you can find a gold mine only if you have collected data in a regular
> grid (if that is what you mean with ?regular data"). Indeed, his methods
> can be used for usual prospecting data ? which typically is irregular or
> not in a regular grid.
>
> Cheers, J.
>
> On 19 Dec 2019, at 20:04, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
> Hello!
>
> Is there a method for Kriging for irregular data, please?  I?m about 99.9%
> sure that there is not, but just thought I would double check.
>
> Thanks so much for your help!
>
> Happy Holidays!
>
> Sincerely,
> Erin
>
> --
> Erin Hodgess, PhD
> mailto: erinm.hodgess at gmail.com
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>

	[[alternative HTML version deleted]]


From jo@eph|ew|@1992 @end|ng |rom gm@||@com  Thu Dec 19 20:47:33 2019
From: jo@eph|ew|@1992 @end|ng |rom gm@||@com (Joe Lewis)
Date: Thu, 19 Dec 2019 19:47:33 +0000
Subject: [R-sig-Geo] Kriging question, please
In-Reply-To: <CAM0xMQ9D3Sz4WLiz0CBs1zZw1M8PUhUsqKTpTOYSTvsMxCo44w@mail.gmail.com>
References: <CACxE24n8fDPTOsq2DzXbFt4_pfDmUV4MhvSYEWwpBMFrRMu_6w@mail.gmail.com>
 <51253374-5066-4B49-9088-BED2CFD7524F@oulu.fi>
 <CACxE24nDjtfncBcMvvx-8ocpcEQseUArYYmM+5k0ZqvD=XvVXQ@mail.gmail.com>
 <CAM0xMQ9D3Sz4WLiz0CBs1zZw1M8PUhUsqKTpTOYSTvsMxCo44w@mail.gmail.com>
Message-ID: <CAM0xMQ-j4o8dbTHLHdU40TtaTADeG0xDR-kpBThJdv2dcTmCjw@mail.gmail.com>

Hi Erin,

Depending on what you're trying to do, you may want to look into point
pattern analysis, particularly on linear networks (lots of research on how
to conduct kernel density estimations with traffic accident data)

See spatial analysis along networks (Okabe and Sugihara, 2012) and chapter
17 of spatial point patterns methodology and applications with R (Baddeley
et al. 2016) for introductions.

Hope that helps.

Kind regards,
Joseph Lewis

On Thu, Dec 19, 2019 at 7:45 PM Joe Lewis <josephlewis1992 at gmail.com> wrote:

> Hi Erin,
>
> Depending on what you're trying to do, you may want to look into point
> pattern analysis, particularly on linear networks (lots of research on how
> to conduct kernel density estimations with traffic accident data)
>
> See spatial analysis along networks (Okabe and Sugihara, 2012) and chapter
> 17 of spatial point patterns methodology and applications with R (Baddeley
> et al. 2016) for introductions.
>
> Hope that helps.
>
> Kind regards,
> Joseph Lewis
>
>
> On Thu, 19 Dec 2019, 19:37 Erin Hodgess, <erinm.hodgess at gmail.com> wrote:
>
>> Hi again everyone:
>>
>> Sorry for the unclear question.
>>
>> Typically, I have data that is at say, 8 locations and collected every
>> hour.  So spatial and/or spatial-temporal kriging works fine.
>>
>> In this case, I am dealing with traffic accident data.  So it can be at
>> "any" location and at any time.  I had the (probably incorrect) impression
>> that kriging may not work in that setting.
>>
>> Any info much appreciated.
>>
>> Thanks for your understanding.
>>
>> Sincerely,
>> Erin
>>
>>
>>
>> Erin Hodgess, PhD
>> mailto: erinm.hodgess at gmail.com
>>
>>
>> On Thu, Dec 19, 2019 at 12:15 PM Jari Oksanen <jari.oksanen at oulu.fi>
>> wrote:
>>
>> > Howdy,
>> >
>> > Danie Kriege  [d?ni? ?kri??]
>> > <https://en.wikipedia.org/wiki/Help:IPA/Afrikaans> was a mining
>> engineer.
>> > I find it very hard to believe there is a practicing engineer who thinks
>> > that you can find a gold mine only if you have collected data in a
>> regular
>> > grid (if that is what you mean with ?regular data"). Indeed, his methods
>> > can be used for usual prospecting data ? which typically is irregular or
>> > not in a regular grid.
>> >
>> > Cheers, J.
>> >
>> > On 19 Dec 2019, at 20:04, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>> >
>> > Hello!
>> >
>> > Is there a method for Kriging for irregular data, please?  I?m about
>> 99.9%
>> > sure that there is not, but just thought I would double check.
>> >
>> > Thanks so much for your help!
>> >
>> > Happy Holidays!
>> >
>> > Sincerely,
>> > Erin
>> >
>> > --
>> > Erin Hodgess, PhD
>> > mailto: erinm.hodgess at gmail.com
>> >
>> > [[alternative HTML version deleted]]
>> >
>> > _______________________________________________
>> > R-sig-Geo mailing list
>> > R-sig-Geo at r-project.org
>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> >
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>

	[[alternative HTML version deleted]]


From er|nm@hodge@@ @end|ng |rom gm@||@com  Thu Dec 19 21:13:19 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Thu, 19 Dec 2019 13:13:19 -0700
Subject: [R-sig-Geo] Kriging question, please
In-Reply-To: <CAM0xMQ-j4o8dbTHLHdU40TtaTADeG0xDR-kpBThJdv2dcTmCjw@mail.gmail.com>
References: <CACxE24n8fDPTOsq2DzXbFt4_pfDmUV4MhvSYEWwpBMFrRMu_6w@mail.gmail.com>
 <51253374-5066-4B49-9088-BED2CFD7524F@oulu.fi>
 <CACxE24nDjtfncBcMvvx-8ocpcEQseUArYYmM+5k0ZqvD=XvVXQ@mail.gmail.com>
 <CAM0xMQ9D3Sz4WLiz0CBs1zZw1M8PUhUsqKTpTOYSTvsMxCo44w@mail.gmail.com>
 <CAM0xMQ-j4o8dbTHLHdU40TtaTADeG0xDR-kpBThJdv2dcTmCjw@mail.gmail.com>
Message-ID: <CACxE24kvuy6Sp3X7vYhNzB+vYYaNS9SH3=y3ezgLtG7q0W-qTg@mail.gmail.com>

This is great!

Thanks Joe!

Sincerely,
Erin
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com


On Thu, Dec 19, 2019 at 12:47 PM Joe Lewis <josephlewis1992 at gmail.com>
wrote:

> Hi Erin,
>
> Depending on what you're trying to do, you may want to look into point
> pattern analysis, particularly on linear networks (lots of research on how
> to conduct kernel density estimations with traffic accident data)
>
> See spatial analysis along networks (Okabe and Sugihara, 2012) and chapter
> 17 of spatial point patterns methodology and applications with R (Baddeley
> et al. 2016) for introductions.
>
> Hope that helps.
>
> Kind regards,
> Joseph Lewis
>
> On Thu, Dec 19, 2019 at 7:45 PM Joe Lewis <josephlewis1992 at gmail.com>
> wrote:
>
>> Hi Erin,
>>
>> Depending on what you're trying to do, you may want to look into point
>> pattern analysis, particularly on linear networks (lots of research on how
>> to conduct kernel density estimations with traffic accident data)
>>
>> See spatial analysis along networks (Okabe and Sugihara, 2012) and
>> chapter 17 of spatial point patterns methodology and applications with R
>> (Baddeley et al. 2016) for introductions.
>>
>> Hope that helps.
>>
>> Kind regards,
>> Joseph Lewis
>>
>>
>> On Thu, 19 Dec 2019, 19:37 Erin Hodgess, <erinm.hodgess at gmail.com> wrote:
>>
>>> Hi again everyone:
>>>
>>> Sorry for the unclear question.
>>>
>>> Typically, I have data that is at say, 8 locations and collected every
>>> hour.  So spatial and/or spatial-temporal kriging works fine.
>>>
>>> In this case, I am dealing with traffic accident data.  So it can be at
>>> "any" location and at any time.  I had the (probably incorrect)
>>> impression
>>> that kriging may not work in that setting.
>>>
>>> Any info much appreciated.
>>>
>>> Thanks for your understanding.
>>>
>>> Sincerely,
>>> Erin
>>>
>>>
>>>
>>> Erin Hodgess, PhD
>>> mailto: erinm.hodgess at gmail.com
>>>
>>>
>>> On Thu, Dec 19, 2019 at 12:15 PM Jari Oksanen <jari.oksanen at oulu.fi>
>>> wrote:
>>>
>>> > Howdy,
>>> >
>>> > Danie Kriege  [d?ni? ?kri??]
>>> > <https://en.wikipedia.org/wiki/Help:IPA/Afrikaans> was a mining
>>> engineer.
>>> > I find it very hard to believe there is a practicing engineer who
>>> thinks
>>> > that you can find a gold mine only if you have collected data in a
>>> regular
>>> > grid (if that is what you mean with ?regular data"). Indeed, his
>>> methods
>>> > can be used for usual prospecting data ? which typically is irregular
>>> or
>>> > not in a regular grid.
>>> >
>>> > Cheers, J.
>>> >
>>> > On 19 Dec 2019, at 20:04, Erin Hodgess <erinm.hodgess at gmail.com>
>>> wrote:
>>> >
>>> > Hello!
>>> >
>>> > Is there a method for Kriging for irregular data, please?  I?m about
>>> 99.9%
>>> > sure that there is not, but just thought I would double check.
>>> >
>>> > Thanks so much for your help!
>>> >
>>> > Happy Holidays!
>>> >
>>> > Sincerely,
>>> > Erin
>>> >
>>> > --
>>> > Erin Hodgess, PhD
>>> > mailto: erinm.hodgess at gmail.com
>>> >
>>> > [[alternative HTML version deleted]]
>>> >
>>> > _______________________________________________
>>> > R-sig-Geo mailing list
>>> > R-sig-Geo at r-project.org
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>> >
>>> >
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>

	[[alternative HTML version deleted]]


From @r|e|@|uente@d| @end|ng |rom u@@ch@c|  Tue Dec 24 17:29:31 2019
From: @r|e|@|uente@d| @end|ng |rom u@@ch@c| (Ariel Fuentesdi)
Date: Tue, 24 Dec 2019 13:29:31 -0300
Subject: [R-sig-Geo] Point Pattern question
Message-ID: <CAD0a9Khs3sBNtR-EmWch9FkL6-fiFNUn8jHxttdi9a=COUmgwQ@mail.gmail.com>

Hi!

I have a conceptual question, can I use the Point Pattern model to find a
clustered region, but in each location it is more than one case?. What I
mean is to use attributes for the spatial locations.

Regards,
Ariel Fuentes

	[[alternative HTML version deleted]]


From @r|e|@|uente@d| @end|ng |rom u@@ch@c|  Mon Dec 30 14:04:43 2019
From: @r|e|@|uente@d| @end|ng |rom u@@ch@c| (Ariel Fuentesdi)
Date: Mon, 30 Dec 2019 10:04:43 -0300
Subject: [R-sig-Geo] Point Pattern question
In-Reply-To: <57057EB1-2BD8-4064-A45B-88F1D1526584@wm.edu>
References: <CAD0a9Khs3sBNtR-EmWch9FkL6-fiFNUn8jHxttdi9a=COUmgwQ@mail.gmail.com>
 <57057EB1-2BD8-4064-A45B-88F1D1526584@wm.edu>
Message-ID: <CAD0a9KigHVLiVVuduaA=6-gN3SB=C0=6XWuN8ZWyfq7ocdDLxA@mail.gmail.com>

I want to study if there is a spatial clustering on fare evasion stops data
(as points) e.g. there are 9 evaders in stop "A" but in stop "B" are 20
evaders. So what I've read about point patterns is that considers the
location of an event only and not its attributes, so I was wondering if it
is possible to include an attribute as weight of a given location in the
study of spatial clustering. ?Is the right approach or should I do
a geostatistical interpolation?

No, I haven't read that book but I did read the Applied Spatial Data
Analysis (https://asdar-book.org/)

Regards,
Ariel Fuentes

Le jeu. 26 d?c. 2019 ? 22:49, Frazier, Tyler <tjfrazier at wm.edu> a ?crit :

> Hi Ariel,
>
> I believe you are asking if it is possible to estimate a model with marks
> (different variable outcomes) as attributes to the points in your planar
> point pattern.  Last I read this was still under development, but not sure
> if I properly interpreted your question, or if it is currently possible.
>
> Have you read https://book.spatstat.org ?
>
> Best wishes,
> Tyler
>
> Tyler J. Frazier, Ph.D.
> *Lecturer of Interdisciplinary Studies*
> *Data Science Program*
> *College of William and Mary*
> *Webpage:* https://tyler-frazier.github.io
>
>
>
> On Dec 24, 2019, at 11:29 AM, Ariel Fuentesdi <ariel.fuentesdi at usach.cl>
> wrote:
>
> Hi!
>
> I have a conceptual question, can I use the Point Pattern model to find a
> clustered region, but in each location it is more than one case?. What I
> mean is to use attributes for the spatial locations.
>
> Regards,
> Ariel Fuentes
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>

	[[alternative HTML version deleted]]


From Roger@B|v@nd @end|ng |rom nhh@no  Mon Dec 30 14:13:29 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Mon, 30 Dec 2019 13:13:29 +0000
Subject: [R-sig-Geo] Point Pattern question
In-Reply-To: <CAD0a9KigHVLiVVuduaA=6-gN3SB=C0=6XWuN8ZWyfq7ocdDLxA@mail.gmail.com>
References: <CAD0a9Khs3sBNtR-EmWch9FkL6-fiFNUn8jHxttdi9a=COUmgwQ@mail.gmail.com>
 <57057EB1-2BD8-4064-A45B-88F1D1526584@wm.edu>,
 <CAD0a9KigHVLiVVuduaA=6-gN3SB=C0=6XWuN8ZWyfq7ocdDLxA@mail.gmail.com>
Message-ID: <f780c0334cca4ccd92ae6c508cbb40fd@nhh.no>

You were advised to read the Spatstat book https://book.spatstat.org - that documents the package you are most likely to need. ASDAR simply shows that the sp data representations presented in the first part can be used for analysis - the second part is not a comprehensive guide to available methods, and indeed very much has been added since the book was designed 15 years ago.

--
Roger Bivand
Norwegian School of Economics
Helleveien 30, 5045 Bergen, Norway
Roger.Bivand at nhh.no


________________________________________
Fra: R-sig-Geo <r-sig-geo-bounces at r-project.org> p? vegne av Ariel Fuentesdi <ariel.fuentesdi at usach.cl>
Sendt: mandag 30. desember 2019 14.04
Til: Frazier, Tyler
Kopi: r-sig-geo at r-project.org
Emne: Re: [R-sig-Geo] Point Pattern question

I want to study if there is a spatial clustering on fare evasion stops data
(as points) e.g. there are 9 evaders in stop "A" but in stop "B" are 20
evaders. So what I've read about point patterns is that considers the
location of an event only and not its attributes, so I was wondering if it
is possible to include an attribute as weight of a given location in the
study of spatial clustering. ?Is the right approach or should I do
a geostatistical interpolation?

No, I haven't read that book but I did read the Applied Spatial Data
Analysis (https://asdar-book.org/)

Regards,
Ariel Fuentes

Le jeu. 26 d?c. 2019 ? 22:49, Frazier, Tyler <tjfrazier at wm.edu> a ?crit :

> Hi Ariel,
>
> I believe you are asking if it is possible to estimate a model with marks
> (different variable outcomes) as attributes to the points in your planar
> point pattern.  Last I read this was still under development, but not sure
> if I properly interpreted your question, or if it is currently possible.
>
> Have you read https://book.spatstat.org ?
>
> Best wishes,
> Tyler
>
> Tyler J. Frazier, Ph.D.
> *Lecturer of Interdisciplinary Studies*
> *Data Science Program*
> *College of William and Mary*
> *Webpage:* https://tyler-frazier.github.io
>
>
>
> On Dec 24, 2019, at 11:29 AM, Ariel Fuentesdi <ariel.fuentesdi at usach.cl>
> wrote:
>
> Hi!
>
> I have a conceptual question, can I use the Point Pattern model to find a
> clustered region, but in each location it is more than one case?. What I
> mean is to use attributes for the spatial locations.
>
> Regards,
> Ariel Fuentes
>
> [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From m@ur|z|o@m@rch| @end|ng |rom |bbr@cnr@|t  Tue Dec 31 13:54:10 2019
From: m@ur|z|o@m@rch| @end|ng |rom |bbr@cnr@|t (maurizio marchi)
Date: Tue, 31 Dec 2019 13:54:10 +0100
Subject: [R-sig-Geo] Graphic CPU usage in R
Message-ID: <631d0266-a8aa-d60b-5c68-089238851bc1@ibbr.cnr.it>

HI all,
I'm a Linux user and (Ubuntu 18.04) and I was trying to find a way to 
speed up my R code. Reading on the web I found than some packages such 
as parallel or gpuR have been developed to allow faster calculation in 
R. Anyway I was wondering whether any other ways were available. My 
question concern how to speed up old R codes involving GIS procedures 
and mainly using rgdal, raster, biomod2, dismo, sp or other Spatial 
packages.
According to this post (here 
<https://starbeamrainbowlabs.com/blog/article.php?article=posts%2F254-run-program-on-amd-dedicated-graphics-card.html>) 
it seems that as linuk user we can launch a specific program using GPU 
so I was wondering if this could be used with R. In other words I would 
like to solve the issue from the beginning, opening an R session from 
terminal running on the GPU instead of on CPU(s). Is it possible? Does 
anyone has experience on it?
Here 
<https://www.researchgate.net/post/Parallel_computing_and_graphic_CPU_GCPU_usage_in_R_is_it_possible_with_ALL_R_packages> 
the question I opened on ResearchGate.
Thank you in advance and happy new year to everybody

-- 
*Maurizio Marchi,
PhD Forest Science - Ecological Mathematics*
Researcher
CNR - Institute of Biosciences and BioResources (IBBR), Florence 
division (Italy)
SkypeID: maurizioxyz
http://ibbr.cnr.it/ibbr/info/people/maurizio-marchi
#####------#####
Annals of Silvicultural Research Associated Editor
EUFGIS National Focal Point for Italy (www.eufgis.org)
Scopus Author ID: 57188626512
ResearcherID: T-3813-2019
http://b4est.eu/ project

	[[alternative HTML version deleted]]


From r@hep@rd @end|ng |rom @pp|-eco@y@@com  Tue Dec 31 14:57:27 2019
From: r@hep@rd @end|ng |rom @pp|-eco@y@@com (Rich Shepard)
Date: Tue, 31 Dec 2019 05:57:27 -0800 (PST)
Subject: [R-sig-Geo] Graphic CPU usage in R
In-Reply-To: <631d0266-a8aa-d60b-5c68-089238851bc1@ibbr.cnr.it>
References: <631d0266-a8aa-d60b-5c68-089238851bc1@ibbr.cnr.it>
Message-ID: <alpine.LNX.2.20.1912310548000.25416@salmo.appl-ecosys.com>

On Tue, 31 Dec 2019, maurizio marchi wrote:

> Anyway I was wondering whether any other ways were available. My question
> concern how to speed up old R codes involving GIS procedures and mainly
> using rgdal, raster, biomod2, dismo, sp or other Spatial packages.

Maurizio,

How many cores are in the CPU of your machine? AMD processors have two
threads per core (e.g., the Ryzen7 in my desktop has 8 cores and 16
threads). Programs need to be compiled to use multiple threads and you need
libraries such as mesa or opengl to take advantage of that.

Also, how much memory is installed on that system? More is always better.

> I was wondering if this could be used with R. In other words I would like
> to solve the issue from the beginning, opening an R session from terminal
> running on the GPU instead of on CPU(s).

Something else for you to consider is that there are two types of video
cards: those designed for gamers and those designed for technical work. An
explanation of the differences (focused on nVidia's products) is here:
<https://www.quora.com/What-is-the-different-between-gaming-GPU-vs-professional-graphics-programming-GPU>.

There are multiple facturs involved so it's not a simple solution. Of
course, if you have a long spatial model running you can start it using
screen and it will continue running even after you log out as long as the
computer is running.

Hope this helps,

Rich


From Roger@B|v@nd @end|ng |rom nhh@no  Tue Dec 31 15:46:57 2019
From: Roger@B|v@nd @end|ng |rom nhh@no (Roger Bivand)
Date: Tue, 31 Dec 2019 15:46:57 +0100
Subject: [R-sig-Geo] Graphic CPU usage in R
In-Reply-To: <alpine.LNX.2.20.1912310548000.25416@salmo.appl-ecosys.com>
References: <631d0266-a8aa-d60b-5c68-089238851bc1@ibbr.cnr.it>
 <alpine.LNX.2.20.1912310548000.25416@salmo.appl-ecosys.com>
Message-ID: <alpine.LFD.2.21.1912311536200.202418@reclus.nhh.no>

On Tue, 31 Dec 2019, Rich Shepard wrote:

> On Tue, 31 Dec 2019, maurizio marchi wrote:
>
>>  Anyway I was wondering whether any other ways were available. My question
>>  concern how to speed up old R codes involving GIS procedures and mainly
>>  using rgdal, raster, biomod2, dismo, sp or other Spatial packages.
>
> Maurizio,
>
> How many cores are in the CPU of your machine? AMD processors have two
> threads per core (e.g., the Ryzen7 in my desktop has 8 cores and 16
> threads). Programs need to be compiled to use multiple threads and you need
> libraries such as mesa or opengl to take advantage of that.
>
> Also, how much memory is installed on that system? More is always better.
>
>>  I was wondering if this could be used with R. In other words I would like
>>  to solve the issue from the beginning, opening an R session from terminal
>>  running on the GPU instead of on CPU(s).
>
> Something else for you to consider is that there are two types of video
> cards: those designed for gamers and those designed for technical work. An
> explanation of the differences (focused on nVidia's products) is here:
> <https://www.quora.com/What-is-the-different-between-gaming-GPU-vs-professional-graphics-programming-GPU>.
>
> There are multiple facturs involved so it's not a simple solution. Of
> course, if you have a long spatial model running you can start it using
> screen and it will continue running even after you log out as long as the
> computer is running.

GPU's are where the action was about ten years ago, but are not now. Many 
of the spatial packages that can benefit from multiple processors already 
facilitate their use, but often inter-process communication is the 
bottleneck, not per processor computation. A report from ten years ago is: 
https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1690584_code1391513.pdf?abstractid=1690584&mirid=1

Now, look to the stars and gdalcubes and many others, where the data are 
held in the cloud, and processing may be assigned to cloud nodes, with 
only target resolution output needing to be downloaded. The cloud nodes 
may actually be GPUs, but for the user this is transparent.

There are plenty of R packages accessing GPUs, described on the HPC task 
view: https://cran.r-project.org/view=HighPerformanceComputing.

Hope this clarifies,

Roger

>
> Hope this helps,
>
> Rich
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
https://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From er|nm@hodge@@ @end|ng |rom gm@||@com  Tue Dec 31 15:51:13 2019
From: er|nm@hodge@@ @end|ng |rom gm@||@com (Erin Hodgess)
Date: Tue, 31 Dec 2019 07:51:13 -0700
Subject: [R-sig-Geo] Graphic CPU usage in R
In-Reply-To: <631d0266-a8aa-d60b-5c68-089238851bc1@ibbr.cnr.it>
References: <631d0266-a8aa-d60b-5c68-089238851bc1@ibbr.cnr.it>
Message-ID: <CACxE24mz36Hz1MXGzhJp-4xDw1=7xRvB7uR1ZxoMuJRx64vHEQ@mail.gmail.com>

Hello!

Are you using the PGI compiler, please?  If not, you may want to check that
out.  It works with Linux.
Thanks
Erin

On Tue, Dec 31, 2019 at 5:54 AM maurizio marchi <maurizio.marchi at ibbr.cnr.it>
wrote:

> HI all,
> I'm a Linux user and (Ubuntu 18.04) and I was trying to find a way to
> speed up my R code. Reading on the web I found than some packages such
> as parallel or gpuR have been developed to allow faster calculation in
> R. Anyway I was wondering whether any other ways were available. My
> question concern how to speed up old R codes involving GIS procedures
> and mainly using rgdal, raster, biomod2, dismo, sp or other Spatial
> packages.
> According to this post (here
> <
> https://starbeamrainbowlabs.com/blog/article.php?article=posts%2F254-run-program-on-amd-dedicated-graphics-card.html>)
>
> it seems that as linuk user we can launch a specific program using GPU
> so I was wondering if this could be used with R. In other words I would
> like to solve the issue from the beginning, opening an R session from
> terminal running on the GPU instead of on CPU(s). Is it possible? Does
> anyone has experience on it?
> Here
> <
> https://www.researchgate.net/post/Parallel_computing_and_graphic_CPU_GCPU_usage_in_R_is_it_possible_with_ALL_R_packages>
>
> the question I opened on ResearchGate.
> Thank you in advance and happy new year to everybody
>
> --
> *Maurizio Marchi,
> PhD Forest Science - Ecological Mathematics*
> Researcher
> CNR - Institute of Biosciences and BioResources (IBBR), Florence
> division (Italy)
> SkypeID: maurizioxyz
> http://ibbr.cnr.it/ibbr/info/people/maurizio-marchi
> #####------#####
> Annals of Silvicultural Research Associated Editor
> EUFGIS National Focal Point for Italy (www.eufgis.org)
> Scopus Author ID: 57188626512
> ResearcherID: T-3813-2019
> http://b4est.eu/ project
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Erin Hodgess, PhD
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


