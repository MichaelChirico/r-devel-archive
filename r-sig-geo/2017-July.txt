From himalayanhari at gmail.com  Sun Jul  2 18:02:42 2017
From: himalayanhari at gmail.com (Hari Sharma)
Date: Mon, 3 Jul 2017 00:02:42 +0800
Subject: [R-sig-Geo] R_code
Message-ID: <990EAD92-DDF1-4F12-98CD-08CA06327B8B@gmail.com>

Hi
I want to reproject raster layer and want to mask the projected raster under mask file. However, I got problem. Could you please help me to fix this problem:

crs_84 <- CRS("+proj=longlat +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +no_defs")
crs_change <- CRS("+proj=tmerc +lat_0=0 +lon_0=84 +k=0.9999 +x_0=500000 +y_0=0 +a=6377276.345 +b=6356075.41314024 +units=m +no_defs")
template1000 <- raster(xmn = 95742.4, xmx = 936885.22, ymn = 2901798.01, ymx = 3391155.19, resolution = 1000, crs = crs_change)
layer_01_p_1000 <- projectRaster(from = layer_01, to = template1000) ####upto this its fine.
layer_01_p_1000_m <- mask(x = layer_01_p_1000, mask = maskfile, updatevalue = NA)  ###when I run this I got following error.

Error in mask(x = layer_01_p_1000, mask = maskfile, updatevalue = NA) : 
 unused arguments (x = layer_01_p_1000, mask = maskfile, updatevalue = NA)


I am looking forward from your response.

Harry

From javiermoreira at gmail.com  Mon Jul  3 18:25:25 2017
From: javiermoreira at gmail.com (Javier Moreira)
Date: Mon, 3 Jul 2017 13:25:25 -0300
Subject: [R-sig-Geo] adjust a polynomial function to a rasterVis time serie
Message-ID: <CAEyHP-3Rwm3=yxCkKfqUyYoOd3N9sWTMwtrQv4Aa=qMC91QvAw@mail.gmail.com>

hi,
i have to raster stacks with sequence of ndvi, from diferent satellites.
Both are time series with ndvi data, but the dates dont match. So, i need
to adjust a polynomial to each one in order to predict the ndvi for the
same date.
Im watching rasterVis package, using bowplot, and that let me create the
grafic to generate this, but, how can i create a function over that?

thanks!


-- 
Javier Moreira de Souza
Ingeniero Agr?nomo
099 406 006

	[[alternative HTML version deleted]]


From Zalman.Kaufman at MOH.HEALTH.GOV.IL  Thu Jul  6 07:23:06 2017
From: Zalman.Kaufman at MOH.HEALTH.GOV.IL (Zalman.Kaufman at MOH.HEALTH.GOV.IL)
Date: Thu, 6 Jul 2017 05:23:06 +0000
Subject: [R-sig-Geo] Geographically weighted regression
Message-ID: <E62DF41FFFBFFB40B1DFABB435A9BBD1D83AB5EA@XMTT10-DAG2.MOH.HEALTH.GOV.IL>

Dear group members,
Are you familiar with R packages doing geographically weighted regressions implementing negative binomial distribution?

Could you, please, refer to the JSS article by Gollini et al. on GWmodel?

Thanks a lot, Zalman






	[[alternative HTML version deleted]]


From zergioibarra at gmail.com  Thu Jul  6 16:16:52 2017
From: zergioibarra at gmail.com (Sergio Ibarra)
Date: Thu, 06 Jul 2017 14:16:52 +0000
Subject: [R-sig-Geo] Warning in st_intersection
Message-ID: <CABsRJgZDv+LTWY1tsEmoiELox21eA_8AiiP208+Xwfb9S37r_w@mail.gmail.com>

Hello everyone

In my work, I use to intersect polygon grids with lines or points. I use
the package "sf" with the function 'st_intersection". However, every time I
run st_intersection I got the following Warning Message:

Warning message:attribute variables are assumed to be spatially
constant throughout all geometries


What does this warning mean?

Thanks
-- 
?Sergio Ibarra Espinosa
Doutorando em Meteorologia
Instituto de Astronomia, Geof?sica e Ci?ncias Atmosf?ricas
Universidade de S?o Paulo
Rua do Mat?o, 1226
Cidade Universit?ria
S?o Paulo-SP - Brasil -
05508-090
+55-11-3091-2833
+55-11-97425-3791
Skype: sergio_ibarra1
Blog: www.sergioibarra.blogspot.com


[image: photo]
*Sergio Ibarra Espinosa*
MSc, Departamento de Ci?ncias Atmosf?ricas, Universidade de S?o Paulo
+ 55 11 3091-4731 <+%2055%2011%203091-4731> | +55 11 974 253 791
<+55%2011%20974%20253%20791> | sergio.ibarra at iag.usp.br |
www.sergioibarra.blogspot.com | Skype: sergio_ibarra1 <#> | Rua do Mat?o,
1226 - Cidade Universit?ria S?o Paulo-SP - Brasil - 05508-090
<http://us.linkedin.com/in/sibarra>  <http://twitter.com/sergio_ibarra>
<http://plus.google.com/+SergioIbarraEspinosa>  <http://github.com/xhie>
<http://wiseintro.co/sergioibarra>
Get a signature like this: Click here!
<http://ws-promos.appspot.com/r?rdata=eyJydXJsIjogImh0dHA6Ly93d3cud2lzZXN0YW1wLmNvbS9lbWFpbC1pbnN0YWxsP3dzX25jaWQ9NjcyMjk0MDA4JnV0bV9zb3VyY2U9ZXh0ZW5zaW9uJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPXByb21vXzU3MzI1Njg1NDg3Njk3OTIiLCAiZSI6ICI1NzMyNTY4NTQ4NzY5NzkyIn0=&u=651454521820988>

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Thu Jul  6 17:37:14 2017
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Thu, 6 Jul 2017 17:37:14 +0200
Subject: [R-sig-Geo] Warning in st_intersection
In-Reply-To: <CABsRJgZDv+LTWY1tsEmoiELox21eA_8AiiP208+Xwfb9S37r_w@mail.gmail.com>
References: <CABsRJgZDv+LTWY1tsEmoiELox21eA_8AiiP208+Xwfb9S37r_w@mail.gmail.com>
Message-ID: <ced31c26-be06-efaa-ef2d-265ab5436921@uni-muenster.de>



On 06/07/17 16:16, Sergio Ibarra wrote:
> Hello everyone
> 
> In my work, I use to intersect polygon grids with lines or points. I use
> the package "sf" with the function 'st_intersection". However, every time I
> run st_intersection I got the following Warning Message:
> 
> Warning message:attribute variables are assumed to be spatially
> constant throughout all geometries
> 
> 
> What does this warning mean?

Attribute values are assigned to sub-geometries; if these are spatially
constant, as for instance for land use, then this is fine. If they are
aggregates, such as population count, then this is not fine, wrong even.
By default, they are set to NA.

e.g.

demo(nc)
st_agr(nc) = "constant"

takes away the warning, as it sets all attribute values (in this case
wrongly) to "constant", as of their agr value (agr standing for
"attribute geometry relationship").

More here:
https://cran.r-project.org/web/packages/sf/vignettes/sf1.html#how-attributes-relate-to-geometries

> 
> Thanks
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 473 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170706/597f5e59/attachment.sig>

From johnwasige at gmail.com  Thu Jul  6 18:17:36 2017
From: johnwasige at gmail.com (John Wasige)
Date: Thu, 6 Jul 2017 18:17:36 +0200
Subject: [R-sig-Geo] Error: had status 127
Message-ID: <CAJgdCD6MF8qH=7q1wupNfuZumPXURNCX5NGhF4GQNV76-Hiv9g@mail.gmail.com>

?Dear all,

I am trying to mosaic MODIS tif data but I get the following error:

 running command 'gdal_merge.py -o mosaic_2002257.tif
G:/New_folder/MOD13Q1.A2002257.h11v07.005.2008256031543_NDVI_WGS84.tif
G:/New_folder/MOD13Q1.A2002257.h12v07.005.2008256032801_NDVI_WGS84.tif'
had status 127

?
?How ca?n I fix this error?

Thanks for your help
John

	[[alternative HTML version deleted]]


From zergioibarra at gmail.com  Thu Jul  6 19:32:38 2017
From: zergioibarra at gmail.com (Sergio Ibarra)
Date: Thu, 06 Jul 2017 17:32:38 +0000
Subject: [R-sig-Geo] Warning in st_intersection
In-Reply-To: <ced31c26-be06-efaa-ef2d-265ab5436921@uni-muenster.de>
References: <CABsRJgZDv+LTWY1tsEmoiELox21eA_8AiiP208+Xwfb9S37r_w@mail.gmail.com>
 <ced31c26-be06-efaa-ef2d-265ab5436921@uni-muenster.de>
Message-ID: <CABsRJgYDPPTjb63P3dEyhHrwmqCs6qwMFqreuaU7r0dNyCnbbA@mail.gmail.com>

So, I need to know the attribute geometry relationship (agr) (constant,
aggregate or identity)  of each variable of mine data set

Thank you Edzer


[image: photo]
*Sergio Ibarra Espinosa*
MSc Environment, PhD Meteorology, Departamento de Ci?ncias Atmosf?ricas,
Universidade de S?o Paulo
+ 55 11 3091-4731 | +55 11 974 253 791 | sergio.ibarra at usp.br |
www.sergioibarra.blogspot.com | Skype: sergio_ibarra1 <#> | Rua do Mat?o,
1226 - Cidade Universit?ria S?o Paulo-SP - Brasil - 05508-090
<http://github.com/ibarraespinosa> <http://us.linkedin.com/in/sibarra>
<http://wiseintro.co/sergioibarra> <http://blogger.com/profile/sergioibarra>
Get your own email signature
<https://wisestamp.com/email-install?utm_source=promotion&utm_medium=signature&utm_campaign=get_your_own>

Em qui, 6 de jul de 2017 ?s 12:37, Edzer Pebesma <
edzer.pebesma at uni-muenster.de> escreveu:

>
>
> On 06/07/17 16:16, Sergio Ibarra wrote:
> > Hello everyone
> >
> > In my work, I use to intersect polygon grids with lines or points. I use
> > the package "sf" with the function 'st_intersection". However, every
> time I
> > run st_intersection I got the following Warning Message:
> >
> > Warning message:attribute variables are assumed to be spatially
> > constant throughout all geometries
> >
> >
> > What does this warning mean?
>
> Attribute values are assigned to sub-geometries; if these are spatially
> constant, as for instance for land use, then this is fine. If they are
> aggregates, such as population count, then this is not fine, wrong even.
> By default, they are set to NA.
>
> e.g.
>
> demo(nc)
> st_agr(nc) = "constant"
>
> takes away the warning, as it sets all attribute values (in this case
> wrongly) to "constant", as of their agr value (agr standing for
> "attribute geometry relationship").
>
> More here:
>
> https://cran.r-project.org/web/packages/sf/vignettes/sf1.html#how-attributes-relate-to-geometries
>
> >
> > Thanks
> >
>
> --
> Edzer Pebesma
> Institute for Geoinformatics  (ifgi),  University of M?nster
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> <+49%20251%208333081>
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
?Sergio Ibarra Espinosa
Doutorando em Meteorologia
Instituto de Astronomia, Geof?sica e Ci?ncias Atmosf?ricas
Universidade de S?o Paulo
Rua do Mat?o, 1226
Cidade Universit?ria
S?o Paulo-SP - Brasil -
05508-090
+55-11-3091-2833
+55-11-97425-3791
Skype: sergio_ibarra1
Blog: www.sergioibarra.blogspot.com


[image: photo]
*Sergio Ibarra Espinosa*
MSc, Departamento de Ci?ncias Atmosf?ricas, Universidade de S?o Paulo
+ 55 11 3091-4731 <+%2055%2011%203091-4731> | +55 11 974 253 791
<+55%2011%20974%20253%20791> | sergio.ibarra at iag.usp.br |
www.sergioibarra.blogspot.com | Skype: sergio_ibarra1 <#> | Rua do Mat?o,
1226 - Cidade Universit?ria S?o Paulo-SP - Brasil - 05508-090
<http://us.linkedin.com/in/sibarra>  <http://twitter.com/sergio_ibarra>
<http://plus.google.com/+SergioIbarraEspinosa>  <http://github.com/xhie>
<http://wiseintro.co/sergioibarra>
Get a signature like this: Click here!
<http://ws-promos.appspot.com/r?rdata=eyJydXJsIjogImh0dHA6Ly93d3cud2lzZXN0YW1wLmNvbS9lbWFpbC1pbnN0YWxsP3dzX25jaWQ9NjcyMjk0MDA4JnV0bV9zb3VyY2U9ZXh0ZW5zaW9uJnV0bV9tZWRpdW09ZW1haWwmdXRtX2NhbXBhaWduPXByb21vXzU3MzI1Njg1NDg3Njk3OTIiLCAiZSI6ICI1NzMyNTY4NTQ4NzY5NzkyIn0=&u=651454521820988>

	[[alternative HTML version deleted]]


From tech_dev at wildintellect.com  Fri Jul  7 20:17:51 2017
From: tech_dev at wildintellect.com (Alex M)
Date: Fri, 7 Jul 2017 11:17:51 -0700
Subject: [R-sig-Geo] Possible rgdal enhancement request related to null
	attributes
Message-ID: <b9ef1a95-2553-4022-0566-c90a712e89f0@wildintellect.com>

Roger et al,

I couldn't find a ticket system for rgdal so this seemed the best place
to post.

We just hit a bug (in QGIS [1]) of sorts due to a change in behavior in
GDAL from 2.1.x to 2.2.x in the way null fields are handled. [2]

Short story if you have a column of null values and you save a geojson
those fields are not saved. So the next time you open up the file, you
have less columns than before. It's great for minifying geojson but
terrible for many other use cases.

I'm not 100% sure this will show up in rgdal (2.2 hasn't been pushed to
my Ubuntu yet so I haven't tested), but there's a good chance it will
depending on how R deals with unset vs null. If someone would like to
test, you can get a sample file from [3], readOGR and then writeOGR (to
geojson) and see if the null columns vanish.

The enhancement requested to deal with this, is to provide the user an
option that allows them to decide when a field is unset vs null. Or
maybe just a clarification on how R represents that data and how a user
could set a field as null instead of unset and vice versa.

Even R. pointed out that this can be controlled with the
OGR_F_IsFieldNull() part of the ogr api.

[1] https://issues.qgis.org/issues/16812
[2] https://trac.osgeo.org/gdal/wiki/rfc67_nullfieldvalues
[3] https://github.com/UCDavisLibrary/ava/issues/270

Thanks,
Alex


From rob00x at gmail.com  Mon Jul 10 14:03:46 2017
From: rob00x at gmail.com (Robin Lovelace)
Date: Mon, 10 Jul 2017 13:03:46 +0100
Subject: [R-sig-Geo] Any plans to provide access to CGAL in R?
Message-ID: <CAF16KkWXEH+Bno0HLPocqBb8AR9fWQmgKAbbN6HUCfYjbNNwhw@mail.gmail.com>

Hi all,

I've just been made aware of the CGAL <http://www.cgal.org/> C++ library
for computational geometry.

It seems to be able to do some things that may be useful to some readers of
of this list, as outlined in this overview of a syllabus based on CGAL:
http://delivery.acm.org/10.1145/2930000/2927362/a8-alliez.pdf

I've found some non-CRAN packages that link to it but nothing that seems to
provide access to all CGAL functions (listed below). Any ideas if there are
plans afoot for this or even if it's worth doing?


   - http://kien-kieu.github.io/lite/rlite.html
   - https://github.com/rcqls/EBSpatCGAL

There is also TDA on CRAN:
https://cran.r-project.org/web/packages/TDA/index.html

And a C++ library that claims to support it for simple features (found by
looking for support in QGIS): https://github.com/Oslandia/SFCGAL

Plus a question on StackExchange:
https://stackoverflow.com/questions/14463590/cgal-tools-is-there-an-interface-to-cgal-or-equivalent-toolset-in-r

Thanks in advance,

Robin

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Mon Jul 10 14:50:48 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Mon, 10 Jul 2017 12:50:48 +0000
Subject: [R-sig-Geo] Any plans to provide access to CGAL in R?
In-Reply-To: <CAF16KkWXEH+Bno0HLPocqBb8AR9fWQmgKAbbN6HUCfYjbNNwhw@mail.gmail.com>
References: <CAF16KkWXEH+Bno0HLPocqBb8AR9fWQmgKAbbN6HUCfYjbNNwhw@mail.gmail.com>
Message-ID: <CAAcGz9_-9seB13hO-1kraPT2d==-hgd=SHoWXZ5wfQkNgYiJig@mail.gmail.com>

On Mon, 10 Jul 2017 at 22:03 Robin Lovelace <rob00x at gmail.com> wrote:

> Hi all,
>
> I've just been made aware of the CGAL <http://www.cgal.org/> C++ library
> for computational geometry.
>
>
Hi Robin, we've discussed this here:

https://github.com/r-spatial/discuss/issues/6

FWIW a few years ago I did some simple Rcpp tests:

https://github.com/r-gris/cgalgris

Then I discovered RTriangle (it and TDA use the same underlying library)
and that provides the main thing I was after, now in package sfdct and
applied for more general use here:

https://github.com/r-gris/rangl

There is a lot you can do with a constrained triangulation, it's a "hammer"
and sometimes overkill but it's extremely flexible and powerful. I'm happy
to flesh out examples for problems you think might be solvable with it to
help promote why CGAL would be useful in R.

I'm very interested in general forms for these data, and the idea is to put
all the 3D rangl stuff on this footing eventually:

https://github.com/mdsumner/sc

tidygraph is a very good example of how we can modernize these indexed data
structures, and other newish packages icosa, webglobe, dggridR, also touch
on this "mesh" way of working. For that matter spdep and GEOS work that way
internally, but we only have fragments of the concept available at an R
level.

Simon Urbanek has s-u/rcgal on github but it doesn't expose very much.

Cheers, Mike.






> It seems to be able to do some things that may be useful to some readers of
> of this list, as outlined in this overview of a syllabus based on CGAL:
> http://delivery.acm.org/10.1145/2930000/2927362/a8-alliez.pdf
>
> I've found some non-CRAN packages that link to it but nothing that seems to
> provide access to all CGAL functions (listed below). Any ideas if there are
> plans afoot for this or even if it's worth doing?
>
>
>    - http://kien-kieu.github.io/lite/rlite.html
>    - https://github.com/rcqls/EBSpatCGAL
>
> There is also TDA on CRAN:
> https://cran.r-project.org/web/packages/TDA/index.html
>
> And a C++ library that claims to support it for simple features (found by
> looking for support in QGIS): https://github.com/Oslandia/SFCGAL
>
> Plus a question on StackExchange:
>
> https://stackoverflow.com/questions/14463590/cgal-tools-is-there-an-interface-to-cgal-or-equivalent-toolset-in-r
>
> Thanks in advance,
>
> Robin
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Mon Jul 10 18:04:30 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 10 Jul 2017 18:04:30 +0200
Subject: [R-sig-Geo] Possible rgdal enhancement request related to null
 attributes
In-Reply-To: <b9ef1a95-2553-4022-0566-c90a712e89f0@wildintellect.com>
References: <b9ef1a95-2553-4022-0566-c90a712e89f0@wildintellect.com>
Message-ID: <alpine.LFD.2.20.1707101754530.19173@reclus.nhh.no>

Hi Alex,

GDAL 2.2 is supported in rgdal and in sf - has been for some time. If GDAL 
>= 2.2:

https://r-forge.r-project.org/scm/viewvc.php/pkg/src/OGR_write.cpp?root=rgdal&r1=650&r2=660

and

https://r-forge.r-project.org/scm/viewvc.php/pkg/src/ogrsource.cpp?root=rgdal&r1=650&r2=660

This is general, not just GeoJSON, see

https://trac.osgeo.org/gdal/wiki/rfc67_nullfieldvalues

I don't see a good reason for making the user have to choose - could you 
suggest one with a use case reprex?

See also the sf code, which is relevant here.

Roger

On Fri, 7 Jul 2017, Alex M wrote:

> Roger et al,
>
> I couldn't find a ticket system for rgdal so this seemed the best place
> to post.
>
> We just hit a bug (in QGIS [1]) of sorts due to a change in behavior in
> GDAL from 2.1.x to 2.2.x in the way null fields are handled. [2]
>
> Short story if you have a column of null values and you save a geojson
> those fields are not saved. So the next time you open up the file, you
> have less columns than before. It's great for minifying geojson but
> terrible for many other use cases.
>
> I'm not 100% sure this will show up in rgdal (2.2 hasn't been pushed to
> my Ubuntu yet so I haven't tested), but there's a good chance it will
> depending on how R deals with unset vs null. If someone would like to
> test, you can get a sample file from [3], readOGR and then writeOGR (to
> geojson) and see if the null columns vanish.
>
> The enhancement requested to deal with this, is to provide the user an
> option that allows them to decide when a field is unset vs null. Or
> maybe just a clarification on how R represents that data and how a user
> could set a field as null instead of unset and vice versa.
>
> Even R. pointed out that this can be controlled with the
> OGR_F_IsFieldNull() part of the ogr api.
>
> [1] https://issues.qgis.org/issues/16812
> [2] https://trac.osgeo.org/gdal/wiki/rfc67_nullfieldvalues
> [3] https://github.com/UCDavisLibrary/ava/issues/270
>
> Thanks,
> Alex
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From tech_dev at wildintellect.com  Mon Jul 10 18:38:30 2017
From: tech_dev at wildintellect.com (- -)
Date: Mon, 10 Jul 2017 09:38:30 -0700 (PDT)
Subject: [R-sig-Geo] Possible rgdal enhancement request related to null
 attributes
In-Reply-To: <alpine.LFD.2.20.1707101754530.19173@reclus.nhh.no>
References: <b9ef1a95-2553-4022-0566-c90a712e89f0@wildintellect.com>
 <alpine.LFD.2.20.1707101754530.19173@reclus.nhh.no>
Message-ID: <1548093580.352100.1499704710864@email.1and1.com>

Roger,

It appears you preserved the pre 2.2 behavior which is what I think most people would expect, keep the attribute and set it as Null. 

That is not the case with other OGR related tools. I just hadn't had a chance to verify it in R, and had the issue come up in QGIS, so I appreciate you pointing out where in the code you addressed it (I did try to read through that file and didn't find that on my own).

The only reason I could see people wanting to drop nulls, is that certain formats would be more efficient. In the case of GeoJson it could result in a significantly smaller file. Though personally I think that a separate minification tool would be a better place to make that change. Which would mean a user would only need an override option if you had gone with UnsetField as the default.

So at this time I don't see any additional changes needed.

Thanks,
Alex
> On July 10, 2017 at 9:04 AM Roger Bivand <Roger.Bivand at nhh.no> wrote:
> 
> 
> Hi Alex,
> 
> GDAL 2.2 is supported in rgdal and in sf - has been for some time. If GDAL 
> >= 2.2:
> 
> https://r-forge.r-project.org/scm/viewvc.php/pkg/src/OGR_write.cpp?root=rgdal&r1=650&r2=660
> 
> and
> 
> https://r-forge.r-project.org/scm/viewvc.php/pkg/src/ogrsource.cpp?root=rgdal&r1=650&r2=660
> 
> This is general, not just GeoJSON, see
> 
> https://trac.osgeo.org/gdal/wiki/rfc67_nullfieldvalues
> 
> I don't see a good reason for making the user have to choose - could you 
> suggest one with a use case reprex?
> 
> See also the sf code, which is relevant here.
> 
> Roger
> 
> On Fri, 7 Jul 2017, Alex M wrote:
> 
> > Roger et al,
> >
> > I couldn't find a ticket system for rgdal so this seemed the best place
> > to post.
> >
> > We just hit a bug (in QGIS [1]) of sorts due to a change in behavior in
> > GDAL from 2.1.x to 2.2.x in the way null fields are handled. [2]
> >
> > Short story if you have a column of null values and you save a geojson
> > those fields are not saved. So the next time you open up the file, you
> > have less columns than before. It's great for minifying geojson but
> > terrible for many other use cases.
> >
> > I'm not 100% sure this will show up in rgdal (2.2 hasn't been pushed to
> > my Ubuntu yet so I haven't tested), but there's a good chance it will
> > depending on how R deals with unset vs null. If someone would like to
> > test, you can get a sample file from [3], readOGR and then writeOGR (to
> > geojson) and see if the null columns vanish.
> >
> > The enhancement requested to deal with this, is to provide the user an
> > option that allows them to decide when a field is unset vs null. Or
> > maybe just a clarification on how R represents that data and how a user
> > could set a field as null instead of unset and vice versa.
> >
> > Even R. pointed out that this can be controlled with the
> > OGR_F_IsFieldNull() part of the ogr api.
> >
> > [1] https://issues.qgis.org/issues/16812
> > [2] https://trac.osgeo.org/gdal/wiki/rfc67_nullfieldvalues
> > [3] https://github.com/UCDavisLibrary/ava/issues/270
> >
> > Thanks,
> > Alex
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
> 
> -- 
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From wolz1 at illinois.edu  Mon Jul 10 23:34:49 2017
From: wolz1 at illinois.edu (Kevin Wolz)
Date: Mon, 10 Jul 2017 16:34:49 -0500
Subject: [R-sig-Geo] Add county attribute to gSSURGO soils raster
Message-ID: <CAL0dvJ-LJL81tnWJb+OV_UfWr5wcPLSUk19xUJq3WuUfKQ9uMw@mail.gmail.com>

I have a high-resolution (10m cell size) raster of soils data for a US
state (gSSURGO). Pixel values are the map unit key (mukey). Every soil type
has a unique mukey, but a given mukey can occur in one or more counties
within a state.

I am trying to merge data into this raster attribute table, but the data
I'm trying to merge has unique values for each mukey in *each county* that
mukey occurs in. Consequently, I cannot make the merge until I first add a
county field into the raster attribute table.

My current approach to doing this is to use the "rasterize" function in the
raster R package on a polygon .shp file of the state's counties to the same
resolution as the gSSURGO data. Since the cell sizes in gSSURGO are so
small, however, this is taking in incredible amount of time. Even with 8
cores on a fast machine...

Is there another good way to do this?! If gSSURGO has county values hidden
in one of its many tables, I cannot find it. I have also thought about
cropping the gSSURGO raster to each county using the county .shp file,
directly adding that county name as an attribute, and then merging all of
the individual county rasters back together again. The problem with this
approach is that the cropping only goes so far...you also need to use the
"mask" function to make cells outside of the county NA, and this takes a
long time too.

Thanks!
Kevin
--
Kevin Wolz
PhD Candidate, DeLucia Lab
University of Illinois at Urbana-Champaign
Phone: (708) 476-9929

	[[alternative HTML version deleted]]


From Wade.A.Wall at erdc.dren.mil  Tue Jul 11 18:42:32 2017
From: Wade.A.Wall at erdc.dren.mil (Wall, Wade A ERDC-RDE-CERL-IL CIV)
Date: Tue, 11 Jul 2017 16:42:32 +0000
Subject: [R-sig-Geo] Failure during raster IO
Message-ID: <C1524BE4BF45454293B8DF38FB9B5ECAD7A7FF39@MS-EX1VKS.erdc.dren.mil>

Hi all,

I am trying to crop a raster brick, but am running into issues trying to do it on an HPC. 

I have a raster, env, with 60786 rows, 64100 columns, and 19 layers.

I am tyring to split it into N sub rasters based on row number, but I keep getting the error message "Failure during raster IO".

I am able to run the data with a smaller subset, but not with the full data set. The code runs fine on my desktop, but takes 5 days. 

I have tried to change the location of the temporary files, but nothing seems to work.

Here is the relevant code

#### Start prediction
sub = raster::crop(env,raster::extent(env,min(rows_sub),max(rows_sub),1,ncol(env))) ### This is where it fails
raster::predict(object=env, model = model, filename=paste(out.path,"/",basename(args[2]),".grd",sep=""),overwrite=TRUE)

Is this an issue on the HPC? Not sure how to diagnose. Thanks for any help

Wade


From Roger.Bivand at nhh.no  Tue Jul 11 19:15:54 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 11 Jul 2017 19:15:54 +0200
Subject: [R-sig-Geo] Geographically weighted regression
In-Reply-To: <E62DF41FFFBFFB40B1DFABB435A9BBD1D83AB5EA@XMTT10-DAG2.MOH.HEALTH.GOV.IL>
References: <E62DF41FFFBFFB40B1DFABB435A9BBD1D83AB5EA@XMTT10-DAG2.MOH.HEALTH.GOV.IL>
Message-ID: <alpine.LFD.2.20.1707111915410.23140@reclus.nhh.no>

On Thu, 6 Jul 2017, Zalman.Kaufman at MOH.HEALTH.GOV.IL wrote:

> Dear group members,
> Are you familiar with R packages doing geographically weighted regressions implementing negative binomial distribution?
>
> Could you, please, refer to the JSS article by Gollini et al. on GWmodel?

https://www.jstatsoft.org/article/view/v063i17

>
> Thanks a lot, Zalman
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From e0425926 at student.tuwien.ac.at  Fri Jul 14 15:46:08 2017
From: e0425926 at student.tuwien.ac.at (Kevin Stadler)
Date: Fri, 14 Jul 2017 15:46:08 +0200
Subject: [R-sig-Geo] Introducing: an R package for calling functions within
 a running instance of QGIS (ALPHA)
Message-ID: <CALufGKKcdFbAaSv==xfMG-JbBrCnsA1huS3Vh5rNeUN+EWoAcA@mail.gmail.com>

Hello,

as part of my Google Summer of Code project I am currently developing
a QGIS plugin that allows controlling a running QGIS instance through
a HTTP interface -- and an R package making use of this 'Network API'
along with it.

This package is quite different from the existing RQGIS in that it
connects to a running instance of QGIS, meaning that programmatic
control from within R + user interaction can be combined (the QGIS
instance being controlled does not even have to run on the same
machine).

Both components (QGIS plugin and R package) are still deep in alpha
stages, however for those interested there is now sufficient
documentation to give testing a shot, and I would welcome all feedback
and suggestions. This simple example tutorial vignette should give you
an idea of how the package works:
http://qgisapi.gitlab.io/rqgisapi/articles/tutorial.html

Instructions for installing the two components can be found here:
https://gitlab.com/qgisapi/networkapi#testing
and here:
http://qgisapi.gitlab.io/rqgisapi/#installation


I will be expanding the package as well developing example use cases
in the form of Rmarkdown and JuPyteR notebooks until the end of the
summer, so I'd be happy to receive requests for QGIS functionality
that you'd like to see exposed to R (access to processing functions in
particular is not implemented at the moment). If there is more
interest I'd be happy to keep posting regular updates about new
features to this list too.

Best!
Kevin


From paolo.piras at uniroma3.it  Mon Jul 17 10:32:02 2017
From: paolo.piras at uniroma3.it (Paolo Piras)
Date: Mon, 17 Jul 2017 08:32:02 +0000
Subject: [R-sig-Geo] Test for difference between two pint patterns
In-Reply-To: <CALufGKKcdFbAaSv==xfMG-JbBrCnsA1huS3Vh5rNeUN+EWoAcA@mail.gmail.com>
References: <CALufGKKcdFbAaSv==xfMG-JbBrCnsA1huS3Vh5rNeUN+EWoAcA@mail.gmail.com>
Message-ID: <VI1PR04MB3166975FFB09906661A59856B3A00@VI1PR04MB3166.eurprd04.prod.outlook.com>

Hi folks,

I'm new in point pattern analysis; thus my question could appear trivial for experts;

Given two ppp objects (spatstat package) [with different number of points] I'm looking for a way to assess the statistical difference in the distributions of points. studpermu.test() seems doing the job but for groups of point patterns while I have only two distributions.

Kest() could be used to look for deviation from Poisson distribution but I should compare the two patterns.

I think a hand-made permutation test where points are randomly re-assigned between the two distribution and K stat is computed at any run (and compared with the observed ones) could be ok but I'm not sure however.

Thanks in advance for any advice

Paolo

	[[alternative HTML version deleted]]


From marcelino.delacruz at urjc.es  Mon Jul 17 10:40:04 2017
From: marcelino.delacruz at urjc.es (Marcelino de la Cruz Rot)
Date: Mon, 17 Jul 2017 10:40:04 +0200
Subject: [R-sig-Geo] Test for difference between two pint patterns
In-Reply-To: <VI1PR04MB3166975FFB09906661A59856B3A00@VI1PR04MB3166.eurprd04.prod.outlook.com>
References: <CALufGKKcdFbAaSv==xfMG-JbBrCnsA1huS3Vh5rNeUN+EWoAcA@mail.gmail.com>
 <VI1PR04MB3166975FFB09906661A59856B3A00@VI1PR04MB3166.eurprd04.prod.outlook.com>
Message-ID: <84ce37b4-efec-dd0b-1d86-d8a1715d1c3f@urjc.es>

Hi Paolo, see the help page of Kcross(), pcfcross() or any other crossed 
function in spatstat (and also that of envelope() for the tests).

Best,

Marcelino

El 17/07/2017 a las 10:32, Paolo Piras escribi?:
> Hi folks,
>
> I'm new in point pattern analysis; thus my question could appear trivial for experts;
>
> Given two ppp objects (spatstat package) [with different number of points] I'm looking for a way to assess the statistical difference in the distributions of points. studpermu.test() seems doing the job but for groups of point patterns while I have only two distributions.
>
> Kest() could be used to look for deviation from Poisson distribution but I should compare the two patterns.
>
> I think a hand-made permutation test where points are randomly re-assigned between the two distribution and K stat is computed at any run (and compared with the observed ones) could be ok but I'm not sure however.
>
> Thanks in advance for any advice
>
> Paolo
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> .
>

-- 
Marcelino de la Cruz Rot
Depto. de Biolog?a y Geolog?a
F?sica y Qu?mica Inorg?nica
Universidad Rey Juan Carlos
M?stoles Espa?a


From lisa.menez at free.fr  Mon Jul 17 15:25:37 2017
From: lisa.menez at free.fr (Lisa Menez)
Date: Mon, 17 Jul 2017 15:25:37 +0200
Subject: [R-sig-Geo] [Local.moran.exact] Error in integrate : evaluation of
	function gave a result of wrong type
References: <214A69E2-7A20-4E37-A737-5F2934AF980B@free.fr>
Message-ID: <BA258D91-C791-42EE-AE90-8F38EA919FAA@free.fr>

Good morning to all 

I am currently trying to use local.moran.exact function from spdep package.
More precisely, I am willing to use the alternative version (local.moran.exact.alt) 
but I obtain an error message I can?t figure out :

"Error in integrate(integrand, lower = 0, upper = upper) : evaluation of function gave a result of wrong type"
 

#======================================
#My issue is somewhere is those lines : 
#======
setwd(".")
library(spdep)
EU_NUTS <- read.table("./mydata.txt")

###distance based neighbors - - - 
Coord <- cbind(EU_NUTS$"x", EU_NUTS$"y")
EU_NUTS_kn0 <- knn2nb(knearneigh(Coord, k = 1), row.names =EU_NUTS$NUTS_ID)
dist <- unlist(nbdists(EU_NUTS_kn0, Coord))
summary(dist)
max_k0 <- max(dist)
EU_NUTS_kd1 <- dnearneigh(Coord, d1 = 0, d2 = 1 * max_k0, row.names =EU_NUTS$NUTS_ID)
Kd1list<-nb2listw(EU_NUTS_kd1, glist=NULL, style="W", zero.policy=FALSE)

  ###OLS model
  OLS<-lm(EU_NUTS$"GrowthR" ~ log(EU_NUTS$"X2000"))
  
  ###SAR model
  ERROR<-errorsarlm(OLS,, Kd1list , na.omit, etype="emixed", method="eigen", quiet=NULL,tol.solve=1.0e-10, zero.policy= TRUE)
  lm.error <- lm(ERROR$tary ~ ERROR$tarX - 1)

  Omega <- invIrW(Kd1list, rho=ERROR$lambda)
  Omega <- tcrossprod(Omega)


localmoran.exact.alt(lm.error, nb=EU_NUTS_kd1, Omega=Omega, style = "S",  zero.policy = TRUE,  alternative = "greater", spChk = NULL,  resfun = residuals, 
				 save.Vi = FALSE, useTP=TRUE,  truncErr=1e-6,  zeroTreat=0.1)


#======
#The error message is get is  :  
#======

Error in integrate(integrand, lower = 0, upper = upper) : evaluation of function gave a result of wrong type

#=======================


Does someone have any idea of what I am doing wrong ?



The full dataset is available here : 
http://www.i3s.unice.fr/~menez/RCode/mydata.txt <http://www.i3s.unice.fr/~menez/RCode/mydata.txt>

The Rcode is here : http://www.i3s.unice.fr/~menez/RCode/moran.R <http://www.i3s.unice.fr/~menez/RCode/moran.R>



Thank you for your attention and your help.
Best Regards

Lisa





	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Mon Jul 17 17:04:39 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 17 Jul 2017 17:04:39 +0200
Subject: [R-sig-Geo] [Local.moran.exact] Error in integrate : evaluation
 of	function gave a result of wrong type
In-Reply-To: <BA258D91-C791-42EE-AE90-8F38EA919FAA@free.fr>
References: <214A69E2-7A20-4E37-A737-5F2934AF980B@free.fr>
 <BA258D91-C791-42EE-AE90-8F38EA919FAA@free.fr>
Message-ID: <alpine.LFD.2.20.1707171627450.9010@reclus.nhh.no>

On Mon, 17 Jul 2017, Lisa Menez wrote:

> Good morning to all 
>
> I am currently trying to use local.moran.exact function from spdep package.
> More precisely, I am willing to use the alternative version (local.moran.exact.alt) 
> but I obtain an error message I can?t figure out :
>
> "Error in integrate(integrand, lower = 0, upper = upper) : evaluation of function gave a result of wrong type"
> 
>
> #======================================
> #My issue is somewhere is those lines : 
> #======
> setwd(".")
> library(spdep)
> EU_NUTS <- read.table("./mydata.txt")
>
> ###distance based neighbors - - - 
> Coord <- cbind(EU_NUTS$"x", EU_NUTS$"y")
> EU_NUTS_kn0 <- knn2nb(knearneigh(Coord, k = 1), row.names =EU_NUTS$NUTS_ID)
> dist <- unlist(nbdists(EU_NUTS_kn0, Coord))
> summary(dist)
> max_k0 <- max(dist)
> EU_NUTS_kd1 <- dnearneigh(Coord, d1 = 0, d2 = 1 * max_k0, row.names =EU_NUTS$NUTS_ID)
> Kd1list<-nb2listw(EU_NUTS_kd1, glist=NULL, style="W", zero.policy=FALSE)
>
>  ###OLS model
>  OLS<-lm(EU_NUTS$"GrowthR" ~ log(EU_NUTS$"X2000"))
>
>  ###SAR model
>  ERROR<-errorsarlm(OLS,, Kd1list , na.omit, etype="emixed", method="eigen", quiet=NULL,tol.solve=1.0e-10, zero.policy= TRUE)
>  lm.error <- lm(ERROR$tary ~ ERROR$tarX - 1)
>
>  Omega <- invIrW(Kd1list, rho=ERROR$lambda)
>  Omega <- tcrossprod(Omega)
>
>
> localmoran.exact.alt(lm.error, nb=EU_NUTS_kd1, Omega=Omega, style = "S",  zero.policy = TRUE,  alternative = "greater", spChk = NULL,  resfun = residuals,
> 				 save.Vi = FALSE, useTP=TRUE,  truncErr=1e-6,  zeroTreat=0.1)
>
>
> #======
> #The error message is get is  : 
> #======
>
> Error in integrate(integrand, lower = 0, upper = upper) : evaluation of function gave a result of wrong type
>
> #=======================
>
>
> Does someone have any idea of what I am doing wrong ?
>

Thanks for providing a reprex.

First, the points you are using are in geographical coordinates, which you 
ignore when calculating distances, so the distance criterion and the 
first nearest neighbours are wrong.

Second, you include all NUTS regions, including Reunion and other very 
overseas territories, with the consequence that most regions are defined 
as each other's neighbours (there are ~ 50% non-zero weights). This is far 
from sparse! Please motivate your choice of regions - it is very hard to 
argue that Reunion influences its nearest neighbours (Crete, Cyprus??). 
With so many neighbours on average, the spatial weights are all small.

Third, you do not motivate using the error Durbin model. If you do not use 
it, localmoran.exact.alt() does complete. However, few of the exact 
p-values for alternative="greater" are "significant" (t0 with Omega=Omega, 
t1 with Omega=diag(276) - the identity matrix):

> table(findInterval(as.data.frame(t0)[,5], c(0, 0.001, 0.01, 0.1, 0.5, 
0.9, 0.99, 0.999, 1)))

   1   2   3   4   5   6   7
   1   6  34 108 104  21   2
> table(findInterval(as.data.frame(t1)[,5], c(0, 0.001, 0.01, 0.1, 0.5, 
0.9, 0.99, 0.999, 1)))

  1  2  3  4  5  6  7
  7 13 48 94 90 22  2

So using the alternative model taking account of global autocorrelation 
does have an effect (7 not 20 regions with unadjusted p < 0.01), but for 
an odd selection of units and weights).

Note that p-values must be adjusted for multiple comparisons, for example:

> table(findInterval(p.adjust(as.data.frame(t0)[,5], "fdr"), c(0, 0.001, 
0.01, 0.1, 0.5, 0.9, 0.99, 0.999, 1)))

   3   4   5   6   7
   1  13 133 125   4
>
> table(findInterval(p.adjust(as.data.frame(t1)[,5], "fdr"), c(0, 0.001, 
0.01, 0.1, 0.5, 0.9, 0.99, 0.999, 1)))

  1  2  3  4  5  6  7
  1  2 11 91 71 96  4


So unless we know that you actually wanted to do what you did, 1-3 above 
suggest that your choices need to be motivated.

Hope this clarifies,

Roger

>
>
> The full dataset is available here : 
> http://www.i3s.unice.fr/~menez/RCode/mydata.txt <http://www.i3s.unice.fr/~menez/RCode/mydata.txt>
>
> The Rcode is here : http://www.i3s.unice.fr/~menez/RCode/moran.R <http://www.i3s.unice.fr/~menez/RCode/moran.R>
>
>
>
> Thank you for your attention and your help.
> Best Regards
>
> Lisa
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From dncgst at gnewarchaeology.it  Tue Jul 18 16:30:06 2017
From: dncgst at gnewarchaeology.it (Domenico Giusti)
Date: Tue, 18 Jul 2017 16:30:06 +0200
Subject: [R-sig-Geo] Wavelet analysis of anisotropy in point patterns
Message-ID: <145affb5-61b6-b8d9-746c-0b560c871df8@gnewarchaeology.it>

Hi all,

I'm running wavelet analysis of anisotropy in point patterns using the 
PASSaGE software by Rosenberg et al.

Rosenberg, M.S., and C.D. Anderson (2011) PASSaGE: Pattern Analysis, 
Spatial Statistics and Geographic Exegesis. Version 2. Methods in 
Ecology & Evolution 2(3):229-232.
http://www.passagesoftware.net

Rosenberg, M.S. 2004. Wavelet analysis for detecting anisotropy in point 
patterns. Journal of Vegetation Science 15:277-284.
DOI: 10.1111/j.1654-1103.2004.tb02262.x

I wonder how I can do the same analysis using R.

Thanks,

Dome

-- 
Domenico Giusti <dncgst at gnewarchaeology.it>
GPG keyID: 2048R/A3AB7054F6E5D778


From marcelino.delacruz at urjc.es  Tue Jul 18 18:17:57 2017
From: marcelino.delacruz at urjc.es (Marcelino de la Cruz Rot)
Date: Tue, 18 Jul 2017 18:17:57 +0200
Subject: [R-sig-Geo] Wavelet analysis of anisotropy in point patterns
In-Reply-To: <145affb5-61b6-b8d9-746c-0b560c871df8@gnewarchaeology.it>
References: <145affb5-61b6-b8d9-746c-0b560c871df8@gnewarchaeology.it>
Message-ID: <4b812207-02dc-de47-08a7-f9b440fdfc7d@urjc.es>

Hi Domenico,

In spatstat you have several functions to detect anysotropy in point 
patterns:
- the sector K-function, Ksector();
- the pair orientation distribution, pairorient();
- the anisotropic pair correlation function, Kmeasure().

See the help pages of these functios or, even better, consult pages 
236:242 in  Adrian Baddeley, Ege Rubak, Rolf Turner (2015). Spatial 
Point  Patterns: Methodology and Applications with R. London: Chapman 
and   Hall/CRC Press, 2015.
http://www.crcpress.com/Spatial-Point-Patterns-Methodology-and-Applications-with-R/Baddeley-Rubak-Turner/9781482210200/

Cheers,

Marcelino

El 18/07/2017 a las 16:30, Domenico Giusti escribi?:
> Hi all,
>
> I'm running wavelet analysis of anisotropy in point patterns using the 
> PASSaGE software by Rosenberg et al.
>
> Rosenberg, M.S., and C.D. Anderson (2011) PASSaGE: Pattern Analysis, 
> Spatial Statistics and Geographic Exegesis. Version 2. Methods in 
> Ecology & Evolution 2(3):229-232.
> http://www.passagesoftware.net
>
> Rosenberg, M.S. 2004. Wavelet analysis for detecting anisotropy in 
> point patterns. Journal of Vegetation Science 15:277-284.
> DOI: 10.1111/j.1654-1103.2004.tb02262.x
>
> I wonder how I can do the same analysis using R.
>
> Thanks,
>
> Dome
>

-- 
Marcelino de la Cruz Rot
Depto. de Biolog?a y Geolog?a
F?sica y Qu?mica Inorg?nica
Universidad Rey Juan Carlos
M?stoles Espa?a


From dncgst at gnewarchaeology.it  Wed Jul 19 12:12:39 2017
From: dncgst at gnewarchaeology.it (Domenico Giusti)
Date: Wed, 19 Jul 2017 12:12:39 +0200
Subject: [R-sig-Geo] Wavelet analysis of anisotropy in point patterns
In-Reply-To: <4b812207-02dc-de47-08a7-f9b440fdfc7d@urjc.es>
References: <145affb5-61b6-b8d9-746c-0b560c871df8@gnewarchaeology.it>
 <4b812207-02dc-de47-08a7-f9b440fdfc7d@urjc.es>
Message-ID: <db109eb0-e0b0-dfd5-5e99-000f31289574@gnewarchaeology.it>

Thank you Marcelino,

I already had a look at those functions and, although they do the job, I 
was wondering how to analyze anisotropy in point pattern using wavelet 
analysis.

I tried to use the WaveletComp package, but with no success. It looks to 
me that packages for wavelet analysis work fine with temporal data, but 
I couldn't find the way to use it with spatial point patterns.

How could I get in R the same analysis provided by the PASSaGE software?

Thank you!

On 07/18/2017 06:17 PM, Marcelino de la Cruz Rot wrote:
> Hi Domenico,
>
> In spatstat you have several functions to detect anysotropy in point 
> patterns:
> - the sector K-function, Ksector();
> - the pair orientation distribution, pairorient();
> - the anisotropic pair correlation function, Kmeasure().
>
> See the help pages of these functios or, even better, consult pages 
> 236:242 in  Adrian Baddeley, Ege Rubak, Rolf Turner (2015). Spatial 
> Point  Patterns: Methodology and Applications with R. London: Chapman 
> and   Hall/CRC Press, 2015.
> http://www.crcpress.com/Spatial-Point-Patterns-Methodology-and-Applications-with-R/Baddeley-Rubak-Turner/9781482210200/ 
>
>
> Cheers,
>
> Marcelino
>
> El 18/07/2017 a las 16:30, Domenico Giusti escribi?:
>> Hi all,
>>
>> I'm running wavelet analysis of anisotropy in point patterns using 
>> the PASSaGE software by Rosenberg et al.
>>
>> Rosenberg, M.S., and C.D. Anderson (2011) PASSaGE: Pattern Analysis, 
>> Spatial Statistics and Geographic Exegesis. Version 2. Methods in 
>> Ecology & Evolution 2(3):229-232.
>> http://www.passagesoftware.net
>>
>> Rosenberg, M.S. 2004. Wavelet analysis for detecting anisotropy in 
>> point patterns. Journal of Vegetation Science 15:277-284.
>> DOI: 10.1111/j.1654-1103.2004.tb02262.x
>>
>> I wonder how I can do the same analysis using R.
>>
>> Thanks,
>>
>> Dome
>>
>


From marcelino.delacruz at urjc.es  Wed Jul 19 14:14:17 2017
From: marcelino.delacruz at urjc.es (Marcelino de la Cruz Rot)
Date: Wed, 19 Jul 2017 14:14:17 +0200
Subject: [R-sig-Geo] Wavelet analysis of anisotropy in point patterns
In-Reply-To: <db109eb0-e0b0-dfd5-5e99-000f31289574@gnewarchaeology.it>
References: <145affb5-61b6-b8d9-746c-0b560c871df8@gnewarchaeology.it>
 <4b812207-02dc-de47-08a7-f9b440fdfc7d@urjc.es>
 <db109eb0-e0b0-dfd5-5e99-000f31289574@gnewarchaeology.it>
Message-ID: <fb1b3d26-82de-b4a5-fabb-2d47a1d0e6f7@urjc.es>

Hi Domenico,

AFAIK, you should build your own functions to get the same analysis 
provided by the PASSaGE software. If you have some elementary R 
programming skills, following the description of the analysis proposed 
by Rosenberg(2004) shouldn't be much complicated. Otherwise...I don't know.

Good luck!

Marcelino


El 19/07/2017 a las 12:12, Domenico Giusti escribi?:
> Thank you Marcelino,
>
> I already had a look at those functions and, although they do the job, 
> I was wondering how to analyze anisotropy in point pattern using 
> wavelet analysis.
>
> I tried to use the WaveletComp package, but with no success. It looks 
> to me that packages for wavelet analysis work fine with temporal data, 
> but I couldn't find the way to use it with spatial point patterns.
>
> How could I get in R the same analysis provided by the PASSaGE software?
>
> Thank you!
>
> On 07/18/2017 06:17 PM, Marcelino de la Cruz Rot wrote:
>> Hi Domenico,
>>
>> In spatstat you have several functions to detect anysotropy in point 
>> patterns:
>> - the sector K-function, Ksector();
>> - the pair orientation distribution, pairorient();
>> - the anisotropic pair correlation function, Kmeasure().
>>
>> See the help pages of these functios or, even better, consult pages 
>> 236:242 in  Adrian Baddeley, Ege Rubak, Rolf Turner (2015). Spatial 
>> Point  Patterns: Methodology and Applications with R. London: Chapman 
>> and   Hall/CRC Press, 2015.
>> http://www.crcpress.com/Spatial-Point-Patterns-Methodology-and-Applications-with-R/Baddeley-Rubak-Turner/9781482210200/ 
>>
>>
>> Cheers,
>>
>> Marcelino
>>
>> El 18/07/2017 a las 16:30, Domenico Giusti escribi?:
>>> Hi all,
>>>
>>> I'm running wavelet analysis of anisotropy in point patterns using 
>>> the PASSaGE software by Rosenberg et al.
>>>
>>> Rosenberg, M.S., and C.D. Anderson (2011) PASSaGE: Pattern Analysis, 
>>> Spatial Statistics and Geographic Exegesis. Version 2. Methods in 
>>> Ecology & Evolution 2(3):229-232.
>>> http://www.passagesoftware.net
>>>
>>> Rosenberg, M.S. 2004. Wavelet analysis for detecting anisotropy in 
>>> point patterns. Journal of Vegetation Science 15:277-284.
>>> DOI: 10.1111/j.1654-1103.2004.tb02262.x
>>>
>>> I wonder how I can do the same analysis using R.
>>>
>>> Thanks,
>>>
>>> Dome
>>>
>>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


-- 
Marcelino de la Cruz Rot
Depto. de Biolog?a y Geolog?a
F?sica y Qu?mica Inorg?nica
Universidad Rey Juan Carlos
M?stoles Espa?a


From mauricio.zambrano at ufrontera.cl  Wed Jul 19 18:12:44 2017
From: mauricio.zambrano at ufrontera.cl (Mauricio Zambrano Bigiarini)
Date: Wed, 19 Jul 2017 12:12:44 -0400
Subject: [R-sig-Geo] Adding text to rasterVis levelplot
Message-ID: <CAP6Varv6DTFYRPkPv8We-_r+BHnQd+RvtQzsK6sKT1-kd0q-Yw@mail.gmail.com>

Dear all,

I would like to add custom text labels for the columns and rows of a
levelplot object created with he rasterVis package, similar to the
c("Fall", "Winter", "Spring", Summer") used to label the rows and the
c("a",..., "h") used to label the columns in the plot shown int he
following link:

https://stackoverflow.com/questions/43007241/how-to-add-text-to-a-specific-fixed-location-in-rastervis-levelplot

In particular, how can I add c("col1", "col2") and c("row1", "row2")
as column and row headers, respectively, to the following example
plot:

--------- START--------
library(rasterVis)
f <- system.file("external/test.grd", package="raster")
r <- raster(f)
s <- stack(r, r+500, r-500, r+200)
levelplot(s, layout=c(2,2), names.att=rep("",4))
--------- END--------


Thanks in advance for your help,


Mauricio Zambrano-Bigiarini, PhD

=====================================
Department of Civil Engineering
Faculty of Engineering and Sciences
Universidad de La Frontera, Temuco, Chile
=====================================
"The ultimate inspiration is the deadline"
(Nolan Bushnell)
=====================================
Linux user #454569 -- Linux Mint user

-- 
La informaci?n contenida en este correo electr?nico y cualquier anexo o 
respuesta relacionada, puede contener datos e informaci?n confidencial y no 
puede ser usada o difundida por personas distintas a su(s) destinatario(s). 
Si usted no es el destinatario de esta comunicaci?n, le informamos que 
cualquier divulgaci?n, distribuci?n o copia de esta informaci?n constituye 
un delito conforme a la ley chilena. Si lo ha recibido por error, por favor 
borre el mensaje y todos sus anexos y notifique al remitente.


From javiermoreira at gmail.com  Wed Jul 19 22:23:47 2017
From: javiermoreira at gmail.com (Javier Moreira)
Date: Wed, 19 Jul 2017 17:23:47 -0300
Subject: [R-sig-Geo] spatial structure of residuals in nlme mixed model
Message-ID: <CAEyHP-1ULBK3npT1WNd+T+oCksDKueiiq9t8zMkF61WNZ7zHpg@mail.gmail.com>

hi,
im trying to asses about the diference in puting the spatial estructure in
a mixed linear model.
What i need to do is to compare the actual distribution of the residuals of
two models, one that consider the correlation and one that not.

model with and without:

*without*

modelo3_MM<-lme(REND_SE~1+TRATAMIENTO*AMBIENTE,

 random=list(BLOQUE=pdIdent(~1),AMBIENTE=pdIdent(~1),TRATAMIENTO=pdIdent(~1)),
                 data=data1,
                 control=lmeControl(niterEM=150,msMaxIter=200))
modelo33_MM<-update(modelo3_MM,
                    weights=varComb(varIdent(form=~1|TRATAMIENTO)))

*with*
modelo34_MM<-update(modelo33_MM,
                    correlation=corExp(form=~1|BLOQUE/AMBIENTE/TRATAMIENTO))

until now, what i could do is to cheq the existance of that structure by
computing the semivariogram to the residuals.

plot(Variogram(modelo33_MM))

plot((modelo34_MM,resType="n",robust=T))

the question is how can i do the next step and krige this residuals in
order to get a map of predicted values and variance values?
im used to do it wity gstat package but i cant figured out with an object
of class "lme"

thanks

-- 
Javier Moreira de Souza
Ingeniero Agr?nomo
099 406 006

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Thu Jul 20 02:47:06 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Thu, 20 Jul 2017 00:47:06 +0000
Subject: [R-sig-Geo] raster - unrotate?
In-Reply-To: <EEC29A11-E125-4658-A321-F4343A28BF0E@bigelow.org>
References: <6BD652E0-73C1-46CD-BC60-E5FFFCDBF1DB@bigelow.org>
 <CAAcGz9-sWzxyO6DDvdgA3XWz0eEEGVpRHH7vVn3pUWxW4jGhfg@mail.gmail.com>
 <A62EB8E3-74B9-4F2D-879E-8D5B87813372@bigelow.org>
 <CAAcGz9_kW5TdaFZHBL=dVdRz4e=diCO0TSfbB-wLh1JVE6PS2g@mail.gmail.com>
 <EEC29A11-E125-4658-A321-F4343A28BF0E@bigelow.org>
Message-ID: <CAAcGz9-QwSit3eaa1ntkqCFwN-VYAqSofH5+Cn8F47Q7U_+9ow@mail.gmail.com>

That's great, thanks for the update! I didn't actually realize all that
oceandata stuff was on Thredds. You might try ROpenSci's rerddap package
too.

I will try that source with the in-dev https://Github.com/hypertidy/tidync
. Please check it out if you can, and are feeling brave!

(tidync aims to provide more general outputs than raster can, more easily
than the API wrappers.)

Cheers, Mike

On Thu, 20 Jul 2017, 10:39 Ben Tupper, <btupper at bigelow.org> wrote:

> Ahoy!
>
> My (current) solution is to user raster::merge().  Works a treat and fits
> my workflow nicely.  Since I am obtaining the raster data using OPeNDAP it
> is easy to get the two regions, rotate the extent of the eastern region,
> and then merge.  Below is a pseudo-example (that works!) as it crops a
> local file twice, once for each region, rather than using an OPeNDaP calls
> via `ncdf4::nccvar_get(start = blah, count = something)` to get the two
> regions.
>
>
> ### start
>
> library(raster)
> uri <- '
> https://oceandata.sci.gsfc.nasa.gov:443/opendap/MODISA/L3SMI/2016/001/A20160012016032.L3m_R32_SST_sst_9km.nc
> '
> R <- raster::raster(uri, varname = 'sst')
>
> R
> #class       : RasterLayer
> #dimensions  : 180, 360, 64800  (nrow, ncol, ncell)
> #resolution  : 1, 1  (x, y)
> #extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> #coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> #data source : in memory
> #names       : layer
> #values      : 1.482572e-05, 0.9999928  (min, max)
>
> par(mfrow = c(2,1))
> plot(R)
> lines(c(180, 100, 100, 180), c(80,80,0,0))
> lines(c(-180,-90,-90,-180), c(80,80,0,0))
>
> # eastern section
> eR <- raster::crop(R, c(100, 180, 0, 80))
>
> # rotate the extent
> eExtent <- raster::extent(eR)
> raster::extent(eR) <- c(-260, -180, eExtent[3:4])
>
> # western section
> wR <- raster::crop(R, c(-180, -90, 0, 80))
>
> # merge
> newR <- raster::merge(eR, wR)
> newR
>
> plot(newR)
>
> #### end
>
> Cheers,
> Ben
>
> > On Jun 22, 2017, at 7:26 PM, Michael Sumner <mdsumner at gmail.com> wrote:
> >
> >
> >
> >
> > On Thu, 22 Jun 2017 at 21:28 Ben Tupper <btupper at bigelow.org> wrote:
> > Hi,
> >
> > Wow!  A treasure from the past!
> >
> > Hmmm.   I see what you mean; it might be best to snip-clip-and-smush
> from the native presentation.  We are testing out the performance of the
> dineof function in the sinkr package (
> https://github.com/marchtaylor/sinkr/blob/master/R/dineof.R) to
> interpolate patchy chlorophyl data.
> >
> > Thanks for the tip!
> >
> >
> >
> > For what it's worth, I'm happy to help, there's no one size fits all
> here. The dateline is one of those "easy problems" that does not have a
> general fix and will bite in many different contexts. :)
> >
> > The best distillation of my tricks is in this project, but it does
> depend on your workflow and the actual data in use.
> >
> > https://github.com/hypertidy/tabularaster
> >
> > If you have performance issues raster's cell abstraction  will help, but
> they are a bit old-school when it comes to multi-part geometries. (It's
> identical to standard database indexing concepts as are all "spatial"
> optimization tricks)
> >
> > (I see a desperate need for a general API for this kind of problem so
> that we can build tools from a general framework, which I'm working on -
> that's some kind of excuse for why this and related projects are quite raw
> and unfinished.)
> >
> > Cheers, Mike.
> >
> > Cheers,
> > Ben
> >
> >> On Jun 22, 2017, at 4:46 AM, Michael Sumner <mdsumner at gmail.com> wrote:
> >>
> >>
> >> It used to do the inverse, and I preserved a copy of the old behaviour
> here:
> >>
> >>
> https://github.com/AustralianAntarcticDivision/raadtools/blob/master/R/utils.R#L9
> >>
> >> You're probably best to learn from how it approaches it rather than
> just use it, since it's not a general capability, just works for those very
> specific cases.
> >>
> >> I just tested it and it still seems to work fine, I used
> >>
> >>
> oceandata.sci.gsfc.nasa.gov/MODISA/Mapped/Monthly/9km/chlor/A20021822002212.L3m_MO_CHL_chlor_a_9km.nc
> >>
> >> obtainable with
> >>
> >>
> https://oceandata.sci.gsfc.nasa.gov/cgi/getfile/A20021822002212.L3m_MO_CHL_chlor_a_9km.nc
> >>
> >> If you're extracting points you might as well just convert them into
> the "Atlantic" range, but it's painful to get that right for polygons or
> lines (and very hard to generalize once you get it working anyway).
> >>
> >> For simple regions like the one you show I'd probably split it into two
> extents() and use those - but depends on your workflow, all these options
> are useful here and there.
> >>
> >> Cheers, Mike.
> >>
> >>
> >> On Thu, 22 Jun 2017 at 18:30 Ben Tupper <btupper at bigelow.org> wrote:
> >> Hello,
> >>
> >> We have rasters that span [-180, 180] from NASA's Ocean Color (
> https://oceancolor.gsfc.nasa.gov/)  datasets.  We are trying to extract a
> region that spans 100E to 90W, that is 100 to -90.  The region 'wraps'
> across the edges as shown by the plot at the address below.
> >>
> >> https://dl.dropboxusercontent.com/u/8433654/sst.png
> >>
> >>
> >> library(raster)
> >> uri <- '
> https://oceandata.sci.gsfc.nasa.gov:443/opendap/MODISA/L3SMI/2016/001/A20160012016032.L3m_R32_SST_sst_9km.nc
> '
> >> R <- raster::raster(uri, varname = 'sst')
> >>
> >> R
> >> #class       : RasterLayer
> >> #dimensions  : 180, 360, 64800  (nrow, ncol, ncell)
> >> #resolution  : 1, 1  (x, y)
> >> #extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> >> #coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> >> #data source : in memory
> >> #names       : layer
> >> #values      : 1.482572e-05, 0.9999928  (min, max)
> >>
> >> plot(R)
> >> lines(c(180, 100, 100, 180), c(80,80,0,0))
> >> lines(c(-180,-90,-90,-180), c(80,80,0,0))
> >>
> >> I see that there is the nice raster::rotate() function to rotate raster
> coordinates from [0,360] to [-180,180]. That would make extracting the
> region super easy if the inverse were available.  Is there an equivalent
> way to implement the inverse or raster::rotate()?  I could dig into the
> source of raster::rotate() to see how to code one up, but I hate like heck
> to reinvent the wheel.
> >>
> >> Thanks!
> >> Ben
> >>
> >>
> >> Ben Tupper
> >> Bigelow Laboratory for Ocean Sciences
> >> 60 Bigelow Drive, P.O. Box 380
> >> East Boothbay, Maine 04544
> >> http://www.bigelow.org
> >>
> >> _______________________________________________
> >> R-sig-Geo mailing list
> >> R-sig-Geo at r-project.org
> >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >> --
> >> Dr. Michael Sumner
> >> Software and Database Engineer
> >> Australian Antarctic Division
> >> 203 Channel Highway
> >> Kingston Tasmania 7050 Australia
> >>
> >
> > Ben Tupper
> > Bigelow Laboratory for Ocean Sciences
> > 60 Bigelow Drive, P.O. Box 380
> > East Boothbay, Maine 04544
> > http://www.bigelow.org
> >
> >
> >
> > --
> > Dr. Michael Sumner
> > Software and Database Engineer
> > Australian Antarctic Division
> > 203 Channel Highway
> > Kingston Tasmania 7050 Australia
>
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
>
> Ecocast Reports: http://seascapemodeling.org/ecocast.html
> Tick Reports: https://report.bigelow.org/tick/
> Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/
>
>
>
> --
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From thi_veloso at yahoo.com.br  Thu Jul 20 05:07:48 2017
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Thu, 20 Jul 2017 03:07:48 +0000 (UTC)
Subject: [R-sig-Geo] Adding text to rasterVis levelplot
In-Reply-To: <CAP6Varv6DTFYRPkPv8We-_r+BHnQd+RvtQzsK6sKT1-kd0q-Yw@mail.gmail.com>
References: <CAP6Varv6DTFYRPkPv8We-_r+BHnQd+RvtQzsK6sKT1-kd0q-Yw@mail.gmail.com>
Message-ID: <958982085.926218.1500520068103@mail.yahoo.com>

You can manually define the names that you want and pass them as arguments to 'names.attr' (the name of each panel) and 'ylab' (the title of the y axis).
Following your example:
col.titles = c('col1','col2','','')
row.titles = c('row1','row2')

levelplot(s, layout=c(2,2),
??????????names.attr=col.titles,
??????????ylab=row.titles)

HTH,
Greetings,?-- Thiago V. dos Santos
PhD studentLand and Atmospheric ScienceUniversity of Minnesota


On Wednesday, July 19, 2017, 11:13:31 AM CDT, Mauricio Zambrano Bigiarini <mauricio.zambrano at ufrontera.cl> wrote:

Dear all,

I would like to add custom text labels for the columns and rows of a
levelplot object created with he rasterVis package, similar to the
c("Fall", "Winter", "Spring", Summer") used to label the rows and the
c("a",..., "h") used to label the columns in the plot shown int he
following link:

https://stackoverflow.com/questions/43007241/how-to-add-text-to-a-specific-fixed-location-in-rastervis-levelplot

In particular, how can I add c("col1", "col2") and c("row1", "row2")
as column and row headers, respectively, to the following example
plot:

--------- START--------
library(rasterVis)
f <- system.file("external/test.grd", package="raster")
r <- raster(f)
s <- stack(r, r+500, r-500, r+200)
levelplot(s, layout=c(2,2), names.att=rep("",4))
--------- END--------


Thanks in advance for your help,


Mauricio Zambrano-Bigiarini, PhD

=====================================
Department of Civil Engineering
Faculty of Engineering and Sciences
Universidad de La Frontera, Temuco, Chile
=====================================
"The ultimate inspiration is the deadline"
(Nolan Bushnell)
=====================================
Linux user #454569 -- Linux Mint user

-- 
La informaci?n contenida en este correo electr?nico y cualquier anexo o 
respuesta relacionada, puede contener datos e informaci?n confidencial y no 
puede ser usada o difundida por personas distintas a su(s) destinatario(s). 
Si usted no es el destinatario de esta comunicaci?n, le informamos que 
cualquier divulgaci?n, distribuci?n o copia de esta informaci?n constituye 
un delito conforme a la ley chilena. Si lo ha recibido por error, por favor 
borre el mensaje y todos sus anexos y notifique al remitente.

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo
	[[alternative HTML version deleted]]


From btupper at bigelow.org  Thu Jul 20 02:39:25 2017
From: btupper at bigelow.org (Ben Tupper)
Date: Wed, 19 Jul 2017 20:39:25 -0400
Subject: [R-sig-Geo] raster - unrotate?
In-Reply-To: <CAAcGz9_kW5TdaFZHBL=dVdRz4e=diCO0TSfbB-wLh1JVE6PS2g@mail.gmail.com>
References: <6BD652E0-73C1-46CD-BC60-E5FFFCDBF1DB@bigelow.org>
 <CAAcGz9-sWzxyO6DDvdgA3XWz0eEEGVpRHH7vVn3pUWxW4jGhfg@mail.gmail.com>
 <A62EB8E3-74B9-4F2D-879E-8D5B87813372@bigelow.org>
 <CAAcGz9_kW5TdaFZHBL=dVdRz4e=diCO0TSfbB-wLh1JVE6PS2g@mail.gmail.com>
Message-ID: <EEC29A11-E125-4658-A321-F4343A28BF0E@bigelow.org>

Ahoy!

My (current) solution is to user raster::merge().  Works a treat and fits my workflow nicely.  Since I am obtaining the raster data using OPeNDAP it is easy to get the two regions, rotate the extent of the eastern region, and then merge.  Below is a pseudo-example (that works!) as it crops a local file twice, once for each region, rather than using an OPeNDaP calls via `ncdf4::nccvar_get(start = blah, count = something)` to get the two regions.


### start

library(raster)
uri <- 'https://oceandata.sci.gsfc.nasa.gov:443/opendap/MODISA/L3SMI/2016/001/A20160012016032.L3m_R32_SST_sst_9km.nc'
R <- raster::raster(uri, varname = 'sst')

R 
#class       : RasterLayer 
#dimensions  : 180, 360, 64800  (nrow, ncol, ncell)
#resolution  : 1, 1  (x, y)
#extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
#coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 
#data source : in memory
#names       : layer 
#values      : 1.482572e-05, 0.9999928  (min, max)

par(mfrow = c(2,1))
plot(R)
lines(c(180, 100, 100, 180), c(80,80,0,0))
lines(c(-180,-90,-90,-180), c(80,80,0,0))

# eastern section
eR <- raster::crop(R, c(100, 180, 0, 80))

# rotate the extent
eExtent <- raster::extent(eR)
raster::extent(eR) <- c(-260, -180, eExtent[3:4])

# western section
wR <- raster::crop(R, c(-180, -90, 0, 80))

# merge
newR <- raster::merge(eR, wR)
newR

plot(newR)

#### end

Cheers,
Ben

> On Jun 22, 2017, at 7:26 PM, Michael Sumner <mdsumner at gmail.com> wrote:
> 
> 
> 
> 
> On Thu, 22 Jun 2017 at 21:28 Ben Tupper <btupper at bigelow.org> wrote:
> Hi,
> 
> Wow!  A treasure from the past!
> 
> Hmmm.   I see what you mean; it might be best to snip-clip-and-smush from the native presentation.  We are testing out the performance of the dineof function in the sinkr package (https://github.com/marchtaylor/sinkr/blob/master/R/dineof.R) to interpolate patchy chlorophyl data.  
> 
> Thanks for the tip!
> 
> 
> 
> For what it's worth, I'm happy to help, there's no one size fits all here. The dateline is one of those "easy problems" that does not have a general fix and will bite in many different contexts. :)
> 
> The best distillation of my tricks is in this project, but it does depend on your workflow and the actual data in use. 
> 
> https://github.com/hypertidy/tabularaster
> 
> If you have performance issues raster's cell abstraction  will help, but they are a bit old-school when it comes to multi-part geometries. (It's identical to standard database indexing concepts as are all "spatial" optimization tricks)
> 
> (I see a desperate need for a general API for this kind of problem so that we can build tools from a general framework, which I'm working on - that's some kind of excuse for why this and related projects are quite raw and unfinished.)
> 
> Cheers, Mike.  
> 
> Cheers,
> Ben
> 
>> On Jun 22, 2017, at 4:46 AM, Michael Sumner <mdsumner at gmail.com> wrote:
>> 
>> 
>> It used to do the inverse, and I preserved a copy of the old behaviour here: 
>> 
>> https://github.com/AustralianAntarcticDivision/raadtools/blob/master/R/utils.R#L9
>> 
>> You're probably best to learn from how it approaches it rather than just use it, since it's not a general capability, just works for those very specific cases. 
>> 
>> I just tested it and it still seems to work fine, I used 
>> 
>> oceandata.sci.gsfc.nasa.gov/MODISA/Mapped/Monthly/9km/chlor/A20021822002212.L3m_MO_CHL_chlor_a_9km.nc 
>> 
>> obtainable with 
>> 
>> https://oceandata.sci.gsfc.nasa.gov/cgi/getfile/A20021822002212.L3m_MO_CHL_chlor_a_9km.nc 
>> 
>> If you're extracting points you might as well just convert them into the "Atlantic" range, but it's painful to get that right for polygons or lines (and very hard to generalize once you get it working anyway). 
>> 
>> For simple regions like the one you show I'd probably split it into two extents() and use those - but depends on your workflow, all these options are useful here and there. 
>> 
>> Cheers, Mike. 
>> 
>> 
>> On Thu, 22 Jun 2017 at 18:30 Ben Tupper <btupper at bigelow.org> wrote:
>> Hello,
>> 
>> We have rasters that span [-180, 180] from NASA's Ocean Color (https://oceancolor.gsfc.nasa.gov/)  datasets.  We are trying to extract a region that spans 100E to 90W, that is 100 to -90.  The region 'wraps' across the edges as shown by the plot at the address below.
>> 
>> https://dl.dropboxusercontent.com/u/8433654/sst.png
>> 
>> 
>> library(raster)
>> uri <- 'https://oceandata.sci.gsfc.nasa.gov:443/opendap/MODISA/L3SMI/2016/001/A20160012016032.L3m_R32_SST_sst_9km.nc'
>> R <- raster::raster(uri, varname = 'sst')
>> 
>> R
>> #class       : RasterLayer
>> #dimensions  : 180, 360, 64800  (nrow, ncol, ncell)
>> #resolution  : 1, 1  (x, y)
>> #extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>> #coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>> #data source : in memory
>> #names       : layer
>> #values      : 1.482572e-05, 0.9999928  (min, max)
>> 
>> plot(R)
>> lines(c(180, 100, 100, 180), c(80,80,0,0))
>> lines(c(-180,-90,-90,-180), c(80,80,0,0))
>> 
>> I see that there is the nice raster::rotate() function to rotate raster coordinates from [0,360] to [-180,180]. That would make extracting the region super easy if the inverse were available.  Is there an equivalent way to implement the inverse or raster::rotate()?  I could dig into the source of raster::rotate() to see how to code one up, but I hate like heck to reinvent the wheel.
>> 
>> Thanks!
>> Ben
>> 
>> 
>> Ben Tupper
>> Bigelow Laboratory for Ocean Sciences
>> 60 Bigelow Drive, P.O. Box 380
>> East Boothbay, Maine 04544
>> http://www.bigelow.org
>> 
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> -- 
>> Dr. Michael Sumner
>> Software and Database Engineer
>> Australian Antarctic Division
>> 203 Channel Highway
>> Kingston Tasmania 7050 Australia
>> 
> 
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
> 
> 
> 
> -- 
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecocast Reports: http://seascapemodeling.org/ecocast.html
Tick Reports: https://report.bigelow.org/tick/
Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/


From tristan.bourgeois at gmail.com  Thu Jul 20 15:02:57 2017
From: tristan.bourgeois at gmail.com (Tristan Bourgeois)
Date: Thu, 20 Jul 2017 15:02:57 +0200
Subject: [R-sig-Geo] RQGIS : looking for a geoalgorithm
Message-ID: <CACNjsfXnFyx9+DOU0QuGPQfC1JjO_Sg_3FE8qaMZeQkW+0E23g@mail.gmail.com>

Dear all,

Currently working on a new project I would like to create a dynamic report
with R (using probably XML package for writting)

In this report I'll plot some graphs (I'm ok with ggplots2) but also map.


I just discovered the RQGIS package and I think I understood its functions.
My question is the following one :

Is there any geoalgorithms which allows to create an atlas ?

I searched via the "find_algorithms()" but found nothing.

Thanks in advance !

Tristan.

-- 
Tristan Bourgeois

	[[alternative HTML version deleted]]


From florian.detsch at staff.uni-marburg.de  Fri Jul 21 10:05:08 2017
From: florian.detsch at staff.uni-marburg.de (Florian Detsch)
Date: Fri, 21 Jul 2017 10:05:08 +0200
Subject: [R-sig-Geo] Adding text to rasterVis levelplot
In-Reply-To: <958982085.926218.1500520068103@mail.yahoo.com>
References: <CAP6Varv6DTFYRPkPv8We-_r+BHnQd+RvtQzsK6sKT1-kd0q-Yw@mail.gmail.com>
 <958982085.926218.1500520068103@mail.yahoo.com>
Message-ID: <8d8c5e22-5cbb-6122-ecbc-c51466c72bbd@staff.uni-marburg.de>

Hi Mauricio,

Complementing Thiago's answer using 'names.att', here is a more manual 
approach which might come in handy when a finer control over text 
annotations is required. It is built upon lattice::trellis.focus() which 
lets you select relevant panels of your trellis graph in an iterative 
manner.

---
p <- levelplot(s, layout = c(2, 2), names.att = rep("", 4),
                scales = list(y = list(rot = 90)))

library(grid)

png("~/rasterVis.png", width = 14, height = 16, units = "cm", res = 300L)
grid.newpage()
print(p, newpage = FALSE)

## loop over panels to be labelled (ie 1:3)
panels <- trellis.currentLayout()
for (i in 1:3) {

   # focus on current panel of interest and disable clipping
   ids <- which(panels == i, arr.ind = TRUE)
   trellis.focus("panel", ids[2], ids[1], clip.off = TRUE)

   # add labels
   if (i %in% c(1, 3)) {
     if (i == 1) {
       grid.text(cls[1], x = .5, y = 1.1) # add 'col1'
       grid.text(rws[1], x = -.35, y = .5, rot = 90) # add 'row1'
     } else {
       grid.text(rws[2], x = -.35, y = .5, rot = 90) # add 'row2'
     }
   } else {
     grid.text(cls[2], x = .5, y = 1.1) # add 'col2'
   }

   trellis.unfocus()
}

dev.off()
---

A very similar use case, from which the above approach definitely took 
some inspiration, can be found here: 
https://stackoverflow.com/questions/7649737/is-it-possible-to-update-a-lattice-panel-in-r. 
Have a look at ?grid.text() and ?gpar() to learn more about 
customization options.

Cheers,
Florian

-- 
Dr. Florian Detsch
Environmental Informatics
Department of Geography
Philipps-Universit?t Marburg
Deutschhausstra?e 12
35032 (parcel post: 35037) Marburg, Germany

Phone: +49 (0) 6421 28-25323
Web: http://www.uni-marburg.de/fb19/fachgebiete/umweltinformatik/detschf/index.html


On 20.07.2017 05:07, Thiago V. dos Santos via R-sig-Geo wrote:
> You can manually define the names that you want and pass them as arguments to 'names.attr' (the name of each panel) and 'ylab' (the title of the y axis).
> Following your example:
> col.titles = c('col1','col2','','')
> row.titles = c('row1','row2')
>
> levelplot(s, layout=c(2,2),
>            names.attr=col.titles,
>            ylab=row.titles)
>
> HTH,
> Greetings, -- Thiago V. dos Santos
> PhD studentLand and Atmospheric ScienceUniversity of Minnesota
>
>
> On Wednesday, July 19, 2017, 11:13:31 AM CDT, Mauricio Zambrano Bigiarini <mauricio.zambrano at ufrontera.cl> wrote:
>
> Dear all,
>
> I would like to add custom text labels for the columns and rows of a
> levelplot object created with he rasterVis package, similar to the
> c("Fall", "Winter", "Spring", Summer") used to label the rows and the
> c("a",..., "h") used to label the columns in the plot shown int he
> following link:
>
> https://stackoverflow.com/questions/43007241/how-to-add-text-to-a-specific-fixed-location-in-rastervis-levelplot
>
> In particular, how can I add c("col1", "col2") and c("row1", "row2")
> as column and row headers, respectively, to the following example
> plot:
>
> --------- START--------
> library(rasterVis)
> f <- system.file("external/test.grd", package="raster")
> r <- raster(f)
> s <- stack(r, r+500, r-500, r+200)
> levelplot(s, layout=c(2,2), names.att=rep("",4))
> --------- END--------
>
>
> Thanks in advance for your help,
>
>
> Mauricio Zambrano-Bigiarini, PhD
>
> =====================================
> Department of Civil Engineering
> Faculty of Engineering and Sciences
> Universidad de La Frontera, Temuco, Chile
> =====================================
> "The ultimate inspiration is the deadline"
> (Nolan Bushnell)
> =====================================
> Linux user #454569 -- Linux Mint user
>


From florian.detsch at staff.uni-marburg.de  Fri Jul 21 10:21:11 2017
From: florian.detsch at staff.uni-marburg.de (Florian Detsch)
Date: Fri, 21 Jul 2017 10:21:11 +0200
Subject: [R-sig-Geo] Fwd: Re:  Adding text to rasterVis levelplot
In-Reply-To: <8d8c5e22-5cbb-6122-ecbc-c51466c72bbd@staff.uni-marburg.de>
References: <8d8c5e22-5cbb-6122-ecbc-c51466c72bbd@staff.uni-marburg.de>
Message-ID: <829819bb-07f2-4c7e-f6b7-c4e1ae26d0cb@staff.uni-marburg.de>

Ah, I forgot to include the definition of text vectors which must be 
inserted before the for() loop - sorry for that.

---
## labels
cls <- c("col1", "col2")
rws <- c("row1", "row2")
---


-------- Forwarded Message --------
Subject: 	Re: [R-sig-Geo] Adding text to rasterVis levelplot
Date: 	Fri, 21 Jul 2017 10:05:08 +0200
From: 	Florian Detsch <florian.detsch at staff.Uni-Marburg.DE>
To: 	r-sig-geo at r-project.org



Hi Mauricio,

Complementing Thiago's answer using 'names.att', here is a more manual
approach which might come in handy when a finer control over text
annotations is required. It is built upon lattice::trellis.focus() which
lets you select relevant panels of your trellis graph in an iterative
manner.

---
p <- levelplot(s, layout = c(2, 2), names.att = rep("", 4),
                scales = list(y = list(rot = 90)))

library(grid)

png("~/rasterVis.png", width = 14, height = 16, units = "cm", res = 300L)
grid.newpage()
print(p, newpage = FALSE)

## loop over panels to be labelled (ie 1:3)
panels <- trellis.currentLayout()
for (i in 1:3) {

   # focus on current panel of interest and disable clipping
   ids <- which(panels == i, arr.ind = TRUE)
   trellis.focus("panel", ids[2], ids[1], clip.off = TRUE)

   # add labels
   if (i %in% c(1, 3)) {
     if (i == 1) {
       grid.text(cls[1], x = .5, y = 1.1) # add 'col1'
       grid.text(rws[1], x = -.35, y = .5, rot = 90) # add 'row1'
     } else {
       grid.text(rws[2], x = -.35, y = .5, rot = 90) # add 'row2'
     }
   } else {
     grid.text(cls[2], x = .5, y = 1.1) # add 'col2'
   }

   trellis.unfocus()
}

dev.off()
---

A very similar use case, from which the above approach definitely took
some inspiration, can be found here:
https://stackoverflow.com/questions/7649737/is-it-possible-to-update-a-lattice-panel-in-r.
Have a look at ?grid.text() and ?gpar() to learn more about
customization options.

Cheers,
Florian

-- 
Dr. Florian Detsch
Environmental Informatics
Department of Geography
Philipps-Universit?t Marburg
Deutschhausstra?e 12
35032 (parcel post: 35037) Marburg, Germany

Phone: +49 (0) 6421 28-25323
Web: http://www.uni-marburg.de/fb19/fachgebiete/umweltinformatik/detschf/index.html


On 20.07.2017 05:07, Thiago V. dos Santos via R-sig-Geo wrote:
> You can manually define the names that you want and pass them as arguments to 'names.attr' (the name of each panel) and 'ylab' (the title of the y axis).
> Following your example:
> col.titles = c('col1','col2','','')
> row.titles = c('row1','row2')
>
> levelplot(s, layout=c(2,2),
>            names.attr=col.titles,
>            ylab=row.titles)
>
> HTH,
> Greetings, -- Thiago V. dos Santos
> PhD studentLand and Atmospheric ScienceUniversity of Minnesota
>
>
> On Wednesday, July 19, 2017, 11:13:31 AM CDT, Mauricio Zambrano Bigiarini <mauricio.zambrano at ufrontera.cl> wrote:
>
> Dear all,
>
> I would like to add custom text labels for the columns and rows of a
> levelplot object created with he rasterVis package, similar to the
> c("Fall", "Winter", "Spring", Summer") used to label the rows and the
> c("a",..., "h") used to label the columns in the plot shown int he
> following link:
>
> https://stackoverflow.com/questions/43007241/how-to-add-text-to-a-specific-fixed-location-in-rastervis-levelplot
>
> In particular, how can I add c("col1", "col2") and c("row1", "row2")
> as column and row headers, respectively, to the following example
> plot:
>
> --------- START--------
> library(rasterVis)
> f <- system.file("external/test.grd", package="raster")
> r <- raster(f)
> s <- stack(r, r+500, r-500, r+200)
> levelplot(s, layout=c(2,2), names.att=rep("",4))
> --------- END--------
>
>
> Thanks in advance for your help,
>
>
> Mauricio Zambrano-Bigiarini, PhD
>
> =====================================
> Department of Civil Engineering
> Faculty of Engineering and Sciences
> Universidad de La Frontera, Temuco, Chile
> =====================================
> "The ultimate inspiration is the deadline"
> (Nolan Bushnell)
> =====================================
> Linux user #454569 -- Linux Mint user
>

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From jerome.mathieu at upmc.fr  Fri Jul 21 11:19:07 2017
From: jerome.mathieu at upmc.fr (=?UTF-8?Q?J=C3=A9rome_Mathieu?=)
Date: Fri, 21 Jul 2017 11:19:07 +0200
Subject: [R-sig-Geo] RQGIS : looking for a geoalgorithm
In-Reply-To: <CACNjsfXnFyx9+DOU0QuGPQfC1JjO_Sg_3FE8qaMZeQkW+0E23g@mail.gmail.com>
References: <CACNjsfXnFyx9+DOU0QuGPQfC1JjO_Sg_3FE8qaMZeQkW+0E23g@mail.gmail.com>
Message-ID: <CALEhSiQvqbrqZRowTb99S2T-zHRy09cywW+rQu1NOi_o2ZXOVg@mail.gmail.com>

Hi Tristan,

Perhaps qtm() in package tmap? It can automatically generate a list of maps
based on different attributes of a unique shapefile of polygons.

If you work with rasters you can create a raster stack and plot all rasters
at once.

HTH
Jerome


2017-07-20 15:02 GMT+02:00 Tristan Bourgeois <tristan.bourgeois at gmail.com>:
>
> Dear all,
>
> Currently working on a new project I would like to create a dynamic report
> with R (using probably XML package for writting)
>
> In this report I'll plot some graphs (I'm ok with ggplots2) but also map.
>
>
> I just discovered the RQGIS package and I think I understood its
functions.
> My question is the following one :
>
> Is there any geoalgorithms which allows to create an atlas ?
>
> I searched via the "find_algorithms()" but found nothing.
>
> Thanks in advance !
>
> Tristan.
>
> --
> Tristan Bourgeois
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From mauricio.zambrano at ufrontera.cl  Sat Jul 22 02:10:40 2017
From: mauricio.zambrano at ufrontera.cl (Mauricio Zambrano Bigiarini)
Date: Fri, 21 Jul 2017 20:10:40 -0400
Subject: [R-sig-Geo] Fwd: Re: Adding text to rasterVis levelplot
In-Reply-To: <829819bb-07f2-4c7e-f6b7-c4e1ae26d0cb@staff.uni-marburg.de>
References: <8d8c5e22-5cbb-6122-ecbc-c51466c72bbd@staff.uni-marburg.de>
 <829819bb-07f2-4c7e-f6b7-c4e1ae26d0cb@staff.uni-marburg.de>
Message-ID: <CAP6VartQbYu0G=C3T7xjSZDrPxLVZrMo4a-H50KfNq65r_1bjw@mail.gmail.com>

Thank you very much you both Thiago and Florian for your help.

Just for future searches on Google, the minimal example that allows
creating the plot i was looking for was:

------ START -----
library(rasterVis)

f <- system.file("external/test.grd", package="raster")
r <- raster(f)
s <- stack(r, r+500, r-500, r+200)

col.titles = c('col1','col2','','')
row.titles = c('row1','row2')

levelplot(s, layout=c(2,2),
          names.attr=col.titles,
          ylab=row.titles,
          scales = list(y = list(rot = 90) )
          )

------ END -----

However, the example provided by Florian opened up new possibilities
with the grid package for more customisation.

Kind regards,

Mauricio

Mauricio Zambrano-Bigiarini, PhD

=====================================
Department of Civil Engineering
Faculty of Engineering and Sciences
Universidad de La Frontera, Temuco, Chile
=====================================
"The ultimate inspiration is the deadline"
(Nolan Bushnell)
=====================================
Linux user #454569 -- Linux Mint user


On 21 July 2017 at 04:21, Florian Detsch
<florian.detsch at staff.uni-marburg.de> wrote:
> Ah, I forgot to include the definition of text vectors which must be
> inserted before the for() loop - sorry for that.
>
> ---
> ## labels
> cls <- c("col1", "col2")
> rws <- c("row1", "row2")
> ---
>
>
> -------- Forwarded Message --------
> Subject:        Re: [R-sig-Geo] Adding text to rasterVis levelplot
> Date:   Fri, 21 Jul 2017 10:05:08 +0200
> From:   Florian Detsch <florian.detsch at staff.Uni-Marburg.DE>
> To:     r-sig-geo at r-project.org
>
>
>
> Hi Mauricio,
>
> Complementing Thiago's answer using 'names.att', here is a more manual
> approach which might come in handy when a finer control over text
> annotations is required. It is built upon lattice::trellis.focus() which
> lets you select relevant panels of your trellis graph in an iterative
> manner.
>
> ---
> p <- levelplot(s, layout = c(2, 2), names.att = rep("", 4),
>                 scales = list(y = list(rot = 90)))
>
> library(grid)
>
> png("~/rasterVis.png", width = 14, height = 16, units = "cm", res = 300L)
> grid.newpage()
> print(p, newpage = FALSE)
>
> ## loop over panels to be labelled (ie 1:3)
> panels <- trellis.currentLayout()
> for (i in 1:3) {
>
>    # focus on current panel of interest and disable clipping
>    ids <- which(panels == i, arr.ind = TRUE)
>    trellis.focus("panel", ids[2], ids[1], clip.off = TRUE)
>
>    # add labels
>    if (i %in% c(1, 3)) {
>      if (i == 1) {
>        grid.text(cls[1], x = .5, y = 1.1) # add 'col1'
>        grid.text(rws[1], x = -.35, y = .5, rot = 90) # add 'row1'
>      } else {
>        grid.text(rws[2], x = -.35, y = .5, rot = 90) # add 'row2'
>      }
>    } else {
>      grid.text(cls[2], x = .5, y = 1.1) # add 'col2'
>    }
>
>    trellis.unfocus()
> }
>
> dev.off()
> ---
>
> A very similar use case, from which the above approach definitely took
> some inspiration, can be found here:
> https://stackoverflow.com/questions/7649737/is-it-possible-to-update-a-lattice-panel-in-r.
> Have a look at ?grid.text() and ?gpar() to learn more about
> customization options.
>
> Cheers,
> Florian
>
> --
> Dr. Florian Detsch
> Environmental Informatics
> Department of Geography
> Philipps-Universit?t Marburg
> Deutschhausstra?e 12
> 35032 (parcel post: 35037) Marburg, Germany
>
> Phone: +49 (0) 6421 28-25323
> Web: http://www.uni-marburg.de/fb19/fachgebiete/umweltinformatik/detschf/index.html
>
>
> On 20.07.2017 05:07, Thiago V. dos Santos via R-sig-Geo wrote:
>> You can manually define the names that you want and pass them as arguments to 'names.attr' (the name of each panel) and 'ylab' (the title of the y axis).
>> Following your example:
>> col.titles = c('col1','col2','','')
>> row.titles = c('row1','row2')
>>
>> levelplot(s, layout=c(2,2),
>>            names.attr=col.titles,
>>            ylab=row.titles)
>>
>> HTH,
>> Greetings, -- Thiago V. dos Santos
>> PhD studentLand and Atmospheric ScienceUniversity of Minnesota
>>
>>
>> On Wednesday, July 19, 2017, 11:13:31 AM CDT, Mauricio Zambrano Bigiarini <mauricio.zambrano at ufrontera.cl> wrote:
>>
>> Dear all,
>>
>> I would like to add custom text labels for the columns and rows of a
>> levelplot object created with he rasterVis package, similar to the
>> c("Fall", "Winter", "Spring", Summer") used to label the rows and the
>> c("a",..., "h") used to label the columns in the plot shown int he
>> following link:
>>
>> https://stackoverflow.com/questions/43007241/how-to-add-text-to-a-specific-fixed-location-in-rastervis-levelplot
>>
>> In particular, how can I add c("col1", "col2") and c("row1", "row2")
>> as column and row headers, respectively, to the following example
>> plot:
>>
>> --------- START--------
>> library(rasterVis)
>> f <- system.file("external/test.grd", package="raster")
>> r <- raster(f)
>> s <- stack(r, r+500, r-500, r+200)
>> levelplot(s, layout=c(2,2), names.att=rep("",4))
>> --------- END--------
>>
>>
>> Thanks in advance for your help,
>>
>>
>> Mauricio Zambrano-Bigiarini, PhD
>>
>> =====================================
>> Department of Civil Engineering
>> Faculty of Engineering and Sciences
>> Universidad de La Frontera, Temuco, Chile
>> =====================================
>> "The ultimate inspiration is the deadline"
>> (Nolan Bushnell)
>> =====================================
>> Linux user #454569 -- Linux Mint user
>>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
La informaci?n contenida en este correo electr?nico y cualquier anexo o 
respuesta relacionada, puede contener datos e informaci?n confidencial y no 
puede ser usada o difundida por personas distintas a su(s) destinatario(s). 
Si usted no es el destinatario de esta comunicaci?n, le informamos que 
cualquier divulgaci?n, distribuci?n o copia de esta informaci?n constituye 
un delito conforme a la ley chilena. Si lo ha recibido por error, por favor 
borre el mensaje y todos sus anexos y notifique al remitente.


From rafa.pereira.br at gmail.com  Mon Jul 24 16:42:00 2017
From: rafa.pereira.br at gmail.com (Rafael Pereira)
Date: Mon, 24 Jul 2017 15:42:00 +0100
Subject: [R-sig-Geo] bivariate spatial correlation in R
Message-ID: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>

Hi all,

I would like to ask whether some you conducted bi-variate spatial
correlation in R.

I know the bi-variate Moran's I is not implemented in the spdep library. I
left a question on SO but also wanted to hear if anyone if the mainlist
have come across this.
https://stackoverflow.com/questions/45177590/map-of-bivariate-spatial-correlation-in-r-bivariate-lisa

I also know Roger Bivand has implemented the L index proposed by Lee (2001)
in spdep, but I'm not I'm not sure whether the L local correlation
coefficients can be interpreted the same way as the local Moran's I
coefficients. I couldn't find any reference commenting on this issue. I
would very much appreciate your thoughts this.

best,

Rafael HM Pereira
http://urbandemographics.blogspot.com

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Mon Jul 24 20:56:31 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 24 Jul 2017 20:56:31 +0200
Subject: [R-sig-Geo] bivariate spatial correlation in R
In-Reply-To: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>
References: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1707242012390.6677@reclus.nhh.no>

On Mon, 24 Jul 2017, Rafael Pereira wrote:

> Hi all,
>
> I would like to ask whether some you conducted bi-variate spatial
> correlation in R.
>
> I know the bi-variate Moran's I is not implemented in the spdep library.
> I left a question on SO but also wanted to hear if anyone if the mainlist
> have come across this.
> https://stackoverflow.com/questions/45177590/map-of-bivariate-spatial-correlation-in-r-bivariate-lisa
>
> I also know Roger Bivand has implemented the L index proposed by Lee (2001)
> in spdep, but I'm not I'm not sure whether the L local correlation
> coefficients can be interpreted the same way as the local Moran's I
> coefficients. I couldn't find any reference commenting on this issue. I
> would very much appreciate your thoughts this.

In the SO question, and in the follow-up, your presumably throw-away 
example makes fundamental mistakes. The code in spdep by Virgilio 
G?mez-Rubio is for uni- and bivariate L, and produces point values of 
local L. This isn't the main problem, which is rather that you are not 
taking account of the underlying population counts, nor shrinking any 
estimates of significance to accommodate population sizes. Population 
sizes vary from 0 to 11858, with the lower quartile at 3164 and upper 
5698: plot(ecdf(oregon.tract$pop2000)). Should you be comparing rates in 
stead? These are also compositional variables (sum to pop2000, or 1 if 
rates) with the other missing components. You would probably be better 
served by tools examining spatial segregation, such as for example the seg 
package.

The 0 count populations cause problems for an unofficial alternative, the 
black/white ratio:

oregon.tract1 <- oregon.tract[oregon.tract$white > 0,]
oregon.tract1$rat <- oregon.tract1$black/oregon.tract1$white
nb <- poly2nb(oregon.tract1)
lw <- nb2listw(nb)

which should still be adjusted by weighting:

lm0 <- lm(rat ~ 1, weights=pop2000, data=oregon.tract1)

I'm not advising this, but running localmoran.sad on this model output 
yields SAD p-values < 0.05 after FDR correction only in contiguous tracts 
on the Washington state line in Portland between the Columbia and 
Williamette rivers. So do look at the variables you are using before 
rushing into things.

Hope this clarifies,

Roger

>
> best,
>
> Rafael HM Pereira
> http://urbandemographics.blogspot.com
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From ulrike.grimm at uni-leipzig.de  Tue Jul 25 18:32:58 2017
From: ulrike.grimm at uni-leipzig.de (ulrike.grimm at uni-leipzig.de)
Date: Tue, 25 Jul 2017 18:32:58 +0200
Subject: [R-sig-Geo] Error-Mapping
Message-ID: <20170725183258.Horde.L_MrnZsRs4NvL6U96KWrRXB@mail.uni-leipzig.de>

Dear all,

I am a R- starter and not really experienced, but I try my best. For  
my PhD thesis I want to create an error map of a palaeo-relief and I  
found the instruction from Tomislav Hengl 2009 in "A practical Guide  
to Geostatistical mapping" (page 221 ff) really interesting. He also  
recommended this mailing list.

Actually I have two questions.

1. Question: Is it possible to create an empty grid with a individual  
shape of the research area? I found several instruction at the  
Internet but nothing really useful. Useful in this case means, that I  
can further follow the script from Hengl.

2. Question: The gstat package developed further and the function  
"overlay", which he used, is not available anymore. Instead, I think,  
the function "over" should be used, but then I don?t get the result  
which I need for further steps. Because the message reveals: Error in  
plot(cross.ov at coords[, 1], cross.ov at data[[1]], type = "l", xlab = "X",  
  : trying to get slot "coords" from an object of a basic class  
("numeric") with no slots. But of course, the initial objects had  
slots. If I got it right the function "over" just produces numeric  
values. What is the equivalent function for "overlay" from 2009?

I would be really happy about any hint or maybe newer/better solutions  
for error mapping of DEM?s. In my case paloae-DEM?s.

Best regards,
Ulrike Grimm


-- 
Dipl. Geographin

Universit?t Leipzig
Institut f?r Geographie
Johannisallee 19a
D-04103 Leipzig

http://geographie.physgeo.uni-leipzig.de/landschaft/mitarbeiter/ulrike-grimm/


From rafa.pereira.br at gmail.com  Wed Jul 26 10:18:35 2017
From: rafa.pereira.br at gmail.com (Rafael Pereira)
Date: Wed, 26 Jul 2017 09:18:35 +0100
Subject: [R-sig-Geo] bivariate spatial correlation in R
In-Reply-To: <alpine.LFD.2.20.1707242012390.6677@reclus.nhh.no>
References: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>
 <alpine.LFD.2.20.1707242012390.6677@reclus.nhh.no>
Message-ID: <CAA42DGm1dhzEvN9vYyRfKS7XuPVnXFgVBfRHWFLGS29H4R65Cw@mail.gmail.com>

Roger,

This example was provided only for the sake or making the code easily
reproducible for others and I'm more interested in how the bi-variate Moran
could be implemented in R, but your comments are very much welcomed and
I've made changes to the question.

My actual case study looks at bi-variate spatial correlation between (a)
average household income per capita and (b) proportion of jobs in the city
that are accessible under 60 minutes by transit. I don't think I could use
rates in this case but I will normalize the variables using
scale(data$variable).

best,

Rafael H M Pereira

On Mon, Jul 24, 2017 at 7:56 PM, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Mon, 24 Jul 2017, Rafael Pereira wrote:
>
> Hi all,
>>
>> I would like to ask whether some you conducted bi-variate spatial
>> correlation in R.
>>
>> I know the bi-variate Moran's I is not implemented in the spdep library.
>> I left a question on SO but also wanted to hear if anyone if the mainlist
>> have come across this.
>> https://stackoverflow.com/questions/45177590/map-of-bivariat
>> e-spatial-correlation-in-r-bivariate-lisa
>>
>> I also know Roger Bivand has implemented the L index proposed by Lee
>> (2001)
>> in spdep, but I'm not I'm not sure whether the L local correlation
>> coefficients can be interpreted the same way as the local Moran's I
>> coefficients. I couldn't find any reference commenting on this issue. I
>> would very much appreciate your thoughts this.
>>
>
> In the SO question, and in the follow-up, your presumably throw-away
> example makes fundamental mistakes. The code in spdep by Virgilio
> G?mez-Rubio is for uni- and bivariate L, and produces point values of local
> L. This isn't the main problem, which is rather that you are not taking
> account of the underlying population counts, nor shrinking any estimates of
> significance to accommodate population sizes. Population sizes vary from 0
> to 11858, with the lower quartile at 3164 and upper 5698:
> plot(ecdf(oregon.tract$pop2000)). Should you be comparing rates in stead?
> These are also compositional variables (sum to pop2000, or 1 if rates) with
> the other missing components. You would probably be better served by tools
> examining spatial segregation, such as for example the seg package.
>
> The 0 count populations cause problems for an unofficial alternative, the
> black/white ratio:
>
> oregon.tract1 <- oregon.tract[oregon.tract$white > 0,]
> oregon.tract1$rat <- oregon.tract1$black/oregon.tract1$white
> nb <- poly2nb(oregon.tract1)
> lw <- nb2listw(nb)
>
> which should still be adjusted by weighting:
>
> lm0 <- lm(rat ~ 1, weights=pop2000, data=oregon.tract1)
>
> I'm not advising this, but running localmoran.sad on this model output
> yields SAD p-values < 0.05 after FDR correction only in contiguous tracts
> on the Washington state line in Portland between the Columbia and
> Williamette rivers. So do look at the variables you are using before
> rushing into things.
>
> Hope this clarifies,
>
> Roger
>
>
>> best,
>>
>> Rafael HM Pereira
>> http://urbandemographics.blogspot.com
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Wed Jul 26 12:07:00 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 26 Jul 2017 12:07:00 +0200
Subject: [R-sig-Geo] bivariate spatial correlation in R
In-Reply-To: <CAA42DGm1dhzEvN9vYyRfKS7XuPVnXFgVBfRHWFLGS29H4R65Cw@mail.gmail.com>
References: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>
 <alpine.LFD.2.20.1707242012390.6677@reclus.nhh.no>
 <CAA42DGm1dhzEvN9vYyRfKS7XuPVnXFgVBfRHWFLGS29H4R65Cw@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1707261152010.18617@reclus.nhh.no>

On Wed, 26 Jul 2017, Rafael Pereira wrote:

> Roger,
>
> This example was provided only for the sake or making the code easily
> reproducible for others and I'm more interested in how the bi-variate Moran
> could be implemented in R, but your comments are very much welcomed and
> I've made changes to the question.
>
> My actual case study looks at bi-variate spatial correlation between (a)
> average household income per capita and (b) proportion of jobs in the city
> that are accessible under 60 minutes by transit. I don't think I could use
> rates in this case but I will normalize the variables using
> scale(data$variable).

Please provide a reproducible example, either with a link to a data 
subset, or using a builtin data set. My guess is that you do not need 
bi-variate spatial correlation at all, but rather a spatial regression.

The "causal" variable would then the the proportion of jobs accessible 
within 60 minutes by transit, though this is extremely blunt, and lots of 
other covariates (demography, etc.) impact average household income per 
capita (per block/tract?). Since there are many missing variables in your 
specification, any spatial correlation would be most closely associated 
with them (demography, housing costs, education, etc.), and the choice of 
units of measurement would dominate the outcome.

This is also why bi-variate spatial correlation is seldom a good idea, I 
believe. It can be done, but most likely shouldn't, unless it can be 
motivated properly.

By the way, the weighted and FDR-corrected SAD local Moran's I p-values of 
the black/white ratio for Oregon (your toy example) did deliver the goods 
- if you zoom in in mapview::mapview, you can see that it detects a rate 
hotspot between the rivers.

Roger

>
> best,
>
> Rafael H M Pereira
>
> On Mon, Jul 24, 2017 at 7:56 PM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Mon, 24 Jul 2017, Rafael Pereira wrote:
>>
>> Hi all,
>>>
>>> I would like to ask whether some you conducted bi-variate spatial
>>> correlation in R.
>>>
>>> I know the bi-variate Moran's I is not implemented in the spdep library.
>>> I left a question on SO but also wanted to hear if anyone if the mainlist
>>> have come across this.
>>> https://stackoverflow.com/questions/45177590/map-of-bivariat
>>> e-spatial-correlation-in-r-bivariate-lisa
>>>
>>> I also know Roger Bivand has implemented the L index proposed by Lee
>>> (2001)
>>> in spdep, but I'm not I'm not sure whether the L local correlation
>>> coefficients can be interpreted the same way as the local Moran's I
>>> coefficients. I couldn't find any reference commenting on this issue. I
>>> would very much appreciate your thoughts this.
>>>
>>
>> In the SO question, and in the follow-up, your presumably throw-away
>> example makes fundamental mistakes. The code in spdep by Virgilio
>> G?mez-Rubio is for uni- and bivariate L, and produces point values of local
>> L. This isn't the main problem, which is rather that you are not taking
>> account of the underlying population counts, nor shrinking any estimates of
>> significance to accommodate population sizes. Population sizes vary from 0
>> to 11858, with the lower quartile at 3164 and upper 5698:
>> plot(ecdf(oregon.tract$pop2000)). Should you be comparing rates in stead?
>> These are also compositional variables (sum to pop2000, or 1 if rates) with
>> the other missing components. You would probably be better served by tools
>> examining spatial segregation, such as for example the seg package.
>>
>> The 0 count populations cause problems for an unofficial alternative, the
>> black/white ratio:
>>
>> oregon.tract1 <- oregon.tract[oregon.tract$white > 0,]
>> oregon.tract1$rat <- oregon.tract1$black/oregon.tract1$white
>> nb <- poly2nb(oregon.tract1)
>> lw <- nb2listw(nb)
>>
>> which should still be adjusted by weighting:
>>
>> lm0 <- lm(rat ~ 1, weights=pop2000, data=oregon.tract1)
>>
>> I'm not advising this, but running localmoran.sad on this model output
>> yields SAD p-values < 0.05 after FDR correction only in contiguous tracts
>> on the Washington state line in Portland between the Columbia and
>> Williamette rivers. So do look at the variables you are using before
>> rushing into things.
>>
>> Hope this clarifies,
>>
>> Roger
>>
>>
>>> best,
>>>
>>> Rafael HM Pereira
>>> http://urbandemographics.blogspot.com
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
>> http://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From lucianolasala at yahoo.com.ar  Wed Jul 26 15:28:24 2017
From: lucianolasala at yahoo.com.ar (Luciano La Sala)
Date: Wed, 26 Jul 2017 10:28:24 -0300
Subject: [R-sig-Geo] Finding overlapping polygons
Message-ID: <939ba90c-bab6-3eff-a8bf-907311e19b5b@yahoo.com.ar>

Dear everyone,

I am studying the potential spatal distribution between vertebrate 
invasive species and native, threatened ones in the souther cone of 
South America. The invasive species' distribution was grossly 
approximated using an ecoregion approach, which is not relevant here. 
Then, I downloaded shapefiles for all terrestrial mammals and birds of 
the world from BirdLife International 
(http://www.biodiversityinfo.org/spcdownload/r5h8a1/) and IUCN 
(http://www.iucnredlist.org/technical-documents/spatial-data) public 
domains.

Our goal: using a two-step approach, identify native species (polygons) 
whose distributionoverlap -totally or partially- with that of the invasive.

Our problem: Some species (with small distribution) which are known to 
occur within the invasive distribution range are being left out of the 
final selection (checkedin QGIS). I suspect the function "isin" in the 
fastshp package (2nd loop) fails to select some of the overlapping 
polygons in Step 2, or some other problem. If a reproducible example is 
absolutely required, I can upload to dropbox.

For the sake of simplicity, I will present present a case example 
including one invasive species (e.g. wild boar) and all terrestrial 
mammals.

# Analysis of Wild Boar distributional overlap with IUCN mammal species 
install.packages("rgdal") install.packages("sp") 
install.packages("ggplot2") install.packages("rgeos") 
install.packages("raster") install.packages("fastshp", repos = 
"http://rforge.net", type = "source")install.packages("cleangeo")

mypath <- ("/Users/ ... /Terrestrial mammals") files <- 
list.files(path=mypath, pattern = "\\.shp$", full.names=T) 
length(files)  # 5,465 mammal species # Load study area

study_area <- read.shp("C:/Users/.../Study_area.shp", format="table", 
close=FALSE) salon <- range(study_area$x, na.rm=T) salat <- 
range(study_area$y, na.rm=T) bounding <- c(salon, salat) box <- 
data.frame(salon, salat) colnames(box) <- c("Longitud","Latitude") box

# Step 1. Run all native IUCN species' distribution through a loop and 
select those which overlap with the invasive's bounding box distribution

keep <- rep(NA, length(files)) length(keep)  # 5,465 for(i in 
1:length(files)){   r <- read.shp(files[i], format="table")   rlon <- 
range(r$x)   rlat <- range(r$y) ## Is out of the bounding box? test.lat 
<- (min(rlat) > max(salat)) |     (max(rlat) < min(salat))   test.lon <- 
(min(rlon) > max(salon)) |     (max(rlon) < min(salon)) keep[i] <- 
!(test.lon | test.lat) } keep <- which(keep==1) # Index of shapefiles 
that passed the first filter length(keep) # 773 species passed 1st 
filter # Step 2. Load invasive species distribution area

study_area <- readOGR("/Users/.../Study area","Study_area") study_area 
<- spTransform(study_area, CRS("+init=epsg:32721")) # To UTM # Get a 
report of geometry validity & issues for a sp spatial object report <- 
clgeo_CollectionReport(study_area) summary <- 
clgeo_SummaryReport(report) summary # Remove holes and simplify geometry 
of study area for computational efficiency outer <- 
Filter(function(x){x at ringDir==1}, study_area at polygons[[1]]@Polygons) # 
PREGUNTAR study_area <- SpatialPolygons(list(Polygons(outer, ID=1))) 
study_area <- disaggregate(study_area) areas <- gArea(study_area, 
byid=TRUE) #Remove polygons smaller than 3000 km2 study_area <- 
study_area[which(areas > 3E9),] study_area <- gSimplify(study_area, 
20000, topologyPreserve=TRUE) # Simplify geometry 
proj4string(study_area) <- CRS("+init=epsg:32721") study_area <- 
spTransform(study_area, CRS("+proj=longlat +datum=WGS84 +no_defs 
+ellps=WGS84 +towgs84=0,0,0")) xy <- fortify(study_area) # 
SpatialPolygons to data frame keep2 <- rep(NA,length(keep)) # 773 sp. 
that passed first filter for(i in 1:length(keep)){   r <- 
read.shp(files[keep[i]], format="polygon")   isin <- any(inside(r, 
x=xy$long, y=xy$lat), na.rm=T)   keep2[i] <- isin } keep2 <- 
which(keep2) # Index of shapefiles that passed the second sort 
length(keep2)  # 532 species that passed the second filter species <- 
list.files(path=mypath, pattern = "\\.shp$",full.names=F) species <- 
sub(".shp", "", species) species <- species[keep[keep2]]



---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From lucianolasala at yahoo.com.ar  Wed Jul 26 15:30:30 2017
From: lucianolasala at yahoo.com.ar (Luciano La Sala)
Date: Wed, 26 Jul 2017 10:30:30 -0300
Subject: [R-sig-Geo] Finding overlapping polygons
Message-ID: <5e5b70be-62b7-ee45-ce0c-2a61292ef605@yahoo.com.ar>

Dear everyone,

I am studying the potential spatial distribution between vertebrate 
invasive species and native ones (threatened) in the souther cone of 
South America. The invasive species' distribution was grossly 
approximated using an ecoregional approach, which is not relevant here. 
The analysis is based on shapefiles for all terrestrial mammals and 
birds of the world obtained from BirdLife International 
(http://www.biodiversityinfo.org/spcdownload/r5h8a1/) and IUCN 
(http://www.iucnredlist.org/technical-documents/spatial-data) public 
domains.

Our goal: using a two-step approach, identify native species (polygons) 
whose distributionoverlap -totally or partially- with that of the invasive.

Our problem: Some species (with small distribution) which are known to 
occur within the invasive distribution range are being left out of the 
final selection (checkedin QGIS). I suspect the function "isin" in the 
fastshp package (2nd loop) fails to select some of the overlapping 
polygons in Step 2, or some other problem. If a reproducible example is 
absolutely required, I can upload to dropbox.

For the sake of simplicity, I will present present a case example 
including one invasive species (e.g. wild boar) and all terrestrial 
mammals.

# Analysis of Wild Boar distributional overlap with IUCN mammal species 
install.packages("rgdal") install.packages("sp") 
install.packages("ggplot2") install.packages("rgeos") 
install.packages("raster") install.packages("fastshp", repos = 
"http://rforge.net", type = "source")install.packages("cleangeo")

mypath <- ("/Users/ ... /Terrestrial mammals") files <- 
list.files(path=mypath, pattern = "\\.shp$", full.names=T) 
length(files)  # 5,465 mammal species # Load study area

study_area <- read.shp("C:/Users/.../Study_area.shp", format="table", 
close=FALSE) salon <- range(study_area$x, na.rm=T) salat <- 
range(study_area$y, na.rm=T) bounding <- c(salon, salat) box <- 
data.frame(salon, salat) colnames(box) <- c("Longitud","Latitude") box

# Step 1. Run all native IUCN species' distribution through a loop and 
select those which overlap with the invasive's bounding box distribution

keep <- rep(NA, length(files)) length(keep)  # 5,465 for(i in 
1:length(files)){   r <- read.shp(files[i], format="table")   rlon <- 
range(r$x)   rlat <- range(r$y) ## Is out of the bounding box? test.lat 
<- (min(rlat) > max(salat)) |     (max(rlat) < min(salat))   test.lon <- 
(min(rlon) > max(salon)) |     (max(rlon) < min(salon)) keep[i] <- 
!(test.lon | test.lat) } keep <- which(keep==1) # Index of shapefiles 
that passed the first filter length(keep) # 773 species passed 1st 
filter # Step 2. Load invasive species distribution area

study_area <- readOGR("/Users/.../Study area","Study_area") study_area 
<- spTransform(study_area, CRS("+init=epsg:32721")) # To UTM # Get a 
report of geometry validity & issues for a sp spatial object report <- 
clgeo_CollectionReport(study_area) summary <- 
clgeo_SummaryReport(report) summary # Remove holes and simplify geometry 
of study area for computational efficiency outer <- 
Filter(function(x){x at ringDir==1}, study_area at polygons[[1]]@Polygons) # 
PREGUNTAR study_area <- SpatialPolygons(list(Polygons(outer, ID=1))) 
study_area <- disaggregate(study_area) areas <- gArea(study_area, 
byid=TRUE) #Remove polygons smaller than 3000 km2 study_area <- 
study_area[which(areas > 3E9),] study_area <- gSimplify(study_area, 
20000, topologyPreserve=TRUE) # Simplify geometry 
proj4string(study_area) <- CRS("+init=epsg:32721") study_area <- 
spTransform(study_area, CRS("+proj=longlat +datum=WGS84 +no_defs 
+ellps=WGS84 +towgs84=0,0,0")) xy <- fortify(study_area) # 
SpatialPolygons to data frame keep2 <- rep(NA,length(keep)) # 773 sp. 
that passed first filter for(i in 1:length(keep)){   r <- 
read.shp(files[keep[i]], format="polygon")   isin <- any(inside(r, 
x=xy$long, y=xy$lat), na.rm=T)   keep2[i] <- isin } keep2 <- 
which(keep2) # Index of shapefiles that passed the second sort 
length(keep2)  # 532 species that passed the second filter species <- 
list.files(path=mypath, pattern = "\\.shp$",full.names=F) species <- 
sub(".shp", "", species) species <- species[keep[keep2]]



---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


.


From thi_veloso at yahoo.com.br  Thu Jul 27 06:09:53 2017
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Thu, 27 Jul 2017 04:09:53 +0000 (UTC)
Subject: [R-sig-Geo] How to objectively subset cities by population
References: <1634627903.292923.1501128593268.ref@mail.yahoo.com>
Message-ID: <1634627903.292923.1501128593268@mail.yahoo.com>

Dear all,

I have temperature records of nearly 1200 locations in southern Brazil.

I am writing a shiny app that will show an interactive map with the locations plotted as circles, where the user can click a location to see its temperature time series.

However, if I show all the locations in the map, it will look really bad, too cramped.

Therefore, in an attempt to make the map look a bit cleaner, I am trying to think of an objective way to subset the locations. My initial approach would be to show only the "largest" locations, i.e. the ones with a population above a certain threshold.

The problem is: the distribution of the population is so positively skewed that I am having a hard time determining the optimal cutoff point.

Does anybody here know any tool or method, possibly spatial, that can assist me with this analysis?

These are the locations I am working with:

#-------------------------------
# Download and summarize
locs <- read.csv("https://www.dropbox.com/s/ykdd8x1mlc76klt/locations.csv?raw=1")
hist(locs$Population)
summary(locs$Population)

# Convert to spatial points and plot
require(sp)
coordinates(locs) <- cbind(locs$Lon , locs$Lat)
plot(locs)
bubble(locs,"Population")
#-------------------------------

Thanks in advance,
 -- Thiago V. dos Santos

PhD student
Land and Atmospheric Science
University of Minnesota
	[[alternative HTML version deleted]]


From b.graeler at 52north.org  Thu Jul 27 10:00:19 2017
From: b.graeler at 52north.org (=?UTF-8?Q?Dr._Benedikt_Gr=c3=a4ler?=)
Date: Thu, 27 Jul 2017 10:00:19 +0200
Subject: [R-sig-Geo] How to objectively subset cities by population
In-Reply-To: <1634627903.292923.1501128593268@mail.yahoo.com>
References: <1634627903.292923.1501128593268.ref@mail.yahoo.com>
 <1634627903.292923.1501128593268@mail.yahoo.com>
Message-ID: <023b30ed-47e5-5859-8f3a-c5ca9989d9eb@52north.org>

Dear Thiago,

if you want them spatially evenly distributed, you could overlay a grid 
and select the largest per grid box - or maybe more intuitive, select 
the largest per predefined administrative areas (counties/postal 
codes/...). This could also change based on zoom-level. An alternative 
is to group sensors and expand and zoom in by clicking on the group (see 
e.g. [1]).

HTH,

  Ben

[1] http://sensorweb.demo.52north.org/client/#/map


On 27/07/2017 06:09, Thiago V. dos Santos via R-sig-Geo wrote:
> Dear all,
> 
> I have temperature records of nearly 1200 locations in southern Brazil.
> 
> I am writing a shiny app that will show an interactive map with the locations plotted as circles, where the user can click a location to see its temperature time series.
> 
> However, if I show all the locations in the map, it will look really bad, too cramped.
> 
> Therefore, in an attempt to make the map look a bit cleaner, I am trying to think of an objective way to subset the locations. My initial approach would be to show only the "largest" locations, i.e. the ones with a population above a certain threshold.
> 
> The problem is: the distribution of the population is so positively skewed that I am having a hard time determining the optimal cutoff point.
> 
> Does anybody here know any tool or method, possibly spatial, that can assist me with this analysis?
> 
> These are the locations I am working with:
> 
> #-------------------------------
> # Download and summarize
> locs <- read.csv("https://www.dropbox.com/s/ykdd8x1mlc76klt/locations.csv?raw=1")
> hist(locs$Population)
> summary(locs$Population)
> 
> # Convert to spatial points and plot
> require(sp)
> coordinates(locs) <- cbind(locs$Lon , locs$Lat)
> plot(locs)
> bubble(locs,"Population")
> #-------------------------------
> 
> Thanks in advance,
>   -- Thiago V. dos Santos
> 
> PhD student
> Land and Atmospheric Science
> University of Minnesota
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Dr. Benedikt Gr?ler
52?North Initiative for Geospatial Open Source Software GmbH
Martin-Luther-King-Weg 24
48155 Muenster, Germany

E-Mail: b.graeler at 52north.org
Fon: +49-(0)-251/396371-39
Fax: +49-(0)-251/396371-11

http://52north.org/
Twitter: @FiveTwoN

General Managers: Dr. Albert Remke, Dr. Andreas Wytzisk
Local Court Muenster HRB 10849
-------------- next part --------------
A non-text attachment was scrubbed...
Name: b_graeler.vcf
Type: text/x-vcard
Size: 445 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170727/afb90b97/attachment.vcf>

From kent3737 at gmail.com  Thu Jul 27 14:29:43 2017
From: kent3737 at gmail.com (Kent Johnson)
Date: Thu, 27 Jul 2017 08:29:43 -0400
Subject: [R-sig-Geo] How to objectively subset cities by population
Message-ID: <CAPP0wyjteLow4_C6eyxodvwyoVpUtv0z_27mEQedoBUAO_t=VQ@mail.gmail.com>

>
> Date: Thu, 27 Jul 2017 04:09:53 +0000 (UTC)
> From: "Thiago V. dos Santos" <thi_veloso at yahoo.com.br>
> To: R-sig-geo Mailing List <r-sig-geo at r-project.org>
> Subject: [R-sig-Geo] How to objectively subset cities by population
> Message-ID: <1634627903.292923.1501128593268 at mail.yahoo.com>
> Content-Type: text/plain; charset="UTF-8"
>
> Dear all,
>
> I have temperature records of nearly 1200 locations in southern Brazil.
>
> I am writing a shiny app that will show an interactive map with the
> locations plotted as circles, where the user can click a location to see
> its temperature time series.
>
> However, if I show all the locations in the map, it will look really bad,
> too cramped.
>

Have you considered using leaflet to make an interactive map? This is a
good start:

library(leaflet)
locs <- read.csv("
https://www.dropbox.com/s/ykdd8x1mlc76klt/locations.csv?raw=1")

leaflet(locs) %>% addTiles() %>%
  addCircles(radius=~20*sqrt(Population), label=~as.character(Geocode),
             stroke=FALSE, fillOpacity=0.5)

You can configure popups to show HTML or get Shiny events on click, for
example clicking on a city could display the time series in a separate
panel.

Docs and many examples on the leaflet for R web site:
https://rstudio.github.io/leaflet/

Kent

	[[alternative HTML version deleted]]


From thi_veloso at yahoo.com.br  Thu Jul 27 16:11:33 2017
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Thu, 27 Jul 2017 14:11:33 +0000 (UTC)
Subject: [R-sig-Geo] How to objectively subset cities by population
In-Reply-To: <CAPP0wyjteLow4_C6eyxodvwyoVpUtv0z_27mEQedoBUAO_t=VQ@mail.gmail.com>
References: <CAPP0wyjteLow4_C6eyxodvwyoVpUtv0z_27mEQedoBUAO_t=VQ@mail.gmail.com>
Message-ID: <1825692188.530978.1501164693299@mail.yahoo.com>

Hi Kent,
Thank you very much for the response.
This is exactly the same approach I am using now, with a different radius formula though.
It is not at all terrible, but do you see how overlapped the largest cities are, and how difficult it is to click on some smaller cities?
This is why I am looking for an objective way to filter out the smallest cities, while at the same time keeping as much information on the map as possible (i.e. not leaving too many "blank" areas).?
Best,?-- Thiago V. dos Santos
PhD studentLand and Atmospheric ScienceUniversity of Minnesota


On Thursday, July 27, 2017, 7:29:44 AM CDT, Kent Johnson <kent3737 at gmail.com> wrote:


Date: Thu, 27 Jul 2017 04:09:53 +0000 (UTC)
From: "Thiago V. dos Santos" <thi_veloso at yahoo.com.br>
To: R-sig-geo Mailing List <r-sig-geo at r-project.org>
Subject: [R-sig-Geo] How to objectively subset cities by population
Message-ID: <1634627903.292923. 1501128593268 at mail.yahoo.com>
Content-Type: text/plain; charset="UTF-8"

Dear all,

I have temperature records of nearly 1200 locations in southern Brazil.

I am writing a shiny app that will show an interactive map with the locations plotted as circles, where the user can click a location to see its temperature time series.

However, if I show all the locations in the map, it will look really bad, too cramped.


Have you considered using leaflet to make an interactive map? This is a good start:
library(leaflet)locs <- read.csv("https://www.dropbox.com/s/ykdd8x1mlc76klt/locations.csv?raw=1")
leaflet(locs) %>% addTiles() %>%?? addCircles(radius=~20*sqrt(Population), label=~as.character(Geocode),?? ? ? ? ? ? ?stroke=FALSE, fillOpacity=0.5)
You can configure popups to show HTML or get Shiny events on click, for example clicking on a city could display the time series in a separate panel.
Docs and many examples on the leaflet for R web site:
https://rstudio.github.io/leaflet/

Kent
	[[alternative HTML version deleted]]


From thi_veloso at yahoo.com.br  Thu Jul 27 16:27:16 2017
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Thu, 27 Jul 2017 14:27:16 +0000 (UTC)
Subject: [R-sig-Geo] How to objectively subset cities by population
In-Reply-To: <023b30ed-47e5-5859-8f3a-c5ca9989d9eb@52north.org>
References: <1634627903.292923.1501128593268.ref@mail.yahoo.com>
 <1634627903.292923.1501128593268@mail.yahoo.com>
 <023b30ed-47e5-5859-8f3a-c5ca9989d9eb@52north.org>
Message-ID: <2072897457.572932.1501165636879@mail.yahoo.com>

Dear Dr. Gr?ler,
Thanks for your contribution. I very much enjoyed the clustering suggestion, and it seems to be available in R's leaflet through the "markerClusterOptions" command. It could solve my problem, so I will take a closer look at that.
Regarding your first suggestion, can you point me out some example that uses the overlaid grid approach?
Thanks,?-- Thiago V. dos Santos
PhD studentLand and Atmospheric ScienceUniversity of Minnesota


On Thursday, July 27, 2017, 3:00:55 AM CDT, Dr. Benedikt Gr?ler <b.graeler at 52north.org> wrote:

Dear Thiago,

if you want them spatially evenly distributed, you could overlay a grid 
and select the largest per grid box - or maybe more intuitive, select 
the largest per predefined administrative areas (counties/postal 
codes/...). This could also change based on zoom-level. An alternative 
is to group sensors and expand and zoom in by clicking on the group (see 
e.g. [1]).

HTH,

? Ben

[1] http://sensorweb.demo.52north.org/client/#/map


On 27/07/2017 06:09, Thiago V. dos Santos via R-sig-Geo wrote:
> Dear all,
> 
> I have temperature records of nearly 1200 locations in southern Brazil.
> 
> I am writing a shiny app that will show an interactive map with the locations plotted as circles, where the user can click a location to see its temperature time series.
> 
> However, if I show all the locations in the map, it will look really bad, too cramped.
> 
> Therefore, in an attempt to make the map look a bit cleaner, I am trying to think of an objective way to subset the locations. My initial approach would be to show only the "largest" locations, i.e. the ones with a population above a certain threshold.
> 
> The problem is: the distribution of the population is so positively skewed that I am having a hard time determining the optimal cutoff point.
> 
> Does anybody here know any tool or method, possibly spatial, that can assist me with this analysis?
> 
> These are the locations I am working with:
> 
> #-------------------------------
> # Download and summarize
> locs <- read.csv("https://www.dropbox.com/s/ykdd8x1mlc76klt/locations.csv?raw=1")
> hist(locs$Population)
> summary(locs$Population)
> 
> # Convert to spatial points and plot
> require(sp)
> coordinates(locs) <- cbind(locs$Lon , locs$Lat)
> plot(locs)
> bubble(locs,"Population")
> #-------------------------------
> 
> Thanks in advance,
>? -- Thiago V. dos Santos
> 
> PhD student
> Land and Atmospheric Science
> University of Minnesota
> ??? [[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Dr. Benedikt Gr?ler
52?North Initiative for Geospatial Open Source Software GmbH
Martin-Luther-King-Weg 24
48155 Muenster, Germany

E-Mail: b.graeler at 52north.org
Fon: +49-(0)-251/396371-39
Fax: +49-(0)-251/396371-11

http://52north.org/
Twitter: @FiveTwoN

General Managers: Dr. Albert Remke, Dr. Andreas Wytzisk
Local Court Muenster HRB 10849
_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo
	[[alternative HTML version deleted]]


From kent3737 at gmail.com  Thu Jul 27 19:13:11 2017
From: kent3737 at gmail.com (Kent Johnson)
Date: Thu, 27 Jul 2017 13:13:11 -0400
Subject: [R-sig-Geo] How to objectively subset cities by population
In-Reply-To: <1825692188.530978.1501164693299@mail.yahoo.com>
References: <CAPP0wyjteLow4_C6eyxodvwyoVpUtv0z_27mEQedoBUAO_t=VQ@mail.gmail.com>
 <1825692188.530978.1501164693299@mail.yahoo.com>
Message-ID: <CAPP0wyhVC6x-h8_FA_GxdOeUAOKLReVfjO2O1O2go2jO7GsP5w@mail.gmail.com>

On Thu, Jul 27, 2017 at 10:11 AM, Thiago V. dos Santos <
thi_veloso at yahoo.com.br> wrote:

> Hi Kent,
>
> Thank you very much for the response.
>
> This is exactly the same approach I am using now, with a different radius
> formula though.
>
> It is not at all terrible, but do you see how overlapped the largest
> cities are, and how difficult it is to click on some smaller cities?
>

To my eye
  radius=~10*pmax(sqrt(Population), 100)
keeps the overlap manageable and makes the small cities a little bigger.

The advantage of leaflet vs plot is that you can easily zoom in to see the
smaller cities and click on them.

Kent


> This is why I am looking for an objective way to filter out the smallest
> cities, while at the same time keeping as much information on the map as
> possible (i.e. not leaving too many "blank" areas).
>
> Best,
>  -- Thiago V. dos Santos
>
> PhD student
> Land and Atmospheric Science
> University of Minnesota
>
>
>
> On Thursday, July 27, 2017, 7:29:44 AM CDT, Kent Johnson <
> kent3737 at gmail.com> wrote:
>
>
> Date: Thu, 27 Jul 2017 04:09:53 +0000 (UTC)
> From: "Thiago V. dos Santos" <thi_veloso at yahoo.com.br>
> To: R-sig-geo Mailing List <r-sig-geo at r-project.org>
> Subject: [R-sig-Geo] How to objectively subset cities by population
> Message-ID: <1634627903.292923. 1501128593268 at mail.yahoo.com
> <1634627903.292923.1501128593268 at mail.yahoo.com>>
> Content-Type: text/plain; charset="UTF-8"
>
> Dear all,
>
> I have temperature records of nearly 1200 locations in southern Brazil.
>
> I am writing a shiny app that will show an interactive map with the
> locations plotted as circles, where the user can click a location to see
> its temperature time series.
>
> However, if I show all the locations in the map, it will look really bad,
> too cramped.
>
>
> Have you considered using leaflet to make an interactive map? This is a
> good start:
>
> library(leaflet)
> locs <- read.csv("https://www.dropbox.com/s/ykdd8x1mlc76klt/
> locations.csv?raw=1")
>
> leaflet(locs) %>% addTiles() %>%
>   addCircles(radius=~20*sqrt(Population), label=~as.character(Geocode),
>              stroke=FALSE, fillOpacity=0.5)
>
> You can configure popups to show HTML or get Shiny events on click, for
> example clicking on a city could display the time series in a separate
> panel.
>
> Docs and many examples on the leaflet for R web site:
> https://rstudio.github.io/leaflet/
>
> Kent
>

	[[alternative HTML version deleted]]


From milujisb at gmail.com  Fri Jul 28 18:21:05 2017
From: milujisb at gmail.com (Miluji Sb)
Date: Fri, 28 Jul 2017 18:21:05 +0200
Subject: [R-sig-Geo] Problem in extracting data from GRIB files
Message-ID: <CAMLwc7Pt-4PCFQHB9yU9YZ9eyt-bcxdScw83QUDhM=7HQ-oeQw@mail.gmail.com>

Dear all,

I have a set of coordinates for cities:

structure(list(lon = c(3.7174243, 3.2246995, 2.928656, 33.3822764,
10.40237, 24.7535746, 7.2619532, -0.370679, -4.486076, -4.097899
), lat = c(51.0543422, 51.209348, 51.21543, 35.1855659, 55.403756,
59.4369608, 43.7101728, 49.182863, 48.390394, 47.997542)), .Names =
c("lon",
"lat"), na.action = structure(c(5L, 6L, 31L, 55L, 59L, 61L, 67L,
68L), .Names = c("5", "6", "31", "55", "59", "61", "67", "68"
), class = "omit"), row.names = c(1L, 2L, 3L, 4L, 7L, 8L, 9L,
10L, 11L, 12L), class = "data.frame")

I am trying to extract climatic data from GLDAS climatic data (1 degree x 1
degree) GRIB files with the following code:

# Convert to spatial points
pts_dams <- SpatialPoints(lonlat,proj4string=CRS("+proj=longlat"))

# Extract
 filelist<-list.files(pattern=".grb$")

    data<-stack(filelist[1])
    data at layers<-sapply(filelist, function(x) raster(x,band=12))
    data_dams_size<-extract(data,pts_dams)
    DF_data_dams_size<- as.data.frame(data_dams_size)

Unfortunately, I get mostly NAs for the data, it seems that there's an
issue with the CRS projections for the city coordinates. Is there a
specific projection for city level coordinates? Or am I doing something
completely wrong? Thank you!

Sincerely,

Milu

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Sat Jul 29 00:36:46 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Fri, 28 Jul 2017 22:36:46 +0000
Subject: [R-sig-Geo] Problem in extracting data from GRIB files
In-Reply-To: <CAMLwc7Pt-4PCFQHB9yU9YZ9eyt-bcxdScw83QUDhM=7HQ-oeQw@mail.gmail.com>
References: <CAMLwc7Pt-4PCFQHB9yU9YZ9eyt-bcxdScw83QUDhM=7HQ-oeQw@mail.gmail.com>
Message-ID: <CAAcGz99SqFs5mutw9379-H7hbx1eP0YXLX422wAyHvoLDwkwiw@mail.gmail.com>

On Sat, 29 Jul 2017 at 02:21 Miluji Sb <milujisb at gmail.com> wrote:

> Dear all,
>
> I have a set of coordinates for cities:
>
> structure(list(lon = c(3.7174243, 3.2246995, 2.928656, 33.3822764,
> 10.40237, 24.7535746, 7.2619532, -0.370679, -4.486076, -4.097899
> ), lat = c(51.0543422, 51.209348, 51.21543, 35.1855659, 55.403756,
> 59.4369608, 43.7101728, 49.182863, 48.390394, 47.997542)), .Names =
> c("lon",
> "lat"), na.action = structure(c(5L, 6L, 31L, 55L, 59L, 61L, 67L,
> 68L), .Names = c("5", "6", "31", "55", "59", "61", "67", "68"
> ), class = "omit"), row.names = c(1L, 2L, 3L, 4L, 7L, 8L, 9L,
> 10L, 11L, 12L), class = "data.frame")
>
> I am trying to extract climatic data from GLDAS climatic data (1 degree x 1
> degree) GRIB files with the following code:
>
> # Convert to spatial points
> pts_dams <- SpatialPoints(lonlat,proj4string=CRS("+proj=longlat"))
>
> # Extract
>  filelist<-list.files(pattern=".grb$")
>
>
This part is a bit wrong:


>     data<-stack(filelist[1])
>     data at layers<-sapply(filelist, function(x) raster(x,band=12))
>

Better would be

rdata <-  raster::stack(lapply(filelist, function(x) raster(x,band=12)))

For the rest we will need to see details about the data, so please do

print(rdata)

and send the resulting print-out.

 A minor thing "data" is already a function, so it's advisable not to use
it as a name - if  only to reduce possible confusion.

Also, it's not obvious that the process by which raster(file) goes though
(it passes down to rgdal::readGDAL) will result in a correct interpretation
as the data source was intended, since GRIB is a domain-specific format for
time-series and volume-slice series data and the required translation to
the GDAL and raster data models is not always straight-forward.

It totally can work though, I do it routine with many sources but they all
need some oversight to ensure that upfront translation is complete and
appropriate.

It may just require setting raster::extent and/or raster::projection, but
there's no way to know without exploration.

Cheers, Mike.



>     data_dams_size<-extract(data,pts_dams)
>     DF_data_dams_size<- as.data.frame(data_dams_size)
>
> Unfortunately, I get mostly NAs for the data, it seems that there's an
> issue with the CRS projections for the city coordinates. Is there a
> specific projection for city level coordinates? Or am I doing something
> completely wrong? Thank you!
>
> Sincerely,
>
> Milu
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From javier.garcia at ehu.eus  Sat Jul 29 03:51:23 2017
From: javier.garcia at ehu.eus (=?iso-8859-1?Q?Javier_Garc=EDa?=)
Date: Sat, 29 Jul 2017 03:51:23 +0200
Subject: [R-sig-Geo] Differences between moran.test and lm.morantest
Message-ID: <001a01d3080d$2f0d3c70$8d27b550$@ehu.eus>

Hello everybody:

 

Currently I am working on a paper in which we need to analyze the presence
of possible spatial correlation in the data. With this aim I am running some
tests in R. I am a little bit confused about the differences between
moran.test and lm.morantest R functions. The problem I face to is that when
I run moran.test on my  regression residuals the result is totally different
from the one I obtain when I use lm.morantest with the lm regression object
(please, see below the different outputs I get and after it a reproducible
example). In particular, whereas the observed Moran I is the same, the
expectation and variance differ dramatically, getting opposite conclusions.
I would appreciate very much if someone could clarify for me which is the
cause behind this. By the way, I also run LM tests (LMerr, RLMerr, LMlag and
RLMlag) not rejecting the null hypothesis in any of them (all p-values are
higher than 0.7), which is in clear contradiction with the lm.morantest
 how
is this possible?

 

 

MY PARTICULAR CASE 

 

reg.OLS <- lm(y~z1+z2+z3+z4+z5+z6+z7+z8+z9+z10, data=datos)

 

moran.test(resid(reg.OLS),alternative="two.sided", W_n)

 

        Moran I test under randomisation

 

data:  resid(reg.OLS)  

weights: W_n  

 

Moran I statistic standard deviate = 0.4434, p-value = 0.6575

alternative hypothesis: two.sided

sample estimates:

Moran I statistic       Expectation          Variance 

     1.596378e-05     -3.595829e-04      7.173448e-07

 

 

 

moran.lm <-lm.morantest(reg.OLS, W_n, alternative="two.sided")

print(moran.lm)

 

        Global Moran I for regression residuals

 

data:  

model: lm(formula = y ~ z1 + z2 + z3 + z4 + z5 + z6 + z7 + z8 + z9 + z10

, data = datos)

weights: W_n

 

Moran I statistic standard deviate = 11.649, p-value < 2.2e-16

alternative hypothesis: two.sided

sample estimates:

Observed Moran I      Expectation         Variance 

    1.596378e-05    -1.913005e-03     2.741816e-08

 

 

A REPRODUCIBLE EXAMPLE

 

 

library(spdep)

data(oldcol)

oldcrime.lm <- lm(CRIME ~ HOVAL + INC + OPEN + PLUMB + DISCBD + PERIM, data
= COL.OLD)

 

moran.test(resid(oldcrime.lm), nb2listw(COL.nb, style="W"))

 

        Moran I test under randomisation

 

data:  resid(oldcrime.lm)  

weights: nb2listw(COL.nb, style = "W")  

 

Moran I statistic standard deviate = 1.2733, p-value = 0.1015

alternative hypothesis: greater

sample estimates:

Moran I statistic       Expectation          Variance 

      0.096711162      -0.020833333       0.008521765

 

 

lm.morantest(oldcrime.lm, nb2listw(COL.nb, style="W"))

 

        Global Moran I for regression residuals

 

data:  

model: lm(formula = CRIME ~ HOVAL + INC + OPEN + PLUMB + DISCBD +

PERIM, data = COL.OLD)

weights: nb2listw(COL.nb, style = "W")

 

Moran I statistic standard deviate = 1.6668, p-value = 0.04777

alternative hypothesis: greater

sample estimates:

Observed Moran I      Expectation         Variance 

     0.096711162     -0.052848581      0.008050938 

 

Thanks a lot in advance and sorry for the inconvenience.

 

Javi

 

 

	


JAVIER GARC?A 

 

Departamento de Econom?a Aplicada III (Econometr?a y Estad?stica)

Facultad de Econom?a y Empresa (Secci?n Sarriko)
Avda. Lehendakari Aguirre 83

48015 BILBAO
T.: +34 601 7126 F.: +34 601 3754
 <http://www.ehu.es/> www.ehu.es 

http://www.unibertsitate-hedakuntza.ehu.es/p268-content/es/contenidos/inform
acion/manual_id_corp/es_manual/images/firma_email_upv_euskampus_bilingue.gif

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170729/3f3c891e/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.gif
Type: image/gif
Size: 6359 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20170729/3f3c891e/attachment.gif>

From johnwasige at gmail.com  Sat Jul 29 13:56:27 2017
From: johnwasige at gmail.com (John Wasige)
Date: Sat, 29 Jul 2017 13:56:27 +0200
Subject: [R-sig-Geo] Error in basename(x) : path too long
Message-ID: <CAJgdCD4V5=QA6mGpbAwy0Lp50g6JfGsR7=-z+YusDk0nrCdMHg@mail.gmail.com>

?Dear all,

I am running the script below & I get the following error:
Error in basename(x) : path too long
?
What could be the problem?
Thanks for your help
John

?### Script?

setwd("I:/Mauritius2001_2015")  # directory of data
newlist= read.csv('I:/Mauritius2001_2015/Mauritius.csv',header=F)

refr <- raster(paste("I:/Mauritius2001_2015/",newlist[i,1],sep = ""))
refr[!is.na(refr)] <- 0
for(i in seq(1,345,by=23)){
  rsum <- refr
  for(p in 0:22){
    r <- raster(paste("I:/Mauritius2001_2015/",newlist[i+p],sep = ""))
    rsum <- rsum + r
  }
  #   rsum <- rsum
  writeRaster(rsum,
filename=paste("D:/Mauritius2001_2015/Annual/",substr(newlist[i],1,6),".tif",sep=''),
format="GTiff", overwrite=TRUE)
}

	[[alternative HTML version deleted]]


From btupper at bigelow.org  Sat Jul 29 14:30:48 2017
From: btupper at bigelow.org (Ben Tupper)
Date: Sat, 29 Jul 2017 08:30:48 -0400
Subject: [R-sig-Geo] Error in basename(x) : path too long
In-Reply-To: <CAJgdCD4V5=QA6mGpbAwy0Lp50g6JfGsR7=-z+YusDk0nrCdMHg@mail.gmail.com>
References: <CAJgdCD4V5=QA6mGpbAwy0Lp50g6JfGsR7=-z+YusDk0nrCdMHg@mail.gmail.com>
Message-ID: <2EAC64E0-5E0A-45BB-9601-EB7D860A719D@bigelow.org>

Hi,

It's not possible to know but I have a suspicion and a suggestion.

suspicion:  You are accessing the elements of the data frame "newlist" different ways ...

> refr <- raster(paste("I:/Mauritius2001_2015/",newlist[i,1],sep = ""))

and 

> r <- raster(paste("I:/Mauritius2001_2015/",newlist[i+p],sep = ""))

and 

> filename=paste("D:/Mauritius2001_2015/Annual/",substr(newlist[i],1,6),".tif",sep='')


The first likely works.  The latter two return the (i+p)'th and i'th columns each as a new data frame so the filenames may indeed be quite long.  So, it looks like you need to work on your data frame indexing. 

suggestion:  Instead of using paste() to make file paths I suggest that you give file.path() a try.  It works across all platforms and is easier than using paste.

CHeers,
Ben

> On Jul 29, 2017, at 7:56 AM, John Wasige <johnwasige at gmail.com> wrote:
> 
> ?Dear all,
> 
> I am running the script below & I get the following error:
> Error in basename(x) : path too long
> ?
> What could be the problem?
> Thanks for your help
> John
> 
> ?### Script?
> 
> setwd("I:/Mauritius2001_2015")  # directory of data
> newlist= read.csv('I:/Mauritius2001_2015/Mauritius.csv',header=F)
> 
> refr <- raster(paste("I:/Mauritius2001_2015/",newlist[i,1],sep = ""))
> refr[!is.na(refr)] <- 0
> for(i in seq(1,345,by=23)){
>  rsum <- refr
>  for(p in 0:22){
>    r <- raster(paste("I:/Mauritius2001_2015/",newlist[i+p],sep = ""))
>    rsum <- rsum + r
>  }
>  #   rsum <- rsum
>  writeRaster(rsum,
> filename=paste("D:/Mauritius2001_2015/Annual/",substr(newlist[i],1,6),".tif",sep=''),
> format="GTiff", overwrite=TRUE)
> }
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecocast Reports: http://seascapemodeling.org/ecocast.html
Tick Reports: https://report.bigelow.org/tick/
Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/


From btupper at bigelow.org  Sat Jul 29 17:37:24 2017
From: btupper at bigelow.org (Ben Tupper)
Date: Sat, 29 Jul 2017 11:37:24 -0400
Subject: [R-sig-Geo] Error in basename(x) : path too long
In-Reply-To: <CAJgdCD4V5=QA6mGpbAwy0Lp50g6JfGsR7=-z+YusDk0nrCdMHg@mail.gmail.com>
References: <CAJgdCD4V5=QA6mGpbAwy0Lp50g6JfGsR7=-z+YusDk0nrCdMHg@mail.gmail.com>
Message-ID: <527BFC03-5E04-4DC4-891E-1CB881579C30@bigelow.org>

Hi again,

A late thought - I'm still on the first cups of coffee.

It looks to me like you are iterating over a stack to select certain layers to sum.  You could achieve the same outcome with possibly much less work.  The following example will create a sum of 24-layer blocks along a stack of rasters. 

# from https://gist.github.com/btupper/20879e0b46e5ed63d402d7cff424dbb7
#' Split a vector into groups of MAX (or possibly fewer)
#'
#' @param v vector or list to split
#' @param MAX numeric the maximum size per group
#' @return a list of the vector split into groups
split_vector <- function(v, MAX = 200){
    nv <- length(v)
    if (nv <= MAX) return(list('1' = v))
    split(v, findInterval(1:nv, seq(from = 1, to = nv, by = MAX)))
}


library(raster)

N <- 345
n <- 24
nc <- 4
nr <- 3

R <- raster(matrix(runif(nc*nr), ncol = nc, nrow = nr))

RR <- stack(lapply(seq_len(N), function(i) R))

ix <- split_vector(seq_len(N), MAX = n)

SS <- lapply(ix, function(index) sum(RR[[index]]))

So, S[[1]], which looks like this...

$`1`
class       : RasterLayer 
dimensions  : 3, 4, 12  (nrow, ncol, ncell)
resolution  : 0.25, 0.3333333  (x, y)
extent      : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)
coord. ref. : NA 
data source : in memory
names       : layer 
values      : 0.9451534, 20.0503  (min, max)

... is the sum of the first 24 layers of RR. SS[[2]] will be the sum of the next 24, and so on.

Is that what you are trying to do?

Cheers,
Ben




> On Jul 29, 2017, at 7:56 AM, John Wasige <johnwasige at gmail.com> wrote:
> 
> ?Dear all,
> 
> I am running the script below & I get the following error:
> Error in basename(x) : path too long
> ?
> What could be the problem?
> Thanks for your help
> John
> 
> ?### Script?
> 
> setwd("I:/Mauritius2001_2015")  # directory of data
> newlist= read.csv('I:/Mauritius2001_2015/Mauritius.csv',header=F)
> 
> refr <- raster(paste("I:/Mauritius2001_2015/",newlist[i,1],sep = ""))
> refr[!is.na(refr)] <- 0
> for(i in seq(1,345,by=23)){
>  rsum <- refr
>  for(p in 0:22){
>    r <- raster(paste("I:/Mauritius2001_2015/",newlist[i+p],sep = ""))
>    rsum <- rsum + r
>  }
>  #   rsum <- rsum
>  writeRaster(rsum,
> filename=paste("D:/Mauritius2001_2015/Annual/",substr(newlist[i],1,6),".tif",sep=''),
> format="GTiff", overwrite=TRUE)
> }
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecocast Reports: http://seascapemodeling.org/ecocast.html
Tick Reports: https://report.bigelow.org/tick/
Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/


From rafa.pereira.br at gmail.com  Sun Jul 30 00:38:49 2017
From: rafa.pereira.br at gmail.com (Rafael Pereira)
Date: Sat, 29 Jul 2017 23:38:49 +0100
Subject: [R-sig-Geo] bivariate spatial correlation in R
In-Reply-To: <alpine.LFD.2.20.1707261152010.18617@reclus.nhh.no>
References: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>
 <alpine.LFD.2.20.1707242012390.6677@reclus.nhh.no>
 <CAA42DGm1dhzEvN9vYyRfKS7XuPVnXFgVBfRHWFLGS29H4R65Cw@mail.gmail.com>
 <alpine.LFD.2.20.1707261152010.18617@reclus.nhh.no>
Message-ID: <CAA42DGkhtyTHpW3ZoFAOHdDcjY6iK0LZpZFdkvnmihieujzbGg@mail.gmail.com>

Hi all,

here is a reproducible example to calculate in R bivariate Moran's I and
LISA clusters. This example is based on a this answer provided in SO* and
it uses a toy model of my data. The R script and the shape file with the
data are available on this link.
https://gist.github.com/rafapereirabr/5348193abf779625f5e8c5090776a228

What this example does is to estimate the spatial association between
household income per capita and the gains in accessibility to jobs. The aim
is to analyze who benefits the recent changes in the transport system in
terms of access to jobs. So the idea is not to find causal relationships,
but spatial association between areas of high/low income who had high/low
gains in accessibility.

The variables in the data show info on the proportion of jobs accessible in
both years 2014 and 2017 (access2014, access2017) and the difference
between the two years in percentage points (diffaccess).

Roger, I know you have shown to be a bit sceptical about this application
of bivariate Moran's I. Do you still think a spatial regression would be
more appropriate?

Also, I would be glad to hear if others have comments on the code. This
function is not implemented in any package so it would be great to have
some feedback.

Rafael H M Pereira
urbandemographics.blogspot.com







*
https://stackoverflow.com/questions/45177590/map-of-bivariate-spatial-correlation-in-r-bivariate-lisa



On Wed, Jul 26, 2017 at 11:07 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Wed, 26 Jul 2017, Rafael Pereira wrote:
>
> Roger,
>>
>> This example was provided only for the sake or making the code easily
>> reproducible for others and I'm more interested in how the bi-variate
>> Moran
>> could be implemented in R, but your comments are very much welcomed and
>> I've made changes to the question.
>>
>> My actual case study looks at bi-variate spatial correlation between (a)
>> average household income per capita and (b) proportion of jobs in the city
>> that are accessible under 60 minutes by transit. I don't think I could use
>> rates in this case but I will normalize the variables using
>> scale(data$variable).
>>
>
> Please provide a reproducible example, either with a link to a data
> subset, or using a builtin data set. My guess is that you do not need
> bi-variate spatial correlation at all, but rather a spatial regression.
>
> The "causal" variable would then the the proportion of jobs accessible
> within 60 minutes by transit, though this is extremely blunt, and lots of
> other covariates (demography, etc.) impact average household income per
> capita (per block/tract?). Since there are many missing variables in your
> specification, any spatial correlation would be most closely associated
> with them (demography, housing costs, education, etc.), and the choice of
> units of measurement would dominate the outcome.
>
> This is also why bi-variate spatial correlation is seldom a good idea, I
> believe. It can be done, but most likely shouldn't, unless it can be
> motivated properly.
>
> By the way, the weighted and FDR-corrected SAD local Moran's I p-values of
> the black/white ratio for Oregon (your toy example) did deliver the goods -
> if you zoom in in mapview::mapview, you can see that it detects a rate
> hotspot between the rivers.
>
> Roger
>
>
>
>> best,
>>
>> Rafael H M Pereira
>>
>> On Mon, Jul 24, 2017 at 7:56 PM, Roger Bivand <Roger.Bivand at nhh.no>
>> wrote:
>>
>> On Mon, 24 Jul 2017, Rafael Pereira wrote:
>>>
>>> Hi all,
>>>
>>>>
>>>> I would like to ask whether some you conducted bi-variate spatial
>>>> correlation in R.
>>>>
>>>> I know the bi-variate Moran's I is not implemented in the spdep library.
>>>> I left a question on SO but also wanted to hear if anyone if the
>>>> mainlist
>>>> have come across this.
>>>> https://stackoverflow.com/questions/45177590/map-of-bivariat
>>>> e-spatial-correlation-in-r-bivariate-lisa
>>>>
>>>> I also know Roger Bivand has implemented the L index proposed by Lee
>>>> (2001)
>>>> in spdep, but I'm not I'm not sure whether the L local correlation
>>>> coefficients can be interpreted the same way as the local Moran's I
>>>> coefficients. I couldn't find any reference commenting on this issue. I
>>>> would very much appreciate your thoughts this.
>>>>
>>>>
>>> In the SO question, and in the follow-up, your presumably throw-away
>>> example makes fundamental mistakes. The code in spdep by Virgilio
>>> G?mez-Rubio is for uni- and bivariate L, and produces point values of
>>> local
>>> L. This isn't the main problem, which is rather that you are not taking
>>> account of the underlying population counts, nor shrinking any estimates
>>> of
>>> significance to accommodate population sizes. Population sizes vary from
>>> 0
>>> to 11858, with the lower quartile at 3164 and upper 5698:
>>> plot(ecdf(oregon.tract$pop2000)). Should you be comparing rates in
>>> stead?
>>> These are also compositional variables (sum to pop2000, or 1 if rates)
>>> with
>>> the other missing components. You would probably be better served by
>>> tools
>>> examining spatial segregation, such as for example the seg package.
>>>
>>> The 0 count populations cause problems for an unofficial alternative, the
>>> black/white ratio:
>>>
>>> oregon.tract1 <- oregon.tract[oregon.tract$white > 0,]
>>> oregon.tract1$rat <- oregon.tract1$black/oregon.tract1$white
>>> nb <- poly2nb(oregon.tract1)
>>> lw <- nb2listw(nb)
>>>
>>> which should still be adjusted by weighting:
>>>
>>> lm0 <- lm(rat ~ 1, weights=pop2000, data=oregon.tract1)
>>>
>>> I'm not advising this, but running localmoran.sad on this model output
>>> yields SAD p-values < 0.05 after FDR correction only in contiguous tracts
>>> on the Washington state line in Portland between the Columbia and
>>> Williamette rivers. So do look at the variables you are using before
>>> rushing into things.
>>>
>>> Hope this clarifies,
>>>
>>> Roger
>>>
>>>
>>> best,
>>>>
>>>> Rafael HM Pereira
>>>> http://urbandemographics.blogspot.com
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>
>>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
>>> Editor-in-Chief of The R Journal, https://journal.r-project.org/
>>> index.html
>>> http://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
> Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Sun Jul 30 08:41:56 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 30 Jul 2017 06:41:56 +0000
Subject: [R-sig-Geo] bivariate spatial correlation in R
In-Reply-To: <CAA42DGkhtyTHpW3ZoFAOHdDcjY6iK0LZpZFdkvnmihieujzbGg@mail.gmail.com>
References: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>
 <alpine.LFD.2.20.1707242012390.6677@reclus.nhh.no>
 <CAA42DGm1dhzEvN9vYyRfKS7XuPVnXFgVBfRHWFLGS29H4R65Cw@mail.gmail.com>
 <alpine.LFD.2.20.1707261152010.18617@reclus.nhh.no>,
 <CAA42DGkhtyTHpW3ZoFAOHdDcjY6iK0LZpZFdkvnmihieujzbGg@mail.gmail.com>
Message-ID: <6124293B00A50295.e0f9c23d-4edb-4035-aab0-756e53b6db82@mail.outlook.com>

Thanks, I'll get back when able, offline now. What are the units of observation, and are aggregate household incomes observed only once?

Roger

Roger Bivand
Norwegian School of Economics
Bergen, Norway



Fra: Rafael Pereira
Sendt: s?ndag 30. juli, 00.39
Emne: Re: [R-sig-Geo] bivariate spatial correlation in R
Kopi: Rog?rio Barbosa, r-sig-geo at r-project.org


Hi all, here is a reproducible example to calculate in R bivariate Moran's I and LISA clusters. This example is based on a this answer provided in SO* and it uses a toy model of my data. The R script and the shape file with the data are available on this link. https://gist.github.com/rafapereirabr/5348193abf779625f5e8c5090776a228 What this example does is to estimate the spatial association between household income per capita and the gains in accessibility to jobs. The aim is to analyze who benefits the recent changes in the transport system in terms of access to jobs. So the idea is not to find causal relationships, but spatial association between areas of high/low income who had high/low gains in accessibility. The variables in the data show info on the proportion of jobs accessible in both years 2014 and 2017 (access2014, access2017) and the difference between the two years in percentage points (diffaccess). Roger, I know you have shown to be a bit sceptical about this application of bivariate Moran's I. Do you still think a spatial regression would be more appropriate? Also, I would be glad to hear if others have comments on the code. This function is not implemented in any package so it would be great to have some feedback. Rafael H M Pereira urbandemographics.blogspot.com * https://stackoverflow.com/questions/45177590/map-of-bivariate-spatial-correlation-in-r-bivariate-lisa On Wed, Jul 26, 2017 at 11:07 AM, Roger Bivand wrote: > On Wed, 26 Jul 2017, Rafael Pereira wrote: > > Roger, >> >> This example was provided only for the sake or making the code easily >> reproducible for others and I'm more interested in how the bi-variate >> Moran >> could be implemented in R, but your comments are very much welcomed and >> I've made changes to the question. >> >> My actual case study looks at bi-variate spatial correlation between (a) >> average household income per capita and (b) proportion of jobs in the city >> that are accessible under 60 minutes by transit. I don't think I could use >> rates in this case but I will normalize the variables using >> scale(data$variable). >> > > Please provide a reproducible example, either with a link to a data > subset, or using a builtin data set. My guess is that you do not need > bi-variate spatial correlation at all, but rather a spatial regression. > > The "causal" variable would then the the proportion of jobs accessible > within 60 minutes by transit, though this is extremely blunt, and lots of > other covariates (demography, etc.) impact average household income per > capita (per block/tract?). Since there are many missing variables in your > specification, any spatial correlation would be most closely associated > with them (demography, housing costs, education, etc.), and the choice of > units of measurement would dominate the outcome. > > This is also why bi-variate spatial correlation is seldom a good idea, I > believe. It can be done, but most likely shouldn't, unless it can be > motivated properly. > > By the way, the weighted and FDR-corrected SAD local Moran's I p-values of > the black/white ratio for Oregon (your toy example) did deliver the goods - > if you zoom in in mapview::mapview, you can see that it detects a rate > hotspot between the rivers. > > Roger > > > >> best, >> >> Rafael H M Pereira >> >> On Mon, Jul 24, 2017 at 7:56 PM, Roger Bivand >> wrote: >> >> On Mon, 24 Jul 2017, Rafael Pereira wrote: >>> >>> Hi all, >>> >>>> >>>> I would like to ask whether some you conducted bi-variate spatial >>>> correlation in R. >>>> >>>> I know the bi-variate Moran's I is not implemented in the spdep library. >>>> I left a question on SO but also wanted to hear if anyone if the >>>> mainlist >>>> have come across this. >>>> https://stackoverflow.com/questions/45177590/map-of-bivariat >>>> e-spatial-correlation-in-r-bivariate-lisa >>>> >>>> I also know Roger Bivand has implemented the L index proposed by Lee >>>> (2001) >>>> in spdep, but I'm not I'm not sure whether the L local correlation >>>> coefficients can be interpreted the same way as the local Moran's I >>>> coefficients. I couldn't find any reference commenting on this issue. I >>>> would very much appreciate your thoughts this. >>>> >>>> >>> In the SO question, and in the follow-up, your presumably throw-away >>> example makes fundamental mistakes. The code in spdep by Virgilio >>> G?mez-Rubio is for uni- and bivariate L, and produces point values of >>> local >>> L. This isn't the main problem, which is rather that you are not taking >>> account of the underlying population counts, nor shrinking any estimates >>> of >>> significance to accommodate population sizes. Population sizes vary from >>> 0 >>> to 11858, with the lower quartile at 3164 and upper 5698: >>> plot(ecdf(oregon.tract$pop2000)). Should you be comparing rates in >>> stead? >>> These are also compositional variables (sum to pop2000, or 1 if rates) >>> with >>> the other missing components. You would probably be better served by >>> tools >>> examining spatial segregation, such as for example the seg package. >>> >>> The 0 count populations cause problems for an unofficial alternative, the >>> black/white ratio: >>> >>> oregon.tract1 0,] >>> oregon.tract1$rat >> nb >> lw >> >>> which should still be adjusted by weighting: >>> >>> lm0 >> >>> I'm not advising this, but running localmoran.sad on this model output >>> yields SAD p-values < 0.05 after FDR correction only in contiguous tracts >>> on the Washington state line in Portland between the Columbia and >>> Williamette rivers. So do look at the variables you are using before >>> rushing into things. >>> >>> Hope this clarifies, >>> >>> Roger >>> >>> >>> best, >>>> >>>> Rafael HM Pereira >>>> http://urbandemographics.blogspot.com >>>> >>>> [[alternative HTML version deleted]] >>>> >>>> _______________________________________________ >>>> R-sig-Geo mailing list >>>> R-sig-Geo at r-project.org >>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo >>>> >>>> >>>> -- >>> Roger Bivand >>> Department of Economics, Norwegian School of Economics, >>> Helleveien 30, N-5045 Bergen, Norway. >>> voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no >>> Editor-in-Chief of The R Journal, https://journal.r-project.org/ >>> index.html >>> http://orcid.org/0000-0003-2392-6140 >>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en >>> >> >> [[alternative HTML version deleted]] >> >> _______________________________________________ >> R-sig-Geo mailing list >> R-sig-Geo at r-project.org >> https://stat.ethz.ch/mailman/listinfo/r-sig-geo >> > > -- > Roger Bivand > Department of Economics, Norwegian School of Economics, > Helleveien 30, N-5045 Bergen, Norway. > voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no > Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html > http://orcid.org/0000-0003-2392-6140 > https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en > [[alternative HTML version deleted]] _______________________________________________ R-sig-Geo mailing list R-sig-Geo at r-project.org https://stat.ethz.ch/mailman/listinfo/r-sig-geo


	[[alternative HTML version deleted]]


From rafa.pereira.br at gmail.com  Sun Jul 30 16:31:45 2017
From: rafa.pereira.br at gmail.com (Rafael Pereira)
Date: Sun, 30 Jul 2017 15:31:45 +0100
Subject: [R-sig-Geo] bivariate spatial correlation in R
In-Reply-To: <6124293B00A50295.e0f9c23d-4edb-4035-aab0-756e53b6db82@mail.outlook.com>
References: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>
 <alpine.LFD.2.20.1707242012390.6677@reclus.nhh.no>
 <CAA42DGm1dhzEvN9vYyRfKS7XuPVnXFgVBfRHWFLGS29H4R65Cw@mail.gmail.com>
 <alpine.LFD.2.20.1707261152010.18617@reclus.nhh.no>
 <CAA42DGkhtyTHpW3ZoFAOHdDcjY6iK0LZpZFdkvnmihieujzbGg@mail.gmail.com>
 <6124293B00A50295.e0f9c23d-4edb-4035-aab0-756e53b6db82@mail.outlook.com>
Message-ID: <CAA42DGk8agiCMPp5FLwSrd1wszaVcNHd5uB6fCkdT9rb8QHLOw@mail.gmail.com>

Roger,

Population and income data are single point in time and come from the 2010
Census.

Accessibility variables in 2014 and 2017 show the proportion of jobs
accessible by public transport under 60 minutes. The variable diffaccess
shows the difference between these two. It's in percentage points
(access2017 - access2014)

best,

Rafael H M Pereira
urbandemographics.blogspot.com

On Sun, Jul 30, 2017 at 7:41 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> Thanks, I'll get back when able, offline now. What are the units of
> observation, and are aggregate household incomes observed only once?
>
> Roger
>
> Roger Bivand
> Norwegian School of Economics
> Bergen, Norway
>
>
>
> Fra: Rafael Pereira
> Sendt: s?ndag 30. juli, 00.39
> Emne: Re: [R-sig-Geo] bivariate spatial correlation in R
> Kopi: Rog?rio Barbosa, r-sig-geo at r-project.org
>
>
> Hi all, here is a reproducible example to calculate in R bivariate Moran's
> I and LISA clusters. This example is based on a this answer provided in SO*
> and it uses a toy model of my data. The R script and the shape file with
> the data are available on this link. https://gist.github.com/
> rafapereirabr/5348193abf779625f5e8c5090776a228 What this example does is
> to estimate the spatial association between household income per capita and
> the gains in accessibility to jobs. The aim is to analyze who benefits the
> recent changes in the transport system in terms of access to jobs. So the
> idea is not to find causal relationships, but spatial association between
> areas of high/low income who had high/low gains in accessibility. The
> variables in the data show info on the proportion of jobs accessible in
> both years 2014 and 2017 (access2014, access2017) and the difference
> between the two years in percentage points (diffaccess). Roger, I know you
> have shown to be a bit sceptical about this application of bivariate
> Moran's I. Do you still think a spatial regression would be more
> appropriate? Also, I would be glad to hear if others have comments on the
> code. This function is not implemented in any package so it would be great
> to have some feedback. Rafael H M Pereira urbandemographics.blogspot.com
> * https://stackoverflow.com/questions/45177590/map-of-
> bivariate-spatial-correlation-in-r-bivariate-lisa On Wed, Jul 26, 2017 at
> 11:07 AM, Roger Bivand wrote: > On Wed, 26 Jul 2017, Rafael Pereira wrote:
> > > Roger, >> >> This example was provided only for the sake or making the
> code easily >> reproducible for others and I'm more interested in how the
> bi-variate >> Moran >> could be implemented in R, but your comments are
> very much welcomed and >> I've made changes to the question. >> >> My
> actual case study looks at bi-variate spatial correlation between (a) >>
> average household income per capita and (b) proportion of jobs in the city
> >> that are accessible under 60 minutes by transit. I don't think I could
> use >> rates in this case but I will normalize the variables using >>
> scale(data$variable). >> > > Please provide a reproducible example, either
> with a link to a data > subset, or using a builtin data set. My guess is
> that you do not need > bi-variate spatial correlation at all, but rather a
> spatial regression. > > The "causal" variable would then the the proportion
> of jobs accessible > within 60 minutes by transit, though this is extremely
> blunt, and lots of > other covariates (demography, etc.) impact average
> household income per > capita (per block/tract?). Since there are many
> missing variables in your > specification, any spatial correlation would be
> most closely associated > with them (demography, housing costs, education,
> etc.), and the choice of > units of measurement would dominate the outcome.
> > > This is also why bi-variate spatial correlation is seldom a good idea,
> I > believe. It can be done, but most likely shouldn't, unless it can be >
> motivated properly. > > By the way, the weighted and FDR-corrected SAD
> local Moran's I p-values of > the black/white ratio for Oregon (your toy
> example) did deliver the goods - > if you zoom in in mapview::mapview, you
> can see that it detects a rate > hotspot between the rivers. > > Roger > >
> > >> best, >> >> Rafael H M Pereira >> >> On Mon, Jul 24, 2017 at 7:56 PM,
> Roger Bivand >> wrote: >> >> On Mon, 24 Jul 2017, Rafael Pereira wrote: >>>
> >>> Hi all, >>> >>>> >>>> I would like to ask whether some you conducted
> bi-variate spatial >>>> correlation in R. >>>> >>>> I know the bi-variate
> Moran's I is not implemented in the spdep library. >>>> I left a question
> on SO but also wanted to hear if anyone if the >>>> mainlist >>>> have come
> across this. >>>> https://stackoverflow.com/questions/45177590/map-of-
> bivariat >>>> e-spatial-correlation-in-r-bivariate-lisa >>>> >>>> I also
> know Roger Bivand has implemented the L index proposed by Lee >>>> (2001)
> >>>> in spdep, but I'm not I'm not sure whether the L local correlation
> >>>> coefficients can be interpreted the same way as the local Moran's I
> >>>> coefficients. I couldn't find any reference commenting on this issue.
> I >>>> would very much appreciate your thoughts this. >>>> >>>> >>> In the
> SO question, and in the follow-up, your presumably throw-away >>> example
> makes fundamental mistakes. The code in spdep by Virgilio >>> G?mez-Rubio
> is for uni- and bivariate L, and produces point values of >>> local >>> L.
> This isn't the main problem, which is rather that you are not taking >>>
> account of the underlying population counts, nor shrinking any estimates
> >>> of >>> significance to accommodate population sizes. Population sizes
> vary from >>> 0 >>> to 11858, with the lower quartile at 3164 and upper
> 5698: >>> plot(ecdf(oregon.tract$pop2000)). Should you be comparing rates
> in >>> stead? >>> These are also compositional variables (sum to pop2000,
> or 1 if rates) >>> with >>> the other missing components. You would
> probably be better served by >>> tools >>> examining spatial segregation,
> such as for example the seg package. >>> >>> The 0 count populations cause
> problems for an unofficial alternative, the >>> black/white ratio: >>> >>>
> oregon.tract1 0,] >>> oregon.tract1$rat >> nb >> lw >> >>> which should
> still be adjusted by weighting: >>> >>> lm0 >> >>> I'm not advising this,
> but running localmoran.sad on this model output >>> yields SAD p-values <
> 0.05 after FDR correction only in contiguous tracts >>> on the Washington
> state line in Portland between the Columbia and >>> Williamette rivers. So
> do look at the variables you are using before >>> rushing into things. >>>
> >>> Hope this clarifies, >>> >>> Roger >>> >>> >>> best, >>>> >>>> Rafael
> HM Pereira >>>> http://urbandemographics.blogspot.com >>>> >>>>
> [[alternative HTML version deleted]] >>>> >>>>
> _______________________________________________ >>>> R-sig-Geo mailing
> list >>>> R-sig-Geo at r-project.org >>>> https://stat.ethz.ch/mailman/
> listinfo/r-sig-geo >>>> >>>> >>>> -- >>> Roger Bivand >>> Department of
> Economics, Norwegian School of Economics, >>> Helleveien 30, N-5045 Bergen,
> Norway. >>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
> Roger.Bivand at nhh.no >>> Editor-in-Chief of The R Journal,
> https://journal.r-project.org/ >>> index.html >>>
> http://orcid.org/0000-0003-2392-6140 >>> https://scholar.google.no/
> citations?user=AWeghB0AAAAJ&hl=en >>> >> >> [[alternative HTML version
> deleted]] >> >> _______________________________________________ >>
> R-sig-Geo mailing list >> R-sig-Geo at r-project.org >>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo >> > > -- > Roger Bivand
> > Department of Economics, Norwegian School of Economics, > Helleveien 30,
> N-5045 Bergen, Norway. > voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>;
> e-mail: Roger.Bivand at nhh.no > Editor-in-Chief of The R Journal,
> https://journal.r-project.org/index.html > http://orcid.org/0000-0003-
> 2392-6140 > https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en >
> [[alternative HTML version deleted]] _______________________________________________
> R-sig-Geo mailing list R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Sun Jul 30 18:42:07 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 30 Jul 2017 18:42:07 +0200
Subject: [R-sig-Geo] Differences between moran.test and lm.morantest
In-Reply-To: <001a01d3080d$2f0d3c70$8d27b550$@ehu.eus>
References: <001a01d3080d$2f0d3c70$8d27b550$@ehu.eus>
Message-ID: <alpine.LFD.2.20.1707301818590.10808@reclus.nhh.no>

On Sat, 29 Jul 2017, Javier Garc?a wrote:

> Hello everybody:
>
>
>
> Currently I am working on a paper in which we need to analyze the presence
> of possible spatial correlation in the data. With this aim I am running some
> tests in R. I am a little bit confused about the differences between
> moran.test and lm.morantest R functions. The problem I face to is that when
> I run moran.test on my  regression residuals the result is totally different
> from the one I obtain when I use lm.morantest with the lm regression object
> (please, see below the different outputs I get and after it a reproducible
> example). In particular, whereas the observed Moran I is the same, the
> expectation and variance differ dramatically, getting opposite conclusions.
> I would appreciate very much if someone could clarify for me which is the
> cause behind this. By the way, I also run LM tests (LMerr, RLMerr, LMlag and
> RLMlag) not rejecting the null hypothesis in any of them (all p-values are
> higher than 0.7), which is in clear contradiction with the lm.morantest? how
> is this possible?
>

moran.test() is for "primary" variables only - read the reference in 
?moran.test. The mean model applied to the this variable is the intercept, 
that is the mean only:

> moran.test(COL.OLD$CRIME, nb2listw(COL.nb, style="W"), 
+ randomisation=FALSE, alternative="two.sided")

 	Moran I test under normality

data:  COL.OLD$CRIME
weights: nb2listw(COL.nb, style = "W")

Moran I statistic standard deviate = 5.6754, p-value = 1.384e-08
alternative hypothesis: two.sided
sample estimates:
Moran I statistic       Expectation          Variance
       0.510951264      -0.020833333       0.008779831

under the Normal assumption.

lm.morantest() can reproduce the same results for the same model:

> lm.morantest(lm(CRIME ~ 1, data=COL.OLD), nb2listw(COL.nb, style="W"), 
+ alternative="two.sided")

 	Global Moran I for regression residuals

data:
model: lm(formula = CRIME ~ 1, data = COL.OLD)
weights: nb2listw(COL.nb, style = "W")

Moran I statistic standard deviate = 5.6754, p-value = 1.384e-08
alternative hypothesis: two.sided
sample estimates:
Observed Moran I      Expectation         Variance
      0.510951264     -0.020833333      0.008779831

Note that the Normal VI for moran.test() is:

         VI <- (wc$nn * wc$S1 - wc$n * wc$S2 + 3 * S02)/(S02 *
             (wc$nn - 1))

and for lm.morantest():

     XtXinv <- chol2inv(model$qr$qr[p1, p1, drop = FALSE])
     X <- model.matrix(terms(model), model.frame(model))
...
     Z <- lag.listw(listw.U, X, zero.policy = zero.policy)
     C1 <- t(X) %*% Z
     trA <- (sum(diag(XtXinv %*% C1)))
     EI <- -((N * trA)/((N - p) * S0))
     C2 <- t(Z) %*% Z
     C3 <- XtXinv %*% C1
     trA2 <- sum(diag(C3 %*% C3))
     trB <- sum(diag(4 * (XtXinv %*% C2)))
     VI <- (((N * N)/((S0 * S0) * (N - p) * (N - p + 2))) * (S1 +
         2 * trA2 - trB - ((2 * (trA^2))/(N - p))))

where you see easily that the issue of dependent residuals (only n-p are 
independent even in the aspatial case, so you see n-p instead of n) is 
handled, as are products of X, WX, and (X'X)^{-1}.

Reading the references is essential and should not be neglected. Reading 
the code may help, but doesn't explain the reasoning behind the code and 
the output. If you need to test model residuals, use the appropriate test. 
Using moran.test() on extracted residuals if covariates are included in 
the model is never justified.

Different outcomes from Moran's I and LM suggest severe mis-specification 
in your model, see for example:

https://groups.google.com/forum/#!msg/openspace-list/k4F4jI9cU1I/s5bj8r4nwn4J

for a simplified flowchart for choosing models.

Hope this clarifies,

Roger

>
>
>
>
> MY PARTICULAR CASE
>
>
>
> reg.OLS <- lm(y~z1+z2+z3+z4+z5+z6+z7+z8+z9+z10, data=datos)
>
>
>
> moran.test(resid(reg.OLS),alternative="two.sided", W_n)
>
>
>
>        Moran I test under randomisation
>
>
>
> data:  resid(reg.OLS)
>
> weights: W_n
>
>
>
> Moran I statistic standard deviate = 0.4434, p-value = 0.6575
>
> alternative hypothesis: two.sided
>
> sample estimates:
>
> Moran I statistic       Expectation          Variance
>
>     1.596378e-05     -3.595829e-04      7.173448e-07
>
>
>
>
>
>
>
> moran.lm <-lm.morantest(reg.OLS, W_n, alternative="two.sided")
>
> print(moran.lm)
>
>
>
>        Global Moran I for regression residuals
>
>
>
> data:
>
> model: lm(formula = y ~ z1 + z2 + z3 + z4 + z5 + z6 + z7 + z8 + z9 + z10
>
> , data = datos)
>
> weights: W_n
>
>
>
> Moran I statistic standard deviate = 11.649, p-value < 2.2e-16
>
> alternative hypothesis: two.sided
>
> sample estimates:
>
> Observed Moran I      Expectation         Variance
>
>    1.596378e-05    -1.913005e-03     2.741816e-08
>
>
>
>
>
> A REPRODUCIBLE EXAMPLE
>
>
>
>
>
> library(spdep)
>
> data(oldcol)
>
> oldcrime.lm <- lm(CRIME ~ HOVAL + INC + OPEN + PLUMB + DISCBD + PERIM, data
> = COL.OLD)
>
>
>
> moran.test(resid(oldcrime.lm), nb2listw(COL.nb, style="W"))
>
>
>
>        Moran I test under randomisation
>
>
>
> data:  resid(oldcrime.lm)
>
> weights: nb2listw(COL.nb, style = "W")
>
>
>
> Moran I statistic standard deviate = 1.2733, p-value = 0.1015
>
> alternative hypothesis: greater
>
> sample estimates:
>
> Moran I statistic       Expectation          Variance
>
>      0.096711162      -0.020833333       0.008521765
>
>
>
>
>
> lm.morantest(oldcrime.lm, nb2listw(COL.nb, style="W"))
>
>
>
>        Global Moran I for regression residuals
>
>
>
> data:
>
> model: lm(formula = CRIME ~ HOVAL + INC + OPEN + PLUMB + DISCBD +
>
> PERIM, data = COL.OLD)
>
> weights: nb2listw(COL.nb, style = "W")
>
>
>
> Moran I statistic standard deviate = 1.6668, p-value = 0.04777
>
> alternative hypothesis: greater
>
> sample estimates:
>
> Observed Moran I      Expectation         Variance
>
>     0.096711162     -0.052848581      0.008050938
>
>
>
> Thanks a lot in advance and sorry for the inconvenience.
>
>
>
> Javi
>
>
>
>
>
>
>
>
> JAVIER GARC?A
>
>
>
> Departamento de Econom?a Aplicada III (Econometr?a y Estad?stica)
>
> Facultad de Econom?a y Empresa (Secci?n Sarriko)
> Avda. Lehendakari Aguirre 83
>
> 48015 BILBAO
> T.: +34 601 7126 F.: +34 601 3754
> <http://www.ehu.es/> www.ehu.es
>
> http://www.unibertsitate-hedakuntza.ehu.es/p268-content/es/contenidos/inform
> acion/manual_id_corp/es_manual/images/firma_email_upv_euskampus_bilingue.gif
>
>
>
>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

From johnwasige at gmail.com  Mon Jul 31 11:19:27 2017
From: johnwasige at gmail.com (John Wasige)
Date: Mon, 31 Jul 2017 11:19:27 +0200
Subject: [R-sig-Geo] Error in basename(x) : path too long
In-Reply-To: <527BFC03-5E04-4DC4-891E-1CB881579C30@bigelow.org>
References: <CAJgdCD4V5=QA6mGpbAwy0Lp50g6JfGsR7=-z+YusDk0nrCdMHg@mail.gmail.com>
 <527BFC03-5E04-4DC4-891E-1CB881579C30@bigelow.org>
Message-ID: <CAJgdCD4QBfsjOGNzj85XcXon8S0zQFTAn7EeFKnnosmPMZXDAQ@mail.gmail.com>

Thank you Ben,
I would like to generate annual sums & means for each year from a list
rasters (not stack or brick because of memory issues with R). For 15 years
I have 345 individual rasters and frequency of 23 per year.

I will be very glad for an idea on how to do that.

Thanks

John





On Sat, Jul 29, 2017 at 5:37 PM, Ben Tupper <btupper at bigelow.org> wrote:

> Hi again,
>
> A late thought - I'm still on the first cups of coffee.
>
> It looks to me like you are iterating over a stack to select certain
> layers to sum.  You could achieve the same outcome with possibly much less
> work.  The following example will create a sum of 24-layer blocks along a
> stack of rasters.
>
> # from https://gist.github.com/btupper/20879e0b46e5ed63d402d7cff424dbb7
> #' Split a vector into groups of MAX (or possibly fewer)
> #'
> #' @param v vector or list to split
> #' @param MAX numeric the maximum size per group
> #' @return a list of the vector split into groups
> split_vector <- function(v, MAX = 200){
>     nv <- length(v)
>     if (nv <= MAX) return(list('1' = v))
>     split(v, findInterval(1:nv, seq(from = 1, to = nv, by = MAX)))
> }
>
>
> library(raster)
>
> N <- 345
> n <- 24
> nc <- 4
> nr <- 3
>
> R <- raster(matrix(runif(nc*nr), ncol = nc, nrow = nr))
>
> RR <- stack(lapply(seq_len(N), function(i) R))
>
> ix <- split_vector(seq_len(N), MAX = n)
>
> SS <- lapply(ix, function(index) sum(RR[[index]]))
>
> So, S[[1]], which looks like this...
>
> $`1`
> class       : RasterLayer
> dimensions  : 3, 4, 12  (nrow, ncol, ncell)
> resolution  : 0.25, 0.3333333  (x, y)
> extent      : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)
> coord. ref. : NA
> data source : in memory
> names       : layer
> values      : 0.9451534, 20.0503  (min, max)
>
> ... is the sum of the first 24 layers of RR. SS[[2]] will be the sum of
> the next 24, and so on.
>
> Is that what you are trying to do?
>
> Cheers,
> Ben
>
>
>
>
> > On Jul 29, 2017, at 7:56 AM, John Wasige <johnwasige at gmail.com> wrote:
> >
> > ?Dear all,
> >
> > I am running the script below & I get the following error:
> > Error in basename(x) : path too long
> > ?
> > What could be the problem?
> > Thanks for your help
> > John
> >
> > ?### Script?
> >
> > setwd("I:/Mauritius2001_2015")  # directory of data
> > newlist= read.csv('I:/Mauritius2001_2015/Mauritius.csv',header=F)
> >
> > refr <- raster(paste("I:/Mauritius2001_2015/",newlist[i,1],sep = ""))
> > refr[!is.na(refr)] <- 0
> > for(i in seq(1,345,by=23)){
> >  rsum <- refr
> >  for(p in 0:22){
> >    r <- raster(paste("I:/Mauritius2001_2015/",newlist[i+p],sep = ""))
> >    rsum <- rsum + r
> >  }
> >  #   rsum <- rsum
> >  writeRaster(rsum,
> > filename=paste("D:/Mauritius2001_2015/Annual/",
> substr(newlist[i],1,6),".tif",sep=''),
> > format="GTiff", overwrite=TRUE)
> > }
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
>
> Ecocast Reports: http://seascapemodeling.org/ecocast.html
> Tick Reports: https://report.bigelow.org/tick/
> Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/
>
>
>
>


-- 
John Wasige
"There are no REGRATES in LIFE, just lessons (Jennifer Aniston)?

	[[alternative HTML version deleted]]


From malNamalJa at gmx.de  Mon Jul 31 12:08:50 2017
From: malNamalJa at gmx.de (=?UTF-8?Q?=22Jannes_M=C3=BCnchow=22?=)
Date: Mon, 31 Jul 2017 12:08:50 +0200
Subject: [R-sig-Geo] RQGIS : looking for a geoalgorithm (Tristan
	Bourgeois)
In-Reply-To: <0MfEZG-1dDGzz269a-00OpOY@mail.gmx.com>
References: <mailman.3.1500631202.3181.r-sig-geo@r-project.org>
 <0MfEZG-1dDGzz269a-00OpOY@mail.gmx.com>
Message-ID: <trinity-54cf3cc9-904a-4b6f-a370-82a1d83e939a-1501495730457@3capp-gmx-bs39>

RQGIS makes use of the processing toolbox and related geoalgorithms. The Atlas tool is not part of the processing toolbox that's why find_algorithms() does not return anything. However, automatic map making is possible in R using e.g., the packages sp, sf and/or raster (tmap has been already mentioned in a previous reply). If you want to stick with the QGIS map canvas, maybe rqgisapi might let you do that (http://r-sig-geo.2731867.n2.nabble.com/Introducing-an-R-package-for-calling-functions-within-a-running-instance-of-QGIS-ALPHA-tt7591302.html).

HTH,

Jannes



> Subject: RQGIS : looking for a geoalgorithm
>
> Dear all,
> 
> Currently working on a new project I would like to create a dynamic report
> with R (using probably XML package for writting)
> 
> In this report I'll plot some graphs (I'm ok with ggplots2) but also map.
> 
>
> I just discovered the RQGIS package and I think I understood its functions.
> My question is the following one :
> 
> Is there any geoalgorithms which allows to create an atlas ?
> 
> I searched via the "find_algorithms()" but found nothing.
> 
> Thanks in advance !
> 
> Tristan.


From milujisb at gmail.com  Mon Jul 31 12:13:39 2017
From: milujisb at gmail.com (Miluji Sb)
Date: Mon, 31 Jul 2017 12:13:39 +0200
Subject: [R-sig-Geo] Problem in extracting data from GRIB files
In-Reply-To: <CAAcGz99SqFs5mutw9379-H7hbx1eP0YXLX422wAyHvoLDwkwiw@mail.gmail.com>
References: <CAMLwc7Pt-4PCFQHB9yU9YZ9eyt-bcxdScw83QUDhM=7HQ-oeQw@mail.gmail.com>
 <CAAcGz99SqFs5mutw9379-H7hbx1eP0YXLX422wAyHvoLDwkwiw@mail.gmail.com>
Message-ID: <CAMLwc7MyE2y1p5G06mC_VYWzTFV5nNzcsrvaSZQhPNGFhmWT2w@mail.gmail.com>

Dear Mike,

Thank you very much for your suggestion, apologies for the delayed reply -
I did not have access to internet over the weekend.

Here is the print-out for rdata:

class       : RasterStack
dimensions  : 150, 360, 54000, 8  (nrow, ncol, ncell, nlayers)
resolution  : 1, 1  (x, y)
extent      : -180, 180, -60, 90  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +a=6367470 +b=6367470 +no_defs
names       : GLDAS_NOA//215507.020, GLDAS_NOA//215507.020,
GLDAS_NOA//215507.020, GLDAS_NOA//215507.020, GLDAS_NOA//215507.020,
GLDAS_NOA//215517.020, GLDAS_NOA//215517.020, GLDAS_NOA//215517.020

I also made this change [rdata <-  raster::stack(lapply(filelist,
function(x) raster(x,band=12)))] but still getting NAs. The full (ugly)
code is below

I would be grateful for any help. Thank you again!

Sincerely,

Milu

####
for (y in year) {
  setwd (y)

  day <- dir(path=getwd(),full.names=TRUE, recursive=FALSE,pattern =
"^[0-9]" )

  n=0

  for(d in day) {
    setwd (d)

    n=n+1


    filelist<-list.files(pattern=".grb$")

    rdata <-  raster::stack(lapply(filelist, function(x) raster(x,band=12)))

    data_dams_size<-extract(rdata,pts_dams)
    DF_data_dams_size<- as.data.frame(data_dams_size)

    #Join ISO3, lon, lat, dams data, climate data
    joined_dams <- cbind(foo_dams, DF_data_dams_size)

    #Keep only ISO3  LON  LAT  id
    joined_dams_reduced <- joined_dams
    dams_codes <- joined_dams[,c(1:3)]

    names(joined_dams_reduced) <- gsub("GLDAS_NOAH10_3H.A", "",
names(joined_dams_reduced))
    names(joined_dams_reduced) <- gsub(".020", "",
names(joined_dams_reduced))

    #From mm/s to mm_day
    joined_dams_reduced[joined_dams_reduced==9999] <- NA

    ##for prec, snow and runoff use:
    #joined_dams_reduced$mm_day<-rowSums(joined_dams_reduced[,4:11])*10800

    ##for temperature use:
    joined_dams_reduced$mm_day<-rowMeans(joined_dams_reduced[,4:11])-273.15

    joined_mm_day<-joined_dams_reduced[,12]

    #Names
    yr_name <- substr(filelist,18,21)
    csvname <- paste(paste(yr_name, sep="_"), ".csv", sep="")
    csvname <- csvname[1]

    #Stack days to get full year
    if (n==1) {joined_mm_day_year <- joined_mm_day} else
{joined_mm_day_year <- cbind(joined_mm_day_year, joined_mm_day)}

  }

  #Join mm_day with dams codes
  joined_mm_day_year_names=cbind(dams_codes,joined_mm_day_year)

  newnames <-t(as.data.frame(c(paste ("day",
seq(1,(ncol(joined_mm_day_year_names)-3),1), sep="_"))))
  names(joined_mm_day_year_names)[4:ncol(joined_mm_day_year_names)]=
newnames


  write.csv(joined_mm_day_year_names,
file=file.path("C:/Users/Desktop/temp/",csvname))

}


On Sat, Jul 29, 2017 at 12:36 AM, Michael Sumner <mdsumner at gmail.com> wrote:

>
>
> On Sat, 29 Jul 2017 at 02:21 Miluji Sb <milujisb at gmail.com> wrote:
>
>> Dear all,
>>
>> I have a set of coordinates for cities:
>>
>> structure(list(lon = c(3.7174243, 3.2246995, 2.928656, 33.3822764,
>> 10.40237, 24.7535746, 7.2619532, -0.370679, -4.486076, -4.097899
>> ), lat = c(51.0543422, 51.209348, 51.21543, 35.1855659, 55.403756,
>> 59.4369608, 43.7101728, 49.182863, 48.390394, 47.997542)), .Names =
>> c("lon",
>> "lat"), na.action = structure(c(5L, 6L, 31L, 55L, 59L, 61L, 67L,
>> 68L), .Names = c("5", "6", "31", "55", "59", "61", "67", "68"
>> ), class = "omit"), row.names = c(1L, 2L, 3L, 4L, 7L, 8L, 9L,
>> 10L, 11L, 12L), class = "data.frame")
>>
>> I am trying to extract climatic data from GLDAS climatic data (1 degree x
>> 1
>> degree) GRIB files with the following code:
>>
>> # Convert to spatial points
>> pts_dams <- SpatialPoints(lonlat,proj4string=CRS("+proj=longlat"))
>>
>> # Extract
>>  filelist<-list.files(pattern=".grb$")
>>
>>
> This part is a bit wrong:
>
>
>>     data<-stack(filelist[1])
>>     data at layers<-sapply(filelist, function(x) raster(x,band=12))
>>
>
> Better would be
>
> rdata <-  raster::stack(lapply(filelist, function(x) raster(x,band=12)))
>
> For the rest we will need to see details about the data, so please do
>
> print(rdata)
>
> and send the resulting print-out.
>
>  A minor thing "data" is already a function, so it's advisable not to use
> it as a name - if  only to reduce possible confusion.
>
> Also, it's not obvious that the process by which raster(file) goes though
> (it passes down to rgdal::readGDAL) will result in a correct interpretation
> as the data source was intended, since GRIB is a domain-specific format for
> time-series and volume-slice series data and the required translation to
> the GDAL and raster data models is not always straight-forward.
>
> It totally can work though, I do it routine with many sources but they all
> need some oversight to ensure that upfront translation is complete and
> appropriate.
>
> It may just require setting raster::extent and/or raster::projection, but
> there's no way to know without exploration.
>
> Cheers, Mike.
>
>
>
>>     data_dams_size<-extract(data,pts_dams)
>>     DF_data_dams_size<- as.data.frame(data_dams_size)
>>
>> Unfortunately, I get mostly NAs for the data, it seems that there's an
>> issue with the CRS projections for the city coordinates. Is there a
>> specific projection for city level coordinates? Or am I doing something
>> completely wrong? Thank you!
>>
>> Sincerely,
>>
>> Milu
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>

	[[alternative HTML version deleted]]


From btupper at bigelow.org  Mon Jul 31 13:31:43 2017
From: btupper at bigelow.org (Ben Tupper)
Date: Mon, 31 Jul 2017 07:31:43 -0400
Subject: [R-sig-Geo] Error in basename(x) : path too long
In-Reply-To: <CAJgdCD4QBfsjOGNzj85XcXon8S0zQFTAn7EeFKnnosmPMZXDAQ@mail.gmail.com>
References: <CAJgdCD4V5=QA6mGpbAwy0Lp50g6JfGsR7=-z+YusDk0nrCdMHg@mail.gmail.com>
 <527BFC03-5E04-4DC4-891E-1CB881579C30@bigelow.org>
 <CAJgdCD4QBfsjOGNzj85XcXon8S0zQFTAn7EeFKnnosmPMZXDAQ@mail.gmail.com>
Message-ID: <1014FCDF-BA89-445B-B7CF-017B224F8E29@bigelow.org>

Hi John,

Using a raster::stack() should the perfect solution for you; I work with very large raster stacks all of the time.  From the introductory vignette https://cran.r-project.org/web/packages/raster/vignettes/Raster.pdf

"A notable feature of the raster package is that it can work with raster
datasets that are stored on disk and are too large to be loaded into memory
(RAM). The package can work with large files because the objects it creates
from these files only contain information about the structure of the data, such
as the number of rows and columns, the spatial extent, and the filename, but it
does not attempt to read all the cell values in memory."

Have you tried something like...

library(raster)
S <- stack(vector_of_filenames)

Ben




> On Jul 31, 2017, at 5:19 AM, John Wasige <johnwasige at gmail.com> wrote:
> 
> Thank you Ben,
> I would like to generate annual sums & means for each year from a list rasters (not stack or brick because of memory issues with R). For 15 years I have 345 individual rasters and frequency of 23 per year. 
> 
> I will be very glad for an idea on how to do that.
> 
> Thanks
> 
> John
> 
> 
> 
> 
> 
> On Sat, Jul 29, 2017 at 5:37 PM, Ben Tupper <btupper at bigelow.org> wrote:
> Hi again,
> 
> A late thought - I'm still on the first cups of coffee.
> 
> It looks to me like you are iterating over a stack to select certain layers to sum.  You could achieve the same outcome with possibly much less work.  The following example will create a sum of 24-layer blocks along a stack of rasters.
> 
> # from https://gist.github.com/btupper/20879e0b46e5ed63d402d7cff424dbb7
> #' Split a vector into groups of MAX (or possibly fewer)
> #'
> #' @param v vector or list to split
> #' @param MAX numeric the maximum size per group
> #' @return a list of the vector split into groups
> split_vector <- function(v, MAX = 200){
>     nv <- length(v)
>     if (nv <= MAX) return(list('1' = v))
>     split(v, findInterval(1:nv, seq(from = 1, to = nv, by = MAX)))
> }
> 
> 
> library(raster)
> 
> N <- 345
> n <- 24
> nc <- 4
> nr <- 3
> 
> R <- raster(matrix(runif(nc*nr), ncol = nc, nrow = nr))
> 
> RR <- stack(lapply(seq_len(N), function(i) R))
> 
> ix <- split_vector(seq_len(N), MAX = n)
> 
> SS <- lapply(ix, function(index) sum(RR[[index]]))
> 
> So, S[[1]], which looks like this...
> 
> $`1`
> class       : RasterLayer
> dimensions  : 3, 4, 12  (nrow, ncol, ncell)
> resolution  : 0.25, 0.3333333  (x, y)
> extent      : 0, 1, 0, 1  (xmin, xmax, ymin, ymax)
> coord. ref. : NA
> data source : in memory
> names       : layer
> values      : 0.9451534, 20.0503  (min, max)
> 
> ... is the sum of the first 24 layers of RR. SS[[2]] will be the sum of the next 24, and so on.
> 
> Is that what you are trying to do?
> 
> Cheers,
> Ben
> 
> 
> 
> 
> > On Jul 29, 2017, at 7:56 AM, John Wasige <johnwasige at gmail.com> wrote:
> >
> > ?Dear all,
> >
> > I am running the script below & I get the following error:
> > Error in basename(x) : path too long
> > ?
> > What could be the problem?
> > Thanks for your help
> > John
> >
> > ?### Script?
> >
> > setwd("I:/Mauritius2001_2015")  # directory of data
> > newlist= read.csv('I:/Mauritius2001_2015/Mauritius.csv',header=F)
> >
> > refr <- raster(paste("I:/Mauritius2001_2015/",newlist[i,1],sep = ""))
> > refr[!is.na(refr)] <- 0
> > for(i in seq(1,345,by=23)){
> >  rsum <- refr
> >  for(p in 0:22){
> >    r <- raster(paste("I:/Mauritius2001_2015/",newlist[i+p],sep = ""))
> >    rsum <- rsum + r
> >  }
> >  #   rsum <- rsum
> >  writeRaster(rsum,
> > filename=paste("D:/Mauritius2001_2015/Annual/",substr(newlist[i],1,6),".tif",sep=''),
> > format="GTiff", overwrite=TRUE)
> > }
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
> 
> Ecocast Reports: http://seascapemodeling.org/ecocast.html
> Tick Reports: https://report.bigelow.org/tick/
> Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/
> 
> 
> 
> 
> 
> 
> -- 
> John Wasige
> "There are no REGRATES in LIFE, just lessons (Jennifer Aniston)?

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecocast Reports: http://seascapemodeling.org/ecocast.html
Tick Reports: https://report.bigelow.org/tick/
Jellyfish Reports: https://jellyfish.bigelow.org/jellyfish/


From mdsumner at gmail.com  Mon Jul 31 16:31:29 2017
From: mdsumner at gmail.com (Michael Sumner)
Date: Mon, 31 Jul 2017 14:31:29 +0000
Subject: [R-sig-Geo] Problem in extracting data from GRIB files
In-Reply-To: <CAMLwc7MyE2y1p5G06mC_VYWzTFV5nNzcsrvaSZQhPNGFhmWT2w@mail.gmail.com>
References: <CAMLwc7Pt-4PCFQHB9yU9YZ9eyt-bcxdScw83QUDhM=7HQ-oeQw@mail.gmail.com>
 <CAAcGz99SqFs5mutw9379-H7hbx1eP0YXLX422wAyHvoLDwkwiw@mail.gmail.com>
 <CAMLwc7MyE2y1p5G06mC_VYWzTFV5nNzcsrvaSZQhPNGFhmWT2w@mail.gmail.com>
Message-ID: <CAAcGz9-YKYkSX5qbRjsJ=b4+CQmoaK4UcEHjFyyHyBYOq57nCQ@mail.gmail.com>

The print-out of rdata is good, but I can't really address the issue you
say about NAs without knowing what is in "pt_dams". For that we need to
know what kind of thing it is:

   class(pt_dams)

what projection it's in

   projection(pt_dams)

and it's extent, and unfortunately for that you need to know what kind of
thing it is so either

    extent(pt_dams)  ## assuming it's a classed spatial-kind-of-thing

or this will vaguely give the idea

    head(pt_dams)  ## assuming it's a matrix or data frame

You've shown a lot of code but I think we only need to concentrated on
"rdata", as a representative of the raster data all your files will have,
and pt_dams and this line:

     data_dams_size<-extract(rdata,pts_dams)

(It's also possible to make the entire task much faster by building an
index at that first step, possibly with cellnumbers = TRUE

But, you'll have to make sure that extract line is doing what you need it
to first. It's very hard to do this by email, and you really haven't honed
into the key details yet. If the coordinates in pt_dams don't fall within
the extent of rdata, then you're in trouble (or just in the wrong
projection, probably). raster makes life easy if you know the coordinates
are in the same space as the raster, but none of this metadata stuff is in
any way consistent or complete so as ever "it depends".

Cheers, Mike.





On Mon, 31 Jul 2017 at 20:13 Miluji Sb <milujisb at gmail.com> wrote:

> Dear Mike,
>
> Thank you very much for your suggestion, apologies for the delayed reply -
> I did not have access to internet over the weekend.
>
> Here is the print-out for rdata:
>
> class       : RasterStack
> dimensions  : 150, 360, 54000, 8  (nrow, ncol, ncell, nlayers)
> resolution  : 1, 1  (x, y)
> extent      : -180, 180, -60, 90  (xmin, xmax, ymin, ymax)
> coord. ref. : +proj=longlat +a=6367470 +b=6367470 +no_defs
> names       : GLDAS_NOA//215507.020, GLDAS_NOA//215507.020,
> GLDAS_NOA//215507.020, GLDAS_NOA//215507.020, GLDAS_NOA//215507.020,
> GLDAS_NOA//215517.020, GLDAS_NOA//215517.020, GLDAS_NOA//215517.020
>
> I also made this change [rdata <-  raster::stack(lapply(filelist,
> function(x) raster(x,band=12)))] but still getting NAs. The full (ugly)
> code is below
>
> I would be grateful for any help. Thank you again!
>
> Sincerely,
>
> Milu
>
> ####
> for (y in year) {
>   setwd (y)
>
>   day <- dir(path=getwd(),full.names=TRUE, recursive=FALSE,pattern =
> "^[0-9]" )
>
>   n=0
>
>   for(d in day) {
>     setwd (d)
>
>     n=n+1
>
>
>     filelist<-list.files(pattern=".grb$")
>
>     rdata <-  raster::stack(lapply(filelist, function(x)
> raster(x,band=12)))
>
>     data_dams_size<-extract(rdata,pts_dams)
>     DF_data_dams_size<- as.data.frame(data_dams_size)
>
>     #Join ISO3, lon, lat, dams data, climate data
>     joined_dams <- cbind(foo_dams, DF_data_dams_size)
>
>     #Keep only ISO3  LON  LAT  id
>     joined_dams_reduced <- joined_dams
>     dams_codes <- joined_dams[,c(1:3)]
>
>     names(joined_dams_reduced) <- gsub("GLDAS_NOAH10_3H.A", "",
> names(joined_dams_reduced))
>     names(joined_dams_reduced) <- gsub(".020", "",
> names(joined_dams_reduced))
>
>     #From mm/s to mm_day
>     joined_dams_reduced[joined_dams_reduced==9999] <- NA
>
>     ##for prec, snow and runoff use:
>     #joined_dams_reduced$mm_day<-rowSums(joined_dams_reduced[,4:11])*10800
>
>     ##for temperature use:
>     joined_dams_reduced$mm_day<-rowMeans(joined_dams_reduced[,4:11])-273.15
>
>     joined_mm_day<-joined_dams_reduced[,12]
>
>     #Names
>     yr_name <- substr(filelist,18,21)
>     csvname <- paste(paste(yr_name, sep="_"), ".csv", sep="")
>     csvname <- csvname[1]
>
>     #Stack days to get full year
>     if (n==1) {joined_mm_day_year <- joined_mm_day} else
> {joined_mm_day_year <- cbind(joined_mm_day_year, joined_mm_day)}
>
>   }
>
>   #Join mm_day with dams codes
>   joined_mm_day_year_names=cbind(dams_codes,joined_mm_day_year)
>
>   newnames <-t(as.data.frame(c(paste ("day",
> seq(1,(ncol(joined_mm_day_year_names)-3),1), sep="_"))))
>   names(joined_mm_day_year_names)[4:ncol(joined_mm_day_year_names)]=
> newnames
>
>
>   write.csv(joined_mm_day_year_names,
> file=file.path("C:/Users/Desktop/temp/",csvname))
>
> }
>
>
> On Sat, Jul 29, 2017 at 12:36 AM, Michael Sumner <mdsumner at gmail.com>
> wrote:
>
>>
>>
>> On Sat, 29 Jul 2017 at 02:21 Miluji Sb <milujisb at gmail.com> wrote:
>>
>>> Dear all,
>>>
>>> I have a set of coordinates for cities:
>>>
>>> structure(list(lon = c(3.7174243, 3.2246995, 2.928656, 33.3822764,
>>> 10.40237, 24.7535746, 7.2619532, -0.370679, -4.486076, -4.097899
>>> ), lat = c(51.0543422, 51.209348, 51.21543, 35.1855659, 55.403756,
>>> 59.4369608, 43.7101728, 49.182863, 48.390394, 47.997542)), .Names =
>>> c("lon",
>>> "lat"), na.action = structure(c(5L, 6L, 31L, 55L, 59L, 61L, 67L,
>>> 68L), .Names = c("5", "6", "31", "55", "59", "61", "67", "68"
>>> ), class = "omit"), row.names = c(1L, 2L, 3L, 4L, 7L, 8L, 9L,
>>> 10L, 11L, 12L), class = "data.frame")
>>>
>>> I am trying to extract climatic data from GLDAS climatic data (1 degree
>>> x 1
>>> degree) GRIB files with the following code:
>>>
>>> # Convert to spatial points
>>> pts_dams <- SpatialPoints(lonlat,proj4string=CRS("+proj=longlat"))
>>>
>>> # Extract
>>>  filelist<-list.files(pattern=".grb$")
>>>
>>>
>> This part is a bit wrong:
>>
>>
>>>     data<-stack(filelist[1])
>>>     data at layers<-sapply(filelist, function(x) raster(x,band=12))
>>>
>>
>> Better would be
>>
>> rdata <-  raster::stack(lapply(filelist, function(x) raster(x,band=12)))
>>
>> For the rest we will need to see details about the data, so please do
>>
>> print(rdata)
>>
>> and send the resulting print-out.
>>
>>  A minor thing "data" is already a function, so it's advisable not to use
>> it as a name - if  only to reduce possible confusion.
>>
>> Also, it's not obvious that the process by which raster(file) goes though
>> (it passes down to rgdal::readGDAL) will result in a correct interpretation
>> as the data source was intended, since GRIB is a domain-specific format for
>> time-series and volume-slice series data and the required translation to
>> the GDAL and raster data models is not always straight-forward.
>>
>> It totally can work though, I do it routine with many sources but they
>> all need some oversight to ensure that upfront translation is complete and
>> appropriate.
>>
>> It may just require setting raster::extent and/or raster::projection, but
>> there's no way to know without exploration.
>>
>> Cheers, Mike.
>>
>>
>>
>>>     data_dams_size<-extract(data,pts_dams)
>>>     DF_data_dams_size<- as.data.frame(data_dams_size)
>>>
>>> Unfortunately, I get mostly NAs for the data, it seems that there's an
>>> issue with the CRS projections for the city coordinates. Is there a
>>> specific projection for city level coordinates? Or am I doing something
>>> completely wrong? Thank you!
>>>
>>> Sincerely,
>>>
>>> Milu
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>> --
>> Dr. Michael Sumner
>> Software and Database Engineer
>> Australian Antarctic Division
>> 203 Channel Highway
>> Kingston Tasmania 7050 Australia
>>
>>
> --
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From javier.garcia at ehu.eus  Mon Jul 31 22:37:11 2017
From: javier.garcia at ehu.eus (=?iso-8859-1?Q?Javier_Garc=EDa?=)
Date: Mon, 31 Jul 2017 22:37:11 +0200
Subject: [R-sig-Geo] Differences between moran.test and lm.morantest
In-Reply-To: <alpine.LFD.2.20.1707301818590.10808@reclus.nhh.no>
References: <001a01d3080d$2f0d3c70$8d27b550$@ehu.eus>
 <alpine.LFD.2.20.1707301818590.10808@reclus.nhh.no>
Message-ID: <000601d30a3c$cdd10720$69731560$@ehu.eus>

Thanks a lot for such a detailed response, Roger.

Best
Javi

-----Mensaje original-----
De: Roger Bivand [mailto:Roger.Bivand at nhh.no] 
Enviado el: domingo, 30 de julio de 2017 18:42
Para: Javier Garc?a
CC: r-sig-geo at r-project.org
Asunto: Re: [R-sig-Geo] Differences between moran.test and lm.morantest

On Sat, 29 Jul 2017, Javier Garc?a wrote:

> Hello everybody:
>
>
>
> Currently I am working on a paper in which we need to analyze the 
> presence of possible spatial correlation in the data. With this aim I 
> am running some tests in R. I am a little bit confused about the 
> differences between moran.test and lm.morantest R functions. The 
> problem I face to is that when I run moran.test on my  regression 
> residuals the result is totally different from the one I obtain when I 
> use lm.morantest with the lm regression object (please, see below the 
> different outputs I get and after it a reproducible example). In 
> particular, whereas the observed Moran I is the same, the expectation and
variance differ dramatically, getting opposite conclusions.
> I would appreciate very much if someone could clarify for me which is 
> the cause behind this. By the way, I also run LM tests (LMerr, RLMerr, 
> LMlag and
> RLMlag) not rejecting the null hypothesis in any of them (all p-values 
> are higher than 0.7), which is in clear contradiction with the 
> lm.morantest? how is this possible?
>

moran.test() is for "primary" variables only - read the reference in
?moran.test. The mean model applied to the this variable is the intercept,
that is the mean only:

> moran.test(COL.OLD$CRIME, nb2listw(COL.nb, style="W"),
+ randomisation=FALSE, alternative="two.sided")

 	Moran I test under normality

data:  COL.OLD$CRIME
weights: nb2listw(COL.nb, style = "W")

Moran I statistic standard deviate = 5.6754, p-value = 1.384e-08
alternative hypothesis: two.sided
sample estimates:
Moran I statistic       Expectation          Variance
       0.510951264      -0.020833333       0.008779831

under the Normal assumption.

lm.morantest() can reproduce the same results for the same model:

> lm.morantest(lm(CRIME ~ 1, data=COL.OLD), nb2listw(COL.nb, style="W"), 
+ alternative="two.sided")

 	Global Moran I for regression residuals

data:
model: lm(formula = CRIME ~ 1, data = COL.OLD)
weights: nb2listw(COL.nb, style = "W")

Moran I statistic standard deviate = 5.6754, p-value = 1.384e-08
alternative hypothesis: two.sided
sample estimates:
Observed Moran I      Expectation         Variance
      0.510951264     -0.020833333      0.008779831

Note that the Normal VI for moran.test() is:

         VI <- (wc$nn * wc$S1 - wc$n * wc$S2 + 3 * S02)/(S02 *
             (wc$nn - 1))

and for lm.morantest():

     XtXinv <- chol2inv(model$qr$qr[p1, p1, drop = FALSE])
     X <- model.matrix(terms(model), model.frame(model))
...
     Z <- lag.listw(listw.U, X, zero.policy = zero.policy)
     C1 <- t(X) %*% Z
     trA <- (sum(diag(XtXinv %*% C1)))
     EI <- -((N * trA)/((N - p) * S0))
     C2 <- t(Z) %*% Z
     C3 <- XtXinv %*% C1
     trA2 <- sum(diag(C3 %*% C3))
     trB <- sum(diag(4 * (XtXinv %*% C2)))
     VI <- (((N * N)/((S0 * S0) * (N - p) * (N - p + 2))) * (S1 +
         2 * trA2 - trB - ((2 * (trA^2))/(N - p))))

where you see easily that the issue of dependent residuals (only n-p are 
independent even in the aspatial case, so you see n-p instead of n) is 
handled, as are products of X, WX, and (X'X)^{-1}.

Reading the references is essential and should not be neglected. Reading 
the code may help, but doesn't explain the reasoning behind the code and 
the output. If you need to test model residuals, use the appropriate test. 
Using moran.test() on extracted residuals if covariates are included in 
the model is never justified.

Different outcomes from Moran's I and LM suggest severe mis-specification 
in your model, see for example:

https://groups.google.com/forum/#!msg/openspace-list/k4F4jI9cU1I/s5bj8r4nwn4
J

for a simplified flowchart for choosing models.

Hope this clarifies,

Roger

>
>
>
>
> MY PARTICULAR CASE
>
>
>
> reg.OLS <- lm(y~z1+z2+z3+z4+z5+z6+z7+z8+z9+z10, data=datos)
>
>
>
> moran.test(resid(reg.OLS),alternative="two.sided", W_n)
>
>
>
>        Moran I test under randomisation
>
>
>
> data:  resid(reg.OLS)
>
> weights: W_n
>
>
>
> Moran I statistic standard deviate = 0.4434, p-value = 0.6575
>
> alternative hypothesis: two.sided
>
> sample estimates:
>
> Moran I statistic       Expectation          Variance
>
>     1.596378e-05     -3.595829e-04      7.173448e-07
>
>
>
>
>
>
>
> moran.lm <-lm.morantest(reg.OLS, W_n, alternative="two.sided")
>
> print(moran.lm)
>
>
>
>        Global Moran I for regression residuals
>
>
>
> data:
>
> model: lm(formula = y ~ z1 + z2 + z3 + z4 + z5 + z6 + z7 + z8 + z9 + z10
>
> , data = datos)
>
> weights: W_n
>
>
>
> Moran I statistic standard deviate = 11.649, p-value < 2.2e-16
>
> alternative hypothesis: two.sided
>
> sample estimates:
>
> Observed Moran I      Expectation         Variance
>
>    1.596378e-05    -1.913005e-03     2.741816e-08
>
>
>
>
>
> A REPRODUCIBLE EXAMPLE
>
>
>
>
>
> library(spdep)
>
> data(oldcol)
>
> oldcrime.lm <- lm(CRIME ~ HOVAL + INC + OPEN + PLUMB + DISCBD + PERIM,
data
> = COL.OLD)
>
>
>
> moran.test(resid(oldcrime.lm), nb2listw(COL.nb, style="W"))
>
>
>
>        Moran I test under randomisation
>
>
>
> data:  resid(oldcrime.lm)
>
> weights: nb2listw(COL.nb, style = "W")
>
>
>
> Moran I statistic standard deviate = 1.2733, p-value = 0.1015
>
> alternative hypothesis: greater
>
> sample estimates:
>
> Moran I statistic       Expectation          Variance
>
>      0.096711162      -0.020833333       0.008521765
>
>
>
>
>
> lm.morantest(oldcrime.lm, nb2listw(COL.nb, style="W"))
>
>
>
>        Global Moran I for regression residuals
>
>
>
> data:
>
> model: lm(formula = CRIME ~ HOVAL + INC + OPEN + PLUMB + DISCBD +
>
> PERIM, data = COL.OLD)
>
> weights: nb2listw(COL.nb, style = "W")
>
>
>
> Moran I statistic standard deviate = 1.6668, p-value = 0.04777
>
> alternative hypothesis: greater
>
> sample estimates:
>
> Observed Moran I      Expectation         Variance
>
>     0.096711162     -0.052848581      0.008050938
>
>
>
> Thanks a lot in advance and sorry for the inconvenience.
>
>
>
> Javi
>
>
>
>
>
>
>
>
> JAVIER GARC?A
>
>
>
> Departamento de Econom?a Aplicada III (Econometr?a y Estad?stica)
>
> Facultad de Econom?a y Empresa (Secci?n Sarriko)
> Avda. Lehendakari Aguirre 83
>
> 48015 BILBAO
> T.: +34 601 7126 F.: +34 601 3754
> <http://www.ehu.es/> www.ehu.es
>
>
http://www.unibertsitate-hedakuntza.ehu.es/p268-content/es/contenidos/inform
>
acion/manual_id_corp/es_manual/images/firma_email_upv_euskampus_bilingue.gif
>
>
>
>
>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en


From Roger.Bivand at nhh.no  Mon Jul 31 23:52:13 2017
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 31 Jul 2017 23:52:13 +0200
Subject: [R-sig-Geo] bivariate spatial correlation in R
In-Reply-To: <CAA42DGk8agiCMPp5FLwSrd1wszaVcNHd5uB6fCkdT9rb8QHLOw@mail.gmail.com>
References: <CAA42DGnbWzkOfhMWttTkjaMEdOyDKLkZ=DDDYjywJrhFrKgjtw@mail.gmail.com>
 <alpine.LFD.2.20.1707242012390.6677@reclus.nhh.no>
 <CAA42DGm1dhzEvN9vYyRfKS7XuPVnXFgVBfRHWFLGS29H4R65Cw@mail.gmail.com>
 <alpine.LFD.2.20.1707261152010.18617@reclus.nhh.no>
 <CAA42DGkhtyTHpW3ZoFAOHdDcjY6iK0LZpZFdkvnmihieujzbGg@mail.gmail.com>
 <6124293B00A50295.e0f9c23d-4edb-4035-aab0-756e53b6db82@mail.outlook.com>
 <CAA42DGk8agiCMPp5FLwSrd1wszaVcNHd5uB6fCkdT9rb8QHLOw@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1707312250480.10503@reclus.nhh.no>

Rafael,

I'm sorry, but there is no way you can logically "analyze who benefits the 
recent changes in the transport system in terms of access to jobs" from 
the data you have.

Even if you had aggregate household income data for 2014 and 2017 (not for 
2010 only), you would not know whether wealthier families had not dispaced 
poorer families as accessibility improved. You need individual data, 
either survey or register, preferably panel, to show that changes in 
accessibility change the economic welfare of households controlling for 
movement of households. The timestamps on the data make any attempt to do 
this very risky; the real findings from a hypothetical surevey-based panel 
might be completely different, especially if poorer households were 
displaced (also indirectly, through rising house prices driven by improved 
accessibility). Gauging the welfare effects of transport investments is 
very hard to instrument.

The closest I could get was to map deciles of the change in access (more 
negatives than positives) and compare the aspatial income distributions:

library(spdep)
library(rgdal)
map <- readOGR(dsn=".", layer="test_map")
library(classInt)
cI <- classIntervals(map$diffaccess, n=10, style="quantile")
library(RColorBrewer)
ybrpal <- brewer.pal(6, "YlOrBr")
fC <- findColours(cI, ybrpal)
qtm(map, fill="diffaccess", fill.breaks=cI$brks, format="Europe2")
map$faccess <- factor(findInterval(map$diffaccess, cI$brks,
   all.inside=TRUE), labels=names(attr(fC, "table")))
qtm(map, fill="diffaccess", fill.breaks=cI$brks, format="Europe2")
acc_income <- split(map$income, map$faccess)
do.call("rbind", lapply(acc_income, summary))
dens <- lapply(acc_income, density)
plot(1, ylab="", xlab="", type="n", xlim=c(-2000, 11000), ylim=c(0,
   0.002))
for (i in seq(along=dens)) lines(dens[[i]], col=i)
legend("topright", legend=names(dens), col=1:length(dens), lty=1, bty="n")

These density curves really do not suggest any clear relationship, other 
than that some areas with increased accessibility had higher incomes in 
2010.

You can examine the reverse relationship - were aggregate areas that were 
more wealthy in 2010 able to attract more changes to accessibility? The 
answer seems to be yes, they were able to do this:

nb <- poly2nb(map)
lw <- nb2listw(nb, style = "W", zero.policy = T)
lm.morantest(lm(diffaccess ~ I(income/1000), map), lw)
# SLX model
summary(lmSLX(diffaccess ~ I(income/1000), map, lw))
lm.morantest(lmSLX(diffaccess ~ I(income/1000), map, lw), lw)
# Spatial Durbin error model - SDEM
obj <- errorsarlm(diffaccess ~ I(income/1000), map, lw, etype="emixed")
summary(impacts(obj))
summary(impacts(lmSLX(diffaccess ~ I(income/1000), map, lw)))
LR.sarlm(lmSLX(diffaccess ~ I(income/1000), map, lw), obj)

It would be possible to run lm.morantest.sad() on the output of the SDEM 
model taking global spatial autocorrelation into account. If you need 
that, follow up in this thread.

Bivariate Moran's I should not be used in this case, but could be used in 
other cases - use in change over time is troubling because randomisation 
will not be a good guide as t=1 and t=2 are subject to temporal as well as 
spatial autocorrelation, so you cannot use permutation bootstrap to find 
a usable measure of significance.

Hope this clarifies, and thanks for the code.

Roger

On Sun, 30 Jul 2017, Rafael Pereira wrote:

> Roger,
>
> Population and income data are single point in time and come from the 2010
> Census.
>
> Accessibility variables in 2014 and 2017 show the proportion of jobs
> accessible by public transport under 60 minutes. The variable diffaccess
> shows the difference between these two. It's in percentage points
> (access2017 - access2014)
>
> best,
>
> Rafael H M Pereira
> urbandemographics.blogspot.com
>
> On Sun, Jul 30, 2017 at 7:41 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> Thanks, I'll get back when able, offline now. What are the units of
>> observation, and are aggregate household incomes observed only once?
>>
>> Roger
>>
>> Roger Bivand
>> Norwegian School of Economics
>> Bergen, Norway
>>
>>
>>
>> Fra: Rafael Pereira
>> Sendt: s?ndag 30. juli, 00.39
>> Emne: Re: [R-sig-Geo] bivariate spatial correlation in R
>> Kopi: Rog?rio Barbosa, r-sig-geo at r-project.org
>>
>>
>> Hi all, here is a reproducible example to calculate in R bivariate Moran's
>> I and LISA clusters. This example is based on a this answer provided in SO*
>> and it uses a toy model of my data. The R script and the shape file with
>> the data are available on this link. https://gist.github.com/
>> rafapereirabr/5348193abf779625f5e8c5090776a228 What this example does is
>> to estimate the spatial association between household income per capita and
>> the gains in accessibility to jobs. The aim is to analyze who benefits the
>> recent changes in the transport system in terms of access to jobs. So the
>> idea is not to find causal relationships, but spatial association between
>> areas of high/low income who had high/low gains in accessibility. The
>> variables in the data show info on the proportion of jobs accessible in
>> both years 2014 and 2017 (access2014, access2017) and the difference
>> between the two years in percentage points (diffaccess). Roger, I know you
>> have shown to be a bit sceptical about this application of bivariate
>> Moran's I. Do you still think a spatial regression would be more
>> appropriate? Also, I would be glad to hear if others have comments on the
>> code. This function is not implemented in any package so it would be great
>> to have some feedback. Rafael H M Pereira urbandemographics.blogspot.com
>> * https://stackoverflow.com/questions/45177590/map-of-
>> bivariate-spatial-correlation-in-r-bivariate-lisa On Wed, Jul 26, 2017 at
>> 11:07 AM, Roger Bivand wrote: > On Wed, 26 Jul 2017, Rafael Pereira wrote:
>>>> Roger, >> >> This example was provided only for the sake or making the
>> code easily >> reproducible for others and I'm more interested in how the
>> bi-variate >> Moran >> could be implemented in R, but your comments are
>> very much welcomed and >> I've made changes to the question. >> >> My
>> actual case study looks at bi-variate spatial correlation between (a) >>
>> average household income per capita and (b) proportion of jobs in the city
>>>> that are accessible under 60 minutes by transit. I don't think I could
>> use >> rates in this case but I will normalize the variables using >>
>> scale(data$variable). >> > > Please provide a reproducible example, either
>> with a link to a data > subset, or using a builtin data set. My guess is
>> that you do not need > bi-variate spatial correlation at all, but rather a
>> spatial regression. > > The "causal" variable would then the the proportion
>> of jobs accessible > within 60 minutes by transit, though this is extremely
>> blunt, and lots of > other covariates (demography, etc.) impact average
>> household income per > capita (per block/tract?). Since there are many
>> missing variables in your > specification, any spatial correlation would be
>> most closely associated > with them (demography, housing costs, education,
>> etc.), and the choice of > units of measurement would dominate the outcome.
>>>> This is also why bi-variate spatial correlation is seldom a good idea,
>> I > believe. It can be done, but most likely shouldn't, unless it can be >
>> motivated properly. > > By the way, the weighted and FDR-corrected SAD
>> local Moran's I p-values of > the black/white ratio for Oregon (your toy
>> example) did deliver the goods - > if you zoom in in mapview::mapview, you
>> can see that it detects a rate > hotspot between the rivers. > > Roger > >
>>>>> best, >> >> Rafael H M Pereira >> >> On Mon, Jul 24, 2017 at 7:56 PM,
>> Roger Bivand >> wrote: >> >> On Mon, 24 Jul 2017, Rafael Pereira wrote: >>>
>>>>> Hi all, >>> >>>> >>>> I would like to ask whether some you conducted
>> bi-variate spatial >>>> correlation in R. >>>> >>>> I know the bi-variate
>> Moran's I is not implemented in the spdep library. >>>> I left a question
>> on SO but also wanted to hear if anyone if the >>>> mainlist >>>> have come
>> across this. >>>> https://stackoverflow.com/questions/45177590/map-of-
>> bivariat >>>> e-spatial-correlation-in-r-bivariate-lisa >>>> >>>> I also
>> know Roger Bivand has implemented the L index proposed by Lee >>>> (2001)
>>>>>> in spdep, but I'm not I'm not sure whether the L local correlation
>>>>>> coefficients can be interpreted the same way as the local Moran's I
>>>>>> coefficients. I couldn't find any reference commenting on this issue.
>> I >>>> would very much appreciate your thoughts this. >>>> >>>> >>> In the
>> SO question, and in the follow-up, your presumably throw-away >>> example
>> makes fundamental mistakes. The code in spdep by Virgilio >>> G?mez-Rubio
>> is for uni- and bivariate L, and produces point values of >>> local >>> L.
>> This isn't the main problem, which is rather that you are not taking >>>
>> account of the underlying population counts, nor shrinking any estimates
>>>>> of >>> significance to accommodate population sizes. Population sizes
>> vary from >>> 0 >>> to 11858, with the lower quartile at 3164 and upper
>> 5698: >>> plot(ecdf(oregon.tract$pop2000)). Should you be comparing rates
>> in >>> stead? >>> These are also compositional variables (sum to pop2000,
>> or 1 if rates) >>> with >>> the other missing components. You would
>> probably be better served by >>> tools >>> examining spatial segregation,
>> such as for example the seg package. >>> >>> The 0 count populations cause
>> problems for an unofficial alternative, the >>> black/white ratio: >>> >>>
>> oregon.tract1 0,] >>> oregon.tract1$rat >> nb >> lw >> >>> which should
>> still be adjusted by weighting: >>> >>> lm0 >> >>> I'm not advising this,
>> but running localmoran.sad on this model output >>> yields SAD p-values <
>> 0.05 after FDR correction only in contiguous tracts >>> on the Washington
>> state line in Portland between the Columbia and >>> Williamette rivers. So
>> do look at the variables you are using before >>> rushing into things. >>>
>>>>> Hope this clarifies, >>> >>> Roger >>> >>> >>> best, >>>> >>>> Rafael
>> HM Pereira >>>> http://urbandemographics.blogspot.com >>>> >>>>
>> [[alternative HTML version deleted]] >>>> >>>>
>> _______________________________________________ >>>> R-sig-Geo mailing
>> list >>>> R-sig-Geo at r-project.org >>>> https://stat.ethz.ch/mailman/
>> listinfo/r-sig-geo >>>> >>>> >>>> -- >>> Roger Bivand >>> Department of
>> Economics, Norwegian School of Economics, >>> Helleveien 30, N-5045 Bergen,
>> Norway. >>> voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>; e-mail:
>> Roger.Bivand at nhh.no >>> Editor-in-Chief of The R Journal,
>> https://journal.r-project.org/ >>> index.html >>>
>> http://orcid.org/0000-0003-2392-6140 >>> https://scholar.google.no/
>> citations?user=AWeghB0AAAAJ&hl=en >>> >> >> [[alternative HTML version
>> deleted]] >> >> _______________________________________________ >>
>> R-sig-Geo mailing list >> R-sig-Geo at r-project.org >>
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo >> > > -- > Roger Bivand
>>> Department of Economics, Norwegian School of Economics, > Helleveien 30,
>> N-5045 Bergen, Norway. > voice: +47 55 95 93 55 <+47%2055%2095%2093%2055>;
>> e-mail: Roger.Bivand at nhh.no > Editor-in-Chief of The R Journal,
>> https://journal.r-project.org/index.html > http://orcid.org/0000-0003-
>> 2392-6140 > https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en >
>> [[alternative HTML version deleted]] _______________________________________________
>> R-sig-Geo mailing list R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; e-mail: Roger.Bivand at nhh.no
Editor-in-Chief of The R Journal, https://journal.r-project.org/index.html
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en

