From milujisb at gmail.com  Thu Jun  2 10:42:35 2016
From: milujisb at gmail.com (Miluji Sb)
Date: Thu, 2 Jun 2016 09:42:35 +0100
Subject: [R-sig-Geo] Convert ncdf data to dataframe
Message-ID: <CAMLwc7Px6R6=P3ij7_YHmPALerA4-d0O6evmE56w347UYYcXbw@mail.gmail.com>

Dear all,

I have used the following code to read in a ncdf file

library(chron)
library(lattice)
library(ncdf4)
library(data.table)

ncname <- ("/file_path")
ncfname <- paste(ncname, ".nc", sep = "")
dname <- "ssl"  # note: tmp means temperature (not temporary)

ncin <- nc_open(ncfname)
print(ncin)

The attributes of the file are:

     4 variables (excluding dimension variables):
        double longitude[row]
            long_name: longitude
            units: degrees_east
            standard_name: longitude
        double latitude[row]
            long_name: latitude
            units: degrees_north
            standard_name: latitude
        double ssl[col,row]
            long_name: storm surge level
            units: m
            _FillValue: -99999
            scale_factor: 1
            add_offset: 1
        float RP[col]
            long_name: return period
            units: yr
            Contents: The RPs have been estimated following the Peak Over
Threshold Method (see reference below)
            Starting date: 01-Dec-2009
            End date: 30-Nov-2099 21:00:00
     2 dimensions:
        col  Size:8
        row  Size:2242

I would like to convert the data into a dataframe by longitude and
latitude. Is that possible? Thank you!

Sincerely,

Milu

	[[alternative HTML version deleted]]


From open at tobywilkinson.co.uk  Thu Jun  2 12:22:13 2016
From: open at tobywilkinson.co.uk (T C Wilkinson)
Date: Thu, 2 Jun 2016 11:22:13 +0100
Subject: [R-sig-Geo] gDistance transition objects: dealing with NAs in
 source cost raster
Message-ID: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>

Dear all,

I?m using two functions (copied below, based on Jacob van Etten?s own documentation for distance) in order to construct the required transition objects for use in gDistance functions such as shortestPath and accCost, using an input DEM (digital elevation model).

The particular DEM I am using includes an area of sea which is marked as NA and, for my particular analyses, should be ignored (i.e. should be modelled as 0 conductance) in the distance calculations. ?My assumption was that these functions would ignore the area of NA-marked sea by default (in part because the equivalent Arc- models exclude NoData areas). ?

However, a plot of the transition as raster `plot(raster(conductance))` suggests the sea is included as a low-level conductance (it appears as pink from the col.palette and not black as specified in colNA field). ?Similarly, subsequently-created accCost rasters include the sea as part of the accumulate cost surface and not, as I hoped, as NA.

I tried a different altDiffFunction, in an attempt to exclude the NA values, but I suspect this is merely a less elegant method of producing the same result.

Whilst I wondered if some kind of transitionStack might solve the problem, my (albeit limited) understanding of the way gDistance transition layers work suggest that this is overkill for what is essentially a single-variable (slope) function.

Does the resolution of the DEM play any role here?

Any suggestions for how to solve this (i.e. how to exclude the NA area from the transition model) would be very gratefully received!

Many thanks,
Toby




**CODE EXTRACT**:

sw_slope <- function(dem, dirs=16, symm=FALSE){
? #calculate the altitudinal differences between cells
? #altDiffFunction <- function(x){x[2] - x[1]}
? altDiffFunction <- function(x){
? ? return(ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA))
? }
??
? heightDiff <- transition(dem, altDiffFunction, directions=dirs, symm=symm)
? #divide by the distance between cells
? slope <- geoCorrection(heightDiff)
? return(slope)
}?



sw_conductance <- function(dem, dirs=16, symm=FALSE){
? #first find slope
? slope <- sw_slope(dem, dirs, symm)
??
? #create an index for adjacent cells (adj) with the function adjacent
? adj <- adjacent(dem, cells=1:ncell(dem), pairs=TRUE, directions=16)
? cost <- slope
??
? #extract and replace adjacent cells
? #note the 1/ to convert from resistivity (as cited in publication above) to conductivity
? cost[adj] <- 1 / ((((1337.8 * cost[adj]^6) + (278.19 * cost[adj]^5) - (517.39 * cost[adj]^4) - (78.199 * cost[adj]^3) + (93.419 * cost[adj]^2) + (19.825 * cost[adj]) + 1.64)))
??
? #correction to take account of distance between cell centres
? gc <- geoCorrection(cost)
??
? return(gc)
}


dem <- raster() # the dem, based on an reprojected aster gdem tile, is loaded from file

conductance <-?sw_conductance(dem)

plot(raster(conductance),
? ? ? ?col = col.palette,
? ? ? ?colNA = "black",
? )


From jacobvanetten at yahoo.com  Thu Jun  2 13:30:29 2016
From: jacobvanetten at yahoo.com (Jacob van Etten)
Date: Thu, 2 Jun 2016 11:30:29 +0000 (UTC)
Subject: [R-sig-Geo] gDistance transition objects: dealing with NAs in
 source cost raster
In-Reply-To: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
References: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
Message-ID: <563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>

Dear Toby,
TransitionLayers should normally not hold NA values. If you want to set cell connections to zero conductance, the transitionFunction should give a value zero if it finds an NA.?
Your function gives an NA as result, which is not what you want:
ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA)

You probably want to do:
ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0)

Also, note that gDistance is not the name of the package.
Best,
Jacob 

    On Thursday, 2 June 2016, 4:24, T C Wilkinson <open at tobywilkinson.co.uk> wrote:
 

 Dear all,

I?m using two functions (copied below, based on Jacob van Etten?s own documentation for distance) in order to construct the required transition objects for use in gDistance functions such as shortestPath and accCost, using an input DEM (digital elevation model).

The particular DEM I am using includes an area of sea which is marked as NA and, for my particular analyses, should be ignored (i.e. should be modelled as 0 conductance) in the distance calculations. ?My assumption was that these functions would ignore the area of NA-marked sea by default (in part because the equivalent Arc- models exclude NoData areas). ?

However, a plot of the transition as raster `plot(raster(conductance))` suggests the sea is included as a low-level conductance (it appears as pink from the col.palette and not black as specified in colNA field). ?Similarly, subsequently-created accCost rasters include the sea as part of the accumulate cost surface and not, as I hoped, as NA.

I tried a different altDiffFunction, in an attempt to exclude the NA values, but I suspect this is merely a less elegant method of producing the same result.

Whilst I wondered if some kind of transitionStack might solve the problem, my (albeit limited) understanding of the way gDistance transition layers work suggest that this is overkill for what is essentially a single-variable (slope) function.

Does the resolution of the DEM play any role here?

Any suggestions for how to solve this (i.e. how to exclude the NA area from the transition model) would be very gratefully received!

Many thanks,
Toby




**CODE EXTRACT**:

sw_slope <- function(dem, dirs=16, symm=FALSE){
? #calculate the altitudinal differences between cells
? #altDiffFunction <- function(x){x[2] - x[1]}
? altDiffFunction <- function(x){
? ? return(ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA))
? }
??
? heightDiff <- transition(dem, altDiffFunction, directions=dirs, symm=symm)
? #divide by the distance between cells
? slope <- geoCorrection(heightDiff)
? return(slope)
}?



sw_conductance <- function(dem, dirs=16, symm=FALSE){
? #first find slope
? slope <- sw_slope(dem, dirs, symm)
??
? #create an index for adjacent cells (adj) with the function adjacent
? adj <- adjacent(dem, cells=1:ncell(dem), pairs=TRUE, directions=16)
? cost <- slope
??
? #extract and replace adjacent cells
? #note the 1/ to convert from resistivity (as cited in publication above) to conductivity
? cost[adj] <- 1 / ((((1337.8 * cost[adj]^6) + (278.19 * cost[adj]^5) - (517.39 * cost[adj]^4) - (78.199 * cost[adj]^3) + (93.419 * cost[adj]^2) + (19.825 * cost[adj]) + 1.64)))
??
? #correction to take account of distance between cell centres
? gc <- geoCorrection(cost)
??
? return(gc)
}


dem <- raster() # the dem, based on an reprojected aster gdem tile, is loaded from file

conductance <-?sw_conductance(dem)

plot(raster(conductance),
? ? ? ?col = col.palette,
? ? ? ?colNA = "black",
? )

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

  
	[[alternative HTML version deleted]]


From englishchristophera at gmail.com  Thu Jun  2 14:19:24 2016
From: englishchristophera at gmail.com (chris english)
Date: Thu, 2 Jun 2016 15:19:24 +0300
Subject: [R-sig-Geo] gDistance transition objects: dealing with NAs in
 source cost raster
In-Reply-To: <563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
References: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
	<563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAASFQpTdj6Kh6dx8XsAZH-SJxk9JdT9TZWcEE+_8S3XLxP=reA@mail.gmail.com>

Toby,

Jacob van Etten's package is "gdistance", as distinct from
rgeos:::gDistance that also gets discussed here.
Just for clarity.

Chris

On Thu, Jun 2, 2016 at 2:30 PM, Jacob van Etten via R-sig-Geo <
r-sig-geo at r-project.org> wrote:

> Dear Toby,
> TransitionLayers should normally not hold NA values. If you want to set
> cell connections to zero conductance, the transitionFunction should give a
> value zero if it finds an NA.
> Your function gives an NA as result, which is not what you want:
> ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA)
>
> You probably want to do:
> ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0)
>
> Also, note that gDistance is not the name of the package.
> Best,
> Jacob
>
>     On Thursday, 2 June 2016, 4:24, T C Wilkinson <
> open at tobywilkinson.co.uk> wrote:
>
>
>  Dear all,
>
> I?m using two functions (copied below, based on Jacob van Etten?s own
> documentation for distance) in order to construct the required transition
> objects for use in gDistance functions such as shortestPath and accCost,
> using an input DEM (digital elevation model).
>
> The particular DEM I am using includes an area of sea which is marked as
> NA and, for my particular analyses, should be ignored (i.e. should be
> modelled as 0 conductance) in the distance calculations.  My assumption was
> that these functions would ignore the area of NA-marked sea by default (in
> part because the equivalent Arc- models exclude NoData areas).
>
> However, a plot of the transition as raster `plot(raster(conductance))`
> suggests the sea is included as a low-level conductance (it appears as pink
> from the col.palette and not black as specified in colNA field).
> Similarly, subsequently-created accCost rasters include the sea as part of
> the accumulate cost surface and not, as I hoped, as NA.
>
> I tried a different altDiffFunction, in an attempt to exclude the NA
> values, but I suspect this is merely a less elegant method of producing the
> same result.
>
> Whilst I wondered if some kind of transitionStack might solve the problem,
> my (albeit limited) understanding of the way gDistance transition layers
> work suggest that this is overkill for what is essentially a
> single-variable (slope) function.
>
> Does the resolution of the DEM play any role here?
>
> Any suggestions for how to solve this (i.e. how to exclude the NA area
> from the transition model) would be very gratefully received!
>
> Many thanks,
> Toby
>
>
>
>
> **CODE EXTRACT**:
>
> sw_slope <- function(dem, dirs=16, symm=FALSE){
>   #calculate the altitudinal differences between cells
>   #altDiffFunction <- function(x){x[2] - x[1]}
>   altDiffFunction <- function(x){
>     return(ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA))
>   }
>
>   heightDiff <- transition(dem, altDiffFunction, directions=dirs,
> symm=symm)
>   #divide by the distance between cells
>   slope <- geoCorrection(heightDiff)
>   return(slope)
> }
>
>
>
> sw_conductance <- function(dem, dirs=16, symm=FALSE){
>   #first find slope
>   slope <- sw_slope(dem, dirs, symm)
>
>   #create an index for adjacent cells (adj) with the function adjacent
>   adj <- adjacent(dem, cells=1:ncell(dem), pairs=TRUE, directions=16)
>   cost <- slope
>
>   #extract and replace adjacent cells
>   #note the 1/ to convert from resistivity (as cited in publication above)
> to conductivity
>   cost[adj] <- 1 / ((((1337.8 * cost[adj]^6) + (278.19 * cost[adj]^5) -
> (517.39 * cost[adj]^4) - (78.199 * cost[adj]^3) + (93.419 * cost[adj]^2) +
> (19.825 * cost[adj]) + 1.64)))
>
>   #correction to take account of distance between cell centres
>   gc <- geoCorrection(cost)
>
>   return(gc)
> }
>
>
> dem <- raster() # the dem, based on an reprojected aster gdem tile, is
> loaded from file
>
> conductance <- sw_conductance(dem)
>
> plot(raster(conductance),
>        col = col.palette,
>        colNA = "black",
>   )
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From open at tobywilkinson.co.uk  Thu Jun  2 15:06:59 2016
From: open at tobywilkinson.co.uk (T C Wilkinson)
Date: Thu, 2 Jun 2016 14:06:59 +0100
Subject: [R-sig-Geo] gdistance transition objects: dealing with NAs in
 source cost raster
In-Reply-To: <563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
References: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
	<563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <etPan.57502f73.67e9084b.2a1b@Eurasia-2.local>

Dear Jacob,

Many thanks for fast reply. [Apologies for the typo and potential confusion on gdistance, I am still relative new to R ? and thus borrowing camelCase habit from previous coding.]

I?ve tried your suggested re-worked function to avoid NA in the transition, but I still get the same result.

When plotted, the `plot(raster(conductance))` still colours the sea (i.e. doesn?t show it in the colNA colour) and the resultant accCosts etc include the sea as part of the result surface.

An interactive `click(raster(conductance))` reveals the sea to be a uniform value of 0.001436461.

Thanks again,
Toby



On 2 June 2016 at 12:30:43, Jacob van Etten (jacobvanetten at yahoo.com) wrote:
> Dear Toby,
> TransitionLayers should normally not hold NA values. If you want to set cell connections  
> to zero conductance, the transitionFunction should give a value zero if it finds an NA.  
> Your function gives an NA as result, which is not what you want:
> ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA)
>  
> You probably want to do:
> ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0)
>  
> Also, note that gDistance is not the name of the package.
> Best,
> Jacob


From englishchristophera at gmail.com  Thu Jun  2 15:10:32 2016
From: englishchristophera at gmail.com (chris english)
Date: Thu, 2 Jun 2016 16:10:32 +0300
Subject: [R-sig-Geo] Building a prediction raster when the statistical
 model was built from sampling units of different sizes
In-Reply-To: <CY1PR05MB27303108E8419879545ACB5899420@CY1PR05MB2730.namprd05.prod.outlook.com>
References: <CY1PR05MB27303108E8419879545ACB5899420@CY1PR05MB2730.namprd05.prod.outlook.com>
Message-ID: <CAASFQpRa9ybZtEwtDgcMCS_rHTTBohOHge-KKfo3c9jHaNdSJg@mail.gmail.com>

Hi Nell,

Just a couple of questions. Trapping sites range from 24km^2 - 236km^2, and
there are 50 such sites, looking at the 50 sites might there be a way to
bin them reasonably into trap area groups?

Your 50 observations suggests one thing was trapped and thereafter trapping
was discontinued. Is this correct?

And just for general information, what was being trapped?

Sticking closer to your data, you might consider GAM(ing) the bins and
summing the resultant GAMs. Need to think some more on the predictive
raster aspect. Sorry for an essentially inconclusive answer.

Chris

	[[alternative HTML version deleted]]


From jstachel at sfwmd.gov  Thu Jun  2 15:20:39 2016
From: jstachel at sfwmd.gov (Stachelek, Joseph)
Date: Thu, 2 Jun 2016 13:20:39 +0000
Subject: [R-sig-Geo] Convert ncdf data to dataframe
In-Reply-To: <CAMLwc7Px6R6=P3ij7_YHmPALerA4-d0O6evmE56w347UYYcXbw@mail.gmail.com>
References: <CAMLwc7Px6R6=P3ij7_YHmPALerA4-d0O6evmE56w347UYYcXbw@mail.gmail.com>
Message-ID: <D51374C4B889BC47B3C5286047C86DA1A02FD8CB@whqembx03p.ad.sfwmd.gov>

Hi Milu,

I would suggest reading your .nc file directly into a raster object using the `raster` package.

###
library(raster)
ncname <- ("/file_path")
ncfname <- paste(ncname, ".nc", sep = "")
r <- raster::raster(ncfname) # use raster::stack() if more than one "layer"
###

I have suggested that the author of the `ncdf4` package (David Pierce) make note of this easier way to load .nc files in the `ncdf4` documentation. If you need the coordinates of each raster cell you might take a look at the instructions here:

http://gis.stackexchange.com/questions/142156/r-how-to-get-latitudes-and-longitudes-from-a-rasterlayer

Joseph Stachelek

-----Original Message-----
From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of Miluji Sb
Sent: Thursday, June 02, 2016 4:43 AM
To: R-sig-geo mailing list <r-sig-geo at r-project.org>
Subject: [R-sig-Geo] Convert ncdf data to dataframe

Dear all,

I have used the following code to read in a ncdf file

library(chron)
library(lattice)
library(ncdf4)
library(data.table)

ncname <- ("/file_path")
ncfname <- paste(ncname, ".nc", sep = "")
dname <- "ssl"  # note: tmp means temperature (not temporary)

ncin <- nc_open(ncfname)
print(ncin)

The attributes of the file are:

     4 variables (excluding dimension variables):
        double longitude[row]
            long_name: longitude
            units: degrees_east
            standard_name: longitude
        double latitude[row]
            long_name: latitude
            units: degrees_north
            standard_name: latitude
        double ssl[col,row]
            long_name: storm surge level
            units: m
            _FillValue: -99999
            scale_factor: 1
            add_offset: 1
        float RP[col]
            long_name: return period
            units: yr
            Contents: The RPs have been estimated following the Peak Over
Threshold Method (see reference below)
            Starting date: 01-Dec-2009
            End date: 30-Nov-2099 21:00:00
     2 dimensions:
        col  Size:8
        row  Size:2242

I would like to convert the data into a dataframe by longitude and
latitude. Is that possible? Thank you!

Sincerely,

Milu

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


We value your opinion. Please take a few minutes to shar...{{dropped:5}}


From englishchristophera at gmail.com  Thu Jun  2 15:29:38 2016
From: englishchristophera at gmail.com (chris english)
Date: Thu, 2 Jun 2016 16:29:38 +0300
Subject: [R-sig-Geo] gdistance transition objects: dealing with NAs in
 source cost raster
In-Reply-To: <etPan.57502f73.67e9084b.2a1b@Eurasia-2.local>
References: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
	<563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
	<etPan.57502f73.67e9084b.2a1b@Eurasia-2.local>
Message-ID: <CAASFQpSS2PQy=a8Cd5sPi2m69aj74wDEmwhgr4KX5x23fmUCZQ@mail.gmail.com>

Toby,

This may seem silly, and a thousandth of something is not zero, but perhaps
this is  a kind of preallocation issue and using Jacob's if else with 0.0
as else might do the trick.

Fingers crossed?
Chris
On Jun 2, 2016 16:07, "T C Wilkinson" <open at tobywilkinson.co.uk> wrote:

> Dear Jacob,
>
> Many thanks for fast reply. [Apologies for the typo and potential
> confusion on gdistance, I am still relative new to R ? and thus borrowing
> camelCase habit from previous coding.]
>
> I?ve tried your suggested re-worked function to avoid NA in the
> transition, but I still get the same result.
>
> When plotted, the `plot(raster(conductance))` still colours the sea (i.e.
> doesn?t show it in the colNA colour) and the resultant accCosts etc include
> the sea as part of the result surface.
>
> An interactive `click(raster(conductance))` reveals the sea to be a
> uniform value of 0.001436461.
>
> Thanks again,
> Toby
>
>
>
> On 2 June 2016 at 12:30:43, Jacob van Etten (jacobvanetten at yahoo.com)
> wrote:
> > Dear Toby,
> > TransitionLayers should normally not hold NA values. If you want to set
> cell connections
> > to zero conductance, the transitionFunction should give a value zero if
> it finds an NA.
> > Your function gives an NA as result, which is not what you want:
> > ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA)
> >
> > You probably want to do:
> > ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0)
> >
> > Also, note that gDistance is not the name of the package.
> > Best,
> > Jacob
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From open at tobywilkinson.co.uk  Thu Jun  2 18:28:40 2016
From: open at tobywilkinson.co.uk (T C Wilkinson)
Date: Thu, 2 Jun 2016 17:28:40 +0100
Subject: [R-sig-Geo] gdistance transition objects: dealing with NAs in
 source cost raster
In-Reply-To: <CAASFQpSS2PQy=a8Cd5sPi2m69aj74wDEmwhgr4KX5x23fmUCZQ@mail.gmail.com>
References: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
	<563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
	<etPan.57502f73.67e9084b.2a1b@Eurasia-2.local>
	<CAASFQpSS2PQy=a8Cd5sPi2m69aj74wDEmwhgr4KX5x23fmUCZQ@mail.gmail.com>
Message-ID: <etPan.57505eb8.34fbf36a.2df4@Eurasia-2.local>

Dear Chris, dear Jacob,

Many thanks indeed for suggestions and help.

1. The input raster (dem) definitely shows NA in the areas of sea using click().

2. Tried the 0.0 in the ifelse: no luck, same result.

I tried plotting and clicking for the values of the initial `slope` TransitionLayer: here the area of the sea reports as a value of NaN, so perhaps the problem is not in the transition function, but in the later conversion of slope to cost (in sw_conductance).

I tried changing adjacent() to adjacencyFromTransition(), which does indeed result in the sea being plotted as NA (or presumably NaN), but the values of the conductance on land now don?t seem correct (i.e. some areas that I know are flat are shown as relatively lower conductance than areas which are sloping).

Any ideas?

Thank you again,
Toby



sw_slope <- function(dem, dirs=16, symm=FALSE){
? # calculate the altitudinal differences between cells:
? #altDiffFunction <- function(x){x[2] - x[1]}
? # with a little help from Jacob van Etten and Chris English:
? altDiffFunction <- function(x){ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0.0)}
??
? heightDiff <- transition(dem, altDiffFunction, directions=dirs, symm=symm)
? #divide by the distance between cells
? slope <- geoCorrection(heightDiff)
? return(slope)
}?

slope <- sw_slope(dem)
plot(raster(slope))
click(raster(slope))

sw_conductance <- function(dem, dirs=16, symm=FALSE){
? #first find slope
? slope <- sw_slope(dem, dirs, symm)
??
? #create an index for adjacent cells (adj) with the function adjacent
? #adj <- adjacent(dem, cells=1:ncell(dem), pairs=TRUE, directions=16)
??adj <- adjacencyFromTransition(slope)
? cost <- slope
??
? #extract and replace adjacent cells
? #note the 1/ to convert from resistivity (as cited in publication above) to conductivity
? cost[adj] <- 1 / ((((1337.8 * cost[adj]^6) + (278.19 * cost[adj]^5) - (517.39 * cost[adj]^4) - (78.199 * cost[adj]^3) + (93.419 * cost[adj]^2) + (19.825 * cost[adj]) + 1.64)))
??
? #correction to take account of distance between cell centres
? gc <- geoCorrection(cost)
??
? return(gc)
}

plot(raster(sw_conductance(dem))



On 2 June 2016 at 14:29:42, chris english (englishchristophera at gmail.com) wrote:
> Toby,
>  
> This may seem silly, and a thousandth of something is not zero, but perhaps
> this is a kind of preallocation issue and using Jacob's if else with 0.0
> as else might do the trick.
>  
> Fingers crossed?
> Chris
> On Jun 2, 2016 16:07, "T C Wilkinson" wrote:
>  
> > Dear Jacob,
> >
> > Many thanks for fast reply. [Apologies for the typo and potential
> > confusion on gdistance, I am still relative new to R ? and thus borrowing
> > camelCase habit from previous coding.]
> >
> > I?ve tried your suggested re-worked function to avoid NA in the
> > transition, but I still get the same result.
> >
> > When plotted, the `plot(raster(conductance))` still colours the sea (i.e.
> > doesn?t show it in the colNA colour) and the resultant accCosts etc include
> > the sea as part of the result surface.
> >
> > An interactive `click(raster(conductance))` reveals the sea to be a
> > uniform value of 0.001436461.
> >
> > Thanks again,
> > Toby
> >
> >
> >
> > On 2 June 2016 at 12:30:43, Jacob van Etten (jacobvanetten at yahoo.com)
> > wrote:
> > > Dear Toby,
> > > TransitionLayers should normally not hold NA values. If you want to set
> > cell connections
> > > to zero conductance, the transitionFunction should give a value zero if
> > it finds an NA.
> > > Your function gives an NA as result, which is not what you want:
> > > ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA)
> > >
> > > You probably want to do:
> > > ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0)
> > >
> > > Also, note that gDistance is not the name of the package.
> > > Best,
> > > Jacob
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>  


From englishchristophera at gmail.com  Thu Jun  2 19:00:20 2016
From: englishchristophera at gmail.com (chris english)
Date: Thu, 2 Jun 2016 20:00:20 +0300
Subject: [R-sig-Geo] gdistance transition objects: dealing with NAs in
 source cost raster
In-Reply-To: <etPan.57505eb8.34fbf36a.2df4@Eurasia-2.local>
References: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
	<563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
	<etPan.57502f73.67e9084b.2a1b@Eurasia-2.local>
	<CAASFQpSS2PQy=a8Cd5sPi2m69aj74wDEmwhgr4KX5x23fmUCZQ@mail.gmail.com>
	<etPan.57505eb8.34fbf36a.2df4@Eurasia-2.local>
Message-ID: <CAASFQpTVX-uMbRpM4pJmTXoDXgw+-qNFUh+S=cWjJd0qq18yqA@mail.gmail.com>

Toby,

I've only just installed Jacob's gdistance package and don't know as yet
the underlying assumptions nor the, what I will term 'services', his
package offers. Were I in your position, having my DEM, and knowing what
you already know about using gdistance, I would swap out my sea NA(s) for
0.0(s), in my case probably using GDAL, and see how things went thereafter.

Jacob will have a much better suggestion, but basically it seems like
'package x doesn't like my data, so I'll make my data more consumable for
package x'. Wish I could be of more help but I'm playing in a different
distance space than Hausdorf and probably shouldn't comment further, except
as to what I'd try to finagle as to my DEM.

Chris

On Thu, Jun 2, 2016 at 7:28 PM, T C Wilkinson <open at tobywilkinson.co.uk>
wrote:

> Dear Chris, dear Jacob,
>
> Many thanks indeed for suggestions and help.
>
> 1. The input raster (dem) definitely shows NA in the areas of sea using
> click().
>
> 2. Tried the 0.0 in the ifelse: no luck, same result.
>
> I tried plotting and clicking for the values of the initial `slope`
> TransitionLayer: here the area of the sea reports as a value of NaN, so
> perhaps the problem is not in the transition function, but in the later
> conversion of slope to cost (in sw_conductance).
>
> I tried changing adjacent() to adjacencyFromTransition(), which does
> indeed result in the sea being plotted as NA (or presumably NaN), but the
> values of the conductance on land now don?t seem correct (i.e. some areas
> that I know are flat are shown as relatively lower conductance than areas
> which are sloping).
>
> Any ideas?
>
> Thank you again,
> Toby
>
>
>
> sw_slope <- function(dem, dirs=16, symm=FALSE){
>   # calculate the altitudinal differences between cells:
>   #altDiffFunction <- function(x){x[2] - x[1]}
>   # with a little help from Jacob van Etten and Chris English:
>   altDiffFunction <- function(x){ifelse(((!is.na(x[2])) & (!is.na(x[1]))),
> (x[2] - x[1]), 0.0)}
>
>   heightDiff <- transition(dem, altDiffFunction, directions=dirs,
> symm=symm)
>   #divide by the distance between cells
>   slope <- geoCorrection(heightDiff)
>   return(slope)
> }
>
> slope <- sw_slope(dem)
> plot(raster(slope))
> click(raster(slope))
>
> sw_conductance <- function(dem, dirs=16, symm=FALSE){
>   #first find slope
>   slope <- sw_slope(dem, dirs, symm)
>
>   #create an index for adjacent cells (adj) with the function adjacent
>   #adj <- adjacent(dem, cells=1:ncell(dem), pairs=TRUE, directions=16)
>   adj <- adjacencyFromTransition(slope)
>   cost <- slope
>
>   #extract and replace adjacent cells
>   #note the 1/ to convert from resistivity (as cited in publication above)
> to conductivity
>   cost[adj] <- 1 / ((((1337.8 * cost[adj]^6) + (278.19 * cost[adj]^5) -
> (517.39 * cost[adj]^4) - (78.199 * cost[adj]^3) + (93.419 * cost[adj]^2) +
> (19.825 * cost[adj]) + 1.64)))
>
>   #correction to take account of distance between cell centres
>   gc <- geoCorrection(cost)
>
>   return(gc)
> }
>
> plot(raster(sw_conductance(dem))
>
>
>
> On 2 June 2016 at 14:29:42, chris english (englishchristophera at gmail.com)
> wrote:
> > Toby,
> >
> > This may seem silly, and a thousandth of something is not zero, but
> perhaps
> > this is a kind of preallocation issue and using Jacob's if else with 0.0
> > as else might do the trick.
> >
> > Fingers crossed?
> > Chris
> > On Jun 2, 2016 16:07, "T C Wilkinson" wrote:
> >
> > > Dear Jacob,
> > >
> > > Many thanks for fast reply. [Apologies for the typo and potential
> > > confusion on gdistance, I am still relative new to R ? and thus
> borrowing
> > > camelCase habit from previous coding.]
> > >
> > > I?ve tried your suggested re-worked function to avoid NA in the
> > > transition, but I still get the same result.
> > >
> > > When plotted, the `plot(raster(conductance))` still colours the sea
> (i.e.
> > > doesn?t show it in the colNA colour) and the resultant accCosts etc
> include
> > > the sea as part of the result surface.
> > >
> > > An interactive `click(raster(conductance))` reveals the sea to be a
> > > uniform value of 0.001436461.
> > >
> > > Thanks again,
> > > Toby
> > >
> > >
> > >
> > > On 2 June 2016 at 12:30:43, Jacob van Etten (jacobvanetten at yahoo.com)
> > > wrote:
> > > > Dear Toby,
> > > > TransitionLayers should normally not hold NA values. If you want to
> set
> > > cell connections
> > > > to zero conductance, the transitionFunction should give a value zero
> if
> > > it finds an NA.
> > > > Your function gives an NA as result, which is not what you want:
> > > > ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA)
> > > >
> > > > You probably want to do:
> > > > ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0)
> > > >
> > > > Also, note that gDistance is not the name of the package.
> > > > Best,
> > > > Jacob
> > >
> > > _______________________________________________
> > > R-sig-Geo mailing list
> > > R-sig-Geo at r-project.org
> > > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
>

	[[alternative HTML version deleted]]


From jacobvanetten at yahoo.com  Thu Jun  2 19:16:40 2016
From: jacobvanetten at yahoo.com (Jacob van Etten)
Date: Thu, 2 Jun 2016 17:16:40 +0000 (UTC)
Subject: [R-sig-Geo] gdistance transition objects: dealing with NAs in
 source cost raster
In-Reply-To: <CAASFQpTVX-uMbRpM4pJmTXoDXgw+-qNFUh+S=cWjJd0qq18yqA@mail.gmail.com>
References: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
	<563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
	<etPan.57502f73.67e9084b.2a1b@Eurasia-2.local>
	<CAASFQpSS2PQy=a8Cd5sPi2m69aj74wDEmwhgr4KX5x23fmUCZQ@mail.gmail.com>
	<etPan.57505eb8.34fbf36a.2df4@Eurasia-2.local>
	<CAASFQpTVX-uMbRpM4pJmTXoDXgw+-qNFUh+S=cWjJd0qq18yqA@mail.gmail.com>
Message-ID: <631169623.6400845.1464887800473.JavaMail.yahoo@mail.yahoo.com>

There is a contradiction in that you create zero conductance for the sea, but then use this very long formula to convert from resistance (cost) to conductance, whereas you already have conductance values for the sea. The formula gives this result for a value of zero.
# value zerocost <- c(0)adj <- 1
# your complicated formula1 / ((((1337.8 * cost[adj]^6) + (278.19 * cost[adj]^5) - (517.39 * cost[adj]^4) - (78.199 * cost[adj]^3) + (93.419 * cost[adj]^2) + (19.825 * cost[adj]) + 1.64)))[1] 0.6097561
It usually works better if you think about the conversion you need and write a single transitionFunction and test this with some values before you plug it into the transition() function. The transition function does nothing special with it, so it is just a matter of clear thinking.
Actually, there is no need to work with adjacency() in your case, I think. Try to create a single transitionFunction...
Jacob
 

    On Thursday, 2 June 2016, 11:00, chris english <englishchristophera at gmail.com> wrote:
 

 Toby,
I've only just installed Jacob's gdistance package and don't know as yet the underlying assumptions nor the, what I will term 'services', his package offers. Were I in your position, having my DEM, and knowing what you already know about using gdistance, I would swap out my sea NA(s) for 0.0(s), in my case probably using GDAL, and see how things went thereafter.?
Jacob will have a much better suggestion, but basically it seems like 'package x doesn't like my data, so I'll make my data more consumable for package x'. Wish I could be of more help but I'm playing in a different distance space than Hausdorf and probably shouldn't comment further, except as to what I'd try to finagle as to my DEM.
Chris?
On Thu, Jun 2, 2016 at 7:28 PM, T C Wilkinson <open at tobywilkinson.co.uk> wrote:

Dear Chris, dear Jacob,

Many thanks indeed for suggestions and help.

1. The input raster (dem) definitely shows NA in the areas of sea using click().

2. Tried the 0.0 in the ifelse: no luck, same result.

I tried plotting and clicking for the values of the initial `slope` TransitionLayer: here the area of the sea reports as a value of NaN, so perhaps the problem is not in the transition function, but in the later conversion of slope to cost (in sw_conductance).

I tried changing adjacent() to adjacencyFromTransition(), which does indeed result in the sea being plotted as NA (or presumably NaN), but the values of the conductance on land now don?t seem correct (i.e. some areas that I know are flat are shown as relatively lower conductance than areas which are sloping).

Any ideas?

Thank you again,
Toby



sw_slope <- function(dem, dirs=16, symm=FALSE){
? # calculate the altitudinal differences between cells:
? #altDiffFunction <- function(x){x[2] - x[1]}
? # with a little help from Jacob van Etten and Chris English:
? altDiffFunction <- function(x){ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0.0)}
??
? heightDiff <- transition(dem, altDiffFunction, directions=dirs, symm=symm)
? #divide by the distance between cells
? slope <- geoCorrection(heightDiff)
? return(slope)
}?

slope <- sw_slope(dem)
plot(raster(slope))
click(raster(slope))

sw_conductance <- function(dem, dirs=16, symm=FALSE){
? #first find slope
? slope <- sw_slope(dem, dirs, symm)
??
? #create an index for adjacent cells (adj) with the function adjacent
? #adj <- adjacent(dem, cells=1:ncell(dem), pairs=TRUE, directions=16)
??adj <- adjacencyFromTransition(slope)
? cost <- slope
??
? #extract and replace adjacent cells
? #note the 1/ to convert from resistivity (as cited in publication above) to conductivity
? cost[adj] <- 1 / ((((1337.8 * cost[adj]^6) + (278.19 * cost[adj]^5) - (517.39 * cost[adj]^4) - (78.199 * cost[adj]^3) + (93.419 * cost[adj]^2) + (19.825 * cost[adj]) + 1.64)))
??
? #correction to take account of distance between cell centres
? gc <- geoCorrection(cost)
??
? return(gc)
}

plot(raster(sw_conductance(dem))



On 2 June 2016 at 14:29:42, chris english (englishchristophera at gmail.com) wrote:
> Toby,
>
> This may seem silly, and a thousandth of something is not zero, but perhaps
> this is a kind of preallocation issue and using Jacob's if else with 0.0
> as else might do the trick.
>
> Fingers crossed?
> Chris
> On Jun 2, 2016 16:07, "T C Wilkinson" wrote:
>
> > Dear Jacob,
> >
> > Many thanks for fast reply. [Apologies for the typo and potential
> > confusion on gdistance, I am still relative new to R ? and thus borrowing
> > camelCase habit from previous coding.]
> >
> > I?ve tried your suggested re-worked function to avoid NA in the
> > transition, but I still get the same result.
> >
> > When plotted, the `plot(raster(conductance))` still colours the sea (i.e.
> > doesn?t show it in the colNA colour) and the resultant accCosts etc include
> > the sea as part of the result surface.
> >
> > An interactive `click(raster(conductance))` reveals the sea to be a
> > uniform value of 0.001436461.
> >
> > Thanks again,
> > Toby
> >
> >
> >
> > On 2 June 2016 at 12:30:43, Jacob van Etten (jacobvanetten at yahoo.com)
> > wrote:
> > > Dear Toby,
> > > TransitionLayers should normally not hold NA values. If you want to set
> > cell connections
> > > to zero conductance, the transitionFunction should give a value zero if
> > it finds an NA.
> > > Your function gives an NA as result, which is not what you want:
> > > ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA)
> > >
> > > You probably want to do:
> > > ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0)
> > >
> > > Also, note that gDistance is not the name of the package.
> > > Best,
> > > Jacob
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>





  
	[[alternative HTML version deleted]]


From jacobvanetten at yahoo.com  Thu Jun  2 19:23:23 2016
From: jacobvanetten at yahoo.com (Jacob van Etten)
Date: Thu, 2 Jun 2016 17:23:23 +0000 (UTC)
Subject: [R-sig-Geo] gdistance transition objects: dealing with NAs in
 source cost raster
In-Reply-To: <etPan.57502f73.67e9084b.2a1b@Eurasia-2.local>
References: <etPan.575008d5.32e55b96.2a1b@Eurasia-2.local>
	<563374375.5966941.1464867029799.JavaMail.yahoo@mail.yahoo.com>
	<etPan.57502f73.67e9084b.2a1b@Eurasia-2.local>
Message-ID: <420827062.6349891.1464888203794.JavaMail.yahoo@mail.yahoo.com>

I saw that my mail application does not respect the line breaks...
Here the correct R code.
# value zerocost <- c(0)
adj <- 1
# your complicated formula
1 / ((((1337.8 * cost[adj]^6) + (278.19 * cost[adj]^5) - (517.39 * cost[adj]^4) - (78.199 * cost[adj]^3) + (93.419 * cost[adj]^2) + (19.825 * cost[adj]) + 1.64)))[1] 0.6097561 

    On Thursday, 2 June 2016, 7:07, T C Wilkinson <open at tobywilkinson.co.uk> wrote:
 

 Dear Jacob,

Many thanks for fast reply. [Apologies for the typo and potential confusion on gdistance, I am still relative new to R ? and thus borrowing camelCase habit from previous coding.]

I?ve tried your suggested re-worked function to avoid NA in the transition, but I still get the same result.

When plotted, the `plot(raster(conductance))` still colours the sea (i.e. doesn?t show it in the colNA colour) and the resultant accCosts etc include the sea as part of the result surface.

An interactive `click(raster(conductance))` reveals the sea to be a uniform value of 0.001436461.

Thanks again,
Toby



On 2 June 2016 at 12:30:43, Jacob van Etten (jacobvanetten at yahoo.com) wrote:
> Dear Toby,
> TransitionLayers should normally not hold NA values. If you want to set cell connections? 
> to zero conductance, the transitionFunction should give a value zero if it finds an NA.? 
> Your function gives an NA as result, which is not what you want:
> ifelse(((!is.na(x[2])) & (!is.na(x[1]))),(x[2] - x[1]),NA)
>? 
> You probably want to do:
> ifelse(((!is.na(x[2])) & (!is.na(x[1]))), (x[2] - x[1]), 0)
>? 
> Also, note that gDistance is not the name of the package.
> Best,
> Jacob



  
	[[alternative HTML version deleted]]


From thi_veloso at yahoo.com.br  Thu Jun  2 22:30:48 2016
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Thu, 2 Jun 2016 20:30:48 +0000 (UTC)
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>

Dear all,

I am working with daily time series of meteorological variables. This is an example of the dataset:

library(raster)

# Create date sequence
idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")

# Create raster stack and assign dates
r <- raster(ncol=20, nrow=20)
s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
s <- setZ(s, idx)


Now, let's assume those values represent daily precipitation. What I need to do is to integrate daily to monthly values, 
and then take a monthly climatology. Climatology in this case means multi-year average of selected months, e.g., an average of the 30 Octobers from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.

On the other hand, let's assume the raster values represent daily temperature. Integrating daily to monthly temperature doesn't make sense. Hence, instead of integrating daily values, I need to take monthly means (e.g. mean value of all days in every month), and then calculate the climatology.

What would be the best approach to achieve that using the raster package?

 Greetings,
 -- Thiago V. dos Santos

PhD student
Land and Atmospheric Science
University of Minnesota


From vijaylulla at gmail.com  Fri Jun  3 02:02:01 2016
From: vijaylulla at gmail.com (Vijay Lulla)
Date: Thu, 2 Jun 2016 20:02:01 -0400
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>

I think the following StackOverflow question has the answer:
http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775

Following the instructions listed on that page for your case might go
something like below:

> idxYM <- as.integer(strftime(idx,"%Y%m"))
> idxM <- unique(idxYM)%%100
> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
> meanYM
class       : RasterBrick
dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
resolution  : 18, 9  (x, y)
extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
data source : in memory
names       : X196101, X196102, X196103, X196104, X196105, X196106,
X196107, X196108, X196109, X196110, X196111, X196112, X196201,
X196202, X196203, ...
min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
 0.3334, ...
max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
 0.6255, ...

> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
> meanM
class       : RasterBrick
dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
resolution  : 18, 9  (x, y)
extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
data source : in memory
names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
   X8,     X9,    X10,    X11,    X12
min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
0.4724, 0.4629, 0.4774, 0.4736, 0.4708
max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
0.5260, 0.5256, 0.5281, 0.5279, 0.5286

>

I'm not sure how [in]efficient this is for actual (i.e. not toy
example) data.  Maybe others more experienced, and knowledgeable,
members can provide better answers.

HTH,
Vijay.

On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
> Dear all,
>
> I am working with daily time series of meteorological variables. This is an example of the dataset:
>
> library(raster)
>
> # Create date sequence
> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>
> # Create raster stack and assign dates
> r <- raster(ncol=20, nrow=20)
> s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
> s <- setZ(s, idx)
>
>
> Now, let's assume those values represent daily precipitation. What I need to do is to integrate daily to monthly values,
> and then take a monthly climatology. Climatology in this case means multi-year average of selected months, e.g., an average of the 30 Octobers from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>
> On the other hand, let's assume the raster values represent daily temperature. Integrating daily to monthly temperature doesn't make sense. Hence, instead of integrating daily values, I need to take monthly means (e.g. mean value of all days in every month), and then calculate the climatology.
>
> What would be the best approach to achieve that using the raster package?
>
>  Greetings,
>  -- Thiago V. dos Santos
>
> PhD student
> Land and Atmospheric Science
> University of Minnesota
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From thi_veloso at yahoo.com.br  Fri Jun  3 04:05:09 2016
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Fri, 3 Jun 2016 02:05:09 +0000 (UTC)
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
Message-ID: <530880567.497436.1464919509268.JavaMail.yahoo@mail.yahoo.com>

Dear Vijay,

Thank you so much for your answer - it was exactly what I was looking for.

In fact, I compared your code (applied to my actual data) to the code I wrote in cdo (a tool to deal with climate data in netcdf format) and the results match to the eleventh decimal place. With the obvious advantage of not breaking my workflow in R.
 Cheers,
 -- Thiago V. dos Santos

PhD student
Land and Atmospheric Science
University of Minnesota



On Thursday, June 2, 2016 7:02 PM, Vijay Lulla <vijaylulla at gmail.com> wrote:
I think the following StackOverflow question has the answer:
http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775

Following the instructions listed on that page for your case might go
something like below:

> idxYM <- as.integer(strftime(idx,"%Y%m"))
> idxM <- unique(idxYM)%%100
> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
> meanYM
class       : RasterBrick
dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
resolution  : 18, 9  (x, y)
extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
data source : in memory
names       : X196101, X196102, X196103, X196104, X196105, X196106,
X196107, X196108, X196109, X196110, X196111, X196112, X196201,
X196202, X196203, ...
min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
0.3334, ...
max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
0.6255, ...

> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
> meanM
class       : RasterBrick
dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
resolution  : 18, 9  (x, y)
extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
data source : in memory
names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
   X8,     X9,    X10,    X11,    X12
min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
0.4724, 0.4629, 0.4774, 0.4736, 0.4708
max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
0.5260, 0.5256, 0.5281, 0.5279, 0.5286

>

I'm not sure how [in]efficient this is for actual (i.e. not toy
example) data.  Maybe others more experienced, and knowledgeable,
members can provide better answers.

HTH,
Vijay.


On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
> Dear all,
>
> I am working with daily time series of meteorological variables. This is an example of the dataset:
>
> library(raster)
>
> # Create date sequence
> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>
> # Create raster stack and assign dates
> r <- raster(ncol=20, nrow=20)
> s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
> s <- setZ(s, idx)
>
>
> Now, let's assume those values represent daily precipitation. What I need to do is to integrate daily to monthly values,
> and then take a monthly climatology. Climatology in this case means multi-year average of selected months, e.g., an average of the 30 Octobers from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>
> On the other hand, let's assume the raster values represent daily temperature. Integrating daily to monthly temperature doesn't make sense. Hence, instead of integrating daily values, I need to take monthly means (e.g. mean value of all days in every month), and then calculate the climatology.
>
> What would be the best approach to achieve that using the raster package?
>
>  Greetings,
>  -- Thiago V. dos Santos
>
> PhD student
> Land and Atmospheric Science
> University of Minnesota
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From loic.dutrieux at wur.nl  Fri Jun  3 11:21:55 2016
From: loic.dutrieux at wur.nl (=?UTF-8?Q?Lo=c3=afc_Dutrieux?=)
Date: Fri, 3 Jun 2016 11:21:55 +0200
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
Message-ID: <57514C33.7010400@wur.nl>

This can also be done with zApply:

library(zoo)

sYM <- zApply(s, by = as.yearmon, sum)
sM <- zApply(sYM, by = months, mean)

Cheers,
Lo?c

On 06/03/2016 02:02 AM, Vijay Lulla wrote:
> I think the following StackOverflow question has the answer:
> http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775
>
> Following the instructions listed on that page for your case might go
> something like below:
>
>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>> idxM <- unique(idxYM)%%100
>> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
>> meanYM
> class       : RasterBrick
> dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
> resolution  : 18, 9  (x, y)
> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> data source : in memory
> names       : X196101, X196102, X196103, X196104, X196105, X196106,
> X196107, X196108, X196109, X196110, X196111, X196112, X196201,
> X196202, X196203, ...
> min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
> 0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
>   0.3334, ...
> max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
> 0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
>   0.6255, ...
>
>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>> meanM
> class       : RasterBrick
> dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
> resolution  : 18, 9  (x, y)
> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> data source : in memory
> names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
>     X8,     X9,    X10,    X11,    X12
> min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
> 0.4724, 0.4629, 0.4774, 0.4736, 0.4708
> max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
> 0.5260, 0.5256, 0.5281, 0.5279, 0.5286
>
>>
>
> I'm not sure how [in]efficient this is for actual (i.e. not toy
> example) data.  Maybe others more experienced, and knowledgeable,
> members can provide better answers.
>
> HTH,
> Vijay.
>
> On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
> <r-sig-geo at r-project.org> wrote:
>> Dear all,
>>
>> I am working with daily time series of meteorological variables. This is an example of the dataset:
>>
>> library(raster)
>>
>> # Create date sequence
>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>
>> # Create raster stack and assign dates
>> r <- raster(ncol=20, nrow=20)
>> s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
>> s <- setZ(s, idx)
>>
>>
>> Now, let's assume those values represent daily precipitation. What I need to do is to integrate daily to monthly values,
>> and then take a monthly climatology. Climatology in this case means multi-year average of selected months, e.g., an average of the 30 Octobers from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>>
>> On the other hand, let's assume the raster values represent daily temperature. Integrating daily to monthly temperature doesn't make sense. Hence, instead of integrating daily values, I need to take monthly means (e.g. mean value of all days in every month), and then calculate the climatology.
>>
>> What would be the best approach to achieve that using the raster package?
>>
>>   Greetings,
>>   -- Thiago V. dos Santos
>>
>> PhD student
>> Land and Atmospheric Science
>> University of Minnesota
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From thi_veloso at yahoo.com.br  Fri Jun  3 17:30:44 2016
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Fri, 3 Jun 2016 15:30:44 +0000 (UTC)
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <57514C33.7010400@wur.nl>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
	<57514C33.7010400@wur.nl>
Message-ID: <2127614783.774242.1464967844732.JavaMail.yahoo@mail.yahoo.com>

Cool Lo?c, thanks for showing one more option.

By the way, in this case zApply is surprisingly faster than calc:

> system.time(meanYM <- calc(s,fun=function(x) { by(x, idxYM, sum) }))
user  system elapsed 
21.700   0.248  22.095 


> system.time(sYM <- zApply(s, by = as.yearmon, sum))
user  system elapsed 
0.811   0.047   0.866

 Cheers,
 -- Thiago V. dos Santos

PhD student
Land and Atmospheric Science
University of Minnesota



On Friday, June 3, 2016 4:25 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl> wrote:
This can also be done with zApply:

library(zoo)

sYM <- zApply(s, by = as.yearmon, sum)
sM <- zApply(sYM, by = months, mean)

Cheers,
Lo?c

On 06/03/2016 02:02 AM, Vijay Lulla wrote:
> I think the following StackOverflow question has the answer:
> http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775
>
> Following the instructions listed on that page for your case might go
> something like below:
>
>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>> idxM <- unique(idxYM)%%100
>> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
>> meanYM
> class       : RasterBrick
> dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
> resolution  : 18, 9  (x, y)
> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> data source : in memory
> names       : X196101, X196102, X196103, X196104, X196105, X196106,
> X196107, X196108, X196109, X196110, X196111, X196112, X196201,
> X196202, X196203, ...
> min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
> 0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
>   0.3334, ...
> max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
> 0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
>   0.6255, ...
>
>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>> meanM
> class       : RasterBrick
> dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
> resolution  : 18, 9  (x, y)
> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> data source : in memory
> names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
>     X8,     X9,    X10,    X11,    X12
> min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
> 0.4724, 0.4629, 0.4774, 0.4736, 0.4708
> max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
> 0.5260, 0.5256, 0.5281, 0.5279, 0.5286
>
>>
>
> I'm not sure how [in]efficient this is for actual (i.e. not toy
> example) data.  Maybe others more experienced, and knowledgeable,
> members can provide better answers.
>
> HTH,
> Vijay.
>
> On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
> <r-sig-geo at r-project.org> wrote:
>> Dear all,
>>
>> I am working with daily time series of meteorological variables. This is an example of the dataset:
>>
>> library(raster)
>>
>> # Create date sequence
>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>
>> # Create raster stack and assign dates
>> r <- raster(ncol=20, nrow=20)
>> s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
>> s <- setZ(s, idx)
>>
>>
>> Now, let's assume those values represent daily precipitation. What I need to do is to integrate daily to monthly values,
>> and then take a monthly climatology. Climatology in this case means multi-year average of selected months, e.g., an average of the 30 Octobers from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>>
>> On the other hand, let's assume the raster values represent daily temperature. Integrating daily to monthly temperature doesn't make sense. Hence, instead of integrating daily values, I need to take monthly means (e.g. mean value of all days in every month), and then calculate the climatology.
>>
>> What would be the best approach to achieve that using the raster package?
>>
>>   Greetings,
>>   -- Thiago V. dos Santos
>>
>> PhD student
>> Land and Atmospheric Science
>> University of Minnesota
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

>

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From vijaylulla at gmail.com  Fri Jun  3 20:47:17 2016
From: vijaylulla at gmail.com (Vijay Lulla)
Date: Fri, 3 Jun 2016 14:47:17 -0400
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <2127614783.774242.1464967844732.JavaMail.yahoo@mail.yahoo.com>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
	<57514C33.7010400@wur.nl>
	<2127614783.774242.1464967844732.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAKkiGbsx=Kt6dagz0fHCiGA7tHZzMkZyFFbr2MBEkqSmxjX35A@mail.gmail.com>

Looking at Lo?c's answer and from reading ?zApply I learned of
stackApply, which is used internally by zApply.  While zApply is for
time series of layers, stackApply is more general and can be used for
applying functions to groups of layers in a stack/brick.  It is very
similar to base R's `tapply`.  And it is also fast!

> system.time(meanYM1 <- stackApply(s, idxYM, mean))
   user  system elapsed
   0.45    0.03    0.48
> system.time(meanYM <- calc(s, fun=function(x) { by(x, idxYM, mean)}))
   user  system elapsed
  17.50    0.01   17.61
> all(meanYM[] == meanYM1[])
[1] TRUE

Thanks Lo?c for pointing out zApply.

On Fri, Jun 3, 2016 at 11:30 AM, Thiago V. dos Santos via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
> Cool Lo?c, thanks for showing one more option.
>
> By the way, in this case zApply is surprisingly faster than calc:
>
>> system.time(meanYM <- calc(s,fun=function(x) { by(x, idxYM, sum) }))
> user  system elapsed
> 21.700   0.248  22.095
>
>
>> system.time(sYM <- zApply(s, by = as.yearmon, sum))
> user  system elapsed
> 0.811   0.047   0.866
>
>  Cheers,
>  -- Thiago V. dos Santos
>
> PhD student
> Land and Atmospheric Science
> University of Minnesota
>
>
>
> On Friday, June 3, 2016 4:25 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl> wrote:
> This can also be done with zApply:
>
> library(zoo)
>
> sYM <- zApply(s, by = as.yearmon, sum)
> sM <- zApply(sYM, by = months, mean)
>
> Cheers,
> Lo?c
>
> On 06/03/2016 02:02 AM, Vijay Lulla wrote:
>> I think the following StackOverflow question has the answer:
>> http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775
>>
>> Following the instructions listed on that page for your case might go
>> something like below:
>>
>>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>>> idxM <- unique(idxYM)%%100
>>> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
>>> meanYM
>> class       : RasterBrick
>> dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
>> resolution  : 18, 9  (x, y)
>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>> data source : in memory
>> names       : X196101, X196102, X196103, X196104, X196105, X196106,
>> X196107, X196108, X196109, X196110, X196111, X196112, X196201,
>> X196202, X196203, ...
>> min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
>> 0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
>>   0.3334, ...
>> max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
>> 0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
>>   0.6255, ...
>>
>>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>>> meanM
>> class       : RasterBrick
>> dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
>> resolution  : 18, 9  (x, y)
>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>> data source : in memory
>> names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
>>     X8,     X9,    X10,    X11,    X12
>> min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
>> 0.4724, 0.4629, 0.4774, 0.4736, 0.4708
>> max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
>> 0.5260, 0.5256, 0.5281, 0.5279, 0.5286
>>
>>>
>>
>> I'm not sure how [in]efficient this is for actual (i.e. not toy
>> example) data.  Maybe others more experienced, and knowledgeable,
>> members can provide better answers.
>>
>> HTH,
>> Vijay.
>>
>> On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
>> <r-sig-geo at r-project.org> wrote:
>>> Dear all,
>>>
>>> I am working with daily time series of meteorological variables. This is an example of the dataset:
>>>
>>> library(raster)
>>>
>>> # Create date sequence
>>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>>
>>> # Create raster stack and assign dates
>>> r <- raster(ncol=20, nrow=20)
>>> s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
>>> s <- setZ(s, idx)
>>>
>>>
>>> Now, let's assume those values represent daily precipitation. What I need to do is to integrate daily to monthly values,
>>> and then take a monthly climatology. Climatology in this case means multi-year average of selected months, e.g., an average of the 30 Octobers from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>>>
>>> On the other hand, let's assume the raster values represent daily temperature. Integrating daily to monthly temperature doesn't make sense. Hence, instead of integrating daily values, I need to take monthly means (e.g. mean value of all days in every month), and then calculate the climatology.
>>>
>>> What would be the best approach to achieve that using the raster package?
>>>
>>>   Greetings,
>>>   -- Thiago V. dos Santos
>>>
>>> PhD student
>>> Land and Atmospheric Science
>>> University of Minnesota
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From thi_veloso at yahoo.com.br  Fri Jun  3 22:25:41 2016
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Fri, 3 Jun 2016 20:25:41 +0000 (UTC)
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <CAKkiGbsx=Kt6dagz0fHCiGA7tHZzMkZyFFbr2MBEkqSmxjX35A@mail.gmail.com>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
	<57514C33.7010400@wur.nl>
	<2127614783.774242.1464967844732.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsx=Kt6dagz0fHCiGA7tHZzMkZyFFbr2MBEkqSmxjX35A@mail.gmail.com>
Message-ID: <1875625356.902419.1464985541127.JavaMail.yahoo@mail.yahoo.com>

Dear Vijay and Lo?c,


I have identified some artifacts in the results when using zApply on my data. To reproduce it, please download this shapefile (https://www.dropbox.com/s/in2z10mlerr2sfu/southern.zip?dl=0) and run the code below (see the comments after the plot commands):

-----------------------
library(raster)
library(zoo)

# Create the date sequence
idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")

# Create raster stack and assign dates
r <- raster(ncol=24, nrow=21, xmn=-58, xmx=-47.5, ymn=-34, ymx=-22, resolution=0.5)
s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
s <- setZ(s, idx)

# Load shapefile and crop data
shp <- shapefile('~/Downloads/southern.shp', warnPRJ=F)
s.c <- crop(s, extent(shp))
s.c <- mask(s.c, shp)
plot(s.c,1)
plot(shp,add=T) # Looks good

# Lo?c's approach using zApply
sYM <- zApply(s.c, by = as.yearmon, sum)
sM <- zApply(sYM, by = months, mean)
plot(sYM,1)  # Note that the "empty" space across the extent was filled with 0's
plot(sM,1)  # The same "gray area" here

# Vijay's approach using calc
idxYM <- as.integer(strftime(idx,"%Y%m"))
idxM <- unique(idxYM)%%100
meanYM <- calc(s.c, fun=function(x) { by(x, idxYM, sum) })
meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
plot(meanYM,1)  # Note that no 0 was added across the extent
plot(meanM,1)  # This is OK too, no grey area
-----------------------

Basically, a portion of the raster extent was filled with 0's after using zApply. It doesn't happen when using calc.

Any ideas on what can be causing this?

 Greetings,
 -- Thiago V. dos Santos

PhD student
Land and Atmospheric Science
University of Minnesota



On Friday, June 3, 2016 1:47 PM, Vijay Lulla <vijaylulla at gmail.com> wrote:
Looking at Lo?c's answer and from reading ?zApply I learned of
stackApply, which is used internally by zApply.  While zApply is for
time series of layers, stackApply is more general and can be used for
applying functions to groups of layers in a stack/brick.  It is very
similar to base R's `tapply`.  And it is also fast!

> system.time(meanYM1 <- stackApply(s, idxYM, mean))
   user  system elapsed
   0.45    0.03    0.48
> system.time(meanYM <- calc(s, fun=function(x) { by(x, idxYM, mean)}))
   user  system elapsed
  17.50    0.01   17.61
> all(meanYM[] == meanYM1[])
[1] TRUE

Thanks Lo?c for pointing out zApply.


On Fri, Jun 3, 2016 at 11:30 AM, Thiago V. dos Santos via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
> Cool Lo?c, thanks for showing one more option.
>
> By the way, in this case zApply is surprisingly faster than calc:
>
>> system.time(meanYM <- calc(s,fun=function(x) { by(x, idxYM, sum) }))
> user  system elapsed
> 21.700   0.248  22.095
>
>
>> system.time(sYM <- zApply(s, by = as.yearmon, sum))
> user  system elapsed
> 0.811   0.047   0.866
>
>  Cheers,
>  -- Thiago V. dos Santos
>
> PhD student
> Land and Atmospheric Science
> University of Minnesota
>
>
>
> On Friday, June 3, 2016 4:25 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl> wrote:
> This can also be done with zApply:
>
> library(zoo)
>
> sYM <- zApply(s, by = as.yearmon, sum)
> sM <- zApply(sYM, by = months, mean)
>
> Cheers,
> Lo?c
>
> On 06/03/2016 02:02 AM, Vijay Lulla wrote:
>> I think the following StackOverflow question has the answer:
>> http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775
>>
>> Following the instructions listed on that page for your case might go
>> something like below:
>>
>>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>>> idxM <- unique(idxYM)%%100
>>> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
>>> meanYM
>> class       : RasterBrick
>> dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
>> resolution  : 18, 9  (x, y)
>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>> data source : in memory
>> names       : X196101, X196102, X196103, X196104, X196105, X196106,
>> X196107, X196108, X196109, X196110, X196111, X196112, X196201,
>> X196202, X196203, ...
>> min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
>> 0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
>>   0.3334, ...
>> max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
>> 0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
>>   0.6255, ...
>>
>>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>>> meanM
>> class       : RasterBrick
>> dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
>> resolution  : 18, 9  (x, y)
>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>> data source : in memory
>> names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
>>     X8,     X9,    X10,    X11,    X12
>> min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
>> 0.4724, 0.4629, 0.4774, 0.4736, 0.4708
>> max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
>> 0.5260, 0.5256, 0.5281, 0.5279, 0.5286
>>
>>>
>>
>> I'm not sure how [in]efficient this is for actual (i.e. not toy
>> example) data.  Maybe others more experienced, and knowledgeable,
>> members can provide better answers.
>>
>> HTH,
>> Vijay.
>>
>> On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
>> <r-sig-geo at r-project.org> wrote:
>>> Dear all,
>>>
>>> I am working with daily time series of meteorological variables. This is an example of the dataset:
>>>
>>> library(raster)
>>>
>>> # Create date sequence
>>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>>
>>> # Create raster stack and assign dates
>>> r <- raster(ncol=20, nrow=20)
>>> s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
>>> s <- setZ(s, idx)
>>>
>>>
>>> Now, let's assume those values represent daily precipitation. What I need to do is to integrate daily to monthly values,
>>> and then take a monthly climatology. Climatology in this case means multi-year average of selected months, e.g., an average of the 30 Octobers from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>>>
>>> On the other hand, let's assume the raster values represent daily temperature. Integrating daily to monthly temperature doesn't make sense. Hence, instead of integrating daily values, I need to take monthly means (e.g. mean value of all days in every month), and then calculate the climatology.
>>>
>>> What would be the best approach to achieve that using the raster package?
>>>
>>>   Greetings,
>>>   -- Thiago V. dos Santos
>>>
>>> PhD student
>>> Land and Atmospheric Science
>>> University of Minnesota
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From loic.dutrieux at wur.nl  Sat Jun  4 15:48:45 2016
From: loic.dutrieux at wur.nl (=?UTF-8?Q?Lo=c3=afc_Dutrieux?=)
Date: Sat, 4 Jun 2016 15:48:45 +0200
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <1875625356.902419.1464985541127.JavaMail.yahoo@mail.yahoo.com>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
	<57514C33.7010400@wur.nl>
	<2127614783.774242.1464967844732.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsx=Kt6dagz0fHCiGA7tHZzMkZyFFbr2MBEkqSmxjX35A@mail.gmail.com>
	<1875625356.902419.1464985541127.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5752DC3D.1060904@wur.nl>

Hi Thiago,

Yes, I can reproduce.
It's probably because zApply() defaults to na.rm = TRUE (in fact the 
na.rm = argument is ignored if you pass it), and

sum(c(NA, NA), na.rm = TRUE)
[1] 0 (I was surprised by that).

Perhaps the stackApply() call in the zApply() function definition is 
missing an ellipsis.

Cheers,
Lo?c

On 06/03/2016 10:25 PM, Thiago V. dos Santos wrote:
> Dear Vijay and Lo?c,
>
>
> I have identified some artifacts in the results when using zApply on my data. To reproduce it, please download this shapefile (https://www.dropbox.com/s/in2z10mlerr2sfu/southern.zip?dl=0) and run the code below (see the comments after the plot commands):
>
> -----------------------
> library(raster)
> library(zoo)
>
> # Create the date sequence
> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>
> # Create raster stack and assign dates
> r <- raster(ncol=24, nrow=21, xmn=-58, xmx=-47.5, ymn=-34, ymx=-22, resolution=0.5)
> s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
> s <- setZ(s, idx)
>
> # Load shapefile and crop data
> shp <- shapefile('~/Downloads/southern.shp', warnPRJ=F)
> s.c <- crop(s, extent(shp))
> s.c <- mask(s.c, shp)
> plot(s.c,1)
> plot(shp,add=T) # Looks good
>
> # Lo?c's approach using zApply
> sYM <- zApply(s.c, by = as.yearmon, sum)
> sM <- zApply(sYM, by = months, mean)
> plot(sYM,1)  # Note that the "empty" space across the extent was filled with 0's
> plot(sM,1)  # The same "gray area" here
>
> # Vijay's approach using calc
> idxYM <- as.integer(strftime(idx,"%Y%m"))
> idxM <- unique(idxYM)%%100
> meanYM <- calc(s.c, fun=function(x) { by(x, idxYM, sum) })
> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
> plot(meanYM,1)  # Note that no 0 was added across the extent
> plot(meanM,1)  # This is OK too, no grey area
> -----------------------
>
> Basically, a portion of the raster extent was filled with 0's after using zApply. It doesn't happen when using calc.
>
> Any ideas on what can be causing this?
>
>   Greetings,
>   -- Thiago V. dos Santos
>
> PhD student
> Land and Atmospheric Science
> University of Minnesota
>
>
>
> On Friday, June 3, 2016 1:47 PM, Vijay Lulla <vijaylulla at gmail.com> wrote:
> Looking at Lo?c's answer and from reading ?zApply I learned of
> stackApply, which is used internally by zApply.  While zApply is for
> time series of layers, stackApply is more general and can be used for
> applying functions to groups of layers in a stack/brick.  It is very
> similar to base R's `tapply`.  And it is also fast!
>
>> system.time(meanYM1 <- stackApply(s, idxYM, mean))
>     user  system elapsed
>     0.45    0.03    0.48
>> system.time(meanYM <- calc(s, fun=function(x) { by(x, idxYM, mean)}))
>     user  system elapsed
>    17.50    0.01   17.61
>> all(meanYM[] == meanYM1[])
> [1] TRUE
>
> Thanks Lo?c for pointing out zApply.
>
>
> On Fri, Jun 3, 2016 at 11:30 AM, Thiago V. dos Santos via R-sig-Geo
> <r-sig-geo at r-project.org> wrote:
>> Cool Lo?c, thanks for showing one more option.
>>
>> By the way, in this case zApply is surprisingly faster than calc:
>>
>>> system.time(meanYM <- calc(s,fun=function(x) { by(x, idxYM, sum) }))
>> user  system elapsed
>> 21.700   0.248  22.095
>>
>>
>>> system.time(sYM <- zApply(s, by = as.yearmon, sum))
>> user  system elapsed
>> 0.811   0.047   0.866
>>
>>   Cheers,
>>   -- Thiago V. dos Santos
>>
>> PhD student
>> Land and Atmospheric Science
>> University of Minnesota
>>
>>
>>
>> On Friday, June 3, 2016 4:25 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl> wrote:
>> This can also be done with zApply:
>>
>> library(zoo)
>>
>> sYM <- zApply(s, by = as.yearmon, sum)
>> sM <- zApply(sYM, by = months, mean)
>>
>> Cheers,
>> Lo?c
>>
>> On 06/03/2016 02:02 AM, Vijay Lulla wrote:
>>> I think the following StackOverflow question has the answer:
>>> http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775
>>>
>>> Following the instructions listed on that page for your case might go
>>> something like below:
>>>
>>>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>>>> idxM <- unique(idxYM)%%100
>>>> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
>>>> meanYM
>>> class       : RasterBrick
>>> dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
>>> resolution  : 18, 9  (x, y)
>>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>> data source : in memory
>>> names       : X196101, X196102, X196103, X196104, X196105, X196106,
>>> X196107, X196108, X196109, X196110, X196111, X196112, X196201,
>>> X196202, X196203, ...
>>> min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
>>> 0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
>>>    0.3334, ...
>>> max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
>>> 0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
>>>    0.6255, ...
>>>
>>>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>>>> meanM
>>> class       : RasterBrick
>>> dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
>>> resolution  : 18, 9  (x, y)
>>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>> data source : in memory
>>> names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
>>>      X8,     X9,    X10,    X11,    X12
>>> min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
>>> 0.4724, 0.4629, 0.4774, 0.4736, 0.4708
>>> max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
>>> 0.5260, 0.5256, 0.5281, 0.5279, 0.5286
>>>
>>>>
>>>
>>> I'm not sure how [in]efficient this is for actual (i.e. not toy
>>> example) data.  Maybe others more experienced, and knowledgeable,
>>> members can provide better answers.
>>>
>>> HTH,
>>> Vijay.
>>>
>>> On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
>>> <r-sig-geo at r-project.org> wrote:
>>>> Dear all,
>>>>
>>>> I am working with daily time series of meteorological variables. This is an example of the dataset:
>>>>
>>>> library(raster)
>>>>
>>>> # Create date sequence
>>>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>>>
>>>> # Create raster stack and assign dates
>>>> r <- raster(ncol=20, nrow=20)
>>>> s <- stack(lapply(1:length(idx), function(x) setValues(r, runif(ncell(r)))))
>>>> s <- setZ(s, idx)
>>>>
>>>>
>>>> Now, let's assume those values represent daily precipitation. What I need to do is to integrate daily to monthly values,
>>>> and then take a monthly climatology. Climatology in this case means multi-year average of selected months, e.g., an average of the 30 Octobers from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>>>>
>>>> On the other hand, let's assume the raster values represent daily temperature. Integrating daily to monthly temperature doesn't make sense. Hence, instead of integrating daily values, I need to take monthly means (e.g. mean value of all days in every month), and then calculate the climatology.
>>>>
>>>> What would be the best approach to achieve that using the raster package?
>>>>
>>>>    Greetings,
>>>>    -- Thiago V. dos Santos
>>>>
>>>> PhD student
>>>> Land and Atmospheric Science
>>>> University of Minnesota
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>>>
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From jcarcamo03 at gmail.com  Sat Jun  4 16:58:24 2016
From: jcarcamo03 at gmail.com (=?UTF-8?Q?Jorge_C=C3=A1rcamo?=)
Date: Sat, 4 Jun 2016 16:58:24 +0200
Subject: [R-sig-Geo] Question impacts lagged independent variables lagsarlm
Message-ID: <CAAcGTKCer5FBq6VXJctwRu+-LNtSfAEF1OgGM6kiby3QWX2Ypw@mail.gmail.com>

Dear all,

I am working with a lagsarlm "mixed" model I executed:
library(spdep)
library(coda)
dsts15 <- nbdists(nbs15, data.xy)
idw15 <- lapply(dsts15, function(x) 1/(x))
sy15 <- nb2listw(nbs15, glist=idw15, style="W")
mod.sdm.15<-lagsarlm(AR32 ~ SD46 + Totaland + PC18 + PC22 + sra + sla + saa
+ owue + yearrain + yearfdi, data=data, listw=sy15, type="mixed",
tol.solve=1.0e-12)
summary(mod.sdm.15)

And got following results (abridged table):

               Estimate Std. Error z value Pr(>|z|)
(Intercept)   1.4255019  0.4824828  2.9545 0.003132
SD46          0.0149057  0.0116146  1.2834 0.199365
Totaland     -0.0217556  0.0072475 -3.0018 0.002684
...
yearrain      0.0557373  0.0485612  1.1478 0.251062
yearfdi      -0.0109979  0.0069536 -1.5816 0.113741
lag.SD46      0.0262273  0.0278045  0.9433 0.345539
lag.Totaland  0.0060992  0.0141515  0.4310 0.666474
...
lag.yearrain -0.2083118  0.0772751 -2.6957 0.007024
lag.yearfdi   0.0111460  0.0081562  1.3666 0.171761
Rho: -0.54702, LR test value: 10.052, p-value: 0.0015215

Immediatly, I exectued impacts(mod.sdm.15, R=1000), to get the impacts:

               Direct     Indirect         Total
SD46      0.013136761  0.013451739  0.0265884995
Totaland -0.023517332  0.013397014 -0.0101203177
...
yearrain  0.079110119 -0.177734631 -0.0986245121
yearfdi  -0.012677636  0.012773412  0.0000957761

Through simulation I manage to get credible intervals for direct, indirect
and total impacts (HPDinterval(impacts, choice="XXX")).

However, how can I get the impacts of the lagged variables? I have seen
some publications such as: L?pple, D., & Kelley, H. (2014). Spatial
dependence in the adoption of organic drystock farming in Ireland. That
report a posterior mean and credible intervals for the lagged variables.

All suggestions are welcome.

Best regards,

*Ing. Jorge Alfredo C?rcamo, M. Sc., Ph. D. (c)*
Agriculture economics
Georg-August-Universit?t G?ttingen

	[[alternative HTML version deleted]]


From vijaylulla at gmail.com  Sat Jun  4 17:30:15 2016
From: vijaylulla at gmail.com (Vijay Lulla)
Date: Sat, 4 Jun 2016 11:30:15 -0400
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <5752DC3D.1060904@wur.nl>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
	<57514C33.7010400@wur.nl>
	<2127614783.774242.1464967844732.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsx=Kt6dagz0fHCiGA7tHZzMkZyFFbr2MBEkqSmxjX35A@mail.gmail.com>
	<1875625356.902419.1464985541127.JavaMail.yahoo@mail.yahoo.com>
	<5752DC3D.1060904@wur.nl>
Message-ID: <CAKkiGbsYFLmxizSGmNUk-=NJR=133=-efRU7XSZOGGD6tx1Rrg@mail.gmail.com>

Yes, it is because of na.rm argument.  It's stackApply that defaults
na.rm=TRUE not zApply.  You can check that from the following
interaction:

> args(mean.default)
function (x, trim = 0, na.rm = FALSE, ...)
NULL
> args(sum)
function (..., na.rm = FALSE)
NULL
> args(stackApply)
function (x, indices, fun, filename = "", na.rm = TRUE, ...)
NULL
> args(zApply)
function (x, by, fun = mean, name = "", ...)
NULL
>

It is surprising that stackApply defaults differently than most of the
R functions but it kinda makes sense too.  I suspect the defaults are
what they are because in remote sensing related image procesing work
NAs get introduced, especially when doing projection transformation
and this can trip up many users and the authors might've decided to be
helpful to them.  I don't know...I'm just speculating.

HTH,
Vijay.

On Sat, Jun 4, 2016 at 9:48 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl> wrote:
> Hi Thiago,
>
> Yes, I can reproduce.
> It's probably because zApply() defaults to na.rm = TRUE (in fact the na.rm =
> argument is ignored if you pass it), and
>
> sum(c(NA, NA), na.rm = TRUE)
> [1] 0 (I was surprised by that).
>
> Perhaps the stackApply() call in the zApply() function definition is missing
> an ellipsis.
>
> Cheers,
> Lo?c
>
>
> On 06/03/2016 10:25 PM, Thiago V. dos Santos wrote:
>>
>> Dear Vijay and Lo?c,
>>
>>
>> I have identified some artifacts in the results when using zApply on my
>> data. To reproduce it, please download this shapefile
>> (https://www.dropbox.com/s/in2z10mlerr2sfu/southern.zip?dl=0) and run the
>> code below (see the comments after the plot commands):
>>
>> -----------------------
>> library(raster)
>> library(zoo)
>>
>> # Create the date sequence
>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>
>> # Create raster stack and assign dates
>> r <- raster(ncol=24, nrow=21, xmn=-58, xmx=-47.5, ymn=-34, ymx=-22,
>> resolution=0.5)
>> s <- stack(lapply(1:length(idx), function(x) setValues(r,
>> runif(ncell(r)))))
>> s <- setZ(s, idx)
>>
>> # Load shapefile and crop data
>> shp <- shapefile('~/Downloads/southern.shp', warnPRJ=F)
>> s.c <- crop(s, extent(shp))
>> s.c <- mask(s.c, shp)
>> plot(s.c,1)
>> plot(shp,add=T) # Looks good
>>
>> # Lo?c's approach using zApply
>> sYM <- zApply(s.c, by = as.yearmon, sum)
>> sM <- zApply(sYM, by = months, mean)
>> plot(sYM,1)  # Note that the "empty" space across the extent was filled
>> with 0's
>> plot(sM,1)  # The same "gray area" here
>>
>> # Vijay's approach using calc
>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>> idxM <- unique(idxYM)%%100
>> meanYM <- calc(s.c, fun=function(x) { by(x, idxYM, sum) })
>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>> plot(meanYM,1)  # Note that no 0 was added across the extent
>> plot(meanM,1)  # This is OK too, no grey area
>> -----------------------
>>
>> Basically, a portion of the raster extent was filled with 0's after using
>> zApply. It doesn't happen when using calc.
>>
>> Any ideas on what can be causing this?
>>
>>   Greetings,
>>   -- Thiago V. dos Santos
>>
>> PhD student
>> Land and Atmospheric Science
>> University of Minnesota
>>
>>
>>
>> On Friday, June 3, 2016 1:47 PM, Vijay Lulla <vijaylulla at gmail.com> wrote:
>> Looking at Lo?c's answer and from reading ?zApply I learned of
>> stackApply, which is used internally by zApply.  While zApply is for
>> time series of layers, stackApply is more general and can be used for
>> applying functions to groups of layers in a stack/brick.  It is very
>> similar to base R's `tapply`.  And it is also fast!
>>
>>> system.time(meanYM1 <- stackApply(s, idxYM, mean))
>>
>>     user  system elapsed
>>     0.45    0.03    0.48
>>>
>>> system.time(meanYM <- calc(s, fun=function(x) { by(x, idxYM, mean)}))
>>
>>     user  system elapsed
>>    17.50    0.01   17.61
>>>
>>> all(meanYM[] == meanYM1[])
>>
>> [1] TRUE
>>
>> Thanks Lo?c for pointing out zApply.
>>
>>
>> On Fri, Jun 3, 2016 at 11:30 AM, Thiago V. dos Santos via R-sig-Geo
>> <r-sig-geo at r-project.org> wrote:
>>>
>>> Cool Lo?c, thanks for showing one more option.
>>>
>>> By the way, in this case zApply is surprisingly faster than calc:
>>>
>>>> system.time(meanYM <- calc(s,fun=function(x) { by(x, idxYM, sum) }))
>>>
>>> user  system elapsed
>>> 21.700   0.248  22.095
>>>
>>>
>>>> system.time(sYM <- zApply(s, by = as.yearmon, sum))
>>>
>>> user  system elapsed
>>> 0.811   0.047   0.866
>>>
>>>   Cheers,
>>>   -- Thiago V. dos Santos
>>>
>>> PhD student
>>> Land and Atmospheric Science
>>> University of Minnesota
>>>
>>>
>>>
>>> On Friday, June 3, 2016 4:25 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl>
>>> wrote:
>>> This can also be done with zApply:
>>>
>>> library(zoo)
>>>
>>> sYM <- zApply(s, by = as.yearmon, sum)
>>> sM <- zApply(sYM, by = months, mean)
>>>
>>> Cheers,
>>> Lo?c
>>>
>>> On 06/03/2016 02:02 AM, Vijay Lulla wrote:
>>>>
>>>> I think the following StackOverflow question has the answer:
>>>>
>>>> http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775
>>>>
>>>> Following the instructions listed on that page for your case might go
>>>> something like below:
>>>>
>>>>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>>>>> idxM <- unique(idxYM)%%100
>>>>> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
>>>>> meanYM
>>>>
>>>> class       : RasterBrick
>>>> dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
>>>> resolution  : 18, 9  (x, y)
>>>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>> data source : in memory
>>>> names       : X196101, X196102, X196103, X196104, X196105, X196106,
>>>> X196107, X196108, X196109, X196110, X196111, X196112, X196201,
>>>> X196202, X196203, ...
>>>> min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
>>>> 0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
>>>>    0.3334, ...
>>>> max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
>>>> 0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
>>>>    0.6255, ...
>>>>
>>>>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>>>>> meanM
>>>>
>>>> class       : RasterBrick
>>>> dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
>>>> resolution  : 18, 9  (x, y)
>>>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>> data source : in memory
>>>> names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
>>>>      X8,     X9,    X10,    X11,    X12
>>>> min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
>>>> 0.4724, 0.4629, 0.4774, 0.4736, 0.4708
>>>> max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
>>>> 0.5260, 0.5256, 0.5281, 0.5279, 0.5286
>>>>
>>>>>
>>>>
>>>> I'm not sure how [in]efficient this is for actual (i.e. not toy
>>>> example) data.  Maybe others more experienced, and knowledgeable,
>>>> members can provide better answers.
>>>>
>>>> HTH,
>>>> Vijay.
>>>>
>>>> On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
>>>> <r-sig-geo at r-project.org> wrote:
>>>>>
>>>>> Dear all,
>>>>>
>>>>> I am working with daily time series of meteorological variables. This
>>>>> is an example of the dataset:
>>>>>
>>>>> library(raster)
>>>>>
>>>>> # Create date sequence
>>>>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>>>>
>>>>> # Create raster stack and assign dates
>>>>> r <- raster(ncol=20, nrow=20)
>>>>> s <- stack(lapply(1:length(idx), function(x) setValues(r,
>>>>> runif(ncell(r)))))
>>>>> s <- setZ(s, idx)
>>>>>
>>>>>
>>>>> Now, let's assume those values represent daily precipitation. What I
>>>>> need to do is to integrate daily to monthly values,
>>>>> and then take a monthly climatology. Climatology in this case means
>>>>> multi-year average of selected months, e.g., an average of the 30 Octobers
>>>>> from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>>>>>
>>>>> On the other hand, let's assume the raster values represent daily
>>>>> temperature. Integrating daily to monthly temperature doesn't make sense.
>>>>> Hence, instead of integrating daily values, I need to take monthly means
>>>>> (e.g. mean value of all days in every month), and then calculate the
>>>>> climatology.
>>>>>
>>>>> What would be the best approach to achieve that using the raster
>>>>> package?
>>>>>
>>>>>    Greetings,
>>>>>    -- Thiago V. dos Santos
>>>>>
>>>>> PhD student
>>>>> Land and Atmospheric Science
>>>>> University of Minnesota
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>
>>>>
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From thi_veloso at yahoo.com.br  Sat Jun  4 22:59:00 2016
From: thi_veloso at yahoo.com.br (Thiago V. dos Santos)
Date: Sat, 4 Jun 2016 20:59:00 +0000 (UTC)
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <CAKkiGbsYFLmxizSGmNUk-=NJR=133=-efRU7XSZOGGD6tx1Rrg@mail.gmail.com>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
	<57514C33.7010400@wur.nl>
	<2127614783.774242.1464967844732.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsx=Kt6dagz0fHCiGA7tHZzMkZyFFbr2MBEkqSmxjX35A@mail.gmail.com>
	<1875625356.902419.1464985541127.JavaMail.yahoo@mail.yahoo.com>
	<5752DC3D.1060904@wur.nl>
	<CAKkiGbsYFLmxizSGmNUk-=NJR=133=-efRU7XSZOGGD6tx1Rrg@mail.gmail.com>
Message-ID: <2018207050.1264934.1465073940728.JavaMail.yahoo@mail.yahoo.com>

Thanks again Lo?c and Vijay for helping to identify the issue.

Hopefully Robert or any of the contributors will see this question and kindly suggest a workaround.
 Cheers,
 -- Thiago V. dos Santos

PhD student
Land and Atmospheric Science
University of Minnesota



On Saturday, June 4, 2016 10:30 AM, Vijay Lulla <vijaylulla at gmail.com> wrote:
Yes, it is because of na.rm argument.  It's stackApply that defaults
na.rm=TRUE not zApply.  You can check that from the following
interaction:

> args(mean.default)
function (x, trim = 0, na.rm = FALSE, ...)
NULL
> args(sum)
function (..., na.rm = FALSE)
NULL
> args(stackApply)
function (x, indices, fun, filename = "", na.rm = TRUE, ...)
NULL
> args(zApply)
function (x, by, fun = mean, name = "", ...)
NULL
>

It is surprising that stackApply defaults differently than most of the
R functions but it kinda makes sense too.  I suspect the defaults are
what they are because in remote sensing related image procesing work
NAs get introduced, especially when doing projection transformation
and this can trip up many users and the authors might've decided to be
helpful to them.  I don't know...I'm just speculating.

HTH,
Vijay.


On Sat, Jun 4, 2016 at 9:48 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl> wrote:
> Hi Thiago,
>
> Yes, I can reproduce.
> It's probably because zApply() defaults to na.rm = TRUE (in fact the na.rm =
> argument is ignored if you pass it), and
>
> sum(c(NA, NA), na.rm = TRUE)
> [1] 0 (I was surprised by that).
>
> Perhaps the stackApply() call in the zApply() function definition is missing
> an ellipsis.
>
> Cheers,
> Lo?c
>
>
> On 06/03/2016 10:25 PM, Thiago V. dos Santos wrote:
>>
>> Dear Vijay and Lo?c,
>>
>>
>> I have identified some artifacts in the results when using zApply on my
>> data. To reproduce it, please download this shapefile
>> (https://www.dropbox.com/s/in2z10mlerr2sfu/southern.zip?dl=0) and run the
>> code below (see the comments after the plot commands):
>>
>> -----------------------
>> library(raster)
>> library(zoo)
>>
>> # Create the date sequence
>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>
>> # Create raster stack and assign dates
>> r <- raster(ncol=24, nrow=21, xmn=-58, xmx=-47.5, ymn=-34, ymx=-22,
>> resolution=0.5)
>> s <- stack(lapply(1:length(idx), function(x) setValues(r,
>> runif(ncell(r)))))
>> s <- setZ(s, idx)
>>
>> # Load shapefile and crop data
>> shp <- shapefile('~/Downloads/southern.shp', warnPRJ=F)
>> s.c <- crop(s, extent(shp))
>> s.c <- mask(s.c, shp)
>> plot(s.c,1)
>> plot(shp,add=T) # Looks good
>>
>> # Lo?c's approach using zApply
>> sYM <- zApply(s.c, by = as.yearmon, sum)
>> sM <- zApply(sYM, by = months, mean)
>> plot(sYM,1)  # Note that the "empty" space across the extent was filled
>> with 0's
>> plot(sM,1)  # The same "gray area" here
>>
>> # Vijay's approach using calc
>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>> idxM <- unique(idxYM)%%100
>> meanYM <- calc(s.c, fun=function(x) { by(x, idxYM, sum) })
>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>> plot(meanYM,1)  # Note that no 0 was added across the extent
>> plot(meanM,1)  # This is OK too, no grey area
>> -----------------------
>>
>> Basically, a portion of the raster extent was filled with 0's after using
>> zApply. It doesn't happen when using calc.
>>
>> Any ideas on what can be causing this?
>>
>>   Greetings,
>>   -- Thiago V. dos Santos
>>
>> PhD student
>> Land and Atmospheric Science
>> University of Minnesota
>>
>>
>>
>> On Friday, June 3, 2016 1:47 PM, Vijay Lulla <vijaylulla at gmail.com> wrote:
>> Looking at Lo?c's answer and from reading ?zApply I learned of
>> stackApply, which is used internally by zApply.  While zApply is for
>> time series of layers, stackApply is more general and can be used for
>> applying functions to groups of layers in a stack/brick.  It is very
>> similar to base R's `tapply`.  And it is also fast!
>>
>>> system.time(meanYM1 <- stackApply(s, idxYM, mean))
>>
>>     user  system elapsed
>>     0.45    0.03    0.48
>>>
>>> system.time(meanYM <- calc(s, fun=function(x) { by(x, idxYM, mean)}))
>>
>>     user  system elapsed
>>    17.50    0.01   17.61
>>>
>>> all(meanYM[] == meanYM1[])
>>
>> [1] TRUE
>>
>> Thanks Lo?c for pointing out zApply.
>>
>>
>> On Fri, Jun 3, 2016 at 11:30 AM, Thiago V. dos Santos via R-sig-Geo
>> <r-sig-geo at r-project.org> wrote:
>>>
>>> Cool Lo?c, thanks for showing one more option.
>>>
>>> By the way, in this case zApply is surprisingly faster than calc:
>>>
>>>> system.time(meanYM <- calc(s,fun=function(x) { by(x, idxYM, sum) }))
>>>
>>> user  system elapsed
>>> 21.700   0.248  22.095
>>>
>>>
>>>> system.time(sYM <- zApply(s, by = as.yearmon, sum))
>>>
>>> user  system elapsed
>>> 0.811   0.047   0.866
>>>
>>>   Cheers,
>>>   -- Thiago V. dos Santos
>>>
>>> PhD student
>>> Land and Atmospheric Science
>>> University of Minnesota
>>>
>>>
>>>
>>> On Friday, June 3, 2016 4:25 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl>
>>> wrote:
>>> This can also be done with zApply:
>>>
>>> library(zoo)
>>>
>>> sYM <- zApply(s, by = as.yearmon, sum)
>>> sM <- zApply(sYM, by = months, mean)
>>>
>>> Cheers,
>>> Lo?c
>>>
>>> On 06/03/2016 02:02 AM, Vijay Lulla wrote:
>>>>
>>>> I think the following StackOverflow question has the answer:
>>>>
>>>> http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775
>>>>
>>>> Following the instructions listed on that page for your case might go
>>>> something like below:
>>>>
>>>>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>>>>> idxM <- unique(idxYM)%%100
>>>>> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
>>>>> meanYM
>>>>
>>>> class       : RasterBrick
>>>> dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
>>>> resolution  : 18, 9  (x, y)
>>>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>> data source : in memory
>>>> names       : X196101, X196102, X196103, X196104, X196105, X196106,
>>>> X196107, X196108, X196109, X196110, X196111, X196112, X196201,
>>>> X196202, X196203, ...
>>>> min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
>>>> 0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
>>>>    0.3334, ...
>>>> max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
>>>> 0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
>>>>    0.6255, ...
>>>>
>>>>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>>>>> meanM
>>>>
>>>> class       : RasterBrick
>>>> dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
>>>> resolution  : 18, 9  (x, y)
>>>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>> data source : in memory
>>>> names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
>>>>      X8,     X9,    X10,    X11,    X12
>>>> min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
>>>> 0.4724, 0.4629, 0.4774, 0.4736, 0.4708
>>>> max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
>>>> 0.5260, 0.5256, 0.5281, 0.5279, 0.5286
>>>>
>>>>>
>>>>
>>>> I'm not sure how [in]efficient this is for actual (i.e. not toy
>>>> example) data.  Maybe others more experienced, and knowledgeable,
>>>> members can provide better answers.
>>>>
>>>> HTH,
>>>> Vijay.
>>>>
>>>> On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
>>>> <r-sig-geo at r-project.org> wrote:
>>>>>
>>>>> Dear all,
>>>>>
>>>>> I am working with daily time series of meteorological variables. This
>>>>> is an example of the dataset:
>>>>>
>>>>> library(raster)
>>>>>
>>>>> # Create date sequence
>>>>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>>>>
>>>>> # Create raster stack and assign dates
>>>>> r <- raster(ncol=20, nrow=20)
>>>>> s <- stack(lapply(1:length(idx), function(x) setValues(r,
>>>>> runif(ncell(r)))))
>>>>> s <- setZ(s, idx)
>>>>>
>>>>>
>>>>> Now, let's assume those values represent daily precipitation. What I
>>>>> need to do is to integrate daily to monthly values,
>>>>> and then take a monthly climatology. Climatology in this case means
>>>>> multi-year average of selected months, e.g., an average of the 30 Octobers
>>>>> from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>>>>>
>>>>> On the other hand, let's assume the raster values represent daily
>>>>> temperature. Integrating daily to monthly temperature doesn't make sense.
>>>>> Hence, instead of integrating daily values, I need to take monthly means
>>>>> (e.g. mean value of all days in every month), and then calculate the
>>>>> climatology.
>>>>>
>>>>> What would be the best approach to achieve that using the raster
>>>>> package?
>>>>>
>>>>>    Greetings,
>>>>>    -- Thiago V. dos Santos
>>>>>
>>>>> PhD student
>>>>> Land and Atmospheric Science
>>>>> University of Minnesota
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>>
>>>>
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From Roger.Bivand at nhh.no  Sun Jun  5 14:17:34 2016
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 5 Jun 2016 14:17:34 +0200
Subject: [R-sig-Geo] Question impacts lagged independent variables
 lagsarlm
In-Reply-To: <CAAcGTKCer5FBq6VXJctwRu+-LNtSfAEF1OgGM6kiby3QWX2Ypw@mail.gmail.com>
References: <CAAcGTKCer5FBq6VXJctwRu+-LNtSfAEF1OgGM6kiby3QWX2Ypw@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1606051408140.3301@reclus.nhh.no>

On Sat, 4 Jun 2016, Jorge C?rcamo wrote:

> Dear all,
>
> I am working with a lagsarlm "mixed" model I executed:
> library(spdep)
> library(coda)
> dsts15 <- nbdists(nbs15, data.xy)
> idw15 <- lapply(dsts15, function(x) 1/(x))
> sy15 <- nb2listw(nbs15, glist=idw15, style="W")
> mod.sdm.15<-lagsarlm(AR32 ~ SD46 + Totaland + PC18 + PC22 + sra + sla + saa
> + owue + yearrain + yearfdi, data=data, listw=sy15, type="mixed",
> tol.solve=1.0e-12)
> summary(mod.sdm.15)
>
> And got following results (abridged table):
>
>               Estimate Std. Error z value Pr(>|z|)
> (Intercept)   1.4255019  0.4824828  2.9545 0.003132
> SD46          0.0149057  0.0116146  1.2834 0.199365
> Totaland     -0.0217556  0.0072475 -3.0018 0.002684
> ...
> yearrain      0.0557373  0.0485612  1.1478 0.251062
> yearfdi      -0.0109979  0.0069536 -1.5816 0.113741
> lag.SD46      0.0262273  0.0278045  0.9433 0.345539
> lag.Totaland  0.0060992  0.0141515  0.4310 0.666474
> ...
> lag.yearrain -0.2083118  0.0772751 -2.6957 0.007024
> lag.yearfdi   0.0111460  0.0081562  1.3666 0.171761
> Rho: -0.54702, LR test value: 10.052, p-value: 0.0015215
>
> Immediatly, I exectued impacts(mod.sdm.15, R=1000), to get the impacts:
>
>               Direct     Indirect         Total
> SD46      0.013136761  0.013451739  0.0265884995
> Totaland -0.023517332  0.013397014 -0.0101203177
> ...
> yearrain  0.079110119 -0.177734631 -0.0986245121
> yearfdi  -0.012677636  0.012773412  0.0000957761
>
> Through simulation I manage to get credible intervals for direct, indirect
> and total impacts (HPDinterval(impacts, choice="XXX")).

As you must know from the references on the help page for impacts methods, 
these are the combined impacts of the variables:

S_r(W) = (I - \rho W)^{-1} (\beta_r I - \gamma_r W)

where the direct impacts are sum(S_r(W))/n, etc. The \gamma_r are the 
coefficients on W x_r.

>
> However, how can I get the impacts of the lagged variables? I have seen
> some publications such as: L?pple, D., & Kelley, H. (2014). Spatial
> dependence in the adoption of organic drystock farming in Ireland. That
> report a posterior mean and credible intervals for the lagged variables.

Given the above, either you are misreading L?pple & Kelley (I do not have 
access), or both you and they are wrong. There are by definition on 
separable impacts for the lagged X variables.

Hope this clarifies,

Roger

>
> All suggestions are welcome.
>
> Best regards,
>
> *Ing. Jorge Alfredo C?rcamo, M. Sc., Ph. D. (c)*
> Agriculture economics
> Georg-August-Universit?t G?ttingen
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
http://depsy.org/person/434412

From jcarcamo03 at gmail.com  Sun Jun  5 16:18:20 2016
From: jcarcamo03 at gmail.com (=?UTF-8?Q?Jorge_C=C3=A1rcamo?=)
Date: Sun, 5 Jun 2016 16:18:20 +0200
Subject: [R-sig-Geo] Question impacts lagged independent variables
	lagsarlm
In-Reply-To: <alpine.LFD.2.20.1606051408140.3301@reclus.nhh.no>
References: <CAAcGTKCer5FBq6VXJctwRu+-LNtSfAEF1OgGM6kiby3QWX2Ypw@mail.gmail.com>
	<alpine.LFD.2.20.1606051408140.3301@reclus.nhh.no>
Message-ID: <CAAcGTKCuU=KukWTZsNOeEhZPJWCqaZn4++5GqSifURfqqx7mmQ@mail.gmail.com>

Prof. Bivand,

Many thanks for your clarification. I think that I should read L?pple &
Kelley again.

One more question, I assume that if I create independent lag variables
with: data$w_xxx <- lag.listw(sy15,data$xxx) and add them into the model I
can get the impacts of these lag variables; however, if I add them into the
model, do I need to change the type="lag" instead of "mixed" when running
lagsarlm command?

Many thanks for your attention.

Jorge

*Ing. Jorge Alfredo C?rcamo, M. Sc., Ph. D. (c)*
Agriculture economics
Georg-August-Universit?t G?ttingen

On Sun, Jun 5, 2016 at 2:17 PM, Roger Bivand <Roger.Bivand at nhh.no> wrote:

> On Sat, 4 Jun 2016, Jorge C?rcamo wrote:
>
> Dear all,
>>
>> I am working with a lagsarlm "mixed" model I executed:
>> library(spdep)
>> library(coda)
>> dsts15 <- nbdists(nbs15, data.xy)
>> idw15 <- lapply(dsts15, function(x) 1/(x))
>> sy15 <- nb2listw(nbs15, glist=idw15, style="W")
>> mod.sdm.15<-lagsarlm(AR32 ~ SD46 + Totaland + PC18 + PC22 + sra + sla +
>> saa
>> + owue + yearrain + yearfdi, data=data, listw=sy15, type="mixed",
>> tol.solve=1.0e-12)
>> summary(mod.sdm.15)
>>
>> And got following results (abridged table):
>>
>>               Estimate Std. Error z value Pr(>|z|)
>> (Intercept)   1.4255019  0.4824828  2.9545 0.003132
>> SD46          0.0149057  0.0116146  1.2834 0.199365
>> Totaland     -0.0217556  0.0072475 -3.0018 0.002684
>> ...
>> yearrain      0.0557373  0.0485612  1.1478 0.251062
>> yearfdi      -0.0109979  0.0069536 -1.5816 0.113741
>> lag.SD46      0.0262273  0.0278045  0.9433 0.345539
>> lag.Totaland  0.0060992  0.0141515  0.4310 0.666474
>> ...
>> lag.yearrain -0.2083118  0.0772751 -2.6957 0.007024
>> lag.yearfdi   0.0111460  0.0081562  1.3666 0.171761
>> Rho: -0.54702, LR test value: 10.052, p-value: 0.0015215
>>
>> Immediatly, I exectued impacts(mod.sdm.15, R=1000), to get the impacts:
>>
>>               Direct     Indirect         Total
>> SD46      0.013136761  0.013451739  0.0265884995
>> Totaland -0.023517332  0.013397014 -0.0101203177
>> ...
>> yearrain  0.079110119 -0.177734631 -0.0986245121
>> yearfdi  -0.012677636  0.012773412  0.0000957761
>>
>> Through simulation I manage to get credible intervals for direct, indirect
>> and total impacts (HPDinterval(impacts, choice="XXX")).
>>
>
> As you must know from the references on the help page for impacts methods,
> these are the combined impacts of the variables:
>
> S_r(W) = (I - \rho W)^{-1} (\beta_r I - \gamma_r W)
>
> where the direct impacts are sum(S_r(W))/n, etc. The \gamma_r are the
> coefficients on W x_r.
>
>
>> However, how can I get the impacts of the lagged variables? I have seen
>> some publications such as: L?pple, D., & Kelley, H. (2014). Spatial
>> dependence in the adoption of organic drystock farming in Ireland. That
>> report a posterior mean and credible intervals for the lagged variables.
>>
>
> Given the above, either you are misreading L?pple & Kelley (I do not have
> access), or both you and they are wrong. There are by definition on
> separable impacts for the lagged X variables.
>
> Hope this clarifies,
>
> Roger
>
>
>> All suggestions are welcome.
>>
>> Best regards,
>>
>> *Ing. Jorge Alfredo C?rcamo, M. Sc., Ph. D. (c)*
>> Agriculture economics
>> Georg-August-Universit?t G?ttingen
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; fax +47 55 95 91 00
> e-mail: Roger.Bivand at nhh.no
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> http://depsy.org/person/434412

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Sun Jun  5 17:20:58 2016
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 5 Jun 2016 17:20:58 +0200
Subject: [R-sig-Geo] Question impacts lagged independent variables
 lagsarlm
In-Reply-To: <CAAcGTKCuU=KukWTZsNOeEhZPJWCqaZn4++5GqSifURfqqx7mmQ@mail.gmail.com>
References: <CAAcGTKCer5FBq6VXJctwRu+-LNtSfAEF1OgGM6kiby3QWX2Ypw@mail.gmail.com>
	<alpine.LFD.2.20.1606051408140.3301@reclus.nhh.no>
	<CAAcGTKCuU=KukWTZsNOeEhZPJWCqaZn4++5GqSifURfqqx7mmQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1606051715330.15629@reclus.nhh.no>

On Sun, 5 Jun 2016, Jorge C?rcamo wrote:

> Prof. Bivand,
>
> Many thanks for your clarification. I think that I should read L?pple &
> Kelley again.
>
> One more question, I assume that if I create independent lag variables
> with: data$w_xxx <- lag.listw(sy15,data$xxx) and add them into the model I
> can get the impacts of these lag variables; however, if I add them into the
> model, do I need to change the type="lag" instead of "mixed" when running
> lagsarlm command?

Don't even think about this, it is senseless. The impact of x_r is as 
given in the formula, the only contrast is between the impacts in the 
model="lag" case and the model="mixed" case. Because of the interaction 
between \rho and the \betas (and \gammas) in the y = \rho W y + ... 
models, you only get one set of impacts per x_r per model. You can 
interprete the differences between the three impact components between 
models, but avoid playing with stuff without doing all of the maths first.

Roger

>
> Many thanks for your attention.
>
> Jorge
>
> *Ing. Jorge Alfredo C?rcamo, M. Sc., Ph. D. (c)*
> Agriculture economics
> Georg-August-Universit?t G?ttingen
>
> On Sun, Jun 5, 2016 at 2:17 PM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Sat, 4 Jun 2016, Jorge C?rcamo wrote:
>>
>> Dear all,
>>>
>>> I am working with a lagsarlm "mixed" model I executed:
>>> library(spdep)
>>> library(coda)
>>> dsts15 <- nbdists(nbs15, data.xy)
>>> idw15 <- lapply(dsts15, function(x) 1/(x))
>>> sy15 <- nb2listw(nbs15, glist=idw15, style="W")
>>> mod.sdm.15<-lagsarlm(AR32 ~ SD46 + Totaland + PC18 + PC22 + sra + sla +
>>> saa
>>> + owue + yearrain + yearfdi, data=data, listw=sy15, type="mixed",
>>> tol.solve=1.0e-12)
>>> summary(mod.sdm.15)
>>>
>>> And got following results (abridged table):
>>>
>>>               Estimate Std. Error z value Pr(>|z|)
>>> (Intercept)   1.4255019  0.4824828  2.9545 0.003132
>>> SD46          0.0149057  0.0116146  1.2834 0.199365
>>> Totaland     -0.0217556  0.0072475 -3.0018 0.002684
>>> ...
>>> yearrain      0.0557373  0.0485612  1.1478 0.251062
>>> yearfdi      -0.0109979  0.0069536 -1.5816 0.113741
>>> lag.SD46      0.0262273  0.0278045  0.9433 0.345539
>>> lag.Totaland  0.0060992  0.0141515  0.4310 0.666474
>>> ...
>>> lag.yearrain -0.2083118  0.0772751 -2.6957 0.007024
>>> lag.yearfdi   0.0111460  0.0081562  1.3666 0.171761
>>> Rho: -0.54702, LR test value: 10.052, p-value: 0.0015215
>>>
>>> Immediatly, I exectued impacts(mod.sdm.15, R=1000), to get the impacts:
>>>
>>>               Direct     Indirect         Total
>>> SD46      0.013136761  0.013451739  0.0265884995
>>> Totaland -0.023517332  0.013397014 -0.0101203177
>>> ...
>>> yearrain  0.079110119 -0.177734631 -0.0986245121
>>> yearfdi  -0.012677636  0.012773412  0.0000957761
>>>
>>> Through simulation I manage to get credible intervals for direct, indirect
>>> and total impacts (HPDinterval(impacts, choice="XXX")).
>>>
>>
>> As you must know from the references on the help page for impacts methods,
>> these are the combined impacts of the variables:
>>
>> S_r(W) = (I - \rho W)^{-1} (\beta_r I - \gamma_r W)
>>
>> where the direct impacts are sum(S_r(W))/n, etc. The \gamma_r are the
>> coefficients on W x_r.
>>
>>
>>> However, how can I get the impacts of the lagged variables? I have seen
>>> some publications such as: L?pple, D., & Kelley, H. (2014). Spatial
>>> dependence in the adoption of organic drystock farming in Ireland. That
>>> report a posterior mean and credible intervals for the lagged variables.
>>>
>>
>> Given the above, either you are misreading L?pple & Kelley (I do not have
>> access), or both you and they are wrong. There are by definition on
>> separable impacts for the lagged X variables.
>>
>> Hope this clarifies,
>>
>> Roger
>>
>>
>>> All suggestions are welcome.
>>>
>>> Best regards,
>>>
>>> *Ing. Jorge Alfredo C?rcamo, M. Sc., Ph. D. (c)*
>>> Agriculture economics
>>> Georg-August-Universit?t G?ttingen
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; fax +47 55 95 91 00
>> e-mail: Roger.Bivand at nhh.no
>> http://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>> http://depsy.org/person/434412
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
http://depsy.org/person/434412

From jcarcamo03 at gmail.com  Sun Jun  5 17:33:13 2016
From: jcarcamo03 at gmail.com (=?UTF-8?Q?Jorge_C=C3=A1rcamo?=)
Date: Sun, 5 Jun 2016 17:33:13 +0200
Subject: [R-sig-Geo] Question impacts lagged independent variables
	lagsarlm
In-Reply-To: <alpine.LFD.2.20.1606051715330.15629@reclus.nhh.no>
References: <CAAcGTKCer5FBq6VXJctwRu+-LNtSfAEF1OgGM6kiby3QWX2Ypw@mail.gmail.com>
	<alpine.LFD.2.20.1606051408140.3301@reclus.nhh.no>
	<CAAcGTKCuU=KukWTZsNOeEhZPJWCqaZn4++5GqSifURfqqx7mmQ@mail.gmail.com>
	<alpine.LFD.2.20.1606051715330.15629@reclus.nhh.no>
Message-ID: <CAAcGTKAgXdEcg6GHr3hUzDTR6sinoaGqcvR-kxN7xeT3E+Ra1Q@mail.gmail.com>

Dear Prof. Bivand,

Many thanks for your time and attention. I will do as you suggest.

Best regards,

Jorge
On Jun 5, 2016 17:22, "Roger Bivand" <Roger.Bivand at nhh.no> wrote:

> On Sun, 5 Jun 2016, Jorge C?rcamo wrote:
>
> Prof. Bivand,
>>
>> Many thanks for your clarification. I think that I should read L?pple &
>> Kelley again.
>>
>> One more question, I assume that if I create independent lag variables
>> with: data$w_xxx <- lag.listw(sy15,data$xxx) and add them into the model I
>> can get the impacts of these lag variables; however, if I add them into
>> the
>> model, do I need to change the type="lag" instead of "mixed" when running
>> lagsarlm command?
>>
>
> Don't even think about this, it is senseless. The impact of x_r is as
> given in the formula, the only contrast is between the impacts in the
> model="lag" case and the model="mixed" case. Because of the interaction
> between \rho and the \betas (and \gammas) in the y = \rho W y + ... models,
> you only get one set of impacts per x_r per model. You can interprete the
> differences between the three impact components between models, but avoid
> playing with stuff without doing all of the maths first.
>
> Roger
>
>
>> Many thanks for your attention.
>>
>> Jorge
>>
>> *Ing. Jorge Alfredo C?rcamo, M. Sc., Ph. D. (c)*
>> Agriculture economics
>> Georg-August-Universit?t G?ttingen
>>
>> On Sun, Jun 5, 2016 at 2:17 PM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>>
>> On Sat, 4 Jun 2016, Jorge C?rcamo wrote:
>>>
>>> Dear all,
>>>
>>>>
>>>> I am working with a lagsarlm "mixed" model I executed:
>>>> library(spdep)
>>>> library(coda)
>>>> dsts15 <- nbdists(nbs15, data.xy)
>>>> idw15 <- lapply(dsts15, function(x) 1/(x))
>>>> sy15 <- nb2listw(nbs15, glist=idw15, style="W")
>>>> mod.sdm.15<-lagsarlm(AR32 ~ SD46 + Totaland + PC18 + PC22 + sra + sla +
>>>> saa
>>>> + owue + yearrain + yearfdi, data=data, listw=sy15, type="mixed",
>>>> tol.solve=1.0e-12)
>>>> summary(mod.sdm.15)
>>>>
>>>> And got following results (abridged table):
>>>>
>>>>               Estimate Std. Error z value Pr(>|z|)
>>>> (Intercept)   1.4255019  0.4824828  2.9545 0.003132
>>>> SD46          0.0149057  0.0116146  1.2834 0.199365
>>>> Totaland     -0.0217556  0.0072475 -3.0018 0.002684
>>>> ...
>>>> yearrain      0.0557373  0.0485612  1.1478 0.251062
>>>> yearfdi      -0.0109979  0.0069536 -1.5816 0.113741
>>>> lag.SD46      0.0262273  0.0278045  0.9433 0.345539
>>>> lag.Totaland  0.0060992  0.0141515  0.4310 0.666474
>>>> ...
>>>> lag.yearrain -0.2083118  0.0772751 -2.6957 0.007024
>>>> lag.yearfdi   0.0111460  0.0081562  1.3666 0.171761
>>>> Rho: -0.54702, LR test value: 10.052, p-value: 0.0015215
>>>>
>>>> Immediatly, I exectued impacts(mod.sdm.15, R=1000), to get the impacts:
>>>>
>>>>               Direct     Indirect         Total
>>>> SD46      0.013136761  0.013451739  0.0265884995
>>>> Totaland -0.023517332  0.013397014 -0.0101203177
>>>> ...
>>>> yearrain  0.079110119 -0.177734631 -0.0986245121
>>>> yearfdi  -0.012677636  0.012773412  0.0000957761
>>>>
>>>> Through simulation I manage to get credible intervals for direct,
>>>> indirect
>>>> and total impacts (HPDinterval(impacts, choice="XXX")).
>>>>
>>>>
>>> As you must know from the references on the help page for impacts
>>> methods,
>>> these are the combined impacts of the variables:
>>>
>>> S_r(W) = (I - \rho W)^{-1} (\beta_r I - \gamma_r W)
>>>
>>> where the direct impacts are sum(S_r(W))/n, etc. The \gamma_r are the
>>> coefficients on W x_r.
>>>
>>>
>>> However, how can I get the impacts of the lagged variables? I have seen
>>>> some publications such as: L?pple, D., & Kelley, H. (2014). Spatial
>>>> dependence in the adoption of organic drystock farming in Ireland. That
>>>> report a posterior mean and credible intervals for the lagged variables.
>>>>
>>>>
>>> Given the above, either you are misreading L?pple & Kelley (I do not have
>>> access), or both you and they are wrong. There are by definition on
>>> separable impacts for the lagged X variables.
>>>
>>> Hope this clarifies,
>>>
>>> Roger
>>>
>>>
>>> All suggestions are welcome.
>>>>
>>>> Best regards,
>>>>
>>>> *Ing. Jorge Alfredo C?rcamo, M. Sc., Ph. D. (c)*
>>>> Agriculture economics
>>>> Georg-August-Universit?t G?ttingen
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>
>>> --
>>> Roger Bivand
>>> Department of Economics, Norwegian School of Economics,
>>> Helleveien 30, N-5045 Bergen, Norway.
>>> voice: +47 55 95 93 55; fax +47 55 95 91 00
>>> e-mail: Roger.Bivand at nhh.no
>>> http://orcid.org/0000-0003-2392-6140
>>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>>> http://depsy.org/person/434412
>>>
>>
>>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; fax +47 55 95 91 00
> e-mail: Roger.Bivand at nhh.no
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> http://depsy.org/person/434412

	[[alternative HTML version deleted]]


From pascaltitle at gmail.com  Sun Jun  5 23:58:45 2016
From: pascaltitle at gmail.com (Pascal Title)
Date: Sun, 5 Jun 2016 17:58:45 -0400
Subject: [R-sig-Geo] warning in errorsarlm
Message-ID: <CAPSA=ycvNjqbYzgyXmKAPkTKtys1uKUsfnYixQL+SP5GBkRi=w@mail.gmail.com>

Hi,

I am trying to fit a simultaneous autoregressive model to some data using
the spdep package. In order to identify the appropriate spatial distance to
use, I use the correlog function to find the distance at which spatial
autocorrelation of the residuals from a linear model drops to 0.

When I run this code, I get a warning that states:
In errorsarlm(dat$rangeSize ~ dat$clim, listw = nlw, na.action = na.omit,  :
  inversion of asymptotic covariance matrix failed for tol.solve = 1e-10
  reciprocal condition number = 1.64249e-12 - using numerical Hessian.

I don't know enough about these models or the model fitting procedure to
understand whether or not this is a warning that can safely be ignored, or
if this is an indication that the results of the regression cannot be
trusted. Is there something I can/should do to improve the model fitting
procedure?

Thanks!

-Pascal

the data can be downloaded from here:
https://dl.dropboxusercontent.com/u/34644229/sampleDat.rds

code:

library(ncf)
library(spdep)

dat <- readRDS('sampleDat.rds')

fit1 <- lm(dat$rangeSize ~ dat$clim)
x <- dat$long
y <- dat$lat
z <- fit1$residuals
co <- correlog(x, y, z, increment = 500, resamp = 0, latlon = FALSE, na.rm
= TRUE)
nei <- dnearneigh(cbind(x, y), d1 = 0, d2 = co$x.intercept, longlat = FALSE)
nlw <- nb2listw(nei, style = "W", zero.policy = TRUE)

m1 <- errorsarlm(dat$rangeSize ~ dat$clim, listw = nlw, na.action =
na.omit, zero.policy = TRUE)
summary(m1, Nagelkerke = TRUE)

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Mon Jun  6 11:45:58 2016
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 6 Jun 2016 11:45:58 +0200
Subject: [R-sig-Geo] warning in errorsarlm
In-Reply-To: <CAPSA=ycvNjqbYzgyXmKAPkTKtys1uKUsfnYixQL+SP5GBkRi=w@mail.gmail.com>
References: <CAPSA=ycvNjqbYzgyXmKAPkTKtys1uKUsfnYixQL+SP5GBkRi=w@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1606061137110.17097@reclus.nhh.no>

On Sun, 5 Jun 2016, Pascal Title wrote:

> Hi,
>
> I am trying to fit a simultaneous autoregressive model to some data using
> the spdep package. In order to identify the appropriate spatial distance to
> use, I use the correlog function to find the distance at which spatial
> autocorrelation of the residuals from a linear model drops to 0.
>
> When I run this code, I get a warning that states:
> In errorsarlm(dat$rangeSize ~ dat$clim, listw = nlw, na.action = na.omit,  :
>  inversion of asymptotic covariance matrix failed for tol.solve = 1e-10
>  reciprocal condition number = 1.64249e-12 - using numerical Hessian.
>
> I don't know enough about these models or the model fitting procedure to
> understand whether or not this is a warning that can safely be ignored, or
> if this is an indication that the results of the regression cannot be
> trusted. Is there something I can/should do to improve the model fitting
> procedure?

This is like: "I don't know enough about poisonous mushrooms, so I'll just 
eat them raw to check the effect"?

Computers do not do conceptual linear alegebra, so that in doing numerical 
linear algebra, you have to think of the scaling of matrices, here in 
calculating the variance-covariance matrix of the model coefficients. If 
the scale is too large at the squared level, 1e-3 and 1e+3 say before 
squaring, you run out of road, and may choose to set a smaller tolerance 
check (walking closer to the edge of a cliff). See the tol.solve= argument 
where all of this is documented.

But really - never drive a car you cannot re-assemble yourself if your 
life, other people's lives or your career depends on it. All of this 
really is well documented in ?solve and ?errorsarlm.

I won't say RTFM, but in this case it would be justified.

Roger


>
> Thanks!
>
> -Pascal
>
> the data can be downloaded from here:
> https://dl.dropboxusercontent.com/u/34644229/sampleDat.rds
>
> code:
>
> library(ncf)
> library(spdep)
>
> dat <- readRDS('sampleDat.rds')
>
> fit1 <- lm(dat$rangeSize ~ dat$clim)
> x <- dat$long
> y <- dat$lat
> z <- fit1$residuals
> co <- correlog(x, y, z, increment = 500, resamp = 0, latlon = FALSE, na.rm
> = TRUE)
> nei <- dnearneigh(cbind(x, y), d1 = 0, d2 = co$x.intercept, longlat = FALSE)
> nlw <- nb2listw(nei, style = "W", zero.policy = TRUE)
>
> m1 <- errorsarlm(dat$rangeSize ~ dat$clim, listw = nlw, na.action =
> na.omit, zero.policy = TRUE)
> summary(m1, Nagelkerke = TRUE)
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
http://depsy.org/person/434412


From Roger.Bivand at nhh.no  Mon Jun  6 15:24:01 2016
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 6 Jun 2016 15:24:01 +0200
Subject: [R-sig-Geo] dissolve tiny polygons to others with
 unionSpatialPolygons{maptools}
In-Reply-To: <CABK368iX8Qa7ferBcpsdcvQu4cWxWHGuNTPYHD=SKBTgpjJ0XA@mail.gmail.com>
References: <CABK368iX8Qa7ferBcpsdcvQu4cWxWHGuNTPYHD=SKBTgpjJ0XA@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1606061516010.10035@reclus.nhh.no>

On Mon, 6 Jun 2016, Kumar Mainali wrote:

> I am trying to use unionSpatialPolygons() of maptools to eliminate sliver
> in species range. I want to dissolve tiny sliver polygons in a shapefile to
> bigger polygons as "Eliminate (Data Management)" of ArcMap does. Whereas I
> can dissolve polygons that have identical features in the argument "IDs", I
> cannot dissolve tiny polygons based on some threshold in area. In fact, the
> argument "threshold" has no effect in the output.

Indeed, threshold is not passed through, it was used when the function 
used gpclib rather than rgeos; I'm minded to deprecate 
maptools::unionSpatialPolygons anyway. Note that the data are in 
geographical coordinates, which may very well not be appropriate for the 
topological operations you are trying to do. Use rgeos::gUnaryUnion 
instead, and refer to this thread:

https://stat.ethz.ch/pipermail/r-sig-geo/2015-November/023667.html

for using rgeos::set_RGEOS_polyThreshold() and friends. They do not, 
however, try to guess which of the polygons neighbouring the sliver should 
get the extra area, so you'll have to think that through yourself.

Googling for lists:R-sig-geo dissolve slivers gets a fair number of hits.

There is always also the upstream question of where the slivers came from, 
and whether the resolution is not in the earlier process - generate a map 
without slivers that says exactly what you mean, rather than fudging it 
afterwards.

Roger

>
> ?Input data is available here: ?
> https://www.dropbox.com/sh/a0x5bbo9u60y7is/AAB6RjXHFQKZv-i-t4JclF3ba?dl=0
>
> p.ranges <- shapefile{raster}
> (IDs <- p.ranges$style_id)
> library(maptools)
> unionSpatialPolygons(p.ranges, IDs = IDs, threshold = 1.5)
>
> ?-- Kumar Mainali
> Postdoctoral Associate
> Department of Biology
> University of Maryland, College Park
>
>
> ?
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
http://depsy.org/person/434412

From wsavran at gmail.com  Mon Jun  6 18:56:55 2016
From: wsavran at gmail.com (William Savran)
Date: Mon, 6 Jun 2016 09:56:55 -0700
Subject: [R-sig-Geo] multivariate sequential gaussian cosimulation using
	gstat in R
Message-ID: <D7A358FD-2080-4C0E-8D32-E07A1A3F33B1@gmail.com>

Dear All,

I am new to gstat and fairly new to applying geostatistical methods, so I have a few questions.  I am looking to simulate a set of 3 (out of 4) correlated stochastic fields, which are all related through a linear model of coregionalization. One of the fields will be used as conditional data for the simulation.  With that being said, if anything seems alarming to you please let me know. 

Right now, my algorithm is working as follows (almost exactly following the demo):

# define gstat object
sim.g <- gstat(id='slip', formula=slip.sc~1, data=sim, nmax = 30, maxdist = 200, beta=0, set = list(nocheck = 1))
sim.g <- gstat(sim.g, 'psv', psv.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
sim.g <- gstat(sim.g, 'vrup', vrup.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
sim.g <- gstat(sim.g, 'mu0', mu0.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
sim.g <- gstat(sim.g, model=vgm(1,"Exp",1000,1,anis=c(90,0.5)), fill.all=T)

# fit lmc
sim.fit = fit.lmc(var, sim.g, correct.diagonal = 1.01)

# perform SGSim
z <- predict(sim.fit, newdata=xy, nsim=1, debug.level = -1)  If I specify fit.ranges = TRUE and fit.LMC = TRUE, I get the warning message 

"Warning messages:
1: In fit.variogram(object, model, fit.sills = fit.sills, fit.ranges = fit.ranges,  :
  No convergence after 200 iterations: try different initial values?
2: In predict.gstat(sim.fit, newdata = xy, nsim = 1, debug.level = -1) :
  No Intrinsic Correlation or Linear Model of Coregionalization found
Reason: ranges differ?

and a simulation still happens.  What is going on behind the scenes here when there is no acceptable model of coregionalization?  

Main Question:
----------------------------
In the SGSim step, how can I apply pre-existing data which can be used as conditional in the simulation process?  Based on the ids present in the gstat object,  I will have a data set of mu0, and conditional on this I want to simulate ?psv?, ?vrup?, and ?slip?.  I have tried putting the data in the sim.fit object as well as new data and it doesn?t seem to make a difference.  What am I doing wrong?

Thanks in advance!

Best,
- William Savran


From wsavran at gmail.com  Mon Jun  6 19:03:30 2016
From: wsavran at gmail.com (William Savran)
Date: Mon, 6 Jun 2016 10:03:30 -0700
Subject: [R-sig-Geo] multivariate sequential gaussian cosimulation using
	gstat in R
Message-ID: <A67C6057-27D4-4FBE-B271-BB378258752D@gmail.com>

Dear All,

I am new to gstat, R, and fairly new to applying geostatistical methods, so I have a few questions.  I am looking to simulate a set of 3 (out of 4) correlated stochastic fields, which are all related through a linear model of coregionalization. One of the fields will be used as conditional data for the simulation.  With that being said, if anything seems alarming to you please let me know. Also, if you have any tips/trick that would be helpful for a novice please let me know.

Right now, my algorithm is working as follows (almost exactly following the demo):

# define gstat object
sim.g <- gstat(id='slip', formula=slip.sc~1, data=sim, nmax = 30, maxdist = 200, beta=0, set = list(nocheck = 1))
sim.g <- gstat(sim.g, 'psv', psv.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
sim.g <- gstat(sim.g, 'vrup', vrup.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
sim.g <- gstat(sim.g, 'mu0', mu0.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
sim.g <- gstat(sim.g, model=vgm(1,"Exp",1000,1,anis=c(90,0.5)), fill.all=T)

# fit lmc
sim.fit = fit.lmc(var, sim.g, correct.diagonal = 1.01)

# perform SGSim
z <- predict(sim.fit, newdata=xy, nsim=1, debug.level = -1)  If I specify fit.ranges = TRUE and fit.LMC = TRUE, I get the warning message 

"Warning messages:
1: In fit.variogram(object, model, fit.sills = fit.sills, fit.ranges = fit.ranges,  :
 No convergence after 200 iterations: try different initial values?
2: In predict.gstat(sim.fit, newdata = xy, nsim = 1, debug.level = -1) :
 No Intrinsic Correlation or Linear Model of Coregionalization found
Reason: ranges differ?

and a simulation still happens.  What is going on behind the scenes here when there is no acceptable model of coregionalization or intrinsic correlation?  Do I only want to simulate using a constant range for all my 

Main Question:
----------------------------
In the SGSim step, how can I apply pre-existing data which can be used as conditional in the simulation process?  Based on the ids present in the gstat object,  I will have a data set of mu0, and conditional on this I want to simulate ?psv?, ?vrup?, and ?slip?.  I have tried putting the data in the sim.fit object as well as in newdata and it doesn?t seem to make a difference.  What am I doing wrong?

Thanks in advance!

Best,
- William Savran


From edzer.pebesma at uni-muenster.de  Mon Jun  6 19:26:06 2016
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Mon, 6 Jun 2016 19:26:06 +0200
Subject: [R-sig-Geo] multivariate sequential gaussian cosimulation using
 gstat in R
In-Reply-To: <A67C6057-27D4-4FBE-B271-BB378258752D@gmail.com>
References: <A67C6057-27D4-4FBE-B271-BB378258752D@gmail.com>
Message-ID: <5755B22E.1010200@uni-muenster.de>



On 06/06/16 19:03, William Savran wrote:
> Dear All,
> 
> I am new to gstat, R, and fairly new to applying geostatistical methods, so I have a few questions.  I am looking to simulate a set of 3 (out of 4) correlated stochastic fields, which are all related through a linear model of coregionalization. One of the fields will be used as conditional data for the simulation.  With that being said, if anything seems alarming to you please let me know. Also, if you have any tips/trick that would be helpful for a novice please let me know.
> 
> Right now, my algorithm is working as follows (almost exactly following the demo):
> 
> # define gstat object
> sim.g <- gstat(id='slip', formula=slip.sc~1, data=sim, nmax = 30, maxdist = 200, beta=0, set = list(nocheck = 1))
> sim.g <- gstat(sim.g, 'psv', psv.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
> sim.g <- gstat(sim.g, 'vrup', vrup.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
> sim.g <- gstat(sim.g, 'mu0', mu0.sc~1, sim, nmax = 30, maxdist = 200, beta=0)
> sim.g <- gstat(sim.g, model=vgm(1,"Exp",1000,1,anis=c(90,0.5)), fill.all=T)
> 
> # fit lmc
> sim.fit = fit.lmc(var, sim.g, correct.diagonal = 1.01)
> 
> # perform SGSim
> z <- predict(sim.fit, newdata=xy, nsim=1, debug.level = -1)  If I specify fit.ranges = TRUE and fit.LMC = TRUE, I get the warning message 
> 
> "Warning messages:
> 1: In fit.variogram(object, model, fit.sills = fit.sills, fit.ranges = fit.ranges,  :
>  No convergence after 200 iterations: try different initial values?
> 2: In predict.gstat(sim.fit, newdata = xy, nsim = 1, debug.level = -1) :
>  No Intrinsic Correlation or Linear Model of Coregionalization found
> Reason: ranges differ?
> 
> and a simulation still happens.  What is going on behind the scenes here when there is no acceptable model of coregionalization or intrinsic correlation?  Do I only want to simulate using a constant range for all my 
> 
> Main Question:
> ----------------------------
> In the SGSim step, how can I apply pre-existing data which can be used as conditional in the simulation process?  Based on the ids present in the gstat object,  I will have a data set of mu0, and conditional on this I want to simulate ?psv?, ?vrup?, and ?slip?.  I have tried putting the data in the sim.fit object as well as in newdata and it doesn?t seem to make a difference.  What am I doing wrong?
> 
> Thanks in advance!
> 
> Best,
> - William Savran
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

Your script is quite messy and incomplete (where do sim, var, and xy
come from? Does the "If I specify..." comment really relate to the
predict call? Which command generates which warning?), and is not
reproducible.

The way you specify conditioning data is correct, so it is the basis for
conditioning your simulations. The warnings should not be ignored,
though: have you tried plotting the variograms & cross variograms with
the (wrongly?) fitted model?

When you give this little insight in what you do, I can only reply with
wild guesses.
-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20160606/256f3f4e/attachment.bin>

From strimas at zoology.ubc.ca  Mon Jun  6 19:36:53 2016
From: strimas at zoology.ubc.ca (Matt Strimas-Mackey)
Date: Mon, 6 Jun 2016 10:36:53 -0700
Subject: [R-sig-Geo] Fwd: dissolve tiny polygons to others with
	unionSpatialPolygons{maptools}
In-Reply-To: <CABK368hgW1kzCz15kzc1HzFXC_fxbgNP9Tv+L3SH3u14+Px6Kw@mail.gmail.com>
References: <CABK368iX8Qa7ferBcpsdcvQu4cWxWHGuNTPYHD=SKBTgpjJ0XA@mail.gmail.com>
	<alpine.LFD.2.20.1606061516010.10035@reclus.nhh.no>
	<CABK368hgW1kzCz15kzc1HzFXC_fxbgNP9Tv+L3SH3u14+Px6Kw@mail.gmail.com>
Message-ID: <CAHb9yCDk_1ZaYpFt1ucyz0H2jA5KrsvW7D2bAP5eWiXhoOBZEA@mail.gmail.com>

Hi Kumar,

Based on the exchange in the thread Roger referenced I wrote a blog post
summarizing my understanding of these issue of scale and numerical
precision when performing topological operations with rgeos. It doesn't
address your issue directly, however, I think it will be of some value.
Good luck!

http://strimas.com/spatial/rgeos-scale/

M
On Jun 6, 2016 10:01 AM, "Kumar Mainali" <kpmainali at gmail.com> wrote:

> Hi Matt,
>
> You seem to have used gUnaryUnion for dissolving slivers, as I can see
> your exchanges in R listserv. Your link to your R markdown as an example of
> how to use the functions is dead. Roger responded to my question but I
> cannot find a solution. Would you mind looking at my problem and suggest
> what I should do?
>
> Thanks a lot.
>
> Regards,
> Kumar Mainali
> Postdoctoral Associate
> Department of Biology
> University of Maryland, College Park
>
> ---------- Forwarded message ----------
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Date: Mon, Jun 6, 2016 at 9:24 AM
> Subject: Re: dissolve tiny polygons to others with
> unionSpatialPolygons{maptools}
> To: Kumar Mainali <kpmainali at gmail.com>
> Cc: r-help <r-help at r-project.org>, r-sig-geo at r-project.org
>
>
> On Mon, 6 Jun 2016, Kumar Mainali wrote:
>
> I am trying to use unionSpatialPolygons() of maptools to eliminate sliver
>> in species range. I want to dissolve tiny sliver polygons in a shapefile
>> to
>> bigger polygons as "Eliminate (Data Management)" of ArcMap does. Whereas I
>> can dissolve polygons that have identical features in the argument "IDs",
>> I
>> cannot dissolve tiny polygons based on some threshold in area. In fact,
>> the
>> argument "threshold" has no effect in the output.
>>
>
> Indeed, threshold is not passed through, it was used when the function
> used gpclib rather than rgeos; I'm minded to deprecate
> maptools::unionSpatialPolygons anyway. Note that the data are in
> geographical coordinates, which may very well not be appropriate for the
> topological operations you are trying to do. Use rgeos::gUnaryUnion
> instead, and refer to this thread:
>
> https://stat.ethz.ch/pipermail/r-sig-geo/2015-November/023667.html
>
> for using rgeos::set_RGEOS_polyThreshold() and friends. They do not,
> however, try to guess which of the polygons neighbouring the sliver should
> get the extra area, so you'll have to think that through yourself.
>
> Googling for lists:R-sig-geo dissolve slivers gets a fair number of hits.
>
> There is always also the upstream question of where the slivers came from,
> and whether the resolution is not in the earlier process - generate a map
> without slivers that says exactly what you mean, rather than fudging it
> afterwards.
>
> Roger
>
>
>> ?Input data is available here: ?
>> https://www.dropbox.com/sh/a0x5bbo9u60y7is/AAB6RjXHFQKZv-i-t4JclF3ba?dl=0
>>
>> p.ranges <- shapefile{raster}
>> (IDs <- p.ranges$style_id)
>> library(maptools)
>> unionSpatialPolygons(p.ranges, IDs = IDs, threshold = 1.5)
>>
>> ?-- Kumar Mainali
>> Postdoctoral Associate
>> Department of Biology
>> University of Maryland, College Park
>>
>>
>> ?
>>
>>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; fax +47 55 95 91 00
> e-mail: Roger.Bivand at nhh.no
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> http://depsy.org/person/434412
>
> ?
>

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Mon Jun  6 21:19:25 2016
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 6 Jun 2016 21:19:25 +0200
Subject: [R-sig-Geo] dissolve tiny polygons to others with
 unionSpatialPolygons{maptools}
In-Reply-To: <CABK368jVuK+xJLhJb6LWZC0_Dc9J0ZAKqxWrPULfYj06HAx7AA@mail.gmail.com>
References: <CABK368iX8Qa7ferBcpsdcvQu4cWxWHGuNTPYHD=SKBTgpjJ0XA@mail.gmail.com>
	<alpine.LFD.2.20.1606061516010.10035@reclus.nhh.no>
	<CABK368jVuK+xJLhJb6LWZC0_Dc9J0ZAKqxWrPULfYj06HAx7AA@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1606062014050.31323@reclus.nhh.no>

On Mon, 6 Jun 2016, Kumar Mainali wrote:

> Thank you, Roger. I cannot fix the upstream processes to eliminate
> generation of these problems. So, I need to deal with the data I got.
> I tried various online resources including
> https://gist.github.com/mstrimas/1b4a4b93a9d4a158bce4 and tried:
> setScales()
> set_RGEOS_dropSlivers(TRUE)
> set_RGEOS_polyThreshold()
>
> before performing:
> gUnaryUnion(p.ranges, id=NULL)
>
> This eliminates the slivers but it also dissolves ALL the polygons to get a
> single polygon. That is not something I wanted. I would like to dissolve
> only the tiny polygons to the neighboring bigger ones.
>
> Any help is very much appreciated!

Then you must decide which larger polygon (Multipolygon, Polygons object) 
they are to belong to. They are Polygon objects within Polygons objects, 
and are not entities you can "get at" easily. In your case, all the 
entities are Polygons objects, and perhaps by buffering the first Polygons 
object northwards, you could include all of the entities. They are far 
from trivial in size, and it is not without consequences for downstream 
analysis that you are shifting a boundary by tens of km. Really the 
upstream data source should provide clean data.

Fixing this would be arbitrary and messy, with consequences for downstream 
analysis. In addition, there are empty slivers too. I really advise you to 
get back to the data providers, and I wouldn't trust Eliminate either - 
it's just making a tidy picture, but one that isn't what the data 
processing is saying. I could do it technically, but I wouldn't share that 
script with anyone, because it isn't the way to go.

Roger

>
> ?
>
> Postdoctoral Associate
> Department of Biology
> University of Maryland, College Park
>
> On Mon, Jun 6, 2016 at 9:24 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
>
>> On Mon, 6 Jun 2016, Kumar Mainali wrote:
>>
>> I am trying to use unionSpatialPolygons() of maptools to eliminate sliver
>>> in species range. I want to dissolve tiny sliver polygons in a shapefile
>>> to
>>> bigger polygons as "Eliminate (Data Management)" of ArcMap does. Whereas I
>>> can dissolve polygons that have identical features in the argument "IDs",
>>> I
>>> cannot dissolve tiny polygons based on some threshold in area. In fact,
>>> the
>>> argument "threshold" has no effect in the output.
>>>
>>
>> Indeed, threshold is not passed through, it was used when the function
>> used gpclib rather than rgeos; I'm minded to deprecate
>> maptools::unionSpatialPolygons anyway. Note that the data are in
>> geographical coordinates, which may very well not be appropriate for the
>> topological operations you are trying to do. Use rgeos::gUnaryUnion
>> instead, and refer to this thread:
>>
>> https://stat.ethz.ch/pipermail/r-sig-geo/2015-November/023667.html
>>
>> for using rgeos::set_RGEOS_polyThreshold() and friends. They do not,
>> however, try to guess which of the polygons neighbouring the sliver should
>> get the extra area, so you'll have to think that through yourself.
>>
>> Googling for lists:R-sig-geo dissolve slivers gets a fair number of hits.
>>
>> There is always also the upstream question of where the slivers came from,
>> and whether the resolution is not in the earlier process - generate a map
>> without slivers that says exactly what you mean, rather than fudging it
>> afterwards.
>>
>> Roger
>>
>>
>>> ?Input data is available here: ?
>>> https://www.dropbox.com/sh/a0x5bbo9u60y7is/AAB6RjXHFQKZv-i-t4JclF3ba?dl=0
>>>
>>> p.ranges <- shapefile{raster}
>>> (IDs <- p.ranges$style_id)
>>> library(maptools)
>>> unionSpatialPolygons(p.ranges, IDs = IDs, threshold = 1.5)
>>>
>>> ?-- Kumar Mainali
>>> Postdoctoral Associate
>>> Department of Biology
>>> University of Maryland, College Park
>>>
>>>
>>> ?
>>>
>>>
>> --
>> Roger Bivand
>> Department of Economics, Norwegian School of Economics,
>> Helleveien 30, N-5045 Bergen, Norway.
>> voice: +47 55 95 93 55; fax +47 55 95 91 00
>> e-mail: Roger.Bivand at nhh.no
>> http://orcid.org/0000-0003-2392-6140
>> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
>> http://depsy.org/person/434412
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
http://depsy.org/person/434412

From nell.redu at hotmail.fr  Tue Jun  7 16:53:09 2016
From: nell.redu at hotmail.fr (Nelly Reduan)
Date: Tue, 7 Jun 2016 14:53:09 +0000
Subject: [R-sig-Geo] Building a prediction raster when the statistical
 model was built from sampling units of different sizes
In-Reply-To: <CAASFQpRa9ybZtEwtDgcMCS_rHTTBohOHge-KKfo3c9jHaNdSJg@mail.gmail.com>
References: <CY1PR05MB27303108E8419879545ACB5899420@CY1PR05MB2730.namprd05.prod.outlook.com>,
	<CAASFQpRa9ybZtEwtDgcMCS_rHTTBohOHge-KKfo3c9jHaNdSJg@mail.gmail.com>
Message-ID: <CY1PR05MB2730AB11356CF6879D228C94995D0@CY1PR05MB2730.namprd05.prod.outlook.com>

Hi Chris,

Thank you very much for your answer.


They are striped skunks that have been captured. In my data, all striped skunks that have been captured within a same trapping site have the same capture success. Thus, each of 50 trapping sites was assigned to one capture success. If, I group trapping sites together, I reduce the sampling size. As the actuel sampling size (50 trapping sites) is rather small, can this cause problem for predicted data estimates?

Thanks very much for your time.

Have a nice day.

Nell


________________________________
De : chris english <englishchristophera at gmail.com>
Envoy? : jeudi 2 juin 2016 06:10:32
? : Nelly Reduan
Cc : Help R-Sig_Geo
Objet : Re: [R-sig-Geo] Building a prediction raster when the statistical model was built from sampling units of different sizes


Hi Nell,

Just a couple of questions. Trapping sites range from 24km^2 - 236km^2, and there are 50 such sites, looking at the 50 sites might there be a way to bin them reasonably into trap area groups?

Your 50 observations suggests one thing was trapped and thereafter trapping was discontinued. Is this correct?

And just for general information, what was being trapped?

Sticking closer to your data, you might consider GAM(ing) the bins and summing the resultant GAMs. Need to think some more on the predictive raster aspect. Sorry for an essentially inconclusive answer.

Chris

	[[alternative HTML version deleted]]


From englishchristophera at gmail.com  Wed Jun  8 08:41:09 2016
From: englishchristophera at gmail.com (chris english)
Date: Wed, 8 Jun 2016 09:41:09 +0300
Subject: [R-sig-Geo] Building a prediction raster when the statistical
 model was built from sampling units of different sizes
In-Reply-To: <CY1PR05MB2730AB11356CF6879D228C94995D0@CY1PR05MB2730.namprd05.prod.outlook.com>
References: <CY1PR05MB27303108E8419879545ACB5899420@CY1PR05MB2730.namprd05.prod.outlook.com>
	<CAASFQpRa9ybZtEwtDgcMCS_rHTTBohOHge-KKfo3c9jHaNdSJg@mail.gmail.com>
	<CY1PR05MB2730AB11356CF6879D228C94995D0@CY1PR05MB2730.namprd05.prod.outlook.com>
Message-ID: <CAASFQpTkBPQ=rsmjobFtGVsEbZigcXYdccqDy6qY1i7aE3dG_A@mail.gmail.com>

Nell,

I'm still trying to understand what sounds to me like an embedded data
reduction. Please understand, I don't trap skunks (striped or otherwise). I
have, on many occasions, observed them in the wild but I am cautious not to
make them scared due to the known, smelly results.

Understand as well that I am not versed in capture success, per se, I just
examine data and wonder if it contains generalizable or perhaps surprising
properties.

So if it were me, contrary to my nature and practice of observation,
trapping striped skunks in a 24 km^2 study area of a given land use/land
cover, and I trapped 15 skunks over a period of 2 months, I would deem that
15 observations for that study area/time period.If, in my 32 km^2 study
area of slightly different land cover and different resource availability I
got 70 in two months, I'd have 70 observations. This is how I would view
it, and quite probably within the accepted science I would be going about
it all wrong.

As you present the matter, at least as I understand it, the number of my
hypothetical captures always reduces to one dimension (the study area),
irrespective of the above variance between study areas. This approach
puzzles me as it seems that information about desirable resource
distribution (from the point of view of the skunk) gets lost, and 'capture
success' becomes murky, at least for me.

As my wife always says, "It depends on what the research question is."

What is the research question in this case?

My apologies to you and capture science if I have completely misunderstood
as I all too often do.

Chris

On Tue, Jun 7, 2016 at 5:53 PM, Nelly Reduan <nell.redu at hotmail.fr> wrote:

> Hi Chris,
>
> Thank you very much for your answer.
>
>
> They are striped skunks that have been captured. In my data, all striped
> skunks that have been captured within a same trapping site have the same
> capture success. Thus, each of 50 trapping sites was assigned to one
> capture success. If, I group trapping sites together, I reduce the sampling
> size. As the actuel sampling size (50 trapping sites) is rather small, can
> this cause problem for predicted data estimates?
>
> Thanks very much for your time.
>
> Have a nice day.
>
> Nell
>
>
> ------------------------------
> *De :* chris english <englishchristophera at gmail.com>
> *Envoy? :* jeudi 2 juin 2016 06:10:32
> *? :* Nelly Reduan
> *Cc :* Help R-Sig_Geo
> *Objet :* Re: [R-sig-Geo] Building a prediction raster when the
> statistical model was built from sampling units of different sizes
>
>
> Hi Nell,
>
> Just a couple of questions. Trapping sites range from 24km^2 - 236km^2,
> and there are 50 such sites, looking at the 50 sites might there be a way
> to bin them reasonably into trap area groups?
>
> Your 50 observations suggests one thing was trapped and thereafter
> trapping was discontinued. Is this correct?
>
> And just for general information, what was being trapped?
>
> Sticking closer to your data, you might consider GAM(ing) the bins and
> summing the resultant GAMs. Need to think some more on the predictive
> raster aspect. Sorry for an essentially inconclusive answer.
>
> Chris
>

	[[alternative HTML version deleted]]


From Giacomo_May94 at gmx.de  Wed Jun  8 13:26:39 2016
From: Giacomo_May94 at gmx.de (Giacomo May)
Date: Wed, 8 Jun 2016 13:26:39 +0200
Subject: [R-sig-Geo] Create circular polygon of certain radius in R
Message-ID: <trinity-8c67b310-a797-483b-bf42-b759135bbf0b-1465385199527@3capp-gmx-bs54>

Hi,
I have a raster, which is projected into "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0". Now I would like to create a circular Polygon with a radius of 300 meters around a cell of my choice. I know about the function "spCircle()" in the package "sampSurf" and I heard that this should somehow be possibleusing the "gBuffer"-function from  the "rgeos"-package, but I don't know exactly how to use it.
I have tried achieving this with the following piece of code (altdata is the raster I am using):

coords <- xyFromCell(altdata,12703)
proj <- altdata at crs@projargs
coords_sp <- SpatialPoints(coords,CRS(proj))
polygon <- gBuffer(coords_sp,width=300)

But this returns the following warning message:

Warning message:

In gBuffer(coords_sp, width = 300) :
  Spatial object is not projected; GEOS expects planar coordinates

If anyone could help me with this I would be deeply grateful.
Best regards,
Giacomo May


From bfalevlist at gmail.com  Wed Jun  8 17:29:26 2016
From: bfalevlist at gmail.com (=?UTF-8?Q?Bede-Fazekas_=c3=81kos?=)
Date: Wed, 8 Jun 2016 17:29:26 +0200
Subject: [R-sig-Geo] Create circular polygon of certain radius in R
In-Reply-To: <trinity-8c67b310-a797-483b-bf42-b759135bbf0b-1465385199527@3capp-gmx-bs54>
References: <trinity-8c67b310-a797-483b-bf42-b759135bbf0b-1465385199527@3capp-gmx-bs54>
Message-ID: <7b64a50e-a638-2900-40e4-f94c1a817e13@gmail.com>

Hi Giacomo,

you should use a projection that is planar (azimuthal). Select one 
planar projection, use spTransform() from sp package to transform your 
data to the selected planar projection, and then everything will works fine.
Some links:
https://en.wikipedia.org/wiki/Map_projection#Azimuthal_.28projections_onto_a_plane.29
http://spatialreference.org/ref/epsg/
http://www.inside-r.org/packages/cran/rgdal/docs/spTransform

HTH,
?kos Bede-Fazekas
Hungarian Academy of Sciences

2016.06.08. 13:26 keltez?ssel, Giacomo May ?rta:
> Hi,
> I have a raster, which is projected into "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0". Now I would like to create a circular Polygon with a radius of 300 meters around a cell of my choice. I know about the function "spCircle()" in the package "sampSurf" and I heard that this should somehow be possibleusing the "gBuffer"-function from  the "rgeos"-package, but I don't know exactly how to use it.
> I have tried achieving this with the following piece of code (altdata is the raster I am using):
>
> coords <- xyFromCell(altdata,12703)
> proj <- altdata at crs@projargs
> coords_sp <- SpatialPoints(coords,CRS(proj))
> polygon <- gBuffer(coords_sp,width=300)
>
> But this returns the following warning message:
>
> Warning message:
>
> In gBuffer(coords_sp, width = 300) :
>    Spatial object is not projected; GEOS expects planar coordinates
>
> If anyone could help me with this I would be deeply grateful.
> Best regards,
> Giacomo May
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


From Roger.Bivand at nhh.no  Wed Jun  8 18:04:24 2016
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 8 Jun 2016 18:04:24 +0200
Subject: [R-sig-Geo] Create circular polygon of certain radius in R
In-Reply-To: <7b64a50e-a638-2900-40e4-f94c1a817e13@gmail.com>
References: <trinity-8c67b310-a797-483b-bf42-b759135bbf0b-1465385199527@3capp-gmx-bs54>
	<7b64a50e-a638-2900-40e4-f94c1a817e13@gmail.com>
Message-ID: <alpine.LFD.2.20.1606081800300.15178@reclus.nhh.no>

On Wed, 8 Jun 2016, Bede-Fazekas ?kos wrote:

> Hi Giacomo,
>
> you should use a projection that is planar (azimuthal). Select one planar 
> projection, use spTransform() from sp package to transform your data to the 
> selected planar projection, and then everything will works fine.

Right, this is what rgeos is warning about. I'm afraid, though, that since 
the raster is in geographical coordinates, that may entail transforming 
the circle back from planar coordinates (+units=m) to geographical 
coordinates, and then the number of border line segments may also matter. 
Had the workflow been in projected coordinates from an earlier point, this 
would be simpler.

Roger

> Some links:
> https://en.wikipedia.org/wiki/Map_projection#Azimuthal_.28projections_onto_a_plane.29
> http: //spatialreference.org/ref/epsg/
> http: //www.inside-r.org/packages/cran/rgdal/docs/spTransform
>
> HTH,
> ?kos Bede-Fazekas
> Hungarian Academy of Sciences
>
> 2016.06.08. 13:26 keltez?ssel, Giacomo May ?rta:
>>  Hi,
>>  I have a raster, which is projected into "+proj=longlat +datum=WGS84
>>  +no_defs +ellps=WGS84 +towgs84=0,0,0". Now I would like to create a
>>  circular Polygon with a radius of 300 meters around a cell of my choice. I
>>  know about the function "spCircle()" in the package "sampSurf" and I heard
>>  that this should somehow be possibleusing the "gBuffer"-function from  the
>>  "rgeos"-package, but I don't know exactly how to use it.
>>  I have tried achieving this with the following piece of code (altdata is
>>  the raster I am using):
>>
>>  coords <- xyFromCell(altdata,12703)
>>  proj <- altdata at crs@projargs
>>  coords_sp <- SpatialPoints(coords,CRS(proj))
>>  polygon <- gBuffer(coords_sp,width=300)
>>
>>  But this returns the following warning message:
>>
>>  Warning message:
>>
>>  In gBuffer(coords_sp, width = 300) :
>>     Spatial object is not projected; GEOS expects planar coordinates
>>
>>  If anyone could help me with this I would be deeply grateful.
>>  Best regards,
>>  Giacomo May
>>
>>  _______________________________________________
>>  R-sig-Geo mailing list
>>  R-sig-Geo at r-project.org
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> 
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
http://depsy.org/person/434412

From r.hijmans at gmail.com  Wed Jun  8 20:47:58 2016
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Wed, 8 Jun 2016 11:47:58 -0700
Subject: [R-sig-Geo] Create circular polygon of certain radius in R
In-Reply-To: <alpine.LFD.2.20.1606081800300.15178@reclus.nhh.no>
References: <trinity-8c67b310-a797-483b-bf42-b759135bbf0b-1465385199527@3capp-gmx-bs54>
	<7b64a50e-a638-2900-40e4-f94c1a817e13@gmail.com>
	<alpine.LFD.2.20.1606081800300.15178@reclus.nhh.no>
Message-ID: <CANtt_hz69FkcL4HKfF=wjqdTRqhPg2+pu+qdt7k9wS08y53Ojw@mail.gmail.com>

For lon/lat coordinates, you can use the circles function in "dismo":

library(dismo)
r <- raster(system.file("external/rlogo.grd", package="raster"))
pts <- data.frame(x=c(17, 42, 85, 70, 20, 53, 26, 84), y=c(28, 73, 38,
56, 0, 29, 63, 22))

c1 <- circles(pts, lonlat=FALSE, d=9, dissolve=FALSE)
c2 <- circles(pts, lonlat=TRUE, d=1000000, dissolve=FALSE)

plot(r)
plot(geometry(c1), border='red', lwd=2, add=TRUE)
plot(geometry(c2), border='blue', lwd=2, add=TRUE)



On Wed, Jun 8, 2016 at 9:04 AM, Roger Bivand <Roger.Bivand at nhh.no> wrote:
> On Wed, 8 Jun 2016, Bede-Fazekas ?kos wrote:
>
>> Hi Giacomo,
>>
>> you should use a projection that is planar (azimuthal). Select one planar
>> projection, use spTransform() from sp package to transform your data to the
>> selected planar projection, and then everything will works fine.
>
>
> Right, this is what rgeos is warning about. I'm afraid, though, that since
> the raster is in geographical coordinates, that may entail transforming the
> circle back from planar coordinates (+units=m) to geographical coordinates,
> and then the number of border line segments may also matter. Had the
> workflow been in projected coordinates from an earlier point, this would be
> simpler.
>
> Roger
>
>
>> Some links:
>>
>> https://en.wikipedia.org/wiki/Map_projection#Azimuthal_.28projections_onto_a_plane.29
>> http: //spatialreference.org/ref/epsg/
>> http: //www.inside-r.org/packages/cran/rgdal/docs/spTransform
>>
>> HTH,
>> ?kos Bede-Fazekas
>> Hungarian Academy of Sciences
>>
>> 2016.06.08. 13:26 keltez?ssel, Giacomo May ?rta:
>>>
>>>  Hi,
>>>  I have a raster, which is projected into "+proj=longlat +datum=WGS84
>>>  +no_defs +ellps=WGS84 +towgs84=0,0,0". Now I would like to create a
>>>  circular Polygon with a radius of 300 meters around a cell of my choice.
>>> I
>>>  know about the function "spCircle()" in the package "sampSurf" and I
>>> heard
>>>  that this should somehow be possibleusing the "gBuffer"-function from
>>> the
>>>  "rgeos"-package, but I don't know exactly how to use it.
>>>  I have tried achieving this with the following piece of code (altdata is
>>>  the raster I am using):
>>>
>>>  coords <- xyFromCell(altdata,12703)
>>>  proj <- altdata at crs@projargs
>>>  coords_sp <- SpatialPoints(coords,CRS(proj))
>>>  polygon <- gBuffer(coords_sp,width=300)
>>>
>>>  But this returns the following warning message:
>>>
>>>  Warning message:
>>>
>>>  In gBuffer(coords_sp, width = 300) :
>>>     Spatial object is not projected; GEOS expects planar coordinates
>>>
>>>  If anyone could help me with this I would be deeply grateful.
>>>  Best regards,
>>>  Giacomo May
>>>
>>>  _______________________________________________
>>>  R-sig-Geo mailing list
>>>  R-sig-Geo at r-project.org
>>>  https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Roger Bivand
> Department of Economics, Norwegian School of Economics,
> Helleveien 30, N-5045 Bergen, Norway.
> voice: +47 55 95 93 55; fax +47 55 95 91 00
> e-mail: Roger.Bivand at nhh.no
> http://orcid.org/0000-0003-2392-6140
> https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
> http://depsy.org/person/434412
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From r.hijmans at gmail.com  Wed Jun  8 21:44:49 2016
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Wed, 8 Jun 2016 12:44:49 -0700
Subject: [R-sig-Geo] How to calculate climatology in rasterbricks
In-Reply-To: <2018207050.1264934.1465073940728.JavaMail.yahoo@mail.yahoo.com>
References: <1136618946.369322.1464899448822.JavaMail.yahoo.ref@mail.yahoo.com>
	<1136618946.369322.1464899448822.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsSV+Cs6J-Sw=fBb5Jo2tJBBwxroL2TJvTLatV+RZVi5Q@mail.gmail.com>
	<57514C33.7010400@wur.nl>
	<2127614783.774242.1464967844732.JavaMail.yahoo@mail.yahoo.com>
	<CAKkiGbsx=Kt6dagz0fHCiGA7tHZzMkZyFFbr2MBEkqSmxjX35A@mail.gmail.com>
	<1875625356.902419.1464985541127.JavaMail.yahoo@mail.yahoo.com>
	<5752DC3D.1060904@wur.nl>
	<CAKkiGbsYFLmxizSGmNUk-=NJR=133=-efRU7XSZOGGD6tx1Rrg@mail.gmail.com>
	<2018207050.1264934.1465073940728.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CANtt_hyPVmwzQz_RfYWnTKYB0G8SuTmAJNQPQQ2akgQtdD1LpQ@mail.gmail.com>

This (zapply passing ellipses arguments to stackApply) was fixed, I
think, in the development version. You can try it if you want:

install.packages("raster", repos="http://R-Forge.R-project.org")


On Sat, Jun 4, 2016 at 1:59 PM, Thiago V. dos Santos via R-sig-Geo
<r-sig-geo at r-project.org> wrote:
> Thanks again Lo?c and Vijay for helping to identify the issue.
>
> Hopefully Robert or any of the contributors will see this question and kindly suggest a workaround.
>  Cheers,
>  -- Thiago V. dos Santos
>
> PhD student
> Land and Atmospheric Science
> University of Minnesota
>
>
>
> On Saturday, June 4, 2016 10:30 AM, Vijay Lulla <vijaylulla at gmail.com> wrote:
> Yes, it is because of na.rm argument.  It's stackApply that defaults
> na.rm=TRUE not zApply.  You can check that from the following
> interaction:
>
>> args(mean.default)
> function (x, trim = 0, na.rm = FALSE, ...)
> NULL
>> args(sum)
> function (..., na.rm = FALSE)
> NULL
>> args(stackApply)
> function (x, indices, fun, filename = "", na.rm = TRUE, ...)
> NULL
>> args(zApply)
> function (x, by, fun = mean, name = "", ...)
> NULL
>>
>
> It is surprising that stackApply defaults differently than most of the
> R functions but it kinda makes sense too.  I suspect the defaults are
> what they are because in remote sensing related image procesing work
> NAs get introduced, especially when doing projection transformation
> and this can trip up many users and the authors might've decided to be
> helpful to them.  I don't know...I'm just speculating.
>
> HTH,
> Vijay.
>
>
> On Sat, Jun 4, 2016 at 9:48 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl> wrote:
>> Hi Thiago,
>>
>> Yes, I can reproduce.
>> It's probably because zApply() defaults to na.rm = TRUE (in fact the na.rm =
>> argument is ignored if you pass it), and
>>
>> sum(c(NA, NA), na.rm = TRUE)
>> [1] 0 (I was surprised by that).
>>
>> Perhaps the stackApply() call in the zApply() function definition is missing
>> an ellipsis.
>>
>> Cheers,
>> Lo?c
>>
>>
>> On 06/03/2016 10:25 PM, Thiago V. dos Santos wrote:
>>>
>>> Dear Vijay and Lo?c,
>>>
>>>
>>> I have identified some artifacts in the results when using zApply on my
>>> data. To reproduce it, please download this shapefile
>>> (https://www.dropbox.com/s/in2z10mlerr2sfu/southern.zip?dl=0) and run the
>>> code below (see the comments after the plot commands):
>>>
>>> -----------------------
>>> library(raster)
>>> library(zoo)
>>>
>>> # Create the date sequence
>>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>>
>>> # Create raster stack and assign dates
>>> r <- raster(ncol=24, nrow=21, xmn=-58, xmx=-47.5, ymn=-34, ymx=-22,
>>> resolution=0.5)
>>> s <- stack(lapply(1:length(idx), function(x) setValues(r,
>>> runif(ncell(r)))))
>>> s <- setZ(s, idx)
>>>
>>> # Load shapefile and crop data
>>> shp <- shapefile('~/Downloads/southern.shp', warnPRJ=F)
>>> s.c <- crop(s, extent(shp))
>>> s.c <- mask(s.c, shp)
>>> plot(s.c,1)
>>> plot(shp,add=T) # Looks good
>>>
>>> # Lo?c's approach using zApply
>>> sYM <- zApply(s.c, by = as.yearmon, sum)
>>> sM <- zApply(sYM, by = months, mean)
>>> plot(sYM,1)  # Note that the "empty" space across the extent was filled
>>> with 0's
>>> plot(sM,1)  # The same "gray area" here
>>>
>>> # Vijay's approach using calc
>>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>>> idxM <- unique(idxYM)%%100
>>> meanYM <- calc(s.c, fun=function(x) { by(x, idxYM, sum) })
>>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>>> plot(meanYM,1)  # Note that no 0 was added across the extent
>>> plot(meanM,1)  # This is OK too, no grey area
>>> -----------------------
>>>
>>> Basically, a portion of the raster extent was filled with 0's after using
>>> zApply. It doesn't happen when using calc.
>>>
>>> Any ideas on what can be causing this?
>>>
>>>   Greetings,
>>>   -- Thiago V. dos Santos
>>>
>>> PhD student
>>> Land and Atmospheric Science
>>> University of Minnesota
>>>
>>>
>>>
>>> On Friday, June 3, 2016 1:47 PM, Vijay Lulla <vijaylulla at gmail.com> wrote:
>>> Looking at Lo?c's answer and from reading ?zApply I learned of
>>> stackApply, which is used internally by zApply.  While zApply is for
>>> time series of layers, stackApply is more general and can be used for
>>> applying functions to groups of layers in a stack/brick.  It is very
>>> similar to base R's `tapply`.  And it is also fast!
>>>
>>>> system.time(meanYM1 <- stackApply(s, idxYM, mean))
>>>
>>>     user  system elapsed
>>>     0.45    0.03    0.48
>>>>
>>>> system.time(meanYM <- calc(s, fun=function(x) { by(x, idxYM, mean)}))
>>>
>>>     user  system elapsed
>>>    17.50    0.01   17.61
>>>>
>>>> all(meanYM[] == meanYM1[])
>>>
>>> [1] TRUE
>>>
>>> Thanks Lo?c for pointing out zApply.
>>>
>>>
>>> On Fri, Jun 3, 2016 at 11:30 AM, Thiago V. dos Santos via R-sig-Geo
>>> <r-sig-geo at r-project.org> wrote:
>>>>
>>>> Cool Lo?c, thanks for showing one more option.
>>>>
>>>> By the way, in this case zApply is surprisingly faster than calc:
>>>>
>>>>> system.time(meanYM <- calc(s,fun=function(x) { by(x, idxYM, sum) }))
>>>>
>>>> user  system elapsed
>>>> 21.700   0.248  22.095
>>>>
>>>>
>>>>> system.time(sYM <- zApply(s, by = as.yearmon, sum))
>>>>
>>>> user  system elapsed
>>>> 0.811   0.047   0.866
>>>>
>>>>   Cheers,
>>>>   -- Thiago V. dos Santos
>>>>
>>>> PhD student
>>>> Land and Atmospheric Science
>>>> University of Minnesota
>>>>
>>>>
>>>>
>>>> On Friday, June 3, 2016 4:25 AM, Lo?c Dutrieux <loic.dutrieux at wur.nl>
>>>> wrote:
>>>> This can also be done with zApply:
>>>>
>>>> library(zoo)
>>>>
>>>> sYM <- zApply(s, by = as.yearmon, sum)
>>>> sM <- zApply(sYM, by = months, mean)
>>>>
>>>> Cheers,
>>>> Lo?c
>>>>
>>>> On 06/03/2016 02:02 AM, Vijay Lulla wrote:
>>>>>
>>>>> I think the following StackOverflow question has the answer:
>>>>>
>>>>> http://stackoverflow.com/questions/16135877/applying-a-function-to-a-multidimensional-array-with-grouping-variable/16136775#16136775
>>>>>
>>>>> Following the instructions listed on that page for your case might go
>>>>> something like below:
>>>>>
>>>>>> idxYM <- as.integer(strftime(idx,"%Y%m"))
>>>>>> idxM <- unique(idxYM)%%100
>>>>>> meanYM <- calc(s,fun=function(x) { by(x, idxYM, mean) })
>>>>>> meanYM
>>>>>
>>>>> class       : RasterBrick
>>>>> dimensions  : 20, 20, 400, 360  (nrow, ncol, ncell, nlayers)
>>>>> resolution  : 18, 9  (x, y)
>>>>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>>> data source : in memory
>>>>> names       : X196101, X196102, X196103, X196104, X196105, X196106,
>>>>> X196107, X196108, X196109, X196110, X196111, X196112, X196201,
>>>>> X196202, X196203, ...
>>>>> min values  :  0.3728,  0.2725,  0.3421,  0.3652,  0.3342,  0.3185,
>>>>> 0.3130,  0.3780,  0.3376,  0.3727,  0.3537,  0.3737,  0.3515,  0.3588,
>>>>>    0.3334, ...
>>>>> max values  :  0.6399,  0.6652,  0.6583,  0.6640,  0.6359,  0.6761,
>>>>> 0.6442,  0.6800,  0.6397,  0.6769,  0.6489,  0.6388,  0.6471,  0.6661,
>>>>>    0.6255, ...
>>>>>
>>>>>> meanM <- calc(meanYM, fun=function(x) { by(x, idxM, mean) })
>>>>>> meanM
>>>>>
>>>>> class       : RasterBrick
>>>>> dimensions  : 20, 20, 400, 12  (nrow, ncol, ncell, nlayers)
>>>>> resolution  : 18, 9  (x, y)
>>>>> extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
>>>>> coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
>>>>> data source : in memory
>>>>> names       :     X1,     X2,     X3,     X4,     X5,     X6,     X7,
>>>>>      X8,     X9,    X10,    X11,    X12
>>>>> min values  : 0.4645, 0.4715, 0.4768, 0.4717, 0.4749, 0.4705, 0.4697,
>>>>> 0.4724, 0.4629, 0.4774, 0.4736, 0.4708
>>>>> max values  : 0.5274, 0.5275, 0.5293, 0.5259, 0.5285, 0.5276, 0.5269,
>>>>> 0.5260, 0.5256, 0.5281, 0.5279, 0.5286
>>>>>
>>>>>>
>>>>>
>>>>> I'm not sure how [in]efficient this is for actual (i.e. not toy
>>>>> example) data.  Maybe others more experienced, and knowledgeable,
>>>>> members can provide better answers.
>>>>>
>>>>> HTH,
>>>>> Vijay.
>>>>>
>>>>> On Thu, Jun 2, 2016 at 4:30 PM, Thiago V. dos Santos via R-sig-Geo
>>>>> <r-sig-geo at r-project.org> wrote:
>>>>>>
>>>>>> Dear all,
>>>>>>
>>>>>> I am working with daily time series of meteorological variables. This
>>>>>> is an example of the dataset:
>>>>>>
>>>>>> library(raster)
>>>>>>
>>>>>> # Create date sequence
>>>>>> idx <- seq(as.Date("1961/1/1"), as.Date("1990/12/31"), by = "day")
>>>>>>
>>>>>> # Create raster stack and assign dates
>>>>>> r <- raster(ncol=20, nrow=20)
>>>>>> s <- stack(lapply(1:length(idx), function(x) setValues(r,
>>>>>> runif(ncell(r)))))
>>>>>> s <- setZ(s, idx)
>>>>>>
>>>>>>
>>>>>> Now, let's assume those values represent daily precipitation. What I
>>>>>> need to do is to integrate daily to monthly values,
>>>>>> and then take a monthly climatology. Climatology in this case means
>>>>>> multi-year average of selected months, e.g., an average of the 30 Octobers
>>>>>> from 1961 to 1990, an average of the 30 Novembers from 1961 to 1990 and etc.
>>>>>>
>>>>>> On the other hand, let's assume the raster values represent daily
>>>>>> temperature. Integrating daily to monthly temperature doesn't make sense.
>>>>>> Hence, instead of integrating daily values, I need to take monthly means
>>>>>> (e.g. mean value of all days in every month), and then calculate the
>>>>>> climatology.
>>>>>>
>>>>>> What would be the best approach to achieve that using the raster
>>>>>> package?
>>>>>>
>>>>>>    Greetings,
>>>>>>    -- Thiago V. dos Santos
>>>>>>
>>>>>> PhD student
>>>>>> Land and Atmospheric Science
>>>>>> University of Minnesota
>>>>>>
>>>>>> _______________________________________________
>>>>>> R-sig-Geo mailing list
>>>>>> R-sig-Geo at r-project.org
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From afitz at email.wm.edu  Wed Jun  8 21:49:24 2016
From: afitz at email.wm.edu (Alex Fitz)
Date: Wed, 8 Jun 2016 15:49:24 -0400
Subject: [R-sig-Geo] Determining the area of grid cells from a raster in R
Message-ID: <A2021726-A83A-4696-9C74-017FC7BF99F9@email.wm.edu>

Hi,

I have a raster and I am trying to get the area of each grid cell.  The raster object is a country and I am trying to get the area of my grid cells so I can calculate density.

When I type in the name of my raster this is the output I get:

class       : RasterLayer 
dimensions  : 1155, 1441, 1664355  (nrow, ncol, ncell)
resolution  : 0.008333333, 0.008333333  (x, y)
extent      : 2.666667, 14.675, 4.266667, 13.89167  (xmin, xmax, ymin, ymax)
coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 
data source : /private/var/folders/jw/ynhy9rqn0b3bhrsdfs3qt_bw0000gn/T/RtmpJd12I7/raster/r_tmp_2016-06-08_142202_13015_34709.grd 
names       : gpw.v4.population.count.adjusted.to.2015.unwpp.country.totals_2015 
values      : 0, 65434.81  (min, max)


I have tried using the area() function but get the following Error message:
Error in as.owin.default(w) : Can't interpret W as a window


Any help would be greatly appreciated.

Best,
Alex Fitz


From r.hijmans at gmail.com  Wed Jun  8 23:00:10 2016
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Wed, 8 Jun 2016 14:00:10 -0700
Subject: [R-sig-Geo] Determining the area of grid cells from a raster in
	R
In-Reply-To: <A2021726-A83A-4696-9C74-017FC7BF99F9@email.wm.edu>
References: <A2021726-A83A-4696-9C74-017FC7BF99F9@email.wm.edu>
Message-ID: <CANtt_hyFK61nZS7nXRa75snG4wZLgAJx6tkm4ViE8VpWWJfa=g@mail.gmail.com>

> library(raster)
> library(spatstat)
( ...)
The following objects are masked from ?package:raster?:
    area, rotate, shift


So either do not load spatstat or call the raster function explicitly

raster::area(x)

Best, Robert

On Wed, Jun 8, 2016 at 12:49 PM, Alex Fitz <afitz at email.wm.edu> wrote:
> Hi,
>
> I have a raster and I am trying to get the area of each grid cell.  The raster object is a country and I am trying to get the area of my grid cells so I can calculate density.
>
> When I type in the name of my raster this is the output I get:
>
> class       : RasterLayer
> dimensions  : 1155, 1441, 1664355  (nrow, ncol, ncell)
> resolution  : 0.008333333, 0.008333333  (x, y)
> extent      : 2.666667, 14.675, 4.266667, 13.89167  (xmin, xmax, ymin, ymax)
> coord. ref. : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0
> data source : /private/var/folders/jw/ynhy9rqn0b3bhrsdfs3qt_bw0000gn/T/RtmpJd12I7/raster/r_tmp_2016-06-08_142202_13015_34709.grd
> names       : gpw.v4.population.count.adjusted.to.2015.unwpp.country.totals_2015
> values      : 0, 65434.81  (min, max)
>
>
> I have tried using the area() function but get the following Error message:
> Error in as.owin.default(w) : Can't interpret W as a window
>
>
> Any help would be greatly appreciated.
>
> Best,
> Alex Fitz
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From afitz at email.wm.edu  Thu Jun  9 15:31:58 2016
From: afitz at email.wm.edu (Alex Fitz)
Date: Thu, 9 Jun 2016 09:31:58 -0400
Subject: [R-sig-Geo] Difference in size between extract and crop functions
Message-ID: <3A2DBF7A-9A22-4578-B114-2F97D21E607F@email.wm.edu>

Hi,

I am currently trying to match my area data for each cell in a raster to the population of each cell to calculate the density.  When I run the crop function to get the specific state data that I want I have significantly more rows/cells of data than when I run my extract function to get the populations given.  Since they aren?t the same size I can?t join them together.  I?m thinking maybe I have some NA data or extra points when I run my crop but i?m not sure how to check. Any help would be greatly appreciated.

These are the crop and extract functions I am running:
state_crop <- crop(pop_world_2015, states, df = TRUE)
state_pop_15 <- extract(pop_world_2015, states, df= TRUE)

This is how i?m calculating the area:
state_area <- raster::area(state_crop)

Thanks, Alex Fitz

From janka.vanschoenwinkel at uhasselt.be  Thu Jun  9 15:32:33 2016
From: janka.vanschoenwinkel at uhasselt.be (Janka VANSCHOENWINKEL)
Date: Thu, 9 Jun 2016 15:32:33 +0200
Subject: [R-sig-Geo] Translate a net-cdf file with different years to
 polygon regions.
In-Reply-To: <CAHymutLkmHER46BP40Nup8WT52FTuDxuJbnwStzCa+_jx_7bnA@mail.gmail.com>
References: <CAHymutKcpgJoWExtVVLFsSJ8VG23-b1jumMrMivMk2ZO5Lu6Jg@mail.gmail.com>
	<D51374C4B889BC47B3C5286047C86DA1A02C3A20@whqembx03p.ad.sfwmd.gov>
	<D51374C4B889BC47B3C5286047C86DA1A02C3A70@whqembx03p.ad.sfwmd.gov>
	<CAHymutLi97xoegT8qyYD3Tz7+LwFt9DfGJoMH81dSO3KL9yh4A@mail.gmail.com>
	<CAAcGz9-x7E6B_BHLcCdE5A1t28=tqGR9j4oFF=SUvJJ-BYGWhA@mail.gmail.com>
	<CAHymutLkmHER46BP40Nup8WT52FTuDxuJbnwStzCa+_jx_7bnA@mail.gmail.com>
Message-ID: <CAHymut+2SzP75wbGqwwvqpKbFhV9wsU-G5yF3JD22er0WD=9sA@mail.gmail.com>

Dear Michael, Joseph and other colleagues,

About a month and a half ago, I asked the question below (I summarized it
shorter in this email). I received some hints on how to do it in R, but
given the fact that I am not that experienced with netcdf files in R, I
didn't manage to make it work. I continued to work with ArcGis, but even
there it is not working.

So I am giving it one more try here: could somebody help me with the
problem below?


Download from this website (
http://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts)
the following ZIP file: NUTS_2006_60M_SH.zip and save all
theNUTS_RG_60M_2006 files. In what follows I refer to this as "NUTS3
regions".

Download from this website (
https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/)
 the following nc-file: cru_ts3.23.1991.2000.pet.dat.nc.gz This is a raster
with data all over the world.

The netcdf file has data on one variable (pet) for multiple months per year
(from 1991 to 2000). For each month, I would like to translate all these
values into averages per nuts3 region.

I can open the file in R and make a cvs file from it (see code in the
original email below), but I do not manage to overlap each monthly value
with the nuts3 regions.

Results can be stored in a table by month and by nuts3, or in a shapefile.
Either way is fine!

Does somebody have any experience with this? Any concrete help is highly
appreciated! I have really no experience with netcdf files. I only have
trouble with them.. ;-)

2016-04-29 15:03 GMT+02:00 Janka VANSCHOENWINKEL <
janka.vanschoenwinkel at uhasselt.be>:

> Thanks Mike,
>
> I am running your suggestion but it always get stuck. I also don't know
> how I have to indicate which variable I want to have given the fact that
> there is only one variable, but it is measured for different time periods.
> So for instance, how to I tell R that I want to have "pet from april in
> 1995"?
>
> In mean time, I want to make clear that there are multiple points of the
> same period per nuts3-polygon. So when I was speaking about the average, I
> meant the average of all those points from the same periods, that are
> located in the same polygon. So I do want to have information per period.
>
> Thanks,
>
> Janka
>
> 2016-04-29 14:42 GMT+02:00 Michael Sumner <mdsumner at gmail.com>:
>
>>
>>
>> On Fri, 29 Apr 2016 at 22:23 Janka VANSCHOENWINKEL <
>> janka.vanschoenwinkel at uhasselt.be> wrote:
>>
>>> Hi Joseph,
>>>
>>> Thank you very much for the hint.
>>>
>>> If I may ask you, could you please help me a little bit further. I am not
>>> familiar with these packages and I do not see how to overlap the point
>>> data
>>> or the netcdf data with the nuts3 polygons.
>>>
>>> Basically, I have too options:
>>>
>>> 1) use the cvs-points (with lon, lat, and the data variable) to see which
>>> points are located in which nuts3 regions and then save these results.
>>> (but
>>> then my previous codes still applies)
>>>
>>> 2) use the original netcdf file and overlay it with the nuts3 polygon
>>> shapefile (this was your hint, but I don't know how to start with this).
>>>
>>>
>> Something like this will do it:
>>
>> library(raster)
>> require(rgdal)
>> require(ncdf4)
>>
>> st <- stack("thencfile.nc")  ## might need varname = "sst" or similar
>>
>> poly <- shapefile("theshpfile.shp")
>>
>> extract(st, poly, fun = "mean")  ## and maybe na.rm = TRUE
>>
>> If you don't want the mean, you can just leave out the fun argument and
>> you'll get all the pixel values for every time step in a big list, which
>> may not be obviously helpful but it's all useable with generic R.
>>
>> I can't quite bring myself to get your files and try it, so please try
>> and report details.
>>
>> Cheers, Mike.
>>
>>
>>> Thank you very much for your answer!
>>>
>>>
>>>
>>> 2016-04-19 14:42 GMT+02:00 Stachelek, Joseph <jstachel at sfwmd.gov>:
>>>
>>> > Since your nc file contains multiple layers, you will want to use
>>> > `raster::stack()` rather than `raster::raster()`.
>>> >
>>> >
>>> > -----Original Message-----
>>> > From: Stachelek, Joseph
>>> > Sent: Tuesday, April 19, 2016 8:23 AM
>>> > To: 'Janka VANSCHOENWINKEL' <janka.vanschoenwinkel at uhasselt.be>;
>>> > r-sig-geo at r-project.org
>>> > Subject: RE: [R-sig-Geo] Translate a net-cdf file with different years
>>> to
>>> > polygon regions.
>>> >
>>> > Hi Janka,
>>> >
>>> > I think you can simplify your code a lot by opening your nc file
>>> directly
>>> > using the `raster` package rather than messing with `nc_open` calls.
>>> >
>>> > ```
>>> > ncin <- raster::raster(paste(ncname, ".nc", sep = ""))
>>> > ```
>>> >
>>> > Then you might use `raster::extract()` to pull the values associated
>>> with
>>> > your polygons. Also, I would recommend posting a link to a gist (
>>> > https://gist.github.com/) rather than pasting such a long script into
>>> > your email.
>>> >
>>> > Joe
>>> >
>>> > -----Original Message-----
>>> > From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
>>> > Janka VANSCHOENWINKEL
>>> > Sent: Tuesday, April 19, 2016 4:45 AM
>>> > To: r-sig-geo at r-project.org
>>> > Subject: [R-sig-Geo] Translate a net-cdf file with different years to
>>> > polygon regions.
>>> >
>>> > *DATA*
>>> >
>>> > I have a net-cdf file which has raster of 0.5 on 0.5 degrees. You can
>>> > easily download it here
>>> > <
>>> >
>>> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/
>>> > >
>>> > and
>>> > search for *cru_ts3.23.2001.2010.pet.dat.nc.gz *(it is also
>>> downloadable as
>>> > a dat.file if this is more handy to work with)
>>> >  (Or simply download the net-cdf file directly through:
>>> > cru_ts3.23.2001.2010.pet.dat.nc.gz
>>> > <
>>> >
>>> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/cru_ts3.23.2001.2010.pet.dat.nc.gz
>>> > >
>>> > ).
>>> >
>>> > I opened the file in ArcMap as well and found that the coordinate
>>> system
>>> > used is: GCS_WGS_1984. The net-cdf file contains monthly data from
>>> > 2001-2010
>>> >
>>> > Download from this website
>>> > <
>>> >
>>> http://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts
>>> > >
>>> > the
>>> > following ZIP file: *NUTS_2006_60M_SH.zip* and save all the
>>> > *NUTS_RG_60M_2006
>>> > files *in a folder where you can easily find the back. In what follows
>>> I
>>> > refer to this as "NUTS3".
>>> >
>>> > *WHAT I WANT*
>>> >
>>> > - I want to add the information from the raster files to the NUTS3
>>> > shapefile (which are polygons and no rasters) in order to obtain a
>>> table
>>> > per nuts3 region for each monthtly variable.
>>> >
>>> >
>>> > *WHERE I AM STUCK*
>>> >
>>> > The file appears to be very difficult to work with in ArcGis. Also, I
>>> will
>>> > have to repeat this process a number of times for different variables.
>>> So I
>>> > would like to have one code in R that I can run.
>>> >
>>> > I have a number of separate codes to do the following:
>>> > - translate the net-cdf file to a cvs file with longitude and latitude
>>> as
>>> > identifiers (see below under 1)
>>> > - translate a cvs file with accompanying empty raster file to nuts3
>>> > regions. (this is a code that I have used before when I had a cvs file
>>> and
>>> > a raster). (see below under 2).
>>> >
>>> > However, I don't have a raster file now. Well, technically I do (since
>>> the
>>> > net-ncf file is a raster) but I don't know how to use it in this
>>> format.
>>> >
>>> > Can somebody help me to link the codes below or suggest a different
>>> code to
>>> > obtain what I want?
>>> >
>>> > Thanks a lot!
>>> >
>>> > Janka
>>> >
>>> >
>>> >
>>> >
>>> > *1) With the following code, I can make a cvs file and extract all the
>>> data
>>> > in table format.*
>>> >
>>> > library(fields)
>>> > library(chron)
>>> >
>>> > library(ncdf4)
>>> > ncname<-"cru_ts3.22.2001.2010.pet.dat"
>>> > ncname<-"cru_ts3.23.1991.2000.pet.dat"
>>> > ncfname <- paste(ncname, ".nc", sep = "")
>>> > dname <- "pet"
>>> > ncin <- nc_open(ncfname)
>>> > print(ncin)
>>> >
>>> > lon <- ncvar_get(ncin, "lon")
>>> > nlon <- dim(lon)
>>> > head(lon)
>>> >
>>> > lat <- ncvar_get(ncin, "lat", verbose = F)
>>> > nlat <- dim(lat)
>>> > head(lat)
>>> >
>>> > print(c(nlon, nlat))
>>> >
>>> >
>>> > t <- ncvar_get(ncin, "time")
>>> > tunits <- ncatt_get(ncin, "time", "units")
>>> > nt <- dim(t)
>>> >
>>> > tmp.array <- ncvar_get(ncin, dname)
>>> > dlname <- ncatt_get(ncin, dname, "long_name")
>>> > dunits <- ncatt_get(ncin, dname, "units")
>>> > fillvalue <- ncatt_get(ncin, dname, "_FillValue")
>>> > dim(tmp.array)
>>> >
>>> > title <- ncatt_get(ncin, 0, "title")
>>> > institution <- ncatt_get(ncin, 0, "institution")
>>> > datasource <- ncatt_get(ncin, 0, "source")
>>> > references <- ncatt_get(ncin, 0, "references")
>>> > history <- ncatt_get(ncin, 0, "history")
>>> > Conventions <- ncatt_get(ncin, 0, "Conventions")
>>> >
>>> >
>>> >
>>> > nc_close(ncin)
>>> >
>>> > # split the time units string into fields
>>> > tustr <- strsplit(tunits$value, " ")
>>> > tdstr <- strsplit(unlist(tustr)[3], "-")
>>> > tmonth = as.integer(unlist(tdstr)[2])
>>> > tday = as.integer(unlist(tdstr)[3])
>>> > tyear = as.integer(unlist(tdstr)[1])
>>> > chron(t, origin = c(tmonth, tday, tyear))
>>> >
>>> >
>>> >
>>> > tmp.array[tmp.array == fillvalue$value] <- NA
>>> >
>>> > length(na.omit(as.vector(tmp.array[, , 1])))
>>> >
>>> > m <- 1
>>> > tmp.slice <- tmp.array[, , m]
>>> > library(RColorBrewer)
>>> > image(lon, lat, tmp.slice, col = rev(brewer.pal(10, "RdBu")))
>>> >
>>> > grid <- expand.grid(lon = lon, lat = lat)
>>> > cutpts <- c(-50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50)
>>> > levelplot(tmp.slice ~ lon * lat, data = grid, at = cutpts, cuts = 11,
>>> > pretty = T,
>>> >           col.regions = (rev(brewer.pal(10, "RdBu"))))
>>> >
>>> >
>>> > lonlat <- expand.grid(lon, lat)
>>> > tmp.vec <- as.vector(tmp.slice)
>>> > length(tmp.vec)
>>> >
>>> > tmp.df01 <- data.frame(cbind(lonlat, tmp.vec))
>>> > names(tmp.df01) <- c("lon", "lat", paste(dname, as.character(m), sep =
>>> > "_"))
>>> > head(na.omit(tmp.df01), 20)
>>> >
>>> > csvfile <- "cru_tmp_1.csv"
>>> > write.table(na.omit(tmp.df01), csvfile, row.names = FALSE, sep = ",")
>>> >
>>> >
>>> > tmp.vec.long <- as.vector(tmp.array)
>>> > length(tmp.vec.long)
>>> >
>>> > tmp.mat <- matrix(tmp.vec.long, nrow = nlon * nlat, ncol = nt)
>>> > dim(tmp.mat)
>>> >
>>> > head(na.omit(tmp.mat))
>>> >
>>> > lonlat <- expand.grid(lon, lat)
>>> > tmp.df02 <- data.frame(cbind(lonlat, tmp.mat))
>>> >
>>> > names(tmp.df02) <- c("lon","lat","pet_jan_2001",
>>> >                      "pet_feb_2001",
>>> >                      "pet_mar_2001",
>>> >                      "pet_apr_2001",
>>> >                      "pet_may_2001",
>>> >                      "pet_jun_2001",
>>> >                      "pet_jul_2001",
>>> >                      "pet_aug_2001",
>>> >                      "pet_sep_2001",
>>> >                      "pet_oct_2001",
>>> >                      "pet_nov_2001",
>>> >                      "pet_dec_2001",
>>> >                      "pet_jan_2002",
>>> >                      "pet_feb_2002",
>>> >                      "pet_mar_2002",
>>> >                      "pet_apr_2002",
>>> >                      "pet_may_2002",
>>> >                      "pet_jun_2002",
>>> >                      "pet_jul_2002",
>>> >                      "pet_aug_2002",
>>> >                      "pet_sep_2002",
>>> >                      "pet_oct_2002",
>>> >                      "pet_nov_2002",
>>> >                      "pet_dec_2002",
>>> >                      "pet_jan_2003",
>>> >                      "pet_feb_2003",
>>> >                      "pet_mar_2003",
>>> >                      "pet_apr_2003",
>>> >                      "pet_may_2003",
>>> >                      "pet_jun_2003",
>>> >                      "pet_jul_2003",
>>> >                      "pet_aug_2003",
>>> >                      "pet_sep_2003",
>>> >                      "pet_oct_2003",
>>> >                      "pet_nov_2003",
>>> >                      "pet_dec_2003",
>>> >                      "pet_jan_2004",
>>> >                      "pet_feb_2004",
>>> >                      "pet_mar_2004",
>>> >                      "pet_apr_2004",
>>> >                      "pet_may_2004",
>>> >                      "pet_jun_2004",
>>> >                      "pet_jul_2004",
>>> >                      "pet_aug_2004",
>>> >                      "pet_sep_2004",
>>> >                      "pet_oct_2004",
>>> >                      "pet_nov_2004",
>>> >                      "pet_dec_2004",
>>> >                      "pet_jan_2005",
>>> >                      "pet_feb_2005",
>>> >                      "pet_mar_2005",
>>> >                      "pet_apr_2005",
>>> >                      "pet_may_2005",
>>> >                      "pet_jun_2005",
>>> >                      "pet_jul_2005",
>>> >                      "pet_aug_2005",
>>> >                      "pet_sep_2005",
>>> >                      "pet_oct_2005",
>>> >                      "pet_nov_2005",
>>> >                      "pet_dec_2005",
>>> >                      "pet_jan_2006",
>>> >                      "pet_feb_2006",
>>> >                      "pet_mar_2006",
>>> >                      "pet_apr_2006",
>>> >                      "pet_may_2006",
>>> >                      "pet_jun_2006",
>>> >                      "pet_jul_2006",
>>> >                      "pet_aug_2006",
>>> >                      "pet_sep_2006",
>>> >                      "pet_oct_2006",
>>> >                      "pet_nov_2006",
>>> >                      "pet_dec_2006",
>>> >                      "pet_jan_2007",
>>> >                      "pet_feb_2007",
>>> >                      "pet_mar_2007",
>>> >                      "pet_apr_2007",
>>> >                      "pet_may_2007",
>>> >                      "pet_jun_2007",
>>> >                      "pet_jul_2007",
>>> >                      "pet_aug_2007",
>>> >                      "pet_sep_2007",
>>> >                      "pet_oct_2007",
>>> >                      "pet_nov_2007",
>>> >                      "pet_dec_2007",
>>> >                      "pet_jan_2008",
>>> >                      "pet_feb_2008",
>>> >                      "pet_mar_2008",
>>> >                      "pet_apr_2008",
>>> >                      "pet_may_2008",
>>> >                      "pet_jun_2008",
>>> >                      "pet_jul_2008",
>>> >                      "pet_aug_2008",
>>> >                      "pet_sep_2008",
>>> >                      "pet_oct_2008",
>>> >                      "pet_nov_2008",
>>> >                      "pet_dec_2008",
>>> >                      "pet_jan_2009",
>>> >                      "pet_feb_2009",
>>> >                      "pet_mar_2009",
>>> >                      "pet_apr_2009",
>>> >                      "pet_may_2009",
>>> >                      "pet_jun_2009",
>>> >                      "pet_jul_2009",
>>> >                      "pet_aug_2009",
>>> >                      "pet_sep_2009",
>>> >                      "pet_oct_2009",
>>> >                      "pet_nov_2009",
>>> >                      "pet_dec_2009",
>>> >                      "pet_jan_2010",
>>> >                      "pet_feb_2010",
>>> >                      "pet_mar_2010",
>>> >                      "pet_apr_2010",
>>> >                      "pet_may_2010",
>>> >                      "pet_jun_2010",
>>> >                      "pet_jul_2010",
>>> >                      "pet_aug_2010",
>>> >                      "pet_sep_2010",
>>> >                      "pet_oct_2010",
>>> >                      "pet_nov_2010",
>>> >                      "pet_dec_2010")
>>> >
>>> >
>>> > options(width = 110)
>>> > head(na.omit(tmp.df02, 20))
>>> >
>>> > dim(na.omit(tmp.df02))
>>> >
>>> > csvfile <- "cru_tmp_2.csv"
>>> > write.table(na.omit(tmp.df02), csvfile, row.names = FALSE, sep = ",")
>>> >
>>> >
>>> >
>>> > *2) translate a cvs-file with accompanying raster file to polygon
>>> regions.*
>>> >
>>> > The  "filename.txt" file should contain the variables: lon, latitude,
>>> and
>>> > all the monthly_yearly variables extracted from point 1 above.
>>> >
>>> > The grid shapefile (*grid_025dd.shp*) can be found through the
>>> following
>>> > link but it is only an example and not the correct grid for the problem
>>> > above :
>>> >
>>> >
>>> https://drive.google.com/folderview?id=0By9u5m3kxn9yfjZtdFZLcW82SWpzT1VwZXE1a3FtRGtSdEl1c1NvY205TGpack9xSFc2T2s&usp=sharing
>>> >
>>> > # upload data
>>> > mydata<-read.table("filename.txt", header=TRUE,sep=",",dec=".")
>>> >
>>> >
>>> > # upload empty raster
>>> > library(rgdal)
>>> > # 40 seconds
>>> > grid <- readOGR(".", layer = "grid_025dd")
>>> >
>>> >
>>> > # concatenate data in R
>>> > # 2 seconds
>>> > mydata$lonlat<-do.call(paste, c(mydata[c("lon", "lat")], sep=""))
>>> > grid at data$lonlat<-do.call(paste, c(grid at data[c("LONGITUDE",
>>> "LATITUDE")],
>>> > sep=""))
>>> >
>>> > # use common variable lonlat to merge data in raster
>>> >
>>> > ###### prepare shapefile #####
>>> > library(rgdal)        ## Load geographic info
>>> > library(maps)         ## Projections
>>> > library(maptools)     ## Data management
>>> > #library(sm)           ## Data management
>>> > library(spdep)        ## Spatial autocorrelation
>>> > library(gstat)        ## Geostatistics
>>> > library(splancs)      ## Kernel Density
>>> > library(spatstat)     ## Geostatistics
>>> > library(pgirmess)     ## Spatial autocorrelation
>>> > library(RColorBrewer) ## Visualization
>>> > library(classInt)     ## Class intervals
>>> > library(spgwr)        ## GWR
>>> >
>>> > # Match polygons with data
>>> > idx <- match(grid$lonlat, mydata$lonlat)
>>> > # Places without information
>>> > idxNA <- which(is.na(idx))
>>> > # Information to be added to the SpatialPolygons object
>>> > dat2add <- mydata[idx, ]
>>> > # spCbind uses row names to match polygons with data
>>> > # First, extract polygon IDs
>>> > IDs <- sapply(grid at polygons, function(x)x at ID)
>>> > # and join with the SpatialPolygons
>>> > row.names(dat2add) <- IDs
>>> > datPols <- spCbind(grid, dat2add)
>>> > # Drop those places without information
>>> > datPols <- datPols[-idxNA, ]
>>> > # write new shapefile
>>> > # 7 seconds
>>> > writeOGR(datPols, dsn = ".", layer ='sm2000eu28', driver = 'ESRI
>>> > Shapefile')
>>> > # read new shapefile
>>> > # 51 seconds
>>> > data <- readOGR(".", layer="sm2000eu28")
>>> >
>>> > ############################
>>> > # intersect nuts with grid #
>>> > ############################
>>> >
>>> > library(rgdal)
>>> > nuts <- readOGR(".", layer = "NUTS_RG_60M_2006")
>>> >
>>> > library(rgeos)
>>> > proj4string(data) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
>>> >
>>> >
>>> > #
>>> > grid <- data
>>> > grid at data$lonlat <- NULL
>>> > grid at data$lonlat_1 <- NULL
>>> > grid at data$ID <- NULL
>>> > grid at data$lat <- NULL
>>> > grid at data$lon <- NULL
>>> > grid at data$ELEVATION <- NULL
>>> > grid at data$DAYS_RAIN_ <- NULL
>>> >
>>> >
>>> > # First find out which grid cells intersect your NUTS polygons
>>> > grid_nuts <- gIntersects(grid,nuts,byid = TRUE)
>>> >
>>> > # use the apply() function to calculate the mean, min, and max of your
>>> > value.
>>> > # The loop makes
>>> >
>>> > for(i in names(grid at data)){
>>> >   nuts at data[[paste(i, 'average_value', sep="_")]] <-
>>> > apply(grid_nuts,1,function(x) mean(grid at data[[i]][x]))
>>> >   nuts at data[[paste(i, 'min_value', sep="_")]] <-
>>> > apply(grid_nuts,1,function(x) min(grid at data[[i]][x]))
>>> >   nuts at data[[paste(i, 'max_value', sep="_")]] <-
>>> > apply(grid_nuts,1,function(x) max(grid at data[[i]][x]))
>>> > }
>>> >
>>> > write.table(nuts at data, "nuts_sm2000eu28_unweighted.txt", sep="\t")
>>> >
>>> >         [[alternative HTML version deleted]]
>>> >
>>> > _______________________________________________
>>> > R-sig-Geo mailing list
>>> > R-sig-Geo at r-project.org
>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>> >
>>> >
>>> > We value your opinion. Please take a few minutes to share your
>>> comments on
>>> > the service you received from the District by clicking on this link<
>>> >
>>> http://my.sfwmd.gov/portal/page/portal/pg_grp_surveysystem/survey%20ext?pid=1653
>>> > >.
>>> >
>>>
>>>
>>>
>>> --
>>>
>>> [image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
>>> *Doctoraatsbursaal - PhD *
>>> Milieueconomie - Environmental economics
>>>
>>> T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40
>>>
>>> www.uhasselt.be/eec
>>>
>>> Universiteit Hasselt | Campus Diepenbeek
>>> Agoralaan Gebouw D | B-3590 Diepenbeek
>>> Kantoor F11
>>>
>>> Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt
>>>
>>>
>>> [image: Music For Life]  Maak van UHasselt de #warmsteunief |
>>> www.uhasselt.be/musicforlife
>>>
>>>
>>> P Please consider the environment before printing this e-mail
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>> --
>> Dr. Michael Sumner
>> Software and Database Engineer
>> Australian Antarctic Division
>> 203 Channel Highway
>> Kingston Tasmania 7050 Australia
>>
>>
>
>
> --
>
> [image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
> *Doctoraatsbursaal - PhD *
> Milieueconomie - Environmental economics
>
> T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40
>
> www.uhasselt.be/eec
>
> Universiteit Hasselt | Campus Diepenbeek
> Agoralaan Gebouw D | B-3590 Diepenbeek
> Kantoor F11
>
> Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt
>
>
> [image: Music For Life]  Maak van UHasselt de #warmsteunief |
> www.uhasselt.be/musicforlife
>
>
> P Please consider the environment before printing this e-mail
>
>


-- 

[image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
*Doctoraatsbursaal - PhD *
Milieueconomie - Environmental economics

T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40

www.uhasselt.be/eec

Universiteit Hasselt | Campus Diepenbeek
Agoralaan Gebouw D | B-3590 Diepenbeek
Kantoor F11

Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt


[image: Music For Life]  Maak van UHasselt de #warmsteunief |
www.uhasselt.be/musicforlife


P Please consider the environment before printing this e-mail

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Thu Jun  9 16:51:46 2016
From: mdsumner at gmail.com (Michael Sumner)
Date: Thu, 09 Jun 2016 14:51:46 +0000
Subject: [R-sig-Geo] Translate a net-cdf file with different years to
 polygon regions.
In-Reply-To: <CAHymut+2SzP75wbGqwwvqpKbFhV9wsU-G5yF3JD22er0WD=9sA@mail.gmail.com>
References: <CAHymutKcpgJoWExtVVLFsSJ8VG23-b1jumMrMivMk2ZO5Lu6Jg@mail.gmail.com>
	<D51374C4B889BC47B3C5286047C86DA1A02C3A20@whqembx03p.ad.sfwmd.gov>
	<D51374C4B889BC47B3C5286047C86DA1A02C3A70@whqembx03p.ad.sfwmd.gov>
	<CAHymutLi97xoegT8qyYD3Tz7+LwFt9DfGJoMH81dSO3KL9yh4A@mail.gmail.com>
	<CAAcGz9-x7E6B_BHLcCdE5A1t28=tqGR9j4oFF=SUvJJ-BYGWhA@mail.gmail.com>
	<CAHymutLkmHER46BP40Nup8WT52FTuDxuJbnwStzCa+_jx_7bnA@mail.gmail.com>
	<CAHymut+2SzP75wbGqwwvqpKbFhV9wsU-G5yF3JD22er0WD=9sA@mail.gmail.com>
Message-ID: <CAAcGz9-5H8Xu5LXqDqsmUtUsNWGFvxe158ONYhVGZgz2tMGEVA@mail.gmail.com>

On Thu, 9 Jun 2016 at 23:32 Janka VANSCHOENWINKEL <
janka.vanschoenwinkel at uhasselt.be> wrote:

> Dear Michael, Joseph and other colleagues,
>
> About a month and a half ago, I asked the question below (I summarized it
> shorter in this email). I received some hints on how to do it in R, but
> given the fact that I am not that experienced with netcdf files in R, I
> didn't manage to make it work. I continued to work with ArcGis, but even
> there it is not working.
>
> So I am giving it one more try here: could somebody help me with the
> problem below?
>
>
> Download from this website (
> http://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts)
> the following ZIP file: NUTS_2006_60M_SH.zip and save all
> theNUTS_RG_60M_2006 files. In what follows I refer to this as "NUTS3
> regions".
>
> Download from this website (
> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/)
>  the following nc-file: cru_ts3.23.1991.2000.pet.dat.nc.gz This is a raster
> with data all over the world.
>
> The netcdf file has data on one variable (pet) for multiple months per
> year (from 1991 to 2000). For each month, I would like to translate all
> these values into averages per nuts3 region.
>
> I can open the file in R and make a cvs file from it (see code in the
> original email below), but I do not manage to overlap each monthly value
> with the nuts3 regions.
>
> Results can be stored in a table by month and by nuts3, or in a shapefile.
> Either way is fine!
>
> Does somebody have any experience with this? Any concrete help is highly
> appreciated! I have really no experience with netcdf files. I only have
> trouble with them.. ;-)
>

You are not alone, NetCDF is a very general format and that generality gets
a lot of exercise.  This one is pretty easy as far as I can see, the nc
file seems to conform to a basic gridded raster in longitude-latitude, no
complications.

 Lots more stuff you can try, but this should get you over the first
hurdle. Please pay close attention to how "high-level" most of this is, you
really will benefit from learning how to drive raster and sp without
getting to untidy in the trenches. Raster and sp were not designed and
built together, and have wildly different assumptions and working
philosophies but for the most part they play well together - it just means
you have to learn another two languages (at least) to use them well. Please
read the doc for each function and explore what the options do, there are
gotchas at every step and I've only done the most naive things here.

Finally, this is a pretty big data set - the polygons are quite detailed
and that makes plotting fairly tedious - so use subsetting to hone in on an
area of interest, etc.  D Explore these tools with simpler data before
applying your main work to them.

Make sure to upgrade to R 3.3.0 and get the latest raster package from some
time last week. There were some important bug fixes released.

HTH


##
----------------------------------------------------------------------------
## boring stuff to get the data, skip if you have "nutsdsn" and "nutslayer"
and "ncfile"
nuts <- "
http://ec.europa.eu/eurostat/cache/GISCO/geodatafiles/NUTS_2006_60M_SH.zip"
download.file(nuts, basename(nuts), mode = "wb")
nutsfile <- basename(nuts)
unzip(nutsfile)
#nutslayer <- "NUTS_BN_60M_2006"  ## boundaries
#nutslayer <- "NUTS_JOIN_LI_2006"
#nutslayer <- "NUTS_LB_2006" ## points

rf <- "
https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/cru_ts3.23.1991.2000.pet.dat.nc.gz
"
download.file(rf, basename(rf), mode = "wb")
rfile <- basename(rf)
system(sprintf("gunzip %s",  rfile))




## we have the files
nutsdsn <- "NUTS_2006_60M_SH/shape/data"
nutslayer <- "NUTS_RG_60M_2006" ## bingo, polygons
ncfile <- "cru_ts3.23.1991.2000.pet.dat.nc"

library(rgdal)  ## to read shapefile
admin <- readOGR(nutsdsn, nutslayer)

library(raster)
## looks fine
plot(raster(ncfile))  ## this gets the first layer (or "band")
plot(admin, add = TRUE)

admin
# class       : SpatialPolygonsDataFrame
# features    : 1927
# extent      : -61.80605, 55.83498, -21.37656, 71.15701  (xmin, xmax,
ymin, ymax)
# coord. ref. : +proj=longlat +ellps=GRS80 +no_defs
# variables   : 7
# names       : OBJECTID, NUTS_ID, STAT_LEVL_, AREA, LEN,  Shape_Leng,
Shape_Area
# min values  :        1,      AT,          0,    0,   0,   0.1351160,
1.002343e+00
# max values  :     1927,   UKN05,          3,    0,   0, 312.0817173,
9.997536e-02

## get every layer (without actually loading necessarily)
(cru_ts3 <- brick(ncfile))
# class       : RasterBrick
# dimensions  : 360, 720, 259200, 120  (nrow, ncol, ncell, nlayers)
# resolution  : 0.5, 0.5  (x, y)
# extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
# coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
# data source : ...cru_ts3.23.1991.2000.pet.dat.nc
# names       : X1991.01.16, X1991.02.15, X1991.03.16, X1991.04.16,
X1991.05.16, X1991.06.16, X1991.07.16, X1991.08.16, X1991.09.16,
X1991.10.16, X1991.11.16, X1991.12.16, X1992.01.16, X1992.02.15,
X1992.03.16, ...
# Date        : 1991-01-16, 2000-12-16 (min, max)
# varname     : pet

## extract everything, collapse to mean values

## note that this will automatically transform to the raster
## and warn, you should explore what this does (the plot above does not for
example)
ex <- extract(cru_ts3, admin, fun = mean)
dim(ex)
## [1] 1927  120

## tidy up
d <- as.data.frame(ex)
row.names(d) <- row.names(admin)
pet <- SpatialPolygonsDataFrame(geometry(admin), d)

## try some plots, slow
spplot(pet[, c(1, 10)])






> 2016-04-29 15:03 GMT+02:00 Janka VANSCHOENWINKEL <
> janka.vanschoenwinkel at uhasselt.be>:
>
>> Thanks Mike,
>>
>> I am running your suggestion but it always get stuck. I also don't know
>> how I have to indicate which variable I want to have given the fact that
>> there is only one variable, but it is measured for different time periods.
>> So for instance, how to I tell R that I want to have "pet from april in
>> 1995"?
>>
>
>> In mean time, I want to make clear that there are multiple points of the
>> same period per nuts3-polygon. So when I was speaking about the average, I
>> meant the average of all those points from the same periods, that are
>> located in the same polygon. So I do want to have information per period.
>>
>> Thanks,
>>
>> Janka
>>
>
>> 2016-04-29 14:42 GMT+02:00 Michael Sumner <mdsumner at gmail.com>:
>>
>>>
>>>
>>> On Fri, 29 Apr 2016 at 22:23 Janka VANSCHOENWINKEL <
>>> janka.vanschoenwinkel at uhasselt.be> wrote:
>>>
>>>> Hi Joseph,
>>>>
>>>> Thank you very much for the hint.
>>>>
>>>> If I may ask you, could you please help me a little bit further. I am
>>>> not
>>>> familiar with these packages and I do not see how to overlap the point
>>>> data
>>>> or the netcdf data with the nuts3 polygons.
>>>>
>>>> Basically, I have too options:
>>>>
>>>> 1) use the cvs-points (with lon, lat, and the data variable) to see
>>>> which
>>>> points are located in which nuts3 regions and then save these results.
>>>> (but
>>>> then my previous codes still applies)
>>>>
>>>> 2) use the original netcdf file and overlay it with the nuts3 polygon
>>>> shapefile (this was your hint, but I don't know how to start with this).
>>>>
>>>>
>>> Something like this will do it:
>>>
>>> library(raster)
>>> require(rgdal)
>>> require(ncdf4)
>>>
>>> st <- stack("thencfile.nc")  ## might need varname = "sst" or similar
>>>
>>> poly <- shapefile("theshpfile.shp")
>>>
>>> extract(st, poly, fun = "mean")  ## and maybe na.rm = TRUE
>>>
>>> If you don't want the mean, you can just leave out the fun argument and
>>> you'll get all the pixel values for every time step in a big list, which
>>> may not be obviously helpful but it's all useable with generic R.
>>>
>>> I can't quite bring myself to get your files and try it, so please try
>>> and report details.
>>>
>>> Cheers, Mike.
>>>
>>>
>>>> Thank you very much for your answer!
>>>>
>>>>
>>>>
>>>> 2016-04-19 14:42 GMT+02:00 Stachelek, Joseph <jstachel at sfwmd.gov>:
>>>>
>>>> > Since your nc file contains multiple layers, you will want to use
>>>> > `raster::stack()` rather than `raster::raster()`.
>>>> >
>>>> >
>>>> > -----Original Message-----
>>>> > From: Stachelek, Joseph
>>>> > Sent: Tuesday, April 19, 2016 8:23 AM
>>>> > To: 'Janka VANSCHOENWINKEL' <janka.vanschoenwinkel at uhasselt.be>;
>>>> > r-sig-geo at r-project.org
>>>> > Subject: RE: [R-sig-Geo] Translate a net-cdf file with different
>>>> years to
>>>> > polygon regions.
>>>> >
>>>> > Hi Janka,
>>>> >
>>>> > I think you can simplify your code a lot by opening your nc file
>>>> directly
>>>> > using the `raster` package rather than messing with `nc_open` calls.
>>>> >
>>>> > ```
>>>> > ncin <- raster::raster(paste(ncname, ".nc", sep = ""))
>>>> > ```
>>>> >
>>>> > Then you might use `raster::extract()` to pull the values associated
>>>> with
>>>> > your polygons. Also, I would recommend posting a link to a gist (
>>>> > https://gist.github.com/) rather than pasting such a long script into
>>>> > your email.
>>>> >
>>>> > Joe
>>>> >
>>>> > -----Original Message-----
>>>> > From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf Of
>>>> > Janka VANSCHOENWINKEL
>>>> > Sent: Tuesday, April 19, 2016 4:45 AM
>>>> > To: r-sig-geo at r-project.org
>>>> > Subject: [R-sig-Geo] Translate a net-cdf file with different years to
>>>> > polygon regions.
>>>> >
>>>> > *DATA*
>>>> >
>>>> > I have a net-cdf file which has raster of 0.5 on 0.5 degrees. You can
>>>> > easily download it here
>>>> > <
>>>> >
>>>> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/
>>>> > >
>>>> > and
>>>> > search for *cru_ts3.23.2001.2010.pet.dat.nc.gz *(it is also
>>>> downloadable as
>>>> > a dat.file if this is more handy to work with)
>>>> >  (Or simply download the net-cdf file directly through:
>>>> > cru_ts3.23.2001.2010.pet.dat.nc.gz
>>>> > <
>>>> >
>>>> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/cru_ts3.23.2001.2010.pet.dat.nc.gz
>>>> > >
>>>> > ).
>>>> >
>>>> > I opened the file in ArcMap as well and found that the coordinate
>>>> system
>>>> > used is: GCS_WGS_1984. The net-cdf file contains monthly data from
>>>> > 2001-2010
>>>> >
>>>> > Download from this website
>>>> > <
>>>> >
>>>> http://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts
>>>> > >
>>>> > the
>>>> > following ZIP file: *NUTS_2006_60M_SH.zip* and save all the
>>>> > *NUTS_RG_60M_2006
>>>> > files *in a folder where you can easily find the back. In what
>>>> follows I
>>>> > refer to this as "NUTS3".
>>>> >
>>>> > *WHAT I WANT*
>>>> >
>>>> > - I want to add the information from the raster files to the NUTS3
>>>> > shapefile (which are polygons and no rasters) in order to obtain a
>>>> table
>>>> > per nuts3 region for each monthtly variable.
>>>> >
>>>> >
>>>> > *WHERE I AM STUCK*
>>>> >
>>>> > The file appears to be very difficult to work with in ArcGis. Also, I
>>>> will
>>>> > have to repeat this process a number of times for different
>>>> variables. So I
>>>> > would like to have one code in R that I can run.
>>>> >
>>>> > I have a number of separate codes to do the following:
>>>> > - translate the net-cdf file to a cvs file with longitude and
>>>> latitude as
>>>> > identifiers (see below under 1)
>>>> > - translate a cvs file with accompanying empty raster file to nuts3
>>>> > regions. (this is a code that I have used before when I had a cvs
>>>> file and
>>>> > a raster). (see below under 2).
>>>> >
>>>> > However, I don't have a raster file now. Well, technically I do
>>>> (since the
>>>> > net-ncf file is a raster) but I don't know how to use it in this
>>>> format.
>>>> >
>>>> > Can somebody help me to link the codes below or suggest a different
>>>> code to
>>>> > obtain what I want?
>>>> >
>>>> > Thanks a lot!
>>>> >
>>>> > Janka
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > *1) With the following code, I can make a cvs file and extract all
>>>> the data
>>>> > in table format.*
>>>> >
>>>> > library(fields)
>>>> > library(chron)
>>>> >
>>>> > library(ncdf4)
>>>> > ncname<-"cru_ts3.22.2001.2010.pet.dat"
>>>> > ncname<-"cru_ts3.23.1991.2000.pet.dat"
>>>> > ncfname <- paste(ncname, ".nc", sep = "")
>>>> > dname <- "pet"
>>>> > ncin <- nc_open(ncfname)
>>>> > print(ncin)
>>>> >
>>>> > lon <- ncvar_get(ncin, "lon")
>>>> > nlon <- dim(lon)
>>>> > head(lon)
>>>> >
>>>> > lat <- ncvar_get(ncin, "lat", verbose = F)
>>>> > nlat <- dim(lat)
>>>> > head(lat)
>>>> >
>>>> > print(c(nlon, nlat))
>>>> >
>>>> >
>>>> > t <- ncvar_get(ncin, "time")
>>>> > tunits <- ncatt_get(ncin, "time", "units")
>>>> > nt <- dim(t)
>>>> >
>>>> > tmp.array <- ncvar_get(ncin, dname)
>>>> > dlname <- ncatt_get(ncin, dname, "long_name")
>>>> > dunits <- ncatt_get(ncin, dname, "units")
>>>> > fillvalue <- ncatt_get(ncin, dname, "_FillValue")
>>>> > dim(tmp.array)
>>>> >
>>>> > title <- ncatt_get(ncin, 0, "title")
>>>> > institution <- ncatt_get(ncin, 0, "institution")
>>>> > datasource <- ncatt_get(ncin, 0, "source")
>>>> > references <- ncatt_get(ncin, 0, "references")
>>>> > history <- ncatt_get(ncin, 0, "history")
>>>> > Conventions <- ncatt_get(ncin, 0, "Conventions")
>>>> >
>>>> >
>>>> >
>>>> > nc_close(ncin)
>>>> >
>>>> > # split the time units string into fields
>>>> > tustr <- strsplit(tunits$value, " ")
>>>> > tdstr <- strsplit(unlist(tustr)[3], "-")
>>>> > tmonth = as.integer(unlist(tdstr)[2])
>>>> > tday = as.integer(unlist(tdstr)[3])
>>>> > tyear = as.integer(unlist(tdstr)[1])
>>>> > chron(t, origin = c(tmonth, tday, tyear))
>>>> >
>>>> >
>>>> >
>>>> > tmp.array[tmp.array == fillvalue$value] <- NA
>>>> >
>>>> > length(na.omit(as.vector(tmp.array[, , 1])))
>>>> >
>>>> > m <- 1
>>>> > tmp.slice <- tmp.array[, , m]
>>>> > library(RColorBrewer)
>>>> > image(lon, lat, tmp.slice, col = rev(brewer.pal(10, "RdBu")))
>>>> >
>>>> > grid <- expand.grid(lon = lon, lat = lat)
>>>> > cutpts <- c(-50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50)
>>>> > levelplot(tmp.slice ~ lon * lat, data = grid, at = cutpts, cuts = 11,
>>>> > pretty = T,
>>>> >           col.regions = (rev(brewer.pal(10, "RdBu"))))
>>>> >
>>>> >
>>>> > lonlat <- expand.grid(lon, lat)
>>>> > tmp.vec <- as.vector(tmp.slice)
>>>> > length(tmp.vec)
>>>> >
>>>> > tmp.df01 <- data.frame(cbind(lonlat, tmp.vec))
>>>> > names(tmp.df01) <- c("lon", "lat", paste(dname, as.character(m), sep =
>>>> > "_"))
>>>> > head(na.omit(tmp.df01), 20)
>>>> >
>>>> > csvfile <- "cru_tmp_1.csv"
>>>> > write.table(na.omit(tmp.df01), csvfile, row.names = FALSE, sep = ",")
>>>> >
>>>> >
>>>> > tmp.vec.long <- as.vector(tmp.array)
>>>> > length(tmp.vec.long)
>>>> >
>>>> > tmp.mat <- matrix(tmp.vec.long, nrow = nlon * nlat, ncol = nt)
>>>> > dim(tmp.mat)
>>>> >
>>>> > head(na.omit(tmp.mat))
>>>> >
>>>> > lonlat <- expand.grid(lon, lat)
>>>> > tmp.df02 <- data.frame(cbind(lonlat, tmp.mat))
>>>> >
>>>> > names(tmp.df02) <- c("lon","lat","pet_jan_2001",
>>>> >                      "pet_feb_2001",
>>>> >                      "pet_mar_2001",
>>>> >                      "pet_apr_2001",
>>>> >                      "pet_may_2001",
>>>> >                      "pet_jun_2001",
>>>> >                      "pet_jul_2001",
>>>> >                      "pet_aug_2001",
>>>> >                      "pet_sep_2001",
>>>> >                      "pet_oct_2001",
>>>> >                      "pet_nov_2001",
>>>> >                      "pet_dec_2001",
>>>> >                      "pet_jan_2002",
>>>> >                      "pet_feb_2002",
>>>> >                      "pet_mar_2002",
>>>> >                      "pet_apr_2002",
>>>> >                      "pet_may_2002",
>>>> >                      "pet_jun_2002",
>>>> >                      "pet_jul_2002",
>>>> >                      "pet_aug_2002",
>>>> >                      "pet_sep_2002",
>>>> >                      "pet_oct_2002",
>>>> >                      "pet_nov_2002",
>>>> >                      "pet_dec_2002",
>>>> >                      "pet_jan_2003",
>>>> >                      "pet_feb_2003",
>>>> >                      "pet_mar_2003",
>>>> >                      "pet_apr_2003",
>>>> >                      "pet_may_2003",
>>>> >                      "pet_jun_2003",
>>>> >                      "pet_jul_2003",
>>>> >                      "pet_aug_2003",
>>>> >                      "pet_sep_2003",
>>>> >                      "pet_oct_2003",
>>>> >                      "pet_nov_2003",
>>>> >                      "pet_dec_2003",
>>>> >                      "pet_jan_2004",
>>>> >                      "pet_feb_2004",
>>>> >                      "pet_mar_2004",
>>>> >                      "pet_apr_2004",
>>>> >                      "pet_may_2004",
>>>> >                      "pet_jun_2004",
>>>> >                      "pet_jul_2004",
>>>> >                      "pet_aug_2004",
>>>> >                      "pet_sep_2004",
>>>> >                      "pet_oct_2004",
>>>> >                      "pet_nov_2004",
>>>> >                      "pet_dec_2004",
>>>> >                      "pet_jan_2005",
>>>> >                      "pet_feb_2005",
>>>> >                      "pet_mar_2005",
>>>> >                      "pet_apr_2005",
>>>> >                      "pet_may_2005",
>>>> >                      "pet_jun_2005",
>>>> >                      "pet_jul_2005",
>>>> >                      "pet_aug_2005",
>>>> >                      "pet_sep_2005",
>>>> >                      "pet_oct_2005",
>>>> >                      "pet_nov_2005",
>>>> >                      "pet_dec_2005",
>>>> >                      "pet_jan_2006",
>>>> >                      "pet_feb_2006",
>>>> >                      "pet_mar_2006",
>>>> >                      "pet_apr_2006",
>>>> >                      "pet_may_2006",
>>>> >                      "pet_jun_2006",
>>>> >                      "pet_jul_2006",
>>>> >                      "pet_aug_2006",
>>>> >                      "pet_sep_2006",
>>>> >                      "pet_oct_2006",
>>>> >                      "pet_nov_2006",
>>>> >                      "pet_dec_2006",
>>>> >                      "pet_jan_2007",
>>>> >                      "pet_feb_2007",
>>>> >                      "pet_mar_2007",
>>>> >                      "pet_apr_2007",
>>>> >                      "pet_may_2007",
>>>> >                      "pet_jun_2007",
>>>> >                      "pet_jul_2007",
>>>> >                      "pet_aug_2007",
>>>> >                      "pet_sep_2007",
>>>> >                      "pet_oct_2007",
>>>> >                      "pet_nov_2007",
>>>> >                      "pet_dec_2007",
>>>> >                      "pet_jan_2008",
>>>> >                      "pet_feb_2008",
>>>> >                      "pet_mar_2008",
>>>> >                      "pet_apr_2008",
>>>> >                      "pet_may_2008",
>>>> >                      "pet_jun_2008",
>>>> >                      "pet_jul_2008",
>>>> >                      "pet_aug_2008",
>>>> >                      "pet_sep_2008",
>>>> >                      "pet_oct_2008",
>>>> >                      "pet_nov_2008",
>>>> >                      "pet_dec_2008",
>>>> >                      "pet_jan_2009",
>>>> >                      "pet_feb_2009",
>>>> >                      "pet_mar_2009",
>>>> >                      "pet_apr_2009",
>>>> >                      "pet_may_2009",
>>>> >                      "pet_jun_2009",
>>>> >                      "pet_jul_2009",
>>>> >                      "pet_aug_2009",
>>>> >                      "pet_sep_2009",
>>>> >                      "pet_oct_2009",
>>>> >                      "pet_nov_2009",
>>>> >                      "pet_dec_2009",
>>>> >                      "pet_jan_2010",
>>>> >                      "pet_feb_2010",
>>>> >                      "pet_mar_2010",
>>>> >                      "pet_apr_2010",
>>>> >                      "pet_may_2010",
>>>> >                      "pet_jun_2010",
>>>> >                      "pet_jul_2010",
>>>> >                      "pet_aug_2010",
>>>> >                      "pet_sep_2010",
>>>> >                      "pet_oct_2010",
>>>> >                      "pet_nov_2010",
>>>> >                      "pet_dec_2010")
>>>> >
>>>> >
>>>> > options(width = 110)
>>>> > head(na.omit(tmp.df02, 20))
>>>> >
>>>> > dim(na.omit(tmp.df02))
>>>> >
>>>> > csvfile <- "cru_tmp_2.csv"
>>>> > write.table(na.omit(tmp.df02), csvfile, row.names = FALSE, sep = ",")
>>>> >
>>>> >
>>>> >
>>>> > *2) translate a cvs-file with accompanying raster file to polygon
>>>> regions.*
>>>> >
>>>> > The  "filename.txt" file should contain the variables: lon, latitude,
>>>> and
>>>> > all the monthly_yearly variables extracted from point 1 above.
>>>> >
>>>> > The grid shapefile (*grid_025dd.shp*) can be found through the
>>>> following
>>>> > link but it is only an example and not the correct grid for the
>>>> problem
>>>> > above :
>>>> >
>>>> >
>>>> https://drive.google.com/folderview?id=0By9u5m3kxn9yfjZtdFZLcW82SWpzT1VwZXE1a3FtRGtSdEl1c1NvY205TGpack9xSFc2T2s&usp=sharing
>>>> >
>>>> > # upload data
>>>> > mydata<-read.table("filename.txt", header=TRUE,sep=",",dec=".")
>>>> >
>>>> >
>>>> > # upload empty raster
>>>> > library(rgdal)
>>>> > # 40 seconds
>>>> > grid <- readOGR(".", layer = "grid_025dd")
>>>> >
>>>> >
>>>> > # concatenate data in R
>>>> > # 2 seconds
>>>> > mydata$lonlat<-do.call(paste, c(mydata[c("lon", "lat")], sep=""))
>>>> > grid at data$lonlat<-do.call(paste, c(grid at data[c("LONGITUDE",
>>>> "LATITUDE")],
>>>> > sep=""))
>>>> >
>>>> > # use common variable lonlat to merge data in raster
>>>> >
>>>> > ###### prepare shapefile #####
>>>> > library(rgdal)        ## Load geographic info
>>>> > library(maps)         ## Projections
>>>> > library(maptools)     ## Data management
>>>> > #library(sm)           ## Data management
>>>> > library(spdep)        ## Spatial autocorrelation
>>>> > library(gstat)        ## Geostatistics
>>>> > library(splancs)      ## Kernel Density
>>>> > library(spatstat)     ## Geostatistics
>>>> > library(pgirmess)     ## Spatial autocorrelation
>>>> > library(RColorBrewer) ## Visualization
>>>> > library(classInt)     ## Class intervals
>>>> > library(spgwr)        ## GWR
>>>> >
>>>> > # Match polygons with data
>>>> > idx <- match(grid$lonlat, mydata$lonlat)
>>>> > # Places without information
>>>> > idxNA <- which(is.na(idx))
>>>> > # Information to be added to the SpatialPolygons object
>>>> > dat2add <- mydata[idx, ]
>>>> > # spCbind uses row names to match polygons with data
>>>> > # First, extract polygon IDs
>>>> > IDs <- sapply(grid at polygons, function(x)x at ID)
>>>> > # and join with the SpatialPolygons
>>>> > row.names(dat2add) <- IDs
>>>> > datPols <- spCbind(grid, dat2add)
>>>> > # Drop those places without information
>>>> > datPols <- datPols[-idxNA, ]
>>>> > # write new shapefile
>>>> > # 7 seconds
>>>> > writeOGR(datPols, dsn = ".", layer ='sm2000eu28', driver = 'ESRI
>>>> > Shapefile')
>>>> > # read new shapefile
>>>> > # 51 seconds
>>>> > data <- readOGR(".", layer="sm2000eu28")
>>>> >
>>>> > ############################
>>>> > # intersect nuts with grid #
>>>> > ############################
>>>> >
>>>> > library(rgdal)
>>>> > nuts <- readOGR(".", layer = "NUTS_RG_60M_2006")
>>>> >
>>>> > library(rgeos)
>>>> > proj4string(data) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
>>>> >
>>>> >
>>>> > #
>>>> > grid <- data
>>>> > grid at data$lonlat <- NULL
>>>> > grid at data$lonlat_1 <- NULL
>>>> > grid at data$ID <- NULL
>>>> > grid at data$lat <- NULL
>>>> > grid at data$lon <- NULL
>>>> > grid at data$ELEVATION <- NULL
>>>> > grid at data$DAYS_RAIN_ <- NULL
>>>> >
>>>> >
>>>> > # First find out which grid cells intersect your NUTS polygons
>>>> > grid_nuts <- gIntersects(grid,nuts,byid = TRUE)
>>>> >
>>>> > # use the apply() function to calculate the mean, min, and max of your
>>>> > value.
>>>> > # The loop makes
>>>> >
>>>> > for(i in names(grid at data)){
>>>> >   nuts at data[[paste(i, 'average_value', sep="_")]] <-
>>>> > apply(grid_nuts,1,function(x) mean(grid at data[[i]][x]))
>>>> >   nuts at data[[paste(i, 'min_value', sep="_")]] <-
>>>> > apply(grid_nuts,1,function(x) min(grid at data[[i]][x]))
>>>> >   nuts at data[[paste(i, 'max_value', sep="_")]] <-
>>>> > apply(grid_nuts,1,function(x) max(grid at data[[i]][x]))
>>>> > }
>>>> >
>>>> > write.table(nuts at data, "nuts_sm2000eu28_unweighted.txt", sep="\t")
>>>> >
>>>> >         [[alternative HTML version deleted]]
>>>> >
>>>> > _______________________________________________
>>>> > R-sig-Geo mailing list
>>>> > R-sig-Geo at r-project.org
>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>> >
>>>> >
>>>> > We value your opinion. Please take a few minutes to share your
>>>> comments on
>>>> > the service you received from the District by clicking on this link<
>>>> >
>>>> http://my.sfwmd.gov/portal/page/portal/pg_grp_surveysystem/survey%20ext?pid=1653
>>>> > >.
>>>> >
>>>>
>>>>
>>>>
>>>> --
>>>>
>>>> [image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
>>>> *Doctoraatsbursaal - PhD *
>>>> Milieueconomie - Environmental economics
>>>>
>>>> T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40
>>>>
>>>> www.uhasselt.be/eec
>>>>
>>>> Universiteit Hasselt | Campus Diepenbeek
>>>> Agoralaan Gebouw D | B-3590 Diepenbeek
>>>> Kantoor F11
>>>>
>>>> Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt
>>>>
>>>>
>>>> [image: Music For Life]  Maak van UHasselt de #warmsteunief |
>>>> www.uhasselt.be/musicforlife
>>>>
>>>>
>>>> P Please consider the environment before printing this e-mail
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________
>>>> R-sig-Geo mailing list
>>>> R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>> --
>>> Dr. Michael Sumner
>>> Software and Database Engineer
>>> Australian Antarctic Division
>>> 203 Channel Highway
>>> Kingston Tasmania 7050 Australia
>>>
>>>
>>
>>
>> --
>>
>> [image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
>> *Doctoraatsbursaal - PhD *
>> Milieueconomie - Environmental economics
>>
>> T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40
>>
>> www.uhasselt.be/eec
>>
>> Universiteit Hasselt | Campus Diepenbeek
>> Agoralaan Gebouw D | B-3590 Diepenbeek
>> Kantoor F11
>>
>> Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt
>>
>>
>> [image: Music For Life]  Maak van UHasselt de #warmsteunief |
>> www.uhasselt.be/musicforlife
>>
>>
>> P Please consider the environment before printing this e-mail
>>
>>
>
>
> --
>
> [image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
> *Doctoraatsbursaal - PhD *
> Milieueconomie - Environmental economics
>
> T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40
>
> www.uhasselt.be/eec
>
> Universiteit Hasselt | Campus Diepenbeek
> Agoralaan Gebouw D | B-3590 Diepenbeek
> Kantoor F11
>
> Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt
>
>
> [image: Music For Life]  Maak van UHasselt de #warmsteunief |
> www.uhasselt.be/musicforlife
>
>
> P Please consider the environment before printing this e-mail
>
> --
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From nell.redu at hotmail.fr  Thu Jun  9 17:38:12 2016
From: nell.redu at hotmail.fr (Nelly Reduan)
Date: Thu, 9 Jun 2016 15:38:12 +0000
Subject: [R-sig-Geo] Building a prediction raster when the statistical
 model was built from sampling units of different sizes
In-Reply-To: <CAASFQpTkBPQ=rsmjobFtGVsEbZigcXYdccqDy6qY1i7aE3dG_A@mail.gmail.com>
References: <CY1PR05MB27303108E8419879545ACB5899420@CY1PR05MB2730.namprd05.prod.outlook.com>
	<CAASFQpRa9ybZtEwtDgcMCS_rHTTBohOHge-KKfo3c9jHaNdSJg@mail.gmail.com>
	<CY1PR05MB2730AB11356CF6879D228C94995D0@CY1PR05MB2730.namprd05.prod.outlook.com>,
	<CAASFQpTkBPQ=rsmjobFtGVsEbZigcXYdccqDy6qY1i7aE3dG_A@mail.gmail.com>
Message-ID: <CY1PR05MB273033AA101EC60E6F7F51F5995F0@CY1PR05MB2730.namprd05.prod.outlook.com>

Hi Chris,



Thank you very much for your answer. My objective is to build a predictive map of capture success of striped skunks at large scale in order to delineate areas of high abundance of striped skunks that are susceptible to promote disease transmission. My GAM was used to assess the relationship between proportions of land cover types and capture success within trapping sites. Then, I estimated a capture success value within homogeneous grid cells of 2km x 2km from my GAM that was built from capture success data associated with trapping sites of different sizes. I also think that the approach is quite problematic. So, I am trying to view whether or not there are solutions to minimize bias in the predictions.



Thank you very much for your time.

Have a nice day.

Nell


________________________________
De : chris english <englishchristophera at gmail.com>
Envoy? : mardi 7 juin 2016 23:41:09
? : Nelly Reduan
Cc : Help R-Sig_Geo
Objet : Re: [R-sig-Geo] Building a prediction raster when the statistical model was built from sampling units of different sizes

Nell,

I'm still trying to understand what sounds to me like an embedded data reduction. Please understand, I don't trap skunks (striped or otherwise). I have, on many occasions, observed them in the wild but I am cautious not to make them scared due to the known, smelly results.

Understand as well that I am not versed in capture success, per se, I just examine data and wonder if it contains generalizable or perhaps surprising properties.

So if it were me, contrary to my nature and practice of observation, trapping striped skunks in a 24 km^2 study area of a given land use/land cover, and I trapped 15 skunks over a period of 2 months, I would deem that 15 observations for that study area/time period.If, in my 32 km^2 study area of slightly different land cover and different resource availability I got 70 in two months, I'd have 70 observations. This is how I would view it, and quite probably within the accepted science I would be going about it all wrong.

As you present the matter, at least as I understand it, the number of my hypothetical captures always reduces to one dimension (the study area), irrespective of the above variance between study areas. This approach puzzles me as it seems that information about desirable resource distribution (from the point of view of the skunk) gets lost, and 'capture success' becomes murky, at least for me.

As my wife always says, "It depends on what the research question is."

What is the research question in this case?

My apologies to you and capture science if I have completely misunderstood as I all too often do.

Chris

On Tue, Jun 7, 2016 at 5:53 PM, Nelly Reduan <nell.redu at hotmail.fr<mailto:nell.redu at hotmail.fr>> wrote:

Hi Chris,

Thank you very much for your answer.


They are striped skunks that have been captured. In my data, all striped skunks that have been captured within a same trapping site have the same capture success. Thus, each of 50 trapping sites was assigned to one capture success. If, I group trapping sites together, I reduce the sampling size. As the actuel sampling size (50 trapping sites) is rather small, can this cause problem for predicted data estimates?

Thanks very much for your time.

Have a nice day.

Nell


________________________________
De : chris english <englishchristophera at gmail.com<mailto:englishchristophera at gmail.com>>
Envoy? : jeudi 2 juin 2016 06:10:32
? : Nelly Reduan
Cc : Help R-Sig_Geo
Objet : Re: [R-sig-Geo] Building a prediction raster when the statistical model was built from sampling units of different sizes


Hi Nell,

Just a couple of questions. Trapping sites range from 24km^2 - 236km^2, and there are 50 such sites, looking at the 50 sites might there be a way to bin them reasonably into trap area groups?

Your 50 observations suggests one thing was trapped and thereafter trapping was discontinued. Is this correct?

And just for general information, what was being trapped?

Sticking closer to your data, you might consider GAM(ing) the bins and summing the resultant GAMs. Need to think some more on the predictive raster aspect. Sorry for an essentially inconclusive answer.

Chris


	[[alternative HTML version deleted]]


From wsavran at gmail.com  Thu Jun  9 22:01:32 2016
From: wsavran at gmail.com (William Savran)
Date: Thu, 09 Jun 2016 20:01:32 +0000
Subject: [R-sig-Geo] multivariate sequential gaussian cosimulation using
	gstat
Message-ID: <CADMaJ=Ofowjz_40NdqmWChoBPLX2Sy6a+ODXbxowoKV_zKskbQ@mail.gmail.com>

>On 06/06/16 19:03, William Savran wrote:
>> Dear All,
>>
>> I am new to gstat, R, and fairly new to applying geostatistical methods,
so I have a few questions.  I am looking to simulate a set of 3 (out of 4)
correlated stochastic fields, which  are all related through a linear model
of coregionalization. One of the fields will be used as conditional data
for the simulation.  With that being said, if anything seems alarming to
you please let me know. Also, if you have any tips/trick that would be
helpful for a novice please let me know.
>
> Right now, my algorithm is working as follows (almost exactly following
the demo):
>
> # define gstat object
> sim.g <- gstat(id='slip', formula=slip.sc~1, data=sim, nmax = 30, maxdist
= 200, beta=0, set = list(nocheck = 1))
> sim.g <- gstat(sim.g, 'psv', psv.sc~1, sim, nmax = 30, maxdist = 200,
beta=0)
> sim.g <- gstat(sim.g, 'vrup', vrup.sc~1, sim, nmax = 30, maxdist = 200,
beta=0)
> sim.g <- gstat(sim.g, 'mu0', mu0.sc~1, sim, nmax = 30, maxdist = 200,
beta=0)
> sim.g <- gstat(sim.g, model=vgm(1,"Exp",1000,1,anis=c(90,0.5)),
fill.all=T)
>
> # fit lmc
> sim.fit = fit.lmc(var, sim.g, correct.diagonal = 1.01)
>
> # perform SGSim
> z <- predict(sim.fit, newdata=xy, nsim=1, debug.level = -1)  If I specify
fit.ranges = TRUE and fit.LMC = TRUE, I get the warning message
>
> "Warning messages:
> 1: In fit.variogram(object, model, fit.sills = fit.sills, fit.ranges =
fit.ranges,  :
>  No convergence after 200 iterations: try different initial values?
> 2: In predict.gstat(sim.fit, newdata = xy, nsim = 1, debug.level = -1) :
>  No Intrinsic Correlation or Linear Model of Coregionalization found
> Reason: ranges differ?
>
> and a simulation still happens.  What is going on behind the scenes here
when there is no acceptable model of coregionalization or intrinsic
correlation?  Do I only want to simulate using a constant range for all my
>
> Main Question:
> ----------------------------
> In the SGSim step, how can I apply pre-existing data which can be used as
conditional in the simulation process?  Based on the ids present in the
gstat object,  I will have a data set of mu0, and conditional on this I
want to simulate ?psv?, ?vrup?, and ?slip?.  I have tried putting the data
in the sim.fit object as well as in newdata and it doesn?t seem to make a
difference.  What am I doing wrong?
>
> Thanks in advance!
>
> Best,
> - William Savran
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

> Your script is quite messy and incomplete (where do sim, var, and xy
> come from? Does the "If I specify..." comment really relate to the
> predict call? Which command generates which warning?), and is not
> reproducible.
>
> The way you specify conditioning data is correct, so it is the basis for
> conditioning your simulations. The warnings should not be ignored,
> though: have you tried plotting the variograms & cross variograms with
> the (wrongly?) fitted model?

> When you give this little insight in what you do, I can only reply with
> wild guesses.
> --
> Edzer Pebesma
> Institute for Geoinformatics  (ifgi),  University of M?nster
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
> Spatial Statistics Society http://www.spatialstatistics.info

Hi Edzer,

I apologize about not asking my question clearly, however it appears your
wild guess was spot on.  Now my initial simulations seem to be working as
expected, conditioning on mu0, and simulating slip,psv,and vrup from
scratch. Right now I am creating a new gstat object and copying over the
results from the output of fit.lmc as follows:

# create new gstat object for simulation from output of fit.lmc
sim.d <- gstat(id='mu0', formula=mu0.sc~1, model=sim.fit$model$mu0,
data=sim, nmax=30, beta=0.0)
sim.d <- gstat(sim.d, id='slip', formula=slip.dummy~1,
model=sim.fit$model$slip,  dummy=TRUE, nmax=30, beta=0.0)
sim.d <- gstat(sim.d, id='psv', formula=psv.dummy~1,
model=sim.fit$model$psv, dummy=TRUE, nmax=30, beta=0.0)
sim.d <- gstat(sim.d, id='vrup', formula=vrup.dummy~1,
model=sim.fit$model$vrup, dummy=TRUE, nmax=30, beta=0.0)

# enter cross-variograms
sim.d <- gstat(sim.d, id=c("slip","psv"), model=sim.fit$model$slip.psv)
sim.d <- gstat(sim.d, id=c("slip","vrup"), model=sim.fit$model$slip.vrup)
sim.d <- gstat(sim.d, id=c("slip","mu0"), model=sim.fit$model$slip.mu0)
sim.d <- gstat(sim.d, id=c("psv","vrup"), model=sim.fit$model$psv.vrup)
sim.d <- gstat(sim.d, id=c("psv","mu0"), model=sim.fit$model$psv.mu0)
sim.d <- gstat(sim.d, id=c("vrup","mu0"), model=sim.fit$model$vrup.mu0)

As far as my question regarding the warnings from fit.lmc and predict is
concerned, I increased the complexity of the model slightly and include
variogram basis functions with different correlation lengths.   When I do
this I can achieve a better fit to the empirical variograms, and it
provides an acceptable linear model of coregionalization.  Thanks again for
your help!

Best regards,
William

	[[alternative HTML version deleted]]


From afitz at email.wm.edu  Thu Jun  9 22:16:50 2016
From: afitz at email.wm.edu (Alex Fitz)
Date: Thu, 9 Jun 2016 16:16:50 -0400
Subject: [R-sig-Geo] Incorrect area coming from raster
Message-ID: <67447606-FFDD-437C-BAC8-80CE3E1748AD@email.wm.edu>

Hi All,

Currently I am using a raster of the gridded world population (GWPv4) to calculate density in Nigeria.  I also have the Nigeria shapefiles from GADM.  I am having an issue with the area?s that are being calculated because they are slightly off for each state/lga in Nigeria.  The approach I am taking is the following code:

pop_world_2005 is my raster the the population of the entire world
states is my shapefile containing the state boundaries

state_crop_05 <- crop(pop_world_2005, states, df = FALSE)
area_05 <- raster::area(state_crop_05)
state_area_05 <- extract(area_05, states, df = TRUE)


I think aggregated the data from my extract to check the area?s again known area?s and they seemed to be slightly incorrect with some much more incorrect than others. Here?s the code I used to join the names with the area based on their state ID.

state_area_totals_15 <- aggregate(. ~ ID, state_area_15, sum)
state_df <- as.data.frame(states)
state_names_area <- left_join(x = state_df, y = state_area_totals_15, by = c("ID" = "ID"))



I?m wondering if the shapefiles I downloaded from GADM could be the issue (Different version than those used in GWPv4.  When I some the total population of the country it appears to be correct/very close to correct so it seems to be incorrectly assigning the values to certain states.  Any help would be appreciated in figuring out where the issue is coming from.

Thanks so much,
Alex Fitz

From englishchristophera at gmail.com  Fri Jun 10 00:29:24 2016
From: englishchristophera at gmail.com (chris english)
Date: Fri, 10 Jun 2016 01:29:24 +0300
Subject: [R-sig-Geo] Building a prediction raster when the statistical
 model was built from sampling units of different sizes
In-Reply-To: <CY1PR05MB273033AA101EC60E6F7F51F5995F0@CY1PR05MB2730.namprd05.prod.outlook.com>
References: <CY1PR05MB27303108E8419879545ACB5899420@CY1PR05MB2730.namprd05.prod.outlook.com>
	<CAASFQpRa9ybZtEwtDgcMCS_rHTTBohOHge-KKfo3c9jHaNdSJg@mail.gmail.com>
	<CY1PR05MB2730AB11356CF6879D228C94995D0@CY1PR05MB2730.namprd05.prod.outlook.com>
	<CAASFQpTkBPQ=rsmjobFtGVsEbZigcXYdccqDy6qY1i7aE3dG_A@mail.gmail.com>
	<CY1PR05MB273033AA101EC60E6F7F51F5995F0@CY1PR05MB2730.namprd05.prod.outlook.com>
Message-ID: <CAASFQpS=X-20+Y=ozOpsNoQU6ZCD8DSipzTzsVo+VPjv+bpahg@mail.gmail.com>

I have requested another environmental data scientist  to look at this
thread of conversation as he is better versed in your area of science and
my comments may have been ill-informed or confused /confusing or worse,
both.

But now I understand your research interest, likelihood of disease vector
instances (among striped skunks) given land cover. Is the disease rabies or
some other ailment of concern? I just ask because if rabies, there could be
a large, healthy population given proper resources to sustain them, without
necessarily indicating an areal propensity to rabies. Keeping with rabies
though, you might find you are exploring extremes (here meaning very, very
few are disease vectors) as against general additive models, hence lots of
skunks and relatively very few vectors. Combined in some way that I have
yet to think through, GAM and extreme might lead to the environment more
likely to produce vectors.

So, lets throw land cover under the bus for the moment as it is probably
immaterial, same for area, and ask what environmental conditions are more
likely to generate the vectors of interest, whatever the particular disease
condition is. This indeed would be an interesting study.

Chris
On Jun 9, 2016 18:38, "Nelly Reduan" <nell.redu at hotmail.fr> wrote:

> Hi Chris,
>
>
>
> Thank you very much for your answer. My objective is to build a predictive
> map of capture success of striped skunks at large scale in order to
> delineate areas of high abundance of striped skunks that are susceptible to
> promote disease transmission. My GAM was used to assess the relationship
> between proportions of land cover types and capture success within trapping
> sites. Then, I estimated a capture success value within homogeneous grid
> cells of 2km x 2km from my GAM that was built from capture success data
> associated with trapping sites of different sizes. I also think that the
> approach is quite problematic. So, I am trying to view whether or not there
> are solutions to minimize bias in the predictions.
>
>
>
> Thank you very much for your time.
>
> Have a nice day.
>
> Nell
>
> ------------------------------
> *De :* chris english <englishchristophera at gmail.com>
> *Envoy? :* mardi 7 juin 2016 23:41:09
> *? :* Nelly Reduan
> *Cc :* Help R-Sig_Geo
> *Objet :* Re: [R-sig-Geo] Building a prediction raster when the
> statistical model was built from sampling units of different sizes
>
> Nell,
>
> I'm still trying to understand what sounds to me like an embedded data
> reduction. Please understand, I don't trap skunks (striped or otherwise). I
> have, on many occasions, observed them in the wild but I am cautious not to
> make them scared due to the known, smelly results.
>
> Understand as well that I am not versed in capture success, per se, I just
> examine data and wonder if it contains generalizable or perhaps surprising
> properties.
>
> So if it were me, contrary to my nature and practice of observation,
> trapping striped skunks in a 24 km^2 study area of a given land use/land
> cover, and I trapped 15 skunks over a period of 2 months, I would deem that
> 15 observations for that study area/time period.If, in my 32 km^2 study
> area of slightly different land cover and different resource availability I
> got 70 in two months, I'd have 70 observations. This is how I would view
> it, and quite probably within the accepted science I would be going about
> it all wrong.
>
> As you present the matter, at least as I understand it, the number of my
> hypothetical captures always reduces to one dimension (the study area),
> irrespective of the above variance between study areas. This approach
> puzzles me as it seems that information about desirable resource
> distribution (from the point of view of the skunk) gets lost, and 'capture
> success' becomes murky, at least for me.
>
> As my wife always says, "It depends on what the research question is."
>
> What is the research question in this case?
>
> My apologies to you and capture science if I have completely misunderstood
> as I all too often do.
>
> Chris
>
> On Tue, Jun 7, 2016 at 5:53 PM, Nelly Reduan <nell.redu at hotmail.fr> wrote:
>
>> Hi Chris,
>>
>> Thank you very much for your answer.
>>
>>
>> They are striped skunks that have been captured. In my data, all striped
>> skunks that have been captured within a same trapping site have the same
>> capture success. Thus, each of 50 trapping sites was assigned to one
>> capture success. If, I group trapping sites together, I reduce the sampling
>> size. As the actuel sampling size (50 trapping sites) is rather small, can
>> this cause problem for predicted data estimates?
>>
>> Thanks very much for your time.
>>
>> Have a nice day.
>>
>> Nell
>>
>>
>> ------------------------------
>> *De :* chris english <englishchristophera at gmail.com>
>> *Envoy? :* jeudi 2 juin 2016 06:10:32
>> *? :* Nelly Reduan
>> *Cc :* Help R-Sig_Geo
>> *Objet :* Re: [R-sig-Geo] Building a prediction raster when the
>> statistical model was built from sampling units of different sizes
>>
>>
>> Hi Nell,
>>
>> Just a couple of questions. Trapping sites range from 24km^2 - 236km^2,
>> and there are 50 such sites, looking at the 50 sites might there be a way
>> to bin them reasonably into trap area groups?
>>
>> Your 50 observations suggests one thing was trapped and thereafter
>> trapping was discontinued. Is this correct?
>>
>> And just for general information, what was being trapped?
>>
>> Sticking closer to your data, you might consider GAM(ing) the bins and
>> summing the resultant GAMs. Need to think some more on the predictive
>> raster aspect. Sorry for an essentially inconclusive answer.
>>
>> Chris
>>
>
>

	[[alternative HTML version deleted]]


From nell.redu at hotmail.fr  Fri Jun 10 02:05:52 2016
From: nell.redu at hotmail.fr (Nelly Reduan)
Date: Fri, 10 Jun 2016 00:05:52 +0000
Subject: [R-sig-Geo] Building a prediction raster when the statistical
 model was built from sampling units of different sizes
In-Reply-To: <CAASFQpS=X-20+Y=ozOpsNoQU6ZCD8DSipzTzsVo+VPjv+bpahg@mail.gmail.com>
References: <CY1PR05MB27303108E8419879545ACB5899420@CY1PR05MB2730.namprd05.prod.outlook.com>
	<CAASFQpRa9ybZtEwtDgcMCS_rHTTBohOHge-KKfo3c9jHaNdSJg@mail.gmail.com>
	<CY1PR05MB2730AB11356CF6879D228C94995D0@CY1PR05MB2730.namprd05.prod.outlook.com>
	<CAASFQpTkBPQ=rsmjobFtGVsEbZigcXYdccqDy6qY1i7aE3dG_A@mail.gmail.com>
	<CY1PR05MB273033AA101EC60E6F7F51F5995F0@CY1PR05MB2730.namprd05.prod.outlook.com>,
	<CAASFQpS=X-20+Y=ozOpsNoQU6ZCD8DSipzTzsVo+VPjv+bpahg@mail.gmail.com>
Message-ID: <CY1PR05MB27303BF0330233E0B0BADA69995F0@CY1PR05MB2730.namprd05.prod.outlook.com>

Hi Chris,


Thank you very much for your answer. Yes, it is for rabies. Rabies transmission requires direct contacts between individuals, which depends on population density. So, areas with high abundance of striped skunks would be susceptible to contain more frequent contacts between striped skunks. The objective is also to identify areas at high abundance in order to increase the efficiency of control programs.


Thank you very much for your time.

Have a nice day.

Nell



________________________________
De : chris english <englishchristophera at gmail.com>
Envoy? : jeudi 9 juin 2016 15:29:24
? : Nelly Reduan
Cc : Help R-Sig_Geo
Objet : RE: [R-sig-Geo] Building a prediction raster when the statistical model was built from sampling units of different sizes


I have requested another environmental data scientist  to look at this thread of conversation as he is better versed in your area of science and my comments may have been ill-informed or confused /confusing or worse, both.

But now I understand your research interest, likelihood of disease vector instances (among striped skunks) given land cover. Is the disease rabies or some other ailment of concern? I just ask because if rabies, there could be a large, healthy population given proper resources to sustain them, without necessarily indicating an areal propensity to rabies. Keeping with rabies though, you might find you are exploring extremes (here meaning very, very few are disease vectors) as against general additive models, hence lots of skunks and relatively very few vectors. Combined in some way that I have yet to think through, GAM and extreme might lead to the environment more likely to produce vectors.

So, lets throw land cover under the bus for the moment as it is probably immaterial, same for area, and ask what environmental conditions are more likely to generate the vectors of interest, whatever the particular disease condition is. This indeed would be an interesting study.

Chris

On Jun 9, 2016 18:38, "Nelly Reduan" <nell.redu at hotmail.fr<mailto:nell.redu at hotmail.fr>> wrote:

Hi Chris,



Thank you very much for your answer. My objective is to build a predictive map of capture success of striped skunks at large scale in order to delineate areas of high abundance of striped skunks that are susceptible to promote disease transmission. My GAM was used to assess the relationship between proportions of land cover types and capture success within trapping sites. Then, I estimated a capture success value within homogeneous grid cells of 2km x 2km from my GAM that was built from capture success data associated with trapping sites of different sizes. I also think that the approach is quite problematic. So, I am trying to view whether or not there are solutions to minimize bias in the predictions.



Thank you very much for your time.

Have a nice day.

Nell


________________________________
De : chris english <englishchristophera at gmail.com<mailto:englishchristophera at gmail.com>>
Envoy? : mardi 7 juin 2016 23:41:09
? : Nelly Reduan
Cc : Help R-Sig_Geo
Objet : Re: [R-sig-Geo] Building a prediction raster when the statistical model was built from sampling units of different sizes

Nell,

I'm still trying to understand what sounds to me like an embedded data reduction. Please understand, I don't trap skunks (striped or otherwise). I have, on many occasions, observed them in the wild but I am cautious not to make them scared due to the known, smelly results.

Understand as well that I am not versed in capture success, per se, I just examine data and wonder if it contains generalizable or perhaps surprising properties.

So if it were me, contrary to my nature and practice of observation, trapping striped skunks in a 24 km^2 study area of a given land use/land cover, and I trapped 15 skunks over a period of 2 months, I would deem that 15 observations for that study area/time period.If, in my 32 km^2 study area of slightly different land cover and different resource availability I got 70 in two months, I'd have 70 observations. This is how I would view it, and quite probably within the accepted science I would be going about it all wrong.

As you present the matter, at least as I understand it, the number of my hypothetical captures always reduces to one dimension (the study area), irrespective of the above variance between study areas. This approach puzzles me as it seems that information about desirable resource distribution (from the point of view of the skunk) gets lost, and 'capture success' becomes murky, at least for me.

As my wife always says, "It depends on what the research question is."

What is the research question in this case?

My apologies to you and capture science if I have completely misunderstood as I all too often do.

Chris

On Tue, Jun 7, 2016 at 5:53 PM, Nelly Reduan <nell.redu at hotmail.fr<mailto:nell.redu at hotmail.fr>> wrote:

Hi Chris,

Thank you very much for your answer.


They are striped skunks that have been captured. In my data, all striped skunks that have been captured within a same trapping site have the same capture success. Thus, each of 50 trapping sites was assigned to one capture success. If, I group trapping sites together, I reduce the sampling size. As the actuel sampling size (50 trapping sites) is rather small, can this cause problem for predicted data estimates?

Thanks very much for your time.

Have a nice day.

Nell


________________________________
De : chris english <englishchristophera at gmail.com<mailto:englishchristophera at gmail.com>>
Envoy? : jeudi 2 juin 2016 06:10:32
? : Nelly Reduan
Cc : Help R-Sig_Geo
Objet : Re: [R-sig-Geo] Building a prediction raster when the statistical model was built from sampling units of different sizes


Hi Nell,

Just a couple of questions. Trapping sites range from 24km^2 - 236km^2, and there are 50 such sites, looking at the 50 sites might there be a way to bin them reasonably into trap area groups?

Your 50 observations suggests one thing was trapped and thereafter trapping was discontinued. Is this correct?

And just for general information, what was being trapped?

Sticking closer to your data, you might consider GAM(ing) the bins and summing the resultant GAMs. Need to think some more on the predictive raster aspect. Sorry for an essentially inconclusive answer.

Chris


	[[alternative HTML version deleted]]


From r.hijmans at gmail.com  Fri Jun 10 03:15:36 2016
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Thu, 9 Jun 2016 18:15:36 -0700
Subject: [R-sig-Geo] Incorrect area coming from raster
In-Reply-To: <67447606-FFDD-437C-BAC8-80CE3E1748AD@email.wm.edu>
References: <67447606-FFDD-437C-BAC8-80CE3E1748AD@email.wm.edu>
Message-ID: <CANtt_hxUWXwO2=2g0Ygjhk6RkuXwBOM1W8yCtdZkrvY1OfO=+Q@mail.gmail.com>

Alex,

You should provide a full reproducible example. You can get the
boundaries via raster::getData, and you can create a raster in memory.
Your script should also indicate the packages you use.
To get the area of the polygons, I would do

library(raster)
nga <- getData('GADM', country='NGA', level=1)
area(nga)

Robert

On Thu, Jun 9, 2016 at 1:16 PM, Alex Fitz <afitz at email.wm.edu> wrote:
> Hi All,
>
> Currently I am using a raster of the gridded world population (GWPv4) to calculate density in Nigeria.  I also have the Nigeria shapefiles from GADM.  I am having an issue with the area?s that are being calculated because they are slightly off for each state/lga in Nigeria.  The approach I am taking is the following code:
>
> pop_world_2005 is my raster the the population of the entire world
> states is my shapefile containing the state boundaries
>
> state_crop_05 <- crop(pop_world_2005, states, df = FALSE)
> area_05 <- raster::area(state_crop_05)
> state_area_05 <- extract(area_05, states, df = TRUE)
>
>
> I think aggregated the data from my extract to check the area?s again known area?s and they seemed to be slightly incorrect with some much more incorrect than others. Here?s the code I used to join the names with the area based on their state ID.
>
> state_area_totals_15 <- aggregate(. ~ ID, state_area_15, sum)
> state_df <- as.data.frame(states)
> state_names_area <- left_join(x = state_df, y = state_area_totals_15, by = c("ID" = "ID"))
>
>
>
> I?m wondering if the shapefiles I downloaded from GADM could be the issue (Different version than those used in GWPv4.  When I some the total population of the country it appears to be correct/very close to correct so it seems to be incorrectly assigning the values to certain states.  Any help would be appreciated in figuring out where the issue is coming from.
>
> Thanks so much,
> Alex Fitz
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From tim.appelhans at gmail.com  Sun Jun 12 17:22:50 2016
From: tim.appelhans at gmail.com (Tim Appelhans)
Date: Sun, 12 Jun 2016 17:22:50 +0200
Subject: [R-sig-Geo] mapview-1.1.0 now on CRAN
Message-ID: <575D7E4A.6080905@gmail.com>

Dear all,

we are happy to announce that mapview-1.1.0 is now available on CRAN.
This release marks a major update of the package.

Here's the NEWS file:

new features:
   * addMouseCoordinates: add cursor position information to mapview or 
leaflet map. (thanks to Kent Russell).
   * if available from leaflet version, a scalebar is added to the map.
   * latticeView: view mapview or leaflet maps as small multiples and 
sync some, all or none (thanks to Kent Russell).
   * sync: synchronise two or more leaflet maps (thanks to Kenton Russell).
   * mapshot: to save maps as html page or static image or both.
   * knitr integration (i.e. no need to call the @map slot anymore to 
render in knitr).
   * cubeView: view raster bricks or stacks hovmoeller style, use keys 
up & down, left & right, page up & page down to navigate through y, x, z 
dimensions, respectively.
   * labels: if zcol is set, mouseover will now show the repesctive 
values of zcol, if zcol is not set moseover shows feature ID. Only 
available if suitable leaflet package version is installed.
   * new popup functions popupTable, popupGraph and popupImage.
   * functions to turn coordinates into spatial lines or spatial polygons.
   * mapview objects now work natively on shiny applications (i.e. 
renderMapview and mapviewOutput now available).
   * "zcol = Var" in combination with burst = TRUE now plots one layer 
for each unique value of the variable supplied to zcol.

changes:
   * spplot method has been removed.
   * colors: viridis based colors now default if viridisLite package is 
available.
   * basemaps: new default basemap is "CartoDB.Positron" as colors of 
features are better visible on the grey background.
   * layer names now include the name of the object they originate from 
(e.g. "meuse lead" instead of "lead").

bugfixes:
   * if attribute was of class "character" mapview did through an error 
if passed to zcol.
   * user provided layer names were not respected when zcol was set. See 
also note on changes in default layer names.


In addition, the new online documentation is available at:

http://environmentalinformatics-marburg.github.io/mapview/introduction.html



As always, feedback and suggestions are always welcome.

Feature requests and bug reports should be filed at :

https://github.com/environmentalinformatics-marburg/mapview/issues



All the best,
Tim

-- 
#####################################
Tim Appelhans
Department of Geography
Environmental Informatics
Philipps Universit?t Marburg
Deutschhausstra?e 12
Raum 00A08
35032 Marburg (Paketpost: 35037 Marburg)
Germany

Tel +49 (0) 6421 28-25957

http://environmentalinformatics-marburg.de/


	[[alternative HTML version deleted]]


From m.lattuada87 at gmail.com  Mon Jun 13 10:14:08 2016
From: m.lattuada87 at gmail.com (matteo lattuada)
Date: Mon, 13 Jun 2016 10:14:08 +0200
Subject: [R-sig-Geo] Ncdf with missing variable columns
Message-ID: <CAFKSRLghvVYLzTQOS-Z_CwOfjiCBn9aebbV_YYf8-9SsVm8WTw@mail.gmail.com>

Hi all,

I'm trying to get some variables iteratively from a directory with a large
number of .nc files and create a csv.file.

I wrote this script:

filenames <- list.files("directory", pattern="*.nc", full.names=TRUE)

res<-lapply(filenames,function(nc){
  t<-nc_open(filename=nc)
  cast<-ncvar_get(t,"wod_unique_cast")
  temp<-ncvar_get(t,"Temperature")
  sal<-ncvar_get(t,"Salinity")
  x<-ncvar_get(t,"lon")
  y<-ncvar_get(t,"lat")
  date<-ncvar_get(t,"date")
  depth<-ncvar_get(t,"z")
  df<-data.frame(cast,x,y,date,depth,temp,sal)
})

df.new<-ldply(res,data.frame)

This script is working well. The problem is that some of the .nc file miss
one of the variables. For example some don't have measurements of
"salinity". When a variable is not found, the iteration stops. Do you have
any hint to get all the variables present in a .nc file and add a NA column
for the absent variables?

Thank you
Kindest Regards
Matteo

	[[alternative HTML version deleted]]


From janka.vanschoenwinkel at uhasselt.be  Mon Jun 13 15:30:16 2016
From: janka.vanschoenwinkel at uhasselt.be (Janka VANSCHOENWINKEL)
Date: Mon, 13 Jun 2016 15:30:16 +0200
Subject: [R-sig-Geo] Translate a net-cdf file with different years to
 polygon regions.
In-Reply-To: <CAAcGz9-5H8Xu5LXqDqsmUtUsNWGFvxe158ONYhVGZgz2tMGEVA@mail.gmail.com>
References: <CAHymutKcpgJoWExtVVLFsSJ8VG23-b1jumMrMivMk2ZO5Lu6Jg@mail.gmail.com>
	<D51374C4B889BC47B3C5286047C86DA1A02C3A20@whqembx03p.ad.sfwmd.gov>
	<D51374C4B889BC47B3C5286047C86DA1A02C3A70@whqembx03p.ad.sfwmd.gov>
	<CAHymutLi97xoegT8qyYD3Tz7+LwFt9DfGJoMH81dSO3KL9yh4A@mail.gmail.com>
	<CAAcGz9-x7E6B_BHLcCdE5A1t28=tqGR9j4oFF=SUvJJ-BYGWhA@mail.gmail.com>
	<CAHymutLkmHER46BP40Nup8WT52FTuDxuJbnwStzCa+_jx_7bnA@mail.gmail.com>
	<CAHymut+2SzP75wbGqwwvqpKbFhV9wsU-G5yF3JD22er0WD=9sA@mail.gmail.com>
	<CAAcGz9-5H8Xu5LXqDqsmUtUsNWGFvxe158ONYhVGZgz2tMGEVA@mail.gmail.com>
Message-ID: <CAHymutKAKRiVheZCRoUnM=Mb2auGv51opLuqODO5QJu_6zGBMg@mail.gmail.com>

This is really super! Thank you very much!

I am assuming that the row-numbers in the dataset are in line with the
id-numbers in the shapefile. Is that correct?

Once again thank you very much!

2016-06-09 16:51 GMT+02:00 Michael Sumner <mdsumner at gmail.com>:

>
>
> On Thu, 9 Jun 2016 at 23:32 Janka VANSCHOENWINKEL <
> janka.vanschoenwinkel at uhasselt.be> wrote:
>
>> Dear Michael, Joseph and other colleagues,
>>
>> About a month and a half ago, I asked the question below (I summarized it
>> shorter in this email). I received some hints on how to do it in R, but
>> given the fact that I am not that experienced with netcdf files in R, I
>> didn't manage to make it work. I continued to work with ArcGis, but even
>> there it is not working.
>>
>> So I am giving it one more try here: could somebody help me with the
>> problem below?
>>
>>
>> Download from this website (
>> http://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts)
>> the following ZIP file: NUTS_2006_60M_SH.zip and save all
>> theNUTS_RG_60M_2006 files. In what follows I refer to this as "NUTS3
>> regions".
>>
>> Download from this website (
>> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/)
>>  the following nc-file: cru_ts3.23.1991.2000.pet.dat.nc.gz This is a raster
>> with data all over the world.
>>
>> The netcdf file has data on one variable (pet) for multiple months per
>> year (from 1991 to 2000). For each month, I would like to translate all
>> these values into averages per nuts3 region.
>>
>> I can open the file in R and make a cvs file from it (see code in the
>> original email below), but I do not manage to overlap each monthly value
>> with the nuts3 regions.
>>
>> Results can be stored in a table by month and by nuts3, or in a
>> shapefile. Either way is fine!
>>
>> Does somebody have any experience with this? Any concrete help is highly
>> appreciated! I have really no experience with netcdf files. I only have
>> trouble with them.. ;-)
>>
>
> You are not alone, NetCDF is a very general format and that generality
> gets a lot of exercise.  This one is pretty easy as far as I can see, the
> nc file seems to conform to a basic gridded raster in longitude-latitude,
> no complications.
>
>  Lots more stuff you can try, but this should get you over the first
> hurdle. Please pay close attention to how "high-level" most of this is, you
> really will benefit from learning how to drive raster and sp without
> getting to untidy in the trenches. Raster and sp were not designed and
> built together, and have wildly different assumptions and working
> philosophies but for the most part they play well together - it just means
> you have to learn another two languages (at least) to use them well. Please
> read the doc for each function and explore what the options do, there are
> gotchas at every step and I've only done the most naive things here.
>
> Finally, this is a pretty big data set - the polygons are quite detailed
> and that makes plotting fairly tedious - so use subsetting to hone in on an
> area of interest, etc.  D Explore these tools with simpler data before
> applying your main work to them.
>
> Make sure to upgrade to R 3.3.0 and get the latest raster package from
> some time last week. There were some important bug fixes released.
>
> HTH
>
>
> ##
> ----------------------------------------------------------------------------
> ## boring stuff to get the data, skip if you have "nutsdsn" and
> "nutslayer" and "ncfile"
> nuts <- "
> http://ec.europa.eu/eurostat/cache/GISCO/geodatafiles/NUTS_2006_60M_SH.zip
> "
> download.file(nuts, basename(nuts), mode = "wb")
> nutsfile <- basename(nuts)
> unzip(nutsfile)
> #nutslayer <- "NUTS_BN_60M_2006"  ## boundaries
> #nutslayer <- "NUTS_JOIN_LI_2006"
> #nutslayer <- "NUTS_LB_2006" ## points
>
> rf <- "
> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/cru_ts3.23.1991.2000.pet.dat.nc.gz
> "
> download.file(rf, basename(rf), mode = "wb")
> rfile <- basename(rf)
> system(sprintf("gunzip %s",  rfile))
>
>
>
>
> ## we have the files
> nutsdsn <- "NUTS_2006_60M_SH/shape/data"
> nutslayer <- "NUTS_RG_60M_2006" ## bingo, polygons
> ncfile <- "cru_ts3.23.1991.2000.pet.dat.nc"
>
> library(rgdal)  ## to read shapefile
> admin <- readOGR(nutsdsn, nutslayer)
>
> library(raster)
> ## looks fine
> plot(raster(ncfile))  ## this gets the first layer (or "band")
> plot(admin, add = TRUE)
>
> admin
> # class       : SpatialPolygonsDataFrame
> # features    : 1927
> # extent      : -61.80605, 55.83498, -21.37656, 71.15701  (xmin, xmax,
> ymin, ymax)
> # coord. ref. : +proj=longlat +ellps=GRS80 +no_defs
> # variables   : 7
> # names       : OBJECTID, NUTS_ID, STAT_LEVL_, AREA, LEN,  Shape_Leng,
> Shape_Area
> # min values  :        1,      AT,          0,    0,   0,   0.1351160,
> 1.002343e+00
> # max values  :     1927,   UKN05,          3,    0,   0, 312.0817173,
> 9.997536e-02
>
> ## get every layer (without actually loading necessarily)
> (cru_ts3 <- brick(ncfile))
> # class       : RasterBrick
> # dimensions  : 360, 720, 259200, 120  (nrow, ncol, ncell, nlayers)
> # resolution  : 0.5, 0.5  (x, y)
> # extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)
> # coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0
> # data source : ...cru_ts3.23.1991.2000.pet.dat.nc
> # names       : X1991.01.16, X1991.02.15, X1991.03.16, X1991.04.16,
> X1991.05.16, X1991.06.16, X1991.07.16, X1991.08.16, X1991.09.16,
> X1991.10.16, X1991.11.16, X1991.12.16, X1992.01.16, X1992.02.15,
> X1992.03.16, ...
> # Date        : 1991-01-16, 2000-12-16 (min, max)
> # varname     : pet
>
> ## extract everything, collapse to mean values
>
> ## note that this will automatically transform to the raster
> ## and warn, you should explore what this does (the plot above does not
> for example)
> ex <- extract(cru_ts3, admin, fun = mean)
> dim(ex)
> ## [1] 1927  120
>
> ## tidy up
> d <- as.data.frame(ex)
> row.names(d) <- row.names(admin)
> pet <- SpatialPolygonsDataFrame(geometry(admin), d)
>
> ## try some plots, slow
> spplot(pet[, c(1, 10)])
>
>
>
>
>
>
>> 2016-04-29 15:03 GMT+02:00 Janka VANSCHOENWINKEL <
>> janka.vanschoenwinkel at uhasselt.be>:
>>
>>> Thanks Mike,
>>>
>>> I am running your suggestion but it always get stuck. I also don't know
>>> how I have to indicate which variable I want to have given the fact that
>>> there is only one variable, but it is measured for different time periods.
>>> So for instance, how to I tell R that I want to have "pet from april in
>>> 1995"?
>>>
>>
>>> In mean time, I want to make clear that there are multiple points of the
>>> same period per nuts3-polygon. So when I was speaking about the average, I
>>> meant the average of all those points from the same periods, that are
>>> located in the same polygon. So I do want to have information per period.
>>>
>>> Thanks,
>>>
>>> Janka
>>>
>>
>>> 2016-04-29 14:42 GMT+02:00 Michael Sumner <mdsumner at gmail.com>:
>>>
>>>>
>>>>
>>>> On Fri, 29 Apr 2016 at 22:23 Janka VANSCHOENWINKEL <
>>>> janka.vanschoenwinkel at uhasselt.be> wrote:
>>>>
>>>>> Hi Joseph,
>>>>>
>>>>> Thank you very much for the hint.
>>>>>
>>>>> If I may ask you, could you please help me a little bit further. I am
>>>>> not
>>>>> familiar with these packages and I do not see how to overlap the point
>>>>> data
>>>>> or the netcdf data with the nuts3 polygons.
>>>>>
>>>>> Basically, I have too options:
>>>>>
>>>>> 1) use the cvs-points (with lon, lat, and the data variable) to see
>>>>> which
>>>>> points are located in which nuts3 regions and then save these results.
>>>>> (but
>>>>> then my previous codes still applies)
>>>>>
>>>>> 2) use the original netcdf file and overlay it with the nuts3 polygon
>>>>> shapefile (this was your hint, but I don't know how to start with
>>>>> this).
>>>>>
>>>>>
>>>> Something like this will do it:
>>>>
>>>> library(raster)
>>>> require(rgdal)
>>>> require(ncdf4)
>>>>
>>>> st <- stack("thencfile.nc")  ## might need varname = "sst" or similar
>>>>
>>>> poly <- shapefile("theshpfile.shp")
>>>>
>>>> extract(st, poly, fun = "mean")  ## and maybe na.rm = TRUE
>>>>
>>>> If you don't want the mean, you can just leave out the fun argument and
>>>> you'll get all the pixel values for every time step in a big list, which
>>>> may not be obviously helpful but it's all useable with generic R.
>>>>
>>>> I can't quite bring myself to get your files and try it, so please try
>>>> and report details.
>>>>
>>>> Cheers, Mike.
>>>>
>>>>
>>>>> Thank you very much for your answer!
>>>>>
>>>>>
>>>>>
>>>>> 2016-04-19 14:42 GMT+02:00 Stachelek, Joseph <jstachel at sfwmd.gov>:
>>>>>
>>>>> > Since your nc file contains multiple layers, you will want to use
>>>>> > `raster::stack()` rather than `raster::raster()`.
>>>>> >
>>>>> >
>>>>> > -----Original Message-----
>>>>> > From: Stachelek, Joseph
>>>>> > Sent: Tuesday, April 19, 2016 8:23 AM
>>>>> > To: 'Janka VANSCHOENWINKEL' <janka.vanschoenwinkel at uhasselt.be>;
>>>>> > r-sig-geo at r-project.org
>>>>> > Subject: RE: [R-sig-Geo] Translate a net-cdf file with different
>>>>> years to
>>>>> > polygon regions.
>>>>> >
>>>>> > Hi Janka,
>>>>> >
>>>>> > I think you can simplify your code a lot by opening your nc file
>>>>> directly
>>>>> > using the `raster` package rather than messing with `nc_open` calls.
>>>>> >
>>>>> > ```
>>>>> > ncin <- raster::raster(paste(ncname, ".nc", sep = ""))
>>>>> > ```
>>>>> >
>>>>> > Then you might use `raster::extract()` to pull the values associated
>>>>> with
>>>>> > your polygons. Also, I would recommend posting a link to a gist (
>>>>> > https://gist.github.com/) rather than pasting such a long script
>>>>> into
>>>>> > your email.
>>>>> >
>>>>> > Joe
>>>>> >
>>>>> > -----Original Message-----
>>>>> > From: R-sig-Geo [mailto:r-sig-geo-bounces at r-project.org] On Behalf
>>>>> Of
>>>>> > Janka VANSCHOENWINKEL
>>>>> > Sent: Tuesday, April 19, 2016 4:45 AM
>>>>> > To: r-sig-geo at r-project.org
>>>>> > Subject: [R-sig-Geo] Translate a net-cdf file with different years to
>>>>> > polygon regions.
>>>>> >
>>>>> > *DATA*
>>>>> >
>>>>> > I have a net-cdf file which has raster of 0.5 on 0.5 degrees. You can
>>>>> > easily download it here
>>>>> > <
>>>>> >
>>>>> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/
>>>>> > >
>>>>> > and
>>>>> > search for *cru_ts3.23.2001.2010.pet.dat.nc.gz *(it is also
>>>>> downloadable as
>>>>> > a dat.file if this is more handy to work with)
>>>>> >  (Or simply download the net-cdf file directly through:
>>>>> > cru_ts3.23.2001.2010.pet.dat.nc.gz
>>>>> > <
>>>>> >
>>>>> https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_3.23/cruts.1506241137.v3.23/pet/cru_ts3.23.2001.2010.pet.dat.nc.gz
>>>>> > >
>>>>> > ).
>>>>> >
>>>>> > I opened the file in ArcMap as well and found that the coordinate
>>>>> system
>>>>> > used is: GCS_WGS_1984. The net-cdf file contains monthly data from
>>>>> > 2001-2010
>>>>> >
>>>>> > Download from this website
>>>>> > <
>>>>> >
>>>>> http://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts
>>>>> > >
>>>>> > the
>>>>> > following ZIP file: *NUTS_2006_60M_SH.zip* and save all the
>>>>> > *NUTS_RG_60M_2006
>>>>> > files *in a folder where you can easily find the back. In what
>>>>> follows I
>>>>> > refer to this as "NUTS3".
>>>>> >
>>>>> > *WHAT I WANT*
>>>>> >
>>>>> > - I want to add the information from the raster files to the NUTS3
>>>>> > shapefile (which are polygons and no rasters) in order to obtain a
>>>>> table
>>>>> > per nuts3 region for each monthtly variable.
>>>>> >
>>>>> >
>>>>> > *WHERE I AM STUCK*
>>>>> >
>>>>> > The file appears to be very difficult to work with in ArcGis. Also,
>>>>> I will
>>>>> > have to repeat this process a number of times for different
>>>>> variables. So I
>>>>> > would like to have one code in R that I can run.
>>>>> >
>>>>> > I have a number of separate codes to do the following:
>>>>> > - translate the net-cdf file to a cvs file with longitude and
>>>>> latitude as
>>>>> > identifiers (see below under 1)
>>>>> > - translate a cvs file with accompanying empty raster file to nuts3
>>>>> > regions. (this is a code that I have used before when I had a cvs
>>>>> file and
>>>>> > a raster). (see below under 2).
>>>>> >
>>>>> > However, I don't have a raster file now. Well, technically I do
>>>>> (since the
>>>>> > net-ncf file is a raster) but I don't know how to use it in this
>>>>> format.
>>>>> >
>>>>> > Can somebody help me to link the codes below or suggest a different
>>>>> code to
>>>>> > obtain what I want?
>>>>> >
>>>>> > Thanks a lot!
>>>>> >
>>>>> > Janka
>>>>> >
>>>>> >
>>>>> >
>>>>> >
>>>>> > *1) With the following code, I can make a cvs file and extract all
>>>>> the data
>>>>> > in table format.*
>>>>> >
>>>>> > library(fields)
>>>>> > library(chron)
>>>>> >
>>>>> > library(ncdf4)
>>>>> > ncname<-"cru_ts3.22.2001.2010.pet.dat"
>>>>> > ncname<-"cru_ts3.23.1991.2000.pet.dat"
>>>>> > ncfname <- paste(ncname, ".nc", sep = "")
>>>>> > dname <- "pet"
>>>>> > ncin <- nc_open(ncfname)
>>>>> > print(ncin)
>>>>> >
>>>>> > lon <- ncvar_get(ncin, "lon")
>>>>> > nlon <- dim(lon)
>>>>> > head(lon)
>>>>> >
>>>>> > lat <- ncvar_get(ncin, "lat", verbose = F)
>>>>> > nlat <- dim(lat)
>>>>> > head(lat)
>>>>> >
>>>>> > print(c(nlon, nlat))
>>>>> >
>>>>> >
>>>>> > t <- ncvar_get(ncin, "time")
>>>>> > tunits <- ncatt_get(ncin, "time", "units")
>>>>> > nt <- dim(t)
>>>>> >
>>>>> > tmp.array <- ncvar_get(ncin, dname)
>>>>> > dlname <- ncatt_get(ncin, dname, "long_name")
>>>>> > dunits <- ncatt_get(ncin, dname, "units")
>>>>> > fillvalue <- ncatt_get(ncin, dname, "_FillValue")
>>>>> > dim(tmp.array)
>>>>> >
>>>>> > title <- ncatt_get(ncin, 0, "title")
>>>>> > institution <- ncatt_get(ncin, 0, "institution")
>>>>> > datasource <- ncatt_get(ncin, 0, "source")
>>>>> > references <- ncatt_get(ncin, 0, "references")
>>>>> > history <- ncatt_get(ncin, 0, "history")
>>>>> > Conventions <- ncatt_get(ncin, 0, "Conventions")
>>>>> >
>>>>> >
>>>>> >
>>>>> > nc_close(ncin)
>>>>> >
>>>>> > # split the time units string into fields
>>>>> > tustr <- strsplit(tunits$value, " ")
>>>>> > tdstr <- strsplit(unlist(tustr)[3], "-")
>>>>> > tmonth = as.integer(unlist(tdstr)[2])
>>>>> > tday = as.integer(unlist(tdstr)[3])
>>>>> > tyear = as.integer(unlist(tdstr)[1])
>>>>> > chron(t, origin = c(tmonth, tday, tyear))
>>>>> >
>>>>> >
>>>>> >
>>>>> > tmp.array[tmp.array == fillvalue$value] <- NA
>>>>> >
>>>>> > length(na.omit(as.vector(tmp.array[, , 1])))
>>>>> >
>>>>> > m <- 1
>>>>> > tmp.slice <- tmp.array[, , m]
>>>>> > library(RColorBrewer)
>>>>> > image(lon, lat, tmp.slice, col = rev(brewer.pal(10, "RdBu")))
>>>>> >
>>>>> > grid <- expand.grid(lon = lon, lat = lat)
>>>>> > cutpts <- c(-50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50)
>>>>> > levelplot(tmp.slice ~ lon * lat, data = grid, at = cutpts, cuts = 11,
>>>>> > pretty = T,
>>>>> >           col.regions = (rev(brewer.pal(10, "RdBu"))))
>>>>> >
>>>>> >
>>>>> > lonlat <- expand.grid(lon, lat)
>>>>> > tmp.vec <- as.vector(tmp.slice)
>>>>> > length(tmp.vec)
>>>>> >
>>>>> > tmp.df01 <- data.frame(cbind(lonlat, tmp.vec))
>>>>> > names(tmp.df01) <- c("lon", "lat", paste(dname, as.character(m), sep
>>>>> =
>>>>> > "_"))
>>>>> > head(na.omit(tmp.df01), 20)
>>>>> >
>>>>> > csvfile <- "cru_tmp_1.csv"
>>>>> > write.table(na.omit(tmp.df01), csvfile, row.names = FALSE, sep = ",")
>>>>> >
>>>>> >
>>>>> > tmp.vec.long <- as.vector(tmp.array)
>>>>> > length(tmp.vec.long)
>>>>> >
>>>>> > tmp.mat <- matrix(tmp.vec.long, nrow = nlon * nlat, ncol = nt)
>>>>> > dim(tmp.mat)
>>>>> >
>>>>> > head(na.omit(tmp.mat))
>>>>> >
>>>>> > lonlat <- expand.grid(lon, lat)
>>>>> > tmp.df02 <- data.frame(cbind(lonlat, tmp.mat))
>>>>> >
>>>>> > names(tmp.df02) <- c("lon","lat","pet_jan_2001",
>>>>> >                      "pet_feb_2001",
>>>>> >                      "pet_mar_2001",
>>>>> >                      "pet_apr_2001",
>>>>> >                      "pet_may_2001",
>>>>> >                      "pet_jun_2001",
>>>>> >                      "pet_jul_2001",
>>>>> >                      "pet_aug_2001",
>>>>> >                      "pet_sep_2001",
>>>>> >                      "pet_oct_2001",
>>>>> >                      "pet_nov_2001",
>>>>> >                      "pet_dec_2001",
>>>>> >                      "pet_jan_2002",
>>>>> >                      "pet_feb_2002",
>>>>> >                      "pet_mar_2002",
>>>>> >                      "pet_apr_2002",
>>>>> >                      "pet_may_2002",
>>>>> >                      "pet_jun_2002",
>>>>> >                      "pet_jul_2002",
>>>>> >                      "pet_aug_2002",
>>>>> >                      "pet_sep_2002",
>>>>> >                      "pet_oct_2002",
>>>>> >                      "pet_nov_2002",
>>>>> >                      "pet_dec_2002",
>>>>> >                      "pet_jan_2003",
>>>>> >                      "pet_feb_2003",
>>>>> >                      "pet_mar_2003",
>>>>> >                      "pet_apr_2003",
>>>>> >                      "pet_may_2003",
>>>>> >                      "pet_jun_2003",
>>>>> >                      "pet_jul_2003",
>>>>> >                      "pet_aug_2003",
>>>>> >                      "pet_sep_2003",
>>>>> >                      "pet_oct_2003",
>>>>> >                      "pet_nov_2003",
>>>>> >                      "pet_dec_2003",
>>>>> >                      "pet_jan_2004",
>>>>> >                      "pet_feb_2004",
>>>>> >                      "pet_mar_2004",
>>>>> >                      "pet_apr_2004",
>>>>> >                      "pet_may_2004",
>>>>> >                      "pet_jun_2004",
>>>>> >                      "pet_jul_2004",
>>>>> >                      "pet_aug_2004",
>>>>> >                      "pet_sep_2004",
>>>>> >                      "pet_oct_2004",
>>>>> >                      "pet_nov_2004",
>>>>> >                      "pet_dec_2004",
>>>>> >                      "pet_jan_2005",
>>>>> >                      "pet_feb_2005",
>>>>> >                      "pet_mar_2005",
>>>>> >                      "pet_apr_2005",
>>>>> >                      "pet_may_2005",
>>>>> >                      "pet_jun_2005",
>>>>> >                      "pet_jul_2005",
>>>>> >                      "pet_aug_2005",
>>>>> >                      "pet_sep_2005",
>>>>> >                      "pet_oct_2005",
>>>>> >                      "pet_nov_2005",
>>>>> >                      "pet_dec_2005",
>>>>> >                      "pet_jan_2006",
>>>>> >                      "pet_feb_2006",
>>>>> >                      "pet_mar_2006",
>>>>> >                      "pet_apr_2006",
>>>>> >                      "pet_may_2006",
>>>>> >                      "pet_jun_2006",
>>>>> >                      "pet_jul_2006",
>>>>> >                      "pet_aug_2006",
>>>>> >                      "pet_sep_2006",
>>>>> >                      "pet_oct_2006",
>>>>> >                      "pet_nov_2006",
>>>>> >                      "pet_dec_2006",
>>>>> >                      "pet_jan_2007",
>>>>> >                      "pet_feb_2007",
>>>>> >                      "pet_mar_2007",
>>>>> >                      "pet_apr_2007",
>>>>> >                      "pet_may_2007",
>>>>> >                      "pet_jun_2007",
>>>>> >                      "pet_jul_2007",
>>>>> >                      "pet_aug_2007",
>>>>> >                      "pet_sep_2007",
>>>>> >                      "pet_oct_2007",
>>>>> >                      "pet_nov_2007",
>>>>> >                      "pet_dec_2007",
>>>>> >                      "pet_jan_2008",
>>>>> >                      "pet_feb_2008",
>>>>> >                      "pet_mar_2008",
>>>>> >                      "pet_apr_2008",
>>>>> >                      "pet_may_2008",
>>>>> >                      "pet_jun_2008",
>>>>> >                      "pet_jul_2008",
>>>>> >                      "pet_aug_2008",
>>>>> >                      "pet_sep_2008",
>>>>> >                      "pet_oct_2008",
>>>>> >                      "pet_nov_2008",
>>>>> >                      "pet_dec_2008",
>>>>> >                      "pet_jan_2009",
>>>>> >                      "pet_feb_2009",
>>>>> >                      "pet_mar_2009",
>>>>> >                      "pet_apr_2009",
>>>>> >                      "pet_may_2009",
>>>>> >                      "pet_jun_2009",
>>>>> >                      "pet_jul_2009",
>>>>> >                      "pet_aug_2009",
>>>>> >                      "pet_sep_2009",
>>>>> >                      "pet_oct_2009",
>>>>> >                      "pet_nov_2009",
>>>>> >                      "pet_dec_2009",
>>>>> >                      "pet_jan_2010",
>>>>> >                      "pet_feb_2010",
>>>>> >                      "pet_mar_2010",
>>>>> >                      "pet_apr_2010",
>>>>> >                      "pet_may_2010",
>>>>> >                      "pet_jun_2010",
>>>>> >                      "pet_jul_2010",
>>>>> >                      "pet_aug_2010",
>>>>> >                      "pet_sep_2010",
>>>>> >                      "pet_oct_2010",
>>>>> >                      "pet_nov_2010",
>>>>> >                      "pet_dec_2010")
>>>>> >
>>>>> >
>>>>> > options(width = 110)
>>>>> > head(na.omit(tmp.df02, 20))
>>>>> >
>>>>> > dim(na.omit(tmp.df02))
>>>>> >
>>>>> > csvfile <- "cru_tmp_2.csv"
>>>>> > write.table(na.omit(tmp.df02), csvfile, row.names = FALSE, sep = ",")
>>>>> >
>>>>> >
>>>>> >
>>>>> > *2) translate a cvs-file with accompanying raster file to polygon
>>>>> regions.*
>>>>> >
>>>>> > The  "filename.txt" file should contain the variables: lon,
>>>>> latitude, and
>>>>> > all the monthly_yearly variables extracted from point 1 above.
>>>>> >
>>>>> > The grid shapefile (*grid_025dd.shp*) can be found through the
>>>>> following
>>>>> > link but it is only an example and not the correct grid for the
>>>>> problem
>>>>> > above :
>>>>> >
>>>>> >
>>>>> https://drive.google.com/folderview?id=0By9u5m3kxn9yfjZtdFZLcW82SWpzT1VwZXE1a3FtRGtSdEl1c1NvY205TGpack9xSFc2T2s&usp=sharing
>>>>> >
>>>>> > # upload data
>>>>> > mydata<-read.table("filename.txt", header=TRUE,sep=",",dec=".")
>>>>> >
>>>>> >
>>>>> > # upload empty raster
>>>>> > library(rgdal)
>>>>> > # 40 seconds
>>>>> > grid <- readOGR(".", layer = "grid_025dd")
>>>>> >
>>>>> >
>>>>> > # concatenate data in R
>>>>> > # 2 seconds
>>>>> > mydata$lonlat<-do.call(paste, c(mydata[c("lon", "lat")], sep=""))
>>>>> > grid at data$lonlat<-do.call(paste, c(grid at data[c("LONGITUDE",
>>>>> "LATITUDE")],
>>>>> > sep=""))
>>>>> >
>>>>> > # use common variable lonlat to merge data in raster
>>>>> >
>>>>> > ###### prepare shapefile #####
>>>>> > library(rgdal)        ## Load geographic info
>>>>> > library(maps)         ## Projections
>>>>> > library(maptools)     ## Data management
>>>>> > #library(sm)           ## Data management
>>>>> > library(spdep)        ## Spatial autocorrelation
>>>>> > library(gstat)        ## Geostatistics
>>>>> > library(splancs)      ## Kernel Density
>>>>> > library(spatstat)     ## Geostatistics
>>>>> > library(pgirmess)     ## Spatial autocorrelation
>>>>> > library(RColorBrewer) ## Visualization
>>>>> > library(classInt)     ## Class intervals
>>>>> > library(spgwr)        ## GWR
>>>>> >
>>>>> > # Match polygons with data
>>>>> > idx <- match(grid$lonlat, mydata$lonlat)
>>>>> > # Places without information
>>>>> > idxNA <- which(is.na(idx))
>>>>> > # Information to be added to the SpatialPolygons object
>>>>> > dat2add <- mydata[idx, ]
>>>>> > # spCbind uses row names to match polygons with data
>>>>> > # First, extract polygon IDs
>>>>> > IDs <- sapply(grid at polygons, function(x)x at ID)
>>>>> > # and join with the SpatialPolygons
>>>>> > row.names(dat2add) <- IDs
>>>>> > datPols <- spCbind(grid, dat2add)
>>>>> > # Drop those places without information
>>>>> > datPols <- datPols[-idxNA, ]
>>>>> > # write new shapefile
>>>>> > # 7 seconds
>>>>> > writeOGR(datPols, dsn = ".", layer ='sm2000eu28', driver = 'ESRI
>>>>> > Shapefile')
>>>>> > # read new shapefile
>>>>> > # 51 seconds
>>>>> > data <- readOGR(".", layer="sm2000eu28")
>>>>> >
>>>>> > ############################
>>>>> > # intersect nuts with grid #
>>>>> > ############################
>>>>> >
>>>>> > library(rgdal)
>>>>> > nuts <- readOGR(".", layer = "NUTS_RG_60M_2006")
>>>>> >
>>>>> > library(rgeos)
>>>>> > proj4string(data) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
>>>>> >
>>>>> >
>>>>> > #
>>>>> > grid <- data
>>>>> > grid at data$lonlat <- NULL
>>>>> > grid at data$lonlat_1 <- NULL
>>>>> > grid at data$ID <- NULL
>>>>> > grid at data$lat <- NULL
>>>>> > grid at data$lon <- NULL
>>>>> > grid at data$ELEVATION <- NULL
>>>>> > grid at data$DAYS_RAIN_ <- NULL
>>>>> >
>>>>> >
>>>>> > # First find out which grid cells intersect your NUTS polygons
>>>>> > grid_nuts <- gIntersects(grid,nuts,byid = TRUE)
>>>>> >
>>>>> > # use the apply() function to calculate the mean, min, and max of
>>>>> your
>>>>> > value.
>>>>> > # The loop makes
>>>>> >
>>>>> > for(i in names(grid at data)){
>>>>> >   nuts at data[[paste(i, 'average_value', sep="_")]] <-
>>>>> > apply(grid_nuts,1,function(x) mean(grid at data[[i]][x]))
>>>>> >   nuts at data[[paste(i, 'min_value', sep="_")]] <-
>>>>> > apply(grid_nuts,1,function(x) min(grid at data[[i]][x]))
>>>>> >   nuts at data[[paste(i, 'max_value', sep="_")]] <-
>>>>> > apply(grid_nuts,1,function(x) max(grid at data[[i]][x]))
>>>>> > }
>>>>> >
>>>>> > write.table(nuts at data, "nuts_sm2000eu28_unweighted.txt", sep="\t")
>>>>> >
>>>>> >         [[alternative HTML version deleted]]
>>>>> >
>>>>> > _______________________________________________
>>>>> > R-sig-Geo mailing list
>>>>> > R-sig-Geo at r-project.org
>>>>> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>> >
>>>>> >
>>>>> > We value your opinion. Please take a few minutes to share your
>>>>> comments on
>>>>> > the service you received from the District by clicking on this link<
>>>>> >
>>>>> http://my.sfwmd.gov/portal/page/portal/pg_grp_surveysystem/survey%20ext?pid=1653
>>>>> > >.
>>>>> >
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>>
>>>>> [image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
>>>>> *Doctoraatsbursaal - PhD *
>>>>> Milieueconomie - Environmental economics
>>>>>
>>>>> T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40
>>>>>
>>>>> www.uhasselt.be/eec
>>>>>
>>>>> Universiteit Hasselt | Campus Diepenbeek
>>>>> Agoralaan Gebouw D | B-3590 Diepenbeek
>>>>> Kantoor F11
>>>>>
>>>>> Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt
>>>>>
>>>>>
>>>>> [image: Music For Life]  Maak van UHasselt de #warmsteunief |
>>>>> www.uhasselt.be/musicforlife
>>>>>
>>>>>
>>>>> P Please consider the environment before printing this e-mail
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________
>>>>> R-sig-Geo mailing list
>>>>> R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>>
>>>> --
>>>> Dr. Michael Sumner
>>>> Software and Database Engineer
>>>> Australian Antarctic Division
>>>> 203 Channel Highway
>>>> Kingston Tasmania 7050 Australia
>>>>
>>>>
>>>
>>>
>>> --
>>>
>>> [image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
>>> *Doctoraatsbursaal - PhD *
>>> Milieueconomie - Environmental economics
>>>
>>> T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40
>>>
>>> www.uhasselt.be/eec
>>>
>>> Universiteit Hasselt | Campus Diepenbeek
>>> Agoralaan Gebouw D | B-3590 Diepenbeek
>>> Kantoor F11
>>>
>>> Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt
>>>
>>>
>>> [image: Music For Life]  Maak van UHasselt de #warmsteunief |
>>> www.uhasselt.be/musicforlife
>>>
>>>
>>> P Please consider the environment before printing this e-mail
>>>
>>>
>>
>>
>> --
>>
>> [image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
>> *Doctoraatsbursaal - PhD *
>> Milieueconomie - Environmental economics
>>
>> T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40
>>
>> www.uhasselt.be/eec
>>
>> Universiteit Hasselt | Campus Diepenbeek
>> Agoralaan Gebouw D | B-3590 Diepenbeek
>> Kantoor F11
>>
>> Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt
>>
>>
>> [image: Music For Life]  Maak van UHasselt de #warmsteunief |
>> www.uhasselt.be/musicforlife
>>
>>
>> P Please consider the environment before printing this e-mail
>>
>> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
>


-- 

[image: Logo UHasselt] Mevrouw Janka Vanschoenwinkel
*Doctoraatsbursaal - PhD *
Milieueconomie - Environmental economics

T +32(0)11 26 86 96 | GSM +32(0)476 28 21 40

www.uhasselt.be/eec

Universiteit Hasselt | Campus Diepenbeek
Agoralaan Gebouw D | B-3590 Diepenbeek
Kantoor F11

Postadres: Universiteit Hasselt | Martelarenlaan 42 | B-3500 Hasselt


[image: Music For Life]  Maak van UHasselt de #warmsteunief |
www.uhasselt.be/musicforlife


P Please consider the environment before printing this e-mail

	[[alternative HTML version deleted]]


From afitz at email.wm.edu  Mon Jun 13 18:05:59 2016
From: afitz at email.wm.edu (Alex Fitz)
Date: Mon, 13 Jun 2016 12:05:59 -0400
Subject: [R-sig-Geo] Kernel Density Estimator Help
Message-ID: <D4C3CDE2-7492-4DA8-B63D-128F451D4D1C@email.wm.edu>

Hi,

I?m running across an issue with my code while trying to use a Kernel Density Estimator to cluster points. I am working with my data in two different ways trying to find the most optimal.  First, I have my data in the form of a matrix. An example of this is below, and I have my latitude and longitude in my code attached to the columns and rows in the matrix.  

m <- c(
  c(8.83,8.89,8.81,8.87,8.9,8.87),
  c(8.89,8.94,8.85,8.94,8.96,8.92),
  c(8.84,8.9,8.82,8.92,8.93,8.91),
  c(8.79,8.85,8.79,8.9,8.94,8.92),
  c(8.79,8.88,8.81,8.9,8.95,8.92),
  c(8.8,8.82,8.78,8.91,8.94,8.92),
  c(8.75,8.78,8.77,8.91,8.95,8.92),
  c(8.8,8.8,8.77,8.91,8.95,8.94),
  c(8.74,8.81,8.76,8.93,8.98,8.99),
  c(8.89,8.99,8.92,9.1,9.13,9.11),
  c(8.97,8.97,8.91,9.09,9.11,9.11),
  c(9.04,9.08,9.05,9.25,9.28,9.27),
  c(9,9.01,9,9.2,9.23,9.2),
  c(8.99,8.99,8.98,9.18,9.2,9.19),
  c(8.93,8.97,8.97,9.18,9.2,9.18)
)
dim(m) <- c(15,6)

I also have my data in a data table where column 1 is my latitude, column 2 is my longitude, and column 3 is the value.

z <- c(
  c(8.83,8.89, 2),
  c(8.89,8.94, 4),
  c(8.84,8.9, 1),
  c(8.79,8.852, 4),
  c(8.79,8.88, 5),
  c(8.8,8.82, 2),
  c(8.75,8.78, 1),
  c(8.8,8.8, 2),
  c(8.74,8.81, 7),
  c(8.89,8.99, 1),
  c(8.97,8.97, 6),
  c(9.04,9.08, 8),
  c(9,9.01, 1),
  c(8.99,8.99, 8),
  c(8.93,8.97, 2)
)
dim(z) <- c(15,3)

The actual data I am using is from larger rasters and shapefiles. 
The raster is fromhttp://beta.sedac.ciesin.columbia.edu/data/set/gpw-v4-population-count/data-download <fromhttp://beta.sedac.ciesin.columbia.edu/data/set/gpw-v4-population-count/data-download>. 
And the shapefiles are from  http://www.gadm.org/download <http://www.gadm.org/download>  ? I am using Nigeria.

The main question of this post is clustering and the optimal data format for clustering functions.  I currently have all of the grid points of the entire country with their (Lat, Long, Value).  I want to run a Kernel Density Estimator across all of the points and then cluster based on certain values.  Looking at the pdfCluster package it seems to do just that except i?m not sure how to allow it to accept (lat/long) values and run across a geographic plane.  Since my data is across a geographic area and isn?t completely continuous i?m running in to errors.  Any hints for how to modify the pdfCluster package for accepting such values or what dataset is best to use would be greatly appreciated.

Thanks,
Alex


	[[alternative HTML version deleted]]


From DanielTurenne at hotmail.com  Wed Jun 15 20:00:38 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Wed, 15 Jun 2016 18:00:38 +0000
Subject: [R-sig-Geo] Using GStat for kriging with Anisotropy
Message-ID: <BLUPR12MB0625F4E6A2467B51F05A0E78B7550@BLUPR12MB0625.namprd12.prod.outlook.com>

Hello,


I am trying to use kriging to make temperature predictions but I have run into a slight issue.  I would like to include anisotropy in my variogram model but GStat does not support anisotropy when coordinates are given in longitude and latitude.  Here is a example of what my code looks like, where dailyData is a data frame containing long/lat coordinates and the mean temperature data:



    coordinates(dailyData) = ~long+lat

    proj4string(sp) = CRS("+init=epsg:4326")


    anisParams=estimateAnisotropy(dailyData,"meanTemp")

    sample.variogram=variogram(meanTemp~1,dailyData)


    sillEst=var(dailyData$meanTemp)

    nuggetEst=0

    rangeEst=200


    anis.variogram=vgm(nugget=nuggetEst, model="Exp", range=rangeEst, psill=sillEst,

                                          anis=c(anisParams$direction,1/anisParams$ratio))

    anis.fitVariogram=fit.variogram(sample.variogram,anis.variogram)


    surface=krige(meanTemp~1,locations=dailyData,newdata=fData,model=anis.fitVariogram)



However running this code produces the following error:


    Error in predict.gstat(g, newdata = newdata, block = block, nsim = nsim,  :
    value not allowed for: nnot be defined for long/lat data, anisotropy cannot be defined


I know that the problem is that GStat can't model anisotropy while using great circle distances, but I am wondering if there is a workaround that exists which allows anisotropy to be modelled while using long/lat coordinates?  All of my long/lat data is in decimal format (i.e lat=45.62, long=-75.48).  I have searched through the R-Sig-Geo mailing list and have seen this issue mentioned but I have not been able to find a solution that works for my code.  Any help on this subject would be greatly appreciated.


Thank you in advance,


Daniel Turenne

University of Manitoba

	[[alternative HTML version deleted]]


From timbean at gmail.com  Wed Jun 15 22:41:35 2016
From: timbean at gmail.com (Tim Bean)
Date: Wed, 15 Jun 2016 13:41:35 -0700
Subject: [R-sig-Geo] Problems with gbif function in package dismo
Message-ID: <CAPhRc8eATP6_uP819Cx-COJkGj9613vADTnS8ZA1qVd5UHSFvw@mail.gmail.com>

Hello r-sig-geo,
The gbif function in package dismo does not appear to be functioning. No
matter what genus/species combination I try, the function finds 0 records.
For example:

sp.occur <- gbif(genus='Erethizon', species='', geo=TRUE)

Returns "0 records found."

I've tried to go through the underlying code to generate the gbif API call,
for example:

http://api.gbif.org/v1/occurrence/search?scientificname=Erethizon+dorsatum&limit=1

which returns:

{"offset":0,"limit":1,"endOfRecords":true,"count":0,"results":[],"facets":[]}

The GBIF API documentation suggests that perhaps that should be:

http://api.gbif.org/v1/occurrence/search?scientificName=Erethizon+dorsatum&limit=1

(note "scientificName" with capital 'N'), but I get no results with that
either.

However, identifying the genusKey:

http://api.gbif.org/v1/species/match?genus=Erethizon

which returns genusKey=2437593

and using that to search occurrence records:

http://api.gbif.org/v1/occurrence/search?genusKey=2437593

produces the expected results. Is anybody else having problems with the
dismo function? Does anybody know if GBIF has updated their API? Am I
missing something obvious?

Thanks,
Tim

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Wed Jun 15 22:49:42 2016
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 15 Jun 2016 22:49:42 +0200
Subject: [R-sig-Geo] Using GStat for kriging with Anisotropy
In-Reply-To: <BLUPR12MB0625F4E6A2467B51F05A0E78B7550@BLUPR12MB0625.namprd12.prod.outlook.com>
References: <BLUPR12MB0625F4E6A2467B51F05A0E78B7550@BLUPR12MB0625.namprd12.prod.outlook.com>
Message-ID: <5761BF66.6030006@uni-muenster.de>

Dan, have you considered projecting your data to Carthesian coordinates
(using spTransform), doing the modelling and interpolation there, and
then back-transform results to long/lat?

On 15/06/16 20:00, Dan Turenne wrote:
> Hello,
> 
> 
> I am trying to use kriging to make temperature predictions but I have run into a slight issue.  I would like to include anisotropy in my variogram model but GStat does not support anisotropy when coordinates are given in longitude and latitude.  Here is a example of what my code looks like, where dailyData is a data frame containing long/lat coordinates and the mean temperature data:
> 
> 
> 
>     coordinates(dailyData) = ~long+lat
> 
>     proj4string(sp) = CRS("+init=epsg:4326")
> 
> 
>     anisParams=estimateAnisotropy(dailyData,"meanTemp")
> 
>     sample.variogram=variogram(meanTemp~1,dailyData)
> 
> 
>     sillEst=var(dailyData$meanTemp)
> 
>     nuggetEst=0
> 
>     rangeEst=200
> 
> 
>     anis.variogram=vgm(nugget=nuggetEst, model="Exp", range=rangeEst, psill=sillEst,
> 
>                                           anis=c(anisParams$direction,1/anisParams$ratio))
> 
>     anis.fitVariogram=fit.variogram(sample.variogram,anis.variogram)
> 
> 
>     surface=krige(meanTemp~1,locations=dailyData,newdata=fData,model=anis.fitVariogram)
> 
> 
> 
> However running this code produces the following error:
> 
> 
>     Error in predict.gstat(g, newdata = newdata, block = block, nsim = nsim,  :
>     value not allowed for: nnot be defined for long/lat data, anisotropy cannot be defined
> 
> 
> I know that the problem is that GStat can't model anisotropy while using great circle distances, but I am wondering if there is a workaround that exists which allows anisotropy to be modelled while using long/lat coordinates?  All of my long/lat data is in decimal format (i.e lat=45.62, long=-75.48).  I have searched through the R-Sig-Geo mailing list and have seen this issue mentioned but I have not been able to find a solution that works for my code.  Any help on this subject would be greatly appreciated.
> 
> 
> Thank you in advance,
> 
> 
> Daniel Turenne
> 
> University of Manitoba
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20160615/efbea0d7/attachment.bin>

From DanielTurenne at hotmail.com  Thu Jun 16 14:53:55 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Thu, 16 Jun 2016 12:53:55 +0000
Subject: [R-sig-Geo] Using GStat for kriging with Anisotropy
In-Reply-To: <5761BF66.6030006@uni-muenster.de>
References: <BLUPR12MB0625F4E6A2467B51F05A0E78B7550@BLUPR12MB0625.namprd12.prod.outlook.com>,
	<5761BF66.6030006@uni-muenster.de>
Message-ID: <BLUPR12MB06259C7AE12E9CC149D3ABDCB7560@BLUPR12MB0625.namprd12.prod.outlook.com>

Hello,


I had considered this possibility but I was concerned about the distortion of distances.  If the coordinates are being changed from a projection on a sphere to a 2 dimensional Cartesian plane won't the distances between points be affected?  The area that I am interpolating across is fairly large and has a diagonal of roughly 850 km.  I apologize if these are basic questions, I am writing my thesis for my master's degree in actuarial science  and this project is my first exposure to the field of geostatistics.


Many thanks,

Daniel Turenne



________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Edzer Pebesma <edzer.pebesma at uni-muenster.de>
Sent: June 15, 2016 3:49 PM
To: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Using GStat for kriging with Anisotropy

Dan, have you considered projecting your data to Carthesian coordinates
(using spTransform), doing the modelling and interpolation there, and
then back-transform results to long/lat?

On 15/06/16 20:00, Dan Turenne wrote:
> Hello,
>
>
> I am trying to use kriging to make temperature predictions but I have run into a slight issue.  I would like to include anisotropy in my variogram model but GStat does not support anisotropy when coordinates are given in longitude and latitude.  Here is a example of what my code looks like, where dailyData is a data frame containing long/lat coordinates and the mean temperature data:
>
>
>
>     coordinates(dailyData) = ~long+lat
>
>     proj4string(sp) = CRS("+init=epsg:4326")
>
>
>     anisParams=estimateAnisotropy(dailyData,"meanTemp")
>
>     sample.variogram=variogram(meanTemp~1,dailyData)
>
>
>     sillEst=var(dailyData$meanTemp)
>
>     nuggetEst=0
>
>     rangeEst=200
>
>
>     anis.variogram=vgm(nugget=nuggetEst, model="Exp", range=rangeEst, psill=sillEst,
>
>                                           anis=c(anisParams$direction,1/anisParams$ratio))
>
>     anis.fitVariogram=fit.variogram(sample.variogram,anis.variogram)
>
>
>     surface=krige(meanTemp~1,locations=dailyData,newdata=fData,model=anis.fitVariogram)
>
>
>
> However running this code produces the following error:
>
>
>     Error in predict.gstat(g, newdata = newdata, block = block, nsim = nsim,  :
>     value not allowed for: nnot be defined for long/lat data, anisotropy cannot be defined
>
>
> I know that the problem is that GStat can't model anisotropy while using great circle distances, but I am wondering if there is a workaround that exists which allows anisotropy to be modelled while using long/lat coordinates?  All of my long/lat data is in decimal format (i.e lat=45.62, long=-75.48).  I have searched through the R-Sig-Geo mailing list and have seen this issue mentioned but I have not been able to find a solution that works for my code.  Any help on this subject would be greatly appreciated.
>
>
> Thank you in advance,
>
>
> Daniel Turenne
>
> University of Manitoba
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
R-sig-Geo - R Special Interest Group on using Geographical ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
stat.ethz.ch
R-sig-Geo -- R Special Interest Group on using Geographical data and Mapping About R-sig-Geo



>

--
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info


	[[alternative HTML version deleted]]


From timbean at gmail.com  Thu Jun 16 19:45:11 2016
From: timbean at gmail.com (Tim Bean)
Date: Thu, 16 Jun 2016 10:45:11 -0700
Subject: [R-sig-Geo] Problems with gbif function in package dismo
In-Reply-To: <CABkCotTnZc-p4RnacZKNzpTbbAmLCqDngcXGydX922CCDvZ_3A@mail.gmail.com>
References: <CAPhRc8eATP6_uP819Cx-COJkGj9613vADTnS8ZA1qVd5UHSFvw@mail.gmail.com>
	<CABkCotTnZc-p4RnacZKNzpTbbAmLCqDngcXGydX922CCDvZ_3A@mail.gmail.com>
Message-ID: <CAPhRc8cuv84qNYBArY+NJabvr=M_sOwcem8EpwwgyZp3UjtWXw@mail.gmail.com>

Thanks for the quick response. Update: GBFI has fixed the API bug and the
function is now working as expected.

On Wed, Jun 15, 2016 at 2:40 PM, Manuel Sp?nola <mspinola10 at gmail.com>
wrote:

> Dear Tim,
>
> It could be an error in your code, you wrote:
>
> sp.occur <- gbif(genus='Erethizon', species='', geo=TRUE)
>
> appear that you have a missing quote after species=
>
> Manuel
>
> 2016-06-15 14:41 GMT-06:00 Tim Bean <timbean at gmail.com>:
>
>> Hello r-sig-geo,
>> The gbif function in package dismo does not appear to be functioning. No
>> matter what genus/species combination I try, the function finds 0 records.
>> For example:
>>
>> sp.occur <- gbif(genus='Erethizon', species='', geo=TRUE)
>>
>> Returns "0 records found."
>>
>> I've tried to go through the underlying code to generate the gbif API
>> call,
>> for example:
>>
>>
>> http://api.gbif.org/v1/occurrence/search?scientificname=Erethizon+dorsatum&limit=1
>>
>> which returns:
>>
>>
>> {"offset":0,"limit":1,"endOfRecords":true,"count":0,"results":[],"facets":[]}
>>
>> The GBIF API documentation suggests that perhaps that should be:
>>
>>
>> http://api.gbif.org/v1/occurrence/search?scientificName=Erethizon+dorsatum&limit=1
>>
>> (note "scientificName" with capital 'N'), but I get no results with that
>> either.
>>
>> However, identifying the genusKey:
>>
>> http://api.gbif.org/v1/species/match?genus=Erethizon
>>
>> which returns genusKey=2437593
>>
>> and using that to search occurrence records:
>>
>> http://api.gbif.org/v1/occurrence/search?genusKey=2437593
>>
>> produces the expected results. Is anybody else having problems with the
>> dismo function? Does anybody know if GBIF has updated their API? Am I
>> missing something obvious?
>>
>> Thanks,
>> Tim
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
>
>
> --
> *Manuel Sp?nola, Ph.D.*
> Instituto Internacional en Conservaci?n y Manejo de Vida Silvestre
> Universidad Nacional
> Apartado 1350-3000
> Heredia
> COSTA RICA
> mspinola at una.cr <mspinola at una.ac.cr>
> mspinola10 at gmail.com
> Tel?fono: (506) 8706 - 4662
> Personal website: Lobito de r?o
> <https://sites.google.com/site/lobitoderio/>
> Institutional website: ICOMVIS <http://www.icomvis.una.ac.cr/>
>

	[[alternative HTML version deleted]]


From twah at gmx.ch  Fri Jun 17 22:05:25 2016
From: twah at gmx.ch (twah at gmx.ch)
Date: Fri, 17 Jun 2016 22:05:25 +0200
Subject: [R-sig-Geo] plotKML - html.table description block
Message-ID: <57645805.2050800@gmx.ch>

Hi

I would like to use the plotKML package to create a KML file for google earth. Specifically, I intend to display several attributes of
a spatial points dataframe. How do I define the argument "html.table" as in the function "kml_layer.SpatialPoints" in  order to display
a description block whenever I click a specific spatial point icon on the map. I have no clue how to do this. I assume I may have
to write some html code. Can someone help?

Thanks


From malNamalJa at gmx.de  Sun Jun 19 09:18:12 2016
From: malNamalJa at gmx.de (=?UTF-8?Q?=22Jannes_M=C3=BCnchow=22?=)
Date: Sun, 19 Jun 2016 09:18:12 +0200
Subject: [R-sig-Geo] RQGIS - integrating R with QGIS
Message-ID: <trinity-5ece0871-bdfe-438c-80cd-59f7e12e1d83-1466320692648@3capp-gmx-bs46>

Dear R-GIS users,
?
I would like to draw your attention to a new package integrating R with QGIS. QGIS is probably the most widely used open-source Desktop GIS. Apart from its native functions QGIS provides access to the geoalgorithms of various third-party providers (among others SAGA and GRASS). In total this gives the user access to more than 1000 geoalgorithms. RQGIS brings this incredible powerful geoprocessing environment to the R console. We would like to publish RQGIS on CRAN within the next six weeks. Until then we would appreciate any user feedback. On github (https://github.com/jannes-m/RQGIS) you'll find a short tour how to use and install the developer version of RQGIS and in this post (https://jannesm.wordpress.com/2016/06/03/integrating-r-with-qgis/) I briefly describe the most notable features of RQGIS. I am looking forward to hearing from first user experiences!
?
Best regards,
?
Jannes


From Rainer at krugs.de  Sun Jun 19 11:29:30 2016
From: Rainer at krugs.de (Rainer M Krug)
Date: Sun, 19 Jun 2016 11:29:30 +0200
Subject: [R-sig-Geo] RQGIS - integrating R with QGIS
References: <trinity-5ece0871-bdfe-438c-80cd-59f7e12e1d83-1466320692648@3capp-gmx-bs46>
Message-ID: <m2d1ndttl1.fsf@krugs.de>

"Jannes M?nchow" <malNamalJa at gmx.de> writes:

> Dear R-GIS users,
> ?
> I would like to draw your attention to a new package integrating R
> with QGIS. QGIS is probably the most widely used open-source Desktop
> GIS. Apart from its native functions QGIS provides access to the
> geoalgorithms of various third-party providers (among others SAGA and
> GRASS). In total this gives the user access to more than 1000
> geoalgorithms. RQGIS brings this incredible powerful geoprocessing
> environment to the R console. We would like to publish RQGIS on CRAN
> within the next six weeks. Until then we would appreciate any user
> feedback. On github (https://github.com/jannes-m/RQGIS) you'll find a
> short tour how to use and install the developer version of RQGIS and
> in this post
> (https://jannesm.wordpress.com/2016/06/03/integrating-r-with-qgis/) I
> briefly describe the most notable features of RQGIS. I am looking
> forward to hearing from first user experiences!

Hi

This looks really nice. Thanks.

Just tried the examples on https://github.com/jannes-m/RQGIS on OS X El
Capitan with QGIS 2-8 installed using homebrew ant they worked after
specifying the path to root. But there is an error in the RQGIS usage
site:

It states:

,----
| If you are running RQGIS under Linux or on a Mac, set_env assumes that
| your root path is "/usr" and "/applications/QGIS.app/Contents",
| respectively.
`----

1) The path should be changed to "Applications" instead of
"applications" as the capital A is used in OS X. It normally does not
make a difference, but does not follow conventions.

2) The path (specified in root=...) most not contain "/Contents" but
only the path to the .app folder.

In general, it would be nice if a cal to set_env() could have some
logic to also identify homebrew installesd versions of QGIS.


Thanks,

Rainer


> ?
> Best regards,
> ?
> Jannes
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 454 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20160619/571125aa/attachment.bin>

From kmb56 at berkeley.edu  Sun Jun 19 21:42:44 2016
From: kmb56 at berkeley.edu (Kenny Bell)
Date: Sun, 19 Jun 2016 12:42:44 -0700
Subject: [R-sig-Geo] RQGIS - integrating R with QGIS
In-Reply-To: <trinity-5ece0871-bdfe-438c-80cd-59f7e12e1d83-1466320692648@3capp-gmx-bs46>
References: <trinity-5ece0871-bdfe-438c-80cd-59f7e12e1d83-1466320692648@3capp-gmx-bs46>
Message-ID: <CALOjXYRJBAXEJMN9F-GnDJWZQWgjN-mGV71BZV7G6rRhPg979g@mail.gmail.com>

It would be very useful to provide a mapping between the operations we have
access to in rgeos/raster/sp with those from this package. It would help a
lot with us checking it out!

Thanks so much for this. Looks like it's going to be a big contribution.
On Jun 19, 2016 3:18 AM, Jannes M?nchow <malNamalJa at gmx.de> wrote:

> Dear R-GIS users,
>
> I would like to draw your attention to a new package integrating R with
> QGIS. QGIS is probably the most widely used open-source Desktop GIS. Apart
> from its native functions QGIS provides access to the geoalgorithms of
> various third-party providers (among others SAGA and GRASS). In total this
> gives the user access to more than 1000 geoalgorithms. RQGIS brings this
> incredible powerful geoprocessing environment to the R console. We would
> like to publish RQGIS on CRAN within the next six weeks. Until then we
> would appreciate any user feedback. On github (
> https://github.com/jannes-m/RQGIS) you'll find a short tour how to use
> and install the developer version of RQGIS and in this post (
> https://jannesm.wordpress.com/2016/06/03/integrating-r-with-qgis/) I
> briefly describe the most notable features of RQGIS. I am looking forward
> to hearing from first user experiences!
>
> Best regards,
>
> Jannes
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From b.rowlingson at lancaster.ac.uk  Sun Jun 19 23:35:43 2016
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Sun, 19 Jun 2016 22:35:43 +0100
Subject: [R-sig-Geo] RQGIS - integrating R with QGIS
In-Reply-To: <trinity-5ece0871-bdfe-438c-80cd-59f7e12e1d83-1466320692648@3capp-gmx-bs46>
References: <trinity-5ece0871-bdfe-438c-80cd-59f7e12e1d83-1466320692648@3capp-gmx-bs46>
Message-ID: <CANVKczPREDKaZYZdLBsKK1ZCkow14xhfozKrgqC2xMg7-zDkWQ@mail.gmail.com>

Jannes,

 did you consider using the rPython package? Its easy enough to import
qgis functionality, there's less overhead since you aren't spawning a
new python/qgis session for every command, you can save state between
python calls, and you hand-off the platform dependency to that
package.

Barry

On Sun, Jun 19, 2016 at 8:18 AM, "Jannes M?nchow" <malNamalJa at gmx.de> wrote:
> Dear R-GIS users,
>
> I would like to draw your attention to a new package integrating R with QGIS. QGIS is probably the most widely used open-source Desktop GIS. Apart from its native functions QGIS provides access to the geoalgorithms of various third-party providers (among others SAGA and GRASS). In total this gives the user access to more than 1000 geoalgorithms. RQGIS brings this incredible powerful geoprocessing environment to the R console. We would like to publish RQGIS on CRAN within the next six weeks. Until then we would appreciate any user feedback. On github (https://github.com/jannes-m/RQGIS) you'll find a short tour how to use and install the developer version of RQGIS and in this post (https://jannesm.wordpress.com/2016/06/03/integrating-r-with-qgis/) I briefly describe the most notable features of RQGIS. I am looking forward to hearing from first user experiences!
>
> Best regards,
>
> Jannes
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From scastillop75 at gmail.com  Mon Jun 20 12:41:55 2016
From: scastillop75 at gmail.com (Sergio C)
Date: Mon, 20 Jun 2016 12:41:55 +0200
Subject: [R-sig-Geo] Looking for real data set
Message-ID: <CAETaOdmUXp0dLytTKvohFq+hS+t1QJ5tdJh7=gjkNxP6KQBoww@mail.gmail.com>

Hello!

I'm looking for a real data set to apply nonparametric techniques developed
to geostatistical process. Specifically, I need examples with large data
size, of continuous spatial processes with non constant trend and high
spatial dependence.

Someone could tell me where can I find this kind of real data? Thanks.

	[[alternative HTML version deleted]]


From johnwasige at gmail.com  Mon Jun 20 15:29:09 2016
From: johnwasige at gmail.com (John Wasige)
Date: Mon, 20 Jun 2016 15:29:09 +0200
Subject: [R-sig-Geo] Installing packages of R 3.2.3 in Linux cluster
	computers
Message-ID: <CAJgdCD41T8jeF-ADsh3nxhg6BRxYXv4qMhTt-JanCV6XCVPWDQ@mail.gmail.com>

D
?ear all,

I would like to install the following packages (raster, rgdal,
spatial.tools, gdalUtils, fields, gstat) of R 3.2.3 in Linux cluster
computers.
,
However, I tried install.packages("raster") and I got the following error:
package ?raster? is not available (for R version 3.2.3)


Could somebody help with a hint on how to install?
?
Thanks for your help

John

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Jun 20 17:28:49 2016
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 20 Jun 2016 17:28:49 +0200
Subject: [R-sig-Geo] Installing packages of R 3.2.3 in Linux cluster
	computers
In-Reply-To: <CAJgdCD41T8jeF-ADsh3nxhg6BRxYXv4qMhTt-JanCV6XCVPWDQ@mail.gmail.com>
References: <CAJgdCD41T8jeF-ADsh3nxhg6BRxYXv4qMhTt-JanCV6XCVPWDQ@mail.gmail.com>
Message-ID: <CAJuCY5w9WZ=F8E13gg9bm5kj5kgn80r-8PCdynfnkTthxEJpvg@mail.gmail.com>

Dear John,

You'll need to install raster from source. Binaries are only available for
the previous, current and development versions of R.

Another option is that you are missing dependencies. We need to full error
message to see what's wrong.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2016-06-20 15:29 GMT+02:00 John Wasige <johnwasige at gmail.com>:

> D
> ?ear all,
>
> I would like to install the following packages (raster, rgdal,
> spatial.tools, gdalUtils, fields, gstat) of R 3.2.3 in Linux cluster
> computers.
> ,
> However, I tried install.packages("raster") and I got the following error:
> package ?raster? is not available (for R version 3.2.3)
>
>
> Could somebody help with a hint on how to install?
> ?
> Thanks for your help
>
> John
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From tcorms at gmail.com  Mon Jun 20 18:46:53 2016
From: tcorms at gmail.com (Tina Cormier)
Date: Mon, 20 Jun 2016 12:46:53 -0400
Subject: [R-sig-Geo] RQGIS - integrating R with QGIS
In-Reply-To: <CANVKczPREDKaZYZdLBsKK1ZCkow14xhfozKrgqC2xMg7-zDkWQ@mail.gmail.com>
References: <trinity-5ece0871-bdfe-438c-80cd-59f7e12e1d83-1466320692648@3capp-gmx-bs46>
	<CANVKczPREDKaZYZdLBsKK1ZCkow14xhfozKrgqC2xMg7-zDkWQ@mail.gmail.com>
Message-ID: <CAFfx768LBRCXideLoPAL0rKT_m8sTU3MFVSFP-j51t=MHNLwRg@mail.gmail.com>

Thank you so much for working on this, and I cannot wait to try it out. As
someone who uses both R and QGIS daily, this could be huge. I will report
back after install and testing!

Cheers,
Tina


On Sun, Jun 19, 2016 at 5:35 PM, Barry Rowlingson <
b.rowlingson at lancaster.ac.uk> wrote:

> Jannes,
>
>  did you consider using the rPython package? Its easy enough to import
> qgis functionality, there's less overhead since you aren't spawning a
> new python/qgis session for every command, you can save state between
> python calls, and you hand-off the platform dependency to that
> package.
>
> Barry
>
> On Sun, Jun 19, 2016 at 8:18 AM, "Jannes M?nchow" <malNamalJa at gmx.de>
> wrote:
> > Dear R-GIS users,
> >
> > I would like to draw your attention to a new package integrating R with
> QGIS. QGIS is probably the most widely used open-source Desktop GIS. Apart
> from its native functions QGIS provides access to the geoalgorithms of
> various third-party providers (among others SAGA and GRASS). In total this
> gives the user access to more than 1000 geoalgorithms. RQGIS brings this
> incredible powerful geoprocessing environment to the R console. We would
> like to publish RQGIS on CRAN within the next six weeks. Until then we
> would appreciate any user feedback. On github (
> https://github.com/jannes-m/RQGIS) you'll find a short tour how to use
> and install the developer version of RQGIS and in this post (
> https://jannesm.wordpress.com/2016/06/03/integrating-r-with-qgis/) I
> briefly describe the most notable features of RQGIS. I am looking forward
> to hearing from first user experiences!
> >
> > Best regards,
> >
> > Jannes
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From isaquedanielre at hotmail.com  Mon Jun 20 19:04:15 2016
From: isaquedanielre at hotmail.com (Isaque Daniel)
Date: Mon, 20 Jun 2016 17:04:15 +0000
Subject: [R-sig-Geo] RQGIS - integrating R with QGIS
In-Reply-To: <CAFfx768LBRCXideLoPAL0rKT_m8sTU3MFVSFP-j51t=MHNLwRg@mail.gmail.com>
References: <trinity-5ece0871-bdfe-438c-80cd-59f7e12e1d83-1466320692648@3capp-gmx-bs46>
	<CANVKczPREDKaZYZdLBsKK1ZCkow14xhfozKrgqC2xMg7-zDkWQ@mail.gmail.com>,
	<CAFfx768LBRCXideLoPAL0rKT_m8sTU3MFVSFP-j51t=MHNLwRg@mail.gmail.com>
Message-ID: <SN2PR20MB0784A1AD1845220913508B0DC92A0@SN2PR20MB0784.namprd20.prod.outlook.com>

Hi everyone!
Fantastic job!

Congratulations!
Cheers
Isaque


------------------------------------------------------------------------------------------------------------------
Eng. Agr. Isaque Daniel Rocha Eberhardt
Mestre em Sensoriamento Remoto - Instituto Nacional de Pesquisas Espaciais (INPE)
Doutorando em Transportes - Universidade de Bras??lia (UNB)
Mobile: +55 (061) 99015658
------------------------------------------------------------------------------------------------------------------
Agronomist engineer
Master in Remote Sensing - National  Institute for Space Research (INPE) - Brazil
PHD Student in Transport - Bras??lia University (UNB)


________________________________
De: R-sig-Geo <r-sig-geo-bounces at r-project.org> em nome de Tina Cormier <tcorms at gmail.com>
Enviado: segunda-feira, 20 de junho de 2016 16:46
Para: Barry Rowlingson
Cc: r-sig-geo; Jannes M?nchow
Assunto: Re: [R-sig-Geo] RQGIS - integrating R with QGIS

Thank you so much for working on this, and I cannot wait to try it out. As
someone who uses both R and QGIS daily, this could be huge. I will report
[[elided Hotmail spam]]

Cheers,
Tina


On Sun, Jun 19, 2016 at 5:35 PM, Barry Rowlingson <
b.rowlingson at lancaster.ac.uk> wrote:

> Jannes,
>
>  did you consider using the rPython package? Its easy enough to import
> qgis functionality, there's less overhead since you aren't spawning a
> new python/qgis session for every command, you can save state between
> python calls, and you hand-off the platform dependency to that
> package.
>
> Barry
>
> On Sun, Jun 19, 2016 at 8:18 AM, "Jannes M?nchow" <malNamalJa at gmx.de>
> wrote:
> > Dear R-GIS users,
> >
> > I would like to draw your attention to a new package integrating R with
> QGIS. QGIS is probably the most widely used open-source Desktop GIS. Apart
> from its native functions QGIS provides access to the geoalgorithms of
> various third-party providers (among others SAGA and GRASS). In total this
> gives the user access to more than 1000 geoalgorithms. RQGIS brings this
> incredible powerful geoprocessing environment to the R console. We would
> like to publish RQGIS on CRAN within the next six weeks. Until then we
> would appreciate any user feedback. On github (
> https://github.com/jannes-m/RQGIS) you'll find a short tour how to use
[https://avatars0.githubusercontent.com/u/9986952?v=3&s=400]<https://github.com/jannes-m/RQGIS>

GitHub - jannes-m/RQGIS: RQGIS - integrating R with QGIS<https://github.com/jannes-m/RQGIS>
github.com
RQGIS - integrating R with QGIS ... Please note that RQGIS is still a beta version and under active development.


> and install the developer version of RQGIS and in this post (
> https://jannesm.wordpress.com/2016/06/03/integrating-r-with-qgis/) I
> briefly describe the most notable features of RQGIS. I am looking forward
[[elided Hotmail spam]]
> >
> > Best regards,
> >
> > Jannes
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From DanielTurenne at hotmail.com  Tue Jun 21 04:05:29 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Tue, 21 Jun 2016 02:05:29 +0000
Subject: [R-sig-Geo] Spatio Temporal Kriging in gstat
Message-ID: <BLUPR12MB0625236933754F1DC82ABB94B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>

Hello R-Sig-Geo,


As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.


I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:


    sp = data.frame(long = stations$long, lat = stations$lat)

    coordinates(sp) = ~ long+lat


Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:


    beginDate = as.Date(2000/04/01)

    endDate = as.Date(2000/07/31)

    times = as.POSIXct(seq(beginDate,endDate,by="days"))


    data=data.frame(temps$residual)


And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index




	[[alternative HTML version deleted]]


From DanielTurenne at hotmail.com  Tue Jun 21 04:11:52 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Tue, 21 Jun 2016 02:11:52 +0000
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
Message-ID: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>

My apologies, I accidentally sent an unfinished email, here is the complete version of my question


Hello R-Sig-Geo,


As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.


I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:


    sp = data.frame(long = stations$long, lat = stations$lat)

    coordinates(sp) = ~ long+lat


Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:


    beginDate = as.Date(2000/04/01)

    endDate = as.Date(2000/07/31)

    times = as.POSIXct(seq(beginDate,endDate,by="days"))


    data=data.frame(temps$residual)


And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index


    1   1

    2   1

    3   1

    4   1


    st=STSDF(sp,time,data,index,endTime=delta(time))


however when I try to calculate the sample variogram I get the following error:


    sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)



   Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
   dim(X) must have a positive length
   In addition: There were 50 or more warnings (use warnings() to see the first 50)


All 50 of the errors are :


  In is.na(data[[as.character(as.list(formula)[[2]])]]) :
   is.na() applied to non-(list or vector) of type 'NULL'


Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.


Many Thanks,

Daniel Turenne

University of Manitoba


	[[alternative HTML version deleted]]


From englishchristophera at gmail.com  Tue Jun 21 07:24:05 2016
From: englishchristophera at gmail.com (chris english)
Date: Tue, 21 Jun 2016 08:24:05 +0300
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <CAASFQpTAQsSSrJQBd0+etCsu0-27qCaZ+5XpXYK-JK60cL33Fg@mail.gmail.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<CAASFQpTaKH+bauBfhAxcenR2GdMubFU24460VV2424R-R-xeJg@mail.gmail.com>
	<CAASFQpTAQsSSrJQBd0+etCsu0-27qCaZ+5XpXYK-JK60cL33Fg@mail.gmail.com>
Message-ID: <CAASFQpQkcSSWPYEbSVuXQCzxuEc__LY3UDJT_4D4h=C+chfqHw@mail.gmail.com>

Daniel,

You get no complaints on creating st:

st=STSDF(sp,time,data,index,endTime=delta(time))

class(st) should show "STSDF".

It remains that x$np is pointing at nothing when you get to
sample.STvariogram.

I'd type:
>sample.STvariogram to try and work out what it is actually doing.

Then try debugonce(sample.STvariogram) to see what values are in my
environment.

It may be that index isn't doing what you hope, ie to indicate non-NA
values.

And as usual I could be wrong on all counts.

Chris
On Jun 21, 2016 4:12 AM, "Dan Turenne" <DanielTurenne at hotmail.com> wrote:

My apologies, I accidentally sent an unfinished email, here is the complete
version of my question


Hello R-Sig-Geo,


As part of my masters thesis I am attempting to use spatio-temporal
regression kriging to make predictions with temperature data, and I was
hoping that someone might be able to give some insight as to how the
algorithms work in gstat.  My data consists of daily temperature
observations from April 1 to July 31, 2000.  There are observations from
164 stations across these 122 days, however not all stations have
observations on all days, making for a total of 19282 records.


I have tried to use an STSDF object but I have not had any success.  I
created an sp object of length 164 with the station locations:


    sp = data.frame(long = stations$long, lat = stations$lat)

    coordinates(sp) = ~ long+lat


Then I created a vector of length 122 with the times the observations were
recorded and a data vector of length 19282:


    beginDate = as.Date(2000/04/01)

    endDate = as.Date(2000/07/31)

    times = as.POSIXct(seq(beginDate,endDate,by="days"))


    data=data.frame(temps$residual)


And I also made an index detailing where observations are available, it
looks like this with the first column representing spatial index and the
second representing the time index


    1   1

    2   1

    3   1

    4   1


    st=STSDF(sp,time,data,index,endTime=delta(time))


however when I try to calculate the sample variogram I get the following
error:


    sample.stVariogram=variogramST(residual~1,data=st, tunit="days",
tlags=1:7, progress=TRUE)



   Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
   dim(X) must have a positive length
   In addition: There were 50 or more warnings (use warnings() to see the
first 50)


All 50 of the errors are :


  In is.na(data[[as.character(as.list(formula)[[2]])]]) :
   is.na() applied to non-(list or vector) of type 'NULL'


Can anyone see what I am doing wrong or give me any pointers?  This error
is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help
would be appreciated.


Many Thanks,

Daniel Turenne

University of Manitoba


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Tue Jun 21 08:52:56 2016
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Tue, 21 Jun 2016 08:52:56 +0200
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
Message-ID: <5768E448.70107@uni-muenster.de>

This might be due to a bug (or feature) in the software caused by the
sparseness in your data, which might be different from that used to test
the software. Please make the data available (off-list), along with an R
script, so we can try to reproduce the error message and look into it.

On 21/06/16 04:11, Dan Turenne wrote:
> My apologies, I accidentally sent an unfinished email, here is the complete version of my question
> 
> 
> Hello R-Sig-Geo,
> 
> 
> As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.
> 
> 
> I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:
> 
> 
>     sp = data.frame(long = stations$long, lat = stations$lat)
> 
>     coordinates(sp) = ~ long+lat
> 
> 
> Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:
> 
> 
>     beginDate = as.Date(2000/04/01)
> 
>     endDate = as.Date(2000/07/31)
> 
>     times = as.POSIXct(seq(beginDate,endDate,by="days"))
> 
> 
>     data=data.frame(temps$residual)
> 
> 
> And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index
> 
> 
>     1   1
> 
>     2   1
> 
>     3   1
> 
>     4   1
> 
> 
>     st=STSDF(sp,time,data,index,endTime=delta(time))
> 
> 
> however when I try to calculate the sample variogram I get the following error:
> 
> 
>     sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)
> 
> 
> 
>    Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
>    dim(X) must have a positive length
>    In addition: There were 50 or more warnings (use warnings() to see the first 50)
> 
> 
> All 50 of the errors are :
> 
> 
>   In is.na(data[[as.character(as.list(formula)[[2]])]]) :
>    is.na() applied to non-(list or vector) of type 'NULL'
> 
> 
> Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.
> 
> 
> Many Thanks,
> 
> Daniel Turenne
> 
> University of Manitoba
> 
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20160621/3901fda0/attachment.bin>

From pierluigi.derosa at gmail.com  Tue Jun 21 12:59:07 2016
From: pierluigi.derosa at gmail.com (pierluigi de rosa)
Date: Tue, 21 Jun 2016 12:59:07 +0200
Subject: [R-sig-Geo] O.T. - Conference OGRS 2016 | 12-14 Oct Perugia Italy
Message-ID: <CAAgsrZh0Zzq3mfYE71UKsk2XHeMjggSQeBVqt-h725kjGVsGVQ@mail.gmail.com>

Dear list,
Sorry for cross posting.

I would like to

Dear List,
Sorry for cross posting

This mail is to remember, if someone is interested, to submit  a short
papers for the next OGRS 2016 of Perugia, Italy.

- THE SYMPOSIUM
OGRS 2016 (Open Source Geospatial Research & Education Symposium 2016), is
an international meeting of researchers and developers interested in the
management of geospatial data using Open Source Software.
For more thorough information, please visit the official website:
http://2016.ogrs-community.org/

-THE LOCATION
The symposium will be held in Perugia (Central Italy, close to Rome and
Florence), in the period between 12 and 14 October 2016.
The main venue of the symposium will be in the historical centre of the
town: http://turismo.comune.perugia.it/

--- * ---

In particular members of this list could be interested in the *Statial
Statistic* session.

Moreover others session are described here:
http://2016.ogrs-community.org/scientific-program/sessions.

The short papers (1000-1500 words) submitted to OGRS2016 will be published
in the proceedings of the Symposium in collaboration with PeerJ
<https://peerj.com/>. PeerJ <https://peerj.com/> is an Open Access
publisher covering research in the biological, medical and computer
sciences, with a Spatial and Geographic Information Systems
<https://peerj.com/subjects/spatial-and-geographic-information-systems/>section.
More information about PeerJ <https://peerj.com/> can be found at
https://peerj.com/about/publications/.

PeerJ <https://peerj.com/> are the publisher of two journals,* PeerJ
Computer Science* and *PeerJ *(which covers Biology and Medicine), as well
as an un-peer reviewed preprint server *PeerJ PrePrints* (which covers all
of computer science, biology, medicine etc).



*Programme and symposium proceedings*

Short papers submitted to the Symposium will be rapidly made available
online (~24h) in the PeerJ
<https://peerj.com/collections/35-open-source-geospatial-research-and-education-symposium-2016/>OGRS2016
Collection
<https://peerj.com/collections/35-open-source-geospatial-research-and-education-symposium-2016/>
using
the PeerJ  <https://peerj.com/about/preprints/what-is-a-preprint/>PrePrint
server  <https://peerj.com/about/preprints/what-is-a-preprint/>(see PeerJ
Preprint server scope and instructionshere
<https://peerj.com/about/preprints/scope-and-instructions/>). Once online,
the manuscripts will be evaluated by the Scientific Committee
<http://2016.ogrs-community.org/submission-of-short-papers/81-committees/97-internationalscientificcommittee>
for
inclusion in theOGRS2016 programme according to an open procedure (the
original text, as well as reviewer's feedback and questions are publicly
available, moreover anybody is free to add comments). The committee will
decide whether the proposal is selected as a standard oral presentation or
a poster presentation. In case of rejection by the OGRS2016 editorial
committee, the manuscript will be removed from the OGRS2016 collection, but
not from the PeerJ preprint server.

*Special issue in PeerJ Computer Science*

The best short papers will be selected by the Scientific Committee, and the
authors will be asked to submit a full paper to the PeerJ Computer Science
journal <https://peerj.com/computer-science/> (see author guidelines
hereunder). The manuscripts will go through the usual PeerJ editorial
process and be accepted or rejected subject to the journal editorial
standards. Inclusion in the OGRS2016 symposium proceedings is no guarantee
that the article will be accepted. PeerJ gives the* first decisions back to
authors in a median of 22 days* and, when accepted, each single manuscript
will be published online immediately.
Thanks

Pierluigi

-- 
Ing. Pierluigi De Rosa (PhD)
cel: 3497558268 / fax: 075 7823038
skype: pierluigi.derosa

	[[alternative HTML version deleted]]


From malNamalJa at gmx.de  Tue Jun 21 13:12:12 2016
From: malNamalJa at gmx.de (=?UTF-8?Q?=22Jannes_M=C3=BCnchow=22?=)
Date: Tue, 21 Jun 2016 13:12:12 +0200
Subject: [R-sig-Geo] RQGIS - integrating R with QGIS
Message-ID: <trinity-312d41aa-8a4b-4c8b-ac21-ead65153a8c6-1466507532704@3capp-gmx-bs78>

An HTML attachment was scrubbed...
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20160621/f571a055/attachment.html>

From DanielTurenne at hotmail.com  Tue Jun 21 15:11:23 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Tue, 21 Jun 2016 13:11:23 +0000
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <5768E448.70107@uni-muenster.de>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>,
	<5768E448.70107@uni-muenster.de>
Message-ID: <BLUPR12MB0625091BF96A3B78CEF57E1AB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>

I would be happy to provide a data set and script but I'm not sure what you mean by making the data available "off-list".  Is there a particular forum where R-Sig-Geo users can post data and scripts?  Thanks again for your help.


Daniel Turenne

University of Manitoba


________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Edzer Pebesma <edzer.pebesma at uni-muenster.de>
Sent: June 21, 2016 1:52 AM
To: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat

This might be due to a bug (or feature) in the software caused by the
sparseness in your data, which might be different from that used to test
the software. Please make the data available (off-list), along with an R
script, so we can try to reproduce the error message and look into it.

On 21/06/16 04:11, Dan Turenne wrote:
> My apologies, I accidentally sent an unfinished email, here is the complete version of my question
>
>
> Hello R-Sig-Geo,
>
>
> As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.
>
>
> I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:
>
>
>     sp = data.frame(long = stations$long, lat = stations$lat)
>
>     coordinates(sp) = ~ long+lat
>
>
> Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:
>
>
>     beginDate = as.Date(2000/04/01)
>
>     endDate = as.Date(2000/07/31)
>
>     times = as.POSIXct(seq(beginDate,endDate,by="days"))
>
>
>     data=data.frame(temps$residual)
>
>
> And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index
>
>
>     1   1
>
>     2   1
>
>     3   1
>
>     4   1
>
>
>     st=STSDF(sp,time,data,index,endTime=delta(time))
>
>
> however when I try to calculate the sample variogram I get the following error:
>
>
>     sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)
>
>
>
>    Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
>    dim(X) must have a positive length
>    In addition: There were 50 or more warnings (use warnings() to see the first 50)
>
>
> All 50 of the errors are :
>
>
>   In is.na(data[[as.character(as.list(formula)[[2]])]]) :
>    is.na() applied to non-(list or vector) of type 'NULL'
>
>
> Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.
>
>
> Many Thanks,
>
> Daniel Turenne
>
> University of Manitoba
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
R-sig-Geo - R Special Interest Group on using Geographical ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
stat.ethz.ch
R-sig-Geo -- R Special Interest Group on using Geographical data and Mapping About R-sig-Geo



>

--
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info


	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Tue Jun 21 16:48:10 2016
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Tue, 21 Jun 2016 16:48:10 +0200
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <BLUPR12MB0625091BF96A3B78CEF57E1AB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB0625091BF96A3B78CEF57E1AB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
Message-ID: <576953AA.8000508@uni-muenster.de>



On 21/06/16 15:11, Dan Turenne wrote:
> I would be happy to provide a data set and script but I'm not sure what you mean by making the data available "off-list".  Is there a particular forum where R-Sig-Geo users can post data and scripts?  Thanks again for your help.
> 

Anything that avoids sending large files to all of the 3500 subscribers
of this list: direct email is fine, sharing on dropbox or similar too.

> 
> Daniel Turenne
> 
> University of Manitoba
> 
> 
> ________________________________
> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Edzer Pebesma <edzer.pebesma at uni-muenster.de>
> Sent: June 21, 2016 1:52 AM
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat
> 
> This might be due to a bug (or feature) in the software caused by the
> sparseness in your data, which might be different from that used to test
> the software. Please make the data available (off-list), along with an R
> script, so we can try to reproduce the error message and look into it.
> 
> On 21/06/16 04:11, Dan Turenne wrote:
>> My apologies, I accidentally sent an unfinished email, here is the complete version of my question
>>
>>
>> Hello R-Sig-Geo,
>>
>>
>> As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.
>>
>>
>> I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:
>>
>>
>>     sp = data.frame(long = stations$long, lat = stations$lat)
>>
>>     coordinates(sp) = ~ long+lat
>>
>>
>> Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:
>>
>>
>>     beginDate = as.Date(2000/04/01)
>>
>>     endDate = as.Date(2000/07/31)
>>
>>     times = as.POSIXct(seq(beginDate,endDate,by="days"))
>>
>>
>>     data=data.frame(temps$residual)
>>
>>
>> And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index
>>
>>
>>     1   1
>>
>>     2   1
>>
>>     3   1
>>
>>     4   1
>>
>>
>>     st=STSDF(sp,time,data,index,endTime=delta(time))
>>
>>
>> however when I try to calculate the sample variogram I get the following error:
>>
>>
>>     sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)
>>
>>
>>
>>    Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
>>    dim(X) must have a positive length
>>    In addition: There were 50 or more warnings (use warnings() to see the first 50)
>>
>>
>> All 50 of the errors are :
>>
>>
>>   In is.na(data[[as.character(as.list(formula)[[2]])]]) :
>>    is.na() applied to non-(list or vector) of type 'NULL'
>>
>>
>> Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.
>>
>>
>> Many Thanks,
>>
>> Daniel Turenne
>>
>> University of Manitoba
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> R-sig-Geo - R Special Interest Group on using Geographical ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
> stat.ethz.ch
> R-sig-Geo -- R Special Interest Group on using Geographical data and Mapping About R-sig-Geo
> 
> 
> 
>>
> 
> --
> Edzer Pebesma
> Institute for Geoinformatics  (ifgi),  University of M?nster
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
> Spatial Statistics Society http://www.spatialstatistics.info
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20160621/69c221cd/attachment.bin>

From DanielTurenne at hotmail.com  Tue Jun 21 17:16:23 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Tue, 21 Jun 2016 15:16:23 +0000
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <5768E448.70107@uni-muenster.de>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>,
	<5768E448.70107@uni-muenster.de>
Message-ID: <BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>

Hello again,


After some tinkering with my code last night I got the sample variogram to work however now I have encountered another issue.  When attempting to use the fit.stVariogram function to fit the sample variogram I receive this error:


Error in optim(extractPar(model), fitFun, ..., method = method, lower = lower,  :
  non-finite value supplied by optim


even if I specify the upper and lower arguments to be finite.  Could this error be cause by the initial values that I set for the space, time and joint variograms?  If so can anyone recommend  a good paper/article about how to fit these three variograms?  I have attached a link to my script and data below, if anyone has any insight as to why I am getting this error it would be appreciated.  I tried to be as thorough as I could in my code commenting, please let me know if anything is unclear.


https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0


________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Edzer Pebesma <edzer.pebesma at uni-muenster.de>
Sent: June 21, 2016 1:52 AM
To: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat

This might be due to a bug (or feature) in the software caused by the
sparseness in your data, which might be different from that used to test
the software. Please make the data available (off-list), along with an R
script, so we can try to reproduce the error message and look into it.

On 21/06/16 04:11, Dan Turenne wrote:
> My apologies, I accidentally sent an unfinished email, here is the complete version of my question
>
>
> Hello R-Sig-Geo,
>
>
> As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.
>
>
> I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:
>
>
>     sp = data.frame(long = stations$long, lat = stations$lat)
>
>     coordinates(sp) = ~ long+lat
>
>
> Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:
>
>
>     beginDate = as.Date(2000/04/01)
>
>     endDate = as.Date(2000/07/31)
>
>     times = as.POSIXct(seq(beginDate,endDate,by="days"))
>
>
>     data=data.frame(temps$residual)
>
>
> And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index
>
>
>     1   1
>
>     2   1
>
>     3   1
>
>     4   1
>
>
>     st=STSDF(sp,time,data,index,endTime=delta(time))
>
>
> however when I try to calculate the sample variogram I get the following error:
>
>
>     sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)
>
>
>
>    Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
>    dim(X) must have a positive length
>    In addition: There were 50 or more warnings (use warnings() to see the first 50)
>
>
> All 50 of the errors are :
>
>
>   In is.na(data[[as.character(as.list(formula)[[2]])]]) :
>    is.na() applied to non-(list or vector) of type 'NULL'
>
>
> Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.
>
>
> Many Thanks,
>
> Daniel Turenne
>
> University of Manitoba
>
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
R-sig-Geo - R Special Interest Group on using Geographical ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
stat.ethz.ch
R-sig-Geo -- R Special Interest Group on using Geographical data and Mapping About R-sig-Geo



>

--
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info


	[[alternative HTML version deleted]]


From kmb56 at berkeley.edu  Tue Jun 21 18:08:08 2016
From: kmb56 at berkeley.edu (Kenny Bell)
Date: Tue, 21 Jun 2016 09:08:08 -0700
Subject: [R-sig-Geo] RQGIS - integrating R with QGIS
In-Reply-To: <trinity-312d41aa-8a4b-4c8b-ac21-ead65153a8c6-1466507532704@3capp-gmx-bs78>
References: <trinity-312d41aa-8a4b-4c8b-ac21-ead65153a8c6-1466507532704@3capp-gmx-bs78>
Message-ID: <CALOjXYREoJvKT+hHgtJF7Es2Xp88tT+4GOPgbnmK6PKwyYd2fg@mail.gmail.com>

Hi Jannes,

Perhaps you could provide a quick bullet point version? Each bullet could
be as simple as:

   -  rgeos::gSimplify = RQGIS::<The DP algorithm function>; New features
   are x, y, z.

omitting the first part for those with no current equivalent.

Thanks,
Kendon


On Tue, Jun 21, 2016 at 4:12 AM, "Jannes M?nchow" <malNamalJa at gmx.de> wrote:

> Dear R-GIS users,
>
> thank you so much for the kind and constructive remarks!
>
> @Rainer: We will change the MAC paths in accordance with your
> recommendations. And yes, set_env could search automatically for QGIS
> installations under Linux and Mac OSX. We have already implemented that
> kind of logic for Windows. However, this might make set_env really slow
> depending on the number of folders set_env has to search recursively.
> Therefore, we are not sure if we really should implement that feature. But
> maybe one could search at least the most common paths...
>
> @Barry: Yes, I considered using rPython in the beginning. But then I
> realized it was only available for unix-based OS (or at least a stable
> version). Secondly, I wasn't sure if it was easily possible to import QGIS
> functionality (could you briefly show?). And thank you for pointing out
> other advantages of rPython. I'll surely give it a try.
>
> @Kenny: Currently, I am also writing a paper on RQGIS. Here, I'll also
> compare and discuss GIS functionalities within R. But it will probably
> still take a while until it gets published...
>
> All the best,
>
> Jannes
>



-- 
Kendon Bell
Email: kmb56 at berkeley.edu
Phone: (510) 612-3375

Ph.D. Candidate
Department of Agricultural & Resource Economics
University of California, Berkeley

	[[alternative HTML version deleted]]


From paolo.piras at uniroma3.it  Wed Jun 22 15:39:11 2016
From: paolo.piras at uniroma3.it (Paolo Piras)
Date: Wed, 22 Jun 2016 13:39:11 +0000
Subject: [R-sig-Geo] polar intertia of a generalized polygon.
In-Reply-To: <CAASFQpQkcSSWPYEbSVuXQCzxuEc__LY3UDJT_4D4h=C+chfqHw@mail.gmail.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<CAASFQpTaKH+bauBfhAxcenR2GdMubFU24460VV2424R-R-xeJg@mail.gmail.com>
	<CAASFQpTAQsSSrJQBd0+etCsu0-27qCaZ+5XpXYK-JK60cL33Fg@mail.gmail.com>,
	<CAASFQpQkcSSWPYEbSVuXQCzxuEc__LY3UDJT_4D4h=C+chfqHw@mail.gmail.com>
Message-ID: <HE1PR04MB116145F6732926D8A244D94BB32C0@HE1PR04MB1161.eurprd04.prod.outlook.com>


 Hi folks,
I need to compute the polar inertia of a generalized polygon. It can includes points within the perimeter.
Moreover, points are not ordered (i.e. clockwise, etc.). The same is needed in 3D but 2D is sufficient for the moment.
Do you know some function that can do that?
Thanks in advance
Paolo


	[[alternative HTML version deleted]]


From nickperpin at gmail.com  Wed Jun 22 16:15:56 2016
From: nickperpin at gmail.com (Nick Perpinias)
Date: Wed, 22 Jun 2016 16:15:56 +0200
Subject: [R-sig-Geo] experimental covariogram
Message-ID: <CAPU+BsAPitem0oP8RkVYvVw-Fsb0uHC5ZrhRd-7zrx6HjoGQfw@mail.gmail.com>

Dear R-GIS users,

I am trying to do a performance evaluation/comparison between the
covariogram and semivariogram for certain spatially correlated data.
In particular, I would like to investigate if there is a certain trade off
using either of these two tools when for example there is no trend on the
spatial field.

Although checking the main packages for spatial statistics in R (sp,
spatstat, geoR, geoRglm, gstat) I wasn't able to find a function
calculating the experimental covariogram of a given spatial dataset.

In geoRglm there is only a very specific implementation for covariogram
estimation (covariog function) and in gstat packet the variogram function
has the option of covariogram without making clear if the covariogram is
computed or extracted from the semivariogram.

I would be grateful if you could point out an existing function of a plan
of attack for my question.

Thank you in advance for your time and help.

Best Regards,

Nikos

	[[alternative HTML version deleted]]


From edzer.pebesma at uni-muenster.de  Wed Jun 22 16:27:46 2016
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 22 Jun 2016 16:27:46 +0200
Subject: [R-sig-Geo] experimental covariogram
In-Reply-To: <CAPU+BsAPitem0oP8RkVYvVw-Fsb0uHC5ZrhRd-7zrx6HjoGQfw@mail.gmail.com>
References: <CAPU+BsAPitem0oP8RkVYvVw-Fsb0uHC5ZrhRd-7zrx6HjoGQfw@mail.gmail.com>
Message-ID: <576AA062.5020305@uni-muenster.de>



On 22/06/16 16:15, Nick Perpinias wrote:
> Dear R-GIS users,
> 
> I am trying to do a performance evaluation/comparison between the
> covariogram and semivariogram for certain spatially correlated data.
> In particular, I would like to investigate if there is a certain trade off
> using either of these two tools when for example there is no trend on the
> spatial field.
> 
> Although checking the main packages for spatial statistics in R (sp,
> spatstat, geoR, geoRglm, gstat) I wasn't able to find a function
> calculating the experimental covariogram of a given spatial dataset.

I don't think that sp or spatstat bring you anything in this respect.

gstat::variogram computes the covariogram when asked; the code that
(finally) does it is here:

https://github.com/edzer/gstat/blob/master/src/sem.c#L342

> 
> In geoRglm there is only a very specific implementation for covariogram
> estimation (covariog function) and in gstat packet the variogram function
> has the option of covariogram without making clear if the covariogram is
> computed or extracted from the semivariogram.
> 
> I would be grateful if you could point out an existing function of a plan
> of attack for my question.

I'd be interested in learning why you think this is important.

> 
> Thank you in advance for your time and help.
> 
> Best Regards,
> 
> Nikos
> 
> 	[[alternative HTML version deleted]]
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20160622/cd530406/attachment.bin>

From paolo.piras at uniroma3.it  Wed Jun 22 16:32:11 2016
From: paolo.piras at uniroma3.it (Paolo Piras)
Date: Wed, 22 Jun 2016 14:32:11 +0000
Subject: [R-sig-Geo] polar inertia 2
Message-ID: <HE1PR04MB116186A3FBFE8C1AA5E9C8CEB32C0@HE1PR04MB1161.eurprd04.prod.outlook.com>

Sorry for multiple posting,

my first message was refused by my server


Hi folks,

I need to compute the polar inertia of a generalized polygon. It can includes points within the perimeter.
Moreover, points are not ordered (i.e. clockwise, etc.). The same is needed in 3D but 2D is sufficient for the moment.
Do you know some function that can do that?
Thanks in advance
Paolo


	[[alternative HTML version deleted]]


From benedikt.graeler at rub.de  Thu Jun 23 09:54:34 2016
From: benedikt.graeler at rub.de (Benedikt Graeler)
Date: Thu, 23 Jun 2016 09:54:34 +0200
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
Message-ID: <1454a622-6b1e-38fe-293d-c0d1936bf47c@rub.de>

Dear Dan,

the error occurs because your prior.stvgm is far from the structure of 
your data. The following model provides better starting values:

prior.stvgm=vgmST(stModel="sumMetric",
                         space=vgm(0.1,"Exp",0.5,0.3),
                         time=vgm(0.1,"Exp",0.5,0.3),
                         joint=vgm(0.1,"Exp",0.5,0.3),
                         stAni=1)

To get a first "estimate", it is always useful to look at a plot of the 
sample variogram (as in the spatial case):

plot(sample.stvgm, wireframe=T, scales=list(arrows=F))

 From the plot, one can read spatial and temporal ranges, the magnitude 
of variability and the ratios of nuggets and sills. Sometimes, one is 
better of by starting with a simpler model (e.g. "pure" metric), as the 
routines are more robust for less parameters, to get a first impression 
of the range of parameters. Furthermore, the plot suggests that a much 
smaller spatial upper boundary is sufficient, as the variogram surface 
is mainly flat in the spatial dimension.

Spatio-temporal geostatistics remains tricky in terms of numerical 
routines and model interpretation, we try to give some guidance 
regarding gstat's functionalities in [1].

HTH,

  Ben

[1] https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf


On 21.06.2016 17:16, Dan Turenne wrote:
> Hello again,
>
>
> After some tinkering with my code last night I got the sample variogram to work however now I have encountered another issue.  When attempting to use the fit.stVariogram function to fit the sample variogram I receive this error:
>
>
> Error in optim(extractPar(model), fitFun, ..., method = method, lower = lower,  :
>   non-finite value supplied by optim
>
>
> even if I specify the upper and lower arguments to be finite.  Could this error be cause by the initial values that I set for the space, time and joint variograms?  If so can anyone recommend  a good paper/article about how to fit these three variograms?  I have attached a link to my script and data below, if anyone has any insight as to why I am getting this error it would be appreciated.  I tried to be as thorough as I could in my code commenting, please let me know if anything is unclear.
>
>
> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>
>
> ________________________________
> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Edzer Pebesma <edzer.pebesma at uni-muenster.de>
> Sent: June 21, 2016 1:52 AM
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat
>
> This might be due to a bug (or feature) in the software caused by the
> sparseness in your data, which might be different from that used to test
> the software. Please make the data available (off-list), along with an R
> script, so we can try to reproduce the error message and look into it.
>
> On 21/06/16 04:11, Dan Turenne wrote:
>> My apologies, I accidentally sent an unfinished email, here is the complete version of my question
>>
>>
>> Hello R-Sig-Geo,
>>
>>
>> As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.
>>
>>
>> I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:
>>
>>
>>     sp = data.frame(long = stations$long, lat = stations$lat)
>>
>>     coordinates(sp) = ~ long+lat
>>
>>
>> Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:
>>
>>
>>     beginDate = as.Date(2000/04/01)
>>
>>     endDate = as.Date(2000/07/31)
>>
>>     times = as.POSIXct(seq(beginDate,endDate,by="days"))
>>
>>
>>     data=data.frame(temps$residual)
>>
>>
>> And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index
>>
>>
>>     1   1
>>
>>     2   1
>>
>>     3   1
>>
>>     4   1
>>
>>
>>     st=STSDF(sp,time,data,index,endTime=delta(time))
>>
>>
>> however when I try to calculate the sample variogram I get the following error:
>>
>>
>>     sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)
>>
>>
>>
>>    Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
>>    dim(X) must have a positive length
>>    In addition: There were 50 or more warnings (use warnings() to see the first 50)
>>
>>
>> All 50 of the errors are :
>>
>>
>>   In is.na(data[[as.character(as.list(formula)[[2]])]]) :
>>    is.na() applied to non-(list or vector) of type 'NULL'
>>
>>
>> Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.
>>
>>
>> Many Thanks,
>>
>> Daniel Turenne
>>
>> University of Manitoba
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> R-sig-Geo - R Special Interest Group on using Geographical ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
> stat.ethz.ch
> R-sig-Geo -- R Special Interest Group on using Geographical data and Mapping About R-sig-Geo
>
>
>
>>
>
> --
> Edzer Pebesma
> Institute for Geoinformatics  (ifgi),  University of M?nster
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
> Spatial Statistics Society http://www.spatialstatistics.info
>
>
> 	[[alternative HTML version deleted]]
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Dr. Benedikt Gr?ler
Institute of Hydrology
Ruhr University Bochum
Universit?tsstra?e 150
44801 Bochum

ben.graeler.org

+49 (0234) 32 - 27619


From nickperpin at gmail.com  Thu Jun 23 17:52:30 2016
From: nickperpin at gmail.com (Nick Perpinias)
Date: Thu, 23 Jun 2016 17:52:30 +0200
Subject: [R-sig-Geo] experimental covariogram
In-Reply-To: <576AA062.5020305@uni-muenster.de>
References: <CAPU+BsAPitem0oP8RkVYvVw-Fsb0uHC5ZrhRd-7zrx6HjoGQfw@mail.gmail.com>
	<576AA062.5020305@uni-muenster.de>
Message-ID: <CAPU+BsDxUv0fpG=B-i+ExyXKh3iFNpbnD=hwzxu63up2EB1m1g@mail.gmail.com>

Dear Edzer,

Thank you very much for your prompt answer.

Well, my thinking (and please forgive me if it is wrong) is that there is
no free lunch. What I mean is that covariogram needs a second order
stationarity, but semivariogram allows for a trend, i.e. the field has to
fulfill intrinsic stationarity. That makes semivariogram more general. So
what do you have to "pay" for this generality, especially in cases where
there is no trend in the field but still one uses the semivariogram. I
would intuitively expect that there is something to lose.

Moreover a follow up question is: is there a fitting function for
covariogram?

Thank you again for your help.

Best,

Nikos

2016-06-22 16:27 GMT+02:00 Edzer Pebesma <edzer.pebesma at uni-muenster.de>:

>
>
> On 22/06/16 16:15, Nick Perpinias wrote:
> > Dear R-GIS users,
> >
> > I am trying to do a performance evaluation/comparison between the
> > covariogram and semivariogram for certain spatially correlated data.
> > In particular, I would like to investigate if there is a certain trade
> off
> > using either of these two tools when for example there is no trend on the
> > spatial field.
> >
> > Although checking the main packages for spatial statistics in R (sp,
> > spatstat, geoR, geoRglm, gstat) I wasn't able to find a function
> > calculating the experimental covariogram of a given spatial dataset.
>
> I don't think that sp or spatstat bring you anything in this respect.
>
> gstat::variogram computes the covariogram when asked; the code that
> (finally) does it is here:
>
> https://github.com/edzer/gstat/blob/master/src/sem.c#L342
>
> >
> > In geoRglm there is only a very specific implementation for covariogram
> > estimation (covariog function) and in gstat packet the variogram function
> > has the option of covariogram without making clear if the covariogram is
> > computed or extracted from the semivariogram.
> >
> > I would be grateful if you could point out an existing function of a plan
> > of attack for my question.
>
> I'd be interested in learning why you think this is important.
>
> >
> > Thank you in advance for your time and help.
> >
> > Best Regards,
> >
> > Nikos
> >
> >       [[alternative HTML version deleted]]
> >
> > _______________________________________________
> > R-sig-Geo mailing list
> > R-sig-Geo at r-project.org
> > https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> >
>
> --
> Edzer Pebesma
> Institute for Geoinformatics  (ifgi),  University of M?nster
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
> Spatial Statistics Society http://www.spatialstatistics.info
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

	[[alternative HTML version deleted]]


From paolo.piras at uniroma3.it  Fri Jun 24 15:45:43 2016
From: paolo.piras at uniroma3.it (Paolo Piras)
Date: Fri, 24 Jun 2016 13:45:43 +0000
Subject: [R-sig-Geo] spatial sampling with an un-ordered polygon for which
 links between points are known
In-Reply-To: <BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>,
	<5768E448.70107@uni-muenster.de>,
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
Message-ID: <HE1PR04MB1161117110C53DB377D7D058B32E0@HE1PR04MB1161.eurprd04.prod.outlook.com>

Hi folks,
I write in order to know if there is a solution to the following spatial sampling problem:
I have a polygon that is not ordered; however, I know the "links" (or edges) between points;
I need to sample 2000 points within the polygon. Using spsample() and Polygon() I need an ordered polygon which is not the case.
I tried to use the links information in order to dynamically (and generalizing the problem) obtain the correct
order but I did'nt get effective solution. I dont want an "estimate" of the hull from points or other heuristic strategies; I want to use the links info in order to properly sort my points.
Here below a fully reproducible example.

library(sp)
pol<-matrix(c(30.24854,33.90530,27.48992,30.21646,40.03200,39.26215,33.52038,39.10177,39.99992,47.02477,47.44176,55.17230,57.38561,55.30061,57.25730,57.38561,28.45223,21.78023,31.21084,33.96946,22.10100,22.96708,40.86600,40.06407,40.83392,38.13946,24.79546,29.70323,30.60138,33.61661,31.65992,32.68638),ncol=2)
plot(pol,asp=1)
text(pol[,1],pol[,2],c(1:nrow(pol))) ### as you can see the polygon is not ordered
links<-matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,2,6,1,3,11,5,4,7,8,9,12,13,15,10,16,14),ncol=2)
for(i in 1:nrow(links)){
  segments(pol[links[i,1],1],pol[links[i,1],2],pol[links[i,2],1],pol[links[i,2],2])
} # Fortunately I will have always these links
#### now I would like to sampling,regularly, say 2000 points in the polygon
sfe1<-spsample(Polygon(pol),2000,type="regular")
points(sfe1 at coords) ###  of course this is not what I want.
####  using an "ad hoc" ordering
correct<-c(1,2,6,5,11,12,13,15,16,14,10,9,8,7,4,3) ## this is just an "ad hoc" solution; I need to generalize using the links information
sfe1<-spsample(Polygon(pol[correct,]),2000,type="regular")
points(sfe1 at coords,col=2,pch=19,cex=0.3) ### this is what I want.

This is an ad hoc solution; as I will have many different polygons (thounsed) and all different (some very irregular) but always with links information I would like to know if there is a solution to get the correct order using links.
Thanks in advance for any advice
All the best
Paolo


	[[alternative HTML version deleted]]


From dedethesetter at gmail.com  Fri Jun 24 16:22:06 2016
From: dedethesetter at gmail.com (Debora Nozza)
Date: Fri, 24 Jun 2016 16:22:06 +0200
Subject: [R-sig-Geo] NA data values
Message-ID: <CALYygPfenQurHgqM4kHgGNenEW15a6EADU0U_cMv9+rW_gXfMA@mail.gmail.com>

Good morning,

I am working on creating a STFDF object using the function meteo2STFDF(). I
am trying to  to reproducing (with my data) the same example provided in
http://www.inside-r.org/node/358574.

However, after computing meteo2STFDF() function, the data at tempc is full of
NA values. It seems that I am doing exactly the same thing of the example
so I am not able to understand where is the problem.

Could someone has a solution?

Best Regards,
Debora


Here my variable:

Object of class STFDF
 with Dimensions (s, t, attr): (2, 8761, 1)
[[Spatial:]]
Object of class SpatialPointsDataFrame
Coordinates:
           min        max
lon 14406.0000 14476.0000
lat    45.6737    45.8771
Is projected: FALSE
proj4string :
[+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0]
Number of points: 2
Data attributes:
      Citta        lat             GT1               GT2
 Bagolino:1   Min.   :10.34   Min.   :5058676   Min.   :1604398
 Bione   :1   1st Qu.:10.37   1st Qu.:5064384   1st Qu.:1606377
              Median :10.39   Median :5070092   Median :1608356
              Mean   :10.39   Mean   :5070092   Mean   :1608356
              3rd Qu.:10.42   3rd Qu.:5075799   3rd Qu.:1610335
              Max.   :10.45   Max.   :5081507   Max.   :1612314
[[Temporal:]]
     Index                       timeIndex
 Min.   :2015-05-24 00:00:00   Min.   :   1
 1st Qu.:2015-08-23 06:00:00   1st Qu.:2191
 Median :2015-11-22 12:00:00   Median :4381
 Mean   :2015-11-22 12:00:00   Mean   :4381
 3rd Qu.:2016-02-21 18:00:00   3rd Qu.:6571
 Max.   :2016-05-23 00:00:00   Max.   :8761
[[Data attributes:]]
     tempc
 Min.   : NA
 1st Qu.: NA
 Median : NA
 Mean   :NaN
 3rd Qu.: NA
 Max.   : NA
 NA's   :17522

	[[alternative HTML version deleted]]


From jecogeo at gmail.com  Fri Jun 24 15:45:58 2016
From: jecogeo at gmail.com (Jefferson Ferreira-Ferreira)
Date: Fri, 24 Jun 2016 09:45:58 -0400
Subject: [R-sig-Geo] [off-topic] Raster Distance
Message-ID: <CAFFT+Y4nNPtDSWxGKtVGfn7+m5R7eHf1UB=zzY9dVcUktUetWQ@mail.gmail.com>

Dear listers!

Sorry by this off-topic.
I have an open question on GIS StackExchange: Is there any difference
between gdal_proximity.py and ArcGIS Euclidean Distance?
<http://gis.stackexchange.com/questions/199735/is-there-any-difference-between-gdal-proximity-py-and-arcgis-euclidean-distance>
If someone can help to answer it, I would be grateful.

Thanks a lot.

*Jefferson Ferreira-Ferreira, **PhD Candidate *

*Geographer*



*Ecosystem Dynamics Observatory <http://tscanada.wix.com/ecodyn> -
EcoDyn/UNESP*
*Department of **Geography *
*Institute of Geosciences and Exact Sciences** (IGCE)*
*S?o Paulo State University (UNESP)*
*Rio Claro, SP - Brazil*

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Fri Jun 24 17:22:22 2016
From: mdsumner at gmail.com (Michael Sumner)
Date: Fri, 24 Jun 2016 15:22:22 +0000
Subject: [R-sig-Geo] spatial sampling with an un-ordered polygon for
 which links between points are known
In-Reply-To: <HE1PR04MB1161117110C53DB377D7D058B32E0@HE1PR04MB1161.eurprd04.prod.outlook.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<HE1PR04MB1161117110C53DB377D7D058B32E0@HE1PR04MB1161.eurprd04.prod.outlook.com>
Message-ID: <CAAcGz98fT9xafwDmQsD3r7Mh4EgUUUU0xKRjB3Y0h8fttxTCkw@mail.gmail.com>

The internal "cycles" function that I have here reproduces your correct
ordering:



On Fri, 24 Jun 2016 at 23:46 Paolo Piras <paolo.piras at uniroma3.it> wrote:

> Hi folks,
> I write in order to know if there is a solution to the following spatial
> sampling problem:
> I have a polygon that is not ordered; however, I know the "links" (or
> edges) between points;
> I need to sample 2000 points within the polygon. Using spsample() and
> Polygon() I need an ordered polygon which is not the case.
> I tried to use the links information in order to dynamically (and
> generalizing the problem) obtain the correct
> order but I did'nt get effective solution. I dont want an "estimate" of
> the hull from points or other heuristic strategies; I want to use the links
> info in order to properly sort my points.
> Here below a fully reproducible example.
>
> library(sp)
>
> pol<-matrix(c(30.24854,33.90530,27.48992,30.21646,40.03200,39.26215,33.52038,39.10177,39.99992,47.02477,47.44176,55.17230,57.38561,55.30061,57.25730,57.38561,28.45223,21.78023,31.21084,33.96946,22.10100,22.96708,40.86600,40.06407,40.83392,38.13946,24.79546,29.70323,30.60138,33.61661,31.65992,32.68638),ncol=2)
> plot(pol,asp=1)
> text(pol[,1],pol[,2],c(1:nrow(pol))) ### as you can see the polygon is not
> ordered
>
> links<-matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,2,6,1,3,11,5,4,7,8,9,12,13,15,10,16,14),ncol=2)
> for(i in 1:nrow(links)){
>
> segments(pol[links[i,1],1],pol[links[i,1],2],pol[links[i,2],1],pol[links[i,2],2])
> } # Fortunately I will have always these links
> #### now I would like to sampling,regularly, say 2000 points in the polygon
> sfe1<-spsample(Polygon(pol),2000,type="regular")
> points(sfe1 at coords) ###  of course this is not what I want.
> ####  using an "ad hoc" ordering
> correct<-c(1,2,6,5,11,12,13,15,16,14,10,9,8,7,4,3) ## this is just an "ad
> hoc" solution; I need to generalize using the links information
> sfe1<-spsample(Polygon(pol[correct,]),2000,type="regular")
> points(sfe1 at coords,col=2,pch=19,cex=0.3) ### this is what I want.
>
>

This "detect cycles" function reproduces your "correct" indexing from the
links matrix:

cycles <- function(aa) {
  ii <- 1
  set0 <- ii
  visited <- logical(nrow(aa))
  while(!all(visited)) {
    i0 <- ii
    repeat {
      ii <- which(aa[,1] == aa[ii, 2])
      if (ii == i0) {
        set0 <- c(set0, NA_integer_)
        break;
      }
      set0 <- c(set0, ii)
    }
    visited <- seq(nrow(aa)) %in% na.omit(set0)
    ii <- which(!visited)[1L]
    if (!is.na(ii)) set0 <- c(set0, ii)
  }
  set0
}


cycles(links)
 #  [1]  1  2  6  5 11 12 13 15 16 14 10  9  8  7  4  3 NA

I can't remember now what I was thinking with the finish-up in this
function - clearlyl I didn't quite get it right given that NA at the end -
but it's something I've been using now and then - detect "cycles"  in a
list of edges. There's probably something in the igraph package that's much
more powerful and faster. Very keen to hear what you learn here.

(I was using this to re-compose polygonal rings from meshes in the
RTriangle package, it takes input just like your pol/link vertices/edges
structure, and builds a constrained Delaunay triangulation.)

Cheers, Mike.


This is an ad hoc solution; as I will have many different polygons
> (thounsed) and all different (some very irregular) but always with links
> information I would like to know if there is a solution to get the correct
> order using links.
> Thanks in advance for any advice
> All the best
> Paolo
>
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
-- 
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From mdsumner at gmail.com  Fri Jun 24 17:41:06 2016
From: mdsumner at gmail.com (Michael Sumner)
Date: Fri, 24 Jun 2016 15:41:06 +0000
Subject: [R-sig-Geo] spatial sampling with an un-ordered polygon for
 which links between points are known
In-Reply-To: <CAAcGz98fT9xafwDmQsD3r7Mh4EgUUUU0xKRjB3Y0h8fttxTCkw@mail.gmail.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<HE1PR04MB1161117110C53DB377D7D058B32E0@HE1PR04MB1161.eurprd04.prod.outlook.com>
	<CAAcGz98fT9xafwDmQsD3r7Mh4EgUUUU0xKRjB3Y0h8fttxTCkw@mail.gmail.com>
Message-ID: <CAAcGz9-+NEH4Hx_S_d799_rS_zrTTCPORGHN2ww1iHJTSqBWPw@mail.gmail.com>

On Sat, 25 Jun 2016 at 01:22 Michael Sumner <mdsumner at gmail.com> wrote:

> The internal "cycles" function that I have here reproduces your correct
> ordering:
>
>
>
> On Fri, 24 Jun 2016 at 23:46 Paolo Piras <paolo.piras at uniroma3.it> wrote:
>
>> Hi folks,
>> I write in order to know if there is a solution to the following spatial
>> sampling problem:
>> I have a polygon that is not ordered; however, I know the "links" (or
>> edges) between points;
>> I need to sample 2000 points within the polygon. Using spsample() and
>> Polygon() I need an ordered polygon which is not the case.
>> I tried to use the links information in order to dynamically (and
>> generalizing the problem) obtain the correct
>> order but I did'nt get effective solution. I dont want an "estimate" of
>> the hull from points or other heuristic strategies; I want to use the links
>> info in order to properly sort my points.
>> Here below a fully reproducible example.
>>
>> library(sp)
>>
>> pol<-matrix(c(30.24854,33.90530,27.48992,30.21646,40.03200,39.26215,33.52038,39.10177,39.99992,47.02477,47.44176,55.17230,57.38561,55.30061,57.25730,57.38561,28.45223,21.78023,31.21084,33.96946,22.10100,22.96708,40.86600,40.06407,40.83392,38.13946,24.79546,29.70323,30.60138,33.61661,31.65992,32.68638),ncol=2)
>> plot(pol,asp=1)
>> text(pol[,1],pol[,2],c(1:nrow(pol))) ### as you can see the polygon is
>> not ordered
>>
>> links<-matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,2,6,1,3,11,5,4,7,8,9,12,13,15,10,16,14),ncol=2)
>> for(i in 1:nrow(links)){
>>
>> segments(pol[links[i,1],1],pol[links[i,1],2],pol[links[i,2],1],pol[links[i,2],2])
>> } # Fortunately I will have always these links
>> #### now I would like to sampling,regularly, say 2000 points in the
>> polygon
>> sfe1<-spsample(Polygon(pol),2000,type="regular")
>> points(sfe1 at coords) ###  of course this is not what I want.
>> ####  using an "ad hoc" ordering
>> correct<-c(1,2,6,5,11,12,13,15,16,14,10,9,8,7,4,3) ## this is just an "ad
>> hoc" solution; I need to generalize using the links information
>> sfe1<-spsample(Polygon(pol[correct,]),2000,type="regular")
>> points(sfe1 at coords,col=2,pch=19,cex=0.3) ### this is what I want.
>>
>>
>
> This "detect cycles" function reproduces your "correct" indexing from the
> links matrix:
>
> cycles <- function(aa) {
>   ii <- 1
>   set0 <- ii
>   visited <- logical(nrow(aa))
>   while(!all(visited)) {
>     i0 <- ii
>     repeat {
>       ii <- which(aa[,1] == aa[ii, 2])
>       if (ii == i0) {
>         set0 <- c(set0, NA_integer_)
>         break;
>       }
>       set0 <- c(set0, ii)
>     }
>     visited <- seq(nrow(aa)) %in% na.omit(set0)
>     ii <- which(!visited)[1L]
>     if (!is.na(ii)) set0 <- c(set0, ii)
>   }
>   set0
> }
>
>
>
I found a better way, though it's wasteful on the non-sparse matrix for
links, it gives the right answer (here in a list as there may be more than
one cycle).

cycles2 <- function(links) {
    require(ggm)
    if (any(is.na(links))) stop("missing value/s in links")
    mat <- matrix(0L, max(links), max(links))
    mat[links] <- 1
   lapply(ggm::fundCycles(mat), function(xa) rev(xa[ , 1L]))
}

cycles2(links)

I think you've spurred me onto finding solutions to my own issues, hope
it's of interest.

Thanks, Mike.


> cycles(links)
>  #  [1]  1  2  6  5 11 12 13 15 16 14 10  9  8  7  4  3 NA
>
> I can't remember now what I was thinking with the finish-up in this
> function - clearlyl I didn't quite get it right given that NA at the end -
> but it's something I've been using now and then - detect "cycles"  in a
> list of edges. There's probably something in the igraph package that's much
> more powerful and faster. Very keen to hear what you learn here.
>
> (I was using this to re-compose polygonal rings from meshes in the
> RTriangle package, it takes input just like your pol/link vertices/edges
> structure, and builds a constrained Delaunay triangulation.)
>
> Cheers, Mike.
>
>
> This is an ad hoc solution; as I will have many different polygons
>> (thounsed) and all different (some very irregular) but always with links
>> information I would like to know if there is a solution to get the correct
>> order using links.
>> Thanks in advance for any advice
>> All the best
>> Paolo
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
> --
> Dr. Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> 203 Channel Highway
> Kingston Tasmania 7050 Australia
>
> --
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

	[[alternative HTML version deleted]]


From paolo.piras at uniroma3.it  Fri Jun 24 17:45:29 2016
From: paolo.piras at uniroma3.it (Paolo Piras)
Date: Fri, 24 Jun 2016 15:45:29 +0000
Subject: [R-sig-Geo] spatial sampling with an un-ordered polygon for
 which links between points are known
In-Reply-To: <CAAcGz9-+NEH4Hx_S_d799_rS_zrTTCPORGHN2ww1iHJTSqBWPw@mail.gmail.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<HE1PR04MB1161117110C53DB377D7D058B32E0@HE1PR04MB1161.eurprd04.prod.outlook.com>
	<CAAcGz98fT9xafwDmQsD3r7Mh4EgUUUU0xKRjB3Y0h8fttxTCkw@mail.gmail.com>,
	<CAAcGz9-+NEH4Hx_S_d799_rS_zrTTCPORGHN2ww1iHJTSqBWPw@mail.gmail.com>
Message-ID: <HE1PR04MB1161FCA4B2723C1208C0207DB32E0@HE1PR04MB1161.eurprd04.prod.outlook.com>

Thanks a lot Michael!!

This is perfect!

And elegant of course...

Thankyou  very much!

Best

Paolo


________________________________
Da: Michael Sumner <mdsumner at gmail.com>
Inviato: venerd? 24 giugno 2016 17.41
A: Paolo Piras; r-sig-geo at r-project.org
Oggetto: Re: [R-sig-Geo] spatial sampling with an un-ordered polygon for which links between points are known



On Sat, 25 Jun 2016 at 01:22 Michael Sumner <mdsumner at gmail.com<mailto:mdsumner at gmail.com>> wrote:
The internal "cycles" function that I have here reproduces your correct ordering:



On Fri, 24 Jun 2016 at 23:46 Paolo Piras <paolo.piras at uniroma3.it<mailto:paolo.piras at uniroma3.it>> wrote:
Hi folks,
I write in order to know if there is a solution to the following spatial sampling problem:
I have a polygon that is not ordered; however, I know the "links" (or edges) between points;
I need to sample 2000 points within the polygon. Using spsample() and Polygon() I need an ordered polygon which is not the case.
I tried to use the links information in order to dynamically (and generalizing the problem) obtain the correct
order but I did'nt get effective solution. I dont want an "estimate" of the hull from points or other heuristic strategies; I want to use the links info in order to properly sort my points.
Here below a fully reproducible example.

library(sp)
pol<-matrix(c(30.24854,33.90530,27.48992,30.21646,40.03200,39.26215,33.52038,39.10177,39.99992,47.02477,47.44176,55.17230,57.38561,55.30061,57.25730,57.38561,28.45223,21.78023,31.21084,33.96946,22.10100,22.96708,40.86600,40.06407,40.83392,38.13946,24.79546,29.70323,30.60138,33.61661,31.65992,32.68638),ncol=2)
plot(pol,asp=1)
text(pol[,1],pol[,2],c(1:nrow(pol))) ### as you can see the polygon is not ordered
links<-matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,2,6,1,3,11,5,4,7,8,9,12,13,15,10,16,14),ncol=2)
for(i in 1:nrow(links)){
  segments(pol[links[i,1],1],pol[links[i,1],2],pol[links[i,2],1],pol[links[i,2],2])
} # Fortunately I will have always these links
#### now I would like to sampling,regularly, say 2000 points in the polygon
sfe1<-spsample(Polygon(pol),2000,type="regular")
points(sfe1 at coords) ###  of course this is not what I want.
####  using an "ad hoc" ordering
correct<-c(1,2,6,5,11,12,13,15,16,14,10,9,8,7,4,3) ## this is just an "ad hoc" solution; I need to generalize using the links information
sfe1<-spsample(Polygon(pol[correct,]),2000,type="regular")
points(sfe1 at coords,col=2,pch=19,cex=0.3) ### this is what I want.



This "detect cycles" function reproduces your "correct" indexing from the links matrix:

cycles <- function(aa) {
  ii <- 1
  set0 <- ii
  visited <- logical(nrow(aa))
  while(!all(visited)) {
    i0 <- ii
    repeat {
      ii <- which(aa[,1] == aa[ii, 2])
      if (ii == i0) {
        set0 <- c(set0, NA_integer_)
        break;
      }
      set0 <- c(set0, ii)
    }
    visited <- seq(nrow(aa)) %in% na.omit(set0)
    ii <- which(!visited)[1L]
    if (!is.na<http://is.na>(ii)) set0 <- c(set0, ii)
  }
  set0
}



I found a better way, though it's wasteful on the non-sparse matrix for links, it gives the right answer (here in a list as there may be more than one cycle).

cycles2 <- function(links) {
    require(ggm)
    if (any(is.na<http://is.na>(links))) stop("missing value/s in links")
    mat <- matrix(0L, max(links), max(links))
    mat[links] <- 1
   lapply(ggm::fundCycles(mat), function(xa) rev(xa[ , 1L]))
}

cycles2(links)

I think you've spurred me onto finding solutions to my own issues, hope it's of interest.

Thanks, Mike.

cycles(links)
 #  [1]  1  2  6  5 11 12 13 15 16 14 10  9  8  7  4  3 NA

I can't remember now what I was thinking with the finish-up in this function - clearlyl I didn't quite get it right given that NA at the end - but it's something I've been using now and then - detect "cycles"  in a list of edges. There's probably something in the igraph package that's much more powerful and faster. Very keen to hear what you learn here.

(I was using this to re-compose polygonal rings from meshes in the RTriangle package, it takes input just like your pol/link vertices/edges structure, and builds a constrained Delaunay triangulation.)

Cheers, Mike.


This is an ad hoc solution; as I will have many different polygons (thounsed) and all different (some very irregular) but always with links information I would like to know if there is a solution to get the correct order using links.
Thanks in advance for any advice
All the best
Paolo


        [[alternative HTML version deleted]]

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org<mailto:R-sig-Geo at r-project.org>
https://stat.ethz.ch/mailman/listinfo/r-sig-geo
--
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia

--
Dr. Michael Sumner
Software and Database Engineer
Australian Antarctic Division
203 Channel Highway
Kingston Tasmania 7050 Australia


	[[alternative HTML version deleted]]


From edoardo.baldoni at gmail.com  Sun Jun 26 15:10:50 2016
From: edoardo.baldoni at gmail.com (Edoardo Baldoni)
Date: Sun, 26 Jun 2016 15:10:50 +0200
Subject: [R-sig-Geo] The spml function does not work
Message-ID: <CAOcqoUPWQDgG=PTZrNQTG9c9f+BbX4e5L4ABK73pdwid_4t_DA@mail.gmail.com>

Dear R users,

I am trying to benchmark my code for estimating spatial panels with the
functions in the splm package. The problem is that I cannot make the spml
functions (from package splm) work. Not even the ones in the standard
examples provided. Here is one of the examples that can be found in the R
help page (http://127.0.0.1:24033/library/splm/html/spml.html) and then the
error that comes out:

data(Produc, package = "plm")
data(usaww)
fm <- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
fespaterr <- spml(fm, data = Produc, listw = mat2listw(usaww),
                   model="within", spatial.error="b", Hess = FALSE)

Error in solve(crossprod(xco), crossprod(xco, yco)) :
  error in evaluating the argument 'a' in selecting a method for function
'solve': Error in crossprod(xco) :
  could not find symbol "..." in environment of the generic function


Do you have any idea of what is happening here ? Thanks


Edoardo

	[[alternative HTML version deleted]]


From DanielTurenne at hotmail.com  Sun Jun 26 16:30:20 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Sun, 26 Jun 2016 14:30:20 +0000
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <1454a622-6b1e-38fe-293d-c0d1936bf47c@rub.de>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>,
	<1454a622-6b1e-38fe-293d-c0d1936bf47c@rub.de>
Message-ID: <SN1PR12MB0638D399DC32C01F498D9A13B7200@SN1PR12MB0638.namprd12.prod.outlook.com>

Hello Listers,


https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0


Thanks to the help of Dr. Graeler I have been having some success with my spatio temporal kriging script but I had some more questions I was hoping someone could help with.  When fitting the ST variogram with fit.stVariogram, I have noticed that the function is extremely sensitive to the choice of initial parameters.  I have included a link to my script at the beginning of this email.  If I use


prior.stvgm=vgmST(stModel="sumMetric",
                        space=vgm(0.1,"Exp",0.5,0.3),
                        time=vgm(0.1,"Exp",0.5,0.3),
                        joint=vgm(0.1,"Exp",0.5,0.3),
                        stAni=0.5)

fitted.stvgm=fit.StVariogram(sample.stvgm,prior.stvgm)

fit.stVariogram works without producing any errors, however if I adjust any of these parameters I get some strange errors.  For example by using

prior.stvgm=vgmST(stModel="sumMetric",
                        space=vgm(0.1,"Exp",0.5,0.3),
                        time=vgm(0.1,"Exp",0.5,0.3),
                        joint=vgm(0.1,"Exp",0.5,0.3),
                        stAni=1)

Then I get this error, despite the fact that stAni is clearly positive:

      Error in vgmST("sumMetric", space = vgm(par[1], as.character(model$space$model[2]),  :
      "stAni" must be positive.

As another example if I enter

prior.stvgm=vgmST(stModel="sumMetric",
                        space=vgm(0.1,"Exp",0.5,0.3),
                        time=vgm(0.1,"Exp",0.5,0.3),
                        joint=vgm(0.1,"Exp",0.5,0.3),
                        stAni=0.99)

It will produce the following error, despite the fact that all ranges are clearly positive:

     Error in vgm(par[1], as.character(model$space$model[2]), par[2], par[3],  :
      range should be positive

The only thing that changes among these examples is the value of the stAni parameter.  When stAni=0.5 everything is fine, if stAni=1 then apparently stAni isn't positive and if stAni=0.99 then apparently the range isn't positive.  When I look at the plot of the sample variogram from my data, it appears that the spatial sill =1, spatial range=0.5, temporal sill=0.85, temporal range=4, joint sill=1.1 ,joint range=1.0 however these values also produce an error saying that range should be positive.  I'm finding these errors to be very confusing and would appreciate any tips anyone might be able to give.

Thank you,

Daniel Turenne
University of Manitoba


________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler <benedikt.graeler at rub.de>
Sent: June 23, 2016 2:54 AM
To: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat

Dear Dan,

the error occurs because your prior.stvgm is far from the structure of
your data. The following model provides better starting values:

prior.stvgm=vgmST(stModel="sumMetric",
                         space=vgm(0.1,"Exp",0.5,0.3),
                         time=vgm(0.1,"Exp",0.5,0.3),
                         joint=vgm(0.1,"Exp",0.5,0.3),
                         stAni=1)

To get a first "estimate", it is always useful to look at a plot of the
sample variogram (as in the spatial case):

plot(sample.stvgm, wireframe=T, scales=list(arrows=F))

 From the plot, one can read spatial and temporal ranges, the magnitude
of variability and the ratios of nuggets and sills. Sometimes, one is
better of by starting with a simpler model (e.g. "pure" metric), as the
routines are more robust for less parameters, to get a first impression
of the range of parameters. Furthermore, the plot suggests that a much
smaller spatial upper boundary is sufficient, as the variogram surface
is mainly flat in the spatial dimension.

Spatio-temporal geostatistics remains tricky in terms of numerical
routines and model interpretation, we try to give some guidance
regarding gstat's functionalities in [1].

HTH,

  Ben

[1] https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf
Spatio-Temporal Interpolation using gstat<https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf>
journal.r-project.org
CONTRIBUTED RESEARCH ARTICLE 1 Spatio-Temporal Interpolation using gstat by Benedikt Gr?ler, Edzer Pebesma and Gerard Heuvelink Abstract We present new spatio ...





On 21.06.2016 17:16, Dan Turenne wrote:
> Hello again,
>
>
> After some tinkering with my code last night I got the sample variogram to work however now I have encountered another issue.  When attempting to use the fit.stVariogram function to fit the sample variogram I receive this error:
>
>
> Error in optim(extractPar(model), fitFun, ..., method = method, lower = lower,  :
>   non-finite value supplied by optim
>
>
> even if I specify the upper and lower arguments to be finite.  Could this error be cause by the initial values that I set for the space, time and joint variograms?  If so can anyone recommend  a good paper/article about how to fit these three variograms?  I have attached a link to my script and data below, if anyone has any insight as to why I am getting this error it would be appreciated.  I tried to be as thorough as I could in my code commenting, please let me know if anything is unclear.
>
>
> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>
>
> ________________________________
> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Edzer Pebesma <edzer.pebesma at uni-muenster.de>
> Sent: June 21, 2016 1:52 AM
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat
>
> This might be due to a bug (or feature) in the software caused by the
> sparseness in your data, which might be different from that used to test
> the software. Please make the data available (off-list), along with an R
> script, so we can try to reproduce the error message and look into it.
>
> On 21/06/16 04:11, Dan Turenne wrote:
>> My apologies, I accidentally sent an unfinished email, here is the complete version of my question
>>
>>
>> Hello R-Sig-Geo,
>>
>>
>> As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.
>>
>>
>> I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:
>>
>>
>>     sp = data.frame(long = stations$long, lat = stations$lat)
>>
>>     coordinates(sp) = ~ long+lat
>>
>>
>> Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:
>>
>>
>>     beginDate = as.Date(2000/04/01)
>>
>>     endDate = as.Date(2000/07/31)
>>
>>     times = as.POSIXct(seq(beginDate,endDate,by="days"))
>>
>>
>>     data=data.frame(temps$residual)
>>
>>
>> And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index
>>
>>
>>     1   1
>>
>>     2   1
>>
>>     3   1
>>
>>     4   1
>>
>>
>>     st=STSDF(sp,time,data,index,endTime=delta(time))
>>
>>
>> however when I try to calculate the sample variogram I get the following error:
>>
>>
>>     sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)
>>
>>
>>
>>    Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
>>    dim(X) must have a positive length
>>    In addition: There were 50 or more warnings (use warnings() to see the first 50)
>>
>>
>> All 50 of the errors are :
>>
>>
>>   In is.na(data[[as.character(as.list(formula)[[2]])]]) :
>>    is.na() applied to non-(list or vector) of type 'NULL'
>>
>>
>> Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.
>>
>>
>> Many Thanks,
>>
>> Daniel Turenne
>>
>> University of Manitoba
>>
>>
>>        [[alternative HTML version deleted]]
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> R-sig-Geo - R Special Interest Group on using Geographical ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
> stat.ethz.ch
> R-sig-Geo -- R Special Interest Group on using Geographical data and Mapping About R-sig-Geo
>
>
>
>>
>
> --
> Edzer Pebesma
> Institute for Geoinformatics  (ifgi),  University of M?nster
> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
> Journal of Statistical Software:   http://www.jstatsoft.org/
> Computers & Geosciences:   http://elsevier.com/locate/cageo/
> Spatial Statistics Society http://www.spatialstatistics.info
>
>
>        [[alternative HTML version deleted]]
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

--
Dr. Benedikt Gr?ler
Institute of Hydrology
Ruhr University Bochum
Universit?tsstra?e 150
44801 Bochum

ben.graeler.org

+49 (0234) 32 - 27619

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From Roger.Bivand at nhh.no  Sun Jun 26 19:10:12 2016
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 26 Jun 2016 19:10:12 +0200
Subject: [R-sig-Geo] The spml function does not work
In-Reply-To: <CAOcqoUPWQDgG=PTZrNQTG9c9f+BbX4e5L4ABK73pdwid_4t_DA@mail.gmail.com>
References: <CAOcqoUPWQDgG=PTZrNQTG9c9f+BbX4e5L4ABK73pdwid_4t_DA@mail.gmail.com>
Message-ID: <alpine.LFD.2.20.1606261902310.10812@reclus.nhh.no>

On Sun, 26 Jun 2016, Edoardo Baldoni wrote:

> Dear R users,
>
> I am trying to benchmark my code for estimating spatial panels with the
> functions in the splm package. The problem is that I cannot make the spml
> functions (from package splm) work. Not even the ones in the standard
> examples provided. Here is one of the examples that can be found in the R
> help page (http://127.0.0.1:24033/library/splm/html/spml.html) and then the
> error that comes out:
>
> data(Produc, package = "plm")
> data(usaww)
> fm <- log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp
> fespaterr <- spml(fm, data = Produc, listw = mat2listw(usaww),
>                   model="within", spatial.error="b", Hess = FALSE)
>
> Error in solve(crossprod(xco), crossprod(xco, yco)) :
>  error in evaluating the argument 'a' in selecting a method for function
> 'solve': Error in crossprod(xco) :
>  could not find symbol "..." in environment of the generic function
>

You did not provide the output of sessionInfo(), showing which packages 
were visible in the search path. This code is run in:

https://cran.r-project.org/web/checks/check_results_splm.html

on many platforms and works, so it feels as though you may have another 
solve() in the search path. Can you try it without loading any other 
packages before splm?

Hope this clarifies,

Roger

>
> Do you have any idea of what is happening here ? Thanks
>
>
> Edoardo
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Roger Bivand
Department of Economics, Norwegian School of Economics,
Helleveien 30, N-5045 Bergen, Norway.
voice: +47 55 95 93 55; fax +47 55 95 91 00
e-mail: Roger.Bivand at nhh.no
http://orcid.org/0000-0003-2392-6140
https://scholar.google.no/citations?user=AWeghB0AAAAJ&hl=en
http://depsy.org/person/434412


From englishchristophera at gmail.com  Sat Jun 25 18:00:18 2016
From: englishchristophera at gmail.com (chris english)
Date: Sat, 25 Jun 2016 19:00:18 +0300
Subject: [R-sig-Geo] [off-topic] Raster Distance
In-Reply-To: <CAFFT+Y4nNPtDSWxGKtVGfn7+m5R7eHf1UB=zzY9dVcUktUetWQ@mail.gmail.com>
References: <CAFFT+Y4nNPtDSWxGKtVGfn7+m5R7eHf1UB=zzY9dVcUktUetWQ@mail.gmail.com>
Message-ID: <CAASFQpQLLh9fUgtdJR8fe+kngw4sZATJcQVpF4mhtDmckiB8JQ@mail.gmail.com>

Jefferson,

This won't help with ArcGis Euclidean Distance directly, but this link is
to unit test for the gdal proximity algorithm.

https://trac.osgeo.org/gdal/browser/branches/2.1/autotest/alg/proximity.py

Playing around with this and known values, and then with Arc Euclidean
Distance might be a way to ascertain if they're approached the same.

Just a thought.
Chris
On Jun 24, 2016 16:49, "Jefferson Ferreira-Ferreira" <jecogeo at gmail.com>
wrote:

> Dear listers!
>
> Sorry by this off-topic.
> I have an open question on GIS StackExchange: Is there any difference
> between gdal_proximity.py and ArcGIS Euclidean Distance?
> <
> http://gis.stackexchange.com/questions/199735/is-there-any-difference-between-gdal-proximity-py-and-arcgis-euclidean-distance
> >
> If someone can help to answer it, I would be grateful.
>
> Thanks a lot.
>
> *Jefferson Ferreira-Ferreira, **PhD Candidate *
>
> *Geographer*
>
>
>
> *Ecosystem Dynamics Observatory <http://tscanada.wix.com/ecodyn> -
> EcoDyn/UNESP*
> *Department of **Geography *
> *Institute of Geosciences and Exact Sciences** (IGCE)*
> *S?o Paulo State University (UNESP)*
> *Rio Claro, SP - Brazil*
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From csany22 at hotmail.com  Tue Jun 28 12:07:03 2016
From: csany22 at hotmail.com (Csany22 csany)
Date: Tue, 28 Jun 2016 10:07:03 +0000
Subject: [R-sig-Geo] spatial anova
Message-ID: <DUB112-W113FEE2BA896A3CC1C6E696A9220@phx.gbl>

Hi R-sig-geo people,


what would be the best way to run spatial anova in R? I have a dependent (continuous), independent (categorical) variables and lat and long fo reach of the cell(pixel) based observations. I need to run anova. as would like to compare the means (Tukey) of the independent variable. 
 		 	   		  
	[[alternative HTML version deleted]]


From benedikt.graeler at rub.de  Tue Jun 28 12:23:54 2016
From: benedikt.graeler at rub.de (Benedikt Graeler)
Date: Tue, 28 Jun 2016 12:23:54 +0200
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <SN1PR12MB0638D399DC32C01F498D9A13B7200@SN1PR12MB0638.namprd12.prod.outlook.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<1454a622-6b1e-38fe-293d-c0d1936bf47c@rub.de>
	<SN1PR12MB0638D399DC32C01F498D9A13B7200@SN1PR12MB0638.namprd12.prod.outlook.com>
Message-ID: <e27ae85a-2f0d-9020-7ca0-a52a6f866ea2@rub.de>

Dear Daniel,

the errors occur during the optimisation where optim tries different 
(possibly non-positive) values to find the best fit. Providing sensitive 
limits via "lower" and "upper" to optim should resolve the issue.

HTH,

  Ben

On 26.06.2016 16:30, Dan Turenne wrote:
> Hello Listers,
>
>
> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>
>
> Thanks to the help of Dr. Graeler I have been having some success with my spatio temporal kriging script but I had some more questions I was hoping someone could help with.  When fitting the ST variogram with fit.stVariogram, I have noticed that the function is extremely sensitive to the choice of initial parameters.  I have included a link to my script at the beginning of this email.  If I use
>
>
> prior.stvgm=vgmST(stModel="sumMetric",
>                         space=vgm(0.1,"Exp",0.5,0.3),
>                         time=vgm(0.1,"Exp",0.5,0.3),
>                         joint=vgm(0.1,"Exp",0.5,0.3),
>                         stAni=0.5)
>
> fitted.stvgm=fit.StVariogram(sample.stvgm,prior.stvgm)
>
> fit.stVariogram works without producing any errors, however if I adjust any of these parameters I get some strange errors.  For example by using
>
> prior.stvgm=vgmST(stModel="sumMetric",
>                         space=vgm(0.1,"Exp",0.5,0.3),
>                         time=vgm(0.1,"Exp",0.5,0.3),
>                         joint=vgm(0.1,"Exp",0.5,0.3),
>                         stAni=1)
>
> Then I get this error, despite the fact that stAni is clearly positive:
>
>       Error in vgmST("sumMetric", space = vgm(par[1], as.character(model$space$model[2]),  :
>       "stAni" must be positive.
>
> As another example if I enter
>
> prior.stvgm=vgmST(stModel="sumMetric",
>                         space=vgm(0.1,"Exp",0.5,0.3),
>                         time=vgm(0.1,"Exp",0.5,0.3),
>                         joint=vgm(0.1,"Exp",0.5,0.3),
>                         stAni=0.99)
>
> It will produce the following error, despite the fact that all ranges are clearly positive:
>
>      Error in vgm(par[1], as.character(model$space$model[2]), par[2], par[3],  :
>       range should be positive
>
> The only thing that changes among these examples is the value of the stAni parameter.  When stAni=0.5 everything is fine, if stAni=1 then apparently stAni isn't positive and if stAni=0.99 then apparently the range isn't positive.  When I look at the plot of the sample variogram from my data, it appears that the spatial sill =1, spatial range=0.5, temporal sill=0.85, temporal range=4, joint sill=1.1 ,joint range=1.0 however these values also produce an error saying that range should be positive.  I'm finding these errors to be very confusing and would appreciate any tips anyone might be able to give.
>
> Thank you,
>
> Daniel Turenne
> University of Manitoba
>
>
> ________________________________
> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler <benedikt.graeler at rub.de>
> Sent: June 23, 2016 2:54 AM
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat
>
> Dear Dan,
>
> the error occurs because your prior.stvgm is far from the structure of
> your data. The following model provides better starting values:
>
> prior.stvgm=vgmST(stModel="sumMetric",
>                          space=vgm(0.1,"Exp",0.5,0.3),
>                          time=vgm(0.1,"Exp",0.5,0.3),
>                          joint=vgm(0.1,"Exp",0.5,0.3),
>                          stAni=1)
>
> To get a first "estimate", it is always useful to look at a plot of the
> sample variogram (as in the spatial case):
>
> plot(sample.stvgm, wireframe=T, scales=list(arrows=F))
>
>  From the plot, one can read spatial and temporal ranges, the magnitude
> of variability and the ratios of nuggets and sills. Sometimes, one is
> better of by starting with a simpler model (e.g. "pure" metric), as the
> routines are more robust for less parameters, to get a first impression
> of the range of parameters. Furthermore, the plot suggests that a much
> smaller spatial upper boundary is sufficient, as the variogram surface
> is mainly flat in the spatial dimension.
>
> Spatio-temporal geostatistics remains tricky in terms of numerical
> routines and model interpretation, we try to give some guidance
> regarding gstat's functionalities in [1].
>
> HTH,
>
>   Ben
>
> [1] https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf
> Spatio-Temporal Interpolation using gstat<https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf>
> journal.r-project.org
> CONTRIBUTED RESEARCH ARTICLE 1 Spatio-Temporal Interpolation using gstat by Benedikt Gr?ler, Edzer Pebesma and Gerard Heuvelink Abstract We present new spatio ...
>
>
>
>
>
> On 21.06.2016 17:16, Dan Turenne wrote:
>> Hello again,
>>
>>
>> After some tinkering with my code last night I got the sample variogram to work however now I have encountered another issue.  When attempting to use the fit.stVariogram function to fit the sample variogram I receive this error:
>>
>>
>> Error in optim(extractPar(model), fitFun, ..., method = method, lower = lower,  :
>>   non-finite value supplied by optim
>>
>>
>> even if I specify the upper and lower arguments to be finite.  Could this error be cause by the initial values that I set for the space, time and joint variograms?  If so can anyone recommend  a good paper/article about how to fit these three variograms?  I have attached a link to my script and data below, if anyone has any insight as to why I am getting this error it would be appreciated.  I tried to be as thorough as I could in my code commenting, please let me know if anything is unclear.
>>
>>
>> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>>
>>
>> ________________________________
>> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Edzer Pebesma <edzer.pebesma at uni-muenster.de>
>> Sent: June 21, 2016 1:52 AM
>> To: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat
>>
>> This might be due to a bug (or feature) in the software caused by the
>> sparseness in your data, which might be different from that used to test
>> the software. Please make the data available (off-list), along with an R
>> script, so we can try to reproduce the error message and look into it.
>>
>> On 21/06/16 04:11, Dan Turenne wrote:
>>> My apologies, I accidentally sent an unfinished email, here is the complete version of my question
>>>
>>>
>>> Hello R-Sig-Geo,
>>>
>>>
>>> As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.
>>>
>>>
>>> I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:
>>>
>>>
>>>     sp = data.frame(long = stations$long, lat = stations$lat)
>>>
>>>     coordinates(sp) = ~ long+lat
>>>
>>>
>>> Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:
>>>
>>>
>>>     beginDate = as.Date(2000/04/01)
>>>
>>>     endDate = as.Date(2000/07/31)
>>>
>>>     times = as.POSIXct(seq(beginDate,endDate,by="days"))
>>>
>>>
>>>     data=data.frame(temps$residual)
>>>
>>>
>>> And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index
>>>
>>>
>>>     1   1
>>>
>>>     2   1
>>>
>>>     3   1
>>>
>>>     4   1
>>>
>>>
>>>     st=STSDF(sp,time,data,index,endTime=delta(time))
>>>
>>>
>>> however when I try to calculate the sample variogram I get the following error:
>>>
>>>
>>>     sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)
>>>
>>>
>>>
>>>    Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
>>>    dim(X) must have a positive length
>>>    In addition: There were 50 or more warnings (use warnings() to see the first 50)
>>>
>>>
>>> All 50 of the errors are :
>>>
>>>
>>>   In is.na(data[[as.character(as.list(formula)[[2]])]]) :
>>>    is.na() applied to non-(list or vector) of type 'NULL'
>>>
>>>
>>> Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.
>>>
>>>
>>> Many Thanks,
>>>
>>> Daniel Turenne
>>>
>>> University of Manitoba
>>>
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> R-sig-Geo - R Special Interest Group on using Geographical ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
>> stat.ethz.ch
>> R-sig-Geo -- R Special Interest Group on using Geographical data and Mapping About R-sig-Geo
>>
>>
>>
>>>
>>
>> --
>> Edzer Pebesma
>> Institute for Geoinformatics  (ifgi),  University of M?nster
>> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
>> Journal of Statistical Software:   http://www.jstatsoft.org/
>> Computers & Geosciences:   http://elsevier.com/locate/cageo/
>> Spatial Statistics Society http://www.spatialstatistics.info
>>
>>
>>        [[alternative HTML version deleted]]
>>
>>
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Dr. Benedikt Gr?ler
> Institute of Hydrology
> Ruhr University Bochum
> Universit?tsstra?e 150
> 44801 Bochum
>
> ben.graeler.org
>
> +49 (0234) 32 - 27619
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> 	[[alternative HTML version deleted]]
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Dr. Benedikt Gr?ler
Institute of Hydrology
Ruhr University Bochum
Universit?tsstra?e 150
44801 Bochum

ben.graeler.org

+49 (0234) 32 - 27619


From DanielTurenne at hotmail.com  Tue Jun 28 16:42:01 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Tue, 28 Jun 2016 14:42:01 +0000
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <e27ae85a-2f0d-9020-7ca0-a52a6f866ea2@rub.de>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<1454a622-6b1e-38fe-293d-c0d1936bf47c@rub.de>
	<SN1PR12MB0638D399DC32C01F498D9A13B7200@SN1PR12MB0638.namprd12.prod.outlook.com>,
	<e27ae85a-2f0d-9020-7ca0-a52a6f866ea2@rub.de>
Message-ID: <BLUPR12MB0625BC2865577CD885B4FBE0B7220@BLUPR12MB0625.namprd12.prod.outlook.com>

Thank you for the information, I was able to get around the problems I was having by setting upper to Inf and lower to 0.0000001 (An arbitrary small value to guarantee that the value will be greater then 0).  After working some more on my code I had just a few other general questions I was hoping someone could answer.

  1.  Whenever I call variogramST to calculate the sample variogram I get a bunch of repeating output that says $tunit [1] "days" over and over again.  I have tried setting progress to FALSE but this output still occurs.  Is there anyway to suppress this output?
  2.  When using krigeST to get predictions, the predictions are returned in the form of a single column, however there is no index to say which prediction corresponds to which location.  Is this column organized with the spatial index moving faster or the temporal index moving faster?

Thank you in advance for any advice, everyone on this list has been very friendly and helpful and it is greatly appreciated.

Thanks,
Daniel Turenne
________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler <benedikt.graeler at rub.de>
Sent: June 28, 2016 5:23 AM
To: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat

Dear Daniel,

the errors occur during the optimisation where optim tries different
(possibly non-positive) values to find the best fit. Providing sensitive
limits via "lower" and "upper" to optim should resolve the issue.

HTH,

  Ben

On 26.06.2016 16:30, Dan Turenne wrote:
> Hello Listers,
>
>
> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>
>
> Thanks to the help of Dr. Graeler I have been having some success with my spatio temporal kriging script but I had some more questions I was hoping someone could help with.  When fitting the ST variogram with fit.stVariogram, I have noticed that the function is extremely sensitive to the choice of initial parameters.  I have included a link to my script at the beginning of this email.  If I use
>
>
> prior.stvgm=vgmST(stModel="sumMetric",
>                         space=vgm(0.1,"Exp",0.5,0.3),
>                         time=vgm(0.1,"Exp",0.5,0.3),
>                         joint=vgm(0.1,"Exp",0.5,0.3),
>                         stAni=0.5)
>
> fitted.stvgm=fit.StVariogram(sample.stvgm,prior.stvgm)
>
> fit.stVariogram works without producing any errors, however if I adjust any of these parameters I get some strange errors.  For example by using
>
> prior.stvgm=vgmST(stModel="sumMetric",
>                         space=vgm(0.1,"Exp",0.5,0.3),
>                         time=vgm(0.1,"Exp",0.5,0.3),
>                         joint=vgm(0.1,"Exp",0.5,0.3),
>                         stAni=1)
>
> Then I get this error, despite the fact that stAni is clearly positive:
>
>       Error in vgmST("sumMetric", space = vgm(par[1], as.character(model$space$model[2]),  :
>       "stAni" must be positive.
>
> As another example if I enter
>
> prior.stvgm=vgmST(stModel="sumMetric",
>                         space=vgm(0.1,"Exp",0.5,0.3),
>                         time=vgm(0.1,"Exp",0.5,0.3),
>                         joint=vgm(0.1,"Exp",0.5,0.3),
>                         stAni=0.99)
>
> It will produce the following error, despite the fact that all ranges are clearly positive:
>
>      Error in vgm(par[1], as.character(model$space$model[2]), par[2], par[3],  :
>       range should be positive
>
> The only thing that changes among these examples is the value of the stAni parameter.  When stAni=0.5 everything is fine, if stAni=1 then apparently stAni isn't positive and if stAni=0.99 then apparently the range isn't positive.  When I look at the plot of the sample variogram from my data, it appears that the spatial sill =1, spatial range=0.5, temporal sill=0.85, temporal range=4, joint sill=1.1 ,joint range=1.0 however these values also produce an error saying that range should be positive.  I'm finding these errors to be very confusing and would appreciate any tips anyone might be able to give.
>
> Thank you,
>
> Daniel Turenne
> University of Manitoba
>
>
> ________________________________
> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler <benedikt.graeler at rub.de>
> Sent: June 23, 2016 2:54 AM
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat
>
> Dear Dan,
>
> the error occurs because your prior.stvgm is far from the structure of
> your data. The following model provides better starting values:
>
> prior.stvgm=vgmST(stModel="sumMetric",
>                          space=vgm(0.1,"Exp",0.5,0.3),
>                          time=vgm(0.1,"Exp",0.5,0.3),
>                          joint=vgm(0.1,"Exp",0.5,0.3),
>                          stAni=1)
>
> To get a first "estimate", it is always useful to look at a plot of the
> sample variogram (as in the spatial case):
>
> plot(sample.stvgm, wireframe=T, scales=list(arrows=F))
>
>  From the plot, one can read spatial and temporal ranges, the magnitude
> of variability and the ratios of nuggets and sills. Sometimes, one is
> better of by starting with a simpler model (e.g. "pure" metric), as the
> routines are more robust for less parameters, to get a first impression
> of the range of parameters. Furthermore, the plot suggests that a much
> smaller spatial upper boundary is sufficient, as the variogram surface
> is mainly flat in the spatial dimension.
>
> Spatio-temporal geostatistics remains tricky in terms of numerical
> routines and model interpretation, we try to give some guidance
> regarding gstat's functionalities in [1].
>
> HTH,
>
>   Ben
>
> [1] https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf
> Spatio-Temporal Interpolation using gstat<https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf>
> journal.r-project.org
> CONTRIBUTED RESEARCH ARTICLE 1 Spatio-Temporal Interpolation using gstat by Benedikt Gr?ler, Edzer Pebesma and Gerard Heuvelink Abstract We present new spatio ...
>
>
>
>
>
> On 21.06.2016 17:16, Dan Turenne wrote:
>> Hello again,
>>
>>
>> After some tinkering with my code last night I got the sample variogram to work however now I have encountered another issue.  When attempting to use the fit.stVariogram function to fit the sample variogram I receive this error:
>>
>>
>> Error in optim(extractPar(model), fitFun, ..., method = method, lower = lower,  :
>>   non-finite value supplied by optim
>>
>>
>> even if I specify the upper and lower arguments to be finite.  Could this error be cause by the initial values that I set for the space, time and joint variograms?  If so can anyone recommend  a good paper/article about how to fit these three variograms?  I have attached a link to my script and data below, if anyone has any insight as to why I am getting this error it would be appreciated.  I tried to be as thorough as I could in my code commenting, please let me know if anything is unclear.
>>
>>
>> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>>
>>
>> ________________________________
>> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Edzer Pebesma <edzer.pebesma at uni-muenster.de>
>> Sent: June 21, 2016 1:52 AM
>> To: r-sig-geo at r-project.org
>> Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat
>>
>> This might be due to a bug (or feature) in the software caused by the
>> sparseness in your data, which might be different from that used to test
>> the software. Please make the data available (off-list), along with an R
>> script, so we can try to reproduce the error message and look into it.
>>
>> On 21/06/16 04:11, Dan Turenne wrote:
>>> My apologies, I accidentally sent an unfinished email, here is the complete version of my question
>>>
>>>
>>> Hello R-Sig-Geo,
>>>
>>>
>>> As part of my masters thesis I am attempting to use spatio-temporal regression kriging to make predictions with temperature data, and I was hoping that someone might be able to give some insight as to how the algorithms work in gstat.  My data consists of daily temperature observations from April 1 to July 31, 2000.  There are observations from 164 stations across these 122 days, however not all stations have observations on all days, making for a total of 19282 records.
>>>
>>>
>>> I have tried to use an STSDF object but I have not had any success.  I created an sp object of length 164 with the station locations:
>>>
>>>
>>>     sp = data.frame(long = stations$long, lat = stations$lat)
>>>
>>>     coordinates(sp) = ~ long+lat
>>>
>>>
>>> Then I created a vector of length 122 with the times the observations were recorded and a data vector of length 19282:
>>>
>>>
>>>     beginDate = as.Date(2000/04/01)
>>>
>>>     endDate = as.Date(2000/07/31)
>>>
>>>     times = as.POSIXct(seq(beginDate,endDate,by="days"))
>>>
>>>
>>>     data=data.frame(temps$residual)
>>>
>>>
>>> And I also made an index detailing where observations are available, it looks like this with the first column representing spatial index and the second representing the time index
>>>
>>>
>>>     1   1
>>>
>>>     2   1
>>>
>>>     3   1
>>>
>>>     4   1
>>>
>>>
>>>     st=STSDF(sp,time,data,index,endTime=delta(time))
>>>
>>>
>>> however when I try to calculate the sample variogram I get the following error:
>>>
>>>
>>>     sample.stVariogram=variogramST(residual~1,data=st, tunit="days", tlags=1:7, progress=TRUE)
>>>
>>>
>>>
>>>    Error in apply(do.call(cbind, lapply(ret, function(x) x$np)), 1, sum,  :
>>>    dim(X) must have a positive length
>>>    In addition: There were 50 or more warnings (use warnings() to see the first 50)
>>>
>>>
>>> All 50 of the errors are :
>>>
>>>
>>>   In is.na(data[[as.character(as.list(formula)[[2]])]]) :
>>>    is.na() applied to non-(list or vector) of type 'NULL'
>>>
>>>
>>> Can anyone see what I am doing wrong or give me any pointers?  This error  is rather cryptic and I'm not quite sure what I'm doing wrong.  Any help would be appreciated.
>>>
>>>
>>> Many Thanks,
>>>
>>> Daniel Turenne
>>>
>>> University of Manitoba
>>>
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> _______________________________________________
>>> R-sig-Geo mailing list
>>> R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>> R-sig-Geo - R Special Interest Group on using Geographical ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
>> stat.ethz.ch
>> R-sig-Geo -- R Special Interest Group on using Geographical data and Mapping About R-sig-Geo
>>
>>
>>
>>>
>>
>> --
>> Edzer Pebesma
>> Institute for Geoinformatics  (ifgi),  University of M?nster
>> Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
>> Journal of Statistical Software:   http://www.jstatsoft.org/
>> Computers & Geosciences:   http://elsevier.com/locate/cageo/
>> Spatial Statistics Society http://www.spatialstatistics.info
>>
>>
>>        [[alternative HTML version deleted]]
>>
>>
>>
>> _______________________________________________
>> R-sig-Geo mailing list
>> R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Dr. Benedikt Gr?ler
> Institute of Hydrology
> Ruhr University Bochum
> Universit?tsstra?e 150
> 44801 Bochum
>
> ben.graeler.org
>
> +49 (0234) 32 - 27619
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>        [[alternative HTML version deleted]]
>
>
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

--
Dr. Benedikt Gr?ler
Institute of Hydrology
Ruhr University Bochum
Universit?tsstra?e 150
44801 Bochum

ben.graeler.org

+49 (0234) 32 - 27619

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From mauriziomarchi85 at gmail.com  Tue Jun 28 17:16:54 2016
From: mauriziomarchi85 at gmail.com (Maurizio Marchi)
Date: Tue, 28 Jun 2016 17:16:54 +0200
Subject: [R-sig-Geo] Absences dataset
Message-ID: <CANJhsN34NSnPAhNa68jLY653PYOZfwt93QrJM7_6XtqenaWKzw@mail.gmail.com>

Hello everybody,
I have a question about species distribution modelling and the biomod2
package.
I have been working with the biomod2 package many times and often using the
PA selection function to create more than one pseudo-absences dataset. This
was mainly due to the lack of available information about true absences.
Now I'm i a  very good position because I have to work wit both presences
and absences but a new problem arose: the number of absences is much higher
than the presences (e.g 700 presences versus 6000 absences). So I would
like to calculate more than one run to average the results, similarly to
what I'm use to do with the pseudo-absences... So, my question is: is there
the possibility to do it automatically with the biomod2 package or should I
have to create a cycle to repeat the calculation and average the resulting
maps manually?

Many thanks, maurizio

-- 
Maurizio Marchi
Calenzano (FI) - Italy
ID Skype: maurizioxyz
Ubuntu 14.04 LTS
linux user 552742

	[[alternative HTML version deleted]]


From friedman.steve at gmail.com  Tue Jun 28 18:20:25 2016
From: friedman.steve at gmail.com (Steve Friedman)
Date: Tue, 28 Jun 2016 12:20:25 -0400
Subject: [R-sig-Geo] spatial anova
In-Reply-To: <DUB112-W113FEE2BA896A3CC1C6E696A9220@phx.gbl>
References: <DUB112-W113FEE2BA896A3CC1C6E696A9220@phx.gbl>
Message-ID: <CADkZ+_UN63fn+WOiof_v7hcpXtipJ2r93rH-5=VacOGdBkOf4Q@mail.gmail.com>

I would start by reading google and R posts regarding spatial anova. There
is plenty of help among that literature.  Formulate your model, share it,
and then you might have better questions.

Steve

On Tuesday, June 28, 2016, Csany22 csany <csany22 at hotmail.com> wrote:

> Hi R-sig-geo people,
>
>
> what would be the best way to run spatial anova in R? I have a dependent
> (continuous), independent (categorical) variables and lat and long fo reach
> of the cell(pixel) based observations. I need to run anova. as would like
> to compare the means (Tukey) of the independent variable.
>
>         [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org <javascript:;>
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>


-- 
Steve Friedman
Cell: 517-648-6290
Web:  www.envisioningecosystemsdecisions.com

	[[alternative HTML version deleted]]


From andy.teucher at gmail.com  Tue Jun 28 19:45:33 2016
From: andy.teucher at gmail.com (Andy Teucher)
Date: Tue, 28 Jun 2016 10:45:33 -0700
Subject: [R-sig-Geo] FYI - bug in GDAL 2.1.0: FID detection in GeoJSON
 results wrong feature count
Message-ID: <CAOdCUxNdoGEhjU4hJdn7fPsibnK_5i-V_Oyw-v7NwCSr_P_3ww@mail.gmail.com>

There is a small known bug in GDAL 2.1.0 that I was bitten by when using
rgdal::readOGR to read in GeoJSON data.

The bug is with the GeoJSON format, where if you have multiple polygons
with the same "id" property, only one will be recognized (The actual bug is
more complex than that, but that was my circumstance).

The bug report is here: https://trac.osgeo.org/gdal/ticket/6538, and my
demonstration of it with rgdal is here:
https://gist.github.com/ateucher/59365353d61ab41bf7ae14905941eaab.

It looks like it has been fixed in GDAL 2.1.1. I tested it with GDAL 2.0.1
and the current development version (2.2.x) and the issue is not there, but
I think 2.1.0 is common on a lot of systems right now. So if you are
working with GeoJSON and have that GDAL version, keep your eye out for it.

Cheers,
Andy Teucher

	[[alternative HTML version deleted]]


From isaquedanielre at hotmail.com  Wed Jun 29 03:26:52 2016
From: isaquedanielre at hotmail.com (Isaque Daniel)
Date: Wed, 29 Jun 2016 01:26:52 +0000
Subject: [R-sig-Geo] How to create an ellipse buffer using
	SpatialPointsDataFrame?
Message-ID: <CO1PR20MB0773604CC53D3EC8003BD0F6C9230@CO1PR20MB0773.namprd20.prod.outlook.com>

Hi everyone,


I have been working in a model for spatial analysis and my problem now is generate some ellipses around of SpatialPointsDataFrame.


I looking for some similar gBuffer of rgeos, but until now I can't find a solution.

Any suggestion?

Thanks in advance

Best
Isaque


------------------------------------------------------------------------------------------------------------------
Agronomist engineer
Master in Remote Sensing - National  Institute for Space Research (INPE) - Brazil
PHD Student in Transport - Brazilia University (UNB)

	[[alternative HTML version deleted]]


From FloBetz at web.de  Wed Jun 29 09:35:12 2016
From: FloBetz at web.de (Florian Betz)
Date: Wed, 29 Jun 2016 09:35:12 +0200
Subject: [R-sig-Geo] How to create an ellipse buffer using
 SpatialPointsDataFrame?
In-Reply-To: <CO1PR20MB0773604CC53D3EC8003BD0F6C9230@CO1PR20MB0773.namprd20.prod.outlook.com>
References: <CO1PR20MB0773604CC53D3EC8003BD0F6C9230@CO1PR20MB0773.namprd20.prod.outlook.com>
Message-ID: <57737A30.7030009@web.de>

Dear Isaque,

you could check out the v.buffer module in GRASS GIS which has a buffer 
distance for a major and minor axis. I never tried it out, but maybe 
this is what you are looking for...

You can run the GRASS commands also from R using the rgrass7 package.

1) Initialize the R-GRASS connection using initGRASS()
2)running the v.buffer module via execGRASS(cmd="v.buffer", ...)


Regards,

Flo



Am 29.06.2016 um 03:26 schrieb Isaque Daniel:
> Hi everyone,
>
>
> I have been working in a model for spatial analysis and my problem now is generate some ellipses around of SpatialPointsDataFrame.
>
>
> I looking for some similar gBuffer of rgeos, but until now I can't find a solution.
>
> Any suggestion?
>
> Thanks in advance
>
> Best
> Isaque
>
>
> ------------------------------------------------------------------------------------------------------------------
> Agronomist engineer
> Master in Remote Sensing - National  Institute for Space Research (INPE) - Brazil
> PHD Student in Transport - Brazilia University (UNB)
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From benedikt.graeler at rub.de  Wed Jun 29 10:04:42 2016
From: benedikt.graeler at rub.de (Benedikt Graeler)
Date: Wed, 29 Jun 2016 10:04:42 +0200
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <BLUPR12MB0625BC2865577CD885B4FBE0B7220@BLUPR12MB0625.namprd12.prod.outlook.com>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<1454a622-6b1e-38fe-293d-c0d1936bf47c@rub.de>
	<SN1PR12MB0638D399DC32C01F498D9A13B7200@SN1PR12MB0638.namprd12.prod.outlook.com>
	<e27ae85a-2f0d-9020-7ca0-a52a6f866ea2@rub.de>
	<BLUPR12MB0625BC2865577CD885B4FBE0B7220@BLUPR12MB0625.namprd12.prod.outlook.com>
Message-ID: <94768783-f712-6680-69d4-5c77228bb00b@rub.de>

Dear Daniel,

On 28.06.2016 16:42, Dan Turenne wrote:
> Thank you for the information, I was able to get around the problems
> I was having by setting upper to Inf and lower to 0.0000001 (An
> arbitrary small value to guarantee that the value will be greater
> then 0).  After working some more on my code I had just a few other
> general questions I was hoping someone could answer.
Note, that you can specify a boundary for each parameter. The arguments
upper and lower are recycled up to the length where they match the
parameter vector length. "Inf" is fine as a bound, but narrowing down
the parameter space also might improve/speed up the estimation.

> 1.  Whenever I call variogramST to calculate the sample variogram I
> get a bunch of repeating output that says $tunit [1] "days" over and
> over again.  I have tried setting progress to FALSE but this output
> still occurs.  Is there anyway to suppress this output?
The argument "tunit" is only necessary if a STIDF is provided. In your
script, you create a STSDF and a different routine is used in the
background (assuming "tlags" refers to index differences). All arguments
that are not recognized by "variogramST" are passed on to the space only
function "variogram", that does not know how to handle it.
Solution: just do not provide the argument "tunit" or provide a STIDF
where a "tunit" is necessary.

> 2.  When using krigeST to get predictions, the predictions are
> returned in the form of a single column, however there is no index to
> say which prediction corresponds to which location.  Is this column
> organized with the spatial index moving faster or the temporal index
> moving faster?
The column is supposed to be in the same order as the target geometry.
Which version of gstat are you using? Unless "fullCovariance=TRUE", the
returned object should be of the form ST*DF.

HTH,

  Ben

>
> Thank you in advance for any advice, everyone on this list has been
> very friendly and helpful and it is greatly appreciated.
>
> Thanks, Daniel Turenne ________________________________ From:
> R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt
> Graeler <benedikt.graeler at rub.de> Sent: June 28, 2016 5:23 AM To:
> r-sig-geo at r-project.org Subject: Re: [R-sig-Geo] Spatio Temporal
> kriging in Gstat
>
> Dear Daniel,
>
> the errors occur during the optimisation where optim tries different
> (possibly non-positive) values to find the best fit. Providing
> sensitive limits via "lower" and "upper" to optim should resolve the
> issue.
>
> HTH,
>
> Ben
>
> On 26.06.2016 16:30, Dan Turenne wrote:
>> Hello Listers,
>>
>>
>> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>>
>>
>>
>>
Thanks to the help of Dr. Graeler I have been having some success with 
my spatio temporal kriging script but I had some more questions I was 
hoping someone could help with.  When fitting the ST variogram with 
fit.stVariogram, I have noticed that the function is extremely sensitive 
to the choice of initial parameters.  I have included a link to my 
script at the beginning of this email.  If I use
>>
>>
>> prior.stvgm=vgmST(stModel="sumMetric",
>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=0.5)
>>
>> fitted.stvgm=fit.StVariogram(sample.stvgm,prior.stvgm)
>>
>> fit.stVariogram works without producing any errors, however if I
>> adjust any of these parameters I get some strange errors.  For
>> example by using
>>
>> prior.stvgm=vgmST(stModel="sumMetric",
>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=1)
>>
>> Then I get this error, despite the fact that stAni is clearly
>> positive:
>>
>> Error in vgmST("sumMetric", space = vgm(par[1],
>> as.character(model$space$model[2]),  : "stAni" must be positive.
>>
>> As another example if I enter
>>
>> prior.stvgm=vgmST(stModel="sumMetric",
>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=0.99)
>>
>> It will produce the following error, despite the fact that all
>> ranges are clearly positive:
>>
>> Error in vgm(par[1], as.character(model$space$model[2]), par[2],
>> par[3],  : range should be positive
>>
>> The only thing that changes among these examples is the value of
>> the stAni parameter.  When stAni=0.5 everything is fine, if stAni=1
>> then apparently stAni isn't positive and if stAni=0.99 then
>> apparently the range isn't positive.  When I look at the plot of
>> the sample variogram from my data, it appears that the spatial sill
>> =1, spatial range=0.5, temporal sill=0.85, temporal range=4, joint
>> sill=1.1 ,joint range=1.0 however these values also produce an
>> error saying that range should be positive.  I'm finding these
>> errors to be very confusing and would appreciate any tips anyone
>> might be able to give.
>>
>> Thank you,
>>
>> Daniel Turenne University of Manitoba
>>
>>
>> ________________________________ From: R-sig-Geo
>> <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler
>> <benedikt.graeler at rub.de> Sent: June 23, 2016 2:54 AM To:
>> r-sig-geo at r-project.org Subject: Re: [R-sig-Geo] Spatio Temporal
>> kriging in Gstat
>>
>> Dear Dan,
>>
>> the error occurs because your prior.stvgm is far from the structure
>> of your data. The following model provides better starting values:
>>
>> prior.stvgm=vgmST(stModel="sumMetric",
>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=1)
>>
>> To get a first "estimate", it is always useful to look at a plot of
>> the sample variogram (as in the spatial case):
>>
>> plot(sample.stvgm, wireframe=T, scales=list(arrows=F))
>>
>> From the plot, one can read spatial and temporal ranges, the
>> magnitude of variability and the ratios of nuggets and sills.
>> Sometimes, one is better of by starting with a simpler model (e.g.
>> "pure" metric), as the routines are more robust for less
>> parameters, to get a first impression of the range of parameters.
>> Furthermore, the plot suggests that a much smaller spatial upper
>> boundary is sufficient, as the variogram surface is mainly flat in
>> the spatial dimension.
>>
>> Spatio-temporal geostatistics remains tricky in terms of numerical
>> routines and model interpretation, we try to give some guidance
>> regarding gstat's functionalities in [1].
>>
>> HTH,
>>
>> Ben
>>
>> [1]
>> https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf
>>
>>
Spatio-Temporal Interpolation using 
gstat<https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf>
>> journal.r-project.org CONTRIBUTED RESEARCH ARTICLE 1
>> Spatio-Temporal Interpolation using gstat by Benedikt Gr?ler, Edzer
>> Pebesma and Gerard Heuvelink Abstract We present new spatio ...
>>
>>
>>
>>
>>
>> On 21.06.2016 17:16, Dan Turenne wrote:
>>> Hello again,
>>>
>>>
>>> After some tinkering with my code last night I got the sample
>>> variogram to work however now I have encountered another issue.
>>> When attempting to use the fit.stVariogram function to fit the
>>> sample variogram I receive this error:
>>>
>>>
>>> Error in optim(extractPar(model), fitFun, ..., method = method,
>>> lower = lower,  : non-finite value supplied by optim
>>>
>>>
>>> even if I specify the upper and lower arguments to be finite.
>>> Could this error be cause by the initial values that I set for
>>> the space, time and joint variograms?  If so can anyone recommend
>>> a good paper/article about how to fit these three variograms?  I
>>> have attached a link to my script and data below, if anyone has
>>> any insight as to why I am getting this error it would be
>>> appreciated.  I tried to be as thorough as I could in my code
>>> commenting, please let me know if anything is unclear.
>>>
>>>
>>> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>>>
>>>
>>>
>>>
________________________________
>>> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
>>> Edzer Pebesma <edzer.pebesma at uni-muenster.de> Sent: June 21, 2016
>>> 1:52 AM To: r-sig-geo at r-project.org Subject: Re: [R-sig-Geo]
>>> Spatio Temporal kriging in Gstat
>>>
>>> This might be due to a bug (or feature) in the software caused by
>>> the sparseness in your data, which might be different from that
>>> used to test the software. Please make the data available
>>> (off-list), along with an R script, so we can try to reproduce
>>> the error message and look into it.
>>>
>>> On 21/06/16 04:11, Dan Turenne wrote:
>>>> My apologies, I accidentally sent an unfinished email, here is
>>>> the complete version of my question
>>>>
>>>>
>>>> Hello R-Sig-Geo,
>>>>
>>>>
>>>> As part of my masters thesis I am attempting to use
>>>> spatio-temporal regression kriging to make predictions with
>>>> temperature data, and I was hoping that someone might be able
>>>> to give some insight as to how the algorithms work in gstat.
>>>> My data consists of daily temperature observations from April 1
>>>> to July 31, 2000.  There are observations from 164 stations
>>>> across these 122 days, however not all stations have
>>>> observations on all days, making for a total of 19282 records.
>>>>
>>>>
>>>> I have tried to use an STSDF object but I have not had any
>>>> success.  I created an sp object of length 164 with the station
>>>> locations:
>>>>
>>>>
>>>> sp = data.frame(long = stations$long, lat = stations$lat)
>>>>
>>>> coordinates(sp) = ~ long+lat
>>>>
>>>>
>>>> Then I created a vector of length 122 with the times the
>>>> observations were recorded and a data vector of length 19282:
>>>>
>>>>
>>>> beginDate = as.Date(2000/04/01)
>>>>
>>>> endDate = as.Date(2000/07/31)
>>>>
>>>> times = as.POSIXct(seq(beginDate,endDate,by="days"))
>>>>
>>>>
>>>> data=data.frame(temps$residual)
>>>>
>>>>
>>>> And I also made an index detailing where observations are
>>>> available, it looks like this with the first column
>>>> representing spatial index and the second representing the time
>>>> index
>>>>
>>>>
>>>> 1   1
>>>>
>>>> 2   1
>>>>
>>>> 3   1
>>>>
>>>> 4   1
>>>>
>>>>
>>>> st=STSDF(sp,time,data,index,endTime=delta(time))
>>>>
>>>>
>>>> however when I try to calculate the sample variogram I get the
>>>> following error:
>>>>
>>>>
>>>> sample.stVariogram=variogramST(residual~1,data=st,
>>>> tunit="days", tlags=1:7, progress=TRUE)
>>>>
>>>>
>>>>
>>>> Error in apply(do.call(cbind, lapply(ret, function(x) x$np)),
>>>> 1, sum,  : dim(X) must have a positive length In addition:
>>>> There were 50 or more warnings (use warnings() to see the first
>>>> 50)
>>>>
>>>>
>>>> All 50 of the errors are :
>>>>
>>>>
>>>> In is.na(data[[as.character(as.list(formula)[[2]])]]) : is.na()
>>>> applied to non-(list or vector) of type 'NULL'
>>>>
>>>>
>>>> Can anyone see what I am doing wrong or give me any pointers?
>>>> This error  is rather cryptic and I'm not quite sure what I'm
>>>> doing wrong.  Any help would be appreciated.
>>>>
>>>>
>>>> Many Thanks,
>>>>
>>>> Daniel Turenne
>>>>
>>>> University of Manitoba
>>>>
>>>>
>>>> [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________ R-sig-Geo
>>>> mailing list R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>> R-sig-Geo - R Special Interest Group on using Geographical
>>> ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
>>> stat.ethz.ch R-sig-Geo -- R Special Interest Group on using
>>> Geographical data and Mapping About R-sig-Geo
>>>
>>>
>>>
>>>>
>>>
>>> -- Edzer Pebesma Institute for Geoinformatics  (ifgi),
>>> University of M?nster Heisenbergstra?e 2, 48149 M?nster, Germany;
>>> +49 251 83 33081 Journal of Statistical Software:
>>> http://www.jstatsoft.org/ Computers & Geosciences:
>>> http://elsevier.com/locate/cageo/ Spatial Statistics Society
>>> http://www.spatialstatistics.info
>>>
>>>
>>> [[alternative HTML version deleted]]
>>>
>>>
>>>
>>> _______________________________________________ R-sig-Geo mailing
>>> list R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> -- Dr. Benedikt Gr?ler Institute of Hydrology Ruhr University
>> Bochum Universit?tsstra?e 150 44801 Bochum
>>
>> ben.graeler.org
>>
>> +49 (0234) 32 - 27619
>>
>> _______________________________________________ R-sig-Geo mailing
>> list R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> [[alternative HTML version deleted]]
>>
>>
>>
>> _______________________________________________ R-sig-Geo mailing
>> list R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> -- Dr. Benedikt Gr?ler Institute of Hydrology Ruhr University Bochum
> Universit?tsstra?e 150 44801 Bochum
>
> ben.graeler.org
>
> +49 (0234) 32 - 27619
>
> _______________________________________________ R-sig-Geo mailing
> list R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> [[alternative HTML version deleted]]
>
> _______________________________________________ R-sig-Geo mailing
> list R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

-- 
Dr. Benedikt Gr?ler
Institute of Hydrology
Ruhr University Bochum
Universit?tsstra?e 150
44801 Bochum

ben.graeler.org

+49 (0234) 32 - 27619


From b.rowlingson at lancaster.ac.uk  Wed Jun 29 15:34:28 2016
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 29 Jun 2016 14:34:28 +0100
Subject: [R-sig-Geo] How to create an ellipse buffer using
	SpatialPointsDataFrame?
In-Reply-To: <CO1PR20MB0773604CC53D3EC8003BD0F6C9230@CO1PR20MB0773.namprd20.prod.outlook.com>
References: <CO1PR20MB0773604CC53D3EC8003BD0F6C9230@CO1PR20MB0773.namprd20.prod.outlook.com>
Message-ID: <CANVKczO8dsjqN=V=RGwvb5FB-Neg4b3XbwQ2GtXYN2wXgkE9Vg@mail.gmail.com>

On Wed, Jun 29, 2016 at 2:26 AM, Isaque Daniel
<isaquedanielre at hotmail.com> wrote:
> Hi everyone,
>
>
> I have been working in a model for spatial analysis and my problem now is generate some ellipses around of SpatialPointsDataFrame.
>
>
> I looking for some similar gBuffer of rgeos, but until now I can't find a solution.

 Do you want an ellipse centred on each point of your
SpatialPointsDataFrame? You can compute ellipse coordinates with a bit
of trigonometry given the major/minor axis lengths or eccentricity and
centre. Make these into SpatialPolygons and if they overlap then you
can use rgeos' routines to merge them all.

 No need for buffer routines! An elliptical buffer of a point is just
an ellipse.

Barry


From DanielTurenne at hotmail.com  Wed Jun 29 20:52:58 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Wed, 29 Jun 2016 18:52:58 +0000
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <94768783-f712-6680-69d4-5c77228bb00b@rub.de>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<1454a622-6b1e-38fe-293d-c0d1936bf47c@rub.de>
	<SN1PR12MB0638D399DC32C01F498D9A13B7200@SN1PR12MB0638.namprd12.prod.outlook.com>
	<e27ae85a-2f0d-9020-7ca0-a52a6f866ea2@rub.de>
	<BLUPR12MB0625BC2865577CD885B4FBE0B7220@BLUPR12MB0625.namprd12.prod.outlook.com>,
	<94768783-f712-6680-69d4-5c77228bb00b@rub.de>
Message-ID: <BLUPR12MB06250D8233BF8FA0FD865250B7230@BLUPR12MB0625.namprd12.prod.outlook.com>

Hello Ben,

Thank you for the tip about "tunit", it worked perfectly as you described.  As for my second question, the object returned by krigeST is indeed of the form STFDF.  After some digging I found the following information regarding the data from an STFDF object on inside-r.org:

data<http://inside-r.org/r-doc/utils/data>:
Object of class data.frame<http://inside-r.org/r-doc/base/data.frame>, which holds the measured values; space index cycling first, time order preservedI would interpret this as follows: if I had x prediction locations, the first set of x values from the predictions vector will be for day 1, the second set of x values will be for day 2 etc.  The order of the stations will be the same as they were in the sp object used to create the prediction grid.

Is this interpretation correct or am I missing something?  Once again your help is greatly appreciated.

Thanks,
Daniel Turenne
________________________________
From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler <benedikt.graeler at rub.de>
Sent: June 29, 2016 3:04 AM
To: r-sig-geo at r-project.org
Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat

Dear Daniel,

On 28.06.2016 16:42, Dan Turenne wrote:
> Thank you for the information, I was able to get around the problems
> I was having by setting upper to Inf and lower to 0.0000001 (An
> arbitrary small value to guarantee that the value will be greater
> then 0).  After working some more on my code I had just a few other
> general questions I was hoping someone could answer.
Note, that you can specify a boundary for each parameter. The arguments
upper and lower are recycled up to the length where they match the
parameter vector length. "Inf" is fine as a bound, but narrowing down
the parameter space also might improve/speed up the estimation.

> 1.  Whenever I call variogramST to calculate the sample variogram I
> get a bunch of repeating output that says $tunit [1] "days" over and
> over again.  I have tried setting progress to FALSE but this output
> still occurs.  Is there anyway to suppress this output?
The argument "tunit" is only necessary if a STIDF is provided. In your
script, you create a STSDF and a different routine is used in the
background (assuming "tlags" refers to index differences). All arguments
that are not recognized by "variogramST" are passed on to the space only
function "variogram", that does not know how to handle it.
Solution: just do not provide the argument "tunit" or provide a STIDF
where a "tunit" is necessary.

> 2.  When using krigeST to get predictions, the predictions are
> returned in the form of a single column, however there is no index to
> say which prediction corresponds to which location.  Is this column
> organized with the spatial index moving faster or the temporal index
> moving faster?
The column is supposed to be in the same order as the target geometry.
Which version of gstat are you using? Unless "fullCovariance=TRUE", the
returned object should be of the form ST*DF.

HTH,

  Ben

>
> Thank you in advance for any advice, everyone on this list has been
> very friendly and helpful and it is greatly appreciated.
>
> Thanks, Daniel Turenne ________________________________ From:
> R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt
> Graeler <benedikt.graeler at rub.de> Sent: June 28, 2016 5:23 AM To:
> r-sig-geo at r-project.org Subject: Re: [R-sig-Geo] Spatio Temporal
> kriging in Gstat
>
> Dear Daniel,
>
> the errors occur during the optimisation where optim tries different
> (possibly non-positive) values to find the best fit. Providing
> sensitive limits via "lower" and "upper" to optim should resolve the
> issue.
>
> HTH,
>
> Ben
>
> On 26.06.2016 16:30, Dan Turenne wrote:
>> Hello Listers,
>>
>>
>> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
[https://cf.dropboxstatic.com/static/images/icons128/folder_dropbox.png]<https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0>

Spatio temporal kriging<https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0>
www.dropbox.com
Shared with Dropbox



>>
>>
>>
>>
Thanks to the help of Dr. Graeler I have been having some success with
my spatio temporal kriging script but I had some more questions I was
hoping someone could help with.  When fitting the ST variogram with
fit.stVariogram, I have noticed that the function is extremely sensitive
to the choice of initial parameters.  I have included a link to my
script at the beginning of this email.  If I use
>>
>>
>> prior.stvgm=vgmST(stModel="sumMetric",
>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=0.5)
>>
>> fitted.stvgm=fit.StVariogram(sample.stvgm,prior.stvgm)
>>
>> fit.stVariogram works without producing any errors, however if I
>> adjust any of these parameters I get some strange errors.  For
>> example by using
>>
>> prior.stvgm=vgmST(stModel="sumMetric",
>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=1)
>>
>> Then I get this error, despite the fact that stAni is clearly
>> positive:
>>
>> Error in vgmST("sumMetric", space = vgm(par[1],
>> as.character(model$space$model[2]),  : "stAni" must be positive.
>>
>> As another example if I enter
>>
>> prior.stvgm=vgmST(stModel="sumMetric",
>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=0.99)
>>
>> It will produce the following error, despite the fact that all
>> ranges are clearly positive:
>>
>> Error in vgm(par[1], as.character(model$space$model[2]), par[2],
>> par[3],  : range should be positive
>>
>> The only thing that changes among these examples is the value of
>> the stAni parameter.  When stAni=0.5 everything is fine, if stAni=1
>> then apparently stAni isn't positive and if stAni=0.99 then
>> apparently the range isn't positive.  When I look at the plot of
>> the sample variogram from my data, it appears that the spatial sill
>> =1, spatial range=0.5, temporal sill=0.85, temporal range=4, joint
>> sill=1.1 ,joint range=1.0 however these values also produce an
>> error saying that range should be positive.  I'm finding these
>> errors to be very confusing and would appreciate any tips anyone
>> might be able to give.
>>
>> Thank you,
>>
>> Daniel Turenne University of Manitoba
>>
>>
>> ________________________________ From: R-sig-Geo
>> <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler
>> <benedikt.graeler at rub.de> Sent: June 23, 2016 2:54 AM To:
>> r-sig-geo at r-project.org Subject: Re: [R-sig-Geo] Spatio Temporal
>> kriging in Gstat
>>
>> Dear Dan,
>>
>> the error occurs because your prior.stvgm is far from the structure
>> of your data. The following model provides better starting values:
>>
>> prior.stvgm=vgmST(stModel="sumMetric",
>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=1)
>>
>> To get a first "estimate", it is always useful to look at a plot of
>> the sample variogram (as in the spatial case):
>>
>> plot(sample.stvgm, wireframe=T, scales=list(arrows=F))
>>
>> From the plot, one can read spatial and temporal ranges, the
>> magnitude of variability and the ratios of nuggets and sills.
>> Sometimes, one is better of by starting with a simpler model (e.g.
>> "pure" metric), as the routines are more robust for less
>> parameters, to get a first impression of the range of parameters.
>> Furthermore, the plot suggests that a much smaller spatial upper
>> boundary is sufficient, as the variogram surface is mainly flat in
>> the spatial dimension.
>>
>> Spatio-temporal geostatistics remains tricky in terms of numerical
>> routines and model interpretation, we try to give some guidance
>> regarding gstat's functionalities in [1].
>>
>> HTH,
>>
>> Ben
>>
>> [1]
>> https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf
>>
>>
Spatio-Temporal Interpolation using
gstat<https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf>
>> journal.r-project.org CONTRIBUTED RESEARCH ARTICLE 1
>> Spatio-Temporal Interpolation using gstat by Benedikt Gr?ler, Edzer
>> Pebesma and Gerard Heuvelink Abstract We present new spatio ...
>>
>>
>>
>>
>>
>> On 21.06.2016 17:16, Dan Turenne wrote:
>>> Hello again,
>>>
>>>
>>> After some tinkering with my code last night I got the sample
>>> variogram to work however now I have encountered another issue.
>>> When attempting to use the fit.stVariogram function to fit the
>>> sample variogram I receive this error:
>>>
>>>
>>> Error in optim(extractPar(model), fitFun, ..., method = method,
>>> lower = lower,  : non-finite value supplied by optim
>>>
>>>
>>> even if I specify the upper and lower arguments to be finite.
>>> Could this error be cause by the initial values that I set for
>>> the space, time and joint variograms?  If so can anyone recommend
>>> a good paper/article about how to fit these three variograms?  I
>>> have attached a link to my script and data below, if anyone has
>>> any insight as to why I am getting this error it would be
>>> appreciated.  I tried to be as thorough as I could in my code
>>> commenting, please let me know if anything is unclear.
>>>
>>>
>>> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>>>
>>>
>>>
>>>
________________________________
>>> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
>>> Edzer Pebesma <edzer.pebesma at uni-muenster.de> Sent: June 21, 2016
>>> 1:52 AM To: r-sig-geo at r-project.org Subject: Re: [R-sig-Geo]
>>> Spatio Temporal kriging in Gstat
>>>
>>> This might be due to a bug (or feature) in the software caused by
>>> the sparseness in your data, which might be different from that
>>> used to test the software. Please make the data available
>>> (off-list), along with an R script, so we can try to reproduce
>>> the error message and look into it.
>>>
>>> On 21/06/16 04:11, Dan Turenne wrote:
>>>> My apologies, I accidentally sent an unfinished email, here is
>>>> the complete version of my question
>>>>
>>>>
>>>> Hello R-Sig-Geo,
>>>>
>>>>
>>>> As part of my masters thesis I am attempting to use
>>>> spatio-temporal regression kriging to make predictions with
>>>> temperature data, and I was hoping that someone might be able
>>>> to give some insight as to how the algorithms work in gstat.
>>>> My data consists of daily temperature observations from April 1
>>>> to July 31, 2000.  There are observations from 164 stations
>>>> across these 122 days, however not all stations have
>>>> observations on all days, making for a total of 19282 records.
>>>>
>>>>
>>>> I have tried to use an STSDF object but I have not had any
>>>> success.  I created an sp object of length 164 with the station
>>>> locations:
>>>>
>>>>
>>>> sp = data.frame(long = stations$long, lat = stations$lat)
>>>>
>>>> coordinates(sp) = ~ long+lat
>>>>
>>>>
>>>> Then I created a vector of length 122 with the times the
>>>> observations were recorded and a data vector of length 19282:
>>>>
>>>>
>>>> beginDate = as.Date(2000/04/01)
>>>>
>>>> endDate = as.Date(2000/07/31)
>>>>
>>>> times = as.POSIXct(seq(beginDate,endDate,by="days"))
>>>>
>>>>
>>>> data=data.frame(temps$residual)
>>>>
>>>>
>>>> And I also made an index detailing where observations are
>>>> available, it looks like this with the first column
>>>> representing spatial index and the second representing the time
>>>> index
>>>>
>>>>
>>>> 1   1
>>>>
>>>> 2   1
>>>>
>>>> 3   1
>>>>
>>>> 4   1
>>>>
>>>>
>>>> st=STSDF(sp,time,data,index,endTime=delta(time))
>>>>
>>>>
>>>> however when I try to calculate the sample variogram I get the
>>>> following error:
>>>>
>>>>
>>>> sample.stVariogram=variogramST(residual~1,data=st,
>>>> tunit="days", tlags=1:7, progress=TRUE)
>>>>
>>>>
>>>>
>>>> Error in apply(do.call(cbind, lapply(ret, function(x) x$np)),
>>>> 1, sum,  : dim(X) must have a positive length In addition:
>>>> There were 50 or more warnings (use warnings() to see the first
>>>> 50)
>>>>
>>>>
>>>> All 50 of the errors are :
>>>>
>>>>
>>>> In is.na(data[[as.character(as.list(formula)[[2]])]]) : is.na()
>>>> applied to non-(list or vector) of type 'NULL'
>>>>
>>>>
>>>> Can anyone see what I am doing wrong or give me any pointers?
>>>> This error  is rather cryptic and I'm not quite sure what I'm
>>>> doing wrong.  Any help would be appreciated.
>>>>
>>>>
>>>> Many Thanks,
>>>>
>>>> Daniel Turenne
>>>>
>>>> University of Manitoba
>>>>
>>>>
>>>> [[alternative HTML version deleted]]
>>>>
>>>> _______________________________________________ R-sig-Geo
>>>> mailing list R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>> R-sig-Geo - R Special Interest Group on using Geographical
>>> ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
>>> stat.ethz.ch R-sig-Geo -- R Special Interest Group on using
>>> Geographical data and Mapping About R-sig-Geo
>>>
>>>
>>>
>>>>
>>>
>>> -- Edzer Pebesma Institute for Geoinformatics  (ifgi),
>>> University of M?nster Heisenbergstra?e 2, 48149 M?nster, Germany;
>>> +49 251 83 33081 Journal of Statistical Software:
>>> http://www.jstatsoft.org/ Computers & Geosciences:
>>> http://elsevier.com/locate/cageo/ Spatial Statistics Society
>>> http://www.spatialstatistics.info
>>>
>>>
>>> [[alternative HTML version deleted]]
>>>
>>>
>>>
>>> _______________________________________________ R-sig-Geo mailing
>>> list R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> -- Dr. Benedikt Gr?ler Institute of Hydrology Ruhr University
>> Bochum Universit?tsstra?e 150 44801 Bochum
>>
>> ben.graeler.org
>>
>> +49 (0234) 32 - 27619
>>
>> _______________________________________________ R-sig-Geo mailing
>> list R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> [[alternative HTML version deleted]]
>>
>>
>>
>> _______________________________________________ R-sig-Geo mailing
>> list R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> -- Dr. Benedikt Gr?ler Institute of Hydrology Ruhr University Bochum
> Universit?tsstra?e 150 44801 Bochum
>
> ben.graeler.org
>
> +49 (0234) 32 - 27619
>
> _______________________________________________ R-sig-Geo mailing
> list R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
> [[alternative HTML version deleted]]
>
> _______________________________________________ R-sig-Geo mailing
> list R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

--
Dr. Benedikt Gr?ler
Institute of Hydrology
Ruhr University Bochum
Universit?tsstra?e 150
44801 Bochum

ben.graeler.org

+49 (0234) 32 - 27619

_______________________________________________
R-sig-Geo mailing list
R-sig-Geo at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-sig-geo

	[[alternative HTML version deleted]]


From Lars-Daniel.Weber at gmx.de  Wed Jun 29 23:02:15 2016
From: Lars-Daniel.Weber at gmx.de (Lars-Daniel Weber)
Date: Wed, 29 Jun 2016 23:02:15 +0200
Subject: [R-sig-Geo] analysis of point patterns
Message-ID: <trinity-764723b3-99f3-4a55-bbb4-f541d6ac3bc4-1467234135513@3capp-gmx-bs73>

Hi there!

I've got a road network (polylines) with points on it as geodata. The points have quality measurements of the paving within a distance of 10 to 20 metres. I'd like to run a analysis on these points to detect autocorrelation and hot spots with good and bad paving quality: high-high (etc). Local Moran's I might be a good solution from my point of view. 

I thought of creating 5 classes of quality from 1 (bad) to 6 (perfect) and put it into Local Moran's I. Is this a good approach to get some analysis out of the data?

Which packages would you suggest? Or would it be better to write it on my own? I can export the point data to LAT;LON;CLASS to do easy processing in R.

Best regards,
Lars-Daniel Weber


From edzer.pebesma at uni-muenster.de  Thu Jun 30 01:19:01 2016
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Wed, 29 Jun 2016 16:19:01 -0700
Subject: [R-sig-Geo] analysis of point patterns
In-Reply-To: <trinity-764723b3-99f3-4a55-bbb4-f541d6ac3bc4-1467234135513@3capp-gmx-bs73>
References: <trinity-764723b3-99f3-4a55-bbb4-f541d6ac3bc4-1467234135513@3capp-gmx-bs73>
Message-ID: <57745765.7030304@uni-muenster.de>

Why don't you do a geostatistical analysis and interpolate them?

On 29/06/16 14:02, Lars-Daniel Weber wrote:
> Hi there!
> 
> I've got a road network (polylines) with points on it as geodata. The points have quality measurements of the paving within a distance of 10 to 20 metres. I'd like to run a analysis on these points to detect autocorrelation and hot spots with good and bad paving quality: high-high (etc). Local Moran's I might be a good solution from my point of view. 
> 
> I thought of creating 5 classes of quality from 1 (bad) to 6 (perfect) and put it into Local Moran's I. Is this a good approach to get some analysis out of the data?
> 
> Which packages would you suggest? Or would it be better to write it on my own? I can export the point data to LAT;LON;CLASS to do easy processing in R.
> 
> Best regards,
> Lars-Daniel Weber
> 
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
> 

-- 
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-sig-geo/attachments/20160629/593de224/attachment.bin>

From DanielTurenne at hotmail.com  Thu Jun 30 01:42:32 2016
From: DanielTurenne at hotmail.com (Dan Turenne)
Date: Wed, 29 Jun 2016 23:42:32 +0000
Subject: [R-sig-Geo] Spatio Temporal kriging in Gstat
In-Reply-To: <577456FD.1010702@uni-muenster.de>
References: <BLUPR12MB06254992DEDB5583493F2378B72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<5768E448.70107@uni-muenster.de>
	<BLUPR12MB062505C4192C720149D6673BB72B0@BLUPR12MB0625.namprd12.prod.outlook.com>
	<1454a622-6b1e-38fe-293d-c0d1936bf47c@rub.de>
	<SN1PR12MB0638D399DC32C01F498D9A13B7200@SN1PR12MB0638.namprd12.prod.outlook.com>
	<e27ae85a-2f0d-9020-7ca0-a52a6f866ea2@rub.de>
	<BLUPR12MB0625BC2865577CD885B4FBE0B7220@BLUPR12MB0625.namprd12.prod.outlook.com>
	<94768783-f712-6680-69d4-5c77228bb00b@rub.de>
	<BLUPR12MB06250D8233BF8FA0FD865250B7230@BLUPR12MB0625.namprd12.prod.outlook.com>,
	<577456FD.1010702@uni-muenster.de>
Message-ID: <BLUPR12MB0625A5A1F0D5EAB0508ABD19B7230@BLUPR12MB0625.namprd12.prod.outlook.com>

Thank you very much for the tip Edzer, that was exactly what I was looking for.


Daniel Turenne

________________________________
From: Edzer Pebesma <edzer.pebesma at uni-muenster.de>
Sent: June 29, 2016 6:17 PM
To: Dan Turenne
Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat



On 29/06/16 11:52, Dan Turenne wrote:
> Hello Ben,
>
> Thank you for the tip about "tunit", it worked perfectly as you described.  As for my second question, the object returned by krigeST is indeed of the form STFDF.  After some digging I found the following information regarding the data from an STFDF object on inside-r.org:
>
> data<http://inside-r.org/r-doc/utils/data>:
data {utils} | inside-R | A Community Site for R<http://inside-r.org/r-doc/utils/data>
inside-r.org
Arguments... literal character strings or names. list a character vector. package a character vector giving the package(s) to look in for data sets, or NULL.



> Object of class data.frame<http://inside-r.org/r-doc/base/data.frame>, which holds the measured values; space index cycling first, time order preservedI would interpret this as follows: if I had x prediction locations, the first set of x values from the predictions vector will be for day 1, the second set of x values will be for day 2 etc.  The order of the stations will be the same as they were in the sp object used to create the prediction grid.
>
> Is this interpretation correct or am I missing something?  Once again your help is greatly appreciated.

Probably, but you might also want to use

as(object, "data.frame")

and try to understand what you get.

>
> Thanks,
> Daniel Turenne
> ________________________________
> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler <benedikt.graeler at rub.de>
> Sent: June 29, 2016 3:04 AM
> To: r-sig-geo at r-project.org
> Subject: Re: [R-sig-Geo] Spatio Temporal kriging in Gstat
>
> Dear Daniel,
>
> On 28.06.2016 16:42, Dan Turenne wrote:
>> Thank you for the information, I was able to get around the problems
>> I was having by setting upper to Inf and lower to 0.0000001 (An
>> arbitrary small value to guarantee that the value will be greater
>> then 0).  After working some more on my code I had just a few other
>> general questions I was hoping someone could answer.
> Note, that you can specify a boundary for each parameter. The arguments
> upper and lower are recycled up to the length where they match the
> parameter vector length. "Inf" is fine as a bound, but narrowing down
> the parameter space also might improve/speed up the estimation.
>
>> 1.  Whenever I call variogramST to calculate the sample variogram I
>> get a bunch of repeating output that says $tunit [1] "days" over and
>> over again.  I have tried setting progress to FALSE but this output
>> still occurs.  Is there anyway to suppress this output?
> The argument "tunit" is only necessary if a STIDF is provided. In your
> script, you create a STSDF and a different routine is used in the
> background (assuming "tlags" refers to index differences). All arguments
> that are not recognized by "variogramST" are passed on to the space only
> function "variogram", that does not know how to handle it.
> Solution: just do not provide the argument "tunit" or provide a STIDF
> where a "tunit" is necessary.
>
>> 2.  When using krigeST to get predictions, the predictions are
>> returned in the form of a single column, however there is no index to
>> say which prediction corresponds to which location.  Is this column
>> organized with the spatial index moving faster or the temporal index
>> moving faster?
> The column is supposed to be in the same order as the target geometry.
> Which version of gstat are you using? Unless "fullCovariance=TRUE", the
> returned object should be of the form ST*DF.
>
> HTH,
>
>   Ben
>
>>
>> Thank you in advance for any advice, everyone on this list has been
>> very friendly and helpful and it is greatly appreciated.
>>
>> Thanks, Daniel Turenne ________________________________ From:
>> R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of Benedikt
>> Graeler <benedikt.graeler at rub.de> Sent: June 28, 2016 5:23 AM To:
>> r-sig-geo at r-project.org Subject: Re: [R-sig-Geo] Spatio Temporal
>> kriging in Gstat
>>
>> Dear Daniel,
>>
>> the errors occur during the optimisation where optim tries different
>> (possibly non-positive) values to find the best fit. Providing
>> sensitive limits via "lower" and "upper" to optim should resolve the
>> issue.
>>
>> HTH,
>>
>> Ben
>>
>> On 26.06.2016 16:30, Dan Turenne wrote:
>>> Hello Listers,
>>>
>>>
>>> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
> [https://cf.dropboxstatic.com/static/images/icons128/folder_dropbox.png]<https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0>
>
> Spatio temporal kriging<https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0>
> www.dropbox.com<http://www.dropbox.com>
> Shared with Dropbox
>
>
>
>>>
>>>
>>>
>>>
> Thanks to the help of Dr. Graeler I have been having some success with
> my spatio temporal kriging script but I had some more questions I was
> hoping someone could help with.  When fitting the ST variogram with
> fit.stVariogram, I have noticed that the function is extremely sensitive
> to the choice of initial parameters.  I have included a link to my
> script at the beginning of this email.  If I use
>>>
>>>
>>> prior.stvgm=vgmST(stModel="sumMetric",
>>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=0.5)
>>>
>>> fitted.stvgm=fit.StVariogram(sample.stvgm,prior.stvgm)
>>>
>>> fit.stVariogram works without producing any errors, however if I
>>> adjust any of these parameters I get some strange errors.  For
>>> example by using
>>>
>>> prior.stvgm=vgmST(stModel="sumMetric",
>>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=1)
>>>
>>> Then I get this error, despite the fact that stAni is clearly
>>> positive:
>>>
>>> Error in vgmST("sumMetric", space = vgm(par[1],
>>> as.character(model$space$model[2]),  : "stAni" must be positive.
>>>
>>> As another example if I enter
>>>
>>> prior.stvgm=vgmST(stModel="sumMetric",
>>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=0.99)
>>>
>>> It will produce the following error, despite the fact that all
>>> ranges are clearly positive:
>>>
>>> Error in vgm(par[1], as.character(model$space$model[2]), par[2],
>>> par[3],  : range should be positive
>>>
>>> The only thing that changes among these examples is the value of
>>> the stAni parameter.  When stAni=0.5 everything is fine, if stAni=1
>>> then apparently stAni isn't positive and if stAni=0.99 then
>>> apparently the range isn't positive.  When I look at the plot of
>>> the sample variogram from my data, it appears that the spatial sill
>>> =1, spatial range=0.5, temporal sill=0.85, temporal range=4, joint
>>> sill=1.1 ,joint range=1.0 however these values also produce an
>>> error saying that range should be positive.  I'm finding these
>>> errors to be very confusing and would appreciate any tips anyone
>>> might be able to give.
>>>
>>> Thank you,
>>>
>>> Daniel Turenne University of Manitoba
>>>
>>>
>>> ________________________________ From: R-sig-Geo
>>> <r-sig-geo-bounces at r-project.org> on behalf of Benedikt Graeler
>>> <benedikt.graeler at rub.de> Sent: June 23, 2016 2:54 AM To:
>>> r-sig-geo at r-project.org Subject: Re: [R-sig-Geo] Spatio Temporal
>>> kriging in Gstat
>>>
>>> Dear Dan,
>>>
>>> the error occurs because your prior.stvgm is far from the structure
>>> of your data. The following model provides better starting values:
>>>
>>> prior.stvgm=vgmST(stModel="sumMetric",
>>> space=vgm(0.1,"Exp",0.5,0.3), time=vgm(0.1,"Exp",0.5,0.3),
>>> joint=vgm(0.1,"Exp",0.5,0.3), stAni=1)
>>>
>>> To get a first "estimate", it is always useful to look at a plot of
>>> the sample variogram (as in the spatial case):
>>>
>>> plot(sample.stvgm, wireframe=T, scales=list(arrows=F))
>>>
>>> From the plot, one can read spatial and temporal ranges, the
>>> magnitude of variability and the ratios of nuggets and sills.
>>> Sometimes, one is better of by starting with a simpler model (e.g.
>>> "pure" metric), as the routines are more robust for less
>>> parameters, to get a first impression of the range of parameters.
>>> Furthermore, the plot suggests that a much smaller spatial upper
>>> boundary is sufficient, as the variogram surface is mainly flat in
>>> the spatial dimension.
>>>
>>> Spatio-temporal geostatistics remains tricky in terms of numerical
>>> routines and model interpretation, we try to give some guidance
>>> regarding gstat's functionalities in [1].
>>>
>>> HTH,
>>>
>>> Ben
>>>
>>> [1]
>>> https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf
>>>
>>>
> Spatio-Temporal Interpolation using
> gstat<https://journal.r-project.org/archive/accepted/na-pebesma-heuvelink.pdf>
>>> journal.r-project.org CONTRIBUTED RESEARCH ARTICLE 1
>>> Spatio-Temporal Interpolation using gstat by Benedikt Gr?ler, Edzer
>>> Pebesma and Gerard Heuvelink Abstract We present new spatio ...
>>>
>>>
>>>
>>>
>>>
>>> On 21.06.2016 17:16, Dan Turenne wrote:
>>>> Hello again,
>>>>
>>>>
>>>> After some tinkering with my code last night I got the sample
>>>> variogram to work however now I have encountered another issue.
>>>> When attempting to use the fit.stVariogram function to fit the
>>>> sample variogram I receive this error:
>>>>
>>>>
>>>> Error in optim(extractPar(model), fitFun, ..., method = method,
>>>> lower = lower,  : non-finite value supplied by optim
>>>>
>>>>
>>>> even if I specify the upper and lower arguments to be finite.
>>>> Could this error be cause by the initial values that I set for
>>>> the space, time and joint variograms?  If so can anyone recommend
>>>> a good paper/article about how to fit these three variograms?  I
>>>> have attached a link to my script and data below, if anyone has
>>>> any insight as to why I am getting this error it would be
>>>> appreciated.  I tried to be as thorough as I could in my code
>>>> commenting, please let me know if anything is unclear.
>>>>
>>>>
>>>> https://www.dropbox.com/sh/7ccy5wu68gxf6sf/AACmk0AFxQvetunq4YPc9DXla?dl=0
>>>>
>>>>
>>>>
>>>>
> ________________________________
>>>> From: R-sig-Geo <r-sig-geo-bounces at r-project.org> on behalf of
>>>> Edzer Pebesma <edzer.pebesma at uni-muenster.de> Sent: June 21, 2016
>>>> 1:52 AM To: r-sig-geo at r-project.org Subject: Re: [R-sig-Geo]
>>>> Spatio Temporal kriging in Gstat
>>>>
>>>> This might be due to a bug (or feature) in the software caused by
>>>> the sparseness in your data, which might be different from that
>>>> used to test the software. Please make the data available
>>>> (off-list), along with an R script, so we can try to reproduce
>>>> the error message and look into it.
>>>>
>>>> On 21/06/16 04:11, Dan Turenne wrote:
>>>>> My apologies, I accidentally sent an unfinished email, here is
>>>>> the complete version of my question
>>>>>
>>>>>
>>>>> Hello R-Sig-Geo,
>>>>>
>>>>>
>>>>> As part of my masters thesis I am attempting to use
>>>>> spatio-temporal regression kriging to make predictions with
>>>>> temperature data, and I was hoping that someone might be able
>>>>> to give some insight as to how the algorithms work in gstat.
>>>>> My data consists of daily temperature observations from April 1
>>>>> to July 31, 2000.  There are observations from 164 stations
>>>>> across these 122 days, however not all stations have
>>>>> observations on all days, making for a total of 19282 records.
>>>>>
>>>>>
>>>>> I have tried to use an STSDF object but I have not had any
>>>>> success.  I created an sp object of length 164 with the station
>>>>> locations:
>>>>>
>>>>>
>>>>> sp = data.frame(long = stations$long, lat = stations$lat)
>>>>>
>>>>> coordinates(sp) = ~ long+lat
>>>>>
>>>>>
>>>>> Then I created a vector of length 122 with the times the
>>>>> observations were recorded and a data vector of length 19282:
>>>>>
>>>>>
>>>>> beginDate = as.Date(2000/04/01)
>>>>>
>>>>> endDate = as.Date(2000/07/31)
>>>>>
>>>>> times = as.POSIXct(seq(beginDate,endDate,by="days"))
>>>>>
>>>>>
>>>>> data=data.frame(temps$residual)
>>>>>
>>>>>
>>>>> And I also made an index detailing where observations are
>>>>> available, it looks like this with the first column
>>>>> representing spatial index and the second representing the time
>>>>> index
>>>>>
>>>>>
>>>>> 1   1
>>>>>
>>>>> 2   1
>>>>>
>>>>> 3   1
>>>>>
>>>>> 4   1
>>>>>
>>>>>
>>>>> st=STSDF(sp,time,data,index,endTime=delta(time))
>>>>>
>>>>>
>>>>> however when I try to calculate the sample variogram I get the
>>>>> following error:
>>>>>
>>>>>
>>>>> sample.stVariogram=variogramST(residual~1,data=st,
>>>>> tunit="days", tlags=1:7, progress=TRUE)
>>>>>
>>>>>
>>>>>
>>>>> Error in apply(do.call(cbind, lapply(ret, function(x) x$np)),
>>>>> 1, sum,  : dim(X) must have a positive length In addition:
>>>>> There were 50 or more warnings (use warnings() to see the first
>>>>> 50)
>>>>>
>>>>>
>>>>> All 50 of the errors are :
>>>>>
>>>>>
>>>>> In is.na(data[[as.character(as.list(formula)[[2]])]]) : is.na()
>>>>> applied to non-(list or vector) of type 'NULL'
>>>>>
>>>>>
>>>>> Can anyone see what I am doing wrong or give me any pointers?
>>>>> This error  is rather cryptic and I'm not quite sure what I'm
>>>>> doing wrong.  Any help would be appreciated.
>>>>>
>>>>>
>>>>> Many Thanks,
>>>>>
>>>>> Daniel Turenne
>>>>>
>>>>> University of Manitoba
>>>>>
>>>>>
>>>>> [[alternative HTML version deleted]]
>>>>>
>>>>> _______________________________________________ R-sig-Geo
>>>>> mailing list R-sig-Geo at r-project.org
>>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>> R-sig-Geo - R Special Interest Group on using Geographical
>>>> ...<https://stat.ethz.ch/mailman/listinfo/r-sig-geo>
>>>> stat.ethz.ch R-sig-Geo -- R Special Interest Group on using
>>>> Geographical data and Mapping About R-sig-Geo
>>>>
>>>>
>>>>
>>>>>
>>>>
>>>> -- Edzer Pebesma Institute for Geoinformatics  (ifgi),
>>>> University of M?nster Heisenbergstra?e 2, 48149 M?nster, Germany;
>>>> +49 251 83 33081 Journal of Statistical Software:
>>>> http://www.jstatsoft.org/ Computers & Geosciences:
>>>> http://elsevier.com/locate/cageo/ Spatial Statistics Society
>>>> http://www.spatialstatistics.info
>>>>
>>>>
>>>> [[alternative HTML version deleted]]
>>>>
>>>>
>>>>
>>>> _______________________________________________ R-sig-Geo mailing
>>>> list R-sig-Geo at r-project.org
>>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>>
>>>
>>> -- Dr. Benedikt Gr?ler Institute of Hydrology Ruhr University
>>> Bochum Universit?tsstra?e 150 44801 Bochum
>>>
>>> ben.graeler.org
>>>
>>> +49 (0234) 32 - 27619
>>>
>>> _______________________________________________ R-sig-Geo mailing
>>> list R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>> [[alternative HTML version deleted]]
>>>
>>>
>>>
>>> _______________________________________________ R-sig-Geo mailing
>>> list R-sig-Geo at r-project.org
>>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>>
>>
>> -- Dr. Benedikt Gr?ler Institute of Hydrology Ruhr University Bochum
>> Universit?tsstra?e 150 44801 Bochum
>>
>> ben.graeler.org
>>
>> +49 (0234) 32 - 27619
>>
>> _______________________________________________ R-sig-Geo mailing
>> list R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>> [[alternative HTML version deleted]]
>>
>> _______________________________________________ R-sig-Geo mailing
>> list R-sig-Geo at r-project.org
>> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>>
>
> --
> Dr. Benedikt Gr?ler
> Institute of Hydrology
> Ruhr University Bochum
> Universit?tsstra?e 150
> 44801 Bochum
>
> ben.graeler.org
>
> +49 (0234) 32 - 27619
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>
>        [[alternative HTML version deleted]]
>
> _______________________________________________
> R-sig-Geo mailing list
> R-sig-Geo at r-project.org
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo
>

--
Edzer Pebesma
Institute for Geoinformatics  (ifgi),  University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany; +49 251 83 33081
Journal of Statistical Software:   http://www.jstatsoft.org/
Computers & Geosciences:   http://elsevier.com/locate/cageo/
Spatial Statistics Society http://www.spatialstatistics.info


	[[alternative HTML version deleted]]


From Lars-Daniel.Weber at gmx.de  Thu Jun 30 05:04:50 2016
From: Lars-Daniel.Weber at gmx.de (Lars-Daniel Weber)
Date: Thu, 30 Jun 2016 05:04:50 +0200
Subject: [R-sig-Geo] analysis of point patterns
In-Reply-To: <57745765.7030304@uni-muenster.de>
References: <trinity-764723b3-99f3-4a55-bbb4-f541d6ac3bc4-1467234135513@3capp-gmx-bs73>,
	<57745765.7030304@uni-muenster.de>
Message-ID: <trinity-3e6b11c5-f3d1-4dcd-942d-2effa5d22e2d-1467255890652@3capp-gmx-bs41>

> Gesendet: Donnerstag, 30. Juni 2016 um 01:19 Uhr
> Von: "Edzer Pebesma" <edzer.pebesma at uni-muenster.de>
> An: r-sig-geo at r-project.org
> Betreff: Re: [R-sig-Geo] analysis of point patterns
>
> Why don't you do a geostatistical analysis and interpolate them?

The distribution of values is heterogeneous. I thought, geostatistics needs homogeneous distributions. I'd better now work with interpolation, it might introduce too much errors.

Am I allowed to contact you by email? Seems like you're also German and it is easier for me to write in Germany. I can summerize our discussion/results for the mailing list.


