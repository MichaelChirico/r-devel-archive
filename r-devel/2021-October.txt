From @tp @end|ng |rom p|@kor@k|@com  Fri Oct  1 11:01:39 2021
From: @tp @end|ng |rom p|@kor@k|@com (Andrew Piskorski)
Date: Fri, 1 Oct 2021 05:01:39 -0400
Subject: [Rd] R 4.1.x make check fails, stats-Ex.R,
 step factor reduced below minFactor
Message-ID: <YVbOc7TQQuUN8tYO@piskorski.com>

I recently built R 4.1.1 (Patched) from source, as I have many older
versions over the years.  This version, on Ubuntu 18.04.4 LTS:

  R 4.1.1 (Patched), 2021-09-21, svn.rev 80946, x86_64-pc-linux-gnu

Surprisingly, "make check" fails, which I don't recall seeing before.
The error is in from stats-Ex.R, which unfortunately terminates all
further testing!  This particular error, "step factor ... reduced
below 'minFactor'" does not seem very serious, but I can't figure out
why it's happening.

I installed with "make install install-tests" as usual, which seemed
to work fine.  Running the same tests after install, I'm able to get
more coverage by using errorsAreFatal=FALSE.  However, it seems the
rest of the 'stats' tests after the bad one still do not run.

I'm confused about the intent of this particular test.  The comment
above it seems to says that it's SUPPOSED to throw this error, yet
getting the error still terminates further testing, which seems
strange.  What's supposed to happen here?

Any ideas on why this error might be occurring, and how I should debug
it?  What's the right way for me to disable this one failing test, so
the ones after it can run?

Thanks for your help!


## "make check" output:
make[1]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
make[2]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
make[3]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests/Examples'
Testing examples for package 'base'
Testing examples for package 'tools'
  comparing 'tools-Ex.Rout' to 'tools-Ex.Rout.save' ... OK
Testing examples for package 'utils'
Testing examples for package 'grDevices'
  comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.save' ... OK
Testing examples for package 'graphics'
  comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.save' ... OK
Testing examples for package 'stats'
Error: testing 'stats' failed
Execution halted
Makefile:37: recipe for target 'test-Examples-Base' failed
make[3]: *** [test-Examples-Base] Error 1
make[3]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests/Examples'
../../tests/Makefile.common:198: recipe for target 'test-Examples' failed
make[2]: *** [test-Examples] Error 2
make[2]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
../../tests/Makefile.common:184: recipe for target 'test-all-basics' failed
make[1]: *** [test-all-basics] Error 1
make[1]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
Makefile:305: recipe for target 'check-all' failed
make: *** [check-all] Error 2


## From file:  tests/Examples/stats-Ex.Rout.fail

> ## Here, requiring close convergence, you need to use more accurate numerical
> ##  differentiation; this gives Error: "step factor .. reduced below 'minFactor' .."
> options(digits = 10) # more accuracy for 'trace'
> ## IGNORE_RDIFF_BEGIN
> try(nlm1 <- update(nlmod, control = list(tol = 1e-7))) # where central diff. work here:
Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
  No starting values specified for some parameters.
Initializing 'Const', 'A', 'B' to '1.'.
Consider specifying 'start' or using a selfStart model
>    (nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE))
Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
  No starting values specified for some parameters.
Initializing 'Const', 'A', 'B' to '1.'.
Consider specifying 'start' or using a selfStart model
1017460.306    (4.15e+02): par = (1 1 1)
758164.7503    (2.34e+02): par = (13.42031396 1.961485 0.05947543745)
269506.3538    (3.23e+02): par = (51.75719816 -13.09155957 0.8428607709)
68969.21893    (1.03e+02): par = (76.0006985 -1.935226745 1.0190858)
633.3672230    (1.29e+00): par = (100.3761515 8.624648402 5.104490259)
151.4400218    (9.39e+00): par = (100.6344391 4.913490985 0.2849209569)
53.08739850    (7.24e+00): par = (100.6830407 6.899303317 0.4637755074)
1.344478640    (5.97e-01): par = (100.0368306 9.897714142 0.5169294939)
0.9908415909   (1.55e-02): par = (100.0300625 9.9144191 0.5023516843)
0.9906046057   (1.84e-05): par = (100.0288724 9.916224018 0.5025207336)
0.9906046054   (9.95e-08): par = (100.028875 9.916228366 0.50252165)
0.9906046054   (9.93e-08): par = (100.028875 9.916228366 0.50252165)
Error in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  : 
  step factor 0.000488281 reduced below 'minFactor' of 0.000976562
Calls: update -> update.default -> eval -> eval -> nls
Execution halted


## After install, start R with --vanilla and run tests like this:
## https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Testing-a-Unix_002dalike-Installation
Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
pdf("tests.pdf")
tools::testInstalledPackages(scope="base", errorsAreFatal=FALSE)

-- 
Andrew Piskorski <atp at piskorski.com>


From @eb@meyer @end|ng |rom |@u@de  Fri Oct  1 12:03:25 2021
From: @eb@meyer @end|ng |rom |@u@de (Sebastian Meyer)
Date: Fri, 1 Oct 2021 12:03:25 +0200
Subject: [Rd] R 4.1.x make check fails, stats-Ex.R,
 step factor reduced below minFactor
In-Reply-To: <YVbOc7TQQuUN8tYO@piskorski.com>
References: <YVbOc7TQQuUN8tYO@piskorski.com>
Message-ID: <339c9b7c-baad-8ef5-82db-8b3a2cba945f@fau.de>

For what it's worth, make check runs OK for me with sessionInfo()

R version 4.1.1 Patched (2021-09-30 r80997)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 18.04.6 LTS

Matrix products: default
BLAS:   /home/smeyer/R/base/release/build/lib/libRblas.so
LAPACK: /home/smeyer/R/base/release/build/lib/libRlapack.so

The output of these examples is:

>> try(nlm1 <- update(nlmod, control = list(tol = 1e-7))) # where central diff. work here:
> Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>   No starting values specified for some parameters.
> Initializing ?Const?, ?A?, ?B? to '1.'.
> Consider specifying 'start' or using a selfStart model
> Error in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  : 
>   step factor 0.000488281 reduced below 'minFactor' of 0.000976562
>>    (nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE))
> Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>   No starting values specified for some parameters.
> Initializing ?Const?, ?A?, ?B? to '1.'.
> Consider specifying 'start' or using a selfStart model
> 1017460.306    (4.15e+02): par = (1 1 1)
> 758164.7503    (2.34e+02): par = (13.42031396 1.961485 0.05947543745)
> 269506.3537    (3.23e+02): par = (51.75719817 -13.09155958 0.8428607712)
> 68969.21891    (1.03e+02): par = (76.0006985 -1.93522675 1.0190858)
> 633.3672224    (1.29e+00): par = (100.3761515 8.624648408 5.104490252)
> 151.4400170    (9.39e+00): par = (100.6344391 4.913490999 0.2849209664)
> 53.08739445    (7.24e+00): par = (100.6830407 6.899303393 0.4637755095)
> 1.344478582    (5.97e-01): par = (100.0368306 9.897714144 0.5169294926)
> 0.9908415908   (1.55e-02): par = (100.0300625 9.9144191 0.5023516843)
> 0.9906046057   (1.84e-05): par = (100.0288724 9.916224018 0.5025207336)
> 0.9906046054   (9.94e-08): par = (100.028875 9.916228366 0.50252165)
> 0.9906046054   (5.00e-08): par = (100.028875 9.916228377 0.5025216525)
> Nonlinear regression model
>   model: y ~ Const + A * exp(B * x)
>    data: parent.frame()
>       Const           A           B 
> 100.0288750   9.9162284   0.5025217 
>  residual sum-of-squares: 0.9906046

Running with example(nls) in an interactive session gives the extra output

> Number of iterations to convergence: 11 
> Achieved convergence tolerance: 4.996813e-08

(when the "show.nls.convergence" option is not set to FALSE. It is set 
to FALSE in SSasymp.Rd but not reset at the end.)

Best regards,

	Sebastian


Am 01.10.21 um 11:01 schrieb Andrew Piskorski:
> I recently built R 4.1.1 (Patched) from source, as I have many older
> versions over the years.  This version, on Ubuntu 18.04.4 LTS:
> 
>    R 4.1.1 (Patched), 2021-09-21, svn.rev 80946, x86_64-pc-linux-gnu
> 
> Surprisingly, "make check" fails, which I don't recall seeing before.
> The error is in from stats-Ex.R, which unfortunately terminates all
> further testing!  This particular error, "step factor ... reduced
> below 'minFactor'" does not seem very serious, but I can't figure out
> why it's happening.
> 
> I installed with "make install install-tests" as usual, which seemed
> to work fine.  Running the same tests after install, I'm able to get
> more coverage by using errorsAreFatal=FALSE.  However, it seems the
> rest of the 'stats' tests after the bad one still do not run.
> 
> I'm confused about the intent of this particular test.  The comment
> above it seems to says that it's SUPPOSED to throw this error, yet
> getting the error still terminates further testing, which seems
> strange.  What's supposed to happen here?
> 
> Any ideas on why this error might be occurring, and how I should debug
> it?  What's the right way for me to disable this one failing test, so
> the ones after it can run?
> 
> Thanks for your help!
> 
> 
> ## "make check" output:
> make[1]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
> make[2]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
> make[3]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests/Examples'
> Testing examples for package 'base'
> Testing examples for package 'tools'
>    comparing 'tools-Ex.Rout' to 'tools-Ex.Rout.save' ... OK
> Testing examples for package 'utils'
> Testing examples for package 'grDevices'
>    comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.save' ... OK
> Testing examples for package 'graphics'
>    comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.save' ... OK
> Testing examples for package 'stats'
> Error: testing 'stats' failed
> Execution halted
> Makefile:37: recipe for target 'test-Examples-Base' failed
> make[3]: *** [test-Examples-Base] Error 1
> make[3]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests/Examples'
> ../../tests/Makefile.common:198: recipe for target 'test-Examples' failed
> make[2]: *** [test-Examples] Error 2
> make[2]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
> ../../tests/Makefile.common:184: recipe for target 'test-all-basics' failed
> make[1]: *** [test-all-basics] Error 1
> make[1]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
> Makefile:305: recipe for target 'check-all' failed
> make: *** [check-all] Error 2
> 
> 
> ## From file:  tests/Examples/stats-Ex.Rout.fail
> 
>> ## Here, requiring close convergence, you need to use more accurate numerical
>> ##  differentiation; this gives Error: "step factor .. reduced below 'minFactor' .."
>> options(digits = 10) # more accuracy for 'trace'
>> ## IGNORE_RDIFF_BEGIN
>> try(nlm1 <- update(nlmod, control = list(tol = 1e-7))) # where central diff. work here:
> Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>    No starting values specified for some parameters.
> Initializing 'Const', 'A', 'B' to '1.'.
> Consider specifying 'start' or using a selfStart model
>>     (nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE))
> Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>    No starting values specified for some parameters.
> Initializing 'Const', 'A', 'B' to '1.'.
> Consider specifying 'start' or using a selfStart model
> 1017460.306    (4.15e+02): par = (1 1 1)
> 758164.7503    (2.34e+02): par = (13.42031396 1.961485 0.05947543745)
> 269506.3538    (3.23e+02): par = (51.75719816 -13.09155957 0.8428607709)
> 68969.21893    (1.03e+02): par = (76.0006985 -1.935226745 1.0190858)
> 633.3672230    (1.29e+00): par = (100.3761515 8.624648402 5.104490259)
> 151.4400218    (9.39e+00): par = (100.6344391 4.913490985 0.2849209569)
> 53.08739850    (7.24e+00): par = (100.6830407 6.899303317 0.4637755074)
> 1.344478640    (5.97e-01): par = (100.0368306 9.897714142 0.5169294939)
> 0.9908415909   (1.55e-02): par = (100.0300625 9.9144191 0.5023516843)
> 0.9906046057   (1.84e-05): par = (100.0288724 9.916224018 0.5025207336)
> 0.9906046054   (9.95e-08): par = (100.028875 9.916228366 0.50252165)
> 0.9906046054   (9.93e-08): par = (100.028875 9.916228366 0.50252165)
> Error in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>    step factor 0.000488281 reduced below 'minFactor' of 0.000976562
> Calls: update -> update.default -> eval -> eval -> nls
> Execution halted
> 
> 
> ## After install, start R with --vanilla and run tests like this:
> ## https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Testing-a-Unix_002dalike-Installation
> Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
> pdf("tests.pdf")
> tools::testInstalledPackages(scope="base", errorsAreFatal=FALSE)
>


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Oct  1 12:48:28 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 1 Oct 2021 12:48:28 +0200
Subject: [Rd] 
 translation domain is not inferred correctly from a package's
 print methods -- intended behavior?
In-Reply-To: <CAPRVBcx4PXAAOSs6jqw4cio0E7GxnOft1McUma6TBh_hcaBWLg@mail.gmail.com>
References: <CAPRVBcx4PXAAOSs6jqw4cio0E7GxnOft1McUma6TBh_hcaBWLg@mail.gmail.com>
Message-ID: <24918.59260.330726.837476@stat.math.ethz.ch>

>>>>> Michael Chirico 
>>>>>     on Mon, 12 Jul 2021 14:21:14 -0700 writes:

    > Here is a reprex:


    > # initialize reprex package
    > cd /tmp
    > mkdir myPkg && cd myPkg
    > echo "Package: myPkg" > DESCRIPTION
    > echo "Version: 0.0.1" >> DESCRIPTION
    > mkdir R
    > echo "print.my_class = function(x, ...) { cat(gettext(\"'%s' is
    > deprecated.\"), '\n', gettext(\"'%s' is deprecated.\",
    > domain='R-myPkg'), '\n') }" > R/foo.R
    > echo "S3method(print, my_class)" > NAMESPACE
    > # extract string for translation
    > Rscript -e "tools::update_pkg_po('.')"
    > # add dummy translation
    > msginit -i po/R-myPkg.pot -o po/R-ja.po -l ja --no-translator
    > head -n -1 po/R-ja.po > tmp && mv tmp po/R-ja.po
    > echo 'msgstr "%s successfully translated"' >> po/R-ja.po
    > # install .mo translations
    > Rscript -e "tools::update_pkg_po('.')"
    > # install package & test
    > R CMD INSTALL .
    > LANGUAGE=ja Rscript -e "library(myPkg); print(structure(1, class = 'my_class'))"
    > #  '%s' ???????
    > #  %s successfully translated

Trying to see if the current "R-devel trunk" would still suffer
from this, and prompted by Suharto Anggono's suggestion on R's
bugzilla,   https://bugs.r-project.org/show_bug.cgi?id=17998#c24


I've finally started looking at this ..
(Not having a Japanese locale installed though).

    > Note that the first gettext() call, which doesn't supply domain=,
    > returns the corresponding translation from base R (i.e., the output is
    > the same as gettext("'%s' is deprecated.", domain="R-base")).

I don't see this (not having a Japanase locale?  should I try
with a locale I have installed?)

    > The second gettext() call, where domain= is supplied, returns our
    > dummy translation, which is what I would have expected from the first
    > execution.

I can get the following which seems to say that everything is
fine and fixed now, right?

MM at lynne:myPkg$ LANGUAGE=ja R-devel -s --vanilla -e 'library(myPkg,lib.loc="~/R/library/64-linux-MM-only");structure(1,class="my_class");R.version.string'
%s successfully translated 
 %s successfully translated 
[1] "R Under development (unstable) (2021-09-30 r80997)"


MM at lynne:myPkg$ LANGUAGE=ja `R-devel RHOME`/bin/Rscript --vanilla -e 'library(myPkg,lib.loc="~/R/library/64-linux-MM-only");structure(1,class="my_class");R.version.string'
%s successfully translated 
 %s successfully translated 
[1] "R Under development (unstable) (2021-09-30 r80997)"


Note: During my experiments, I also do observe things confusing to me, when
using Rscript and R from the command line... in some cases
getting errors (in Japanese) ... but that may be just in those
cases I have left any space in the string
((in the case of 'R' which in my case suffers from quoting hell
  because I use wrapper  sh-scripts to call my versions of R ... ))


    > Here is what's in ?gettext:

    >> If domain is NULL or "", and gettext or ngettext is called from a function in the namespace of package pkg the domain is set to "R-pkg". Otherwise there is no default domain.


    > Does that mean the S3 print method is not "in the namespace of myPkg"?

no.

    > Or is there a bug here?

Yes, rather;  or there *was* one.

Thanks a lot, Michael!

Best,
Martin


From pd@|gd @end|ng |rom gm@||@com  Fri Oct  1 13:14:19 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Fri, 1 Oct 2021 13:14:19 +0200
Subject: [Rd] R 4.1.x make check fails, stats-Ex.R,
 step factor reduced below minFactor
In-Reply-To: <339c9b7c-baad-8ef5-82db-8b3a2cba945f@fau.de>
References: <YVbOc7TQQuUN8tYO@piskorski.com>
 <339c9b7c-baad-8ef5-82db-8b3a2cba945f@fau.de>
Message-ID: <6B302244-A695-46E5-876F-2B291FFC82E2@gmail.com>

On Mac, I also don't get the error, but I think this is a different issue.

If I remember correctly, the error is known to occur on some platforms, but not all, which is the reason for the ## IGNORE_RDIFF_BEGIN ... END.

However _when_ it occurs, R should just print the error and continue. If it doesn't, something is up. One possible reason is that something has been playing with options(error=), e.g. in a start-up file. 

-pd


> On 1 Oct 2021, at 12:03 , Sebastian Meyer <seb.meyer at fau.de> wrote:
> 
> For what it's worth, make check runs OK for me with sessionInfo()
> 
> R version 4.1.1 Patched (2021-09-30 r80997)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 18.04.6 LTS
> 
> Matrix products: default
> BLAS:   /home/smeyer/R/base/release/build/lib/libRblas.so
> LAPACK: /home/smeyer/R/base/release/build/lib/libRlapack.so
> 
> The output of these examples is:
> 
>>> try(nlm1 <- update(nlmod, control = list(tol = 1e-7))) # where central diff. work here:
>> Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>>  No starting values specified for some parameters.
>> Initializing ?Const?, ?A?, ?B? to '1.'.
>> Consider specifying 'start' or using a selfStart model
>> Error in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :   step factor 0.000488281 reduced below 'minFactor' of 0.000976562
>>>   (nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE))
>> Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>>  No starting values specified for some parameters.
>> Initializing ?Const?, ?A?, ?B? to '1.'.
>> Consider specifying 'start' or using a selfStart model
>> 1017460.306    (4.15e+02): par = (1 1 1)
>> 758164.7503    (2.34e+02): par = (13.42031396 1.961485 0.05947543745)
>> 269506.3537    (3.23e+02): par = (51.75719817 -13.09155958 0.8428607712)
>> 68969.21891    (1.03e+02): par = (76.0006985 -1.93522675 1.0190858)
>> 633.3672224    (1.29e+00): par = (100.3761515 8.624648408 5.104490252)
>> 151.4400170    (9.39e+00): par = (100.6344391 4.913490999 0.2849209664)
>> 53.08739445    (7.24e+00): par = (100.6830407 6.899303393 0.4637755095)
>> 1.344478582    (5.97e-01): par = (100.0368306 9.897714144 0.5169294926)
>> 0.9908415908   (1.55e-02): par = (100.0300625 9.9144191 0.5023516843)
>> 0.9906046057   (1.84e-05): par = (100.0288724 9.916224018 0.5025207336)
>> 0.9906046054   (9.94e-08): par = (100.028875 9.916228366 0.50252165)
>> 0.9906046054   (5.00e-08): par = (100.028875 9.916228377 0.5025216525)
>> Nonlinear regression model
>>  model: y ~ Const + A * exp(B * x)
>>   data: parent.frame()
>>      Const           A           B 100.0288750   9.9162284   0.5025217  residual sum-of-squares: 0.9906046
> 
> Running with example(nls) in an interactive session gives the extra output
> 
>> Number of iterations to convergence: 11 Achieved convergence tolerance: 4.996813e-08
> 
> (when the "show.nls.convergence" option is not set to FALSE. It is set to FALSE in SSasymp.Rd but not reset at the end.)
> 
> Best regards,
> 
> 	Sebastian
> 
> 
> Am 01.10.21 um 11:01 schrieb Andrew Piskorski:
>> I recently built R 4.1.1 (Patched) from source, as I have many older
>> versions over the years.  This version, on Ubuntu 18.04.4 LTS:
>>   R 4.1.1 (Patched), 2021-09-21, svn.rev 80946, x86_64-pc-linux-gnu
>> Surprisingly, "make check" fails, which I don't recall seeing before.
>> The error is in from stats-Ex.R, which unfortunately terminates all
>> further testing!  This particular error, "step factor ... reduced
>> below 'minFactor'" does not seem very serious, but I can't figure out
>> why it's happening.
>> I installed with "make install install-tests" as usual, which seemed
>> to work fine.  Running the same tests after install, I'm able to get
>> more coverage by using errorsAreFatal=FALSE.  However, it seems the
>> rest of the 'stats' tests after the bad one still do not run.
>> I'm confused about the intent of this particular test.  The comment
>> above it seems to says that it's SUPPOSED to throw this error, yet
>> getting the error still terminates further testing, which seems
>> strange.  What's supposed to happen here?
>> Any ideas on why this error might be occurring, and how I should debug
>> it?  What's the right way for me to disable this one failing test, so
>> the ones after it can run?
>> Thanks for your help!
>> ## "make check" output:
>> make[1]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
>> make[2]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
>> make[3]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests/Examples'
>> Testing examples for package 'base'
>> Testing examples for package 'tools'
>>   comparing 'tools-Ex.Rout' to 'tools-Ex.Rout.save' ... OK
>> Testing examples for package 'utils'
>> Testing examples for package 'grDevices'
>>   comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.save' ... OK
>> Testing examples for package 'graphics'
>>   comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.save' ... OK
>> Testing examples for package 'stats'
>> Error: testing 'stats' failed
>> Execution halted
>> Makefile:37: recipe for target 'test-Examples-Base' failed
>> make[3]: *** [test-Examples-Base] Error 1
>> make[3]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests/Examples'
>> ../../tests/Makefile.common:198: recipe for target 'test-Examples' failed
>> make[2]: *** [test-Examples] Error 2
>> make[2]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
>> ../../tests/Makefile.common:184: recipe for target 'test-all-basics' failed
>> make[1]: *** [test-all-basics] Error 1
>> make[1]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
>> Makefile:305: recipe for target 'check-all' failed
>> make: *** [check-all] Error 2
>> ## From file:  tests/Examples/stats-Ex.Rout.fail
>>> ## Here, requiring close convergence, you need to use more accurate numerical
>>> ##  differentiation; this gives Error: "step factor .. reduced below 'minFactor' .."
>>> options(digits = 10) # more accuracy for 'trace'
>>> ## IGNORE_RDIFF_BEGIN
>>> try(nlm1 <- update(nlmod, control = list(tol = 1e-7))) # where central diff. work here:
>> Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>>   No starting values specified for some parameters.
>> Initializing 'Const', 'A', 'B' to '1.'.
>> Consider specifying 'start' or using a selfStart model
>>>    (nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE))
>> Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>>   No starting values specified for some parameters.
>> Initializing 'Const', 'A', 'B' to '1.'.
>> Consider specifying 'start' or using a selfStart model
>> 1017460.306    (4.15e+02): par = (1 1 1)
>> 758164.7503    (2.34e+02): par = (13.42031396 1.961485 0.05947543745)
>> 269506.3538    (3.23e+02): par = (51.75719816 -13.09155957 0.8428607709)
>> 68969.21893    (1.03e+02): par = (76.0006985 -1.935226745 1.0190858)
>> 633.3672230    (1.29e+00): par = (100.3761515 8.624648402 5.104490259)
>> 151.4400218    (9.39e+00): par = (100.6344391 4.913490985 0.2849209569)
>> 53.08739850    (7.24e+00): par = (100.6830407 6.899303317 0.4637755074)
>> 1.344478640    (5.97e-01): par = (100.0368306 9.897714142 0.5169294939)
>> 0.9908415909   (1.55e-02): par = (100.0300625 9.9144191 0.5023516843)
>> 0.9906046057   (1.84e-05): par = (100.0288724 9.916224018 0.5025207336)
>> 0.9906046054   (9.95e-08): par = (100.028875 9.916228366 0.50252165)
>> 0.9906046054   (9.93e-08): par = (100.028875 9.916228366 0.50252165)
>> Error in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
>>   step factor 0.000488281 reduced below 'minFactor' of 0.000976562
>> Calls: update -> update.default -> eval -> eval -> nls
>> Execution halted
>> ## After install, start R with --vanilla and run tests like this:
>> ## https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Testing-a-Unix_002dalike-Installation
>> Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
>> pdf("tests.pdf")
>> tools::testInstalledPackages(scope="base", errorsAreFatal=FALSE)
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Oct  1 15:45:48 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 1 Oct 2021 15:45:48 +0200
Subject: [Rd] R 4.1.x make check fails, stats-Ex.R,
 step factor reduced below minFactor
In-Reply-To: <YVbOc7TQQuUN8tYO@piskorski.com>
References: <YVbOc7TQQuUN8tYO@piskorski.com>
Message-ID: <24919.4364.817651.951991@stat.math.ethz.ch>

>>>>> Andrew Piskorski 
>>>>>     on Fri, 1 Oct 2021 05:01:39 -0400 writes:

    > I recently built R 4.1.1 (Patched) from source, as I have many older
    > versions over the years.  This version, on Ubuntu 18.04.4 LTS:

    > R 4.1.1 (Patched), 2021-09-21, svn.rev 80946, x86_64-pc-linux-gnu

    > Surprisingly, "make check" fails, which I don't recall seeing before.
    > The error is in from stats-Ex.R, which unfortunately terminates all
    > further testing!  This particular error, "step factor ... reduced
    > below 'minFactor'" does not seem very serious, but I can't figure out
    > why it's happening.

    > I installed with "make install install-tests" as usual, which seemed
    > to work fine.  Running the same tests after install, I'm able to get
    > more coverage by using errorsAreFatal=FALSE.  However, it seems the
    > rest of the 'stats' tests after the bad one still do not run.

    > I'm confused about the intent of this particular test.  The comment
    > above it seems to says that it's SUPPOSED to throw this error, yet
    > getting the error still terminates further testing, which seems
    > strange.  What's supposed to happen here?

    > Any ideas on why this error might be occurring, and how I should debug
    > it?  What's the right way for me to disable this one failing test, so
    > the ones after it can run?

    > Thanks for your help!


    > ## "make check" output:
    > make[1]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
    > make[2]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
    > make[3]: Entering directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests/Examples'
    > Testing examples for package 'base'
    > Testing examples for package 'tools'
    > comparing 'tools-Ex.Rout' to 'tools-Ex.Rout.save' ... OK
    > Testing examples for package 'utils'
    > Testing examples for package 'grDevices'
    > comparing 'grDevices-Ex.Rout' to 'grDevices-Ex.Rout.save' ... OK
    > Testing examples for package 'graphics'
    > comparing 'graphics-Ex.Rout' to 'graphics-Ex.Rout.save' ... OK
    > Testing examples for package 'stats'
    > Error: testing 'stats' failed
    > Execution halted
    > Makefile:37: recipe for target 'test-Examples-Base' failed
    > make[3]: *** [test-Examples-Base] Error 1
    > make[3]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests/Examples'
    > ../../tests/Makefile.common:198: recipe for target 'test-Examples' failed
    > make[2]: *** [test-Examples] Error 2
    > make[2]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
    > ../../tests/Makefile.common:184: recipe for target 'test-all-basics' failed
    > make[1]: *** [test-all-basics] Error 1
    > make[1]: Leaving directory '/home/nobackup/co/R/R-4-1-branch/Build-x86_64/tests'
    > Makefile:305: recipe for target 'check-all' failed
    > make: *** [check-all] Error 2


    > ## From file:  tests/Examples/stats-Ex.Rout.fail

    >> ## Here, requiring close convergence, you need to use more accurate numerical
    >> ##  differentiation; this gives Error: "step factor .. reduced below 'minFactor' .."
    >> options(digits = 10) # more accuracy for 'trace'
    >> ## IGNORE_RDIFF_BEGIN
    >> try(nlm1 <- update(nlmod, control = list(tol = 1e-7))) # where central diff. work here:
    > Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
    > No starting values specified for some parameters.
    > Initializing 'Const', 'A', 'B' to '1.'.
    > Consider specifying 'start' or using a selfStart model

So this did give an error we expected (on some platforms only),
hence used try().

However, the next one "should work" (*)
and failing there, *does* fail the tests :

    >> (nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE))
    > Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
    > No starting values specified for some parameters.
    > Initializing 'Const', 'A', 'B' to '1.'.
    > Consider specifying 'start' or using a selfStart model
    > 1017460.306    (4.15e+02): par = (1 1 1)
    > 758164.7503    (2.34e+02): par = (13.42031396 1.961485 0.05947543745)
    > 269506.3538    (3.23e+02): par = (51.75719816 -13.09155957 0.8428607709)
    > 68969.21893    (1.03e+02): par = (76.0006985 -1.935226745 1.0190858)
    > 633.3672230    (1.29e+00): par = (100.3761515 8.624648402 5.104490259)
    > 151.4400218    (9.39e+00): par = (100.6344391 4.913490985 0.2849209569)
    > 53.08739850    (7.24e+00): par = (100.6830407 6.899303317 0.4637755074)
    > 1.344478640    (5.97e-01): par = (100.0368306 9.897714142 0.5169294939)
    > 0.9908415909   (1.55e-02): par = (100.0300625 9.9144191 0.5023516843)
    > 0.9906046057   (1.84e-05): par = (100.0288724 9.916224018 0.5025207336)
    > 0.9906046054   (9.95e-08): par = (100.028875 9.916228366 0.50252165)
    > 0.9906046054   (9.93e-08): par = (100.028875 9.916228366 0.50252165)
    > Error in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  : 
    > step factor 0.000488281 reduced below 'minFactor' of 0.000976562
    > Calls: update -> update.default -> eval -> eval -> nls
    > Execution halted

On our versions of Linux (and hardware in case it should matter: Intel x64), 
the above has always worked:
e.g.,

> (nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE))
Warning in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
  No starting values specified for some parameters.
Initializing ?Const?, ?A?, ?B? to '1.'.
Consider specifying 'start' or using a selfStart model
1017460.306    (4.15e+02): par = (1 1 1)
758164.7503    (2.34e+02): par = (13.42031396 1.961485 0.05947543746)
269506.3540    (3.23e+02): par = (51.75719814 -13.09155954 0.8428607699)
68969.21900    (1.03e+02): par = (76.00069849 -1.93522673 1.019085799)
633.3672239    (1.29e+00): par = (100.3761515 8.62464841 5.104490279)
151.4400266    (9.39e+00): par = (100.6344391 4.913490966 0.284920948)
53.08740235    (7.24e+00): par = (100.6830408 6.899303242 0.4637755057)
1.344478691    (5.97e-01): par = (100.0368306 9.89771414 0.5169294949)
0.9908415909   (1.55e-02): par = (100.0300625 9.9144191 0.5023516842)
0.9906046057   (1.84e-05): par = (100.0288724 9.916224018 0.5025207337)
0.9906046054   (9.94e-08): par = (100.028875 9.916228366 0.50252165)
0.9906046054   (5.06e-10): par = (100.028875 9.916228388 0.5025216549)
Nonlinear regression model
  model: y ~ Const + A * exp(B * x)
   data: parent.frame()
      Const           A           B 
100.0288750   9.9162284   0.5025217 
 residual sum-of-squares: 0.9906046

However, we had heard of "strange platforms" where it failed,
--> R's bugzilla PR#18165
and for that reason, in the R-devel version of that source file
 <Rsrc>/src/library/stats/man/nls.Rd

have the code amended to not stop on error on those platforms
only:

o2 <- options(digits = 10) # more accuracy for 'trace'
## central differencing works here typically (PR#18165: not converging on *some*):
ctr2 <- nls.control(nDcentral=TRUE, tol = 8e-8, # <- even smaller than above
   warnOnly = (grepl("^aarch64.*linux", R.version$platform) && grepl("^NixOS", osVersion)
              ))
(nlm2 <- update(nlmod, control = ctr2, trace = TRUE)); options(o2)

... now would that run w/o error on your Ubuntu-installed R ?

First guess: no

Is there anything special (system libraries, compilers, ..)
on your platform?

If not, I do wonder because indeed the

   example(nls)

R code (indeed the full help page) is identical as in R 4.1.1,
and I think we can assume for sure that these examples have run
satisfactorily on most Ubuntu 18.04 LTS, no?



    > ## After install, start R with --vanilla and run tests like this:
    > ## https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Testing-a-Unix_002dalike-Installation
    > Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
    > pdf("tests.pdf")
    > tools::testInstalledPackages(scope="base", errorsAreFatal=FALSE)

    > -- 
    > Andrew Piskorski <atp at piskorski.com>

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From @tp @end|ng |rom p|@kor@k|@com  Fri Oct  1 18:05:44 2021
From: @tp @end|ng |rom p|@kor@k|@com (Andrew Piskorski)
Date: Fri, 1 Oct 2021 12:05:44 -0400
Subject: [Rd] R 4.1.x make check fails, stats-Ex.R,
 step factor reduced below minFactor
In-Reply-To: <24919.4364.817651.951991@stat.math.ethz.ch>
References: <YVbOc7TQQuUN8tYO@piskorski.com>
 <24919.4364.817651.951991@stat.math.ethz.ch>
Message-ID: <YVcx2BvQIJZBd0oF@piskorski.com>

On Fri, Oct 01, 2021 at 03:45:48PM +0200, Martin Maechler wrote:

> Is there anything special (system libraries, compilers, ..)
> on your platform?

No.  As far as I know this is an ordinary SuperMicro x86-64 server,
nothing strange or unusual.  /proc/cpuinfo says "Intel(R) Xeon(R) CPU
E5-2670 0 @ 2.60GHz".

> o2 <- options(digits = 10) # more accuracy for 'trace'
> ## central differencing works here typically (PR#18165: not converging on *some*):
> ctr2 <- nls.control(nDcentral=TRUE, tol = 8e-8, # <- even smaller than above
>    warnOnly = (grepl("^aarch64.*linux", R.version$platform) && grepl("^NixOS", osVersion)
>               ))
> (nlm2 <- update(nlmod, control = ctr2, trace = TRUE)); options(o2)
> 
> ... now would that run w/o error on your Ubuntu-installed R ?

Interactively, the code above runs fine.  In fact, the original code
ALSO seems to run fine, no errors at all!  See output below.  I get
the error when running the tests via either "make check" or
tools::testInstalledPackages(scope="base"), but outside of that
testing framework it runs fine.

Ah, interactively, if I ALSO run the code for the immediately prior
test in stats-Ex.R, THEN the nlm2 code fails the same way as with
"make check".  That prior test does set.seed(27), which seems to
trigger the downstream failures.  Simply skipping the set.seed(27)
(interactively) makes the failure go away for me.  But if the
set.seed(27) is necessary, maybe the second test should be doing its
own set.seed() of some sort?

I don't know how/where to comment out that set.seed(27) to try running
tests without it.  Editing "src/library/stats/man/nls.Rd" and re-running
"make check" still does the set.seed(27).


Just run code from the single failing test, it works fine:
------------------------------------------------------------
## R --vanilla
R version 4.1.1 Patched (2021-09-21 r80946) -- "Kick Things"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu/x86_64 (64-bit)

> Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
> options(digits = 10)
> x <- -(1:100)/10
> y <- 100 + 10 * exp(x / 2) + rnorm(x)/10
> nlmod <- nls(y ~  Const + A * exp(B * x))
Warning message:
In nls(y ~ Const + A * exp(B * x)) :
  No starting values specified for some parameters.
Initializing 'Const', 'A', 'B' to '1.'.
Consider specifying 'start' or using a selfStart model

> nlm1 <- update(nlmod, control = list(tol = 1e-7))
Warning message:
In nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
  No starting values specified for some parameters.
Initializing 'Const', 'A', 'B' to '1.'.
Consider specifying 'start' or using a selfStart model

> nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE)
1017400.445    (4.11e+02): par = (1 1 1)
752239.9094    (1.96e+02): par = (13.41553998 1.959746504 0.01471383253)
668978.9926    (1.65e+02): par = (189.3774772 -162.3882591 1.397507535)
375910.4745    (1.20e+02): par = (167.1787529 -119.9960435 1.42386803)
93230.26788    (5.49e+01): par = (133.8879258 -56.45697809 1.498055399)
382.9221937    (2.42e+00): par = (100.6364489 6.806405333 1.84811172)
138.7915397    (9.68e+00): par = (100.6763251 6.489793899 0.7564107501)
24.47843640    (5.42e+00): par = (100.4024547 8.003646622 0.4918079622)
0.8056918383   (4.49e-03): par = (99.9629562 10.01549373 0.4913706525)
0.8056755692   (4.09e-06): par = (99.96295295 10.01549135 0.4914577719)
0.8056755692   (7.83e-09): par = (99.96295344 10.01549217 0.4914579487)
Warning message:
In nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
  No starting values specified for some parameters.
Initializing 'Const', 'A', 'B' to '1.'.
Consider specifying 'start' or using a selfStart model

> nlm1
Nonlinear regression model
  model: y ~ Const + A * exp(B * x)
   data: parent.frame()
     Const          A          B 
99.9629534 10.0154922  0.4914579 
 residual sum-of-squares: 0.8056756

Number of iterations to convergence: 10 
Achieved convergence tolerance: 1.586349e-08

> nlm2
Nonlinear regression model
  model: y ~ Const + A * exp(B * x)
   data: parent.frame()
     Const          A          B 
99.9629534 10.0154922  0.4914579 
 residual sum-of-squares: 0.8056756

Number of iterations to convergence: 10 
Achieved convergence tolerance: 7.832984e-09


Instead run BOTH these tests, now the last one fails:
------------------------------------------------------------
## R --vanilla
R version 4.1.1 Patched (2021-09-21 r80946) -- "Kick Things"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu/x86_64 (64-bit)

## The two examples below show that you can fit a model to
## artificial data with noise but not to artificial data
## without noise.
> x <- 1:10
> y <- 2*x + 3                            # perfect fit
## terminates in an error, because convergence cannot be confirmed:
> try(nls(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321)))
> Error in nls(y ~ a + b * x, start = list(a = 0.12345, b = 0.54321)) : 
  number of iterations exceeded maximum of 50
## adjusting the convergence test by adding 'scaleOffset' to its denominator RSS:
> nls(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321),
    control = list(scaleOffset = 1, printEval=TRUE))
> +   It.   1, fac=           1, eval (no.,total): ( 1,  1): new dev = 1.05935e-12
Nonlinear regression model
  model: y ~ a + b * x
   data: parent.frame()
a b 
3 2 
 residual sum-of-squares: 1.059e-12

Number of iterations to convergence: 1 
Achieved convergence tolerance: 3.639e-07
> ## Alternatively jittering the "too exact" values, slightly:
set.seed(27)
> yeps <- y + rnorm(length(y), sd = 0.01) # added noise
> nls(yeps ~ a + b*x, start = list(a = 0.12345, b = 0.54321))
Nonlinear regression model
  model: yeps ~ a + b * x
   data: parent.frame()
    a     b 
3.001 2.000 
 residual sum-of-squares: 0.001346

Number of iterations to convergence: 2 
Achieved convergence tolerance: 8.658e-09
> 
> ## the nls() internal cheap guess for starting values can be sufficient:
> x <- -(1:100)/10
> y <- 100 + 10 * exp(x / 2) + rnorm(x)/10
> nlmod <- nls(y ~  Const + A * exp(B * x))
Warning message:
In nls(y ~ Const + A * exp(B * x)) :
  No starting values specified for some parameters.
Initializing 'Const', 'A', 'B' to '1.'.
Consider specifying 'start' or using a selfStart model
> options(digits = 10) # more accuracy for 'trace'
> try(nlm1 <- update(nlmod, control = list(tol = 1e-7))) # where central diff. work here:
Warning message:
In nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
  No starting values specified for some parameters.
Initializing 'Const', 'A', 'B' to '1.'.
Consider specifying 'start' or using a selfStart model
>    (nlm2 <- update(nlmod, control = list(tol = 8e-8, nDcentral=TRUE), trace=TRUE))
1017460.306    (4.15e+02): par = (1 1 1)
758164.7503    (2.34e+02): par = (13.42031396 1.961485 0.05947543745)
269506.3538    (3.23e+02): par = (51.75719816 -13.09155957 0.8428607709)
68969.21893    (1.03e+02): par = (76.0006985 -1.935226745 1.0190858)
633.3672230    (1.29e+00): par = (100.3761515 8.624648402 5.104490259)
151.4400218    (9.39e+00): par = (100.6344391 4.913490985 0.2849209569)
53.08739850    (7.24e+00): par = (100.6830407 6.899303317 0.4637755074)
1.344478640    (5.97e-01): par = (100.0368306 9.897714142 0.5169294939)
0.9908415909   (1.55e-02): par = (100.0300625 9.9144191 0.5023516843)
0.9906046057   (1.84e-05): par = (100.0288724 9.916224018 0.5025207336)
0.9906046054   (9.95e-08): par = (100.028875 9.916228366 0.50252165)
0.9906046054   (9.93e-08): par = (100.028875 9.916228366 0.50252165)
Error in nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  : 
  step factor 0.000488281 reduced below 'minFactor' of 0.000976562
In addition: Warning message:
In nls(formula = y ~ Const + A * exp(B * x), algorithm = "default",  :
  No starting values specified for some parameters.
Initializing 'Const', 'A', 'B' to '1.'.
Consider specifying 'start' or using a selfStart model
------------------------------------------------------------

-- 
Andrew Piskorski <atp at piskorski.com>


From brod|e@g@@|@m @end|ng |rom y@hoo@com  Fri Oct  1 18:07:17 2021
From: brod|e@g@@|@m @end|ng |rom y@hoo@com (Brodie Gaslam)
Date: Fri, 1 Oct 2021 16:07:17 +0000 (UTC)
Subject: [Rd] 
 [External] Re: Workaround very slow NAN/Infinities arithmetic?
In-Reply-To: <alpine.DEB.2.22.394.2109301143220.2872@luke-Latitude-7480>
References: <3eebe90a11cc42e1a14a41b407e40690@chu-rouen.fr>
 <1810868025.554414.1633004867238@mail.yahoo.com>
 <alpine.DEB.2.22.394.2109301143220.2872@luke-Latitude-7480>
Message-ID: <19068044.1033359.1633104437752@mail.yahoo.com>

> On Thursday, September 30, 2021, 01:25:02 PM EDT, <luke-tierney at uiowa.edu> wrote:
>
>> On Thu, 30 Sep 2021, brodie gaslam via R-devel wrote:
>>
>> Andr?,
>>
>> I'm not an R core member, but happen to have looked a little bit at this
>> issue myself.? I've seen similar things on Skylake and Coffee Lake 2
>> (9700, one generation past your latest) too.? I think it would make sense
>> to have some handling of this, although I would want to show the trade-off
>> with performance impacts on CPUs that are not affected by this, and on
>> vectors that don't actually have NAs and similar.? I think the performance
>> impact is likely to be small so long as branch prediction is active, but
>> since branch prediction is involved you might need to check with different
>> ratios of NAs (not for your NA bailout branch, but for e.g. interaction
>> of what you add and the existing `na.rm=TRUE` logic).
>
> I would want to see realistic examples where this matters, not
> microbenchmarks, before thinking about complicating the code. Not all
> but most cases where sum(x) returns NaN/NA would eventually result in
> an error; getting to the error faster is not likely to be useful.

That's a very good point, and I'll admit I did not consider it
sufficiently.? There are examples such as `rowSums`/`colSums` where some
rows/columns evaluate to NA thus the result is still contains meaningful
data.? By extension, any loop that applies `sum` to list elements where
some might contain NAs, and others not.? `tapply` or any other group based
aggregation come to mind.

> My understanding is that arm64 does not support proper long doubles
> (they are the same as regular doubles).

Mine is the same.

> So code using long doubles isn't getting the hoped-for improved
> precision. Since that architecture is becoming more common we should
> probably be looking at replacing uses of long doubles with better
> algorithms that can work with regular doubles, e.g Kahan summation or
> variants for sum.

This is probably the bigger issue.? If the future is ARM/AMD, the value of
Intel x87-only optimizations becomes questionable.

More generally is the question of whether to completely replace long
double with algorithmic precision methods, at a cost of performance on
systems that do support hardware long doubles (Intel or otherwise), or
whether both code pathways are kept and selected at compile time.? Or
maybe the aggregation functions gain a low-precision flag for simple
double precision accumulation.

I'm curious to look at the performance and precision implications of e.g.
Kahan summation if no one has done that yet.? Some quick poking around
shows people using processor specific intrinsics to take advantage of
advanced multi-element instructions, but I imagine R would not want to do
that.? Assuming others have not done this already, I will have a look and
report back.

>> Since we're on the topic I want to point out that the default NA in R
>> starts off as a signaling NA:
>>
>>???? example(numToBits)?? # for `bitC`
>>???? bitC(NA_real_)
>>???? ## [1] 0 11111111111 | 0000000000000000000000000000000000000000011110100010
>>???? bitC(NA_real_ + 0)
>>???? ## [1] 0 11111111111 | 1000000000000000000000000000000000000000011110100010
>>
>> Notice the leading bit of the significant starts off as zero, which marks
>> it as a signaling NA, but becomes 1, i.e. non-signaling, after any
>> operation[2].
>>
>> This is meaningful because the mere act of loading a signaling NA into the
>> x87 FPU is sufficient to trigger the slowdowns, even if the NA is not
>> actually used in arithmetic operations.? This happens sometimes under some
>> optimization levels.? I don't now of any benefit of starting off with a
>> signaling NA, especially since the encoding is lost pretty much as soon as
>> it is used.? If folks are interested I can provide patch to turn the NA
>> quiet by default.
>
> In principle this might be a good idea, but the current bit pattern is
> unfortunately baked into a number of packages and documents on
> internals, as well as serialized objects. The work needed to sort that
> out is probably not worth the effort.

One reason why we might not need to sort this out is precisely the
instability shown above.? Anything that relies on the signaling bit set to
a particular value will behave differently with `NA_real_` vs
`NA_real_ + x`.? `R_IsNA` only checks the lower word, so it doesn't care
about the signaling bit or the 19 subsequent ones.? Anything that does
likely has unpredictable behavior **currently**.

Similarly, the documentation[1] only specifies the low word:

> On such platforms NA is represented by the NaN value with low-word 0x7a2
> (1954 in decimal).

This is consistent with the semantics of `R_IsNA`.

> It also doesn't seem to affect the performance issue here since
> setting b[1] <- NA_real_ + 0 produces the same slowdown (at least on
> my current Intel machine).

The subtlety here is that the slowdown happens by merely loading the
signaling NaN onto the X87 FPU.? Consider the following:

??? x <- t(1:1e7+1L)
??? system.time(rowSums(x))
??? ##??? user? system elapsed
??? ##?? 1.685?? 0.007?? 1.693

If we apply the following patch to make NA_REAL quiet:

Index: src/main/arithmetic.c
===================================================================
--- src/main/arithmetic.c?? ?(revision 80996)
+++ src/main/arithmetic.c?? ?(working copy)
@@ -117,7 +117,7 @@
???? /* The gcc shipping with Fedora 9 gets this wrong without
????? * the volatile declaration. Thanks to Marc Schwartz. */
???? volatile ieee_double x;
-??? x.word[hw] = 0x7ff00000;
+??? x.word[hw] = 0x7ff80000;
???? x.word[lw] = 1954;
???? return x.value;
?}

We get a ~25x speedup:

??? x <- t(1:1e7+1L)
??? system.time(rowSums(x))
??? ##??? user? system elapsed
??? ##?? 0.068?? 0.000?? 0.068

This despite no NAs anywhere in sight.? I observe the slow behavior on
both Skylake, Coffee Lake 2 and others, across different OSes, so long as
the -O2 compilation flag is used.

This likely happens because the compiler tries to prepare for the `keepNA`
branch in[1]:

?? ???? case INTSXP:
?? ???? {
?? ??? ?int *ix = INTEGER(x) + (R_xlen_t)n*j;
?? ??? ?for (cnt = 0, sum = 0., i = 0; i < n; i++, ix++)
?? ??? ???? if (*ix != NA_INTEGER) {cnt++; sum += *ix;}
?? ??? ???? else if (keepNA) {sum = NA_REAL; break;}
?? ??? ?break;
?? ???? }

by loading an NA_REAL into the FPU.? At least that was my interpretation
of the following machine code (dumped from src/main/array.o).? I barely
understand ASM, but it looks like 9bd4 loads R_NaReal, and this happens
before the test for NA at 9be8:

??????????? case LGLSXP:?? # looks like INTSXP/LGLSXP branches collapsed
??????????? {
??????????????? int *ix = LOGICAL(x) + (R_xlen_t)n * j;
??? 9bc8:?????? 4d 01 e3??????????????? add??? r11,r12
??????????????? for (R_xlen_t i = 0; i < n; i++, ra++, ix++)
??? 9bcb:?????? 48 85 db??????????????? test?? rbx,rbx
??? 9bce:?????? 0f 8e fc fe ff ff?????? jle??? 9ad0 <do_colsum+0x260>
??????????????????? if (keepNA) {
??????????????????????? if (*ix != NA_LOGICAL) *ra += *ix;
??????????????????????? else *ra = NA_REAL;
??? 9bd4:?????? dd 05 00 00 00 00?????? fld??? QWORD PTR [rip+0x0]??????? # 9bda <do_colsum+0x36a>
??????????????????????? 9bd6: R_X86_64_PC32???? R_NaReal-0x4
??? 9bda:?????? 48 8b 95 60 ff ff ff??? mov??? rdx,QWORD PTR [rbp-0xa0]
??????????????? for (R_xlen_t i = 0; i < n; i++, ra++, ix++)
??? 9be1:?????? 31 c0?????????????????? xor??? eax,eax
??? 9be3:?????? eb 2c?????????????????? jmp??? 9c11 <do_colsum+0x3a1>
??? 9be5:?????? 0f 1f 00??????????????? nop??? DWORD PTR [rax]
??????????????????????? if (*ix != NA_LOGICAL) *ra += *ix;
??? 9be8:?????? 45 39 d0??????????????? cmp??? r8d,r10d
??? 9beb:?????? 74 5b?????????????????? je???? 9c48 <do_colsum+0x3d8>
??? 9bed:?????? 44 89 85 78 ff ff ff??? mov??? DWORD PTR [rbp-0x88],r8d
??? 9bf4:?????? db 85 78 ff ff ff?????? fild?? DWORD PTR [rbp-0x88]
??? 9bfa:?????? db 2a?????????????????? fld??? TBYTE PTR [rdx]
??? 9bfc:?????? de c1?????????????????? faddp? st(1),st
??? 9bfe:?????? db 3a?????????????????? fstp?? TBYTE PTR [rdx]

... SNIP

??? 9c48:?????? db 3a?????????????????? fstp?? TBYTE PTR [rdx]
??? 9c4a:?????? db 2a?????????????????? fld??? TBYTE PTR [rdx]
??? 9c4c:?????? eb b2?????????????????? jmp??? 9c00 <do_colsum+0x390>

If no NA_LOGICAL (NA_INTEGER) are encountered, the NaN is not used.
However, it is always loaded, and for signaling NaNs this alone appears
to switch the FPU to turtle mode.

But more important than whether I can interpret ASM correctly (dubious),
simply changing the NA_REAL value to be of the quiet variety dramatically
improves performance of `rowSums` with integers.

Ironically, this doesn't happen with the REALSXP branch because that one
relies on NaN propagation in the FPU so doesn't load the NA_REAL for the
early-break case when it encounters one.? Of course if it does encounter a
NaN, quiet or not, we get a slowdown.

Compiling with -O3 also fixes this.

Bad performance of `rowSums` on integers alone is not really that big a
deal given the output is numeric, and that's why I never reported this
despite having known about it for a while.? But Andr?'s e-mail pushed me
into saying something about it.? There is a risk that code relies on the
full bit pattern of NA_REAL, but I think those are probably broken
currently since the signaling bit is even more unstable than the general
payload bits.

Best,

B.

[1]: https://github.com/r-devel/r-svn/blob/6891db49680629427e3d5927053531b8fa5d8ee3/src/main/array.c#L1939
[2]: https://cran.r-project.org/doc/manuals/r-release/R-data.html#Special-values

> Best,
>
> luke
>
>
>>
>> Best,
>>
>> B.
>>
>> [1]: https://randomascii.wordpress.com/2012/05/20/thats-not-normalthe-performance-of-odd-floats/
>> [2]: https://en.wikipedia.org/wiki/NaN#Encoding
>>
>>
>>
>>
>>
>>> On Thursday, September 30, 2021, 06:52:59 AM EDT, GILLIBERT, Andre <andre.gillibert at chu-rouen.fr> wrote:
>>>
>>> Dear R developers,
>>>
>>> By default, R uses the "long double" data type to get extra precision for intermediate computations, with a small performance tradeoff.
>>>
>>> Unfortunately, on all Intel x86 computers I have ever seen, long doubles (implemented in the x87 FPU) are extremely slow whenever a special representation (NA, NaN or infinities) is used; probably because it triggers poorly optimized microcode in the CPU firmware. A function such as sum() becomes more than hundred times slower!
>>> Test code:
>>> a=runif(1e7);system.time(for(i in 1:100)sum(a))
>>> b=a;b[1]=NA;system.time(sum(b))
>>>
>>> The slowdown factors are as follows on a few intel CPU:
>>>
>>> 1)????? Pentium Gold G5400 (Coffee Lake, 8th generation) with R 64 bits : 140 times slower with NA
>>>
>>> 2)????? Pentium G4400 (Skylake, 6th generation) with R 64 bits : 150 times slower with NA
>>>
>>> 3)????? Pentium G3220 (Haswell, 4th generation) with R 64 bits : 130 times slower with NA
>>>
>>> 4)????? Celeron J1900 (Atom Silvermont) with R 64 bits : 45 times slower with NA
>>>
>>> I do not have access to more recent Intel CPUs, but I doubt that it has improved much.
>>>
>>> Recent AMD CPUs have no significant slowdown.
>>> There is no significant slowdown on Intel CPUs (more recent than Sandy Bridge) for 64 bits floating point calculations based on SSE2. Therefore, operators using doubles, such as '+' are unaffected.
>>>
>>> I do not know whether recent ARM CPUs have slowdowns on FP64... Maybe somebody can test.
>>>
>>> Since NAs are not rare in real-life, I think that it would worth an extra check in functions based on long doubles, such as sum(). The check for special representations do not necessarily have to be done at each iteration for cumulative functions.
>>> If you are interested, I can write a bunch of patches to fix the main functions using long doubles: cumsum, cumprod, sum, prod, rowSums, colSums, matrix multiplication (matprod="internal").
>>>
>>> What do you think of that?
>>>
>>> --
>>> Sincerely
>>> Andr? GILLIBERT
>>>
>>>????? [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>>
>
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa????????????????? Phone:??????????? 319-335-3386
> Department of Statistics and??????? Fax:????????????? 319-335-3017
>???? Actuarial Science
> 241 Schaeffer Hall????????????????? email:? luke-tierney at uiowa.edu
> Iowa City, IA 52242??????????????? WWW:? http://www.stat.uiowa.edu
>
>


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Fri Oct  1 18:14:57 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Fri, 1 Oct 2021 16:14:57 +0000
Subject: [Rd] 
 [External] Re: Workaround very slow NAN/Infinities arithmetic?
In-Reply-To: <CAD4oTHE7MeOYG3MRbHbwHS76HpHj783__ku0GoNQN9iqOvvxUg@mail.gmail.com>
References: <3eebe90a11cc42e1a14a41b407e40690@chu-rouen.fr>
 <1810868025.554414.1633004867238@mail.yahoo.com>
 <alpine.DEB.2.22.394.2109301143220.2872@luke-Latitude-7480>
 <CAD4oTHE7MeOYG3MRbHbwHS76HpHj783__ku0GoNQN9iqOvvxUg@mail.gmail.com>
Message-ID: <ac7871d0bf6249fa9946e9be5e084679@chu-rouen.fr>


> Mildly related (?) to this discussion, if you happen to be in a situation
> where you know something is a C NAN, but need to check if its a proper R
> NA, the R_IsNA function is surprisingly (to me, at least) expensive to do
> in a tight loop because it calls the (again, surprisingly expensive to me)
> isnan function.  

What is your platform? CPU, OS, compiler?
How much expensive? 5-10 times slower than the improved code you wrote, or 100-200 times slower?

I analyzed the C and assembly source code of R_IsNA on a x86_64 GNU/Linux computer (Celeron J1900) with GCC 5.4 and found that it was somewhat expensive, but the main problems did not seem to come from isnan.

isnan was only responsible of a ucomisd xmm0, xmm0 instruction followed by a conditional jump on x86_64. This instruction is slower on NAN than on normal FP, but it seems to have an acceptable speed.
On x86_32, the isnan is  responsible of a fld mem64, fst mem64, fucomip and conditional jump : it is suboptimal, but things could be worse.

On x86_64, the first problem I noticed is that R_IsNA is not inlined, and the registry-based x86_64 Linux calling convention is not necessarily good for that problem, with added loads/unloads from memory to registry.
Second problem (the worst part) : the write of a 64-bits double followed by the read of a 32-bits integer in the ieee_double union confuses the compiler, that generates very poor code, with unnecessary load/stores.

The second problem can be solved by using a union with a uint64_t and a double fields, and using &0xFFFFFFFF to extract the low part of the uint64_t. This works well for x86_64, but also for x86_32, where GCC avoids useless emulation of 64-bits integers, directly reading the 32-bits integer.

-- 
Sincerely
Andr? GILLIBERT

From pd@|gd @end|ng |rom gm@||@com  Sun Oct  3 18:40:48 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Sun, 3 Oct 2021 18:40:48 +0200
Subject: [Rd] R 4.1.2 scheduled for November 1
Message-ID: <7BC92570-396C-47AD-A035-6558E1FEEAC0@gmail.com>

(Just a quick heads-up for developers.)
 
Full schedule to be made available soon.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pd@|gd @end|ng |rom gm@||@com  Sun Oct  3 18:58:08 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Sun, 3 Oct 2021 18:58:08 +0200
Subject: [Rd] R 4.1.2 scheduled for November 1
In-Reply-To: <7BC92570-396C-47AD-A035-6558E1FEEAC0@gmail.com>
References: <7BC92570-396C-47AD-A035-6558E1FEEAC0@gmail.com>
Message-ID: <EE1D7D21-430D-4FF8-BEF8-3B46DBD29B7A@gmail.com>

Schedule should appear on developer.r-project.org when it gets updated from SVN.

> On 3 Oct 2021, at 18:40 , peter dalgaard <pdalgd at gmail.com> wrote:
> 
> (Just a quick heads-up for developers.)
> 
> Full schedule to be made available soon.
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From t|m@t@y|or @end|ng |rom h|ddene|eph@nt@@co@uk  Wed Oct  6 10:25:37 2021
From: t|m@t@y|or @end|ng |rom h|ddene|eph@nt@@co@uk (Tim Taylor)
Date: Wed, 6 Oct 2021 08:25:37 +0000
Subject: [Rd] Request for stopifnot
Message-ID: <LO0P265MB260254C21367AC632873C463DDB09@LO0P265MB2602.GBRP265.PROD.OUTLOOK.COM>

Would R-core be receptive to adding an additional parameter to stopifnot so we can hide the call in the output as in stop?

i.e. The signature would become:
stopifnot2 <- function (..., exprs, exprObject, local = TRUE, .call = TRUE)

It looks like this would be a one-line change to the the underlying stop call to:
stop(simpleError(msg, call = if((p <- sys.parent(1L)) && isTRUE(.call)) sys.call(p)))

Best

Tim

From tom@@@k@||ber@ @end|ng |rom gm@||@com  Wed Oct  6 11:07:47 2021
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Wed, 6 Oct 2021 11:07:47 +0200
Subject: [Rd] 
 [External] Re: Workaround very slow NAN/Infinities arithmetic?
In-Reply-To: <19068044.1033359.1633104437752@mail.yahoo.com>
References: <3eebe90a11cc42e1a14a41b407e40690@chu-rouen.fr>
 <1810868025.554414.1633004867238@mail.yahoo.com>
 <alpine.DEB.2.22.394.2109301143220.2872@luke-Latitude-7480>
 <19068044.1033359.1633104437752@mail.yahoo.com>
Message-ID: <25ce3992-23d2-9cc4-e989-b83ef2732efc@gmail.com>


On 10/1/21 6:07 PM, Brodie Gaslam via R-devel wrote:
>> On Thursday, September 30, 2021, 01:25:02 PM EDT, <luke-tierney at uiowa.edu> wrote:
>>
>>> On Thu, 30 Sep 2021, brodie gaslam via R-devel wrote:
>>>
>>> Andr?,
>>>
>>> I'm not an R core member, but happen to have looked a little bit at this
>>> issue myself.? I've seen similar things on Skylake and Coffee Lake 2
>>> (9700, one generation past your latest) too.? I think it would make sense
>>> to have some handling of this, although I would want to show the trade-off
>>> with performance impacts on CPUs that are not affected by this, and on
>>> vectors that don't actually have NAs and similar.? I think the performance
>>> impact is likely to be small so long as branch prediction is active, but
>>> since branch prediction is involved you might need to check with different
>>> ratios of NAs (not for your NA bailout branch, but for e.g. interaction
>>> of what you add and the existing `na.rm=TRUE` logic).
>> I would want to see realistic examples where this matters, not
>> microbenchmarks, before thinking about complicating the code. Not all
>> but most cases where sum(x) returns NaN/NA would eventually result in
>> an error; getting to the error faster is not likely to be useful.
> That's a very good point, and I'll admit I did not consider it
> sufficiently.? There are examples such as `rowSums`/`colSums` where some
> rows/columns evaluate to NA thus the result is still contains meaningful
> data.? By extension, any loop that applies `sum` to list elements where
> some might contain NAs, and others not.? `tapply` or any other group based
> aggregation come to mind.
>
>> My understanding is that arm64 does not support proper long doubles
>> (they are the same as regular doubles).
> Mine is the same.

Then there are issues with that "long double" (where not equivalent to 
"double") is implemented differently on different platforms, providing 
different properties. We have ran into that on Power, where "long 
double" it is implemented using a sum of doubles ("double-double"). If 
we could rely just on "double", we would not have to worry about such 
things.

>> So code using long doubles isn't getting the hoped-for improved
>> precision. Since that architecture is becoming more common we should
>> probably be looking at replacing uses of long doubles with better
>> algorithms that can work with regular doubles, e.g Kahan summation or
>> variants for sum.
> This is probably the bigger issue.? If the future is ARM/AMD, the value of
> Intel x87-only optimizations becomes questionable.
>
> More generally is the question of whether to completely replace long
> double with algorithmic precision methods, at a cost of performance on
> systems that do support hardware long doubles (Intel or otherwise), or
> whether both code pathways are kept and selected at compile time.? Or
> maybe the aggregation functions gain a low-precision flag for simple
> double precision accumulation.
>
> I'm curious to look at the performance and precision implications of e.g.
> Kahan summation if no one has done that yet.? Some quick poking around
> shows people using processor specific intrinsics to take advantage of
> advanced multi-element instructions, but I imagine R would not want to do
> that.? Assuming others have not done this already, I will have a look and
> report back.

Processor-specific (or even compiler-specific) code is better avoided, 
but sometimes it is possible to write portable code that is tailored to 
run fast on common platforms, while still working correctly with 
acceptable performance on other.

Sometimes one can give hints to the compiler via OpenMP pragmas to 
vectorize code and/or use vectorized instructions, e.g. when it is ok to 
ignore some specific corner cases (R uses this in mayHaveNaNOrInf to 
tell the compiler it is ok to assume associativity of addition in a 
specific loop/variable, hence allowing it to vectorize better).

>>> Since we're on the topic I want to point out that the default NA in R
>>> starts off as a signaling NA:
>>>
>>>  ???? example(numToBits)?? # for `bitC`
>>>  ???? bitC(NA_real_)
>>>  ???? ## [1] 0 11111111111 | 0000000000000000000000000000000000000000011110100010
>>>  ???? bitC(NA_real_ + 0)
>>>  ???? ## [1] 0 11111111111 | 1000000000000000000000000000000000000000011110100010
>>>
>>> Notice the leading bit of the significant starts off as zero, which marks
>>> it as a signaling NA, but becomes 1, i.e. non-signaling, after any
>>> operation[2].
>>>
>>> This is meaningful because the mere act of loading a signaling NA into the
>>> x87 FPU is sufficient to trigger the slowdowns, even if the NA is not
>>> actually used in arithmetic operations.? This happens sometimes under some
>>> optimization levels.? I don't now of any benefit of starting off with a
>>> signaling NA, especially since the encoding is lost pretty much as soon as
>>> it is used.? If folks are interested I can provide patch to turn the NA
>>> quiet by default.
>> In principle this might be a good idea, but the current bit pattern is
>> unfortunately baked into a number of packages and documents on
>> internals, as well as serialized objects. The work needed to sort that
>> out is probably not worth the effort.
> One reason why we might not need to sort this out is precisely the
> instability shown above.? Anything that relies on the signaling bit set to
> a particular value will behave differently with `NA_real_` vs
> `NA_real_ + x`.? `R_IsNA` only checks the lower word, so it doesn't care
> about the signaling bit or the 19 subsequent ones.? Anything that does
> likely has unpredictable behavior **currently**.
>
> Similarly, the documentation[1] only specifies the low word:
>
>> On such platforms NA is represented by the NaN value with low-word 0x7a2
>> (1954 in decimal).
> This is consistent with the semantics of `R_IsNA`.

The signalling vs non-signalling bit may also impact propagation of the 
NaN payload on some platforms. Some non-portable code could be looking 
at all the bits. Changing the representation based on performance issues 
in a specific processor may not be the right thing to do in principle. 
There would have to be a strong benefit of the change to outweigh these 
risks.

>> It also doesn't seem to affect the performance issue here since
>> setting b[1] <- NA_real_ + 0 produces the same slowdown (at least on
>> my current Intel machine).
> The subtlety here is that the slowdown happens by merely loading the
> signaling NaN onto the X87 FPU.? Consider the following:
>
>  ??? x <- t(1:1e7+1L)
>  ??? system.time(rowSums(x))
>  ??? ##??? user? system elapsed
>  ??? ##?? 1.685?? 0.007?? 1.693
>
> If we apply the following patch to make NA_REAL quiet:
>
> Index: src/main/arithmetic.c
> ===================================================================
> --- src/main/arithmetic.c?? ?(revision 80996)
> +++ src/main/arithmetic.c?? ?(working copy)
> @@ -117,7 +117,7 @@
>  ???? /* The gcc shipping with Fedora 9 gets this wrong without
>  ????? * the volatile declaration. Thanks to Marc Schwartz. */
>  ???? volatile ieee_double x;
> -??? x.word[hw] = 0x7ff00000;
> +??? x.word[hw] = 0x7ff80000;
>  ???? x.word[lw] = 1954;
>  ???? return x.value;
>  ?}
>
> We get a ~25x speedup:
>
>  ??? x <- t(1:1e7+1L)
>  ??? system.time(rowSums(x))
>  ??? ##??? user? system elapsed
>  ??? ##?? 0.068?? 0.000?? 0.068
>
> This despite no NAs anywhere in sight.? I observe the slow behavior on
> both Skylake, Coffee Lake 2 and others, across different OSes, so long as
> the -O2 compilation flag is used.
>
> This likely happens because the compiler tries to prepare for the `keepNA`
> branch in[1]:
>
>  ?? ???? case INTSXP:
>  ?? ???? {
>  ?? ??? ?int *ix = INTEGER(x) + (R_xlen_t)n*j;
>  ?? ??? ?for (cnt = 0, sum = 0., i = 0; i < n; i++, ix++)
>  ?? ??? ???? if (*ix != NA_INTEGER) {cnt++; sum += *ix;}
>  ?? ??? ???? else if (keepNA) {sum = NA_REAL; break;}
>  ?? ??? ?break;
>  ?? ???? }
>
> by loading an NA_REAL into the FPU.? At least that was my interpretation
> of the following machine code (dumped from src/main/array.o).? I barely
> understand ASM, but it looks like 9bd4 loads R_NaReal, and this happens
> before the test for NA at 9be8:
>
>  ??????????? case LGLSXP:?? # looks like INTSXP/LGLSXP branches collapsed
>  ??????????? {
>  ??????????????? int *ix = LOGICAL(x) + (R_xlen_t)n * j;
>  ??? 9bc8:?????? 4d 01 e3??????????????? add??? r11,r12
>  ??????????????? for (R_xlen_t i = 0; i < n; i++, ra++, ix++)
>  ??? 9bcb:?????? 48 85 db??????????????? test?? rbx,rbx
>  ??? 9bce:?????? 0f 8e fc fe ff ff?????? jle??? 9ad0 <do_colsum+0x260>
>  ??????????????????? if (keepNA) {
>  ??????????????????????? if (*ix != NA_LOGICAL) *ra += *ix;
>  ??????????????????????? else *ra = NA_REAL;
>  ??? 9bd4:?????? dd 05 00 00 00 00?????? fld??? QWORD PTR [rip+0x0]??????? # 9bda <do_colsum+0x36a>
>  ??????????????????????? 9bd6: R_X86_64_PC32???? R_NaReal-0x4
>  ??? 9bda:?????? 48 8b 95 60 ff ff ff??? mov??? rdx,QWORD PTR [rbp-0xa0]
>  ??????????????? for (R_xlen_t i = 0; i < n; i++, ra++, ix++)
>  ??? 9be1:?????? 31 c0?????????????????? xor??? eax,eax
>  ??? 9be3:?????? eb 2c?????????????????? jmp??? 9c11 <do_colsum+0x3a1>
>  ??? 9be5:?????? 0f 1f 00??????????????? nop??? DWORD PTR [rax]
>  ??????????????????????? if (*ix != NA_LOGICAL) *ra += *ix;
>  ??? 9be8:?????? 45 39 d0??????????????? cmp??? r8d,r10d
>  ??? 9beb:?????? 74 5b?????????????????? je???? 9c48 <do_colsum+0x3d8>
>  ??? 9bed:?????? 44 89 85 78 ff ff ff??? mov??? DWORD PTR [rbp-0x88],r8d
>  ??? 9bf4:?????? db 85 78 ff ff ff?????? fild?? DWORD PTR [rbp-0x88]
>  ??? 9bfa:?????? db 2a?????????????????? fld??? TBYTE PTR [rdx]
>  ??? 9bfc:?????? de c1?????????????????? faddp? st(1),st
>  ??? 9bfe:?????? db 3a?????????????????? fstp?? TBYTE PTR [rdx]
>
> ... SNIP
>
>  ??? 9c48:?????? db 3a?????????????????? fstp?? TBYTE PTR [rdx]
>  ??? 9c4a:?????? db 2a?????????????????? fld??? TBYTE PTR [rdx]
>  ??? 9c4c:?????? eb b2?????????????????? jmp??? 9c00 <do_colsum+0x390>
>
> If no NA_LOGICAL (NA_INTEGER) are encountered, the NaN is not used.
> However, it is always loaded, and for signaling NaNs this alone appears
> to switch the FPU to turtle mode.
>
> But more important than whether I can interpret ASM correctly (dubious),
> simply changing the NA_REAL value to be of the quiet variety dramatically
> improves performance of `rowSums` with integers.
>
> Ironically, this doesn't happen with the REALSXP branch because that one
> relies on NaN propagation in the FPU so doesn't load the NA_REAL for the
> early-break case when it encounters one.? Of course if it does encounter a
> NaN, quiet or not, we get a slowdown.
>
> Compiling with -O3 also fixes this.

In general it is very difficult to tell performance from the assembly, 
because a lot of optimizations happen at the hardware level, but except 
from compiler experts who understand concrete processors in details, one 
can make guesses and based on them come up with performance 
improvements. If your guess is right about the reason for the slowdown, 
there should be a way to suggest a small change to the C code, along the 
lines of what -O3 does, so that the compiler (recent version of GCC) 
would produce code which doesn't load the value eagerly and runs faster.

> Bad performance of `rowSums` on integers alone is not really that big a
> deal given the output is numeric, and that's why I never reported this
> despite having known about it for a while.? But Andr?'s e-mail pushed me
> into saying something about it.? There is a risk that code relies on the
> full bit pattern of NA_REAL, but I think those are probably broken
> currently since the signaling bit is even more unstable than the general
> payload bits.

Yes, the signalling bit is often lost sooner than the payload.

Best
Tomas

>
> Best,
>
> B.
>
> [1]: https://github.com/r-devel/r-svn/blob/6891db49680629427e3d5927053531b8fa5d8ee3/src/main/array.c#L1939
> [2]: https://cran.r-project.org/doc/manuals/r-release/R-data.html#Special-values
>
>> Best,
>>
>> luke
>>
>>
>>> Best,
>>>
>>> B.
>>>
>>> [1]: https://randomascii.wordpress.com/2012/05/20/thats-not-normalthe-performance-of-odd-floats/
>>> [2]: https://en.wikipedia.org/wiki/NaN#Encoding
>>>
>>>
>>>
>>>
>>>
>>>> On Thursday, September 30, 2021, 06:52:59 AM EDT, GILLIBERT, Andre <andre.gillibert at chu-rouen.fr> wrote:
>>>>
>>>> Dear R developers,
>>>>
>>>> By default, R uses the "long double" data type to get extra precision for intermediate computations, with a small performance tradeoff.
>>>>
>>>> Unfortunately, on all Intel x86 computers I have ever seen, long doubles (implemented in the x87 FPU) are extremely slow whenever a special representation (NA, NaN or infinities) is used; probably because it triggers poorly optimized microcode in the CPU firmware. A function such as sum() becomes more than hundred times slower!
>>>> Test code:
>>>> a=runif(1e7);system.time(for(i in 1:100)sum(a))
>>>> b=a;b[1]=NA;system.time(sum(b))
>>>>
>>>> The slowdown factors are as follows on a few intel CPU:
>>>>
>>>> 1)????? Pentium Gold G5400 (Coffee Lake, 8th generation) with R 64 bits : 140 times slower with NA
>>>>
>>>> 2)????? Pentium G4400 (Skylake, 6th generation) with R 64 bits : 150 times slower with NA
>>>>
>>>> 3)????? Pentium G3220 (Haswell, 4th generation) with R 64 bits : 130 times slower with NA
>>>>
>>>> 4)????? Celeron J1900 (Atom Silvermont) with R 64 bits : 45 times slower with NA
>>>>
>>>> I do not have access to more recent Intel CPUs, but I doubt that it has improved much.
>>>>
>>>> Recent AMD CPUs have no significant slowdown.
>>>> There is no significant slowdown on Intel CPUs (more recent than Sandy Bridge) for 64 bits floating point calculations based on SSE2. Therefore, operators using doubles, such as '+' are unaffected.
>>>>
>>>> I do not know whether recent ARM CPUs have slowdowns on FP64... Maybe somebody can test.
>>>>
>>>> Since NAs are not rare in real-life, I think that it would worth an extra check in functions based on long doubles, such as sum(). The check for special representations do not necessarily have to be done at each iteration for cumulative functions.
>>>> If you are interested, I can write a bunch of patches to fix the main functions using long doubles: cumsum, cumprod, sum, prod, rowSums, colSums, matrix multiplication (matprod="internal").
>>>>
>>>> What do you think of that?
>>>>
>>>> --
>>>> Sincerely
>>>> Andr? GILLIBERT
>>>>
>>>>  ????? [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> --
>> Luke Tierney
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa????????????????? Phone:??????????? 319-335-3386
>> Department of Statistics and??????? Fax:????????????? 319-335-3017
>>  ???? Actuarial Science
>> 241 Schaeffer Hall????????????????? email:? luke-tierney at uiowa.edu
>> Iowa City, IA 52242??????????????? WWW:? http://www.stat.uiowa.edu
>>
>>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From r-deve| @end|ng |rom @ker@t|ng@de  Thu Oct  7 09:26:08 2021
From: r-deve| @end|ng |rom @ker@t|ng@de (Andreas Kersting)
Date: Thu, 07 Oct 2021 09:26:08 +0200 (CEST)
Subject: [Rd] GC: parallelizing the CHARSXP cache maintenance
Message-ID: <E1mYNn6-0000cv-1M@rmmprod05.runbox>

Hi all,

As part of RunGenCollect() (in src/main/memory.c), some maintenance on the CHARSXP cache is done, namely unmarked nodes/CHARSXPs are removed from the hash chains. This requires always touching all CHARSXP in the cache, irrespective of the number of generations which were just garbage collected. In a session with a big CHARSXP cache, this will significantly slow down gc also when just collecting the youngest generation.

However, this part of RunGenCollect() seems to be one of the few which can easily be parallelized without the need for thread synchronization. And it seems to be the one most profiting from parallelization.

Attached patch (?parallel_CHARSXP_cache.diff) implements parallelization over the elements of R_StringHash and gives the following performance improvements on my system when using 4 threads compared to R devel (revision 81008):

Elapsed time for 200 non-full gc in a session after

x <- as.character(runif(1e6))[]
gc(full = TRUE)

8sec -> 2.5sec.

AND

Elapsed time for five non-full gc in a session after

x <- as.character(runif(5e7))[]
gc(full = TRUE)

21sec -> 6sec.

In the patch, I dropped the two lines 

FORWARD_NODE(s);
FORWARD_NODE(CXHEAD(s));

because they are currently both no-ops (and would require synchronization if they were not). They are no-ops because we have

?# define CXHEAD(x) (x)  // in Defn.h

and hence FORWARD_NODE(s)/FORWARD_NODE(CXHEAD(s)) is only called when s is already marked, in which case FORWARD_NODE() does nothing.

I used OpenMP despite the known issues of some of its implementations with hanging after a fork, mostly because it was the easiest thing to do for a PoC. I worked around this similar to e.g. data.table by using only one thread in forked children.

It might be worth considering making the parallelization conditional on the size of the CHARSXP cache and use only the main thread if the cache is (still) small.

In the second attached patch (parallel_CHARSXP_cache_no_forward.diff) I additionally no longer call FORWARD_NODE(R_StringHash) because this will make the following call to PROCESS_NODES() iterate through all elements of R_StringHash again which is unnecessary since all elements are either R_NilValue or an already marked CHARSXP. I rather directly mark & snap R_StringHash. In contrast to the parallelization, this only affects full gcs since R_StringHash will quickly belong to the oldest generation.

Attached gc_test.R is the script I used to get the previously mentioned and more gc timings.

To me this looks like a significant performance improvement, especially given the little changeset. What do you think?

Best regards,
Andreas
-------------- next part --------------
A non-text attachment was scrubbed...
Name: parallel_CHARSXP_cache.diff
Type: text/x-patch
Size: 2424 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20211007/34bed55c/attachment.bin>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: parallel_CHARSXP_cache_no_forward.diff
Type: text/x-patch
Size: 3185 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20211007/34bed55c/attachment-0001.bin>

From r-deve| @end|ng |rom @ker@t|ng@de  Thu Oct  7 09:33:42 2021
From: r-deve| @end|ng |rom @ker@t|ng@de (Andreas Kersting)
Date: Thu, 07 Oct 2021 09:33:42 +0200 (CEST)
Subject: [Rd] GC: improving the marking performance for STRSXPs
Message-ID: <E1mYNuQ-0001Al-8B@rmmprod05.runbox>

Hi all,

in GC (in src/main/memory.c), FORWARD_CHILDREN() (called by PROCESS_NODES()) treats STRSXPs just like VECSXPs, i.e. it calls FORWARD_NODE() for all its children. I claim that this is unnecessarily inefficient since the children of a STRSXP can legitimately only be (atomic) CHARSXPs and could hence be marked directly in the call of FORWARD_CHILDREN() on the STRSXP.

Attached patch (?atomic_CHARSXP.diff) implements this and gives the following performance improvements on my system compared to R devel (revision 81008):

Elapsed time for two full gc in a session after

x <- as.character(runif(5e7))[]

19sec -> 15sec.

This is the best-case scenario for the patch: very many unique/unmarked CHARSXP in the STRSXP. For already marked CHARSXP there is no performance gain since FORWARD_NODE() is a no-op for them.

The relative performance gain is even bigger if iterating through the STRSXP produces many cache misses, as e.g. after

x <- as.character(runif(5e7))[]
x <- sample(x, length(x))

Elapsed time for two full gc here: 83sec -> 52sec. This is because we have less cache misses per CHARSXP.

This patch additionally also assumes that the ATTRIBs of a CHARSXP are not to be traced because they are just used for maintaining the CHARSXP hash chains.

The second attached patch (?atomic_CHARSXP_safe_unlikely.diff) checks both assumptions and calls gc_error() if they are violated and is still noticeably faster than R devel: 19sec -> 17sec and 83sec -> 54sec, respectively.

Attached gc_test.R is the script I used to get the previously mentioned and more gc timings.

Do you think that this is a reasonable change? It does make the code more complex and I am not sure if there might be situations in which the assumptions are violated, even though ?SET_STRING_ELT() and ?installAttrib() do enforce them.

Best regards,
Andreas
-------------- next part --------------
A non-text attachment was scrubbed...
Name: atomic_CHARSXP.diff
Type: text/x-patch
Size: 1948 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20211007/f018a189/attachment.bin>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: atomic_CHARSXP_safe_unlikely.diff
Type: text/x-patch
Size: 2427 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20211007/f018a189/attachment-0001.bin>

From @uh@rto_@nggono @end|ng |rom y@hoo@com  Sat Oct  9 04:40:10 2021
From: @uh@rto_@nggono @end|ng |rom y@hoo@com (Suharto Anggono Suharto Anggono)
Date: Sat, 9 Oct 2021 02:40:10 +0000 (UTC)
Subject: [Rd] formatC(<zero-length>) doesn't keep attributes
References: <1932751023.1279015.1633747210908.ref@mail.yahoo.com>
Message-ID: <1932751023.1279015.1633747210908@mail.yahoo.com>

By r80949, 'formatC' code in R devel has
? ? if (!(n <- length(x))) return(character())

If 'x' has length zero, the return value of 'formatC' doesn't have attributes. It doesn't follow the documented "Value": "A character object of the same size and attributes as x".

Based on my observation, the early return could be removed.
    n <- length(x)


From georg|@bo@hn@kov @end|ng |rom m@nche@ter@@c@uk  Sat Oct  9 10:33:16 2021
From: georg|@bo@hn@kov @end|ng |rom m@nche@ter@@c@uk (Georgi Boshnakov)
Date: Sat, 9 Oct 2021 08:33:16 +0000
Subject: [Rd] =?utf-8?q?Bug_18208_-_tools=3A=3A=3AlatexToUtf8=28tools=3A?=
 =?utf-8?b?OnBhcnNlTGF0ZXgoIlxcJ2kiKSkgc2hvdWxkIGdlbmVyYXRlIMOt?=
In-Reply-To: <CAOsPdQKA=JTEYBrZv9m_AmJ5o+MrrvJ=13j8hETU+M_FMyYhxw@mail.gmail.com>
References: <CAOsPdQKA=JTEYBrZv9m_AmJ5o+MrrvJ=13j8hETU+M_FMyYhxw@mail.gmail.com>
Message-ID: <LOYP265MB2015180ED8689A4175D97255AEB39@LOYP265MB2015.GBRP265.PROD.OUTLOOK.COM>

tools:::latexToUtf8() doesn't convert the LaTeX accented character "\\'i<file://'i>" to  ? (see Bug #18208 https://bugs.r-project.org/show_bug.cgi?id=18208,  reported by Manuel L?pez-Ib??ez for details and justification). For example:

> tools:::latexToUtf8(tools::parseLatex("\\'i<file://'i>"))
\'i

It does the conversion if i is replaced by '\i':

> tools:::latexToUtf8(tools::parseLatex("\\'\\i<file://'//i>"))
?

Both are acceptable in LaTeX.

I think that this can be fixed by adding a line to the definition of makeLatexTable() in R/src/library/tools/R/parseLatex.R, just before it returns:

makeLatexTable <- function(utf8table)
{
    ...

    table[["\\textemdash<file://textemdash>"]] <- "\u2014"
    latexArgCount[["\\textemdash<file://textemdash>"]] <<- 0

    table$"\\'"$"i"  <- table$"\\'"$"\\i<file://i>"    # Georgi

    table
}

I can submit a patch if I get access  to bugzilla.

After building R-devel svn 81022 from 2021-10-08 with the above change, we get what we want:

> tools:::latexToUtf8(tools::parseLatex("\\'i<file://'i>"))
?
> tools:::latexToUtf8(tools::parseLatex("\\'\\i<file://'//i>"))
?
Georgi Boshnakov





	[[alternative HTML version deleted]]


From georg|@bo@hn@kov @end|ng |rom m@nche@ter@@c@uk  Sat Oct  9 12:43:39 2021
From: georg|@bo@hn@kov @end|ng |rom m@nche@ter@@c@uk (Georgi Boshnakov)
Date: Sat, 9 Oct 2021 10:43:39 +0000
Subject: [Rd] 
 =?utf-8?q?Bug_18208_-_tools=3A=3A=3AlatexToUtf8=28tools=3A?=
 =?utf-8?b?OnBhcnNlTGF0ZXgoIlxcJ2kiKSkgc2hvdWxkIGdlbmVyYXRlIMOt?=
In-Reply-To: <CAOsPdQKA=JTEYBrZv9m_AmJ5o+MrrvJ=13j8hETU+M_FMyYhxw@mail.gmail.com>
References: <CAOsPdQKA=JTEYBrZv9m_AmJ5o+MrrvJ=13j8hETU+M_FMyYhxw@mail.gmail.com>
Message-ID: <LOYP265MB2015D0C2F691C9197A530688AEB39@LOYP265MB2015.GBRP265.PROD.OUTLOOK.COM>

Resending this as the original message went out as html and the special characters got garbled.

tools:::latexToUtf8() doesn't convert the LaTeX accented character "\\'i" to? ? (see Bug #18208 https://bugs.r-project.org/show_bug.cgi?id=18208,? reported by Manuel L?pez-Ib??ez for details and justification). For example:

> tools:::latexToUtf8(tools::parseLatex("\\'i"))
\'i 

It does the conversion if i is replaced by '\i':

> tools:::latexToUtf8(tools::parseLatex("file:\\'\\i"))
? 


I think that this can be fixed by adding a line to the definition of makeLatexTable() in R/src/library/tools/R/parseLatex.R, just before it returns: 

makeLatexTable <- function(utf8table)
{
??? ...

? ? table[["file://textemdash"]] <- "\u2014"
? ? latexArgCount[["file://textemdash"]] <<- 0

? ? table$"\\'"$"i" ?<- table$"\\'"$"\\i" ?? # Georgi

? ? table
}

I can submit a patch if I get access? to bugzilla.

After building R-devel svn 81022 from 2021-10-08 with the above change, we get what we want:

> tools:::latexToUtf8(tools::parseLatex("\\'i"))
? 
> tools:::latexToUtf8(tools::parseLatex("\\'\\i"))
? 

Georgi Boshnakov

From Thom@@@SOEIRO @end|ng |rom @p-hm@|r  Sun Oct 10 13:56:00 2021
From: Thom@@@SOEIRO @end|ng |rom @p-hm@|r (SOEIRO Thomas)
Date: Sun, 10 Oct 2021 11:56:00 +0000
Subject: [Rd] Potential bugs in table dnn
Message-ID: <1633866959731.85310@ap-hm.fr>

Dear list,

table does not set dnn for dataframes of length 1:

table(warpbreaks[2:3]) # has dnn
#     tension
# wool L M H
#    A 9 9 9
#    B 9 9 9

table(warpbreaks[2]) # has no dnn
# 
#  A  B 
# 27 27 

This is because of if (length(dnn) != length(args)) (line 53 in https://github.com/wch/r-source/blob/trunk/src/library/base/R/table.R). When commenting this line or modifying it to if (length(dnn) != length(args) || dnn == ""), dnn are set as expected:

table2(warpbreaks[2:3]) # has dnn
#     tension
# wool L M H
#    A 9 9 9
#    B 9 9 9

table2(warpbreaks[2]) # has dnn
# wool
#  A  B 
# 27 27 

However, I do not get the logic for the initial if (length(dnn) != length(args)), so the change may break something else...

In addition, table documentation says "If the argument dnn is not supplied, the internal function list.names is called to compute the ?dimname names?. If the arguments in ... are named, those names are used." Some cases seem inconsistent or may return a warning:

table(warpbreaks[2], dnn = letters) # no warning/not as documented
# wool
#  A  B 
# 27 27 

table(warpbreaks[2], dnn = letters[1]) # as documented
# a
#  A  B 
# 27 27 

table(zzz = warpbreaks[2], dnn = letters[1]) # as documented
# a
#  A  B 
# 27 27 

table(zzz = warpbreaks$wool, dnn = letters[1]) # as documented
# a
#  A  B 
# 27 27 

table(warpbreaks$wool, dnn = letters) # as expected
# Error in names(dn) <- dnn : 
#   attribut 'names' [26] doit ?tre de m?me longueur que le vecteur [1]

Best regards,

Thomas

From tr@ver@c @end|ng |rom gm@||@com  Mon Oct 11 07:05:49 2021
From: tr@ver@c @end|ng |rom gm@||@com (Travers Ching)
Date: Sun, 10 Oct 2021 22:05:49 -0700
Subject: [Rd] Crash/bug when calling match on latin1 strings
Message-ID: <CAPLMX9FKNS2Hf9BNcArWL_GrEkggxOd04-b+rfHtEAceCpH9dA@mail.gmail.com>

Here's a brief example:

# A bunch of words in UTF8; replace *'s
words <- readLines("h****://pastebin.c**/raw/MFCQfhpY", encoding = "UTF-8")
words2 <- iconv(words, "utf-8", "latin1")
gctorture(TRUE)
y <- match(words2, words2)
<The program crashes / segfaults ~90% of the time>

I searched bugzilla but didn't see anything. Apologies if this is already
reported.

The bug appears in both R-devel and the release, but doesn't seem to affect
R 4.0.5.

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Oct 11 08:41:51 2021
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 11 Oct 2021 07:41:51 +0100
Subject: [Rd] Crash/bug when calling match on latin1 strings
In-Reply-To: <CAPLMX9FKNS2Hf9BNcArWL_GrEkggxOd04-b+rfHtEAceCpH9dA@mail.gmail.com>
References: <CAPLMX9FKNS2Hf9BNcArWL_GrEkggxOd04-b+rfHtEAceCpH9dA@mail.gmail.com>
Message-ID: <7bdc35a6-b3c1-88b7-8e82-61d4b5f4ae7a@sapo.pt>

Hello,

R 4.1.1 on Ubuntu 20.04.

I can reproduce this error but not ~90% of the time, only the 1st time I 
run the script.
If I run other (terminal) commands before rerunning the R script it 
sometimes segfaults again but once again very far from 90% of the time.


rui at rui:~/tmp$ R -q -f rhelp.R
 > sessionInfo()
R version 4.1.1 (2021-08-10)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 20.04.3 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0

locale:
  [1] LC_CTYPE=pt_PT.UTF-8       LC_NUMERIC=C
  [3] LC_TIME=pt_PT.UTF-8        LC_COLLATE=pt_PT.UTF-8
  [5] LC_MONETARY=pt_PT.UTF-8    LC_MESSAGES=pt_PT.UTF-8
  [7] LC_PAPER=pt_PT.UTF-8       LC_NAME=C
  [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=pt_PT.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_4.1.1
 >
 > # A bunch of words in UTF8; replace *'s
 > words <- readLines("h****://pastebin.c**/raw/MFCQfhpY", encoding = 
"UTF-8")
 > words2 <- iconv(words, "utf-8", "latin1")
 > gctorture(TRUE)
 > y <- match(words2, words2)

  *** caught segfault ***
address 0x10, cause 'memory not mapped'
*** recursive gc invocation
*** recursive gc invocation
*** recursive gc invocation
*** recursive gc invocation
*** recursive gc invocation
*** recursive gc invocation
*** recursive gc invocation
*** recursive gc invocation
*** recursive gc invocation
*** recursive gc invocation

Traceback:
  1: match(words2, words2)
An irrecoverable exception occurred. R is aborting now ...
Falta de segmenta??o (n?cleo despejado)



This last line is Portuguese for

Segmentation fault (core dumped)

Hope this helps,

Rui Barradas


?s 06:05 de 11/10/21, Travers Ching escreveu:
> Here's a brief example:
> 
> # A bunch of words in UTF8; replace *'s
> words <- readLines("h****://pastebin.c**/raw/MFCQfhpY", encoding = "UTF-8")
> words2 <- iconv(words, "utf-8", "latin1")
> gctorture(TRUE)
> y <- match(words2, words2)
> <The program crashes / segfaults ~90% of the time>
> 
> I searched bugzilla but didn't see anything. Apologies if this is already
> reported.
> 
> The bug appears in both R-devel and the release, but doesn't seem to affect
> R 4.0.5.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon Oct 11 11:31:53 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 11 Oct 2021 11:31:53 +0200
Subject: [Rd] Crash/bug when calling match on latin1 strings
In-Reply-To: <7bdc35a6-b3c1-88b7-8e82-61d4b5f4ae7a@sapo.pt>
References: <CAPLMX9FKNS2Hf9BNcArWL_GrEkggxOd04-b+rfHtEAceCpH9dA@mail.gmail.com>
 <7bdc35a6-b3c1-88b7-8e82-61d4b5f4ae7a@sapo.pt>
Message-ID: <24932.1161.878644.458922@stat.math.ethz.ch>

>>>>> Rui Barradas 
>>>>>     on Mon, 11 Oct 2021 07:41:51 +0100 writes:

    > Hello,

    > R 4.1.1 on Ubuntu 20.04.

    > I can reproduce this error but not ~90% of the time, only the 1st time I 
    > run the script.
    > If I run other (terminal) commands before rerunning the R script it 
    > sometimes segfaults again but once again very far from 90% of the time.


    > rui at rui:~/tmp$ R -q -f rhelp.R
    >> sessionInfo()
    > R version 4.1.1 (2021-08-10)
    > Platform: x86_64-pc-linux-gnu (64-bit)
    > Running under: Ubuntu 20.04.3 LTS

    > Matrix products: default
    > BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
    > LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0

    > locale:
    > [1] LC_CTYPE=pt_PT.UTF-8       LC_NUMERIC=C
    > [3] LC_TIME=pt_PT.UTF-8        LC_COLLATE=pt_PT.UTF-8
    > [5] LC_MONETARY=pt_PT.UTF-8    LC_MESSAGES=pt_PT.UTF-8
    > [7] LC_PAPER=pt_PT.UTF-8       LC_NAME=C
    > [9] LC_ADDRESS=C               LC_TELEPHONE=C
    > [11] LC_MEASUREMENT=pt_PT.UTF-8 LC_IDENTIFICATION=C

    > attached base packages:
    > [1] stats     graphics  grDevices utils     datasets  methods   base

    > loaded via a namespace (and not attached):
    > [1] compiler_4.1.1
    >> 
    >> # A bunch of words in UTF8; replace *'s
    >> words <- readLines("h****://pastebin.c**/raw/MFCQfhpY", encoding = 
    > "UTF-8")
    >> words2 <- iconv(words, "utf-8", "latin1")
    >> gctorture(TRUE)
    >> y <- match(words2, words2)

    > *** caught segfault ***
    > address 0x10, cause 'memory not mapped'
    > *** recursive gc invocation
    > *** recursive gc invocation
    > *** recursive gc invocation
    > *** recursive gc invocation
    > *** recursive gc invocation
    > *** recursive gc invocation
    > *** recursive gc invocation
    > *** recursive gc invocation
    > *** recursive gc invocation
    > *** recursive gc invocation

    > Traceback:
    > 1: match(words2, words2)
    > An irrecoverable exception occurred. R is aborting now ...
    > Falta de segmenta??o (n?cleo despejado)



    > This last line is Portuguese for

    > Segmentation fault (core dumped)

    > Hope this helps,

Yes, it does, thank you!

I can confirm the problem:  Only in R 4.1.0 and newer, and
including current "R-patched" and "R-devel" versions.

I've now turned this into a formal R bug report on R's bugzilla,
and (slightly) extended your (Travers') example into self
contained (no internet access) R script.

Bugzilla PR#18211 :    " match(<latin1>) memory corruption "

  https://bugs.r-project.org/show_bug.cgi?id=18211

  with attachment 2929
  --> https://bugs.r-project.org/attachment.cgi?id=2929&action=edit

==> please if possible follow up on bugzilla

Thanks again to you both!
Martin Maechler


    > Rui Barradas

    > ?s 06:05 de 11/10/21, Travers Ching escreveu:
    >> Here's a brief example:
    >> 
    >> # A bunch of words in UTF8; replace *'s
    >> words <- readLines("h****://pastebin.c**/raw/MFCQfhpY", encoding = "UTF-8")
    >> words2 <- iconv(words, "utf-8", "latin1")
    >> gctorture(TRUE)
    >> y <- match(words2, words2)
    >> <The program crashes / segfaults ~90% of the time>
    >> 
    >> I searched bugzilla but didn't see anything. Apologies if this is already
    >> reported.
    >> 
    >> The bug appears in both R-devel and the release, but doesn't seem to affect
    >> R 4.0.5.


From z@|er@b@rutcuog|u @end|ng |rom gm@||@com  Tue Oct 12 08:04:40 2021
From: z@|er@b@rutcuog|u @end|ng |rom gm@||@com (Zafer Barutcuoglu)
Date: Tue, 12 Oct 2021 02:04:40 -0400
Subject: [Rd] int overflow writing long vectors to socketConnection
Message-ID: <0E317F6A-E81C-47E5-9388-F14A8F392B87@gmail.com>

Hi,

Writing >=2GB to a socketConnection (e.g. via writeBin) does not work correctly, because of this int typecast in modules/internet/sockconn.c:
> static size_t sock_write(const void *ptr, size_t size, size_t nitems,
> 			 Rconnection con)
> {
>     Rsockconn this = (Rsockconn)con->private;
>     ssize_t n = R_SockWrite(this->fd, ptr, (int)(size * nitems),
> 			    this->timeout)/((ssize_t)size);
>     return n > 0 ? n : 0;
> }
which seems uncalled for, given:
> ssize_t R_SockWrite(int sockp, const void *buf, size_t len, int timeout)


Is there a rationale for it, or should it be fixed?

Best,
--
Zafer


	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Oct 12 11:34:59 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 12 Oct 2021 11:34:59 +0200
Subject: [Rd] int overflow writing long vectors to socketConnection
In-Reply-To: <0E317F6A-E81C-47E5-9388-F14A8F392B87@gmail.com>
References: <0E317F6A-E81C-47E5-9388-F14A8F392B87@gmail.com>
Message-ID: <24933.22211.780123.250926@stat.math.ethz.ch>

>>>>> Zafer Barutcuoglu 
>>>>>     on Tue, 12 Oct 2021 02:04:40 -0400 writes:

    > Hi,

    > Writing >=2GB to a socketConnection (e.g. via writeBin) does not work correctly, because of this int typecast in modules/internet/sockconn.c:
    >> static size_t sock_write(const void *ptr, size_t size, size_t nitems,
    >> Rconnection con)
    >> {
    >> Rsockconn this = (Rsockconn)con->private;
    >> ssize_t n = R_SockWrite(this->fd, ptr, (int)(size * nitems),
    this-> timeout)/((ssize_t)size);
    >> return n > 0 ? n : 0;
    >> }
    > which seems uncalled for, given:
    >> ssize_t R_SockWrite(int sockp, const void *buf, size_t len, int timeout)


    > Is there a rationale for it, or should it be fixed?

I've fixed it; it's clearly been a  "typo" introduced at the
same when the type of 'len' in the R_SockWrite() header was
changed from int to size_t .. and the intent must have been to
do the same inside sock_write().

    > Best,
    > --
    > Zafer

Thank you for the report!

Martin Maechler
ETH Zurich  and  R Core


From @osp@m m@iii@g oii @itieid-im@de  Tue Oct 12 12:11:10 2021
From: @osp@m m@iii@g oii @itieid-im@de (@osp@m m@iii@g oii @itieid-im@de)
Date: Tue, 12 Oct 2021 12:11:10 +0200
Subject: [Rd] Slow try in combination with do.call
In-Reply-To: <24899.27400.17297.779309@stat.math.ethz.ch>
References: <CWLP123MB3938E6AE26CF55BE8AC63456F7DC9@CWLP123MB3938.GBRP123.PROD.OUTLOOK.COM>
 <24899.26457.944004.373721@stat.math.ethz.ch>
 <24899.27400.17297.779309@stat.math.ethz.ch>
Message-ID: <61367f47abb6b2f4374c676dba2d8e4915d9cd31.camel@altfeld-im.de>

In fact an attentive user reported the same type of (slow due to deparse) problem in may tryCatchLog package recently when using a large sparse matrix

    https://github.com/aryoda/tryCatchLog/issues/68

and I have fixed it by explicitly using the nlines arg of deparse() instead of using as.character()
which implicitly calls deparse() for a call stack.

Looking for a fix I think I may have found inconsistent deparse default arguments in base R between as.character() and deparse():

    A direct deparse call in R uses
        control = c("keepNA", "keepInteger", "niceNames", "showAttributes")
    as default (see ?.deparseOpts for details).

    The as.character() implementation in the C code of base R calls the internal deparse C function
    with another default for .deparseOpts:
        The SIMPLEDEPARSE C constant which corresponds to control = NULL.
        https://github.com/wch/r-source/blob/54f94f0433c487fe55553b0df9bae477c9babdd1/src/main/deparse.c#L345

This is clearly no bug but maybe the as.character() implementation should use the default args of deparse() for consistency (just a proposal!)...

BTW: You can find my analysis result with the call path and links to the R source code in the github issue:
     https://github.com/aryoda/tryCatchLog/issues/68#issuecomment-930593002



On Thu, 2021-09-16 at 18:04 +0200, Martin Maechler wrote:
> > > > > > Martin Maechler 
> > > > > >     on Thu, 16 Sep 2021 17:48:41 +0200 writes:
> > > > > > Alexander Kaever 
> > > > > >     on Thu, 16 Sep 2021 14:00:03 +0000 writes:
> 
>     >> Hi,
>     >> It seems like a try(do.call(f, args)) can be very slow on error depending on the args size. This is related to a complete deparse of the call
> using deparse(call)[1L] within the try function. How about replacing deparse(call)[1L] by deparse(call, nlines = 1)?
> 
>     >> Best,
>     >> Alex
> 
>     > an *excellent* idea!
> 
>     > I have checked that the resulting try() object continues to contain the
>     > long large call; indeed that is not the problem, but the
>     > deparse()ing  *is* as you say above.
> 
>     > {The experts typically use  tryCatch() directly, instead of  try() ,
>     > which may be the reason other experienced R developers have not
>     > stumbled over this ...}
> 
>     > Thanks a lot, notably also for the clear  repr.ex. below.
> 
>     > Best regards,
>     > Martin
> 
> OTOH, I find so many cases  of   deparse(*)[1]  (or similar) in
> R's own sources, I'm wondering
> if I'm forgetting something ... and using nlines=* is not always
> faster & equivalent and hence better ??
> 
> Martin
> 
> 
> 
> 
>     >> Example:
> 
>     >> fun <- function(x) {
>     >> stop("testing")
>     >> }
>     >> d <- rep(list(mtcars), 10000)
>     >> object.size(d)
>     >> # 72MB
> 
>     >> system.time({
>     >> try(do.call(fun, args = list(x = d)))
>     >> })
>     >> # 8s
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Oct 12 12:43:04 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 12 Oct 2021 12:43:04 +0200
Subject: [Rd] Slow try in combination with do.call
In-Reply-To: <61367f47abb6b2f4374c676dba2d8e4915d9cd31.camel@altfeld-im.de>
References: <CWLP123MB3938E6AE26CF55BE8AC63456F7DC9@CWLP123MB3938.GBRP123.PROD.OUTLOOK.COM>
 <24899.26457.944004.373721@stat.math.ethz.ch>
 <24899.27400.17297.779309@stat.math.ethz.ch>
 <61367f47abb6b2f4374c676dba2d8e4915d9cd31.camel@altfeld-im.de>
Message-ID: <24933.26296.13729.20557@stat.math.ethz.ch>

Just in case, you hadn't noticed:

Since Sep.17, we have had the faster  try()  now in both R-devel
and "R 4.1.1 patched" which will be released as R 4.1.2  by the
end of this month, with NEWS entry

    ? try() is considerably faster in case of an error and long call,
      as e.g., from some do.call().  Thanks to Alexander Kaever's
      suggestion posted to R-devel.

Martin


>>>>> nospam at altfeld-im de 
>>>>>     on Tue, 12 Oct 2021 12:11:10 +0200 writes:

    > In fact an attentive user reported the same type of (slow due to deparse) problem in may tryCatchLog package recently when using a large sparse matrix
    > https://github.com/aryoda/tryCatchLog/issues/68

    > and I have fixed it by explicitly using the nlines arg of deparse() instead of using as.character()
    > which implicitly calls deparse() for a call stack.

    > Looking for a fix I think I may have found inconsistent deparse default arguments in base R between as.character() and deparse():

    > A direct deparse call in R uses
    > control = c("keepNA", "keepInteger", "niceNames", "showAttributes")
    > as default (see ?.deparseOpts for details).

    > The as.character() implementation in the C code of base R calls the internal deparse C function
    > with another default for .deparseOpts:
    > The SIMPLEDEPARSE C constant which corresponds to control = NULL.
    > https://github.com/wch/r-source/blob/54f94f0433c487fe55553b0df9bae477c9babdd1/src/main/deparse.c#L345

    > This is clearly no bug but maybe the as.character() implementation should use the default args of deparse() for consistency (just a proposal!)...

    > BTW: You can find my analysis result with the call path and links to the R source code in the github issue:
    > https://github.com/aryoda/tryCatchLog/issues/68#issuecomment-930593002



    > On Thu, 2021-09-16 at 18:04 +0200, Martin Maechler wrote:
    >> > > > > > Martin Maechler 
    >> > > > > >     on Thu, 16 Sep 2021 17:48:41 +0200 writes:
    >> > > > > > Alexander Kaever 
    >> > > > > >     on Thu, 16 Sep 2021 14:00:03 +0000 writes:
    >> 
    >> >> Hi,
    >> >> It seems like a try(do.call(f, args)) can be very slow on error depending on the args size. This is related to a complete deparse of the call
    >> using deparse(call)[1L] within the try function. How about replacing deparse(call)[1L] by deparse(call, nlines = 1)?
    >> 
    >> >> Best,
    >> >> Alex
    >> 
    >> > an *excellent* idea!
    >> 
    >> > I have checked that the resulting try() object continues to contain the
    >> > long large call; indeed that is not the problem, but the
    >> > deparse()ing  *is* as you say above.
    >> 
    >> > {The experts typically use  tryCatch() directly, instead of  try() ,
    >> > which may be the reason other experienced R developers have not
    >> > stumbled over this ...}
    >> 
    >> > Thanks a lot, notably also for the clear  repr.ex. below.
    >> 
    >> > Best regards,
    >> > Martin
    >> 
    >> OTOH, I find so many cases  of   deparse(*)[1]  (or similar) in
    >> R's own sources, I'm wondering
    >> if I'm forgetting something ... and using nlines=* is not always
    >> faster & equivalent and hence better ??
    >> 
    >> Martin
    >> 
    >> 
    >> 
    >> 
    >> >> Example:
    >> 
    >> >> fun <- function(x) {
    >> >> stop("testing")
    >> >> }
    >> >> d <- rep(list(mtcars), 10000)
    >> >> object.size(d)
    >> >> # 72MB
    >> 
    >> >> system.time({
    >> >> try(do.call(fun, args = list(x = d)))
    >> >> })
    >> >> # 8s
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel


From Thom@@@SOEIRO @end|ng |rom @p-hm@|r  Wed Oct 13 13:12:09 2021
From: Thom@@@SOEIRO @end|ng |rom @p-hm@|r (SOEIRO Thomas)
Date: Wed, 13 Oct 2021 11:12:09 +0000
Subject: [Rd] Potential bugs in table dnn
In-Reply-To: <1633866959731.85310@ap-hm.fr>
References: <1633866959731.85310@ap-hm.fr>
Message-ID: <8d7fe93b9ad64d4ab56edc2f28f2336b@SCWPR-EXDAG1-6A.aphm.ap-hm.fr>

Inline comments below in the previous message

I'm not 100% sure if the current behavior is intended or not. If not, here is a patch (which I can submit on R Bugzilla if appropriate):


diff -u orig/table.R mod/table.R
--- orig/table.R	2021-10-13 10:04:28.560912800 +0200
+++ mod/table.R	2021-10-13 10:43:43.815915100 +0200
@@ -1,7 +1,7 @@
 #  File src/library/base/R/table.R
 #  Part of the R package, https://www.R-project.org
 #
-#  Copyright (C) 1995-2020 The R Core Team
+#  Copyright (C) 1995-2021 The R Core Team
 #
 #  This program is free software; you can redistribute it and/or modify
 #  it under the terms of the GNU General Public License as published by
@@ -50,9 +50,8 @@
     args <- list(...)
     if (length(args) == 1L && is.list(args[[1L]])) { ## e.g. a data.frame
 	args <- args[[1L]]
-	if (length(dnn) != length(args))
-	    dnn <- if (!is.null(argn <- names(args))) argn
-		   else paste(dnn[1L], seq_along(args), sep = ".")
+	dnn <- if (!is.null(argn <- names(args))) argn
+	       else paste(dnn[1L], seq_along(args), sep = ".")
     }
     if (!length(args))
 	stop("nothing to tabulate")
diff -u orig/table.Rd mod/table.Rd
--- orig/table.Rd	2021-10-13 11:39:45.839097000 +0200
+++ mod/table.Rd	2021-10-13 11:56:25.620660900 +0200
@@ -1,6 +1,6 @@
 % File src/library/base/man/table.Rd
 % Part of the R package, https://www.R-project.org
-% Copyright 1995-2021 R Core Team
+% Copyright 1995-2016 R Core Team
 % Distributed under GPL 2 or later
 
 \name{table}
@@ -48,7 +48,7 @@
   \item{useNA}{whether to include \code{NA} values in the table.
     See \sQuote{Details}.  Can be abbreviated.}
   \item{dnn}{the names to be given to the dimensions in the result (the
-    \emph{dimnames names}).}
+    \emph{dimnames names}).  See \sQuote{Details}.}
   \item{deparse.level}{controls how the default \code{dnn} is
     constructed.  See \sQuote{Details}.}
   \item{x}{an arbitrary \R object, or an object inheriting from class
@@ -64,12 +64,15 @@
   \item{sep, base}{passed to \code{\link{provideDimnames}}.}
 }
 \details{
-  If the argument \code{dnn} is not supplied, the internal function
+  If ... is one or more objects which can be interpreted as factors
+  and the argument \code{dnn} is not supplied, the internal function
   \code{list.names} is called to compute the \sQuote{dimname names}.  If the
   arguments in \code{\dots} are named, those names are used.  For the
   remaining arguments, \code{deparse.level = 0} gives an empty name,
   \code{deparse.level = 1} uses the supplied argument if it is a symbol,
-  and \code{deparse.level = 2} will deparse the argument.
+  and \code{deparse.level = 2} will deparse the argument.  Otherwise,
+  if ... is a list (or data frame), its names are used as the
+  \sQuote{dimname names} and the argument \code{dnn} is not used.
 
   Only when \code{exclude} is specified (i.e., not by default) and
   non-empty, will \code{table} potentially drop levels of factor



> Dear list,
> 
> table does not set dnn for dataframes of length 1:
> 
> table(warpbreaks[2:3]) # has dnn
> #     tension
> # wool L M H
> #    A 9 9 9
> #    B 9 9 9
> 
> table(warpbreaks[2]) # has no dnn
> # 
> #  A  B 
> # 27 27 
> 
> This is because of if (length(dnn) != length(args)) (line 53 in https://github.com/wch/r-source/blob/trunk/src/library/base/R/table.R). When commenting this line or modifying it to if (length(dnn) != length(args) || dnn == ""), dnn are set as expected:
> 
> table2(warpbreaks[2:3]) # has dnn
> #     tension
> # wool L M H
> #    A 9 9 9
> #    B 9 9 9
> 
> table2(warpbreaks[2]) # has dnn
> # wool
> #  A  B 
> # 27 27 
> 
> However, I do not get the logic for the initial if (length(dnn) != length(args)), so the change may break something else...

I guess the purpose of this line is to have the possibility to set the dimname names through the dnn argument for lists (or data frames) of length 1, e.g.:

table(warpbreaks[2], dnn = "xxx")
# xxx
#  A  B 
# 27 27

However, this seems inconsistent with the behavior for lists (or data frames) of length >1. Removing the exception introduced by the if clause restore the consistency in dimname names for lists (or data frames) whatever their length.


> In addition, table documentation says "If the argument dnn is not supplied, the internal function list.names is called to compute the 'dimname names'. If the arguments in ... are named, those names are used." Some cases seem inconsistent or may return a warning:

The documentation seems not very clear on how dimname names are computed for lists (or data frames). If removing the if clause [i.e., consistent behavior for lists (or data frames), see above], I think it only requires to document the "precedence" of list (or data frame) names over dnn when ... is a list (or data frame), e.g.: "if ... is a list (or data frame), its names are used as the \sQuote{dimname names} and the argument \code{dnn} is not used." (see the patch above)


> table(warpbreaks[2], dnn = letters) # no warning/not as documented
> # wool
> #  A  B 
> # 27 27 
> 
> table(warpbreaks[2], dnn = letters[1]) # as documented
> # a
> #  A  B 
> # 27 27 
> 
> table(zzz = warpbreaks[2], dnn = letters[1]) # as documented
> # a
> #  A  B 
> # 27 27 
> 
> table(zzz = warpbreaks$wool, dnn = letters[1]) # as documented
> # a
> #  A  B 
> # 27 27 
> 
> table(warpbreaks$wool, dnn = letters) # as expected
> # Error in names(dn) <- dnn : 
> #   attribut 'names' [26] doit ?tre de m?me longueur que le vecteur [1]
> 
> Best regards,
> 
> Thomas


