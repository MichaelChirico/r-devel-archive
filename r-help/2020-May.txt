From @purd|e@@ @end|ng |rom gm@||@com  Fri May  1 07:21:32 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Fri, 1 May 2020 17:21:32 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
Message-ID: <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>

I agree, the documentation gives the impression that stats::spline
would allow "monoH.FC".

My guess is that stats::spline doesn't allow it because the function
is designed to return a list with x and y components. This doesn't
suit monotonic cubic Hermite splines because the function would also
need to return the slopes. Furthermore, additional functions may (or
may not) be required depending on what you want to do with x, y and
slope vectors.

Note that my package kubik, provides a range of functions for working
with cubic Hermite splines.






On Fri, May 1, 2020 at 4:39 AM Samuel Granjeaud IR/Inserm
<samuel.granjeaud at inserm.fr> wrote:
>
> Hi,
>
> I have just noticed that the argument method of the spline function of
> the stats package does not allow to specify monoH.FC although the
> documentation tells it should be possible.
>
> I know how to program a workaround. This post intends to alert the
> maintainers.
>
> Stay safe,
> Samuel
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri May  1 16:14:47 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 1 May 2020 16:14:47 +0200
Subject: [R] [FORGED] Re:  pair correlation function of 3D points
In-Reply-To: <e2fdd5e8-2d1e-a07e-e5ed-4163380c0e17@auckland.ac.nz>
References: <07f9c7f7f729a3a9bf641fd67fd8a165f7d799e5.camel@math.aau.dk>
 <F5F41F99-A6C7-42D8-A197-C4A532C1123B@icmpe.cnrs.fr>
 <e2fdd5e8-2d1e-a07e-e5ed-4163380c0e17@auckland.ac.nz>
Message-ID: <24236.11991.134621.644096@stat.math.ethz.ch>

>>>>> Rolf Turner 
>>>>>     on Thu, 30 Apr 2020 10:42:46 +1200 writes:

    > On 30/04/20 12:28 am, Eric Leroy wrote:

    >> Dear all, I am sorry to see all the reactions I provoked from a
    >> newbie user. Anyway, thank you for the answer I think that the
    >> pcf3est function responds to my question.
    >> Indeed the spatstat is a very impressive library and I am very grateful to the the developers.

    > (1) Not to worry.  Certainly not your fault!

    > (2) I'm glad that the pcf3est() function was useful to you.

    > (3) Thank you for your kind words about spatstat.

    > (4) But *please* --- spatstat is a *package* not a "library"!!!  A 
    > library is a *collection* of packages; the library() function "checks 
    > out" a package from a library, like checking a book out of a "real" 
    > library (biblioth?que en fran?ais, just in case there is any confusion, 
    > "library" and "libraire" being false cognates).  But I'm sure you knew that.

    > I know that insisting on this distinction is being pedantic --- but it 
    > never hurts to get things right!  And saying "library" when you mean 
    > "package" upsets Martin Maechler!!! :-)

Thanks a lot, Rolf!

Well, indeed, I've become a teeny bit wiser with age, so
currently, it's just a little inaudible "ouch", and no longer a
real upsetting ...

And yes

> fortunes::fortune("rrgh")

(3 times...rrrrgh...) and why do you think the mailing list is called R-*packages* ???????????
Please do
  for(i in 1:20) cat("It's a package!\n")
   -- Martin Maechler (after a newly released *package* has been called *library* three times in its announcement
      on R-packages)
      R-help (March 2006)

>

so maybe we can close the thread on a bit of a  cheerful tone.

Best, Martin


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri May  1 18:46:25 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 1 May 2020 17:46:25 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
Message-ID: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>

Dear Contributors,
I am trying to do a plot with multiple y-axis on a common x-axis. I have
made some progress but I am having difficulties with datetime appearing on
x-axis. Instead of as date, it appears as large numbers.

Sample of my data:
78 09 28  0 6.7 -40.4 -3.5 2.3 -3.6 278036 5.8 612
78 09 28  1 5.7 7.3 -4.4 1.4 0.6 261169 4.9 623
78 09 28  2 5.6 -20.9 -3.6 3.1 -1.8 269323 4.8 625
78 09 28  3 5.8 -46.9 -3.4 0.2 -3.6 254221 4.7 626
78 09 28  4 6.2 -41.4 -3.9 1.2 -3.6 248567 4.2 618
78 09 28  5 6.1 -54.6 -3 1 -4.4 252669 4.4 629
78 09 28  6 NA NA NA NA NA 220480 4.3 606
78 09 28  7 7.1 26 -4.8 3.6 2.9 216887 4.4 613
78 09 28  8 7.1 -21.4 -4.7 3.5 -2.3 249928 4 643
78 09 28  9 7 -13.6 -2.7 5.2 -1.4 268806 4 656
78 09 28  10 6.6 -18.5 -3.7 3.6 -1.7 285608 4.2 651
78 09 28  11 6.7 -6.6 -2.1 4.7 -0.6 280446 4.2 661
78 09 28  12 6.7 -29.7 -1.9 4 -2.5 281928 4.2 659
78 09 28  13 6.2 -8.7 -2.1 4.7 -0.8 273465 4.4 651
78 09 28  14 6.1 31.8 -4.7 0.9 3 212374 4.2 631
78 09 28  15 5.4 9.3 -4.5 -0.9 0.8 219662 3.9 636
78 09 28  16 4.9 6.3 -4.5 -0.5 0.5 220610 3.8 628
78 09 28  17 4.7 4.5 -4.3 -0.6 0.3 214432 3.7 628
78 09 28  18 4.9 3.1 -4.5 0.3 0.2 202199 3.5 623
78 09 28  19 5 -13 -4.6 -0.3 -1.1 192859 3.2 619
78 09 28  20 5.1 -26 -3.5 2 -2 193868 3.1 627
78 09 28  21 10.1 -37.5 -5.1 4.9 -5.4 284122 5.9 683
78 09 28  22 8.4 -7.3 -3.6 5.5 -0.8 367499 6.2 694
78 09 28  23 8.2 -17.7 -4.7 4.4 -2.1 346644 4.9 689
78 09 29  0 8 6.3 -4.3 5.8 0.8 269569 4.7 708
78 09 29  1 8.6 35 -3 6.2 4.8 187132 3.2 709
78 09 29  2 8.1 24.8 -4.1 6 3.3 166644 3.5 689
78 09 29  3 15.9 29.6 -2.4 9.6 5.6 720902 6.6 866
78 09 29  4 14.9 18.9 -2.8 13.8 4.8 1587324 8.1 912
78 09 29  5 14.1 8.2 -10.2 8.3 1.9 1509336 8.1 871
78 09 29  6 15.6 -2.9 -6.1 11.5 -0.7 968890 7.8 878
78 09 29  7 19.6 -49.7 1.2 12.1 -14.4 574978 5.4 906
78 09 29  8 23.5 -44.2 -1.4 11.7 -11.5 287721 4.1 865
78 09 29  9 25.3 -71.8 -2.4 7.5 -23.9 58521 5.1 859
78 09 29  10 24.8 -63.1 -2.3 10.9 -22 74956 1.5 821
78 09 29  11 23.8 -50 -2 15.1 -18.1 64510 3 807
78 09 29  12 21.4 -34.3 0.1 17.6 -12 50247 2.3 795
78 09 29  13 18.4 -20 -2 17 -6.3 59939 2.9 802
78 09 29  14 16.4 -13.9 -2.8 15.6 -3.9 56499 1.4 799
78 09 29  15 15.3 -7.9 -2 14.9 -2.1 49346 1.3 759
78 09 29  16 14.1 -0.5 -2.9 13.8 -0.1 57732 0.5 730
78 09 29  17 12.6 14.7 -3.1 11.5 3.1 44351 1.7 722
78 09 29  18 11.2 74.3 0.6 2.9 10.4 68224 0.4 712
78 09 29  19 10.5 60.5 3.2 4 9.1 48082 0.2 706
78 09 29  20 10 54.7 3.9 4.2 8.1 56404 0.2 710
78 09 29  21 9.3 61.1 3.4 2.9 8.1 68824 0.2 694
78 09 29  22 8.6 64.4 2.6 2.7 7.8 44579 0.2 683
78 09 29  23 7.9 63.3 2 3 7.1 39760 0.2 661
78 09 30  0 7.3 59.8 2.3 2.8 6.3 NA NA NA
78 09 30  1 6.6 65.6 1.7 2.1 6 57382 0.5 664
78 09 30  2 6 70.8 1.3 1.4 5.5 63540 0.5 654
78 09 30  3 4.4 45.1 1 2.7 2.9 60856 2.5 635
78 09 30  4 3.6 -28 -1 -0.3 -0.6 80967 1.9 633
78 09 30  5 4 -47.6 -2.2 -0.1 -2.4 48391 1.2 642
78 09 30  6 4.1 -28.9 -3.1 -1.4 -1.9 45012 1.1 643
78 09 30  7 4.2 -42.7 -2.7 1.2 -2.7 45562 1.1 633
 and part of my code is:
Sys.setenv( TZ="GMT" )
dta<-read.table("IFEDIFIG1D",col.names=c("year", "month", "day",
"hour","B","LAT","BX","BZ","BY","SWT","SWD","SW"))
dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
month,day,hour,0,0)))
x =  dta$datetime
B=dta$B
LAT=dta$LAT
BX=dta$BX
BZ=dta$BZ
 par(mfcol = c(3, 1), mar = numeric(4), oma = c(4, 4, 1, 1),
    mgp = c(2, 0.5, 0))#0.5 here controls x-axis margins
plot(x, B,type="l", axes = FALSE)
abline(v=-3)
axis(2L)
plot(x, LAT,type="l", axes = FALSE)
abline(v=-3)
axis(2L)
 plot(x, BX, type="l",axes = FALSE)
abline(v=-3)
axis(1L)
axis(2L)
mtext("A1", side = 1, outer = TRUE, line = 2.2)
mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)

If I do a simple plot of x and B, the x-axis will be fine, appearing as
Thu, Fri, Sat, Sun.
Please let me know what I am doing wrong with the multiple plot code above.
I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.

I am most grateful for your kind response.

Warmest regards
Ogbos

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  1 19:29:56 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 1 May 2020 18:29:56 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
Message-ID: <80280e72-a629-ad6a-5f54-f7691589e59d@sapo.pt>

Hello,

If you don't mind a ggplot2 solution, here it is.
As usual with ggplot, it's better to have the data in long format so I 
also load packages dplyr and tidyr. The reshaping accounts for half the 
code, the code  for the graph itself is not very complicated.



library(ggplot2)
library(scales)
library(dplyr)
library(tidyr)

dta %>%
   select(datetime, B, BX, BZ) %>%
   rename(x = datetime, B1 = B, B2 = BX, B3 = BZ) %>%
   pivot_longer(
     cols = starts_with("B"),
     names_to = "variable",
     values_to = "value"
   ) %>%
   ggplot(aes(x, value)) +
   geom_line() +
   xlab("") + ylab("") +
   scale_x_datetime(
     breaks = seq(min(x), max(x), by = "days"),
     labels = time_format("%a")) +
   facet_grid(rows = vars(variable)) +
   theme_bw()



Hope this helps,

Rui Barradas

?s 17:46 de 01/05/20, Ogbos Okike escreveu:
> Dear Contributors,
> I am trying to do a plot with multiple y-axis on a common x-axis. I have
> made some progress but I am having difficulties with datetime appearing on
> x-axis. Instead of as date, it appears as large numbers.
> 
> Sample of my data:
> 78 09 28  0 6.7 -40.4 -3.5 2.3 -3.6 278036 5.8 612
> 78 09 28  1 5.7 7.3 -4.4 1.4 0.6 261169 4.9 623
> 78 09 28  2 5.6 -20.9 -3.6 3.1 -1.8 269323 4.8 625
> 78 09 28  3 5.8 -46.9 -3.4 0.2 -3.6 254221 4.7 626
> 78 09 28  4 6.2 -41.4 -3.9 1.2 -3.6 248567 4.2 618
> 78 09 28  5 6.1 -54.6 -3 1 -4.4 252669 4.4 629
> 78 09 28  6 NA NA NA NA NA 220480 4.3 606
> 78 09 28  7 7.1 26 -4.8 3.6 2.9 216887 4.4 613
> 78 09 28  8 7.1 -21.4 -4.7 3.5 -2.3 249928 4 643
> 78 09 28  9 7 -13.6 -2.7 5.2 -1.4 268806 4 656
> 78 09 28  10 6.6 -18.5 -3.7 3.6 -1.7 285608 4.2 651
> 78 09 28  11 6.7 -6.6 -2.1 4.7 -0.6 280446 4.2 661
> 78 09 28  12 6.7 -29.7 -1.9 4 -2.5 281928 4.2 659
> 78 09 28  13 6.2 -8.7 -2.1 4.7 -0.8 273465 4.4 651
> 78 09 28  14 6.1 31.8 -4.7 0.9 3 212374 4.2 631
> 78 09 28  15 5.4 9.3 -4.5 -0.9 0.8 219662 3.9 636
> 78 09 28  16 4.9 6.3 -4.5 -0.5 0.5 220610 3.8 628
> 78 09 28  17 4.7 4.5 -4.3 -0.6 0.3 214432 3.7 628
> 78 09 28  18 4.9 3.1 -4.5 0.3 0.2 202199 3.5 623
> 78 09 28  19 5 -13 -4.6 -0.3 -1.1 192859 3.2 619
> 78 09 28  20 5.1 -26 -3.5 2 -2 193868 3.1 627
> 78 09 28  21 10.1 -37.5 -5.1 4.9 -5.4 284122 5.9 683
> 78 09 28  22 8.4 -7.3 -3.6 5.5 -0.8 367499 6.2 694
> 78 09 28  23 8.2 -17.7 -4.7 4.4 -2.1 346644 4.9 689
> 78 09 29  0 8 6.3 -4.3 5.8 0.8 269569 4.7 708
> 78 09 29  1 8.6 35 -3 6.2 4.8 187132 3.2 709
> 78 09 29  2 8.1 24.8 -4.1 6 3.3 166644 3.5 689
> 78 09 29  3 15.9 29.6 -2.4 9.6 5.6 720902 6.6 866
> 78 09 29  4 14.9 18.9 -2.8 13.8 4.8 1587324 8.1 912
> 78 09 29  5 14.1 8.2 -10.2 8.3 1.9 1509336 8.1 871
> 78 09 29  6 15.6 -2.9 -6.1 11.5 -0.7 968890 7.8 878
> 78 09 29  7 19.6 -49.7 1.2 12.1 -14.4 574978 5.4 906
> 78 09 29  8 23.5 -44.2 -1.4 11.7 -11.5 287721 4.1 865
> 78 09 29  9 25.3 -71.8 -2.4 7.5 -23.9 58521 5.1 859
> 78 09 29  10 24.8 -63.1 -2.3 10.9 -22 74956 1.5 821
> 78 09 29  11 23.8 -50 -2 15.1 -18.1 64510 3 807
> 78 09 29  12 21.4 -34.3 0.1 17.6 -12 50247 2.3 795
> 78 09 29  13 18.4 -20 -2 17 -6.3 59939 2.9 802
> 78 09 29  14 16.4 -13.9 -2.8 15.6 -3.9 56499 1.4 799
> 78 09 29  15 15.3 -7.9 -2 14.9 -2.1 49346 1.3 759
> 78 09 29  16 14.1 -0.5 -2.9 13.8 -0.1 57732 0.5 730
> 78 09 29  17 12.6 14.7 -3.1 11.5 3.1 44351 1.7 722
> 78 09 29  18 11.2 74.3 0.6 2.9 10.4 68224 0.4 712
> 78 09 29  19 10.5 60.5 3.2 4 9.1 48082 0.2 706
> 78 09 29  20 10 54.7 3.9 4.2 8.1 56404 0.2 710
> 78 09 29  21 9.3 61.1 3.4 2.9 8.1 68824 0.2 694
> 78 09 29  22 8.6 64.4 2.6 2.7 7.8 44579 0.2 683
> 78 09 29  23 7.9 63.3 2 3 7.1 39760 0.2 661
> 78 09 30  0 7.3 59.8 2.3 2.8 6.3 NA NA NA
> 78 09 30  1 6.6 65.6 1.7 2.1 6 57382 0.5 664
> 78 09 30  2 6 70.8 1.3 1.4 5.5 63540 0.5 654
> 78 09 30  3 4.4 45.1 1 2.7 2.9 60856 2.5 635
> 78 09 30  4 3.6 -28 -1 -0.3 -0.6 80967 1.9 633
> 78 09 30  5 4 -47.6 -2.2 -0.1 -2.4 48391 1.2 642
> 78 09 30  6 4.1 -28.9 -3.1 -1.4 -1.9 45012 1.1 643
> 78 09 30  7 4.2 -42.7 -2.7 1.2 -2.7 45562 1.1 633
>   and part of my code is:
> Sys.setenv( TZ="GMT" )
> dta<-read.table("IFEDIFIG1D",col.names=c("year", "month", "day",
> "hour","B","LAT","BX","BZ","BY","SWT","SWD","SW"))
> dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
> month,day,hour,0,0)))
> x =  dta$datetime
> B=dta$B
> LAT=dta$LAT
> BX=dta$BX
> BZ=dta$BZ
>   par(mfcol = c(3, 1), mar = numeric(4), oma = c(4, 4, 1, 1),
>      mgp = c(2, 0.5, 0))#0.5 here controls x-axis margins
> plot(x, B,type="l", axes = FALSE)
> abline(v=-3)
> axis(2L)
> plot(x, LAT,type="l", axes = FALSE)
> abline(v=-3)
> axis(2L)
>   plot(x, BX, type="l",axes = FALSE)
> abline(v=-3)
> axis(1L)
> axis(2L)
> mtext("A1", side = 1, outer = TRUE, line = 2.2)
> mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)
> 
> If I do a simple plot of x and B, the x-axis will be fine, appearing as
> Thu, Fri, Sat, Sun.
> Please let me know what I am doing wrong with the multiple plot code above.
> I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
> 
> I am most grateful for your kind response.
> 
> Warmest regards
> Ogbos
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  1 19:35:22 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 1 May 2020 18:35:22 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <80280e72-a629-ad6a-5f54-f7691589e59d@sapo.pt>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
 <80280e72-a629-ad6a-5f54-f7691589e59d@sapo.pt>
Message-ID: <28f66e3f-f988-bf07-e07f-f3842e2086b1@sapo.pt>

Hello,

I have just noticed that the facets' labels are on the right, in your 
code example they are on the left. Change the following:


facet_grid(rows = vars(variable), switch = "y")


Hope this helps,

Rui Barradas

?s 18:29 de 01/05/20, Rui Barradas escreveu:
> Hello,
> 
> If you don't mind a ggplot2 solution, here it is.
> As usual with ggplot, it's better to have the data in long format so I 
> also load packages dplyr and tidyr. The reshaping accounts for half the 
> code, the code? for the graph itself is not very complicated.
> 
> 
> 
> library(ggplot2)
> library(scales)
> library(dplyr)
> library(tidyr)
> 
> dta %>%
>  ? select(datetime, B, BX, BZ) %>%
>  ? rename(x = datetime, B1 = B, B2 = BX, B3 = BZ) %>%
>  ? pivot_longer(
>  ??? cols = starts_with("B"),
>  ??? names_to = "variable",
>  ??? values_to = "value"
>  ? ) %>%
>  ? ggplot(aes(x, value)) +
>  ? geom_line() +
>  ? xlab("") + ylab("") +
>  ? scale_x_datetime(
>  ??? breaks = seq(min(x), max(x), by = "days"),
>  ??? labels = time_format("%a")) +
>  ? facet_grid(rows = vars(variable)) +
>  ? theme_bw()
> 
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 17:46 de 01/05/20, Ogbos Okike escreveu:
>> Dear Contributors,
>> I am trying to do a plot with multiple y-axis on a common x-axis. I have
>> made some progress but I am having difficulties with datetime 
>> appearing on
>> x-axis. Instead of as date, it appears as large numbers.
>>
>> Sample of my data:
>> 78 09 28? 0 6.7 -40.4 -3.5 2.3 -3.6 278036 5.8 612
>> 78 09 28? 1 5.7 7.3 -4.4 1.4 0.6 261169 4.9 623
>> 78 09 28? 2 5.6 -20.9 -3.6 3.1 -1.8 269323 4.8 625
>> 78 09 28? 3 5.8 -46.9 -3.4 0.2 -3.6 254221 4.7 626
>> 78 09 28? 4 6.2 -41.4 -3.9 1.2 -3.6 248567 4.2 618
>> 78 09 28? 5 6.1 -54.6 -3 1 -4.4 252669 4.4 629
>> 78 09 28? 6 NA NA NA NA NA 220480 4.3 606
>> 78 09 28? 7 7.1 26 -4.8 3.6 2.9 216887 4.4 613
>> 78 09 28? 8 7.1 -21.4 -4.7 3.5 -2.3 249928 4 643
>> 78 09 28? 9 7 -13.6 -2.7 5.2 -1.4 268806 4 656
>> 78 09 28? 10 6.6 -18.5 -3.7 3.6 -1.7 285608 4.2 651
>> 78 09 28? 11 6.7 -6.6 -2.1 4.7 -0.6 280446 4.2 661
>> 78 09 28? 12 6.7 -29.7 -1.9 4 -2.5 281928 4.2 659
>> 78 09 28? 13 6.2 -8.7 -2.1 4.7 -0.8 273465 4.4 651
>> 78 09 28? 14 6.1 31.8 -4.7 0.9 3 212374 4.2 631
>> 78 09 28? 15 5.4 9.3 -4.5 -0.9 0.8 219662 3.9 636
>> 78 09 28? 16 4.9 6.3 -4.5 -0.5 0.5 220610 3.8 628
>> 78 09 28? 17 4.7 4.5 -4.3 -0.6 0.3 214432 3.7 628
>> 78 09 28? 18 4.9 3.1 -4.5 0.3 0.2 202199 3.5 623
>> 78 09 28? 19 5 -13 -4.6 -0.3 -1.1 192859 3.2 619
>> 78 09 28? 20 5.1 -26 -3.5 2 -2 193868 3.1 627
>> 78 09 28? 21 10.1 -37.5 -5.1 4.9 -5.4 284122 5.9 683
>> 78 09 28? 22 8.4 -7.3 -3.6 5.5 -0.8 367499 6.2 694
>> 78 09 28? 23 8.2 -17.7 -4.7 4.4 -2.1 346644 4.9 689
>> 78 09 29? 0 8 6.3 -4.3 5.8 0.8 269569 4.7 708
>> 78 09 29? 1 8.6 35 -3 6.2 4.8 187132 3.2 709
>> 78 09 29? 2 8.1 24.8 -4.1 6 3.3 166644 3.5 689
>> 78 09 29? 3 15.9 29.6 -2.4 9.6 5.6 720902 6.6 866
>> 78 09 29? 4 14.9 18.9 -2.8 13.8 4.8 1587324 8.1 912
>> 78 09 29? 5 14.1 8.2 -10.2 8.3 1.9 1509336 8.1 871
>> 78 09 29? 6 15.6 -2.9 -6.1 11.5 -0.7 968890 7.8 878
>> 78 09 29? 7 19.6 -49.7 1.2 12.1 -14.4 574978 5.4 906
>> 78 09 29? 8 23.5 -44.2 -1.4 11.7 -11.5 287721 4.1 865
>> 78 09 29? 9 25.3 -71.8 -2.4 7.5 -23.9 58521 5.1 859
>> 78 09 29? 10 24.8 -63.1 -2.3 10.9 -22 74956 1.5 821
>> 78 09 29? 11 23.8 -50 -2 15.1 -18.1 64510 3 807
>> 78 09 29? 12 21.4 -34.3 0.1 17.6 -12 50247 2.3 795
>> 78 09 29? 13 18.4 -20 -2 17 -6.3 59939 2.9 802
>> 78 09 29? 14 16.4 -13.9 -2.8 15.6 -3.9 56499 1.4 799
>> 78 09 29? 15 15.3 -7.9 -2 14.9 -2.1 49346 1.3 759
>> 78 09 29? 16 14.1 -0.5 -2.9 13.8 -0.1 57732 0.5 730
>> 78 09 29? 17 12.6 14.7 -3.1 11.5 3.1 44351 1.7 722
>> 78 09 29? 18 11.2 74.3 0.6 2.9 10.4 68224 0.4 712
>> 78 09 29? 19 10.5 60.5 3.2 4 9.1 48082 0.2 706
>> 78 09 29? 20 10 54.7 3.9 4.2 8.1 56404 0.2 710
>> 78 09 29? 21 9.3 61.1 3.4 2.9 8.1 68824 0.2 694
>> 78 09 29? 22 8.6 64.4 2.6 2.7 7.8 44579 0.2 683
>> 78 09 29? 23 7.9 63.3 2 3 7.1 39760 0.2 661
>> 78 09 30? 0 7.3 59.8 2.3 2.8 6.3 NA NA NA
>> 78 09 30? 1 6.6 65.6 1.7 2.1 6 57382 0.5 664
>> 78 09 30? 2 6 70.8 1.3 1.4 5.5 63540 0.5 654
>> 78 09 30? 3 4.4 45.1 1 2.7 2.9 60856 2.5 635
>> 78 09 30? 4 3.6 -28 -1 -0.3 -0.6 80967 1.9 633
>> 78 09 30? 5 4 -47.6 -2.2 -0.1 -2.4 48391 1.2 642
>> 78 09 30? 6 4.1 -28.9 -3.1 -1.4 -1.9 45012 1.1 643
>> 78 09 30? 7 4.2 -42.7 -2.7 1.2 -2.7 45562 1.1 633
>> ? and part of my code is:
>> Sys.setenv( TZ="GMT" )
>> dta<-read.table("IFEDIFIG1D",col.names=c("year", "month", "day",
>> "hour","B","LAT","BX","BZ","BY","SWT","SWD","SW"))
>> dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
>> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
>> month,day,hour,0,0)))
>> x =? dta$datetime
>> B=dta$B
>> LAT=dta$LAT
>> BX=dta$BX
>> BZ=dta$BZ
>> ? par(mfcol = c(3, 1), mar = numeric(4), oma = c(4, 4, 1, 1),
>> ???? mgp = c(2, 0.5, 0))#0.5 here controls x-axis margins
>> plot(x, B,type="l", axes = FALSE)
>> abline(v=-3)
>> axis(2L)
>> plot(x, LAT,type="l", axes = FALSE)
>> abline(v=-3)
>> axis(2L)
>> ? plot(x, BX, type="l",axes = FALSE)
>> abline(v=-3)
>> axis(1L)
>> axis(2L)
>> mtext("A1", side = 1, outer = TRUE, line = 2.2)
>> mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
>> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
>> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)
>>
>> If I do a simple plot of x and B, the x-axis will be fine, appearing as
>> Thu, Fri, Sat, Sun.
>> Please let me know what I am doing wrong with the multiple plot code 
>> above.
>> I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
>>
>> I am most grateful for your kind response.
>>
>> Warmest regards
>> Ogbos
>>
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rcoppock @end|ng |rom cox@net  Fri May  1 18:24:03 2020
From: rcoppock @end|ng |rom cox@net (Roger Coppock)
Date: Fri, 1 May 2020 09:24:03 -0700
Subject: [R] CRAN library down?
Message-ID: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>

After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded libraries.  Also, the package installer can not contact CRAN libraries either for binaries or sources, to replace the missing loaded libraries.  The package installer can contact "BioConductor", however.  I am now specifically looking for "HURDAT" and "lmtest", which were on CRAN but not "BioConductor".

- -  Roger Coppock (rcoppock at cox.net)

From murdoch@dunc@n @end|ng |rom gm@||@com  Fri May  1 20:19:32 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 1 May 2020 14:19:32 -0400
Subject: [R] CRAN library down?
In-Reply-To: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
References: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
Message-ID: <3a6465ff-519b-7cf4-6a3a-0de5bb86647e@gmail.com>

On 01/05/2020 12:24 p.m., Roger Coppock wrote:
> After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded libraries.  Also, the package installer can not contact CRAN libraries either for binaries or sources, to replace the missing loaded libraries.  The package installer can contact "BioConductor", however.  I am now specifically looking for "HURDAT" and "lmtest", which were on CRAN but not "BioConductor".

I'd guess you're using a mirror that is slow to update.  I recommend 
cloud.r-project.org.  It is very well maintained by the RStudio folks.

Duncan Murdoch


From r@turner @end|ng |rom @uck|@nd@@c@nz  Fri May  1 23:13:25 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 2 May 2020 09:13:25 +1200
Subject: [R] 
 [FORGED]  Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
Message-ID: <7ee9828c-90d3-cb30-c074-35fd952998ef@auckland.ac.nz>


On 2/05/20 4:46 am, Ogbos Okike wrote:

> Dear Contributors,
> I am trying to do a plot with multiple y-axis on a common x-axis.

<SNIP>

I would strongly advise you *not* to.  This, although often done, is a 
bad practice, and can sometimes (often) give misleading impressions.  See

     https://blog.datawrapper.de/dualaxis/

cheers,

Rolf


-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From drj|m|emon @end|ng |rom gm@||@com  Sat May  2 00:46:50 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 2 May 2020 08:46:50 +1000
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
Message-ID: <CA+8X3fWGCtgq7wHC0AvDmgfzbJ4MNgDv62t2ZMT9r6iJf9C9Yg@mail.gmail.com>

Hi Ogbos,
The following code may get you close to what you want. I have used the
names of the columns in "dta" as it is less confusing for me. I think
you meant to request horizontal ablines as none appeared in the
example. In order to get the axis tick label "Sun" you would have to
increase the x axis limits beyond your data.

plot(dta$datetime,dta$B,type="l",axes=FALSE)
# this is out of range and doesn't appear
abline(h=-3,col="red")
axis(2)
plot(dta$datetime,dta$LAT,type="l",axes=FALSE)
abline(h=-3,col="red")
axis(2)
plot(dta$datetime,dta$BX,type="l",axes=FALSE)
abline(h=-3,col="red")
axis(2)
axis.POSIXct(1,dta$datetime,format="%a")
mtext("A1", side = 1, outer = TRUE, line = 2.2)
mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)

Jim

On Sat, May 2, 2020 at 2:47 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Contributors,
> I am trying to do a plot with multiple y-axis on a common x-axis. I have
> made some progress but I am having difficulties with datetime appearing on
> x-axis. Instead of as date, it appears as large numbers.
> ...
> If I do a simple plot of x and B, the x-axis will be fine, appearing as
> Thu, Fri, Sat, Sun.
> Please let me know what I am doing wrong with the multiple plot code above.
> I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
>


From @purd|e@@ @end|ng |rom gm@||@com  Sat May  2 01:05:57 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sat, 2 May 2020 11:05:57 +1200
Subject: [R] [FORGED] Re: pair correlation function of 3D points
In-Reply-To: <24236.11991.134621.644096@stat.math.ethz.ch>
References: <07f9c7f7f729a3a9bf641fd67fd8a165f7d799e5.camel@math.aau.dk>
 <F5F41F99-A6C7-42D8-A197-C4A532C1123B@icmpe.cnrs.fr>
 <e2fdd5e8-2d1e-a07e-e5ed-4163380c0e17@auckland.ac.nz>
 <24236.11991.134621.644096@stat.math.ethz.ch>
Message-ID: <CAB8pepzEhwd4vqPnGnBH8uyfKnrUjLJoqBmtgFPKKz2pBrLV0Q@mail.gmail.com>

It may be of some comfort to some readers, that I received emails from
two moderators, telling me that I was....
I acknowledge that my initial email was a mistake.

However, I need to note recent public comments from the author of the
spatstat package:

    Your internet skills are pathetic.

    There is far too much bland "Shhhh.
    We *mustn't* offend anybody" content in current discourse.
    Tell it like it is!
    Ripley into people!
    If the recipient can't take the heat,
    he or she should get out of the kitchen!

    and not whinge about being offended.

https://stat.ethz.ch/pipermail/r-help/2019-July/463555.html

In short, everything I said was in accordance with the wishes of the
package authors...


From g||ted|||e2014 @end|ng |rom gm@||@com  Sat May  2 04:17:07 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sat, 2 May 2020 03:17:07 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot
In-Reply-To: <28f66e3f-f988-bf07-e07f-f3842e2086b1@sapo.pt>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
 <80280e72-a629-ad6a-5f54-f7691589e59d@sapo.pt>
 <28f66e3f-f988-bf07-e07f-f3842e2086b1@sapo.pt>
Message-ID: <CAC8ss317CgN19BZ9kZ6WJVZ3j6wJOvTeWeZdm+M-1yNwthRVnQ@mail.gmail.com>

Dear Rui Barradas,
Many thanks for your time.

Though I have not used ggplot before, I quickly installed all the required
packages and attempted your code. It produced the plot and solved the
initial problem of the wrong labels on x-axis.
But I noticed several other problems with the plot (some of which I have
circumvented by adjusting the script I displayed earlier). As I am not
quite familiar with ggplot, it was very difficult for me to interact with
your code. I decided to go back to my own code. I tried to use axis(1 ...,
labels=c()...) to assign Thu, Fri .... It worked for me before I got a
third response (very helpful) from the list.
Best regards
Ogbos

On Fri, May 1, 2020 at 6:35 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> I have just noticed that the facets' labels are on the right, in your
> code example they are on the left. Change the following:
>
>
> facet_grid(rows = vars(variable), switch = "y")
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 18:29 de 01/05/20, Rui Barradas escreveu:
> > Hello,
> >
> > If you don't mind a ggplot2 solution, here it is.
> > As usual with ggplot, it's better to have the data in long format so I
> > also load packages dplyr and tidyr. The reshaping accounts for half the
> > code, the code  for the graph itself is not very complicated.
> >
> >
> >
> > library(ggplot2)
> > library(scales)
> > library(dplyr)
> > library(tidyr)
> >
> > dta %>%
> >    select(datetime, B, BX, BZ) %>%
> >    rename(x = datetime, B1 = B, B2 = BX, B3 = BZ) %>%
> >    pivot_longer(
> >      cols = starts_with("B"),
> >      names_to = "variable",
> >      values_to = "value"
> >    ) %>%
> >    ggplot(aes(x, value)) +
> >    geom_line() +
> >    xlab("") + ylab("") +
> >    scale_x_datetime(
> >      breaks = seq(min(x), max(x), by = "days"),
> >      labels = time_format("%a")) +
> >    facet_grid(rows = vars(variable)) +
> >    theme_bw()
> >
> >
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 17:46 de 01/05/20, Ogbos Okike escreveu:
> >> Dear Contributors,
> >> I am trying to do a plot with multiple y-axis on a common x-axis. I have
> >> made some progress but I am having difficulties with datetime
> >> appearing on
> >> x-axis. Instead of as date, it appears as large numbers.
> >>
> >> Sample of my data:
> >> 78 09 28  0 6.7 -40.4 -3.5 2.3 -3.6 278036 5.8 612
> >> 78 09 28  1 5.7 7.3 -4.4 1.4 0.6 261169 4.9 623
> >> 78 09 28  2 5.6 -20.9 -3.6 3.1 -1.8 269323 4.8 625
> >> 78 09 28  3 5.8 -46.9 -3.4 0.2 -3.6 254221 4.7 626
> >> 78 09 28  4 6.2 -41.4 -3.9 1.2 -3.6 248567 4.2 618
> >> 78 09 28  5 6.1 -54.6 -3 1 -4.4 252669 4.4 629
> >> 78 09 28  6 NA NA NA NA NA 220480 4.3 606
> >> 78 09 28  7 7.1 26 -4.8 3.6 2.9 216887 4.4 613
> >> 78 09 28  8 7.1 -21.4 -4.7 3.5 -2.3 249928 4 643
> >> 78 09 28  9 7 -13.6 -2.7 5.2 -1.4 268806 4 656
> >> 78 09 28  10 6.6 -18.5 -3.7 3.6 -1.7 285608 4.2 651
> >> 78 09 28  11 6.7 -6.6 -2.1 4.7 -0.6 280446 4.2 661
> >> 78 09 28  12 6.7 -29.7 -1.9 4 -2.5 281928 4.2 659
> >> 78 09 28  13 6.2 -8.7 -2.1 4.7 -0.8 273465 4.4 651
> >> 78 09 28  14 6.1 31.8 -4.7 0.9 3 212374 4.2 631
> >> 78 09 28  15 5.4 9.3 -4.5 -0.9 0.8 219662 3.9 636
> >> 78 09 28  16 4.9 6.3 -4.5 -0.5 0.5 220610 3.8 628
> >> 78 09 28  17 4.7 4.5 -4.3 -0.6 0.3 214432 3.7 628
> >> 78 09 28  18 4.9 3.1 -4.5 0.3 0.2 202199 3.5 623
> >> 78 09 28  19 5 -13 -4.6 -0.3 -1.1 192859 3.2 619
> >> 78 09 28  20 5.1 -26 -3.5 2 -2 193868 3.1 627
> >> 78 09 28  21 10.1 -37.5 -5.1 4.9 -5.4 284122 5.9 683
> >> 78 09 28  22 8.4 -7.3 -3.6 5.5 -0.8 367499 6.2 694
> >> 78 09 28  23 8.2 -17.7 -4.7 4.4 -2.1 346644 4.9 689
> >> 78 09 29  0 8 6.3 -4.3 5.8 0.8 269569 4.7 708
> >> 78 09 29  1 8.6 35 -3 6.2 4.8 187132 3.2 709
> >> 78 09 29  2 8.1 24.8 -4.1 6 3.3 166644 3.5 689
> >> 78 09 29  3 15.9 29.6 -2.4 9.6 5.6 720902 6.6 866
> >> 78 09 29  4 14.9 18.9 -2.8 13.8 4.8 1587324 8.1 912
> >> 78 09 29  5 14.1 8.2 -10.2 8.3 1.9 1509336 8.1 871
> >> 78 09 29  6 15.6 -2.9 -6.1 11.5 -0.7 968890 7.8 878
> >> 78 09 29  7 19.6 -49.7 1.2 12.1 -14.4 574978 5.4 906
> >> 78 09 29  8 23.5 -44.2 -1.4 11.7 -11.5 287721 4.1 865
> >> 78 09 29  9 25.3 -71.8 -2.4 7.5 -23.9 58521 5.1 859
> >> 78 09 29  10 24.8 -63.1 -2.3 10.9 -22 74956 1.5 821
> >> 78 09 29  11 23.8 -50 -2 15.1 -18.1 64510 3 807
> >> 78 09 29  12 21.4 -34.3 0.1 17.6 -12 50247 2.3 795
> >> 78 09 29  13 18.4 -20 -2 17 -6.3 59939 2.9 802
> >> 78 09 29  14 16.4 -13.9 -2.8 15.6 -3.9 56499 1.4 799
> >> 78 09 29  15 15.3 -7.9 -2 14.9 -2.1 49346 1.3 759
> >> 78 09 29  16 14.1 -0.5 -2.9 13.8 -0.1 57732 0.5 730
> >> 78 09 29  17 12.6 14.7 -3.1 11.5 3.1 44351 1.7 722
> >> 78 09 29  18 11.2 74.3 0.6 2.9 10.4 68224 0.4 712
> >> 78 09 29  19 10.5 60.5 3.2 4 9.1 48082 0.2 706
> >> 78 09 29  20 10 54.7 3.9 4.2 8.1 56404 0.2 710
> >> 78 09 29  21 9.3 61.1 3.4 2.9 8.1 68824 0.2 694
> >> 78 09 29  22 8.6 64.4 2.6 2.7 7.8 44579 0.2 683
> >> 78 09 29  23 7.9 63.3 2 3 7.1 39760 0.2 661
> >> 78 09 30  0 7.3 59.8 2.3 2.8 6.3 NA NA NA
> >> 78 09 30  1 6.6 65.6 1.7 2.1 6 57382 0.5 664
> >> 78 09 30  2 6 70.8 1.3 1.4 5.5 63540 0.5 654
> >> 78 09 30  3 4.4 45.1 1 2.7 2.9 60856 2.5 635
> >> 78 09 30  4 3.6 -28 -1 -0.3 -0.6 80967 1.9 633
> >> 78 09 30  5 4 -47.6 -2.2 -0.1 -2.4 48391 1.2 642
> >> 78 09 30  6 4.1 -28.9 -3.1 -1.4 -1.9 45012 1.1 643
> >> 78 09 30  7 4.2 -42.7 -2.7 1.2 -2.7 45562 1.1 633
> >>   and part of my code is:
> >> Sys.setenv( TZ="GMT" )
> >> dta<-read.table("IFEDIFIG1D",col.names=c("year", "month", "day",
> >> "hour","B","LAT","BX","BZ","BY","SWT","SWD","SW"))
> >> dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
> >> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
> >> month,day,hour,0,0)))
> >> x =  dta$datetime
> >> B=dta$B
> >> LAT=dta$LAT
> >> BX=dta$BX
> >> BZ=dta$BZ
> >>   par(mfcol = c(3, 1), mar = numeric(4), oma = c(4, 4, 1, 1),
> >>      mgp = c(2, 0.5, 0))#0.5 here controls x-axis margins
> >> plot(x, B,type="l", axes = FALSE)
> >> abline(v=-3)
> >> axis(2L)
> >> plot(x, LAT,type="l", axes = FALSE)
> >> abline(v=-3)
> >> axis(2L)
> >>   plot(x, BX, type="l",axes = FALSE)
> >> abline(v=-3)
> >> axis(1L)
> >> axis(2L)
> >> mtext("A1", side = 1, outer = TRUE, line = 2.2)
> >> mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
> >> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
> >> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)
> >>
> >> If I do a simple plot of x and B, the x-axis will be fine, appearing as
> >> Thu, Fri, Sat, Sun.
> >> Please let me know what I am doing wrong with the multiple plot code
> >> above.
> >> I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
> >>
> >> I am most grateful for your kind response.
> >>
> >> Warmest regards
> >> Ogbos
> >>
> >>     [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Sat May  2 04:36:32 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Sat, 2 May 2020 03:36:32 +0100
Subject: [R] Datetime misrepresented on x-axis of a multiple plot: Problem
 Fixd
In-Reply-To: <CA+8X3fWGCtgq7wHC0AvDmgfzbJ4MNgDv62t2ZMT9r6iJf9C9Yg@mail.gmail.com>
References: <CAC8ss33yaDtFY-6+8dFD8jUxks7jtu9HN6n13_CTRKgjHP8vVw@mail.gmail.com>
 <CA+8X3fWGCtgq7wHC0AvDmgfzbJ4MNgDv62t2ZMT9r6iJf9C9Yg@mail.gmail.com>
Message-ID: <CAC8ss31sf+-3pD2F_jBpu5j2G+hq97gc70SMeDtjmmU8Nz_6GQ@mail.gmail.com>

Dear Jim and Rolf Turner,
This is great!!!!!!!!!!!!!!!!
The labels on x-axis are now correctly displayed. Jim, your singular code:
axis.POSIXct(1,dta$datetime,format="%a")
made all the required adjustment, making life quite easy for me.

The abline commands are indeed meant to draw a common vertical line across
the different panels. I have quickly achieved that with:
abline(v=as.POSIXct("1978-09-29 00:00:00"))

I have successfully added the rest of the data, displaying seven/more
variables on a single plot and using some common vertical lines to indicate
the variations in several solar wind plasma data/cosmic rays at Earth at
the time of coronal mass ejections from the sun. It makes an interesting
plot to those in our field.

Confusion or "misleading impressions" do not arise at all as the data used
as well as date plotted are usually online for easy verification by
experts/anyone.

Many thanks for all your contributions.

Warmest regards
Ogbos



On Fri, May 1, 2020 at 11:47 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Ogbos,
> The following code may get you close to what you want. I have used the
> names of the columns in "dta" as it is less confusing for me. I think
> you meant to request horizontal ablines as none appeared in the
> example. In order to get the axis tick label "Sun" you would have to
> increase the x axis limits beyond your data.
>
> plot(dta$datetime,dta$B,type="l",axes=FALSE)
> # this is out of range and doesn't appear
> abline(h=-3,col="red")
> axis(2)
> plot(dta$datetime,dta$LAT,type="l",axes=FALSE)
> abline(h=-3,col="red")
> axis(2)
> plot(dta$datetime,dta$BX,type="l",axes=FALSE)
> abline(h=-3,col="red")
> axis(2)
> axis.POSIXct(1,dta$datetime,format="%a")
> mtext("A1", side = 1, outer = TRUE, line = 2.2)
> mtext("B1", side = 2, outer = TRUE, line = 2.2,at =0.2)
> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.5)
> mtext("B2", side = 2, outer = TRUE, line = 2.2,at =0.8)
>
> Jim
>
> On Sat, May 2, 2020 at 2:47 AM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >
> > Dear Contributors,
> > I am trying to do a plot with multiple y-axis on a common x-axis. I have
> > made some progress but I am having difficulties with datetime appearing
> on
> > x-axis. Instead of as date, it appears as large numbers.
> > ...
> > If I do a simple plot of x and B, the x-axis will be fine, appearing as
> > Thu, Fri, Sat, Sun.
> > Please let me know what I am doing wrong with the multiple plot code
> above.
> > I want the same Thu, Fri, Sat, Sun to appear on the common x-axis.
> >
>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Sat May  2 18:43:11 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 2 May 2020 18:43:11 +0200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
Message-ID: <24237.41759.777920.646961@stat.math.ethz.ch>

>>>>> Abby Spurdle 
>>>>>     on Fri, 1 May 2020 17:21:32 +1200 writes:

    > I agree, the documentation gives the impression that stats::spline
    > would allow "monoH.FC".

You are right, this is a lapsus,  thank you for reporting it!

    > My guess is that stats::spline doesn't allow it because the function
    > is designed to return a list with x and y components. This doesn't
    > suit monotonic cubic Hermite splines because the function would also
    > need to return the slopes. Furthermore, additional functions may (or
    > may not) be required depending on what you want to do with x, y and
    > slope vectors.

Well, not quite.  AFAIR, the reasons I did not add the option for
spline()  were mostly

- using  splinefun() is uniformly more flexible than using spline()
  {you can always get an (x,y) coordinate list from
   splinefun()'s result later}

- splinefun() is using's R feature of "(non-trivial) closure", i.e, 
  it returns a *function* containing its own state

- convenience (less work, notably less maintenance burden)

Still, in spite of the above:  If somebody provides minimal patches
(keeping *elegant* code) to allow spline() to also allow method="monoH.FC",
I'd consider applying that.
For now, I'd rather amend the documentation (as Samuel proposes)

    > Note that my package kubik, provides a range of functions for working
    > with cubic Hermite splines.

That's interesting.  For years, I had wanted to add a bit more
functionality to R's builtin (but not widely known !!) package 'splines',
and just today a colleague asked me about spline interpolation
with general 2nd derivative boundary conditions

   s''(x_1) = s2_1,  s''(x_n) = s2_2

generalizing the usual ( "natural" ) boundary conditions

   s''(x_1) = 0,     s''(x_n) = 0

and I'd "hope" it should really only need a few lines of R code
to generalize from natural interpolating splines to such very
slightly more general ones.
(I don't see very quickly how to do this with 'kubik' either,
but I've only looked for 5 minutes).

---

Note that our CRAN package 'cobs'  is really for
(robust, namely quantile) regression splines where the user can also
ask for monotonicity, convexity, concavity of s() and for bound
constraints in different regions
(and the implementation is fast for larger 'n', thanks to using
 sparse matrices)... which seems partly a generalization of  'kubik'
as it's not only (just) for interpolation and still allows many
shape and bound constraints.
 
> citation("cobs")

To cite the cobs package in publications use:

  Pin T. Ng and Martin Maechler (2020). COBS -- Constrained B-splines (Sparse matrix
  based). R package version 1.3-4. URL https://CRAN.R-project.org/package=cobs

Ng P, Maechler M (2007). ?A Fast and Efficient Implementation of Qualitatively
Constrained Quantile Smoothing Splines.? _Statistical Modelling_, *7*(4),
 315-328. <URL:http://smj.sagepub.com/content/7/4/315.abstract>.


From you@r|@|@nou@ @end|ng |rom gm@||@com  Sun May  3 01:30:49 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Sat, 2 May 2020 19:30:49 -0400
Subject: [R] possible issue with scatterplot function in car package
Message-ID: <CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>

library (car)

 aa <- data.frame(x=c(2,  5, 6, 7, 8),
+  y=c(5,  10, 9, 12, 11),
+ ch=c("N",  "Q", "R", "S", "T"),
+ stringsAsFactors=FALSE)

scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)

Both x and y boxplots are correct
and in particular the median of the x box is at 6 which is confirmed

> median(aa$x)
[1] 6

Now I do only one addition to the scatterplot: I add xlim
> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
FALSE,regLine=F,xlim=c(0,8))

This causes the boxplot on x-axis to be in error:
1) the lower whisker starts now from zero
2) the median is between 4 and 6 and no longer at 6 as before

> sessionInfo()
R version 3.6.3 (2020-02-29)
 [1] car_3.0-7

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun May  3 01:51:51 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 3 May 2020 11:51:51 +1200
Subject: [R] [FORGED] possible issue with scatterplot function in car
 package
In-Reply-To: <CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
References: <CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
Message-ID: <a87259ca-a79d-cd4e-4ec2-6f4ad59cc207@auckland.ac.nz>


On 3/05/20 11:30 am, Yousri Fanous wrote:

> library (car)
> 
>   aa <- data.frame(x=c(2,  5, 6, 7, 8),
> +  y=c(5,  10, 9, 12, 11),
> + ch=c("N",  "Q", "R", "S", "T"),
> + stringsAsFactors=FALSE)
> 
> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
> 
> Both x and y boxplots are correct
> and in particular the median of the x box is at 6 which is confirmed
> 
>> median(aa$x)
> [1] 6
> 
> Now I do only one addition to the scatterplot: I add xlim
>> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F,xlim=c(0,8))
> 
> This causes the boxplot on x-axis to be in error:
> 1) the lower whisker starts now from zero
> 2) the median is between 4 and 6 and no longer at 6 as before
> 
>> sessionInfo()
> R version 3.6.3 (2020-02-29)
>   [1] car_3.0-7

(1) Please present data using dput(); this makes life a lot easier for 
your respondents.  (See posting guide.)

(2) Please do not post in html.  (See posting guide.)

(3) I agree that this looks like a bug.

(4) Email about issues like this should be sent to the maintainer of the 
package in question (see maintainer("car")) and not to r-help.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From rcoppock @end|ng |rom cox@net  Sun May  3 04:18:53 2020
From: rcoppock @end|ng |rom cox@net (Roger Coppock)
Date: Sat, 2 May 2020 19:18:53 -0700
Subject: [R] CRAN library down? - UPDATE
In-Reply-To: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
References: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
Message-ID: <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>

Recently a message appears when I try to view the index of the CRAN library.

Warning: unable to access index for repository http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0:
  cannot open URL 'http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0/PACKAGES'

Does the CRAN library have the Coronavirus?

- -  Roger Coppock (rcoppock at cox.net)

> On May 1, 2020, at 9:24 AM, Roger Coppock <rcoppock at cox.net> wrote:
> 
> After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded libraries.  Also, the package installer can not contact CRAN libraries either for binaries or sources, to replace the missing loaded libraries.  The package installer can contact "BioConductor", however.  I am now specifically looking for "HURDAT" and "lmtest", which were on CRAN but not "BioConductor".
> 
> - -  Roger Coppock (rcoppock at cox.net)


From cry@n @end|ng |rom b|ngh@mton@edu  Sun May  3 04:49:05 2020
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Sat, 2 May 2020 22:49:05 -0400
Subject: [R] CRAN library down? - UPDATE
In-Reply-To: <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>
References: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
 <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>
Message-ID: <4bb01384-4b0e-ba1a-f7a4-00ef6126e0b0@binghamton.edu>

The message at that URL reads:

CRAN mirror restricted to UC Berkeley

The CRAN mirror at UC Berkeley's College of Natural Resources is no
longer available to off campus users and has been removed from the CRAN
mirror list. The load on our server was too much.

Berkeley folks can continue to access the mirror from on-campus networks
or from within the VPN.

The rest of the R community: Sorry, but you'll have to please choose a
different CRAN mirror.

--Chris Ryan


Roger Coppock wrote:
> Recently a message appears when I try to view the index of the CRAN library.
> 
> Warning: unable to access index for repository http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0:
>   cannot open URL 'http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0/PACKAGES'
> 
> Does the CRAN library have the Coronavirus?
> 
> - -  Roger Coppock (rcoppock at cox.net)
> 
>> On May 1, 2020, at 9:24 AM, Roger Coppock <rcoppock at cox.net> wrote:
>>
>> After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded libraries.  Also, the package installer can not contact CRAN libraries either for binaries or sources, to replace the missing loaded libraries.  The package installer can contact "BioConductor", however.  I am now specifically looking for "HURDAT" and "lmtest", which were on CRAN but not "BioConductor".
>>
>> - -  Roger Coppock (rcoppock at cox.net)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May  3 05:10:09 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 02 May 2020 20:10:09 -0700
Subject: [R] CRAN library down? - UPDATE
In-Reply-To: <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>
References: <E17F07D7-3E85-4737-9DB6-EB5C511E619B@cox.net>
 <5AC5E4D9-2D7A-49FD-AC74-475C04BF70A1@cox.net>
Message-ID: <F5F88764-1703-4670-8AD0-7E6A0A7C31CD@dcn.davis.ca.us>

a) You will help yourself and those who you attempt to communicate with if you learn the terminology correctly as described in the RcInstallation and Administration manual (e.g. RShowDoc("R-admin") or https://cran.r-project.org/doc/manuals/r-release/R-admin.html):

  1) Packages are collections of functions and documentation.
  2) Libraries are the directories _on our computers_ in which we keep packages that we want convenient access to.
  3) Repositories are directories of packages on internet servers from which we typically retrieve packages that we want to keep in our libraries.

b) There are many web servers that mirror the contents of the CRAN repository. They are operated as volunteer contributions to the R community. You have referenced one of them in your error message.

c) If you use your web browser to visit the URL you reported in your post then there is a message returned that explains what happened and offers a suggestion as to next steps you should take.

On May 2, 2020 7:18:53 PM PDT, Roger Coppock <rcoppock at cox.net> wrote:
>Recently a message appears when I try to view the index of the CRAN
>library.
>
>Warning: unable to access index for repository
>http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0:
>cannot open URL
>'http://cran.cnr.Berkeley.edu/bin/macosx/contrib/4.0/PACKAGES'
>
>Does the CRAN library have the Coronavirus?
>
>- -  Roger Coppock (rcoppock at cox.net)
>
>> On May 1, 2020, at 9:24 AM, Roger Coppock <rcoppock at cox.net> wrote:
>> 
>> After I changed to Version R version 4.0.0 (2020-04-24) -- "Arbor
>Day" on my MacBook running Mac OS 10.13.6, I lost all my loaded
>libraries.  Also, the package installer can not contact CRAN libraries
>either for binaries or sources, to replace the missing loaded
>libraries.  The package installer can contact "BioConductor", however. 
>I am now specifically looking for "HURDAT" and "lmtest", which were on
>CRAN but not "BioConductor".
>> 
>> - -  Roger Coppock (rcoppock at cox.net)
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From j|ox @end|ng |rom mcm@@ter@c@  Sun May  3 05:47:21 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sun, 3 May 2020 03:47:21 +0000
Subject: [R] possible issue with scatterplot function in car package
In-Reply-To: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
References: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
Message-ID: <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>


Dear Yousri,

Yes, this is clearly a bug, and almost surely a long-standing one. We'll fix it in the next release of the car package.

BTW, stringsAsFactors defaults to FALSE in R 4.0.0 (and you don't use the ch variable in the example). Also, although it has no bearing on the bug, I'd generally prefer

	scatterplot(y ~ x, data=aa, smooth=FALSE, grid=FALSE, 
            	frame=FALSE, regLine=FALSE, xlim=c(0, 8))

Thank you for the bug report,
 John

  -----------------------------
  John Fox, Professor Emeritus
  McMaster University
  Hamilton, Ontario, Canada
  Web: http::/socserv.mcmaster.ca/jfox

> On May 2, 2020, at 7:30 PM, Yousri Fanous <yousri.fanous at gmail.com> wrote:
> 
> library (car)
> 
> aa <- data.frame(x=c(2,  5, 6, 7, 8),
> +  y=c(5,  10, 9, 12, 11),
> + ch=c("N",  "Q", "R", "S", "T"),
> + stringsAsFactors=FALSE)
> 
> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
> 
> Both x and y boxplots are correct
> and in particular the median of the x box is at 6 which is confirmed
> 
>> median(aa$x)
> [1] 6
> 
> Now I do only one addition to the scatterplot: I add xlim
>> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F,xlim=c(0,8))
> 
> This causes the boxplot on x-axis to be in error:
> 1) the lower whisker starts now from zero
> 2) the median is between 4 and 6 and no longer at 6 as before
> 
>> sessionInfo()
> R version 3.6.3 (2020-02-29)
> [1] car_3.0-7
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Sun May  3 06:15:17 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 3 May 2020 16:15:17 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <24237.41759.777920.646961@stat.math.ethz.ch>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
Message-ID: <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>

> and just today a colleague asked me about spline interpolation
> with general 2nd derivative boundary conditions
>    s''(x_1) = s2_1,  s''(x_n) = s2_2

It should possible via cubic Hermite splines.
A nontrivial design decision in my package was the computation of
slopes at the endpoints.
(Something which I got wrong, twice...)

My guess is that I could write a function in about 60 to 90 minutes,
including all the testing and calculus.

However, I need to note two things:
(1) Cubic Hermite splines do not have a continuous second derivative.
(2) Specifying the second derivatives (at the endpoints) would prevent
the user from specifying the first derivatives (aka the slopes).

Could you please confirm if the function would still be of interest...?

And completely diverging...
> it returns a *function* containing its own state

I use function objects extensively.
However, I haven't been able to find a definitive guide to terminology.
The word "closure" appears to have a lisp origin, but it usage in R is
a bit grey.

Many of my functions have attributes.
However, I recognize that the use the function environments is more
popular, and has the advantage that the user can take advantage of
lexical scoping, but has the disadvantage that copying function
objects can have unexpected results.

Recently, I've been using the terms "Self-Referencing Function
Objects" and "Functions Bundled with Data", but was wondering if these
terms are sub-optimal...?


From @dr|@n @end|ng |rom tr@p|ett|@org  Sun May  3 10:06:13 2020
From: @dr|@n @end|ng |rom tr@p|ett|@org (Adrian Trapletti)
Date: Sun, 3 May 2020 10:06:13 +0200
Subject: [R] R 4.0.0 with Intel MKL for Windows
Message-ID: <CAFmikf17XhpXNCCy9Xv+gcmSN98=4X85OwXY-9K5c9Xzechshw@mail.gmail.com>

For Windows users, some instructions how to use R 4.0.0 with Intel MKL:

https://linkedin.com/pulse/r-400-intel-mkl-windows-adrian-trapletti

Best Regards
Adrian

Adrian Trapletti

Steinstrasse 9b, 8610 Uster, Switzerland
P +41 44 994 56 30  |  M +41 79 103 71 31
adrian at trapletti.org  |  www.trapletti.org


From you@r|@|@nou@ @end|ng |rom gm@||@com  Sun May  3 10:37:52 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Sun, 3 May 2020 04:37:52 -0400
Subject: [R] possible issue with scatterplot function in car package
In-Reply-To: <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>
References: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
 <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>
Message-ID: <CADsEwSdxFMTpyoHaYm3LiW4aMyxihAvnuse6VNCYVYCt8hg4mQ@mail.gmail.com>

Thank you Professor John for your answer.

As you rightly said I am not using the ch in my example report as it has no
bearing to the issue.
However it is the ch that led me to find the issue.
I was trying to label each point with its corresponding aa$ch value.
I used this code:

scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
text(aa$x,aa$y, labels=aa$ch,font=1 ,cex=.9,pos=3)

The annotation was correct for 4 points but not for the (2,5) point.
I figured it is because it is close to the margin of the plot hence as a
quick solution I modified xlim to shift the point away from the margin.
This worked for the annotation but eventually led to the issue I described.

Thank you so much for your time

Yousri Fanous

Software Developer
IBM CANADA

On Sat, May 2, 2020 at 11:47 PM Fox, John <jfox at mcmaster.ca> wrote:

>
> Dear Yousri,
>
> Yes, this is clearly a bug, and almost surely a long-standing one. We'll
> fix it in the next release of the car package.
>
> BTW, stringsAsFactors defaults to FALSE in R 4.0.0 (and you don't use the
> ch variable in the example). Also, although it has no bearing on the bug,
> I'd generally prefer
>
>         scatterplot(y ~ x, data=aa, smooth=FALSE, grid=FALSE,
>                 frame=FALSE, regLine=FALSE, xlim=c(0, 8))
>
> Thank you for the bug report,
>  John
>
>   -----------------------------
>   John Fox, Professor Emeritus
>   McMaster University
>   Hamilton, Ontario, Canada
>   Web: http::/socserv.mcmaster.ca/jfox
>
> > On May 2, 2020, at 7:30 PM, Yousri Fanous <yousri.fanous at gmail.com>
> wrote:
> >
> > library (car)
> >
> > aa <- data.frame(x=c(2,  5, 6, 7, 8),
> > +  y=c(5,  10, 9, 12, 11),
> > + ch=c("N",  "Q", "R", "S", "T"),
> > + stringsAsFactors=FALSE)
> >
> > scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F)
> >
> > Both x and y boxplots are correct
> > and in particular the median of the x box is at 6 which is confirmed
> >
> >> median(aa$x)
> > [1] 6
> >
> > Now I do only one addition to the scatterplot: I add xlim
> >> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> > FALSE,regLine=F,xlim=c(0,8))
> >
> > This causes the boxplot on x-axis to be in error:
> > 1) the lower whisker starts now from zero
> > 2) the median is between 4 and 6 and no longer at 6 as before
> >
> >> sessionInfo()
> > R version 3.6.3 (2020-02-29)
> > [1] car_3.0-7
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Sun May  3 11:11:00 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 3 May 2020 21:11:00 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
Message-ID: <CAB8pepwv+xPbnHFpv=dxdd5hihe7w-QpWTf_0FmXqRxUD01x1Q@mail.gmail.com>

I just realized that note (2) is not completely correct.
It would be possible to specify both the first and second derivatives
at the endpoints.
However, that would require subdivision of the outermost spline
segments, which increases the complexity of the algorithm.


From @purd|e@@ @end|ng |rom gm@||@com  Sun May  3 12:19:24 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 3 May 2020 22:19:24 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
Message-ID: <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>

Hi Mark,

The article is good.
However, there's still some grey areas.

The documentation for base::typeof equates a closure with a function.
However, the article defines a closure as a function together with an
environment.
A very minor difference I know, but it creates the problem that the
word closure is used inconsistently.
And that's without even getting into "frames".

Equating a closure with a function (only), is perhaps misleading...???

Also, re-iterating I use function objects with attributes.
(I prefer this approach, because I can make copies of function
objects, and if necessary modify them).
My guess is that doesn't meet the definition of an R closure (if you
ignore the environment)...???, and it's unclear whether it meets the
definition of a closure, more generally...???

So, I've still got the same problem, of how to refer to functions that
have *either* attributes *or* environments, containing data.
Maybe I should just stick to "Self-Referencing Function Objects" and
"Functions Bundled with Data"...???

One last thing, the last time I read S4 documentation, I couldn't tell
if it was possible to have S4-based function objects, and if so, could
the body of the S4-based function object (while being called) access
it's own slots...???


On Sun, May 3, 2020 at 4:27 PM Mark Leeds <markleeds2 at gmail.com> wrote:
>
> Abby: Here is an article on environments/closures which might be useful to you. I was reviewing environments recently and it
> was a clear explanation of how environments/closures work in R. Even though it's from 2000, I'm pretty certain that everything
> said in it still holds.


From m@rk|eed@2 @end|ng |rom gm@||@com  Sun May  3 16:22:08 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Sun, 3 May 2020 10:22:08 -0400
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
Message-ID: <CAHz+bWZi28R6YTJc0DgmJGxFBHZ2DBv2Oa4zqhFvB6iFun_zBw@mail.gmail.com>

Hi Abby: your questions are good but I can't help because I don't need to
actually use them
to that level of detail. I only have to understand it in its generalities.
Hopefully, Martin
or one of the other R-core people or guRus can answer your questions in a
definitive and clear
way.

But, as far as the term closure, I think that they referring to the
environment that
a function looks in after its looks in its own environment. So, if you
defined
a function say, foo, in the global environment, then the foo's closure is
the global
environment because, when finding the values of variables,  foo will first
look in its
environment  and then its closure. So, closure is just an environment
that's the parent
of a function's environment. It answers the question of where a function
looks next
when it can't find the values of some of the variables that it needs to
evaluate.

I'd also love to hear an explanation of "frame' because that term has
always confused me
I think ( emphasis on think ), when a function, foo,  gets called, it
creates a new
environment to evaluate its arguments and this new  environment of foo is
called a "frame".
But where is it and what is its parent ? Is "frame"  just a fancy term for
the
function's own environment ?  or is  the function's own environment the
parent of its evaluation
frame I'm not sure.

I have other documents that may help ( I recently did a document gathering
but I only read
the one I sent you )  but I think it's better to wait to see if anyone can
help here first. Then,
if not, I can send you some  other things. Oh, the slot question was also
interesting but I've never
used S4. Also, this list has gotten thinner over the years so another place
to ask is of course
stackoverflow.

























On Sun, May 3, 2020 at 6:19 AM Abby Spurdle <spurdle.a at gmail.com> wrote:

> Hi Mark,
>
> The article is good.
> However, there's still some grey areas.
>
> The documentation for base::typeof equates a closure with a function.
> However, the article defines a closure as a function together with an
> environment.
> A very minor difference I know, but it creates the problem that the
> word closure is used inconsistently.
> And that's without even getting into "frames".
>
> Equating a closure with a function (only), is perhaps misleading...???
>
> Also, re-iterating I use function objects with attributes.
> (I prefer this approach, because I can make copies of function
> objects, and if necessary modify them).
> My guess is that doesn't meet the definition of an R closure (if you
> ignore the environment)...???, and it's unclear whether it meets the
> definition of a closure, more generally...???
>
> So, I've still got the same problem, of how to refer to functions that
> have *either* attributes *or* environments, containing data.
> Maybe I should just stick to "Self-Referencing Function Objects" and
> "Functions Bundled with Data"...???
>
> One last thing, the last time I read S4 documentation, I couldn't tell
> if it was possible to have S4-based function objects, and if so, could
> the body of the S4-based function object (while being called) access
> it's own slots...???
>
>
> On Sun, May 3, 2020 at 4:27 PM Mark Leeds <markleeds2 at gmail.com> wrote:
> >
> > Abby: Here is an article on environments/closures which might be useful
> to you. I was reviewing environments recently and it
> > was a clear explanation of how environments/closures work in R. Even
> though it's from 2000, I'm pretty certain that everything
> > said in it still holds.
>

	[[alternative HTML version deleted]]


From t|omby @end|ng |rom m@||@@mu@edu  Sun May  3 07:39:58 2020
From: t|omby @end|ng |rom m@||@@mu@edu (Fomby, Tom)
Date: Sun, 3 May 2020 05:39:58 +0000
Subject: [R] Question about "sample" function and inconsistent results I am
 getting across machines.
Message-ID: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>

Please consider the following code:

set.seed(1)

train.index = sample(181,150)
head(train.index)
# [1]  49  67 103 162  36 159  Result from my ASUS computer
#
# [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer

In both cases, version 3.6.3 of R are being used.

In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.

Thank you for considering my question.

Sincerely,

Tom Fomby

Professor of Economics

SMU

Dallas, TX 75275

tfomby at smu.edu


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun May  3 21:32:54 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 3 May 2020 15:32:54 -0400
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
Message-ID: <d100571a-c498-6574-2232-17b685336ad4@gmail.com>

On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
> Please consider the following code:
> 
> set.seed(1)
> 
> train.index = sample(181,150)
> head(train.index)
> # [1]  49  67 103 162  36 159  Result from my ASUS computer
> #
> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
> 
> In both cases, version 3.6.3 of R are being used.
> 
> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.
> 
> Thank you for considering my question.

Likely some of you are storing and restoring workspaces, and have been 
doing so for a long time.  If you type

RNGkind()

what you should see is

[1] "Mersenne-Twister" "Inversion"        "Rejection"

but if the .Random.seed is restored from an old session, you might see

[1] "Mersenne-Twister" "Inversion"        "Rounding"

The latter uses the buggy version of sample().  Those users should run

RNGkind(sample.kind = "Rejection")

to start using the corrected sampling algorithm.  (The default was 
changed in R 3.6.0, but if you saved your seed from a previous version, 
you'd get the old sampler).

They should also stop reloading old workspaces, but that's another 
discussion.

Duncan Murdoch


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May  3 21:33:32 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sun, 03 May 2020 12:33:32 -0700
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
Message-ID: <4A5DEF2D-099F-4846-85E8-DE905517021B@dcn.davis.ca.us>

It is a lot easier from this side of the conversation to view skeptically the claim that all of these installations of R are using the same version than that the software seed has started behaving randomly within the same version of R.

On May 2, 2020 10:39:58 PM PDT, "Fomby, Tom" <tfomby at mail.smu.edu> wrote:
>Please consider the following code:
>
>set.seed(1)
>
>train.index = sample(181,150)
>head(train.index)
># [1]  49  67 103 162  36 159  Result from my ASUS computer
>#
># [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>
>In both cases, version 3.6.3 of R are being used.
>
>In addition, of the 20 students in my Predictive Analytics class, 14
>got the first result while 6 got the latter result.  These results do
>not seem to be specific to MAC (OS) versus PC (Windows).  In several
>cases, students using 3.6.3 got differing results.  This makes grading
>of homework challenging not knowing which partitions of the data are
>being used by the student.
>
>Thank you for considering my question.
>
>Sincerely,
>
>Tom Fomby
>
>Professor of Economics
>
>SMU
>
>Dallas, TX 75275
>
>tfomby at smu.edu
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun May  3 21:36:50 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 3 May 2020 15:36:50 -0400
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
Message-ID: <266325cd-a7ee-1e69-3693-f81e6802c464@gmail.com>

I just tried both versions, and it's the ASUS that's using the buggy old 
algorithm.

Duncan Murdoch

On 03/05/2020 3:32 p.m., Duncan Murdoch wrote:
> On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
>> Please consider the following code:
>>
>> set.seed(1)
>>
>> train.index = sample(181,150)
>> head(train.index)
>> # [1]  49  67 103 162  36 159  Result from my ASUS computer
>> #
>> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>>
>> In both cases, version 3.6.3 of R are being used.
>>
>> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.
>>
>> Thank you for considering my question.
> 
> Likely some of you are storing and restoring workspaces, and have been
> doing so for a long time.  If you type
> 
> RNGkind()
> 
> what you should see is
> 
> [1] "Mersenne-Twister" "Inversion"        "Rejection"
> 
> but if the .Random.seed is restored from an old session, you might see
> 
> [1] "Mersenne-Twister" "Inversion"        "Rounding"
> 
> The latter uses the buggy version of sample().  Those users should run
> 
> RNGkind(sample.kind = "Rejection")
> 
> to start using the corrected sampling algorithm.  (The default was
> changed in R 3.6.0, but if you saved your seed from a previous version,
> you'd get the old sampler).
> 
> They should also stop reloading old workspaces, but that's another
> discussion.
> 
> Duncan Murdoch
>


From |br3524 @end|ng |rom gm@||@com  Sat May  2 18:44:45 2020
From: |br3524 @end|ng |rom gm@||@com (Abraham Rammaha)
Date: Sat, 2 May 2020 11:44:45 -0500
Subject: [R] Is it possible to reopen event in Summer of St. Louis. I am
 interested!
Message-ID: <CAOSJU8aOkRaCD0u0q46ZCQi4rd+D_1uyoPzQ5YvTkOR1OyqoNg@mail.gmail.com>

This is Ibrahim Rammaha. I am not a SLU student but interested in useR!
2021 in St. Louis. How can I register and volunteer/help with event? Can it
be a physical and free virtual conference? I think coronavirus shouldn't
affect the conference if it's online conference

	[[alternative HTML version deleted]]


From m@rk|eed@2 @end|ng |rom gm@||@com  Sun May  3 06:27:35 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Sun, 3 May 2020 00:27:35 -0400
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
Message-ID: <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>

Abby: Here is an article on environments/closures which might be useful to
you. I was reviewing environments recently and it
was a clear explanation of how environments/closures work in R. Even though
it's from 2000, I'm pretty certain that everything
said in it still holds.











On Sun, May 3, 2020 at 12:16 AM Abby Spurdle <spurdle.a at gmail.com> wrote:

> > and just today a colleague asked me about spline interpolation
> > with general 2nd derivative boundary conditions
> >    s''(x_1) = s2_1,  s''(x_n) = s2_2
>
> It should possible via cubic Hermite splines.
> A nontrivial design decision in my package was the computation of
> slopes at the endpoints.
> (Something which I got wrong, twice...)
>
> My guess is that I could write a function in about 60 to 90 minutes,
> including all the testing and calculus.
>
> However, I need to note two things:
> (1) Cubic Hermite splines do not have a continuous second derivative.
> (2) Specifying the second derivatives (at the endpoints) would prevent
> the user from specifying the first derivatives (aka the slopes).
>
> Could you please confirm if the function would still be of interest...?
>
> And completely diverging...
> > it returns a *function* containing its own state
>
> I use function objects extensively.
> However, I haven't been able to find a definitive guide to terminology.
> The word "closure" appears to have a lisp origin, but it usage in R is
> a bit grey.
>
> Many of my functions have attributes.
> However, I recognize that the use the function environments is more
> popular, and has the advantage that the user can take advantage of
> lexical scoping, but has the disadvantage that copying function
> objects can have unexpected results.
>
> Recently, I've been using the terms "Self-Referencing Function
> Objects" and "Functions Bundled with Data", but was wondering if these
> terms are sub-optimal...?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: ih_gent_lexical.pdf
Type: application/pdf
Size: 220482 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200503/fe569c8d/attachment.pdf>

From t|omby @end|ng |rom m@||@@mu@edu  Sun May  3 21:43:05 2020
From: t|omby @end|ng |rom m@||@@mu@edu (Fomby, Tom)
Date: Sun, 3 May 2020 19:43:05 +0000
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>,
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
Message-ID: <6355fc0de4614a4493ac5c64d4803c29@mail.smu.edu>


Dear Duncan,

OK, I will certainly ask my students to download the most recent version of Basic R at the first of each semester and, just to be safe, include the RNGkind(sample.kind="Rejection") command before the students get started on the data partitioning part of their exercise using the sample function.

By the way, how is it that one can take a membership in the R community so as to provide support for volunteers like yourself.

Thank you,

Tom Fomby

Department of Economics

SMU

Dallas, TX 75275


________________________________
From: Duncan Murdoch <murdoch.duncan at gmail.com>
Sent: Sunday, May 3, 2020 2:32 PM
To: Fomby, Tom; r-help at R-project.org
Subject: Re: [R] Question about "sample" function and inconsistent results I am getting across machines.

On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
> Please consider the following code:
>
> set.seed(1)
>
> train.index = sample(181,150)
> head(train.index)
> # [1]  49  67 103 162  36 159  Result from my ASUS computer
> #
> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>
> In both cases, version 3.6.3 of R are being used.
>
> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.
>
> Thank you for considering my question.

Likely some of you are storing and restoring workspaces, and have been
doing so for a long time.  If you type

RNGkind()

what you should see is

[1] "Mersenne-Twister" "Inversion"        "Rejection"

but if the .Random.seed is restored from an old session, you might see

[1] "Mersenne-Twister" "Inversion"        "Rounding"

The latter uses the buggy version of sample().  Those users should run

RNGkind(sample.kind = "Rejection")

to start using the corrected sampling algorithm.  (The default was
changed in R 3.6.0, but if you saved your seed from a previous version,
you'd get the old sampler).

They should also stop reloading old workspaces, but that's another
discussion.

Duncan Murdoch

	[[alternative HTML version deleted]]


From t|omby @end|ng |rom m@||@@mu@edu  Sun May  3 21:49:55 2020
From: t|omby @end|ng |rom m@||@@mu@edu (Fomby, Tom)
Date: Sun, 3 May 2020 19:49:55 +0000
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <266325cd-a7ee-1e69-3693-f81e6802c464@gmail.com>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>,
 <266325cd-a7ee-1e69-3693-f81e6802c464@gmail.com>
Message-ID: <b872ca4d6ef04bbd92e446644d80a895@mail.smu.edu>

Thank you so much, Duncan.

Out of the 20 students in my class, evidently 6 out of the 20 have been using the buggy version of sample().  I am so appreciative that you have helped me get a grip on things.  I was tired of having two keys to my homework exercises.  Amazing that your were able to trace the version of sample() in my ASUS computer.  Me running on 3.6.3 did not fix things because of its determined adherence to the buggy version.

Much appreciation,

Tom Fomby

Department of Economics

SMU

________________________________
From: Duncan Murdoch <murdoch.duncan at gmail.com>
Sent: Sunday, May 3, 2020 2:36:50 PM
To: Fomby, Tom; r-help at R-project.org
Subject: Re: [R] Question about "sample" function and inconsistent results I am getting across machines.

I just tried both versions, and it's the ASUS that's using the buggy old
algorithm.

Duncan Murdoch

On 03/05/2020 3:32 p.m., Duncan Murdoch wrote:
> On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
>> Please consider the following code:
>>
>> set.seed(1)
>>
>> train.index = sample(181,150)
>> head(train.index)
>> # [1]  49  67 103 162  36 159  Result from my ASUS computer
>> #
>> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>>
>> In both cases, version 3.6.3 of R are being used.
>>
>> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results.  This makes grading of homework challenging not knowing which partitions of the data are being used by the student.
>>
>> Thank you for considering my question.
>
> Likely some of you are storing and restoring workspaces, and have been
> doing so for a long time.  If you type
>
> RNGkind()
>
> what you should see is
>
> [1] "Mersenne-Twister" "Inversion"        "Rejection"
>
> but if the .Random.seed is restored from an old session, you might see
>
> [1] "Mersenne-Twister" "Inversion"        "Rounding"
>
> The latter uses the buggy version of sample().  Those users should run
>
> RNGkind(sample.kind = "Rejection")
>
> to start using the corrected sampling algorithm.  (The default was
> changed in R 3.6.0, but if you saved your seed from a previous version,
> you'd get the old sampler).
>
> They should also stop reloading old workspaces, but that's another
> discussion.
>
> Duncan Murdoch
>


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun May  3 21:56:08 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 3 May 2020 15:56:08 -0400
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <6355fc0de4614a4493ac5c64d4803c29@mail.smu.edu>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
 <6355fc0de4614a4493ac5c64d4803c29@mail.smu.edu>
Message-ID: <afe29b94-2f2a-290b-7aa4-11e60d89e9e9@gmail.com>

On 03/05/2020 3:43 p.m., Fomby, Tom wrote:
> 
> Dear Duncan,
> 
> OK, I will certainly ask my students to download the most recent version 
> of Basic R at the first of each semester?and, just to be safe, include 
> the RNGkind(sample.kind="Rejection") command before the students get 
> started on the data partitioning part of their exercise using the sample 
> function.

Actually, it would probably be a better idea to say

RNGkind(kind = "default", normal.kind = "default", sample.kind = "default")

in case bugs are found in any of the current algorithms and they change 
again.

> 
> By the way, how is it that one can take a membership in the R community 
> so as to provide support for volunteers like yourself.

The R Foundation accepts donations to become a "Supporting Member"; see 
here:  https://www.r-project.org/foundation/donors.html.  They sponsor 
various events, so that is one way.  There is probably also a local user 
group somewhere near you that would appreciate contributions of some 
sort.  There's a list of those here: 
https://blog.revolutionanalytics.com/local-r-groups.html, and another 
one here:  https://www.meetup.com/pro/r-user-groups/.  (I haven't 
checked how similar those two lists are.)

Duncan Murdoch


> 
> Thank you,
> 
> Tom Fomby
> 
> Department of Economics
> 
> SMU
> 
> Dallas, TX 75275
> 
> 
> 
> ------------------------------------------------------------------------
> *From:* Duncan Murdoch <murdoch.duncan at gmail.com>
> *Sent:* Sunday, May 3, 2020 2:32 PM
> *To:* Fomby, Tom; r-help at R-project.org
> *Subject:* Re: [R] Question about "sample" function and inconsistent 
> results I am getting across machines.
> On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
>> Please consider the following code:
>> 
>> set.seed(1)
>> 
>> train.index = sample(181,150)
>> head(train.index)
>> # [1]? 49? 67 103 162? 36 159? Result from my ASUS computer
>> #
>> # [1]? 68 167 129 162 43 14? Result from my wife's HP Pavilion computer
>> 
>> In both cases, version 3.6.3 of R are being used.
>> 
>> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.? These results do not seem to be specific to MAC (OS) versus PC (Windows).? In several cases, students using 3.6.3 got differing results. This makes grading of homework challenging not knowing which partitions 
> of the data are being used by the student.
>> 
>> Thank you for considering my question.
> 
> Likely some of you are storing and restoring workspaces, and have been
> doing so for a long time.? If you type
> 
> RNGkind()
> 
> what you should see is
> 
> [1] "Mersenne-Twister" "Inversion"??????? "Rejection"
> 
> but if the .Random.seed is restored from an old session, you might see
> 
> [1] "Mersenne-Twister" "Inversion"??????? "Rounding"
> 
> The latter uses the buggy version of sample().? Those users should run
> 
> RNGkind(sample.kind = "Rejection")
> 
> to start using the corrected sampling algorithm.? (The default was
> changed in R 3.6.0, but if you saved your seed from a previous version,
> you'd get the old sampler).
> 
> They should also stop reloading old workspaces, but that's another
> discussion.
> 
> Duncan Murdoch


From m@rk|eed@2 @end|ng |rom gm@||@com  Sun May  3 21:56:51 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Sun, 3 May 2020 15:56:51 -0400
Subject: [R] Fwd:  stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
Message-ID: <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>

Abby: Just one other thing. A friend of mine recommended reading the
R-language manual which I haven't read in many years.
I don't  know if it's because I'm more expeRienced or just plain oldeR but
I started it today and it's not nearly as daunting as I remember it
being in the past. It's going to take me some time but  I highly recommend
checking it out if you haven't already. Maybe it's like
a fine wine and just gets better with age !!!!!!! Good luck.


Mark





---------- Forwarded message ---------
From: Abby Spurdle <spurdle.a at gmail.com>
Date: Sun, May 3, 2020 at 6:19 AM
Subject: Re: [R] stats:: spline's method could not be monoH.FC
To: Mark Leeds <markleeds2 at gmail.com>
Cc: Martin Maechler <maechler at stat.math.ethz.ch>, Samuel Granjeaud
IR/Inserm <samuel.granjeaud at inserm.fr>, r-help <r-help at r-project.org>


Hi Mark,

The article is good.
However, there's still some grey areas.

The documentation for base::typeof equates a closure with a function.
However, the article defines a closure as a function together with an
environment.
A very minor difference I know, but it creates the problem that the
word closure is used inconsistently.
And that's without even getting into "frames".

Equating a closure with a function (only), is perhaps misleading...???

Also, re-iterating I use function objects with attributes.
(I prefer this approach, because I can make copies of function
objects, and if necessary modify them).
My guess is that doesn't meet the definition of an R closure (if you
ignore the environment)...???, and it's unclear whether it meets the
definition of a closure, more generally...???

So, I've still got the same problem, of how to refer to functions that
have *either* attributes *or* environments, containing data.
Maybe I should just stick to "Self-Referencing Function Objects" and
"Functions Bundled with Data"...???

One last thing, the last time I read S4 documentation, I couldn't tell
if it was possible to have S4-based function objects, and if so, could
the body of the S4-based function object (while being called) access
it's own slots...???


On Sun, May 3, 2020 at 4:27 PM Mark Leeds <markleeds2 at gmail.com> wrote:
>
> Abby: Here is an article on environments/closures which might be useful
to you. I was reviewing environments recently and it
> was a clear explanation of how environments/closures work in R. Even
though it's from 2000, I'm pretty certain that everything
> said in it still holds.

	[[alternative HTML version deleted]]


From t|omby @end|ng |rom m@||@@mu@edu  Sun May  3 21:58:50 2020
From: t|omby @end|ng |rom m@||@@mu@edu (Fomby, Tom)
Date: Sun, 3 May 2020 19:58:50 +0000
Subject: [R] 
 Question about "sample" function and inconsistent results I am
 getting across machines.
In-Reply-To: <afe29b94-2f2a-290b-7aa4-11e60d89e9e9@gmail.com>
References: <c0c8bafef4fe4122aae2614f15cb4574@mail.smu.edu>
 <d100571a-c498-6574-2232-17b685336ad4@gmail.com>
 <6355fc0de4614a4493ac5c64d4803c29@mail.smu.edu>,
 <afe29b94-2f2a-290b-7aa4-11e60d89e9e9@gmail.com>
Message-ID: <526d93f307204a358daa4b1a2c0b6d9c@mail.smu.edu>

Thank you so much Duncan.  I will pitch in.  Tom


________________________________
From: Duncan Murdoch <murdoch.duncan at gmail.com>
Sent: Sunday, May 3, 2020 2:56 PM
To: Fomby, Tom; r-help at R-project.org
Subject: Re: [R] Question about "sample" function and inconsistent results I am getting across machines.

On 03/05/2020 3:43 p.m., Fomby, Tom wrote:
>
> Dear Duncan,
>
> OK, I will certainly ask my students to download the most recent version
> of Basic R at the first of each semester and, just to be safe, include
> the RNGkind(sample.kind="Rejection") command before the students get
> started on the data partitioning part of their exercise using the sample
> function.

Actually, it would probably be a better idea to say

RNGkind(kind = "default", normal.kind = "default", sample.kind = "default")

in case bugs are found in any of the current algorithms and they change
again.

>
> By the way, how is it that one can take a membership in the R community
> so as to provide support for volunteers like yourself.

The R Foundation accepts donations to become a "Supporting Member"; see
here:  https://www.r-project.org/foundation/donors.html.  They sponsor
various events, so that is one way.  There is probably also a local user
group somewhere near you that would appreciate contributions of some
sort.  There's a list of those here:
https://blog.revolutionanalytics.com/local-r-groups.html, and another
one here:  https://www.meetup.com/pro/r-user-groups/.  (I haven't
checked how similar those two lists are.)

Duncan Murdoch


>
> Thank you,
>
> Tom Fomby
>
> Department of Economics
>
> SMU
>
> Dallas, TX 75275
>
>
>
> ------------------------------------------------------------------------
> *From:* Duncan Murdoch <murdoch.duncan at gmail.com>
> *Sent:* Sunday, May 3, 2020 2:32 PM
> *To:* Fomby, Tom; r-help at R-project.org
> *Subject:* Re: [R] Question about "sample" function and inconsistent
> results I am getting across machines.
> On 03/05/2020 1:39 a.m., Fomby, Tom wrote:
>> Please consider the following code:
>>
>> set.seed(1)
>>
>> train.index = sample(181,150)
>> head(train.index)
>> # [1]  49  67 103 162  36 159  Result from my ASUS computer
>> #
>> # [1]  68 167 129 162 43 14  Result from my wife's HP Pavilion computer
>>
>> In both cases, version 3.6.3 of R are being used.
>>
>> In addition, of the 20 students in my Predictive Analytics class, 14 got the first result while 6 got the latter result.  These results do not seem to be specific to MAC (OS) versus PC (Windows).  In several cases, students using 3.6.3 got differing results. This makes grading of homework challenging not knowing which partitions
> of the data are being used by the student.
>>
>> Thank you for considering my question.
>
> Likely some of you are storing and restoring workspaces, and have been
> doing so for a long time.  If you type
>
> RNGkind()
>
> what you should see is
>
> [1] "Mersenne-Twister" "Inversion"        "Rejection"
>
> but if the .Random.seed is restored from an old session, you might see
>
> [1] "Mersenne-Twister" "Inversion"        "Rounding"
>
> The latter uses the buggy version of sample().  Those users should run
>
> RNGkind(sample.kind = "Rejection")
>
> to start using the corrected sampling algorithm.  (The default was
> changed in R 3.6.0, but if you saved your seed from a previous version,
> you'd get the old sampler).
>
> They should also stop reloading old workspaces, but that's another
> discussion.
>
> Duncan Murdoch


	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Mon May  4 04:54:12 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Mon, 4 May 2020 02:54:12 +0000
Subject: [R] possible issue with scatterplot function in car package
In-Reply-To: <CADsEwSdxFMTpyoHaYm3LiW4aMyxihAvnuse6VNCYVYCt8hg4mQ@mail.gmail.com>
References: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
 <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>
 <CADsEwSdxFMTpyoHaYm3LiW4aMyxihAvnuse6VNCYVYCt8hg4mQ@mail.gmail.com>
Message-ID: <55CFE3E3-39A1-4217-B6B1-B8FA9C5D50CF@mcmaster.ca>

Dear Yousri,

The problem with scatterplot() is now fixed in the development version 3.0-8 of the car package on R-Forge, which eventually will be submitted to CRAN. Until then, you can install the package via install.packages("car", repos="http://R-Forge.R-project.org")

Thanks again for the bug report,
 John

> On May 3, 2020, at 4:37 AM, Yousri Fanous <yousri.fanous at gmail.com> wrote:
> 
> Thank you Professor John for your answer.
> 
> As you rightly said I am not using the ch in my example report as it has no bearing to the issue.
> However it is the ch that led me to find the issue.
> I was trying to label each point with its corresponding aa$ch value.
> I used this code:
> 
> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
> text(aa$x,aa$y, labels=aa$ch,font=1 ,cex=.9,pos=3)
> 
> The annotation was correct for 4 points but not for the (2,5) point.
> I figured it is because it is close to the margin of the plot hence as a quick solution I modified xlim to shift the point away from the margin.
> This worked for the annotation but eventually led to the issue I described.
> 
> Thank you so much for your time
> 
> Yousri Fanous
> 
> Software Developer
> IBM CANADA
> 
> On Sat, May 2, 2020 at 11:47 PM Fox, John <jfox at mcmaster.ca> wrote:
> 
> Dear Yousri,
> 
> Yes, this is clearly a bug, and almost surely a long-standing one. We'll fix it in the next release of the car package.
> 
> BTW, stringsAsFactors defaults to FALSE in R 4.0.0 (and you don't use the ch variable in the example). Also, although it has no bearing on the bug, I'd generally prefer
> 
>         scatterplot(y ~ x, data=aa, smooth=FALSE, grid=FALSE, 
>                 frame=FALSE, regLine=FALSE, xlim=c(0, 8))
> 
> Thank you for the bug report,
>  John
> 
>   -----------------------------
>   John Fox, Professor Emeritus
>   McMaster University
>   Hamilton, Ontario, Canada
>   Web: http::/socserv.mcmaster.ca/jfox
> 
> > On May 2, 2020, at 7:30 PM, Yousri Fanous <yousri.fanous at gmail.com> wrote:
> > 
> > library (car)
> > 
> > aa <- data.frame(x=c(2,  5, 6, 7, 8),
> > +  y=c(5,  10, 9, 12, 11),
> > + ch=c("N",  "Q", "R", "S", "T"),
> > + stringsAsFactors=FALSE)
> > 
> > scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame = FALSE,regLine=F)
> > 
> > Both x and y boxplots are correct
> > and in particular the median of the x box is at 6 which is confirmed
> > 
> >> median(aa$x)
> > [1] 6
> > 
> > Now I do only one addition to the scatterplot: I add xlim
> >> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> > FALSE,regLine=F,xlim=c(0,8))
> > 
> > This causes the boxplot on x-axis to be in error:
> > 1) the lower whisker starts now from zero
> > 2) the median is between 4 and 6 and no longer at 6 as before
> > 
> >> sessionInfo()
> > R version 3.6.3 (2020-02-29)
> > [1] car_3.0-7
> > 
> >       [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 


From you@r|@|@nou@ @end|ng |rom gm@||@com  Mon May  4 05:31:35 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Sun, 3 May 2020 23:31:35 -0400
Subject: [R] possible issue with scatterplot function in car package
In-Reply-To: <55CFE3E3-39A1-4217-B6B1-B8FA9C5D50CF@mcmaster.ca>
References: <31777_1588462276_042NV9Hx024074_CADsEwSdLUbnNjnUYALdycEODBknqd5L1PDEv9WNHbXZtHQ8N-Q@mail.gmail.com>
 <A26E6ABA-72CB-4DE0-BD6F-AAF0886D4445@mcmaster.ca>
 <CADsEwSdxFMTpyoHaYm3LiW4aMyxihAvnuse6VNCYVYCt8hg4mQ@mail.gmail.com>
 <55CFE3E3-39A1-4217-B6B1-B8FA9C5D50CF@mcmaster.ca>
Message-ID: <CADsEwSck6+F1K_8Ap=vuqdisoEHuDa71tefi+8zDtZ4we8d7QA@mail.gmail.com>

Great, thanks for the update!

Yousri

On Sun, May 3, 2020 at 10:54 PM Fox, John <jfox at mcmaster.ca> wrote:

> Dear Yousri,
>
> The problem with scatterplot() is now fixed in the development version
> 3.0-8 of the car package on R-Forge, which eventually will be submitted to
> CRAN. Until then, you can install the package via install.packages("car",
> repos="http://R-Forge.R-project.org")
>
> Thanks again for the bug report,
>  John
>
> > On May 3, 2020, at 4:37 AM, Yousri Fanous <yousri.fanous at gmail.com>
> wrote:
> >
> > Thank you Professor John for your answer.
> >
> > As you rightly said I am not using the ch in my example report as it has
> no bearing to the issue.
> > However it is the ch that led me to find the issue.
> > I was trying to label each point with its corresponding aa$ch value.
> > I used this code:
> >
> > scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F)
> > text(aa$x,aa$y, labels=aa$ch,font=1 ,cex=.9,pos=3)
> >
> > The annotation was correct for 4 points but not for the (2,5) point.
> > I figured it is because it is close to the margin of the plot hence as a
> quick solution I modified xlim to shift the point away from the margin.
> > This worked for the annotation but eventually led to the issue I
> described.
> >
> > Thank you so much for your time
> >
> > Yousri Fanous
> >
> > Software Developer
> > IBM CANADA
> >
> > On Sat, May 2, 2020 at 11:47 PM Fox, John <jfox at mcmaster.ca> wrote:
> >
> > Dear Yousri,
> >
> > Yes, this is clearly a bug, and almost surely a long-standing one. We'll
> fix it in the next release of the car package.
> >
> > BTW, stringsAsFactors defaults to FALSE in R 4.0.0 (and you don't use
> the ch variable in the example). Also, although it has no bearing on the
> bug, I'd generally prefer
> >
> >         scatterplot(y ~ x, data=aa, smooth=FALSE, grid=FALSE,
> >                 frame=FALSE, regLine=FALSE, xlim=c(0, 8))
> >
> > Thank you for the bug report,
> >  John
> >
> >   -----------------------------
> >   John Fox, Professor Emeritus
> >   McMaster University
> >   Hamilton, Ontario, Canada
> >   Web: http::/socserv.mcmaster.ca/jfox
> >
> > > On May 2, 2020, at 7:30 PM, Yousri Fanous <yousri.fanous at gmail.com>
> wrote:
> > >
> > > library (car)
> > >
> > > aa <- data.frame(x=c(2,  5, 6, 7, 8),
> > > +  y=c(5,  10, 9, 12, 11),
> > > + ch=c("N",  "Q", "R", "S", "T"),
> > > + stringsAsFactors=FALSE)
> > >
> > > scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> FALSE,regLine=F)
> > >
> > > Both x and y boxplots are correct
> > > and in particular the median of the x box is at 6 which is confirmed
> > >
> > >> median(aa$x)
> > > [1] 6
> > >
> > > Now I do only one addition to the scatterplot: I add xlim
> > >> scatterplot(aa$x,aa$y,smooth = FALSE, grid = FALSE, frame =
> > > FALSE,regLine=F,xlim=c(0,8))
> > >
> > > This causes the boxplot on x-axis to be in error:
> > > 1) the lower whisker starts now from zero
> > > 2) the median is between 4 and 6 and no longer at 6 as before
> > >
> > >> sessionInfo()
> > > R version 3.6.3 (2020-02-29)
> > > [1] car_3.0-7
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
>
>

	[[alternative HTML version deleted]]


From p_conno||y @end|ng |rom @||ng@hot@co@nz  Mon May  4 06:15:42 2020
From: p_conno||y @end|ng |rom @||ng@hot@co@nz (Patrick Connolly)
Date: Mon, 4 May 2020 16:15:42 +1200
Subject: [R] PCRE configure problem with R-4.0.0
Message-ID: <20200504041542.GA7518@slingshot.co.nz>

When I try ./configure --enable-R-shlib

I get this error:

configure: error: PCRE2 library and headers are required, or use --with-pcre1 and PCRE >= 8.32 with UTF-8 support

I have to admit I'm completely in the dark as to what functionality
PCRE provides.

Next, I tried using --with-pcre1 but it made no difference.

There are quite a lot of packages in the repository for Linux Mint
17.2 with 'pcre' in the name and these are installed:

> aptitude search pcre | grep  ^i
i   libpcre3                        - Perl 5 Compatible Regular Expression Libra
i   libpcre3:i386                   - Perl 5 Compatible Regular Expression Libra
i   libpcre3-dev                    - Perl 5 Compatible Regular Expression Libra
i   libpcrecpp0                     - Perl 5 Compatible Regular Expression Libra

Apparantly the '3' doesn't indicate an updated '2' version.  The only
packages with prce2 in the name are these:

> aptitude search pcre2
v   apertium-pcre2                                           -
v   apertium-pcre2:i386                                      -

Suggestions welcome.


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From bgunter@4567 @end|ng |rom gm@||@com  Mon May  4 07:33:30 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 3 May 2020 22:33:30 -0700
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <20200504041542.GA7518@slingshot.co.nz>
References: <20200504041542.GA7518@slingshot.co.nz>
Message-ID: <CAGxFJbT19sUrX1XxvmuZMxHBV0zr+fYzEYanZQg92Q4ATGhXTQ@mail.gmail.com>

"I have to admit I'm completely in the dark as to what functionality
PCRE provides"

https://www.pcre.org/

(Sorry, beyond that I know nothing).

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sun, May 3, 2020 at 9:16 PM Patrick Connolly
<p_connolly at slingshot.co.nz> wrote:
>
> When I try ./configure --enable-R-shlib
>
> I get this error:
>
> configure: error: PCRE2 library and headers are required, or use --with-pcre1 and PCRE >= 8.32 with UTF-8 support
>
> I have to admit I'm completely in the dark as to what functionality
> PCRE provides.
>
> Next, I tried using --with-pcre1 but it made no difference.
>
> There are quite a lot of packages in the repository for Linux Mint
> 17.2 with 'pcre' in the name and these are installed:
>
> > aptitude search pcre | grep  ^i
> i   libpcre3                        - Perl 5 Compatible Regular Expression Libra
> i   libpcre3:i386                   - Perl 5 Compatible Regular Expression Libra
> i   libpcre3-dev                    - Perl 5 Compatible Regular Expression Libra
> i   libpcrecpp0                     - Perl 5 Compatible Regular Expression Libra
>
> Apparantly the '3' doesn't indicate an updated '2' version.  The only
> packages with prce2 in the name are these:
>
> > aptitude search pcre2
> v   apertium-pcre2                                           -
> v   apertium-pcre2:i386                                      -
>
> Suggestions welcome.
>
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>    ___    Patrick Connolly
>  {~._.~}                   Great minds discuss ideas
>  _( Y )_                 Average minds discuss events
> (:_~*~_:)                  Small minds discuss people
>  (_)-(_)                              ..... Eleanor Roosevelt
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Mon May  4 09:51:40 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 4 May 2020 10:51:40 +0300
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <20200504041542.GA7518@slingshot.co.nz>
References: <20200504041542.GA7518@slingshot.co.nz>
Message-ID: <20200504100010.5d771a78@trisector>

First of all, you mentioned Linux Mint, so you might get better advice
on R-SIG-Debian mailing list.

On Mon, 4 May 2020 16:15:42 +1200
Patrick Connolly <p_connolly at slingshot.co.nz> wrote:

>There are quite a lot of packages in the repository for Linux Mint
>17.2 with 'pcre' in the name and these are installed:

>Apparantly the '3' doesn't indicate an updated '2' version

The funny thing about libpcre3 is that it is the old PCRE1 version,
third ABI-incompatible upgrade of it [*], and libpcre2 (available in
current releases of Linux Mint, Ubuntu and Debian) is supposed to be
the newer PCRE2.

Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
April 2014, while PCRE2 has been released in 2015. This might be the
reason why libpcre2 doesn't seem to be available to you (I have tried
searching both repositories, including backports, with no success).
Moreover, Ubuntu 14.04 only has PCRE 8.31, which is too old to work
with R 4.0. The official builds of R 4.0 [**] are not available for
Ubuntu 14.04, either.

> Suggestions welcome.

You can try to backport PCRE 8.32 for Linux Mint 17.2 by taking the
source package for 8.31 (apt-get source pcre3), extracting the new
version of PCRE into it and fiddling with it until it builds (see
[***] for more information on that). This is a complicated procedure,
and if an important system component depends on PCRE, you can end up
breaking the system.

You can also try to install latest PCRE2 from source (./configure; make;
sudo make install) into /usr/local where it shouldn't interfere too much
with the rest of the system.

Another option could be upgrading to a supported release of Linux Mint
and installing the official binary build from [**].

Good luck!

-- 
Best regards,
Ivan

[*]
https://www.debian.org/doc/debian-policy/ch-sharedlibs.html#run-time-shared-libraries

[**] https://cran.r-project.org/bin/linux/ubuntu/

[***] https://www.debian.org/doc/manuals/maint-guide/


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon May  4 16:04:18 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 4 May 2020 16:04:18 +0200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
Message-ID: <24240.8418.914054.939133@stat.math.ethz.ch>

Just about this one some important term  'closure', hence I'm
modifying the subject. 

Note we came here from my 2nd reason why I had added 'monoH.FC'
feature only for splinefun() and not for spline() :

- splinefun() is using's R feature of "(non-trivial) closure", i.e, 
  it returns a *function* containing its own state

-------------

Yes, there is some vagueness / ambiguity about how the term
"closure" is used within R documentation and teaching :

Yes, indeed, the term  stems from lisp ("Scheme" more
specifically according to
 https://en.wikipedia.org/wiki/Closure_(computer_programming) ,
but you should remember that R originally had been implemented
as a "lisp with S-syntax" (that's my paraphrasing), see
 https://en.wikipedia.org/wiki/R_(programming_language)
   and (in the box on the right hand side) its list of 

    "Influenced by" : Common Lisp, S, Scheme[2], XLispStat

and [2] is Ross Ihaka's famous Interface paper :

    Ihaka, Ross (1998). R : Past and Future History (PDF) (Technical report).
    Statistics Department, The University of Auckland, Auckland, New Zealand.
    https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf

And that's the reason why  typeof(f)   for all functions 'f'
which are not primitive is "closure".
All such functions (i.e. *not* the primitives) have an
environment, as Mark Leeds explains,  and what this means and
why this is very important is beyond 'R-help'.
Inside R's own C code, all such R functions are "closures", and
programmers (incl R corers) who think more about the low level
view of R objects would use the term like that ex

OTOH, Hadley Wickham has written a book "Advanced R"  which has been
the best book about advanced R by far (in my view, notably
before it morphed (towards the 2nd edition) to use more and more
non-base R packages).  There, he used "Closure" in a different,
stricter sense, starting the section  'Closures' with

    ?An object is data with functions.
     A closure is a function with data.? ? John D. Cook

Now, most functions have only a "trivial environment" (my own
  terminology,  when I'm teaching "advanced R" courses/classes,
  see https://github.com/mmaechler/ProgRRR/ for some teaching material)

"trivial environment" meaning that their environment is
- either the namespace belonging to the package the function is part of
- or .GlobalEnv  { which is the same as globalenv() }

and most functions with non-trivial environment are just
"helper" functions defined inside other functions which are very
short lived (during the evaluation of the outer function's call).

Now the remaining few functions with non-trivial environments
that you see in "base R"  are those returned by

  splinefun(), approxfun(),  ecdf(), or stepfun()

where the last two actually are implemented via approxfun().

-- -- --

I hope this has been useful  "writeup" about
'closure' ..

Best,
Martin

Martin Maechler
ETH Zurich  and  R Core team


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon May  4 16:25:02 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 4 May 2020 16:25:02 +0200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
Message-ID: <24240.9662.85376.166974@stat.math.ethz.ch>

>>>>> Abby Spurdle 
>>>>>     on Sun, 3 May 2020 16:15:17 +1200 writes:

    >> and just today a colleague asked me about spline interpolation
    >> with general 2nd derivative boundary conditions
    >> s''(x_1) = s2_1,  s''(x_n) = s2_2

actually I was wrong... I *read* it as the above, but what he
really wanted was what the wikipedia page  class "clamped"
boundary conditions, i.e., for the *first* derivative

   s'(x_1) = s1_1,  s'(x_n) = s1_2
   

    > It should possible via cubic Hermite splines.

and indeed that is available via cubic Hermite splines available
in R via stats package's  splinefunH()

{which I wrote when implementing  "monoH.FC"}

    > A nontrivial design decision in my package was the computation of
    > slopes at the endpoints.
    > (Something which I got wrong, twice...)

The "wikiversity" has a very nice small math lecture on this
 https://en.wikiversity.org/wiki/Cubic_Spline_Interpolation

which derives *both* cases (first and 2nd derivative boundary conditions),
the only draw back to quickly do it with ('base R') is that I
need to "translate" that (2nd derivative values $M_i$) parametrization
into either the one used into the (a,b,c,y)-parametrizat of the
default spline() / splinefun() methods or the Hilbert spline
form for  splinefunH(x[],y[],m[]).

Martin

    > My guess is that I could write a function in about 60 to 90 minutes,
    > including all the testing and calculus.

    > However, I need to note two things:
    > (1) Cubic Hermite splines do not have a continuous second derivative.

well, many don't if you allow any slopes at node. However, if
you only set 2 boundary conditions *instead* of the natural
spline ones  f''(x_1) = f''(x_1) = 0, 
you can still remain in C_2 (i.e. continuous 2nd derivative).

(And that's also what the above wikiversity lecture provides).

    > (2) Specifying the second derivatives (at the endpoints) would prevent
    > the user from specifying the first derivatives (aka the slopes).

    > Could you please confirm if the function would still be of interest...?

Well, as I know spent enough time reading and thinking, I'd
really like to add   method = "clamped" to splinefun() and also
the other one where fix the 2nd derivatives (to arbitrary values
instead of zero).

So if you already have the R code leading up to one of the 2
"parametrizations" we use in spline() / splinefun(),  I'd be
grateful, if you have the time.

Martin


From |@t@z@hn @end|ng |rom gm@||@com  Mon May  4 17:03:04 2020
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Mon, 4 May 2020 11:03:04 -0400
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <20200504100010.5d771a78@trisector>
References: <20200504041542.GA7518@slingshot.co.nz>
 <20200504100010.5d771a78@trisector>
Message-ID: <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>

On Mon, May 4, 2020 at 3:51 AM Ivan Krylov <krylov.r00t at gmail.com> wrote:
>
> First of all, you mentioned Linux Mint, so you might get better advice
> on R-SIG-Debian mailing list.
>
> On Mon, 4 May 2020 16:15:42 +1200
> Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
>
> >There are quite a lot of packages in the repository for Linux Mint
> >17.2 with 'pcre' in the name and these are installed:
>
> >Apparantly the '3' doesn't indicate an updated '2' version
>
> The funny thing about libpcre3 is that it is the old PCRE1 version,
> third ABI-incompatible upgrade of it [*], and libpcre2 (available in
> current releases of Linux Mint, Ubuntu and Debian) is supposed to be
> the newer PCRE2.
>
> Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
> April 2014, while PCRE2 has been released in 2015.

Moreover, support for 17.2 ended over a year ago (according to
https://en.wikipedia.org/wiki/Linux_Mint_version_history). I suggest
upgrading to a supported version.

Best,
Ista

This might be the
> reason why libpcre2 doesn't seem to be available to you (I have tried
> searching both repositories, including backports, with no success).
> Moreover, Ubuntu 14.04 only has PCRE 8.31, which is too old to work
> with R 4.0. The official builds of R 4.0 [**] are not available for
> Ubuntu 14.04, either.
>
> > Suggestions welcome.
>
> You can try to backport PCRE 8.32 for Linux Mint 17.2 by taking the
> source package for 8.31 (apt-get source pcre3), extracting the new
> version of PCRE into it and fiddling with it until it builds (see
> [***] for more information on that). This is a complicated procedure,
> and if an important system component depends on PCRE, you can end up
> breaking the system.
>
> You can also try to install latest PCRE2 from source (./configure; make;
> sudo make install) into /usr/local where it shouldn't interfere too much
> with the rest of the system.
>
> Another option could be upgrading to a supported release of Linux Mint
> and installing the official binary build from [**].
>
> Good luck!
>
> --
> Best regards,
> Ivan
>
> [*]
> https://www.debian.org/doc/debian-policy/ch-sharedlibs.html#run-time-shared-libraries
>
> [**] https://cran.r-project.org/bin/linux/ubuntu/
>
> [***] https://www.debian.org/doc/manuals/maint-guide/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon May  4 17:08:44 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 4 May 2020 17:08:44 +0200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <24240.9662.85376.166974@stat.math.ethz.ch>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <24240.9662.85376.166974@stat.math.ethz.ch>
Message-ID: <24240.12284.675977.989318@stat.math.ethz.ch>

>>>>> Martin Maechler 
>>>>>     on Mon, 4 May 2020 16:25:02 +0200 writes:

>>>>> Abby Spurdle 
>>>>>     on Sun, 3 May 2020 16:15:17 +1200 writes:

    >>> and just today a colleague asked me about spline interpolation
    >>> with general 2nd derivative boundary conditions
    >>> s''(x_1) = s2_1,  s''(x_n) = s2_2

    > actually I was wrong... I *read* it as the above, but what he
    > really wanted was what the wikipedia page  class "clamped"
    > boundary conditions, i.e., for the *first* derivative

    > s'(x_1) = s1_1,  s'(x_n) = s1_2
   

    >> It should possible via cubic Hermite splines.

    > and indeed that is available via cubic Hermite splines available
    > in R via stats package's  splinefunH()

    > {which I wrote when implementing  "monoH.FC"}

    >> A nontrivial design decision in my package was the computation of
    >> slopes at the endpoints.
    >> (Something which I got wrong, twice...)

    > The "wikiversity" has a very nice small math lecture on this
    > https://en.wikiversity.org/wiki/Cubic_Spline_Interpolation

    > which derives *both* cases (first and 2nd derivative boundary conditions),
    > the only draw back to quickly do it with ('base R') is that I
    > need to "translate" that (2nd derivative values $M_i$) parametrization
    > into either the one used into the (a,b,c,y)-parametrizat of the
    > default spline() / splinefun() methods or the Hilbert spline
    > form for  splinefunH(x[],y[],m[]).

    > Martin

Well, I should have looked a bit further, first : at least the case for

    s''(x_1) = s2_1,    s''(x_n) = s2_2

seems trivially "hidden" in the C code in

R's  src/library/stats/src/splines.c     i.e.
https://svn.r-project.org/R/trunk/src/library/stats/src/splines.c

at the end of natural_spline() we should set c[1] and c[n] to non-zero.
...
and actually I think in spline_eval() for extrapolation (to the
left? and right), there's currently also an assumption that c[1]
 and c[n] are zero.

So as a matter of fact I would ask for patches there (both in C
and in R calling C ... maybe too much for 30 minutes ;-)

Best,
Martin


    >> My guess is that I could write a function in about 60 to 90 minutes,
    >> including all the testing and calculus.

    >> However, I need to note two things:
    >> (1) Cubic Hermite splines do not have a continuous second derivative.

    > well, many don't if you allow any slopes at node. However, if
    > you only set 2 boundary conditions *instead* of the natural
    > spline ones  f''(x_1) = f''(x_1) = 0, 
    > you can still remain in C_2 (i.e. continuous 2nd derivative).

    > (And that's also what the above wikiversity lecture provides).

    >> (2) Specifying the second derivatives (at the endpoints) would prevent
    >> the user from specifying the first derivatives (aka the slopes).

    >> Could you please confirm if the function would still be of interest...?

    > Well, as I know spent enough time reading and thinking, I'd
    > really like to add   method = "clamped" to splinefun() and also
    > the other one where fix the 2nd derivatives (to arbitrary values
    > instead of zero).

    > So if you already have the R code leading up to one of the 2
    > "parametrizations" we use in spline() / splinefun(),  I'd be
    > grateful, if you have the time.

    > Martin


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Mon May  4 19:15:44 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Mon, 4 May 2020 12:15:44 -0500
Subject: [R] if else statement
Message-ID: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>

Hello,

I have a data frame like this:

> head(b)
       FID   IID FLASER PLASER
1: fam1000 G1000      1      1
2: fam1001 G1001      1      1
3: fam1003 G1003      1      2
4: fam1005 G1005      1      1
5: fam1009 G1009      NA      2
6: fam1052 G1052      1      1
...
> unique(b$PLASER)
[1]  1  2 NA
> unique(b$FLASER)
[1]  1  2 NA

how can I do if else statement so that I am creating a
PHENO =2 if b$FLASER=2 or b$PLASER=2
PHENO=1 if b$FLASER=1 and b$PLASER=1
otherwise PHENO=NA

I tried this but I am not sure if this is correct:
b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
b$FLASER==2,2,NA))

Thanks
Ana

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Mon May  4 19:33:12 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Mon, 4 May 2020 13:33:12 -0400
Subject: [R] if else statement
In-Reply-To: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
Message-ID: <CAJc=yOG82RM0bhf9Npk6eKbKva56=94e4swT4Vh6kOjTgq8nsg@mail.gmail.com>

"I tried this but I am not sure if this is correct:"

Does it provide the expected result for all possible combinations of 1/2/NA
for both variables?

On Mon, May 4, 2020 at 1:16 PM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> Hello,
>
> I have a data frame like this:
>
> > head(b)
>        FID   IID FLASER PLASER
> 1: fam1000 G1000      1      1
> 2: fam1001 G1001      1      1
> 3: fam1003 G1003      1      2
> 4: fam1005 G1005      1      1
> 5: fam1009 G1009      NA      2
> 6: fam1052 G1052      1      1
> ...
> > unique(b$PLASER)
> [1]  1  2 NA
> > unique(b$FLASER)
> [1]  1  2 NA
>
> how can I do if else statement so that I am creating a
> PHENO =2 if b$FLASER=2 or b$PLASER=2
> PHENO=1 if b$FLASER=1 and b$PLASER=1
> otherwise PHENO=NA
>
> I tried this but I am not sure if this is correct:
> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> b$FLASER==2,2,NA))
>
> Thanks
> Ana
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May  4 19:45:05 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 04 May 2020 10:45:05 -0700
Subject: [R] if else statement
In-Reply-To: <CAJc=yOG82RM0bhf9Npk6eKbKva56=94e4swT4Vh6kOjTgq8nsg@mail.gmail.com>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
 <CAJc=yOG82RM0bhf9Npk6eKbKva56=94e4swT4Vh6kOjTgq8nsg@mail.gmail.com>
Message-ID: <D03511ED-4587-489F-B24D-D6DD4571AF1F@dcn.davis.ca.us>

To expand on Patrick's response...

You can use the expand.grid function to generate a test table containing all combinations. However, we would not be in a position to verify that the results you get when you apply your logic to the test table are what you want... you know the requirements much better than we do. Nor is that kind of service what this mailing list is for, so please focus on showing what you cannot figure out how to accomplish rather than asking us to do or check your work for you.

On May 4, 2020 10:33:12 AM PDT, "Patrick (Malone Quantitative)" <malone at malonequantitative.com> wrote:
>"I tried this but I am not sure if this is correct:"
>
>Does it provide the expected result for all possible combinations of
>1/2/NA
>for both variables?
>
>On Mon, May 4, 2020 at 1:16 PM Ana Marija <sokovic.anamarija at gmail.com>
>wrote:
>
>> Hello,
>>
>> I have a data frame like this:
>>
>> > head(b)
>>        FID   IID FLASER PLASER
>> 1: fam1000 G1000      1      1
>> 2: fam1001 G1001      1      1
>> 3: fam1003 G1003      1      2
>> 4: fam1005 G1005      1      1
>> 5: fam1009 G1009      NA      2
>> 6: fam1052 G1052      1      1
>> ...
>> > unique(b$PLASER)
>> [1]  1  2 NA
>> > unique(b$FLASER)
>> [1]  1  2 NA
>>
>> how can I do if else statement so that I am creating a
>> PHENO =2 if b$FLASER=2 or b$PLASER=2
>> PHENO=1 if b$FLASER=1 and b$PLASER=1
>> otherwise PHENO=NA
>>
>> I tried this but I am not sure if this is correct:
>> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
>> b$FLASER==2,2,NA))
>>
>> Thanks
>> Ana
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>

-- 
Sent from my phone. Please excuse my brevity.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Mon May  4 20:05:25 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Mon, 4 May 2020 13:05:25 -0500
Subject: [R] if else statement
In-Reply-To: <D03511ED-4587-489F-B24D-D6DD4571AF1F@dcn.davis.ca.us>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
 <CAJc=yOG82RM0bhf9Npk6eKbKva56=94e4swT4Vh6kOjTgq8nsg@mail.gmail.com>
 <D03511ED-4587-489F-B24D-D6DD4571AF1F@dcn.davis.ca.us>
Message-ID: <CAF9-5jMyxGgk=sXFQKHZh9vvL0-RK5NWm=HkRCh54r7c3_ABFg@mail.gmail.com>

Thank you for the tip about table function, it seems correct:

> table(b$FLASER, b$PLASER, exclude = NULL)

         1   2 <NA>
  1    836 691    6
  2     14  70    8
  <NA>   0  45   28
> table(b$pheno,exclude = NULL)

   1    2 <NA>
 836  828   34

On Mon, May 4, 2020 at 12:45 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> To expand on Patrick's response...
>
> You can use the expand.grid function to generate a test table containing
> all combinations. However, we would not be in a position to verify that the
> results you get when you apply your logic to the test table are what you
> want... you know the requirements much better than we do. Nor is that kind
> of service what this mailing list is for, so please focus on showing what
> you cannot figure out how to accomplish rather than asking us to do or
> check your work for you.
>
> On May 4, 2020 10:33:12 AM PDT, "Patrick (Malone Quantitative)" <
> malone at malonequantitative.com> wrote:
> >"I tried this but I am not sure if this is correct:"
> >
> >Does it provide the expected result for all possible combinations of
> >1/2/NA
> >for both variables?
> >
> >On Mon, May 4, 2020 at 1:16 PM Ana Marija <sokovic.anamarija at gmail.com>
> >wrote:
> >
> >> Hello,
> >>
> >> I have a data frame like this:
> >>
> >> > head(b)
> >>        FID   IID FLASER PLASER
> >> 1: fam1000 G1000      1      1
> >> 2: fam1001 G1001      1      1
> >> 3: fam1003 G1003      1      2
> >> 4: fam1005 G1005      1      1
> >> 5: fam1009 G1009      NA      2
> >> 6: fam1052 G1052      1      1
> >> ...
> >> > unique(b$PLASER)
> >> [1]  1  2 NA
> >> > unique(b$FLASER)
> >> [1]  1  2 NA
> >>
> >> how can I do if else statement so that I am creating a
> >> PHENO =2 if b$FLASER=2 or b$PLASER=2
> >> PHENO=1 if b$FLASER=1 and b$PLASER=1
> >> otherwise PHENO=NA
> >>
> >> I tried this but I am not sure if this is correct:
> >> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> >> b$FLASER==2,2,NA))
> >>
> >> Thanks
> >> Ana
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Mon May  4 20:31:12 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Mon, 4 May 2020 14:31:12 -0400 (EDT)
Subject: [R] COVID-19 datasets...
Message-ID: <961343375.1407915.1588617072873@connect.xfinity.com>

Just curious does anyone know of a website that has data available in a format that R can download and analyze?
?
Thanks


Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon May  4 20:32:06 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 4 May 2020 19:32:06 +0100
Subject: [R] if else statement
In-Reply-To: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
Message-ID: <5862eb7d-db98-98af-280f-1a5ffbcc2587@sapo.pt>

Hello,

Here is a way, using logical indices.

b$pheno <- NA
b$pheno[b$FLASER == 1 & b$PLASER == 1] <- 1
b$pheno[b$FLASER == 2 | b$PLASER == 2] <- 2


Hope this helps,

Rui Barradas

?s 18:15 de 04/05/20, Ana Marija escreveu:
> Hello,
> 
> I have a data frame like this:
> 
>> head(b)
>         FID   IID FLASER PLASER
> 1: fam1000 G1000      1      1
> 2: fam1001 G1001      1      1
> 3: fam1003 G1003      1      2
> 4: fam1005 G1005      1      1
> 5: fam1009 G1009      NA      2
> 6: fam1052 G1052      1      1
> ...
>> unique(b$PLASER)
> [1]  1  2 NA
>> unique(b$FLASER)
> [1]  1  2 NA
> 
> how can I do if else statement so that I am creating a
> PHENO =2 if b$FLASER=2 or b$PLASER=2
> PHENO=1 if b$FLASER=1 and b$PLASER=1
> otherwise PHENO=NA
> 
> I tried this but I am not sure if this is correct:
> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> b$FLASER==2,2,NA))
> 
> Thanks
> Ana
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From btupper @end|ng |rom b|ge|ow@org  Mon May  4 20:34:42 2020
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Mon, 4 May 2020 14:34:42 -0400
Subject: [R] COVID-19 datasets...
In-Reply-To: <961343375.1407915.1588617072873@connect.xfinity.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
Message-ID: <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>

This works very well https://github.com/covid19datahub/COVID19

Cheers,
Ben

On Mon, May 4, 2020 at 2:32 PM Bernard McGarvey <
mcgarvey.bernard at comcast.net> wrote:

> Just curious does anyone know of a website that has data available in a
> format that R can download and analyze?
>
> Thanks
>
>
> Bernard McGarvey
>
>
> Director, Fort Myers Beach Lions Foundation, Inc.
>
>
> Retired (Lilly Engineering Fellow).
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
http://www.bigelow.org/
https://eco.bigelow.org

	[[alternative HTML version deleted]]


From h@@@n@d|w@n @end|ng |rom gm@||@com  Mon May  4 20:43:42 2020
From: h@@@n@d|w@n @end|ng |rom gm@||@com (Hasan Diwan)
Date: Mon, 4 May 2020 11:43:42 -0700
Subject: [R] COVID-19 datasets...
In-Reply-To: <961343375.1407915.1588617072873@connect.xfinity.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
Message-ID: <CAP+bYWDq_bjtNT=aTUCVNH=b30W6GGeq3F3HGbLEW2j47aLPpQ@mail.gmail.com>

On Mon, 4 May 2020 at 11:32, Bernard McGarvey <mcgarvey.bernard at comcast.net>
wrote:

> Just curious does anyone know of a website that has data available in a
> format that R can download and analyze?
>

https://hd1-units.herokuapp.com/covid has a days parameter one can adjust
to go back in time and a suffix parameter to obtain json, yml, or csv. -- H

-- 
OpenPGP:
https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1
If you wish to request my time, please do so using
*bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.
Si vous voudrais faire connnaisance, allez a *bit.ly/hd1AppointmentRequest
<http://bit.ly/hd1AppointmentRequest>*.

<https://sks-keyservers.net/pks/lookup?op=get&search=0xFEBAD7FFD041BBA1>Sent
from my mobile device
Envoye de mon portable

	[[alternative HTML version deleted]]


From you@r|@|@nou@ @end|ng |rom gm@||@com  Mon May  4 20:51:03 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Mon, 4 May 2020 14:51:03 -0400
Subject: [R] why outer function is failing?
Message-ID: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>

Hello

>From outer help page:
outer takes two vectors
<https://renenyffenegger.ch/notes/development/languages/R/data-structures/vector/index>
and a function (that itself takes two arguments) and builds a matrix
<https://renenyffenegger.ch/notes/development/languages/R/data-structures/matrix/index>
by calling the given function for each combination of the elements in the
two vectors.

x<-1:6
y<-3:10

 m<-outer (x,y,function (x,y) rnorm(x,y))
works as expected.

But now when I replace rnorm with rolldie from package (prob) outer
complains
 library (prob)
m<-outer (x,y,function (x,y) nrow(rolldie(x,y)))
Error in rep("X", times) : invalid 'times' argument
In addition: Warning messages:
1: In 1:times : numerical expression has 48 elements: only the first used
2: In 1:nsides : numerical expression has 48 elements: only the first used

nrow(rolldie(5,4))
[1] 1024

1) why outer is failing with rolldie?
2) What does the error mean?

As a workaround I can do this thru a double loop, but I was hoping to get a
more efficient way.

Thanks for the help

Yousri Fanous
Software developer
IBM Canada

	[[alternative HTML version deleted]]


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Mon May  4 20:56:25 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Mon, 4 May 2020 14:56:25 -0400
Subject: [R] COVID-19 datasets...
In-Reply-To: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
References: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
Message-ID: <01B72E8D-CDF6-445F-8E95-EB8D3158CC77@comcast.net>

Thanks, i will take a look

Bernard
Sent from my iPhone so please excuse the spelling!"

> On May 4, 2020, at 2:49 PM, James Spottiswoode <james at jsasoc.com> wrote:
> 
> ?Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
> 
> https://github.com/CSSEGISandData/COVID-19
> 
> All in csv fiormat.
> 
> 
>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>> 
>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>>  
>> Thanks
>> 
>> 
>> Bernard McGarvey
>> 
>> 
>> Director, Fort Myers Beach Lions Foundation, Inc.
>> 
>> 
>> Retired (Lilly Engineering Fellow).
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> James Spottiswoode
> Applied Mathematics & Statistics
> (310) 270 6220
> jamesspottiswoode Skype
> james at jsasoc.com
> 
> 
> 

	[[alternative HTML version deleted]]


From dc@r|@on @end|ng |rom t@mu@edu  Mon May  4 21:18:40 2020
From: dc@r|@on @end|ng |rom t@mu@edu (David Carlson)
Date: Mon, 4 May 2020 14:18:40 -0500
Subject: [R] why outer function is failing?
In-Reply-To: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
References: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
Message-ID: <CAE-dL2pS1meP02WsahMgAyBm3gYRL6u5--24mScX-B-O+xKWLA@mail.gmail.com>

The FUN= argument must be a vectorized function (see the documentation,
?outer), but the function rolldie takes only scalar values as arguments:

rolldie(x, y)
Error in rep("X", times) : invalid 'times' argument
In addition: Warning messages:
1: In 1:times : numerical expression has 6 elements: only the first used
2: In 1:nsides : numerical expression has 8 elements: only the first used

David Carlson
Anthropology Department
Texas A&M University

On Mon, May 4, 2020 at 1:54 PM Yousri Fanous <yousri.fanous at gmail.com>
wrote:

> Hello
>
> From outer help page:
> outer takes two vectors
> <
> https://urldefense.proofpoint.com/v2/url?u=https-3A__renenyffenegger.ch_notes_development_languages_R_data-2Dstructures_vector_index&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=aLWLgMoHHIk7DaXgX4L0emzN-KDe2WFMYPpVwZuk35U&e=
> >
> and a function (that itself takes two arguments) and builds a matrix
> <
> https://urldefense.proofpoint.com/v2/url?u=https-3A__renenyffenegger.ch_notes_development_languages_R_data-2Dstructures_matrix_index&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=j5k3zAaxPR8LMHbTe72GTbSGOV_RE5K1Uc2jytaB8SE&e=
> >
> by calling the given function for each combination of the elements in the
> two vectors.
>
> x<-1:6
> y<-3:10
>
>  m<-outer (x,y,function (x,y) rnorm(x,y))
> works as expected.
>
> But now when I replace rnorm with rolldie from package (prob) outer
> complains
>  library (prob)
> m<-outer (x,y,function (x,y) nrow(rolldie(x,y)))
> Error in rep("X", times) : invalid 'times' argument
> In addition: Warning messages:
> 1: In 1:times : numerical expression has 48 elements: only the first used
> 2: In 1:nsides : numerical expression has 48 elements: only the first used
>
> nrow(rolldie(5,4))
> [1] 1024
>
> 1) why outer is failing with rolldie?
> 2) What does the error mean?
>
> As a workaround I can do this thru a double loop, but I was hoping to get a
> more efficient way.
>
> Thanks for the help
>
> Yousri Fanous
> Software developer
> IBM Canada
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=6jxhmxrSNqyZS4-oU2g8r2R0LEZ0yhtSm-4GdfP0Cbk&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=rFq4TmerLd5tegHnWSbx5ISdfDNks-TIa9Whne6bBaM&e=
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May  4 21:32:16 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 04 May 2020 12:32:16 -0700
Subject: [R] why outer function is failing?
In-Reply-To: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
References: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
Message-ID: <27846454-E813-4FB1-940F-BC190E208039@dcn.davis.ca.us>

The outer function only calls FUN once with two vectors representing all combinations of the inputs. If rolldie is not vectorized then it will have trouble with this input.

Why aren't you using sample?

On May 4, 2020 11:51:03 AM PDT, Yousri Fanous <yousri.fanous at gmail.com> wrote:
>Hello
>
>From outer help page:
>outer takes two vectors
><https://renenyffenegger.ch/notes/development/languages/R/data-structures/vector/index>
>and a function (that itself takes two arguments) and builds a matrix
><https://renenyffenegger.ch/notes/development/languages/R/data-structures/matrix/index>
>by calling the given function for each combination of the elements in
>the
>two vectors.
>
>x<-1:6
>y<-3:10
>
> m<-outer (x,y,function (x,y) rnorm(x,y))
>works as expected.
>
>But now when I replace rnorm with rolldie from package (prob) outer
>complains
> library (prob)
>m<-outer (x,y,function (x,y) nrow(rolldie(x,y)))
>Error in rep("X", times) : invalid 'times' argument
>In addition: Warning messages:
>1: In 1:times : numerical expression has 48 elements: only the first
>used
>2: In 1:nsides : numerical expression has 48 elements: only the first
>used
>
>nrow(rolldie(5,4))
>[1] 1024
>
>1) why outer is failing with rolldie?
>2) What does the error mean?
>
>As a workaround I can do this thru a double loop, but I was hoping to
>get a
>more efficient way.
>
>Thanks for the help
>
>Yousri Fanous
>Software developer
>IBM Canada
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From dc@r|@on @end|ng |rom t@mu@edu  Mon May  4 21:34:18 2020
From: dc@r|@on @end|ng |rom t@mu@edu (David Carlson)
Date: Mon, 4 May 2020 14:34:18 -0500
Subject: [R] why outer function is failing?
In-Reply-To: <CAE-dL2pS1meP02WsahMgAyBm3gYRL6u5--24mScX-B-O+xKWLA@mail.gmail.com>
References: <CADsEwSfuFpV=oJni7E_CnLzUKa-PSOtc8vnRQ5pkj4mLV-y8GA@mail.gmail.com>
 <CAE-dL2pS1meP02WsahMgAyBm3gYRL6u5--24mScX-B-O+xKWLA@mail.gmail.com>
Message-ID: <CAE-dL2rRLtVjubQM7zJii1iTHe=6Lm+TEeVoSQky_ZMNqnztRg@mail.gmail.com>

Re-reading your question it appears you want the number of permutations for
each pair, e.g. the number of permutations of the numbers 1 - 3 taken 1
time, etc. If I am correct, this should get you want you want:

ct <- outer(y, x, "^")
ct
     [,1] [,2] [,3]  [,4]   [,5]    [,6]
[1,]    3    9   27    81    243     729
[2,]    4   16   64   256   1024    4096
[3,]    5   25  125   625   3125   15625
[4,]    6   36  216  1296   7776   46656
[5,]    7   49  343  2401  16807  117649
[6,]    8   64  512  4096  32768  262144
[7,]    9   81  729  6561  59049  531441
[8,]   10  100 1000 10000 100000 1000000

David L Carlson
Anthropology Department
Texas A&M University

On Mon, May 4, 2020 at 2:18 PM David Carlson <dcarlson at tamu.edu> wrote:

> The FUN= argument must be a vectorized function (see the documentation,
> ?outer), but the function rolldie takes only scalar values as arguments:
>
> rolldie(x, y)
> Error in rep("X", times) : invalid 'times' argument
> In addition: Warning messages:
> 1: In 1:times : numerical expression has 6 elements: only the first used
> 2: In 1:nsides : numerical expression has 8 elements: only the first used
>
> David Carlson
> Anthropology Department
> Texas A&M University
>
> On Mon, May 4, 2020 at 1:54 PM Yousri Fanous <yousri.fanous at gmail.com>
> wrote:
>
>> Hello
>>
>> From outer help page:
>> outer takes two vectors
>> <
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__renenyffenegger.ch_notes_development_languages_R_data-2Dstructures_vector_index&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=aLWLgMoHHIk7DaXgX4L0emzN-KDe2WFMYPpVwZuk35U&e=
>> >
>> and a function (that itself takes two arguments) and builds a matrix
>> <
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__renenyffenegger.ch_notes_development_languages_R_data-2Dstructures_matrix_index&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=j5k3zAaxPR8LMHbTe72GTbSGOV_RE5K1Uc2jytaB8SE&e=
>> >
>> by calling the given function for each combination of the elements in the
>> two vectors.
>>
>> x<-1:6
>> y<-3:10
>>
>>  m<-outer (x,y,function (x,y) rnorm(x,y))
>> works as expected.
>>
>> But now when I replace rnorm with rolldie from package (prob) outer
>> complains
>>  library (prob)
>> m<-outer (x,y,function (x,y) nrow(rolldie(x,y)))
>> Error in rep("X", times) : invalid 'times' argument
>> In addition: Warning messages:
>> 1: In 1:times : numerical expression has 48 elements: only the first used
>> 2: In 1:nsides : numerical expression has 48 elements: only the first used
>>
>> nrow(rolldie(5,4))
>> [1] 1024
>>
>> 1) why outer is failing with rolldie?
>> 2) What does the error mean?
>>
>> As a workaround I can do this thru a double loop, but I was hoping to get
>> a
>> more efficient way.
>>
>> Thanks for the help
>>
>> Yousri Fanous
>> Software developer
>> IBM Canada
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=6jxhmxrSNqyZS4-oU2g8r2R0LEZ0yhtSm-4GdfP0Cbk&e=
>> PLEASE do read the posting guide
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwICAg&c=u6LDEWzohnDQ01ySGnxMzg&r=VAaHUElasUXjP9TzIcfIrXdkDpHnJBBZ9Q1u5LcXz9s&m=IdpldO8NxT8R9A1xPy5xD8VynSPRwZchLY3bUEufCYE&s=rFq4TmerLd5tegHnWSbx5ISdfDNks-TIa9Whne6bBaM&e=
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Mon May  4 21:36:33 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 5 May 2020 07:36:33 +1200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
Message-ID: <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>

While we're on this topic...

I was interested in testing hypotheses (plural) that acid base
chemistry (blood H+/HCO3/PCO2) or closely-related metabolic processes
could influence the rate of viral replication, possibly by having some
sort of effect on the acidity of phagocytes, vesicles or other
cellular/intracellular-level "host" environments.

There appears to be a relationship between age and blood pH, with a
possibility of a change point around retirement age. This could (being
somewhat speculative) be related to the differences we see in
mortality rates by age.

However, I've got no idea how to test the second part of the
hypotheses, that blood acidity (or any other metabolic process) could
influence the acidity of phagocytes or vesicles.

Suggestions welcome...


From @purd|e@@ @end|ng |rom gm@||@com  Mon May  4 21:40:00 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 5 May 2020 07:40:00 +1200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
 <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>
Message-ID: <CAB8pepy3BsWyrp8GUOaWEVcHW9oX6pRoRiz3ixsTYF9a+u4S5g@mail.gmail.com>

sorry, blood acidity probably wasn't the best choice of words


From bgunter@4567 @end|ng |rom gm@||@com  Mon May  4 22:42:44 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 4 May 2020 13:42:44 -0700
Subject: [R] COVID-19 datasets...
In-Reply-To: <CAB8pepy3BsWyrp8GUOaWEVcHW9oX6pRoRiz3ixsTYF9a+u4S5g@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
 <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>
 <CAB8pepy3BsWyrp8GUOaWEVcHW9oX6pRoRiz3ixsTYF9a+u4S5g@mail.gmail.com>
Message-ID: <CAGxFJbQ8uyzsJCSnYG+oeB3rw4BDt7RNNJEUWXZR-JM6-spBGg@mail.gmail.com>

Suggestion: Please stay on topic.

This list is R-Help, not about scientific discussions or even what
statistical procedures might be used for specific research questions.
Perhaps https://stats.stackexchange.com/  for the latter.


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, May 4, 2020 at 12:55 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> sorry, blood acidity probably wasn't the best choice of words
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Mon May  4 22:53:01 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 5 May 2020 08:53:01 +1200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CAGxFJbQ8uyzsJCSnYG+oeB3rw4BDt7RNNJEUWXZR-JM6-spBGg@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <CALrbzg1OHAxZ+4yh7LwdADrynhOrj+yaSzWZwCMX7qKZsbMpBg@mail.gmail.com>
 <CAB8pepypKNhY22No4QPAZV48-oO11DodcPgHqa9P8RwmeUFetg@mail.gmail.com>
 <CAB8pepy3BsWyrp8GUOaWEVcHW9oX6pRoRiz3ixsTYF9a+u4S5g@mail.gmail.com>
 <CAGxFJbQ8uyzsJCSnYG+oeB3rw4BDt7RNNJEUWXZR-JM6-spBGg@mail.gmail.com>
Message-ID: <CAB8pepyFb-SbWaufy52kE7OLPvex9-dWboHntWOqdynBx8EesA@mail.gmail.com>

Yes, you're right.

Note that stack exchange is 10x more likely to flag my post as
off-topic, and I was looking for *interesting* applications of R.
There's only so many times one can use Fisher's iris data...


On Tue, May 5, 2020 at 8:42 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Suggestion: Please stay on topic.
>
> This list is R-Help, not about scientific discussions or even what
> statistical procedures might be used for specific research questions.
> Perhaps https://stats.stackexchange.com/  for the latter.
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Mon, May 4, 2020 at 12:55 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >
> > sorry, blood acidity probably wasn't the best choice of words
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Tue May  5 00:11:08 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 5 May 2020 10:11:08 +1200
Subject: [R] COVID-19 datasets...
In-Reply-To: <961343375.1407915.1588617072873@connect.xfinity.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
Message-ID: <CAB8pepyuJ-vLVG2NmigaKF23dmeP0Y_GFpPSVQAp-A_eNLGX_w@mail.gmail.com>

I don't know if this is useful of not.
But here's some dis-aggregated US data.
It's about 10 days old, and I need to improve the dis-aggregation algorithm.
------------------------------------------------------
age <- 0:80
rel.mort.rate <- c (0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
5.97119198173175e-14, 1.49279799543293e-12,
1.80329997848298e-11, 1.40323011570696e-10, 7.91302361419091e-10,
3.45021443896442e-09, 0.0000000121231319447505, 0.0000000353442018829079,
0.0000000874198628343878, 0.000000186810950489511, 0.000000350496608960251,
0.000000586292710567257, 0.00000088809586113832, 0.00000123828572996647,
0.00000161636339411418, 0.00000200770373248443, 0.00000240656741448015,
0.00000281336507897391, 0.00000323131179486445, 0.00000366643530132155,
0.00000412942874923419, 0.00000463520698193433, 0.00000519914074326175,
0.00000583299029582447, 0.00000654330938109396, 0.00000733211405261902,
0.00000819881232642656, 0.00000914397689953956, 0.0000101752369848173,
0.0000113121562971092, 0.0000125849353938131, 0.0000140250299768815,
0.0000156519020717643, 0.0000174632527695486, 0.000019434044086,
0.0000215255604245307, 0.0000237027683240887, 0.0000259554459399679,
0.0000283147468915187, 0.0000308551986409676, 0.0000336771894650306,
0.0000368753737436115, 0.0000405063605263262, 0.0000445688466005247,
0.0000490032796563514, 0.0000537118848883967, 0.0000585949512128272,
0.0000635935967055111, 0.0000687239859205501, 0.0000740884186923496,
0.0000798570919656766, 0.0000862259973389807, 0.0000933650267485426,
0.000101373734773254, 0.000110260898647345, 0.0001199575254273,
0.000130361177821875, 0.000141396704797265, 0.000153071840957868,
0.00016550903061107, 0.000178943792317475, 0.000193689554660746,
0.000210077052171998, 0.00022838282771009, 0.000248763951569319,
0.000271212123374848, 0.000295531695409173, 0.000321339382531619,
0.000348084036019435, 0.000375091815336575, 0.000401647747211891)

names (rel.mort.rate) <- age

options (scipen=4)
mar <- par ("mar")
mar [2] <- mar [2] + 1
p0 <- par (mar=mar)
barplot (rel.mort.rate [41:81], ylim = c (0, 0.0004), xlab="age",
ylab="relative mortality rate\n(ndeaths / npeople | age)")
par (p0)


From byron_dom @end|ng |rom y@hoo@com  Tue May  5 01:46:12 2020
From: byron_dom @end|ng |rom y@hoo@com (Byron Dom)
Date: Mon, 4 May 2020 23:46:12 +0000 (UTC)
Subject: [R] Problems installing rgl (3D graphics package). Error messages
 "... not available, " "non-zero exit status" and so on
References: <1265948541.865911.1588635972728.ref@mail.yahoo.com>
Message-ID: <1265948541.865911.1588635972728@mail.yahoo.com>

I want to install the 3D graphics rgl.?
I'm running R version 3.2.3 on Windows version "10.0.18362 N/A Build 18362".
I've been using R for about 15 years but only as a casual user. I have installed two packages in the past, but the last was several years ago.
When I issued the command "install.packages("rgl",dependencies=TRUE)," The output I got included quite a few error messages. I've pasted below some of the output that came at the end. There was so much output that I've only included the parts that contained the error and warning messages, which was at the end. I am happy to provide all of the output, if that would help in diagnosing the problem(s).
My questions about this are the obvious ones:(1) What is causing it??(2) How can I fix it?
I'm also wondering if installing R 4.0 before installing rgl, would fix the problems. I've been planning to do that in the near future anyway.
Thanks in advance for any help.
? - - Byron Dom

ERROR: dependency 'processx' is not available for package 'callr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/callr'* installing *source* package 'magic' ...** package 'magic' successfully unpacked and MD5 sums checked** R** data** inst** preparing package for lazy loadingWarning: package 'abind' was built under R version 3.2.5** help*** installing help indices** building package indices** installing vignettes** testing if installed package can be loadedWarning: package 'abind' was built under R version 3.2.5* DONE (magic)* installing *source* package 'stringr' ...** package 'stringr' successfully unpacked and MD5 sums checked** R** data*** moving datasets to lazyload DB** inst** preparing package for lazy loadingError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :?? namespace 'stringi' 1.1.5 is being loaded, but >= 1.1.7 is requiredERROR: lazy loading failed for package 'stringr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/stringr'* installing *source* package 'tinytex' ...** package 'tinytex' successfully unpacked and MD5 sums checked** R** inst** preparing package for lazy loading** help*** installing help indices** building package indices** testing if installed package can be loaded* DONE (tinytex)ERROR: dependencies 'htmltools', 'later', 'promises', 'rlang', 'fastmap' are not available for package 'shiny'* removing 'C:/Users/byron/Documents/R/win-library/3.2/shiny'ERROR: dependency 'htmltools' is not available for package 'crosstalk'* removing 'C:/Users/byron/Documents/R/win-library/3.2/crosstalk'* installing *source* package 'rstudioapi' ...** package 'rstudioapi' successfully unpacked and MD5 sums checked** R** inst** preparing package for lazy loading** help*** installing help indices*** copying figures** building package indices** installing vignettes** testing if installed package can be loaded* DONE (rstudioapi)ERROR: dependencies 'shiny', 'htmltools' are not available for package 'miniUI'* removing 'C:/Users/byron/Documents/R/win-library/3.2/miniUI'ERROR: dependency 'callr' is not available for package 'webshot'* removing 'C:/Users/byron/Documents/R/win-library/3.2/webshot'ERROR: dependency 'stringr' is not available for package 'knitr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/knitr'ERROR: dependencies 'shiny', 'miniUI', 'htmltools', 'knitr', 'webshot' are not available for package 'manipulateWidget'* removing 'C:/Users/byron/Documents/R/win-library/3.2/manipulateWidget'ERROR: dependencies 'knitr', 'htmltools', 'stringr' are not available for package 'rmarkdown'* removing 'C:/Users/byron/Documents/R/win-library/3.2/rmarkdown'
The downloaded source packages are in? ? ? ? ?C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb\downloaded_packages?

There were 18 warnings (use warnings() to see them)> warnings()Warning messages:1: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/callr_3.4.3.tar.gz' had status 12: In install.packages("rgl", dependencies = TRUE) :? installation of package ?callr? had non-zero exit status3: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/stringr_1.4.0.tar.gz' had status 14: In install.packages("rgl", dependencies = TRUE) :? installation of package ?stringr? had non-zero exit status5: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/shiny_1.4.0.2.tar.gz' had status 16: In install.packages("rgl", dependencies = TRUE) :? installation of package ?shiny? had non-zero exit status7: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/crosstalk_1.1.0.1.tar.gz' had status 18: In install.packages("rgl", dependencies = TRUE) :? installation of package ?crosstalk? had non-zero exit status9: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/miniUI_0.1.1.1.tar.gz' had status 110: In install.packages("rgl", dependencies = TRUE) :? installation of package ?miniUI? had non-zero exit status11: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/webshot_0.5.2.tar.gz' had status 112: In install.packages("rgl", dependencies = TRUE) :? installation of package ?webshot? had non-zero exit status13: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/knitr_1.28.tar.gz' had status 114: In install.packages("rgl", dependencies = TRUE) :? installation of package ?knitr? had non-zero exit status15: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/manipulateWidget_0.10.1.tar.gz' had status 116: In install.packages("rgl", dependencies = TRUE) :? installation of package ?manipulateWidget? had non-zero exit status17: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/rmarkdown_2.1.tar.gz' had status 118: In install.packages("rgl", dependencies = TRUE) :? installation of package ?rmarkdown? had non-zero exit status>?
> date()[1] "Mon May 04 15:55:35 2020"


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue May  5 01:55:23 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 4 May 2020 19:55:23 -0400
Subject: [R] 
 Problems installing rgl (3D graphics package). Error messages
 "... not available, " "non-zero exit status" and so on
In-Reply-To: <1265948541.865911.1588635972728@mail.yahoo.com>
References: <1265948541.865911.1588635972728.ref@mail.yahoo.com>
 <1265948541.865911.1588635972728@mail.yahoo.com>
Message-ID: <bf9e480b-db97-bb67-01a2-0280f9a3eb67@gmail.com>

On 04/05/2020 7:46 p.m., Byron Dom via R-help wrote:
> I want to install the 3D graphics rgl.
> I'm running R version 3.2.3 on Windows version "10.0.18362 N/A Build 18362".

That's a four year old version of R.  You'll likely have to find 
packages of a similar age to be able to get it to work, or upgrade to 
3.6.3 (the previous version, very stable) or 4.0.0 (the current version).

Duncan Murdoch

> I've been using R for about 15 years but only as a casual user. I have installed two packages in the past, but the last was several years ago.
> When I issued the command "install.packages("rgl",dependencies=TRUE)," The output I got included quite a few error messages. I've pasted below some of the output that came at the end. There was so much output that I've only included the parts that contained the error and warning messages, which was at the end. I am happy to provide all of the output, if that would help in diagnosing the problem(s).
> My questions about this are the obvious ones:(1) What is causing it??(2) How can I fix it?
> I'm also wondering if installing R 4.0 before installing rgl, would fix the problems. I've been planning to do that in the near future anyway.
> Thanks in advance for any help.
>  ? - - Byron Dom
> 
> ERROR: dependency 'processx' is not available for package 'callr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/callr'* installing *source* package 'magic' ...** package 'magic' successfully unpacked and MD5 sums checked** R** data** inst** preparing package for lazy loadingWarning: package 'abind' was built under R version 3.2.5** help*** installing help indices** building package indices** installing vignettes** testing if installed package can be loadedWarning: package 'abind' was built under R version 3.2.5* DONE (magic)* installing *source* package 'stringr' ...** package 'stringr' successfully unpacked and MD5 sums checked** R** data*** moving datasets to lazyload DB** inst** preparing package for lazy loadingError in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :?? namespace 'stringi' 1.1.5 is being loaded, but >= 1.1.7 is requiredERROR: lazy loading failed for package 'stringr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/stringr'* installing *source* package 'tinytex' ...** package 'tinytex' successfully unpacked and MD5 sums checked** R** inst** preparing package for lazy loading** help*** installing help indices** building package indices** testing if installed package can be loaded* DONE (tinytex)ERROR: dependencies 'htmltools', 'later', 'promises', 'rlang', 'fastmap' are not available for package 'shiny'* removing 'C:/Users/byron/Documents/R/win-library/3.2/shiny'ERROR: dependency 'htmltools' is not available for package 'crosstalk'* removing 'C:/Users/byron/Documents/R/win-library/3.2/crosstalk'* installing *source* package 'rstudioapi' ...** package 'rstudioapi' successfully unpacked and MD5 sums checked** R** inst** preparing package for lazy loading** help*** installing help indices*** copying figures** building package indices** installing vignettes** testing if installed package can be loaded* DONE (rstudioapi)ERROR: dependencies 'shiny', 'htmltools' are not available for package 'miniUI'* removing 'C:/Users/byron/Documents/R/win-library/3.2/miniUI'ERROR: dependency 'callr' is not available for package 'webshot'* removing 'C:/Users/byron/Documents/R/win-library/3.2/webshot'ERROR: dependency 'stringr' is not available for package 'knitr'* removing 'C:/Users/byron/Documents/R/win-library/3.2/knitr'ERROR: dependencies 'shiny', 'miniUI', 'htmltools', 'knitr', 'webshot' are not available for package 'manipulateWidget'* removing 'C:/Users/byron/Documents/R/win-library/3.2/manipulateWidget'ERROR: dependencies 'knitr', 'htmltools', 'stringr' are not available for package 'rmarkdown'* removing 'C:/Users/byron/Documents/R/win-library/3.2/rmarkdown'
> The downloaded source packages are in? ? ? ? ?C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb\downloaded_packages?
> 
> There were 18 warnings (use warnings() to see them)> warnings()Warning messages:1: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/callr_3.4.3.tar.gz' had status 12: In install.packages("rgl", dependencies = TRUE) :? installation of package ?callr? had non-zero exit status3: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/stringr_1.4.0.tar.gz' had status 14: In install.packages("rgl", dependencies = TRUE) :? installation of package ?stringr? had non-zero exit status5: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/shiny_1.4.0.2.tar.gz' had status 16: In install.packages("rgl", dependencies = TRUE) :? installation of package ?shiny? had non-zero exit status7: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/crosstalk_1.1.0.1.tar.gz' had status 18: In install.packages("rgl", dependencies = TRUE) :? installation of package ?crosstalk? had non-zero exit status9: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/miniUI_0.1.1.1.tar.gz' had status 110: In install.packages("rgl", dependencies = TRUE) :? installation of package ?miniUI? had non-zero exit status11: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/webshot_0.5.2.tar.gz' had status 112: In install.packages("rgl", dependencies = TRUE) :? installation of package ?webshot? had non-zero exit status13: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/knitr_1.28.tar.gz' had status 114: In install.packages("rgl", dependencies = TRUE) :? installation of package ?knitr? had non-zero exit status15: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/manipulateWidget_0.10.1.tar.gz' had status 116: In install.packages("rgl", dependencies = TRUE) :? installation of package ?manipulateWidget? had non-zero exit status17: running command '"C:/PROGRA~1/R/R-32~1.3/bin/x64/R" CMD INSTALL -l "C:\Users\byron\Documents\R\win-library\3.2" C:\Users\byron\AppData\Local\Temp\Rtmp2lNuLb/downloaded_packages/rmarkdown_2.1.tar.gz' had status 118: In install.packages("rgl", dependencies = TRUE) :? installation of package ?rmarkdown? had non-zero exit status>
>> date()[1] "Mon May 04 15:55:35 2020"
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From r@oknz @end|ng |rom gm@||@com  Tue May  5 02:41:53 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Tue, 5 May 2020 12:41:53 +1200
Subject: [R] if else statement
In-Reply-To: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
Message-ID: <CABcYAdKnu-x6hFBNfJyALNEptw9fJZcsFhdx=sfuYi2z-RQcCw@mail.gmail.com>

Your ifelse expression looks fine.  What goes wrong with it?

On Tue, 5 May 2020 at 05:16, Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I have a data frame like this:
>
> > head(b)
>        FID   IID FLASER PLASER
> 1: fam1000 G1000      1      1
> 2: fam1001 G1001      1      1
> 3: fam1003 G1003      1      2
> 4: fam1005 G1005      1      1
> 5: fam1009 G1009      NA      2
> 6: fam1052 G1052      1      1
> ...
> > unique(b$PLASER)
> [1]  1  2 NA
> > unique(b$FLASER)
> [1]  1  2 NA
>
> how can I do if else statement so that I am creating a
> PHENO =2 if b$FLASER=2 or b$PLASER=2
> PHENO=1 if b$FLASER=1 and b$PLASER=1
> otherwise PHENO=NA
>
> I tried this but I am not sure if this is correct:
> b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> b$FLASER==2,2,NA))
>
> Thanks
> Ana
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Tue May  5 08:46:55 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Tue, 5 May 2020 06:46:55 +0000
Subject: [R] COVID-19 datasets...
In-Reply-To: <01B72E8D-CDF6-445F-8E95-EB8D3158CC77@comcast.net>
References: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <01B72E8D-CDF6-445F-8E95-EB8D3158CC77@comcast.net>
Message-ID: <942c647ec2ac4309a6c3513670af1052@SRVEXCHCM1302.precheza.cz>

Another option is

https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide

Together with instruments from

https://www.repidemicsconsortium.org/

Cheers
Petr

Here is some simple code

library(EpiEstim)
library(ggplot2)
library(lubridate)
library(incidence)
library(distcrete)
library(epitrix)
library(readxl)

mena <- c("Austria", "Czechia", "Germany", "Italy", "Japan", "Russia", "South_Korea", 
"Spain", "Sweden", "Taiwan", "United_States_of_America", "United_Kingdom")

plot_Ri <- function(estimate_R_obj) {
    p_I <- plot(estimate_R_obj, "incid") + ggtitle(staty[vyber][i])  # plots the incidence
    p_SI <- plot(estimate_R_obj, "SI")  # plots the serial interval distribution
    p_Ri <- plot(estimate_R_obj, "R") + ylim(c(0,5))
    return(gridExtra::grid.arrange(p_I, p_Ri, ncol = 1))
}

data <- read_excel("covid.xlsx")
data <- as.data.frame(data)
staty <- levels(factor(data[,7]))
vyber <- which(staty %in% mena)
staty[vyber]

# covid.xlsx is downloaded data

vyber <- which(staty %in% mena)
vyber <- vyber[-6]
staty[vyber]
ddd <- vector("list", length=length(vyber))
pdf("grafy2.pdf")
for (i in 1:length(vyber)) {
temp <- data[data[,7]==staty[vyber][i],]
temp$cas <- ymd(temp$dateRep)
ooo <- order(temp$cas)
temp <- temp[ooo,]
temp<- temp[-1,]
temp <- temp[-(1:min(which(temp$cases>0))-1),]
head(temp)
test <- temp[, c(12,5)]
names(test) <- c("date", "I")
test$I <- abs(test$I)
inc <- rep(test$date, test$I)
inci <- incidence(inc)
peak <- find_peak(inci)
fit <- incidence::fit(inci, split=peak)
print(plot(inci, fit = fit)+ggtitle(staty[vyber][i]))
ddd[[i]] <- rbind(fit$before$info$doubling.conf, fit$after$info$halving.conf)
vysled <- estimate_R(test, method = "uncertain_si",
    config = make_config(list(mean_si = 4, std_mean_si = 2,
        min_mean_si = 1, max_mean_si = 8.4, std_si = 2.4, std_std_si = 1,
        min_std_si = 0.5, max_std_si = 4, n1 = 1000, n2 = 1000)))
print(plot_Ri(vysled))
}
dev.off()
names(ddd) <- staty[vyber]
ddd


> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Bernard Comcast
> Sent: Monday, May 4, 2020 8:56 PM
> To: James Spottiswoode <james at jsasoc.com>
> Cc: r-help at r-project.org
> Subject: Re: [R] COVID-19 datasets...
> 
> Thanks, i will take a look
> 
> Bernard
> Sent from my iPhone so please excuse the spelling!"
> 
> > On May 4, 2020, at 2:49 PM, James Spottiswoode <james at jsasoc.com>
> wrote:
> >
> > ?Sure. COVID-19 Data Repository by the Center for Systems Science and
> Engineering (CSSE) at Johns Hopkins University is available here:
> >
> > https://github.com/CSSEGISandData/COVID-19
> >
> > All in csv fiormat.
> >
> >
> >> On May 4, 2020, at 11:31 AM, Bernard McGarvey
> <mcgarvey.bernard at comcast.net> wrote:
> >>
> >> Just curious does anyone know of a website that has data available in a
> format that R can download and analyze?
> >>
> >> Thanks
> >>
> >>
> >> Bernard McGarvey
> >>
> >>
> >> Director, Fort Myers Beach Lions Foundation, Inc.
> >>
> >>
> >> Retired (Lilly Engineering Fellow).
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > James Spottiswoode
> > Applied Mathematics & Statistics
> > (310) 270 6220
> > jamesspottiswoode Skype
> > james at jsasoc.com
> >
> >
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From petr@p|k@| @end|ng |rom prechez@@cz  Tue May  5 09:12:53 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Tue, 5 May 2020 07:12:53 +0000
Subject: [R] if else statement
In-Reply-To: <5862eb7d-db98-98af-280f-1a5ffbcc2587@sapo.pt>
References: <CAF9-5jODa79FKHbkmRqD4T2Lzak5GrQL4TWji4pPkTpC3ndd4Q@mail.gmail.com>
 <5862eb7d-db98-98af-280f-1a5ffbcc2587@sapo.pt>
Message-ID: <24c9f303c2bb4194805097ff6a28d95d@SRVEXCHCM1302.precheza.cz>

Hi

another possible version 

b$pheno <- ((b$FLASER==2) | (b$PLASER==2))+1

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
> Sent: Monday, May 4, 2020 8:32 PM
> To: sokovic.anamarija at gmail.com; r-help <r-help at r-project.org>
> Subject: Re: [R] if else statement
> 
> Hello,
> 
> Here is a way, using logical indices.
> 
> b$pheno <- NA
> b$pheno[b$FLASER == 1 & b$PLASER == 1] <- 1 b$pheno[b$FLASER == 2 |
> b$PLASER == 2] <- 2
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 18:15 de 04/05/20, Ana Marija escreveu:
> > Hello,
> >
> > I have a data frame like this:
> >
> >> head(b)
> >         FID   IID FLASER PLASER
> > 1: fam1000 G1000      1      1
> > 2: fam1001 G1001      1      1
> > 3: fam1003 G1003      1      2
> > 4: fam1005 G1005      1      1
> > 5: fam1009 G1009      NA      2
> > 6: fam1052 G1052      1      1
> > ...
> >> unique(b$PLASER)
> > [1]  1  2 NA
> >> unique(b$FLASER)
> > [1]  1  2 NA
> >
> > how can I do if else statement so that I am creating a PHENO =2 if
> > b$FLASER=2 or b$PLASER=2
> > PHENO=1 if b$FLASER=1 and b$PLASER=1
> > otherwise PHENO=NA
> >
> > I tried this but I am not sure if this is correct:
> > b$pheno=ifelse(b$PLASER==1 & b$FLASER==1,1,ifelse(b$PLASER==2 |
> > b$FLASER==2,2,NA))
> >
> > Thanks
> > Ana
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From p_conno||y @end|ng |rom @||ng@hot@co@nz  Tue May  5 10:37:17 2020
From: p_conno||y @end|ng |rom @||ng@hot@co@nz (Patrick Connolly)
Date: Tue, 5 May 2020 20:37:17 +1200
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>
References: <20200504041542.GA7518@slingshot.co.nz>
 <20200504100010.5d771a78@trisector>
 <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>
Message-ID: <20200505083717.GB7518@slingshot.co.nz>

On Mon, 04-May-2020 at 11:03AM -0400, Ista Zahn wrote:

|> On Mon, May 4, 2020 at 3:51 AM Ivan Krylov <krylov.r00t at gmail.com> wrote:
|> >
|> > First of all, you mentioned Linux Mint, so you might get better advice
|> > on R-SIG-Debian mailing list.
|> >
|> > On Mon, 4 May 2020 16:15:42 +1200
|> > Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
|> >
|> > >There are quite a lot of packages in the repository for Linux Mint
|> > >17.2 with 'pcre' in the name and these are installed:
|> >
|> > >Apparantly the '3' doesn't indicate an updated '2' version
|> >
|> > The funny thing about libpcre3 is that it is the old PCRE1 version,
|> > third ABI-incompatible upgrade of it [*], and libpcre2 (available in
|> > current releases of Linux Mint, Ubuntu and Debian) is supposed to be
|> > the newer PCRE2.
|> >
|> > Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
|> > April 2014, while PCRE2 has been released in 2015.
|> 
|> Moreover, support for 17.2 ended over a year ago (according to
|> https://en.wikipedia.org/wiki/Linux_Mint_version_history). I suggest
|> upgrading to a supported version.

Thanks for making that clear -- though I don't relish the hassle of
upgrading an OS.  I was quite happy with the features of Mint 17.x.

[...]


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From |@t@z@hn @end|ng |rom gm@||@com  Tue May  5 12:28:52 2020
From: |@t@z@hn @end|ng |rom gm@||@com (Ista Zahn)
Date: Tue, 5 May 2020 06:28:52 -0400
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <20200505083717.GB7518@slingshot.co.nz>
References: <20200504041542.GA7518@slingshot.co.nz>
 <20200504100010.5d771a78@trisector>
 <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>
 <20200505083717.GB7518@slingshot.co.nz>
Message-ID: <CA+vqiLE1rjP6AtgUNkxVZdWX4D4-GTRB_104fP7qh8SzCeFn6Q@mail.gmail.com>

<snip>

> |> > Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
> |> > April 2014, while PCRE2 has been released in 2015.
> |>
> |> Moreover, support for 17.2 ended over a year ago (according to
> |> https://en.wikipedia.org/wiki/Linux_Mint_version_history). I suggest
> |> upgrading to a supported version.
>
> Thanks for making that clear -- though I don't relish the hassle of
> upgrading an OS.  I was quite happy with the features of Mint 17.x.
>

Sure, but there is no free lunch and as your system gets older and
older you'll find that more and more modern software doesn't work with
it. Additionally there are increasing security risks because the
vendor is no longer releasing security patches.

That said, if you really really wanna, you can use
https://docs.conda.io/en/latest/, https://spack.io/, or similar to
install recent software releases on old systems.

Best,
Ista

> [...]
>
>
> --
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>    ___    Patrick Connolly
>  {~._.~}                   Great minds discuss ideas
>  _( Y )_                 Average minds discuss events
> (:_~*~_:)                  Small minds discuss people
>  (_)-(_)                              ..... Eleanor Roosevelt
>
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Tue May  5 12:42:06 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Tue, 5 May 2020 15:12:06 +0430
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
Message-ID: <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>

Hi,
I hope you are doing well!
I have a CSV file which its encoding is ANSI. How can i change its encoding
to UTF-8 in R?
Many thanks!
With best regards,

-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From jr@| @end|ng |rom po@teo@no  Tue May  5 12:56:57 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Tue, 5 May 2020 12:56:57 +0200
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
 <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
Message-ID: <20200505105657.GA1073515@posteo.no>

On 2020-05-05 15:12 +0430, Mehdi Dadkhah wrote:
> I have a CSV file which its encoding is 
> ANSI. How can i change its encoding to 
> UTF-8 in R?

Hi!

I do not know about ANSI, but to read latin1 
encoded csv files into readr, do this:

Determine that your file is latin1-encoded:

	rasmus at twentyfive ~ % file -i SAA.csv
	SAA.csv: application/csv; charset=iso-8859-1

read it in using readr::read_csv

	locale <- readr::locale(encoding = "latin1")
	SAA <- suppressMessages(
	  readr::read_csv(file="SAA.csv",
	                  locale=locale))

Best,
Rasmus


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Tue May  5 13:07:10 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Tue, 5 May 2020 13:07:10 +0200
Subject: [R] How to use pakcage R0
Message-ID: <CAMk+s2T-X8wXp+owCj3E1Cmez-imgE-fByNLNY_XDibHXOgPqQ@mail.gmail.com>

Dear all,
I have been trying to use the package R0
https://www.rdocumentation.org/packages/R0/versions/1.2-6/topics/estimate.R but
the manual is not so rich of words.
The example given is based on the named vector Germany.1918
```
> library("R0")
> data(Germany.1918)
> Germany.1918
1918-09-29 1918-09-30 1918-10-01 1918-10-02 1918-10-03 1918-10-04
1918-10-05
        10          4          4         19          6         13
28
1918-10-06 1918-10-07 1918-10-08 1918-10-09 1918-10-10 1918-10-11
1918-10-12
        23         35         27         42         51         43
78
1918-10-13 1918-10-14 1918-10-15 1918-10-16 1918-10-17 1918-10-18
1918-10-19
        86         80        109        126        126        159
 190
[...]
```

Then it creates a gamma function and applied the estimate.R function:
```
mGT<-generation.time("gamma", c(3, 1.5))
estR0<-estimate.R(Germany.1918, mGT, begin=1, end=27, methods=c("EG", "ML",
"TD", "AR", "SB"),
                  pop.size=100000, nsim=100)
```

I tried with a similar approach for the current epidemics in China:
```
> china_vect
23/01/20 24/01/20 25/01/20 26/01/20 27/01/20 28/01/20 29/01/20 30/01/20
31/01/20
     259      457      688      769     1771     1459     1737     1981
2099
> mGT = generation.time("gamma", c(3, 1.5))   # create distribution
> estR0 = estimate.R(china_vect, mGT, begin=1, end=length(china_vect),
                  methods="EG",
                  pop.size=pop_ch, nsim=100)
Error in integrity.checks(epid, t, GT, begin, end, date.first.obs,
time.step,  :
  If both 'begin'= 1  and 'end'= 103  are provided, they must be of the
same class (dates, character strings or integers).
```
So I gave the value 103 directly (why it did not accept length, is the
first question?) and it worked:
> estR0 = estimate.R(china_vect, mGT, begin=1, end=103,
+                   methods="EG",
+                   pop.size=pop_ch, nsim=100)
Waiting for profiling to be done...
> estR0
Reproduction number estimate using  Exponential Growth  method.
R :  0.3359444[ 0.3209695 , 0.3510899 ]
```
I tried another endpoint, 27 as in the example:
```
> estR0 = estimate.R(china_vect, mGT, begin=1, end=27,
+                   methods="EG",
+                   pop.size=pop_ch, nsim=100)
Waiting for profiling to be done...
Error: no valid set of coefficients has been found: please supply starting
values
In addition: There were 11 warnings (use warnings() to see them)
> warnings()
Warning messages:
1: glm.fit: algorithm did not converge
2: glm.fit: fitted rates numerically 0 occurred
3: glm.fit: fitted rates numerically 0 occurred
4: glm.fit: fitted rates numerically 0 occurred
5: glm.fit: fitted rates numerically 0 occurred
6: glm.fit: fitted rates numerically 0 occurred
7: glm.fit: fitted rates numerically 0 occurred
8: glm.fit: fitted rates numerically 0 occurred
9: glm.fit: fitted rates numerically 0 occurred
10: glm.fit: fitted rates numerically 0 occurred
11: glm.fit: fitted rates numerically 0 occurred
```
Why these errors?
Is there a better tutorial on how to apply this function?
Thank you

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Tue May  5 13:17:31 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Tue, 5 May 2020 23:17:31 +1200
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
 <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
Message-ID: <CABcYAdLUqbfJJJyYnVh1xNODR4E-BS1+C0CcOj_-1YXGszsLqg@mail.gmail.com>

What do you mean "ANSI"?
Do you mean ASCII?  In that case there is nothing to be done.
Do you mean some member of the ISO 8859 family of 8-bit character sets?
Do you mean some Microsoft-specific code page, such as CP-1252?
(Microsoft CP-437 and CP-1252 "ANSI" but if they have any connection
whatever with ANSI I would appreciate being informed of it.)
If you really do mean the ANSI Extended Latin (ANSEL) character
set, you are out of luck.

If it is supported in your environment, the easiest way is that use the
iconv() function.  That's what it is for.  See ?iconv.

But there is something easier, and that is not to.
Just let R know what the external encoding is, and just read the file.
If you check the documentation of read.csv, ?read.csv
you will find the fileEncoding="..." argument.

fileEncoding: character string: if non-empty declares the encoding used
          on a file (not a connection) so the character data can be
          re-encoded.  See the 'Encoding' section of the help for
          'file', the 'R Data Import/Export Manual' and 'Note'.

At a guess, you  want fileEncoding="WINDOWS-1252".

On Tue, 5 May 2020 at 22:42, Mehdi Dadkhah <mehdidadkhah91 at gmail.com> wrote:
>
> Hi,
> I hope you are doing well!
> I have a CSV file which its encoding is ANSI. How can i change its encoding
> to UTF-8 in R?
> Many thanks!
> With best regards,
>
> --
> *Mehdi Dadkhah*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j@me@ @end|ng |rom j@@@oc@com  Mon May  4 20:48:58 2020
From: j@me@ @end|ng |rom j@@@oc@com (James Spottiswoode)
Date: Mon, 4 May 2020 11:48:58 -0700
Subject: [R] COVID-19 datasets...
In-Reply-To: <961343375.1407915.1588617072873@connect.xfinity.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
Message-ID: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>

Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:

https://github.com/CSSEGISandData/COVID-19

All in csv fiormat.


> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
> 
> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>  
> Thanks
> 
> 
> Bernard McGarvey
> 
> 
> Director, Fort Myers Beach Lions Foundation, Inc.
> 
> 
> Retired (Lilly Engineering Fellow).
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

James Spottiswoode
Applied Mathematics & Statistics
(310) 270 6220
jamesspottiswoode Skype
james at jsasoc.com




	[[alternative HTML version deleted]]


From h@yden@m@cdon@|d@8778 @end|ng |rom gm@||@com  Mon May  4 14:03:22 2020
From: h@yden@m@cdon@|d@8778 @end|ng |rom gm@||@com (Hayden MacDonald)
Date: Mon, 4 May 2020 08:03:22 -0400
Subject: [R] [R-pkgs] squashinformr: Politely web scrape data from
 SquashInfo in R
Message-ID: <CANTbTwDL04K6fsgVVzFgPmsMJdkZMSfuKHVW5WFrWVbU_aJ3Xw@mail.gmail.com>

Hi all,

I hope this message finds you well. I have developed a new R package,
squashinformr, that allows users to web scrape data on the Professional
Squash Association World Tour and other squash tournaments. Currently,
squashinformr provides functions for accessing data on players, rankings,
and tournaments. Additionally, squashinformr ethically scrapes data from
SquashInfo <http://www.squashinfo.com/> by adhering to `polite` principles
<https://github.com/dmi3kno/polite>. Version 0.1.2 is now available on CRAN!

Here is a blog post introducing the package and some of its uses:
https://needleinthehay.ca/introducing-squashinformr/

and here is the package's GitHub repository:
https://github.com/HaydenMacDonald/squashinformr

Best wishes,

Hayden MacDonald

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From bjoern@boettcher @end|ng |rom tu-dre@den@de  Mon May  4 11:04:06 2020
From: bjoern@boettcher @end|ng |rom tu-dre@den@de (=?UTF-8?Q?Bj=c3=b6rn_B=c3=b6ttcher?=)
Date: Mon, 4 May 2020 11:04:06 +0200
Subject: [R] [R-pkgs] multivariance: Measuring Multivariate Dependence Using
 Distance Multivariance
Message-ID: <47d83b4d-e077-5956-cb49-b252e72818d3@tu-dresden.de>

Dear R-users and developers,

based on a recent series of of papers [1-6] the package 'multivariance' 
(available on CRAN; latest version 2.3.0, 2020-04-23) was developed.
It provides in particular:

+ *fast global tests of independence* for an arbitrary number of 
variables of arbitrary dimensions

+ a detection and visualization algorithm for *higher order dependence 
structures*

+ estimators for multivariate dependence measures which *characterize 
independence*, i.e. the population version is 0 if and only if the 
variables are independent (in contrast to the standard correlation 'cor')

As a side remark, some food for thought: Note that in [3] it is referred 
to over 350 datasets from more than 150 R-packages, which all feature 
some statistical significant higher order dependencies. Some are 
probably artefacts, but in any case it is likely that these have been 
unnoticed and undiscussed so far. Moreover, since it was purely a brute 
force study, this might provide starting points for plenty of research 
by the corresponding field specialists.

Comments and questions on 'multivariance' and the underlying theory are 
welcome.

Best wishes

Bj?rn B?ttcher


References:

[1] B. B?ttcher, M. Keller-Ressel, R.L. Schilling, Detecting 
independence of random vectors: generalized distance covariance and 
Gaussian covariance.
Modern Stochastics: Theory and Applications, Vol. 5, No. 3 (2018) 353-383.
https://www.vmsta.org/journal/VMSTA/article/127/info

[2] B. B?ttcher, M. Keller-Ressel, R.L. Schilling, Distance 
multivariance: New dependence measures for random vectors.
The Annals of Statistics, Vol. 47, No. 5 (2019) 2757-2789.
https://projecteuclid.org/euclid.aos/1564797863

[3] B. B?ttcher, Dependence and Dependence Structures: Estimation and 
Visualization using the Unifying Concept of Distance Multivariance.
Open Statistics, Vol. 1, No. 1 (2020) 1-46.
https://doi.org/10.1515/stat-2020-0001

[4] G. Berschneider, B. B?ttcher, On complex Gaussian random fields, 
Gaussian quadratic forms and sample distance multivariance. Preprint.
https://arxiv.org/abs/1808.07280

[5] B. B?ttcher, Copula versions of distance multivariance and dHSIC via 
the distributional transform -- a general approach to construct 
invariant dependence measures.
Statistics, (2020) 1-18.
https://doi.org/10.1080/02331888.2020.1748029

[6] B. B?ttcher, Notes on the interpretation of dependence measures -- 
Pearson's correlation, distance correlation, distance multicorrelations 
and their copula versions. Preprint.
https://arxiv.org/abs/2004.07649


-- 
Dr. Bj?rn B?ttcher
TU Dresden
Institut f?r Math. Stochastik
D-01062 Dresden, Germany
Phone: +49 (0) 351 463 32423
Fax:   +49 (0) 351 463 37251
Web:   http://www.math.tu-dresden.de/~boettch/

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From cpoiw@rt m@iii@g oii chemo@org@uk  Tue May  5 22:11:03 2020
From: cpoiw@rt m@iii@g oii chemo@org@uk (cpoiw@rt m@iii@g oii chemo@org@uk)
Date: Tue, 05 May 2020 21:11:03 +0100
Subject: [R] How to use pakcage R0
In-Reply-To: <CAMk+s2QuwOdyykoivk0uyzFdynP-0M5pdvJG5iqCSsxOeQ6-oQ@mail.gmail.com>
References: <CAMk+s2T-X8wXp+owCj3E1Cmez-imgE-fByNLNY_XDibHXOgPqQ@mail.gmail.com>
 <cb4dd3eb-6944-4676-b1e9-524f90568eed@email.android.com>
 <CAMk+s2QWUTcm52=vd75qK+C-QeonjUcsERgZXb=eUAKX_GeMdw@mail.gmail.com>
 <CAMk+s2QuwOdyykoivk0uyzFdynP-0M5pdvJG5iqCSsxOeQ6-oQ@mail.gmail.com>
Message-ID: <85590efbf776b8ff52dba0bdf245ba3a@chemo.org.uk>


>> R0 = estimate.R(germany_vect, mGT, begin=germany_vect[1],
> end=germany_vect[length(germany_vect)], methods="EG", pop.size=pop_de,
> nsim=100)
> 
> Error in begin.nb:end.nb : argument of length 0
> 
>> germany_vect[1]
>   1
> 184
>> germany_vect[length(germany_vect)]
>  57
> 488
> 
> ```
> What might be the problem here?

begin = germany_vect[1]
So begin = 184

but do you not want begin = 1

and same for end?


From ch@r|e@|ehnen @end|ng |rom gm@||@com  Tue May  5 17:25:42 2020
From: ch@r|e@|ehnen @end|ng |rom gm@||@com (Charles Lehnen)
Date: Tue, 5 May 2020 10:25:42 -0500
Subject: [R] Question about combining foodwebs and phylogenetic trees
Message-ID: <CAA560r57hoQHe9x8psci8NaYhoLNGzsd9WQ0PY1MUFf4yNnYAA@mail.gmail.com>

I have trying to combine foodweb outputs like the bipartite package's
plotweb() function of bipartiteD3?s bipartite_D3 function with phylogenetic
trees, similar to a tanglegram. Because of the very large size and a high
amount of variability in my dataset, standard tanglegrams turn out very
convoluted, but the plotweb() outputs are still lovely.

I was able to export tips to manually order the tips of the plotweb()
output to match the order of the phylogenetic tree tips which allowed me to
align tips manually in Inkscape, but this proved extremely time consuming
whenever I made an addition to my dataset.

require('ape')

tree1<-read.tree(text="((A,(B,(C,D))),E);")
tree1<-ladderize(tree1, right = FALSE)

tree2<-read.tree(text="(F,(G,((H,I),(J,K))));")

is_tip <- tree1$edge[,2] <= length(tree1$tip.label)
ordered_tips <- tree1$edge[is_tip, 2]
tree1tips<-tree1$tip.label[ordered_tips]

is_tip <- tree2$edge[,2] <= length(tree2$tip.label)
ordered_tips <- tree2$edge[is_tip, 2]
tree2tips<-tree2$tip.label[ordered_tips]


I tried to edit the plotweb() script to accept phylo class variables as an
additional argument, but that was evidently beyond my abilities at this
time.

I also tried combing the outputs using the grid package, we were able to
visually combine outputs next to one another and match the order of the
tips. However, I have not been able to figure out how to actually line up
the tips of the trees to the outputs of plotweb(). This becomes very
evident with my actual, very large dataset

require('ape')
require('bipartite')
require('ggplotify')
require('cowplot')
require('grid')

tree1<-read.tree(text="((A,(B,(C,D))),E);")
tree1<-ladderize(tree1, right = FALSE)

tree2<-read.tree(text="(F,(G,((H,I),(J,K))));")

bipartite<-cbind(c(0,2,3,2,0,0),c(2,0,2,4,8,0),c(4,3,0,0,5,0),c(0,2,0,0,0,1),c(0,7,2,2,0,0))
colnames(bipartite)<-c("D","C","B","A","E")
rownames(bipartite)<-c("K","J","I","H","G","F")
bipartite<-as.data.frame(bipartite)

p12 = as.grob(~cophyloplot(tree1, tree2))
bipartite = as.data.frame(t(bipartite))
p3 = as.grob(~plotweb(bipartite,
        method = "normal",
        empty = "false",
        text.rot = "90"
        ))
grid.newpage()
grid.draw(p12)
vp = viewport(x = 0.53, y = 0.6, width = 0.6, height = 0.8, angle = -90)
pushViewport(vp)
grid.draw(p3)


If anyone could direct me on how to proceed, I would greatly appreciate it!
I have been coming back to this problem for many months now and have not
been to solve it

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed May  6 00:01:34 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 5 May 2020 15:01:34 -0700
Subject: [R] Question about combining foodwebs and phylogenetic trees
In-Reply-To: <CAA560r57hoQHe9x8psci8NaYhoLNGzsd9WQ0PY1MUFf4yNnYAA@mail.gmail.com>
References: <CAA560r57hoQHe9x8psci8NaYhoLNGzsd9WQ0PY1MUFf4yNnYAA@mail.gmail.com>
Message-ID: <CAGxFJbR7pzTvL-SXjG1T2eJrtaB4mzPPX1rPwunPe+Cj76zR1Q@mail.gmail.com>

I think it unlikely that you'll get such specific help here.
Try posting on:
R-SIG-phylo: R SIG on phylogenetic and comparative methods and analyses
instead.

(I also assume you are aware of:
https://CRAN.R-project.org/view=Phylogenetics  ,
but I have no idea whether it is helpful).


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, May 5, 2020 at 2:12 PM Charles Lehnen <charleslehnen at gmail.com> wrote:
>
> I have trying to combine foodweb outputs like the bipartite package's
> plotweb() function of bipartiteD3?s bipartite_D3 function with phylogenetic
> trees, similar to a tanglegram. Because of the very large size and a high
> amount of variability in my dataset, standard tanglegrams turn out very
> convoluted, but the plotweb() outputs are still lovely.
>
> I was able to export tips to manually order the tips of the plotweb()
> output to match the order of the phylogenetic tree tips which allowed me to
> align tips manually in Inkscape, but this proved extremely time consuming
> whenever I made an addition to my dataset.
>
> require('ape')
>
> tree1<-read.tree(text="((A,(B,(C,D))),E);")
> tree1<-ladderize(tree1, right = FALSE)
>
> tree2<-read.tree(text="(F,(G,((H,I),(J,K))));")
>
> is_tip <- tree1$edge[,2] <= length(tree1$tip.label)
> ordered_tips <- tree1$edge[is_tip, 2]
> tree1tips<-tree1$tip.label[ordered_tips]
>
> is_tip <- tree2$edge[,2] <= length(tree2$tip.label)
> ordered_tips <- tree2$edge[is_tip, 2]
> tree2tips<-tree2$tip.label[ordered_tips]
>
>
> I tried to edit the plotweb() script to accept phylo class variables as an
> additional argument, but that was evidently beyond my abilities at this
> time.
>
> I also tried combing the outputs using the grid package, we were able to
> visually combine outputs next to one another and match the order of the
> tips. However, I have not been able to figure out how to actually line up
> the tips of the trees to the outputs of plotweb(). This becomes very
> evident with my actual, very large dataset
>
> require('ape')
> require('bipartite')
> require('ggplotify')
> require('cowplot')
> require('grid')
>
> tree1<-read.tree(text="((A,(B,(C,D))),E);")
> tree1<-ladderize(tree1, right = FALSE)
>
> tree2<-read.tree(text="(F,(G,((H,I),(J,K))));")
>
> bipartite<-cbind(c(0,2,3,2,0,0),c(2,0,2,4,8,0),c(4,3,0,0,5,0),c(0,2,0,0,0,1),c(0,7,2,2,0,0))
> colnames(bipartite)<-c("D","C","B","A","E")
> rownames(bipartite)<-c("K","J","I","H","G","F")
> bipartite<-as.data.frame(bipartite)
>
> p12 = as.grob(~cophyloplot(tree1, tree2))
> bipartite = as.data.frame(t(bipartite))
> p3 = as.grob(~plotweb(bipartite,
>         method = "normal",
>         empty = "false",
>         text.rot = "90"
>         ))
> grid.newpage()
> grid.draw(p12)
> vp = viewport(x = 0.53, y = 0.6, width = 0.6, height = 0.8, angle = -90)
> pushViewport(vp)
> grid.draw(p3)
>
>
> If anyone could direct me on how to proceed, I would greatly appreciate it!
> I have been coming back to this problem for many months now and have not
> been to solve it
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Wed May  6 06:29:15 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Wed, 6 May 2020 08:59:15 +0430
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <20200505105657.GA1073515@posteo.no>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
 <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
 <20200505105657.GA1073515@posteo.no>
Message-ID: <CAGN=ytOrCUr_=1J8yNUCH8ee+gLQhK-SXjyTN5J3srapxaeszQ@mail.gmail.com>

Thank you!
it works for me.
With best regards,

On Tue, May 5, 2020 at 3:27 PM Rasmus Liland <jral at posteo.no> wrote:

> On 2020-05-05 15:12 +0430, Mehdi Dadkhah wrote:
> > I have a CSV file which its encoding is
> > ANSI. How can i change its encoding to
> > UTF-8 in R?
>
> Hi!
>
> I do not know about ANSI, but to read latin1
> encoded csv files into readr, do this:
>
> Determine that your file is latin1-encoded:
>
>         rasmus at twentyfive ~ % file -i SAA.csv
>         SAA.csv: application/csv; charset=iso-8859-1
>
> read it in using readr::read_csv
>
>         locale <- readr::locale(encoding = "latin1")
>         SAA <- suppressMessages(
>           readr::read_csv(file="SAA.csv",
>                           locale=locale))
>
> Best,
> Rasmus
>


-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Wed May  6 06:29:39 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Wed, 6 May 2020 08:59:39 +0430
Subject: [R] Converting CSV file to UTF-8 encoding
In-Reply-To: <CABcYAdLUqbfJJJyYnVh1xNODR4E-BS1+C0CcOj_-1YXGszsLqg@mail.gmail.com>
References: <CAGN=ytO6qi4M63X7DucyfCKz8HNNbT_SzrwALQU4cRgrPgeVeg@mail.gmail.com>
 <CAGN=ytPxB8xW62BuGxute_62cYE_1mW9oEK3WjOSnHQuXwcAkA@mail.gmail.com>
 <CABcYAdLUqbfJJJyYnVh1xNODR4E-BS1+C0CcOj_-1YXGszsLqg@mail.gmail.com>
Message-ID: <CAGN=ytMg33kOKjYYd6B-WdKtgco0Mf+qOg1zW_69D_xigQ5sAw@mail.gmail.com>

Thank you!
With best regards,

On Tue, May 5, 2020 at 3:47 PM Richard O'Keefe <raoknz at gmail.com> wrote:

> What do you mean "ANSI"?
> Do you mean ASCII?  In that case there is nothing to be done.
> Do you mean some member of the ISO 8859 family of 8-bit character sets?
> Do you mean some Microsoft-specific code page, such as CP-1252?
> (Microsoft CP-437 and CP-1252 "ANSI" but if they have any connection
> whatever with ANSI I would appreciate being informed of it.)
> If you really do mean the ANSI Extended Latin (ANSEL) character
> set, you are out of luck.
>
> If it is supported in your environment, the easiest way is that use the
> iconv() function.  That's what it is for.  See ?iconv.
>
> But there is something easier, and that is not to.
> Just let R know what the external encoding is, and just read the file.
> If you check the documentation of read.csv, ?read.csv
> you will find the fileEncoding="..." argument.
>
> fileEncoding: character string: if non-empty declares the encoding used
>           on a file (not a connection) so the character data can be
>           re-encoded.  See the 'Encoding' section of the help for
>           'file', the 'R Data Import/Export Manual' and 'Note'.
>
> At a guess, you  want fileEncoding="WINDOWS-1252".
>
> On Tue, 5 May 2020 at 22:42, Mehdi Dadkhah <mehdidadkhah91 at gmail.com>
> wrote:
> >
> > Hi,
> > I hope you are doing well!
> > I have a CSV file which its encoding is ANSI. How can i change its
> encoding
> > to UTF-8 in R?
> > Many thanks!
> > With best regards,
> >
> > --
> > *Mehdi Dadkhah*
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
*Mehdi Dadkhah*
PhD candidate & Research assistant
Department of Management, Faculty of Economics and Administrative Sciences,
Ferdowsi University of Mashhad, Mashhad, Iran
*Email Addresses:*
mehdidadkhah91 at gmail.com
Mehdidadkhah at mail.um.ac.ir

	[[alternative HTML version deleted]]


From g@@@powe|| @end|ng |rom protonm@||@com  Wed May  6 06:50:17 2020
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg)
Date: Wed, 06 May 2020 04:50:17 +0000
Subject: [R] How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
Message-ID: <b9UMGduMidxZZVV4YyGRRf4-zyj0dJkZjBd3W-XEybdWx0Kj6Rr58s2Pjqe769m0xP4ARLI4LOUEFYQrLxuCtRM3qNS0CBYepMx_mSFoTSg=@protonmail.com>




I have a data table titled: "dt_count" - it contains:

> "","STATUS","N"
> "1","Resolved",650
> "2","Assigned",135
> "3","Closed",530
> "4","In Progress",56
> "5","Pending",75
> "6","Cancelled",20
> 

> I need to change the "dt_count" data table to a new data table that looks like this:
> 

> "","STATUS","N"
> "1","Resolved/Closed",1180
> "2","Assigned",135
> "3","In Progress",56
> "4","Pending",75
> "5","Cancelled",20
> 

> Or, to state the question:
> 

> I need to combine the "Resolved" Row with the "Closed" Row, into a Third new row titled "Resolved/Closed", whereby the "N" ticket count in each of the "Resolved" row and the "Closed" row are added together in the third new?"Resolved/Closed" - also, would need the old?"Resolved" Row with the "Closed" Rows to go away.
> 

> To complicate the issue, the rows in the "dt_count" data table when they are output, are not always in the same order.
> 

> I have the data.table library is installed.
> 

> I'm thinking there is a very easy way to do this... but I am not finding it. I've spent several hours searching thru among other things? - several data table cheatsheets, and I've also read thru this:
> https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
> 

> Just can't sort it out. Just started using R a few weeks ago.
> 

> Any help would be so very much appreciated!
> 

> Thanks.
> 

> Gregg
> AZ, USA
> 



-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 477 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200506/83cb4840/attachment.sig>

From g@@@powe|| @end|ng |rom protonm@||@com  Wed May  6 06:41:53 2020
From: g@@@powe|| @end|ng |rom protonm@||@com (Gregg)
Date: Wed, 06 May 2020 04:41:53 +0000
Subject: [R] How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
Message-ID: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>

If I have a data table that is essentially output titled: "dt_count" - it contains:

"","STATUS","N"
"1","Resolved",650
"2","Assigned",135
"3","Closed",530
"4","In Progress",56
"5","Pending",75
"6","Cancelled",20

Need to change the "dt_count" data table to a new data table that looks like this:

"","STATUS","N"
"1","Resolved/Closed",1180
"2","Assigned",135
"3","In Progress",56
"4","Pending",75
"5","Cancelled",20

Or, to state the question:

I need to combine the "Resolved" Row with the "Closed" Row, into a Third new row titled "Resolved/Closed", whereby the "N" ticket count in each of the "Resolved" row and the "Closed" row are added together in the third new?"Resolved/Closed" - also, would need the old?"Resolved" Row with the "Closed" Rows to go away.

To complicate the issue, the rows in the "dt_count" data table when they are output, are not always in the same order.

I have the data.table library is installed.

I'm thinking there is a very easy way to do this... but I am not finding it. I've search thru several data table cheatsheets, and I've also read thru this:
https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html

Just can't sort it out. Just started using R a few weeks ago.

Any help would be so very much appreciated!

Thanks.

Gregg
AZ, USA
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 477 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200506/8fddece8/attachment.sig>

From m@||||@t@ @end|ng |rom pp@|net@||  Wed May  6 11:09:42 2020
From: m@||||@t@ @end|ng |rom pp@|net@|| (K. Elo)
Date: Wed, 06 May 2020 12:09:42 +0300
Subject: [R] 
 How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
In-Reply-To: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
References: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
Message-ID: <d33c96e67e0cf3f437fe9838dbbbadd3a04e059e.camel@pp.inet.fi>

Hi!

With 'dplyr':

dt_count %>% mutate(STATUS=ifelse(STATUS %in%
c("Resolved","Closed"),"Resolved/Closed",STATUS)) %>% group_by(STATUS)
%>% summarise(n=sum(N))

Output:

1 Assigned          135
2 Cancelled          20
3 In Progress        56
4 Pending            75
5 Resolved/Closed  1180

HTH,
Kimmo

2020-05-06, 04:41 +0000, Gregg via R-help wrote:
> If I have a data table that is essentially output titled: "dt_count"
> - it contains:
> 
> "","STATUS","N"
> "1","Resolved",650
> "2","Assigned",135
> "3","Closed",530
> "4","In Progress",56
> "5","Pending",75
> "6","Cancelled",20
> 
> Need to change the "dt_count" data table to a new data table that
> looks like this:
> 
> "","STATUS","N"
> "1","Resolved/Closed",1180
> "2","Assigned",135
> "3","In Progress",56
> "4","Pending",75
> "5","Cancelled",20
> 
> Or, to state the question:
> 
> I need to combine the "Resolved" Row with the "Closed" Row, into a
> Third new row titled "Resolved/Closed", whereby the "N" ticket count
> in each of the "Resolved" row and the "Closed" row are added together
> in the third new "Resolved/Closed" - also, would need the
> old "Resolved" Row with the "Closed" Rows to go away.
> 
> To complicate the issue, the rows in the "dt_count" data table when
> they are output, are not always in the same order.
> 
> I have the data.table library is installed.
> 
> I'm thinking there is a very easy way to do this... but I am not
> finding it. I've search thru several data table cheatsheets, and I've
> also read thru this:
> 
https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
> 
> Just can't sort it out. Just started using R a few weeks ago.
> 
> Any help would be so very much appreciated!
> 
> Thanks.
> 
> Gregg
> AZ, USA
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Wed May  6 11:13:22 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 6 May 2020 09:13:22 +0000
Subject: [R] 
 How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
In-Reply-To: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
References: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
Message-ID: <220eb24467d947438526d4961cff8e15@SRVEXCHCM1302.precheza.cz>

Hi

Maybe aggregate?

1. Make your column STATUS a factor
2. Combine levels Resolved and Closed to one common factor named 
Reslolved/Closed
3. aggregate according to new STATUS

temp <- read.table("clipboard", sep=",", header=T)
temp$STATUS
[1] "Resolved"    "Assigned"    "Closed"      "In Progress" "Pending"
[6] "Cancelled"
temp$STATUS <- factor(temp$STATUS)
levels(temp$STATUS)
[1] "Assigned"    "Cancelled"   "Closed"      "In Progress" "Pending"
[6] "Resolved"
levels(temp$STATUS)[c(3,6)] <- "Resolved/Closed"
aggregate(temp$N, list(temp$STATUS), sum)
          Group.1    x
1        Assigned  135
2       Cancelled   20
3 Resolved/Closed 1180
4     In Progress   56
5         Pending   75

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Gregg via R-help
> Sent: Wednesday, May 6, 2020 6:42 AM
> To: r-help at r-project.org
> Subject: [R] How to combine two rows in a data table into a third new row,
> such that the values in the row are added together in the new row?
>
> If I have a data table that is essentially output titled: "dt_count" - it 
> contains:
>
> "","STATUS","N"
> "1","Resolved",650
> "2","Assigned",135
> "3","Closed",530
> "4","In Progress",56
> "5","Pending",75
> "6","Cancelled",20
>
> Need to change the "dt_count" data table to a new data table that looks like
> this:
>
> "","STATUS","N"
> "1","Resolved/Closed",1180
> "2","Assigned",135
> "3","In Progress",56
> "4","Pending",75
> "5","Cancelled",20
>
> Or, to state the question:
>
> I need to combine the "Resolved" Row with the "Closed" Row, into a Third
> new row titled "Resolved/Closed", whereby the "N" ticket count in each of 
> the
> "Resolved" row and the "Closed" row are added together in the third
> new "Resolved/Closed" - also, would need the old "Resolved" Row with the
> "Closed" Rows to go away.
>
> To complicate the issue, the rows in the "dt_count" data table when they are
> output, are not always in the same order.
>
> I have the data.table library is installed.
>
> I'm thinking there is a very easy way to do this... but I am not finding it. 
> I've
> search thru several data table cheatsheets, and I've also read thru this:
> https://cran.r-project.org/web/packages/data.table/vignettes/datatable-
> intro.html
>
> Just can't sort it out. Just started using R a few weeks ago.
>
> Any help would be so very much appreciated!
>
> Thanks.
>
> Gregg
> AZ, USA

From p_conno||y @end|ng |rom @||ng@hot@co@nz  Wed May  6 12:15:07 2020
From: p_conno||y @end|ng |rom @||ng@hot@co@nz (Patrick Connolly)
Date: Wed, 6 May 2020 22:15:07 +1200
Subject: [R] PCRE configure problem with R-4.0.0
In-Reply-To: <CA+vqiLE1rjP6AtgUNkxVZdWX4D4-GTRB_104fP7qh8SzCeFn6Q@mail.gmail.com>
References: <20200504041542.GA7518@slingshot.co.nz>
 <20200504100010.5d771a78@trisector>
 <CA+vqiLEcbaU=Wm7dF=eEdygavtBRx1oxSaR8FyjX16GyBrYj0Q@mail.gmail.com>
 <20200505083717.GB7518@slingshot.co.nz>
 <CA+vqiLE1rjP6AtgUNkxVZdWX4D4-GTRB_104fP7qh8SzCeFn6Q@mail.gmail.com>
Message-ID: <20200506101507.GC7518@slingshot.co.nz>

On Tue, 05-May-2020 at 06:28AM -0400, Ista Zahn wrote:

|> <snip>
|> 
|> > |> > Linux Mint 17.2 is based on Ubuntu 14.04, which has been released in
|> > |> > April 2014, while PCRE2 has been released in 2015.
|> > |>
|> > |> Moreover, support for 17.2 ended over a year ago (according to
|> > |> https://en.wikipedia.org/wiki/Linux_Mint_version_history). I suggest
|> > |> upgrading to a supported version.
|> >
|> > Thanks for making that clear -- though I don't relish the hassle of
|> > upgrading an OS.  I was quite happy with the features of Mint 17.x.
|> >
|> 
|> Sure, but there is no free lunch and as your system gets older and
|> older you'll find that more and more modern software doesn't work with
|> it. Additionally there are increasing security risks because the
|> vendor is no longer releasing security patches.
|> 
|> That said, if you really really wanna, you can use
|> https://docs.conda.io/en/latest/, https://spack.io/, or similar to
|> install recent software releases on old systems.

Thanks.  Condo is a new one to me.  



|> 
|> Best,
|> Ista
|> 
|> > [...]
|> >
|> >
|> > --
|> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
|> >    ___    Patrick Connolly
|> >  {~._.~}                   Great minds discuss ideas
|> >  _( Y )_                 Average minds discuss events
|> > (:_~*~_:)                  Small minds discuss people
|> >  (_)-(_)                              ..... Eleanor Roosevelt
|> >
|> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 15:28:11 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 08:28:11 -0500
Subject: [R] calculating t-score/t-stats as my zscores
Message-ID: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>

Hello,

Can I apply the quantile function qt() this way?
qt(pvals/2, 406-34, lower.tail = F)
to get the T-scores?

Thanks
Ama


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 16:27:20 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 15:27:20 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
Message-ID: <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>

Hello,

That gives the *absolute* t-scores. If it's all you need/want, then the 
answer is yes, you can.

Hope this helps,

Rui Barradas

?s 14:28 de 06/05/20, Ana Marija escreveu:
> Hello,
> 
> Can I apply the quantile function qt() this way?
> qt(pvals/2, 406-34, lower.tail = F)
> to get the T-scores?
> 
> Thanks
> Ama
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:31:38 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:31:38 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
Message-ID: <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>

Hi Rui,

Thank you for getting back to me. Is there is a better way to
calculate Z scores if I have p values, SE and Beta?

Thanks
Ana

On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> That gives the *absolute* t-scores. If it's all you need/want, then the
> answer is yes, you can.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> > Hello,
> >
> > Can I apply the quantile function qt() this way?
> > qt(pvals/2, 406-34, lower.tail = F)
> > to get the T-scores?
> >
> > Thanks
> > Ama
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 16:33:39 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 15:33:39 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
Message-ID: <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>

Hello,

Sorry but after reading my answer I believe it's not completely clear.

I meant absolute values, the actual t-scores as computed from the data 
might be negative. Your code will always produce positive numbers.

Hope this helps,

Rui Barradas

?s 15:27 de 06/05/20, Rui Barradas escreveu:
> Hello,
> 
> That gives the *absolute* t-scores. If it's all you need/want, then the 
> answer is yes, you can.
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 14:28 de 06/05/20, Ana Marija escreveu:
>> Hello,
>>
>> Can I apply the quantile function qt() this way?
>> qt(pvals/2, 406-34, lower.tail = F)
>> to get the T-scores?
>>
>> Thanks
>> Ama
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:37:23 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:37:23 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>
Message-ID: <CAF9-5jNtfNGE-DPjHfXTX_yOAPepkwhXbJBSAYi21XGpGotrhw@mail.gmail.com>

thanks, can you please tell em what would be the way not to get the
absolute (always positive values)

On Wed, May 6, 2020 at 9:33 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> Sorry but after reading my answer I believe it's not completely clear.
>
> I meant absolute values, the actual t-scores as computed from the data
> might be negative. Your code will always produce positive numbers.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 15:27 de 06/05/20, Rui Barradas escreveu:
> > Hello,
> >
> > That gives the *absolute* t-scores. If it's all you need/want, then the
> > answer is yes, you can.
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 14:28 de 06/05/20, Ana Marija escreveu:
> >> Hello,
> >>
> >> Can I apply the quantile function qt() this way?
> >> qt(pvals/2, 406-34, lower.tail = F)
> >> to get the T-scores?
> >>
> >> Thanks
> >> Ama
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:40:51 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:40:51 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jNtfNGE-DPjHfXTX_yOAPepkwhXbJBSAYi21XGpGotrhw@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>
 <CAF9-5jNtfNGE-DPjHfXTX_yOAPepkwhXbJBSAYi21XGpGotrhw@mail.gmail.com>
Message-ID: <CAF9-5jMsxZPpBzwt1Nt5RVhfcuDdWpEEaW+iBP7cCATRs+v22g@mail.gmail.com>

I guess I can have

z-score=Beta/StdErr

On Wed, May 6, 2020 at 9:37 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> thanks, can you please tell em what would be the way not to get the
> absolute (always positive values)
>
> On Wed, May 6, 2020 at 9:33 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >
> > Hello,
> >
> > Sorry but after reading my answer I believe it's not completely clear.
> >
> > I meant absolute values, the actual t-scores as computed from the data
> > might be negative. Your code will always produce positive numbers.
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 15:27 de 06/05/20, Rui Barradas escreveu:
> > > Hello,
> > >
> > > That gives the *absolute* t-scores. If it's all you need/want, then the
> > > answer is yes, you can.
> > >
> > > Hope this helps,
> > >
> > > Rui Barradas
> > >
> > > ?s 14:28 de 06/05/20, Ana Marija escreveu:
> > >> Hello,
> > >>
> > >> Can I apply the quantile function qt() this way?
> > >> qt(pvals/2, 406-34, lower.tail = F)
> > >> to get the T-scores?
> > >>
> > >> Thanks
> > >> Ama
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > >> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 16:41:09 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 15:41:09 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
Message-ID: <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>

Hello,

By z-scores do you mean function help('scale')?

Hope this helps,

Rui Barradas

?s 15:31 de 06/05/20, Ana Marija escreveu:
> Hi Rui,
> 
> Thank you for getting back to me. Is there is a better way to
> calculate Z scores if I have p values, SE and Beta?
> 
> Thanks
> Ana
> 
> On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>
>> Hello,
>>
>> That gives the *absolute* t-scores. If it's all you need/want, then the
>> answer is yes, you can.
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 14:28 de 06/05/20, Ana Marija escreveu:
>>> Hello,
>>>
>>> Can I apply the quantile function qt() this way?
>>> qt(pvals/2, 406-34, lower.tail = F)
>>> to get the T-scores?
>>>
>>> Thanks
>>> Ama
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:49:14 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:49:14 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
Message-ID: <CAF9-5jP2zYyT45tH0ZnCDBWQbQND-ZBQLKL-6+Uq8U81=Qnc2A@mail.gmail.com>

as defined here:
https://huwenboshi.github.io/data%20management/2017/11/23/tips-for-formatting-gwas-summary-stats.html

where Effect size is Beta

On Wed, May 6, 2020 at 9:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> By z-scores do you mean function help('scale')?
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 15:31 de 06/05/20, Ana Marija escreveu:
> > Hi Rui,
> >
> > Thank you for getting back to me. Is there is a better way to
> > calculate Z scores if I have p values, SE and Beta?
> >
> > Thanks
> > Ana
> >
> > On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>
> >> Hello,
> >>
> >> That gives the *absolute* t-scores. If it's all you need/want, then the
> >> answer is yes, you can.
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> >>> Hello,
> >>>
> >>> Can I apply the quantile function qt() this way?
> >>> qt(pvals/2, 406-34, lower.tail = F)
> >>> to get the T-scores?
> >>>
> >>> Thanks
> >>> Ama
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Wed May  6 16:51:50 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Wed, 6 May 2020 10:51:50 -0400
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
Message-ID: <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>

Guessing for Ana, but no, that's a different meaning. Beta/StdErr is a
z statistic--a test statistic against (usually) the tails of the unit
normal distribution. So like a t-test with infinite df.


On Wed, May 6, 2020 at 10:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> By z-scores do you mean function help('scale')?
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 15:31 de 06/05/20, Ana Marija escreveu:
> > Hi Rui,
> >
> > Thank you for getting back to me. Is there is a better way to
> > calculate Z scores if I have p values, SE and Beta?
> >
> > Thanks
> > Ana
> >
> > On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>
> >> Hello,
> >>
> >> That gives the *absolute* t-scores. If it's all you need/want, then the
> >> answer is yes, you can.
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> >>> Hello,
> >>>
> >>> Can I apply the quantile function qt() this way?
> >>> qt(pvals/2, 406-34, lower.tail = F)
> >>> to get the T-scores?
> >>>
> >>> Thanks
> >>> Ama
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 16:54:03 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 09:54:03 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
 <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>
Message-ID: <CAF9-5jMr7SWF6BeW7NvNuaMdPGUPDd8DUiGsH4bMAGoo=0DmEw@mail.gmail.com>

Thanks Patrick, so in conclusion this is fine?
z-score=Beta/StdErr

On Wed, May 6, 2020 at 9:52 AM Patrick (Malone Quantitative)
<malone at malonequantitative.com> wrote:
>
> Guessing for Ana, but no, that's a different meaning. Beta/StdErr is a
> z statistic--a test statistic against (usually) the tails of the unit
> normal distribution. So like a t-test with infinite df.
>
>
> On Wed, May 6, 2020 at 10:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >
> > Hello,
> >
> > By z-scores do you mean function help('scale')?
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 15:31 de 06/05/20, Ana Marija escreveu:
> > > Hi Rui,
> > >
> > > Thank you for getting back to me. Is there is a better way to
> > > calculate Z scores if I have p values, SE and Beta?
> > >
> > > Thanks
> > > Ana
> > >
> > > On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> > >>
> > >> Hello,
> > >>
> > >> That gives the *absolute* t-scores. If it's all you need/want, then the
> > >> answer is yes, you can.
> > >>
> > >> Hope this helps,
> > >>
> > >> Rui Barradas
> > >>
> > >> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> > >>> Hello,
> > >>>
> > >>> Can I apply the quantile function qt() this way?
> > >>> qt(pvals/2, 406-34, lower.tail = F)
> > >>> to get the T-scores?
> > >>>
> > >>> Thanks
> > >>> Ama
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Patrick S. Malone, Ph.D., Malone Quantitative
> NEW Service Models: http://malonequantitative.com
>
> He/Him/His


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 16:54:43 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 15:54:43 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jMsxZPpBzwt1Nt5RVhfcuDdWpEEaW+iBP7cCATRs+v22g@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <4fd55d5a-8028-70e3-5130-b9502204ba4c@sapo.pt>
 <CAF9-5jNtfNGE-DPjHfXTX_yOAPepkwhXbJBSAYi21XGpGotrhw@mail.gmail.com>
 <CAF9-5jMsxZPpBzwt1Nt5RVhfcuDdWpEEaW+iBP7cCATRs+v22g@mail.gmail.com>
Message-ID: <bd18b8bf-fcc7-844d-8544-898390df77b9@sapo.pt>

Hello,

You can write a function to compute the scores:


z_score <- function(x, beta = mean, beta0 = 0){
   beta <- match.fun(beta)
   n <- length(x)
   score <- sqrt(n)*(beta(x) - beta0)/sd(x)
   names(score) <- if(n < 30) "t.score" else "z.score"
   score
}

# data example
x <- rt(20, df = 1)
tt <- t.test(x)

pvals <- tt$p.value

# Now compare these 3 results and see if it answers the question

qt(pvals/2, 19, lower.tail = F)
tt$statistic
z_score(x)


Hope this helps,

Rui Barradas

?s 15:40 de 06/05/20, Ana Marija escreveu:
> I guess I can have
> 
> z-score=Beta/StdErr
> 
> On Wed, May 6, 2020 at 9:37 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>
>> thanks, can you please tell em what would be the way not to get the
>> absolute (always positive values)
>>
>> On Wed, May 6, 2020 at 9:33 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>>
>>> Hello,
>>>
>>> Sorry but after reading my answer I believe it's not completely clear.
>>>
>>> I meant absolute values, the actual t-scores as computed from the data
>>> might be negative. Your code will always produce positive numbers.
>>>
>>> Hope this helps,
>>>
>>> Rui Barradas
>>>
>>> ?s 15:27 de 06/05/20, Rui Barradas escreveu:
>>>> Hello,
>>>>
>>>> That gives the *absolute* t-scores. If it's all you need/want, then the
>>>> answer is yes, you can.
>>>>
>>>> Hope this helps,
>>>>
>>>> Rui Barradas
>>>>
>>>> ?s 14:28 de 06/05/20, Ana Marija escreveu:
>>>>> Hello,
>>>>>
>>>>> Can I apply the quantile function qt() this way?
>>>>> qt(pvals/2, 406-34, lower.tail = F)
>>>>> to get the T-scores?
>>>>>
>>>>> Thanks
>>>>> Ama
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed May  6 17:05:20 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 6 May 2020 16:05:20 +0100
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <CAF9-5jMr7SWF6BeW7NvNuaMdPGUPDd8DUiGsH4bMAGoo=0DmEw@mail.gmail.com>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
 <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>
 <CAF9-5jMr7SWF6BeW7NvNuaMdPGUPDd8DUiGsH4bMAGoo=0DmEw@mail.gmail.com>
Message-ID: <c24c800e-a16b-a28d-fe23-43c3dcc07980@sapo.pt>

Hello,

Another option is to use stats::t.test

t.test(x)$statistic

Or, if you want to test against a beta0 != 0,

t.test(x, mu = beta0)$statistic


But in this case the estimator is the estimator for the mean value.

Hope this helps,

Rui Barradas

?s 15:54 de 06/05/20, Ana Marija escreveu:
> Thanks Patrick, so in conclusion this is fine?
> z-score=Beta/StdErr
> 
> On Wed, May 6, 2020 at 9:52 AM Patrick (Malone Quantitative)
> <malone at malonequantitative.com> wrote:
>>
>> Guessing for Ana, but no, that's a different meaning. Beta/StdErr is a
>> z statistic--a test statistic against (usually) the tails of the unit
>> normal distribution. So like a t-test with infinite df.
>>
>>
>> On Wed, May 6, 2020 at 10:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>>
>>> Hello,
>>>
>>> By z-scores do you mean function help('scale')?
>>>
>>> Hope this helps,
>>>
>>> Rui Barradas
>>>
>>> ?s 15:31 de 06/05/20, Ana Marija escreveu:
>>>> Hi Rui,
>>>>
>>>> Thank you for getting back to me. Is there is a better way to
>>>> calculate Z scores if I have p values, SE and Beta?
>>>>
>>>> Thanks
>>>> Ana
>>>>
>>>> On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>>>>
>>>>> Hello,
>>>>>
>>>>> That gives the *absolute* t-scores. If it's all you need/want, then the
>>>>> answer is yes, you can.
>>>>>
>>>>> Hope this helps,
>>>>>
>>>>> Rui Barradas
>>>>>
>>>>> ?s 14:28 de 06/05/20, Ana Marija escreveu:
>>>>>> Hello,
>>>>>>
>>>>>> Can I apply the quantile function qt() this way?
>>>>>> qt(pvals/2, 406-34, lower.tail = F)
>>>>>> to get the T-scores?
>>>>>>
>>>>>> Thanks
>>>>>> Ama
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Patrick S. Malone, Ph.D., Malone Quantitative
>> NEW Service Models: http://malonequantitative.com
>>
>> He/Him/His


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed May  6 18:02:31 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 May 2020 11:02:31 -0500
Subject: [R] calculating t-score/t-stats as my zscores
In-Reply-To: <c24c800e-a16b-a28d-fe23-43c3dcc07980@sapo.pt>
References: <CAF9-5jNKgvF2Mkkj53BfCxQxDdpDm+Nb1QLkbhkO54bVorgr6Q@mail.gmail.com>
 <41135899-4f0e-ac70-39f7-cef402e46ba2@sapo.pt>
 <CAF9-5jOXc3CWfYtRwxo2f+VuSX3np6o+ZTq8ykyNqH-t4AsbqQ@mail.gmail.com>
 <bd67ebdd-186c-d7b8-8171-a1df8ebc0cf6@sapo.pt>
 <CAJc=yOFehbugQb8ASJ9tuG3s8qdPP66C6=QUtQQvqW7HVC4=-A@mail.gmail.com>
 <CAF9-5jMr7SWF6BeW7NvNuaMdPGUPDd8DUiGsH4bMAGoo=0DmEw@mail.gmail.com>
 <c24c800e-a16b-a28d-fe23-43c3dcc07980@sapo.pt>
Message-ID: <CAF9-5jMMXjPeXt7DyuM0kCWnnGW1Uwi-DjKpFfdWYS1zBWyGbw@mail.gmail.com>

Thank you so much! I mostly worry which of those procedures is the
closest to the z-score

On Wed, May 6, 2020 at 10:05 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> Another option is to use stats::t.test
>
> t.test(x)$statistic
>
> Or, if you want to test against a beta0 != 0,
>
> t.test(x, mu = beta0)$statistic
>
>
> But in this case the estimator is the estimator for the mean value.
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 15:54 de 06/05/20, Ana Marija escreveu:
> > Thanks Patrick, so in conclusion this is fine?
> > z-score=Beta/StdErr
> >
> > On Wed, May 6, 2020 at 9:52 AM Patrick (Malone Quantitative)
> > <malone at malonequantitative.com> wrote:
> >>
> >> Guessing for Ana, but no, that's a different meaning. Beta/StdErr is a
> >> z statistic--a test statistic against (usually) the tails of the unit
> >> normal distribution. So like a t-test with infinite df.
> >>
> >>
> >> On Wed, May 6, 2020 at 10:41 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>>
> >>> Hello,
> >>>
> >>> By z-scores do you mean function help('scale')?
> >>>
> >>> Hope this helps,
> >>>
> >>> Rui Barradas
> >>>
> >>> ?s 15:31 de 06/05/20, Ana Marija escreveu:
> >>>> Hi Rui,
> >>>>
> >>>> Thank you for getting back to me. Is there is a better way to
> >>>> calculate Z scores if I have p values, SE and Beta?
> >>>>
> >>>> Thanks
> >>>> Ana
> >>>>
> >>>> On Wed, May 6, 2020 at 9:27 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>>>>
> >>>>> Hello,
> >>>>>
> >>>>> That gives the *absolute* t-scores. If it's all you need/want, then the
> >>>>> answer is yes, you can.
> >>>>>
> >>>>> Hope this helps,
> >>>>>
> >>>>> Rui Barradas
> >>>>>
> >>>>> ?s 14:28 de 06/05/20, Ana Marija escreveu:
> >>>>>> Hello,
> >>>>>>
> >>>>>> Can I apply the quantile function qt() this way?
> >>>>>> qt(pvals/2, 406-34, lower.tail = F)
> >>>>>> to get the T-scores?
> >>>>>>
> >>>>>> Thanks
> >>>>>> Ama
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >> --
> >> Patrick S. Malone, Ph.D., Malone Quantitative
> >> NEW Service Models: http://malonequantitative.com
> >>
> >> He/Him/His


From thpe @end|ng |rom @|meco|@de  Wed May  6 21:28:06 2020
From: thpe @end|ng |rom @|meco|@de (Thomas Petzoldt)
Date: Wed, 6 May 2020 21:28:06 +0200
Subject: [R] COVID-19 datasets...
In-Reply-To: <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
Message-ID: <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>

Sorry if I'm joining a little bit late.

I've put some related links and scripts together a few weeks ago. Then I 
stopped with this, because there is so much.

The data format employed by John Hopkins CSSE was sort of a big surprise 
to me. An opposite approach was taken in Germany, that organized it as a 
big JSON trees.

Fortunately, both can be "tidied" with R, and represent good didactic 
examples for our students.

Here yet another repo linking to the data:

https://github.com/tpetzoldt/covid


Thomas


On 04.05.2020 at 20:48 James Spottiswoode wrote:
> Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
> 
> https://github.com/CSSEGISandData/COVID-19
> 
> All in csv fiormat.
> 
> 
>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>>
>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>>   
>> Thanks
>>
>>
>> Bernard McGarvey
>>
>>
>> Director, Fort Myers Beach Lions Foundation, Inc.
>>
>>
>> Retired (Lilly Engineering Fellow).
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> James Spottiswoode
> Applied Mathematics & Statistics
> (310) 270 6220
> jamesspottiswoode Skype
> james at jsasoc.com
> 
>


From p@u|bern@|07 @end|ng |rom gm@||@com  Wed May  6 23:20:47 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Wed, 6 May 2020 16:20:47 -0500
Subject: [R] Working with very large datasets and generating an executable
 file
Message-ID: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>

Dear R friends,

Hope you are doing well. I have two questions, the first one is, can I work
with very large datasets in R? That is, say I need to test several machine
learning algorithms, like (random forest, multiple linear regression, etc.)
on datasets having between 50 to 100 columns and 20 million observations,
is there any way that R can handle data that large?

The second question is, is there a way I can develop an R model and turn it
into an executable program that can work on any OS?

Any help and/or guidance will be greatly appreciated,

Best regards,

Paul

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Thu May  7 00:05:15 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 7 May 2020 10:05:15 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <24240.8418.914054.939133@stat.math.ethz.ch>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
Message-ID: <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>

> OTOH, Hadley Wickham has written a book "Advanced R"  which has been
> the best book about advanced R by far (in my view, notably
> before it morphed (towards the 2nd edition) to use more and more
> non-base R packages).  There, he used "Closure" in a different,
> stricter sense, starting the section  'Closures' with
>     ?An object is data with functions.
>      A closure is a function with data.? ? John D. Cook

Martin, Thank you,

I've reviewed everything, and I've come to the following conclusion:
In general, a self-referencing function is not a closure, as such; and
In general, a closure is not a self-referencing function, as such.

So, to describe the superset, maybe I should say something like:

    *Self-Referencing Functions and Closures*

Also, I support multi-paradigm programming, including some global
state data mainly for default formatting-related options.
But if I understand things correctly, you support (or at least have
some preference for) purely-functional programming.
Which is one area, where we diverge.

It seems to me, the most people who advocate closures prefer
purely-functional programming.

If we use the principle, that in purely-functional programming, the
output of a function is not dependent on mutable state data, then
wouldn't it be better to say something like:

A closure is a function with (preferably non-mutable) data.


P.S.
If anyone's interested in creating R functions that reference
themselves, look at:
base::sys.function

Also, my package intoo contains convenience functions (THIS, THAT and
THEN), that wrap sys.function, giving it a slightly more
object-oriented flavor, and explore this idea further.
However, re-reading my documentation, I note that my examples need
some improvement...


From jr@| @end|ng |rom po@teo@no  Thu May  7 00:35:15 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Thu, 7 May 2020 00:35:15 +0200
Subject: [R] 
 How to combine two rows in a data table into a third new row,
 such that the values in the row are added together in the new row?
In-Reply-To: <220eb24467d947438526d4961cff8e15@SRVEXCHCM1302.precheza.cz>
References: <jo12v-il8yoIf0BT6nz1vR8DCnVoTRnU93eBBuSsa_6p3VtEoL0G_Ez-2jVryDXdU6E8gFO4ozJiGAOIJxV-Ili4n145VLMLQD9zFLSAOHw=@protonmail.com>
 <220eb24467d947438526d4961cff8e15@SRVEXCHCM1302.precheza.cz>
Message-ID: <20200506223515.GB1249402@posteo.no>

On 2020-05-06 09:13 +0000, PIKAL Petr wrote:
> Maybe aggregate?

Hi!  I agree aggregate is an elegant solution 
for this, so I continued your example a bit:

dt_count <- '"","STATUS","N"
"1","Resolved",650
"2","Assigned",135
"3","Closed",530
"4","In Progress",56
"5","Pending",75
"6","Cancelled",20'
dt_count <- read.csv(text=dt_count)

dt_count[
  dt_count$STATUS %in%
  c("Resolved", "Closed"),
  "STATUS"] <-
  "Resolved/Closed"
aggregate(
  x=list("N"=dt_count$N),
  by=list("STATUS"=dt_count$STATUS),
  FUN=sum)

Best,
Rasmus

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200507/5178fe64/attachment.sig>

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May  7 01:22:41 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 06 May 2020 16:22:41 -0700
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
Message-ID: <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>

Large data... yes, though how this can be done may vary. I have used machines with 128G of RAM before with no special big data packages.

Making an executable... theoretically, yes, though there are some significant technical (and possibly legal) challenges that will most likely make you question whether it was worth it if you try, particularly if your intent is to obscure your code from the recipient. I (as a random user and programmer on the Internet) would strongly discourage such efforts... it will almost certainly be more practical to deliver code in script/package form.

On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
>Dear R friends,
>
>Hope you are doing well. I have two questions, the first one is, can I
>work
>with very large datasets in R? That is, say I need to test several
>machine
>learning algorithms, like (random forest, multiple linear regression,
>etc.)
>on datasets having between 50 to 100 columns and 20 million
>observations,
>is there any way that R can handle data that large?
>
>The second question is, is there a way I can develop an R model and
>turn it
>into an executable program that can work on any OS?
>
>Any help and/or guidance will be greatly appreciated,
>
>Best regards,
>
>Paul
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Thu May  7 01:30:03 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 6 May 2020 16:30:03 -0700
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
Message-ID: <CAGxFJbSAYqbNyKrn5LcVy+=nr7gkccN_K2kq9qQyCUrnA503rg@mail.gmail.com>

To supplement Jeff's comments:

Big Data:
https://CRAN.R-project.org/view=HighPerformanceComputing

To deploy models:
https://cran.r-project.org/web/views/ModelDeployment.html

Opinion: Executables are a security risk. I wouldn't touch one unless
from a trusted source. I think I understand what you want to do, but I
would second Jeff's comment about using R packages. Don't bother to
disagree with me -- just dismiss if you do-- as this is wandering O/T
anyway.


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Wed, May 6, 2020 at 4:23 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> Large data... yes, though how this can be done may vary. I have used machines with 128G of RAM before with no special big data packages.
>
> Making an executable... theoretically, yes, though there are some significant technical (and possibly legal) challenges that will most likely make you question whether it was worth it if you try, particularly if your intent is to obscure your code from the recipient. I (as a random user and programmer on the Internet) would strongly discourage such efforts... it will almost certainly be more practical to deliver code in script/package form.
>
> On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
> >Dear R friends,
> >
> >Hope you are doing well. I have two questions, the first one is, can I
> >work
> >with very large datasets in R? That is, say I need to test several
> >machine
> >learning algorithms, like (random forest, multiple linear regression,
> >etc.)
> >on datasets having between 50 to 100 columns and 20 million
> >observations,
> >is there any way that R can handle data that large?
> >
> >The second question is, is there a way I can develop an R model and
> >turn it
> >into an executable program that can work on any OS?
> >
> >Any help and/or guidance will be greatly appreciated,
> >
> >Best regards,
> >
> >Paul
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Thu May  7 04:47:10 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 7 May 2020 14:47:10 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
Message-ID: <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>

A closure is a function plus an environment.

That's it.  This is a sixty-year-old thing in programming languages.

A closure is a dynamic value representing an instance of a function in
a particular context which it can refer to.  When a program in Algol
60 passed a procedure P to a procedure Q, what it passed was logically
a pair consisting of a pointer to the executable code of P and a
pointer to the context P was declared in, often called the static
link.  This was nothing other than a closure.  The limit in Algol 60
was that contexts were not retained, they formed a pure stack.  So all
you could do with a procedure parameter was call it or pass it on.
Same thing in Pascal and other Algol-like languages.

Modern functional languages retain just as much of the context as the
function can actually refer to.  My Smalltalk compiler, for example,
classifies variables as
 - only the value needs to be kept
 - the variable needs to be retained *as* a variable
 - not used in a nested function
R doesn't have that luxury, because R is rather more dynamic and
allows environments to be inspected.

For most practical purposes, there is simply no point in bothering to
distinguish "function" and "closure" in R.  The only functions that
are not closures are primitives.  But you can call args(f), body(f),
and environment(f) on any function, whether it is a closure or a
primitive.  If you want to mess around with the environment of a
function, then you need to understand this stuff, but you probably
shouldn't do that.

On Thu, 7 May 2020 at 10:06, Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> > OTOH, Hadley Wickham has written a book "Advanced R"  which has been
> > the best book about advanced R by far (in my view, notably
> > before it morphed (towards the 2nd edition) to use more and more
> > non-base R packages).  There, he used "Closure" in a different,
> > stricter sense, starting the section  'Closures' with
> >     ?An object is data with functions.
> >      A closure is a function with data.? ? John D. Cook
>
> Martin, Thank you,
>
> I've reviewed everything, and I've come to the following conclusion:
> In general, a self-referencing function is not a closure, as such; and
> In general, a closure is not a self-referencing function, as such.
>
> So, to describe the superset, maybe I should say something like:
>
>     *Self-Referencing Functions and Closures*
>
> Also, I support multi-paradigm programming, including some global
> state data mainly for default formatting-related options.
> But if I understand things correctly, you support (or at least have
> some preference for) purely-functional programming.
> Which is one area, where we diverge.
>
> It seems to me, the most people who advocate closures prefer
> purely-functional programming.
>
> If we use the principle, that in purely-functional programming, the
> output of a function is not dependent on mutable state data, then
> wouldn't it be better to say something like:
>
> A closure is a function with (preferably non-mutable) data.
>
>
> P.S.
> If anyone's interested in creating R functions that reference
> themselves, look at:
> base::sys.function
>
> Also, my package intoo contains convenience functions (THIS, THAT and
> THEN), that wrap sys.function, giving it a slightly more
> object-oriented flavor, and explore this idea further.
> However, re-reading my documentation, I note that my examples need
> some improvement...
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Thu May  7 05:27:29 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 7 May 2020 15:27:29 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
Message-ID: <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>

> If you want to mess around with the environment of a
> function, then you need to understand this stuff, but you probably
> shouldn't do that.

What exactly do you mean?

Are you implying that a user should not use environments?
In which case, I would disagree.


From @purd|e@@ @end|ng |rom gm@||@com  Thu May  7 05:34:55 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 7 May 2020 15:34:55 +1200
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
Message-ID: <CAB8pepy3XPii53Jz46bSJ07JJqeV_OSjixEo2ZBLrkS1f86Nmw@mail.gmail.com>

> The second question is, is there a way I can develop an R model and turn it
> into an executable program that can work on any OS?

------myrscript.c--------
int main (int argc, char* argv [])
{   system ("Rscript myrscript.r");
    return 0;
}
-------------------------

command line > gcc -o myrscript.exe myrscript.c
command line > myrscript.exe


From m@rk|eed@2 @end|ng |rom gm@||@com  Thu May  7 05:35:51 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Wed, 6 May 2020 23:35:51 -0400
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
Message-ID: <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>

Hi Abby: I agree with you because below is a perfect example of where not
understanding environments causes a somewhat
mysterious problem. Chuck Berry explains it in a follow up  email.

https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html


On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com> wrote:

> > If you want to mess around with the environment of a
> > function, then you need to understand this stuff, but you probably
> > shouldn't do that.
>
> What exactly do you mean?
>
> Are you implying that a user should not use environments?
> In which case, I would disagree.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Thu May  7 06:40:10 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 7 May 2020 16:40:10 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
Message-ID: <CABcYAdJq5yBAy6K0cX_Gozy=u6+-SZDD2U0BTr9JZ534o2XwCQ@mail.gmail.com>

By "mess around with" I mean environment(f) <- ...

That is for _very_ advanced players.

Never assume that someone meant something stupid, make them prove it.

On Thu, 7 May 2020 at 15:28, Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> > If you want to mess around with the environment of a
> > function, then you need to understand this stuff, but you probably
> > shouldn't do that.
>
> What exactly do you mean?
>
> Are you implying that a user should not use environments?
> In which case, I would disagree.


From r@oknz @end|ng |rom gm@||@com  Thu May  7 06:41:53 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 7 May 2020 16:41:53 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
Message-ID: <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>

That example is NOT an example of "messing around with environments."

On Thu, 7 May 2020 at 15:36, Mark Leeds <markleeds2 at gmail.com> wrote:
>
> Hi Abby: I agree with you because below is a perfect example of where not understanding environments causes a somewhat
> mysterious problem. Chuck Berry explains it in a follow up  email.
>
> https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html
>
>
> On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>>
>> > If you want to mess around with the environment of a
>> > function, then you need to understand this stuff, but you probably
>> > shouldn't do that.
>>
>> What exactly do you mean?
>>
>> Are you implying that a user should not use environments?
>> In which case, I would disagree.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From m@rk|eed@2 @end|ng |rom gm@||@com  Thu May  7 06:47:25 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Thu, 7 May 2020 00:47:25 -0400
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
 <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
Message-ID: <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>

Hi Richard: I didn't say it was and didn't mean to imply it.  All I said is
that it was an example of where "not understanding environments"  can lead
to errors that won't be understood by the person experiencing them. It just
so happens that the use of  "environment(f) <- " can lead to a solution but
there
are other solutions and it was not my intention to talk about the various
solutions. Only that it can be helpful if one understands the notion of
environments in R.








On Thu, May 7, 2020 at 12:42 AM Richard O'Keefe <raoknz at gmail.com> wrote:

> That example is NOT an example of "messing around with environments."
>
> On Thu, 7 May 2020 at 15:36, Mark Leeds <markleeds2 at gmail.com> wrote:
> >
> > Hi Abby: I agree with you because below is a perfect example of where
> not understanding environments causes a somewhat
> > mysterious problem. Chuck Berry explains it in a follow up  email.
> >
> >
> https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html
> >
> >
> > On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com>
> wrote:
> >>
> >> > If you want to mess around with the environment of a
> >> > function, then you need to understand this stuff, but you probably
> >> > shouldn't do that.
> >>
> >> What exactly do you mean?
> >>
> >> Are you implying that a user should not use environments?
> >> In which case, I would disagree.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@rk|eed@2 @end|ng |rom gm@||@com  Thu May  7 06:50:41 2020
From: m@rk|eed@2 @end|ng |rom gm@||@com (Mark Leeds)
Date: Thu, 7 May 2020 00:50:41 -0400
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
 <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
 <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>
Message-ID: <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>

Richard: I may have implied that one should "mess with environments" by
saying that I agree with Abby. If so, my apologies because that's not what
I meant.
I only meant understanding.



On Thu, May 7, 2020 at 12:47 AM Mark Leeds <markleeds2 at gmail.com> wrote:

> Hi Richard: I didn't say it was and didn't mean to imply it.  All I said
> is that it was an example of where "not understanding environments"  can
> lead
> to errors that won't be understood by the person experiencing them. It
> just so happens that the use of  "environment(f) <- " can lead to a
> solution but there
> are other solutions and it was not my intention to talk about the various
> solutions. Only that it can be helpful if one understands the notion of
> environments in R.
>
>
>
>
>
>
>
>
> On Thu, May 7, 2020 at 12:42 AM Richard O'Keefe <raoknz at gmail.com> wrote:
>
>> That example is NOT an example of "messing around with environments."
>>
>> On Thu, 7 May 2020 at 15:36, Mark Leeds <markleeds2 at gmail.com> wrote:
>> >
>> > Hi Abby: I agree with you because below is a perfect example of where
>> not understanding environments causes a somewhat
>> > mysterious problem. Chuck Berry explains it in a follow up  email.
>> >
>> >
>> https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html
>> >
>> >
>> > On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com>
>> wrote:
>> >>
>> >> > If you want to mess around with the environment of a
>> >> > function, then you need to understand this stuff, but you probably
>> >> > shouldn't do that.
>> >>
>> >> What exactly do you mean?
>> >>
>> >> Are you implying that a user should not use environments?
>> >> In which case, I would disagree.
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From r@oknz @end|ng |rom gm@||@com  Thu May  7 06:59:31 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Thu, 7 May 2020 16:59:31 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
 <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
 <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>
 <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>
Message-ID: <CABcYAd+m-0hxCdEik=gMHNyHKNpN=qk3jp3O_FyubqKF+nQ-Kg@mail.gmail.com>

I should clarify that the original *problem* wasn't a case of environment()<-
but a proposed solution was.  As the proposer wrote,
"However, this can lead to headaches downstream".

We agree that it is very important and useful to understand environments in R.
I once set out to construct a formal model of environments in S and R and how
lookup works.  I lost track of all the functions I had to consider and gave up.

On Thu, 7 May 2020 at 16:50, Mark Leeds <markleeds2 at gmail.com> wrote:
>
> Richard: I may have implied that one should "mess with environments" by saying that I agree with Abby. If so, my apologies because that's not what I meant.
> I only meant understanding.
>
>
>
> On Thu, May 7, 2020 at 12:47 AM Mark Leeds <markleeds2 at gmail.com> wrote:
>>
>> Hi Richard: I didn't say it was and didn't mean to imply it.  All I said is that it was an example of where "not understanding environments"  can lead
>> to errors that won't be understood by the person experiencing them. It just so happens that the use of  "environment(f) <- " can lead to a solution but there
>> are other solutions and it was not my intention to talk about the various solutions. Only that it can be helpful if one understands the notion of environments in R.
>>
>>
>>
>>
>>
>>
>>
>>
>> On Thu, May 7, 2020 at 12:42 AM Richard O'Keefe <raoknz at gmail.com> wrote:
>>>
>>> That example is NOT an example of "messing around with environments."
>>>
>>> On Thu, 7 May 2020 at 15:36, Mark Leeds <markleeds2 at gmail.com> wrote:
>>> >
>>> > Hi Abby: I agree with you because below is a perfect example of where not understanding environments causes a somewhat
>>> > mysterious problem. Chuck Berry explains it in a follow up  email.
>>> >
>>> > https://r.789695.n4.nabble.com/issues-with-environment-handling-in-model-frame-td4762855.html
>>> >
>>> >
>>> > On Wed, May 6, 2020 at 11:28 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>>> >>
>>> >> > If you want to mess around with the environment of a
>>> >> > function, then you need to understand this stuff, but you probably
>>> >> > shouldn't do that.
>>> >>
>>> >> What exactly do you mean?
>>> >>
>>> >> Are you implying that a user should not use environments?
>>> >> In which case, I would disagree.
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Thu May  7 07:32:22 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 7 May 2020 17:32:22 +1200
Subject: [R] 
 'closure' (was "stats:: spline's method could not be monoH.FC")
In-Reply-To: <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <CAHz+bWZbV0yCSjFxguzFE7swguWkO-qOY3w8HLOrzZkbXGUd4w@mail.gmail.com>
 <CAB8pepyNhwP5phmeJQt+Rv70ixyavVCoH_-o+uHGyc3McKoftA@mail.gmail.com>
 <CAHz+bWZ=o79FNFUO_ysUA+TqYpXHL2dgzfdSdnY6iDQVR+PF5Q@mail.gmail.com>
 <24240.8418.914054.939133@stat.math.ethz.ch>
 <CAB8pepxxZS+xEEU85bbeMHKDRN3o=i9+rsj5nYAfLLf7YsnYzw@mail.gmail.com>
 <CABcYAd+iYTSWPbh=XqWvTB+utkyc1_3Sy96apJ-vBPVZ6tWLzA@mail.gmail.com>
 <CAB8pepytqkTjgrDKFOVUM=hPgG4KHtRtQu70nu=BeKKiiYxP3Q@mail.gmail.com>
 <CAHz+bWZYZOof=gpVzNV3Rvp5Jtmp7+xk5N3BfyG57w=mtPVBfw@mail.gmail.com>
 <CABcYAdKcVEQOMXdXnmR6+SBWu8S+DJD10Mx+FbJGE3FBNwqFyQ@mail.gmail.com>
 <CAHz+bWa0iS6ToNroo2KdoP+qSD_DYtty6T534WLxs60xf2GKBg@mail.gmail.com>
 <CAHz+bWbH4n2_4TtzNHs4T2J6j2JCJ6P-uQjOGtrucfRA2-m+YA@mail.gmail.com>
Message-ID: <CAB8pepxFXt7V15EKXngc57HiSwBSLoSdjLqs8kMCZgGzzHKNHg@mail.gmail.com>

On Thu, May 7, 2020 at 4:50 PM Mark Leeds <markleeds2 at gmail.com> wrote:
> Richard: I may have implied that one should "mess with environments" by saying that I agree with Abby. If so, my apologies because that's not what I meant.
> I only meant understanding.

Hi Mark,
I don't think you need to apologize.

According the the Cambridge English Dictionary, "Mess Around" means:

    to spend time doing various things
    that are not important,
    without any particular purpose or plan.

Substituting that into the original statement we get:

    If you want to spend time doing various things
    with the environment of a function,
    that are not important,
    without any particular purpose or plan,
    then you need to understand this stuff, but you probably
    shouldn't do that.

It's not my fault or yours, if the previous poster choose ambiguous
language with condescending tone.

I note that the purpose of this thread was to create clarity
surrounding closures and related topics.


From p@u|bern@|07 @end|ng |rom gm@||@com  Thu May  7 07:46:45 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Thu, 7 May 2020 00:46:45 -0500
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAB8pepy3XPii53Jz46bSJ07JJqeV_OSjixEo2ZBLrkS1f86Nmw@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <CAB8pepy3XPii53Jz46bSJ07JJqeV_OSjixEo2ZBLrkS1f86Nmw@mail.gmail.com>
Message-ID: <CAMOcQfOGZ+hRXF4uZkmRxy0P6g4oLQVYfnxW3o=DkfvN0t36xw@mail.gmail.com>

Thank you Abby!

 Cheers!

El mi?., 6 de mayo de 2020 10:35 p. m., Abby Spurdle <spurdle.a at gmail.com>
escribi?:

> > The second question is, is there a way I can develop an R model and turn
> it
> > into an executable program that can work on any OS?
>
> ------myrscript.c--------
> int main (int argc, char* argv [])
> {   system ("Rscript myrscript.r");
>     return 0;
> }
> -------------------------
>
> command line > gcc -o myrscript.exe myrscript.c
> command line > myrscript.exe
>

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Thu May  7 07:53:00 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Thu, 7 May 2020 00:53:00 -0500
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
Message-ID: <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>

Dear Jeff,

Thank you for the feedback. So, after reading your comments, it seems that,
in order to develop an executable model that could be run in any OS, python
might be the way to go then?

I appreciate all of your valuable responses.

Best regards,

Paul

El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
jdnewmil at dcn.davis.ca.us> escribi?:

> Large data... yes, though how this can be done may vary. I have used
> machines with 128G of RAM before with no special big data packages.
>
> Making an executable... theoretically, yes, though there are some
> significant technical (and possibly legal) challenges that will most likely
> make you question whether it was worth it if you try, particularly if your
> intent is to obscure your code from the recipient. I (as a random user and
> programmer on the Internet) would strongly discourage such efforts... it
> will almost certainly be more practical to deliver code in script/package
> form.
>
> On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
> >Dear R friends,
> >
> >Hope you are doing well. I have two questions, the first one is, can I
> >work
> >with very large datasets in R? That is, say I need to test several
> >machine
> >learning algorithms, like (random forest, multiple linear regression,
> >etc.)
> >on datasets having between 50 to 100 columns and 20 million
> >observations,
> >is there any way that R can handle data that large?
> >
> >The second question is, is there a way I can develop an R model and
> >turn it
> >into an executable program that can work on any OS?
> >
> >Any help and/or guidance will be greatly appreciated,
> >
> >Best regards,
> >
> >Paul
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May  7 08:22:46 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 06 May 2020 23:22:46 -0700
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
 <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
Message-ID: <6A387E70-B031-44E3-8B5D-BE4AA244B6C7@dcn.davis.ca.us>

There is no executable that can run on any OS. As for python... it is hardly the only game in town for building executables, but it and those other options are off topic here.

On May 6, 2020 10:53:00 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
>Dear Jeff,
>
>Thank you for the feedback. So, after reading your comments, it seems
>that,
>in order to develop an executable model that could be run in any OS,
>python
>might be the way to go then?
>
>I appreciate all of your valuable responses.
>
>Best regards,
>
>Paul
>
>El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
>jdnewmil at dcn.davis.ca.us> escribi?:
>
>> Large data... yes, though how this can be done may vary. I have used
>> machines with 128G of RAM before with no special big data packages.
>>
>> Making an executable... theoretically, yes, though there are some
>> significant technical (and possibly legal) challenges that will most
>likely
>> make you question whether it was worth it if you try, particularly if
>your
>> intent is to obscure your code from the recipient. I (as a random
>user and
>> programmer on the Internet) would strongly discourage such efforts...
>it
>> will almost certainly be more practical to deliver code in
>script/package
>> form.
>>
>> On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
>wrote:
>> >Dear R friends,
>> >
>> >Hope you are doing well. I have two questions, the first one is, can
>I
>> >work
>> >with very large datasets in R? That is, say I need to test several
>> >machine
>> >learning algorithms, like (random forest, multiple linear
>regression,
>> >etc.)
>> >on datasets having between 50 to 100 columns and 20 million
>> >observations,
>> >is there any way that R can handle data that large?
>> >
>> >The second question is, is there a way I can develop an R model and
>> >turn it
>> >into an executable program that can work on any OS?
>> >
>> >Any help and/or guidance will be greatly appreciated,
>> >
>> >Best regards,
>> >
>> >Paul
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From p@u|bern@|07 @end|ng |rom gm@||@com  Thu May  7 08:39:09 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Thu, 7 May 2020 01:39:09 -0500
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <6A387E70-B031-44E3-8B5D-BE4AA244B6C7@dcn.davis.ca.us>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
 <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
 <6A387E70-B031-44E3-8B5D-BE4AA244B6C7@dcn.davis.ca.us>
Message-ID: <CAMOcQfP+f2uKcH+TY+5QT1CAF6mBss0ZOwc5ZP11QJV5jrzJ5A@mail.gmail.com>

Dear Jeff, an executable in terms of deploying a machine learning model,
whether it a classifocation, regression, time series or deep learning model.

Best regards,

Paul

El jue., 7 de mayo de 2020 1:22 a. m., Jeff Newmiller <
jdnewmil at dcn.davis.ca.us> escribi?:

> There is no executable that can run on any OS. As for python... it is
> hardly the only game in town for building executables, but it and those
> other options are off topic here.
>
> On May 6, 2020 10:53:00 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
> wrote:
> >Dear Jeff,
> >
> >Thank you for the feedback. So, after reading your comments, it seems
> >that,
> >in order to develop an executable model that could be run in any OS,
> >python
> >might be the way to go then?
> >
> >I appreciate all of your valuable responses.
> >
> >Best regards,
> >
> >Paul
> >
> >El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
> >jdnewmil at dcn.davis.ca.us> escribi?:
> >
> >> Large data... yes, though how this can be done may vary. I have used
> >> machines with 128G of RAM before with no special big data packages.
> >>
> >> Making an executable... theoretically, yes, though there are some
> >> significant technical (and possibly legal) challenges that will most
> >likely
> >> make you question whether it was worth it if you try, particularly if
> >your
> >> intent is to obscure your code from the recipient. I (as a random
> >user and
> >> programmer on the Internet) would strongly discourage such efforts...
> >it
> >> will almost certainly be more practical to deliver code in
> >script/package
> >> form.
> >>
> >> On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
> >wrote:
> >> >Dear R friends,
> >> >
> >> >Hope you are doing well. I have two questions, the first one is, can
> >I
> >> >work
> >> >with very large datasets in R? That is, say I need to test several
> >> >machine
> >> >learning algorithms, like (random forest, multiple linear
> >regression,
> >> >etc.)
> >> >on datasets having between 50 to 100 columns and 20 million
> >> >observations,
> >> >is there any way that R can handle data that large?
> >> >
> >> >The second question is, is there a way I can develop an R model and
> >> >turn it
> >> >into an executable program that can work on any OS?
> >> >
> >> >Any help and/or guidance will be greatly appreciated,
> >> >
> >> >Best regards,
> >> >
> >> >Paul
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
>
> --
> Sent from my phone. Please excuse my brevity.
>

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Thu May  7 08:40:00 2020
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Thu, 7 May 2020 01:40:00 -0500
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <0f0645d7-2c4a-4d5a-9d52-b29c5a3cf9fe@email.android.com>
References: <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
 <0f0645d7-2c4a-4d5a-9d52-b29c5a3cf9fe@email.android.com>
Message-ID: <CAMOcQfMNOXF4sMFPMFSp4-PZ4jotvSMwEAE9yZpiOh6fDKKOgA@mail.gmail.com>

That could be the answer, yes.

El jue., 7 de mayo de 2020 1:22 a. m., <cpolwart at chemo.org.uk> escribi?:

> Or maybe a Shiny Application?
>
> On 7 May 2020 06:53, Paul Bernal <paulbernal07 at gmail.com> wrote:
>
> Dear Jeff,
>
> Thank you for the feedback. So, after reading your comments, it seems
> that,
> in order to develop an executable model that could be run in any OS,
> python
> might be the way to go then?
>
> I appreciate all of your valuable responses.
>
> Best regards,
>
> Paul
>
> El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us> escribi?:
>
> > Large data... yes, though how this can be done may vary. I have used
> > machines with 128G of RAM before with no special big data packages.
> >
> > Making an executable... theoretically, yes, though there are some
> > significant technical (and possibly legal) challenges that will most
> likely
> > make you question whether it was worth it if you try, particularly if
> your
> > intent is to obscure your code from the recipient. I (as a random user
> and
> > programmer on the Internet) would strongly discourage such efforts... it
> > will almost certainly be more practical to deliver code in
> script/package
> > form.
> >
> > On May 6, 2020 2:20:47 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
> wrote:
> > >Dear R friends,
> > >
> > >Hope you are doing well. I have two questions, the first one is, can I
> > >work
> > >with very large datasets in R? That is, say I need to test several
> > >machine
> > >learning algorithms, like (random forest, multiple linear regression,
> > >etc.)
> > >on datasets having between 50 to 100 columns and 20 million
> > >observations,
> > >is there any way that R can handle data that large?
> > >
> > >The second question is, is there a way I can develop an R model and
> > >turn it
> > >into an executable program that can work on any OS?
> > >
> > >Any help and/or guidance will be greatly appreciated,
> > >
> > >Best regards,
> > >
> > >Paul
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Sent from my phone. Please excuse my brevity.
> >
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu May  7 09:12:24 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 07 May 2020 00:12:24 -0700
Subject: [R] 
 Working with very large datasets and generating an executable file
In-Reply-To: <CAMOcQfP+f2uKcH+TY+5QT1CAF6mBss0ZOwc5ZP11QJV5jrzJ5A@mail.gmail.com>
References: <CAMOcQfMHwmsS-4Ewjt_OSEy3vUbHUutN5Dqgjr2a4K1DFzbqXA@mail.gmail.com>
 <72C06F2F-A6EB-40AA-83C8-7E01DBA9D61E@dcn.davis.ca.us>
 <CAMOcQfMdBne1PqEQoLND6xt_WGgQZ=Kji0N7AdVoU3QH9TUAgA@mail.gmail.com>
 <6A387E70-B031-44E3-8B5D-BE4AA244B6C7@dcn.davis.ca.us>
 <CAMOcQfP+f2uKcH+TY+5QT1CAF6mBss0ZOwc5ZP11QJV5jrzJ5A@mail.gmail.com>
Message-ID: <3BD66A20-BB5F-438D-9DFE-76172CD8DB18@dcn.davis.ca.us>

You could deploy a rocker image, possibly with an API (built with plumber). But I think it is misleading to refer to that as an executable.

On May 6, 2020 11:39:09 PM PDT, Paul Bernal <paulbernal07 at gmail.com> wrote:
>Dear Jeff, an executable in terms of deploying a machine learning
>model,
>whether it a classifocation, regression, time series or deep learning
>model.
>
>Best regards,
>
>Paul
>
>El jue., 7 de mayo de 2020 1:22 a. m., Jeff Newmiller <
>jdnewmil at dcn.davis.ca.us> escribi?:
>
>> There is no executable that can run on any OS. As for python... it is
>> hardly the only game in town for building executables, but it and
>those
>> other options are off topic here.
>>
>> On May 6, 2020 10:53:00 PM PDT, Paul Bernal <paulbernal07 at gmail.com>
>> wrote:
>> >Dear Jeff,
>> >
>> >Thank you for the feedback. So, after reading your comments, it
>seems
>> >that,
>> >in order to develop an executable model that could be run in any OS,
>> >python
>> >might be the way to go then?
>> >
>> >I appreciate all of your valuable responses.
>> >
>> >Best regards,
>> >
>> >Paul
>> >
>> >El mi?., 6 de mayo de 2020 6:22 p. m., Jeff Newmiller <
>> >jdnewmil at dcn.davis.ca.us> escribi?:
>> >
>> >> Large data... yes, though how this can be done may vary. I have
>used
>> >> machines with 128G of RAM before with no special big data
>packages.
>> >>
>> >> Making an executable... theoretically, yes, though there are some
>> >> significant technical (and possibly legal) challenges that will
>most
>> >likely
>> >> make you question whether it was worth it if you try, particularly
>if
>> >your
>> >> intent is to obscure your code from the recipient. I (as a random
>> >user and
>> >> programmer on the Internet) would strongly discourage such
>efforts...
>> >it
>> >> will almost certainly be more practical to deliver code in
>> >script/package
>> >> form.
>> >>
>> >> On May 6, 2020 2:20:47 PM PDT, Paul Bernal
><paulbernal07 at gmail.com>
>> >wrote:
>> >> >Dear R friends,
>> >> >
>> >> >Hope you are doing well. I have two questions, the first one is,
>can
>> >I
>> >> >work
>> >> >with very large datasets in R? That is, say I need to test
>several
>> >> >machine
>> >> >learning algorithms, like (random forest, multiple linear
>> >regression,
>> >> >etc.)
>> >> >on datasets having between 50 to 100 columns and 20 million
>> >> >observations,
>> >> >is there any way that R can handle data that large?
>> >> >
>> >> >The second question is, is there a way I can develop an R model
>and
>> >> >turn it
>> >> >into an executable program that can work on any OS?
>> >> >
>> >> >Any help and/or guidance will be greatly appreciated,
>> >> >
>> >> >Best regards,
>> >> >
>> >> >Paul
>> >> >
>> >> >       [[alternative HTML version deleted]]
>> >> >
>> >> >______________________________________________
>> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >> >PLEASE do read the posting guide
>> >> >http://www.R-project.org/posting-guide.html
>> >> >and provide commented, minimal, self-contained, reproducible
>code.
>> >>
>> >> --
>> >> Sent from my phone. Please excuse my brevity.
>> >>
>>
>> --
>> Sent from my phone. Please excuse my brevity.
>>

-- 
Sent from my phone. Please excuse my brevity.


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Thu May  7 11:19:16 2020
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Thu, 7 May 2020 14:49:16 +0530
Subject: [R] COVID-19 datasets...
In-Reply-To: <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
Message-ID: <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>

On Thu, May 7, 2020 at 12:58 AM Thomas Petzoldt <thpe at simecol.de> wrote:
>
> Sorry if I'm joining a little bit late.
>
> I've put some related links and scripts together a few weeks ago. Then I
> stopped with this, because there is so much.
>
> The data format employed by John Hopkins CSSE was sort of a big surprise
> to me.

Why? I find it quite convenient to drop the first few columns and
extract the data as a matrix (using data.matrix()).

-Deepayan

> An opposite approach was taken in Germany, that organized it as a
> big JSON trees.
>
> Fortunately, both can be "tidied" with R, and represent good didactic
> examples for our students.
>
> Here yet another repo linking to the data:
>
> https://github.com/tpetzoldt/covid
>
>
> Thomas
>
>
> On 04.05.2020 at 20:48 James Spottiswoode wrote:
> > Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
> >
> > https://github.com/CSSEGISandData/COVID-19
> >
> > All in csv fiormat.
> >
> >
> >> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
> >>
> >> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
> >>
> >> Thanks
> >>
> >>
> >> Bernard McGarvey
> >>
> >>
> >> Director, Fort Myers Beach Lions Foundation, Inc.
> >>
> >>
> >> Retired (Lilly Engineering Fellow).
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > James Spottiswoode
> > Applied Mathematics & Statistics
> > (310) 270 6220
> > jamesspottiswoode Skype
> > james at jsasoc.com
> >
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From thpe @end|ng |rom @|meco|@de  Thu May  7 12:46:34 2020
From: thpe @end|ng |rom @|meco|@de (Thomas Petzoldt)
Date: Thu, 7 May 2020 12:46:34 +0200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
 <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>
Message-ID: <1e2aa343-ebfe-811e-f3e9-0c9bc275396a@simecol.de>

On 07.05.2020 at 11:19 Deepayan Sarkar wrote:
> On Thu, May 7, 2020 at 12:58 AM Thomas Petzoldt <thpe at simecol.de> wrote:
>>
>> Sorry if I'm joining a little bit late.
>>
>> I've put some related links and scripts together a few weeks ago. Then I
>> stopped with this, because there is so much.
>>
>> The data format employed by John Hopkins CSSE was sort of a big surprise
>> to me.
> 
> Why? I find it quite convenient to drop the first few columns and
> extract the data as a matrix (using data.matrix()).
> 
> -Deepayan

Many thanks for the hint to use data.matrix

My aim was not to say that it is difficult, especially as R has all the 
tools for data mangling.

My surprise was that "wide tables" and non-ISO dates as column names are 
not the "data base way" that we in general teach to our students

With reshape2::melt or tidyr::gather resp. pivot_longer, conversion is 
quite easy, regardless if one wants to use tidyverse or not, see example 
below.

Again, thanks, Thomas


library("dplyr")
library("readr")
library("tidyr")

file <- 
"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"

dat <- read_delim(file, delim=",")
names(dat)[1:2] <- c("Province_State", "Country_Region")
dat2 <-
   dat %>%
   ## summarize Country/Region duplicates
   group_by(Country_Region) %>% summarise_at(vars(-(1:4)), sum) %>%
   ## make it a long table
   pivot_longer(cols = -Country_Region, names_to = "time") %>%
   ## convert to ISO 8601 date
   mutate(time = as.POSIXct(time, format="%m/%e/%y"))



> 
>> An opposite approach was taken in Germany, that organized it as a
>> big JSON trees.
>>
>> Fortunately, both can be "tidied" with R, and represent good didactic
>> examples for our students.
>>
>> Here yet another repo linking to the data:
>>
>> https://github.com/tpetzoldt/covid
>>
>>
>> Thomas
>>
>>
>> On 04.05.2020 at 20:48 James Spottiswoode wrote:
>>> Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
>>>
>>> https://github.com/CSSEGISandData/COVID-19
>>>
>>> All in csv fiormat.
>>>
>>>
>>>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>>>>
>>>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>>>>
>>>> Thanks
>>>>
>>>>
>>>> Bernard McGarvey
>>>>
>>>>
>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>
>>>>
>>>> Retired (Lilly Engineering Fellow).
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> James Spottiswoode
>>> Applied Mathematics & Statistics
>>> (310) 270 6220
>>> jamesspottiswoode Skype
>>> james at jsasoc.com
>>>

-- 
Dr. Thomas Petzoldt
senior scientist

Technische Universitaet Dresden
Faculty of Environmental Sciences
Institute of Hydrobiology
01062 Dresden, Germany

https://tu-dresden.de/Members/thomas.petzoldt


From deep@y@n@@@rk@r @end|ng |rom gm@||@com  Thu May  7 13:12:50 2020
From: deep@y@n@@@rk@r @end|ng |rom gm@||@com (Deepayan Sarkar)
Date: Thu, 7 May 2020 16:42:50 +0530
Subject: [R] COVID-19 datasets...
In-Reply-To: <1e2aa343-ebfe-811e-f3e9-0c9bc275396a@simecol.de>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
 <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>
 <1e2aa343-ebfe-811e-f3e9-0c9bc275396a@simecol.de>
Message-ID: <CADfFDC6Duk3A2YC2a0=K2AW-nDbfezq==uEpBgoUruAYbw_2hQ@mail.gmail.com>

On Thu, May 7, 2020 at 4:16 PM Thomas Petzoldt <thpe at simecol.de> wrote:
>
> On 07.05.2020 at 11:19 Deepayan Sarkar wrote:
> > On Thu, May 7, 2020 at 12:58 AM Thomas Petzoldt <thpe at simecol.de> wrote:
> >>
> >> Sorry if I'm joining a little bit late.
> >>
> >> I've put some related links and scripts together a few weeks ago. Then I
> >> stopped with this, because there is so much.
> >>
> >> The data format employed by John Hopkins CSSE was sort of a big surprise
> >> to me.
> >
> > Why? I find it quite convenient to drop the first few columns and
> > extract the data as a matrix (using data.matrix()).
> >
> > -Deepayan
>
> Many thanks for the hint to use data.matrix
>
> My aim was not to say that it is difficult, especially as R has all the
> tools for data mangling.
>
> My surprise was that "wide tables" and non-ISO dates as column names are
> not the "data base way" that we in general teach to our students

Well, I am all for long format data when it makes sense, but I would
disagree that that is always the "right approach". In the case of
regular multiple time series, as in this context, a matrix-like
structure seems much more natural (and nicely handled by ts() in R),
and I wouldn't even bother reshaping the data in the first place.

See, for example,

https://github.com/deepayan/deepayan.github.io/blob/master/covid-19/deaths.rmd

and

https://deepayan.github.io/covid-19/deaths.html

-Deepayan

> With reshape2::melt or tidyr::gather resp. pivot_longer, conversion is
> quite easy, regardless if one wants to use tidyverse or not, see example
> below.
>
> Again, thanks, Thomas
>
>
> library("dplyr")
> library("readr")
> library("tidyr")
>
> file <-
> "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
>
> dat <- read_delim(file, delim=",")
> names(dat)[1:2] <- c("Province_State", "Country_Region")
> dat2 <-
>    dat %>%
>    ## summarize Country/Region duplicates
>    group_by(Country_Region) %>% summarise_at(vars(-(1:4)), sum) %>%
>    ## make it a long table
>    pivot_longer(cols = -Country_Region, names_to = "time") %>%
>    ## convert to ISO 8601 date
>    mutate(time = as.POSIXct(time, format="%m/%e/%y"))
>
>
>
> >
> >> An opposite approach was taken in Germany, that organized it as a
> >> big JSON trees.
> >>
> >> Fortunately, both can be "tidied" with R, and represent good didactic
> >> examples for our students.
> >>
> >> Here yet another repo linking to the data:
> >>
> >> https://github.com/tpetzoldt/covid
> >>
> >>
> >> Thomas
> >>
> >>
> >> On 04.05.2020 at 20:48 James Spottiswoode wrote:
> >>> Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
> >>>
> >>> https://github.com/CSSEGISandData/COVID-19
> >>>
> >>> All in csv fiormat.
> >>>
> >>>
> >>>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
> >>>>
> >>>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
> >>>>
> >>>> Thanks
> >>>>
> >>>>
> >>>> Bernard McGarvey
> >>>>
> >>>>
> >>>> Director, Fort Myers Beach Lions Foundation, Inc.
> >>>>
> >>>>
> >>>> Retired (Lilly Engineering Fellow).
> >>>>
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>> James Spottiswoode
> >>> Applied Mathematics & Statistics
> >>> (310) 270 6220
> >>> jamesspottiswoode Skype
> >>> james at jsasoc.com
> >>>
>
> --
> Dr. Thomas Petzoldt
> senior scientist
>
> Technische Universitaet Dresden
> Faculty of Environmental Sciences
> Institute of Hydrobiology
> 01062 Dresden, Germany
>
> https://tu-dresden.de/Members/thomas.petzoldt


From thpe @end|ng |rom @|meco|@de  Thu May  7 13:53:17 2020
From: thpe @end|ng |rom @|meco|@de (Thomas Petzoldt)
Date: Thu, 7 May 2020 13:53:17 +0200
Subject: [R] COVID-19 datasets...
In-Reply-To: <CADfFDC6Duk3A2YC2a0=K2AW-nDbfezq==uEpBgoUruAYbw_2hQ@mail.gmail.com>
References: <961343375.1407915.1588617072873@connect.xfinity.com>
 <E3562BF5-42BC-43D6-96AB-E043B6EA1C57@jsasoc.com>
 <d6e08a07-71c0-473f-7959-b9e1dcac81d1@simecol.de>
 <CADfFDC6uvYytwfrC1xZHBN9=h5KcbYW8B1buwEJFd80ChiwcOQ@mail.gmail.com>
 <1e2aa343-ebfe-811e-f3e9-0c9bc275396a@simecol.de>
 <CADfFDC6Duk3A2YC2a0=K2AW-nDbfezq==uEpBgoUruAYbw_2hQ@mail.gmail.com>
Message-ID: <e7ddde8e-128b-339e-1cb4-1af4e14ad760@simecol.de>

On 07.05.2020 at 13:12 Deepayan Sarkar wrote:
> On Thu, May 7, 2020 at 4:16 PM Thomas Petzoldt <thpe at simecol.de> wrote:
>> On 07.05.2020 at 11:19 Deepayan Sarkar wrote:
>>> On Thu, May 7, 2020 at 12:58 AM Thomas Petzoldt <thpe at simecol.de> wrote:
>>>> Sorry if I'm joining a little bit late.
>>>>
>>>> I've put some related links and scripts together a few weeks ago. Then I
>>>> stopped with this, because there is so much.
>>>>
>>>> The data format employed by John Hopkins CSSE was sort of a big surprise
>>>> to me.
>>> Why? I find it quite convenient to drop the first few columns and
>>> extract the data as a matrix (using data.matrix()).
>>>
>>> -Deepayan
>> Many thanks for the hint to use data.matrix
>>
>> My aim was not to say that it is difficult, especially as R has all the
>> tools for data mangling.
>>
>> My surprise was that "wide tables" and non-ISO dates as column names are
>> not the "data base way" that we in general teach to our students
> Well, I am all for long format data when it makes sense, but I would
> disagree that that is always the "right approach". In the case of
> regular multiple time series, as in this context, a matrix-like
> structure seems much more natural (and nicely handled by ts() in R),
> and I wouldn't even bother reshaping the data in the first place.
>
> See, for example,
>
> https://github.com/deepayan/deepayan.github.io/blob/master/covid-19/deaths.rmd
>
> and
>
> https://deepayan.github.io/covid-19/deaths.html
>
> -Deepayan

Great, thank you for the link with the comprehensive lattice graphs and 
the explanations. I like your package very much and use it often, since 
it appeared on CRAN (3 of my CRAN packages depend on it). As "dynamic 
modeller", I consider time always as the first column, but I agree on 
the other hand, that long tables are often, but not always the right 
approach, let's think about gridded multi-dimensional netcdf data.

Many thanks for sharing your analysis publicly, I'll add your repo to my 
link list.

Thomas

>> With reshape2::melt or tidyr::gather resp. pivot_longer, conversion is
>> quite easy, regardless if one wants to use tidyverse or not, see example
>> below.
>>
>> Again, thanks, Thomas
>>
>>
>> library("dplyr")
>> library("readr")
>> library("tidyr")
>>
>> file <-
>> "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"
>>
>> dat <- read_delim(file, delim=",")
>> names(dat)[1:2] <- c("Province_State", "Country_Region")
>> dat2 <-
>>     dat %>%
>>     ## summarize Country/Region duplicates
>>     group_by(Country_Region) %>% summarise_at(vars(-(1:4)), sum) %>%
>>     ## make it a long table
>>     pivot_longer(cols = -Country_Region, names_to = "time") %>%
>>     ## convert to ISO 8601 date
>>     mutate(time = as.POSIXct(time, format="%m/%e/%y"))
>>
>>
>>
>>>> An opposite approach was taken in Germany, that organized it as a
>>>> big JSON trees.
>>>>
>>>> Fortunately, both can be "tidied" with R, and represent good didactic
>>>> examples for our students.
>>>>
>>>> Here yet another repo linking to the data:
>>>>
>>>> https://github.com/tpetzoldt/covid
>>>>
>>>>
>>>> Thomas
>>>>
>>>>
>>>> On 04.05.2020 at 20:48 James Spottiswoode wrote:
>>>>> Sure. COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University is available here:
>>>>>
>>>>> https://github.com/CSSEGISandData/COVID-19
>>>>>
>>>>> All in csv fiormat.
>>>>>
>>>>>
>>>>>> On May 4, 2020, at 11:31 AM, Bernard McGarvey <mcgarvey.bernard at comcast.net> wrote:
>>>>>>
>>>>>> Just curious does anyone know of a website that has data available in a format that R can download and analyze?
>>>>>>
>>>>>> Thanks
>>>>>>
>>>>>>
>>>>>> Bernard McGarvey
>>>>>>
>>>>>>
>>>>>> Director, Fort Myers Beach Lions Foundation, Inc.
>>>>>>
>>>>>>
>>>>>> Retired (Lilly Engineering Fellow).
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>> James Spottiswoode
>>>>> Applied Mathematics & Statistics
>>>>> (310) 270 6220
>>>>> jamesspottiswoode Skype
>>>>> james at jsasoc.com
>>>>>
>> --
>> Dr. Thomas Petzoldt
>> senior scientist
>>
>> Technische Universitaet Dresden
>> Faculty of Environmental Sciences
>> Institute of Hydrobiology
>> 01062 Dresden, Germany
>>
>> https://tu-dresden.de/Members/thomas.petzoldt


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Thu May  7 14:26:58 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Thu, 7 May 2020 17:56:58 +0530
Subject: [R] [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
Message-ID: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>

Dear R-users,

I want to estimate ARCH test for multiple columns (i.e.,  from 2:21 COLUMNS
) in my data. For this purpose, I want to run a loop to calculate ARCH test
results for each column in the data frame. I tried by using for loop and
lapply function, but unable to write a loop for computing the ARCH test
simultaneously for each column (i.e., from 2:21 columns) of my data frame.

Below is my ARCH test code which I want to estimate for multiple columns of
the data frame in a loop.

library(tseries)

library(FinTS)

ArchTest (A, lags=1, demean = FALSE)

Hence, A is a vector for which the ARCH test result is calculated. Here, I
want to write a loop so that the ArchTest can be calculated simultaneously
for each column of my data frame. From ARCH test result, I require only the
calculated Chi-square value and its p-value for each column that stored in
another matrix or object for each column as an output file.

For your convenience, I attached my sample data below. Please find it.

Please help me for which I shall be always grateful to you.

Thank you.

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/07/20,
05:51:03 PM

From petr@p|k@| @end|ng |rom prechez@@cz  Thu May  7 15:12:53 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 7 May 2020 13:12:53 +0000
Subject: [R] unstable results of nlxb fit
Message-ID: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>

Dear all

I started to use nlxb instead of nls to get rid of singular gradient error.
I try to fit double exponential function to my data, but results I obtain
are strongly dependent on starting values. 

tsmes ~ A*exp(a*plast) + B* exp(b*plast)

Changing b from 0.1 to 0.01 gives me completely different results. I usually
check result by a plot but could the result be inspected if it achieved good
result without plotting?

Or is there any way how to perform such task?

Cheers
Petr

Below is working example.

> dput(temp)
temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33, 
34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 
44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 
54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 
68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 
97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 
141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 
55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 
66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 
78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90, 
91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 
102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 
112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")

library(nlsr)

fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
start=list(A=1, B=15, a=0.025, b=0.01))
coef(fit)
           A            B            a            b 
3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02 

plot(temp$plast, temp$tsmes, ylim=c(0,200))
lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
ccc <- coef(fit)
lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)

# wrong fit with slightly different b
fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
start=list(A=1, B=15, a=0.025, b=0.1))
coef(fit)
           A            B            a            b 
2911.6448377    6.8320597  -49.1373979    0.0261391 
lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
ccc <- coef(fit)
lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")


From pro|jcn@@h @end|ng |rom gm@||@com  Thu May  7 15:33:32 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Thu, 7 May 2020 09:33:32 -0400
Subject: [R] unstable results of nlxb fit
In-Reply-To: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
Message-ID: <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>

The double exponential is well-known as a disaster to fit. Lanczos in his
1956 book Applied Analysis, p. 276 gives a good example which is worked through.
I've included it with scripts using nlxb in my 2014 book on Nonlinear Parameter
Optimization Using R Tools (Wiley). The scripts were on Wiley's site for the book,
but I've had difficulty getting Wiley to fix things and not checked lately if it
is still accessible. Ask off-list if you want the script and I'll dig into my
archives.

nlxb (preferably from nlsr which you used rather than nlmrt which is now not
maintained), will likely do as well as any general purpose code. There may be
special approaches that do a bit better, but I suspect the reality is that
the underlying problem is such that there are many sets of parameters with
widely different values that will get quite similar sums of squares.

Best, JN


On 2020-05-07 9:12 a.m., PIKAL Petr wrote:
> Dear all
> 
> I started to use nlxb instead of nls to get rid of singular gradient error.
> I try to fit double exponential function to my data, but results I obtain
> are strongly dependent on starting values. 
> 
> tsmes ~ A*exp(a*plast) + B* exp(b*plast)
> 
> Changing b from 0.1 to 0.01 gives me completely different results. I usually
> check result by a plot but could the result be inspected if it achieved good
> result without plotting?
> 
> Or is there any way how to perform such task?
> 
> Cheers
> Petr
> 
> Below is working example.
> 
>> dput(temp)
> temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33, 
> 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 
> 44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 
> 54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 
> 68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 
> 97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 
> 141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 
> 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 
> 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 
> 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90, 
> 91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 
> 102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 
> 112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")
> 
> library(nlsr)
> 
> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> start=list(A=1, B=15, a=0.025, b=0.01))
> coef(fit)
>            A            B            a            b 
> 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02 
> 
> plot(temp$plast, temp$tsmes, ylim=c(0,200))
> lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
> ccc <- coef(fit)
> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
> 
> # wrong fit with slightly different b
> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> start=list(A=1, B=15, a=0.025, b=0.1))
> coef(fit)
>            A            B            a            b 
> 2911.6448377    6.8320597  -49.1373979    0.0261391 
> lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
> ccc <- coef(fit)
> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @orenh @end|ng |rom m@th@@@u@dk  Thu May  7 15:42:59 2020
From: @orenh @end|ng |rom m@th@@@u@dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 7 May 2020 13:42:59 +0000
Subject: [R] Possible bug in optimize (related to naming the arguments)
Message-ID: <77eb990781cc46618e9cf78b8f1246ba@math.aau.dk>

Dear all,

I am wondering if there is a minor bug in the optimimize function; please see below:


---


> ## example taken from optimize documentation
> f <- function (x, a) (x - a)^2
> xmin <- optimize(f, c(0, 1), tol = 0.0001, a = 1/3)
> xmin
$minimum
[1] 0.3333333

$objective
[1] 0

> ## if we change argument a to j things still work fine
> f2 <- function (x, j) (x - j)^2
> xmin2 <- optimize(f2, c(0, 1), tol = 0.0001, j = 1/3)
> xmin2
$minimum
[1] 0.3333333

$objective
[1] 0

> ## if we change argument a to i things fail
> f3 <- function (x, i) (x - i)^2
> xmin3 <- optimize(f3, c(0, 1), tol = 0.0001, i = 1/3)
Error in optimize(f3, c(0, 1), tol = 1e-04, i = 1/3) :
  'xmin' not less than 'xmax'
> xmin3
$minimum
[1] 0.3333333

$objective
[1] 0

> ##Same here
> xmin3 <- optimize(f3, lower=0, upper=1, tol = 0.0001, i = 1/3)
Error in f(arg, ...) (from #1) : argument "i" is missing, with no default
> xmin3
$minimum
[1] 0.3333333

$objective
[1] 0

> ## a workaround is
> xmin3 <- optimize(f3, interval=c(0, 1), tol = 0.0001, i = 1/3)
> xmin3
$minimum
[1] 0.3333333

$objective
[1] 0

---

the problem is, I guess, due to the keyword 'interval' gets mixed
up with 'i'.

Has anyone experienced that?

Best regards
S?ren



	[[alternative HTML version deleted]]


From rub@k @end|ng |rom m@th@@@u@dk  Thu May  7 16:00:00 2020
From: rub@k @end|ng |rom m@th@@@u@dk (Ege Rubak)
Date: Thu, 7 May 2020 14:00:00 +0000
Subject: [R] Possible bug in optimize (related to naming the arguments)
In-Reply-To: <77eb990781cc46618e9cf78b8f1246ba@math.aau.dk>
References: <77eb990781cc46618e9cf78b8f1246ba@math.aau.dk>
Message-ID: <7a440435cb2517fcc40c94fdf5879d98c039dbb3.camel@math.aau.dk>

Dear S?ren,

I suspect that the good R souls wouldn't consider this a bug, but a
logical consequence of the R language design. It is of course a valid
question whether this should be explicitly mentioned in documentation.

If I recall correctly about function evaluation: First all named
arguments are found by partial matching, so in your example you are
really providing the value 1/3 for the argument `interval` which is a
user mistake. The exact same things happens for the apply family, e.g.

> myfun <- function(x, F) x-F
> sapply(1:3, myfun, F = 1)
Error in match.fun(FUN) : '1' is not a function, character or symbol

It works fine if we provide a valid value for the argument `FUN` rather
than the constant value `FUN = 1`:

> sapply(1:3, FUN = myfun, F = 1)
[1] 0 1 2

(I realize no sane person would use the argument name `F` in this case,
but you get the point.)

Best,
Ege

On Thu, 2020-05-07 at 13:42 +0000, S?ren H?jsgaard wrote:
> Dear all,
> 
> I am wondering if there is a minor bug in the optimimize function;
> please see below:
> 
> 
> ---
> 
> 
> > ## example taken from optimize documentation
> > f <- function (x, a) (x - a)^2
> > xmin <- optimize(f, c(0, 1), tol = 0.0001, a = 1/3)
> > xmin
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> > ## if we change argument a to j things still work fine
> > f2 <- function (x, j) (x - j)^2
> > xmin2 <- optimize(f2, c(0, 1), tol = 0.0001, j = 1/3)
> > xmin2
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> > ## if we change argument a to i things fail
> > f3 <- function (x, i) (x - i)^2
> > xmin3 <- optimize(f3, c(0, 1), tol = 0.0001, i = 1/3)
> 
> Error in optimize(f3, c(0, 1), tol = 1e-04, i = 1/3) :
>   'xmin' not less than 'xmax'
> > xmin3
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> > ##Same here
> > xmin3 <- optimize(f3, lower=0, upper=1, tol = 0.0001, i = 1/3)
> 
> Error in f(arg, ...) (from #1) : argument "i" is missing, with no
> default
> > xmin3
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> > ## a workaround is
> > xmin3 <- optimize(f3, interval=c(0, 1), tol = 0.0001, i = 1/3)
> > xmin3
> 
> $minimum
> [1] 0.3333333
> 
> $objective
> [1] 0
> 
> ---
> 
> the problem is, I guess, due to the keyword 'interval' gets mixed
> up with 'i'.
> 
> Has anyone experienced that?
> 
> Best regards
> S?ren
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University
Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

From murdoch@dunc@n @end|ng |rom gm@||@com  Thu May  7 16:07:41 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 7 May 2020 10:07:41 -0400
Subject: [R] unstable results of nlxb fit
In-Reply-To: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
Message-ID: <3d515cd3-ac48-fe4c-52c7-6b957dbe4634@gmail.com>

As John said, sums of exponentials are hard.  One thing that often helps 
a lot is to use the partially linear structure:  given a and b, you've 
got a linear model to compute A and B.  Now that you're down to two 
nonlinear parameters, you can draw a contour plot of nearby values to 
see how much of a mess you're dealing with.

Duncan Murdoch

On 07/05/2020 9:12 a.m., PIKAL Petr wrote:
> Dear all
> 
> I started to use nlxb instead of nls to get rid of singular gradient error.
> I try to fit double exponential function to my data, but results I obtain
> are strongly dependent on starting values.
> 
> tsmes ~ A*exp(a*plast) + B* exp(b*plast)
> 
> Changing b from 0.1 to 0.01 gives me completely different results. I usually
> check result by a plot but could the result be inspected if it achieved good
> result without plotting?
> 
> Or is there any way how to perform such task?
> 
> Cheers
> Petr
> 
> Below is working example.
> 
>> dput(temp)
> temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33,
> 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43,
> 44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54,
> 54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67,
> 68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96,
> 97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133,
> 141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54,
> 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66,
> 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78,
> 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90,
> 91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101,
> 102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112,
> 112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")
> 
> library(nlsr)
> 
> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> start=list(A=1, B=15, a=0.025, b=0.01))
> coef(fit)
>             A            B            a            b
> 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02
> 
> plot(temp$plast, temp$tsmes, ylim=c(0,200))
> lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
> ccc <- coef(fit)
> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
> 
> # wrong fit with slightly different b
> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> start=list(A=1, B=15, a=0.025, b=0.1))
> coef(fit)
>             A            B            a            b
> 2911.6448377    6.8320597  -49.1373979    0.0261391
> lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
> ccc <- coef(fit)
> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Thu May  7 16:09:04 2020
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Thu, 7 May 2020 14:09:04 +0000
Subject: [R] Change the colours of some of the labels of the x-axis
Message-ID: <8B435C9568170B469AE31E8891E8CC4F809B8064@ESINO.regionemarche.intra>

Dear R users,
in a plot I need to change the colours of some of the labels of the x-axis.
In the example below reported, I would like to have the labels of the last and second-last date in red.
I tried to find the solution searching the web, but I could not understand the hints and I was not sure I could adapt them to my example.
There might be an easy way to do that?

Thank you for your attention and your help
Stefano

first_day <- "2005-01-23-09-00"
last_day <- "2005-01-27-09-00"
first_day_POSIX <- as.POSIXct(first_day, format="%Y-%m-%d-%H-%M")
last_day_POSIX <- as.POSIXct(last_day, format="%Y-%m-%d-%H-%M")
df_plot <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="30 mins"))
df_plot$hs1 <- 5
df_plot$hs2 <- 7

plot(df_plot$data_POSIX, df_plot$hs1, type="b", ylim=c(0, 10), col="blue", xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
axis.POSIXct(1, at=seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"), "days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"), by="12 hours"), format="h%H-%d-%m-%y", pos=0)
axis(side=2, at=seq(0, 15, 5))


         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Thu May  7 16:14:52 2020
From: pd@|gd @end|ng |rom gm@||@com (Peter Dalgaard)
Date: Thu, 7 May 2020 16:14:52 +0200
Subject: [R] Possible bug in optimize (related to naming the arguments)
In-Reply-To: <7a440435cb2517fcc40c94fdf5879d98c039dbb3.camel@math.aau.dk>
References: <77eb990781cc46618e9cf78b8f1246ba@math.aau.dk>
 <7a440435cb2517fcc40c94fdf5879d98c039dbb3.camel@math.aau.dk>
Message-ID: <9C25BBA6-5C8E-45FD-8112-EAFA40E70A34@gmail.com>

Partial matching is a feature regretted by its designer, but is not really possible to remove at this point. One early change in R relative to S was to require full match of anything after "...", but as you have noticed, that does not apply for interval= 

So, "don't do that, then"

-pd

> On 7 May 2020, at 16:00 , Ege Rubak <rubak at math.aau.dk> wrote:
> 
> Dear S?ren,
> 
> I suspect that the good R souls wouldn't consider this a bug, but a
> logical consequence of the R language design. It is of course a valid
> question whether this should be explicitly mentioned in documentation.
> 
> If I recall correctly about function evaluation: First all named
> arguments are found by partial matching, so in your example you are
> really providing the value 1/3 for the argument `interval` which is a
> user mistake. The exact same things happens for the apply family, e.g.
> 
>> myfun <- function(x, F) x-F
>> sapply(1:3, myfun, F = 1)
> Error in match.fun(FUN) : '1' is not a function, character or symbol
> 
> It works fine if we provide a valid value for the argument `FUN` rather
> than the constant value `FUN = 1`:
> 
>> sapply(1:3, FUN = myfun, F = 1)
> [1] 0 1 2
> 
> (I realize no sane person would use the argument name `F` in this case,
> but you get the point.)
> 
> Best,
> Ege
> 
> On Thu, 2020-05-07 at 13:42 +0000, S?ren H?jsgaard wrote:
>> Dear all,
>> 
>> I am wondering if there is a minor bug in the optimimize function;
>> please see below:
>> 
>> 
>> ---
>> 
>> 
>>> ## example taken from optimize documentation
>>> f <- function (x, a) (x - a)^2
>>> xmin <- optimize(f, c(0, 1), tol = 0.0001, a = 1/3)
>>> xmin
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>>> ## if we change argument a to j things still work fine
>>> f2 <- function (x, j) (x - j)^2
>>> xmin2 <- optimize(f2, c(0, 1), tol = 0.0001, j = 1/3)
>>> xmin2
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>>> ## if we change argument a to i things fail
>>> f3 <- function (x, i) (x - i)^2
>>> xmin3 <- optimize(f3, c(0, 1), tol = 0.0001, i = 1/3)
>> 
>> Error in optimize(f3, c(0, 1), tol = 1e-04, i = 1/3) :
>>  'xmin' not less than 'xmax'
>>> xmin3
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>>> ##Same here
>>> xmin3 <- optimize(f3, lower=0, upper=1, tol = 0.0001, i = 1/3)
>> 
>> Error in f(arg, ...) (from #1) : argument "i" is missing, with no
>> default
>>> xmin3
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>>> ## a workaround is
>>> xmin3 <- optimize(f3, interval=c(0, 1), tol = 0.0001, i = 1/3)
>>> xmin3
>> 
>> $minimum
>> [1] 0.3333333
>> 
>> $objective
>> [1] 0
>> 
>> ---
>> 
>> the problem is, I guess, due to the keyword 'interval' gets mixed
>> up with 'i'.
>> 
>> Has anyone experienced that?
>> 
>> Best regards
>> S?ren
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> -- 
> Ege Rubak, Associate Professor,
> Department of Mathematical Sciences, Aalborg University
> Skjernvej 4A, 9220 Aalborg East, Denmark
> Phone: (+45)99408861
> Mobile: (+45)30230252
> Email: rubak at math.aau.dk
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu May  7 17:41:41 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 7 May 2020 16:41:41 +0100
Subject: [R] Change the colours of some of the labels of the x-axis
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B8064@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8064@ESINO.regionemarche.intra>
Message-ID: <dff646d6-dc28-5258-b300-275c82a9dfca@sapo.pt>

Hello,

You cannot pass a vector of colors to col.axis, you need to plot the 
axis twice, once the normal axis, like in your code, then overplot just 
those last 2 labels.


xlabels <- seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"), 
"days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"), 
by="12 hours")

plot(df_plot$data_POSIX, df_plot$hs1, type="b",
      xlim = c(min(xlabels), max(xlabels)), ylim=c(0, 10), col="blue", 
xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
axis.POSIXct(1, at = xlabels, format="h%H-%d-%m-%y", pos=0, las = 2)
axis.POSIXct(1, at = tail(xlabels, 2),
              format="h%H-%d-%m-%y", pos=0, las = 2, col.axis = "red")

axis(side=2, at=seq(0, 15, 5))



Hope this helps,

Rui Barradas

?s 15:09 de 07/05/20, Stefano Sofia escreveu:
> Dear R users,
> in a plot I need to change the colours of some of the labels of the x-axis.
> In the example below reported, I would like to have the labels of the last and second-last date in red.
> I tried to find the solution searching the web, but I could not understand the hints and I was not sure I could adapt them to my example.
> There might be an easy way to do that?
> 
> Thank you for your attention and your help
> Stefano
> 
> first_day <- "2005-01-23-09-00"
> last_day <- "2005-01-27-09-00"
> first_day_POSIX <- as.POSIXct(first_day, format="%Y-%m-%d-%H-%M")
> last_day_POSIX <- as.POSIXct(last_day, format="%Y-%m-%d-%H-%M")
> df_plot <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="30 mins"))
> df_plot$hs1 <- 5
> df_plot$hs2 <- 7
> 
> plot(df_plot$data_POSIX, df_plot$hs1, type="b", ylim=c(0, 10), col="blue", xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
> lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
> axis.POSIXct(1, at=seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"), "days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"), by="12 hours"), format="h%H-%d-%m-%y", pos=0)
> axis(side=2, at=seq(0, 15, 5))
> 
> 
>           (oo)
> --oOO--( )--OOo----------------
> Stefano Sofia PhD
> Civil Protection - Marche Region
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona
> Uff: 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> --
> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> This message was scanned by Libra ESVA and is believed to be clean.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From v@r|n@@ch@ @end|ng |rom y@hoo@|r  Thu May  7 22:17:15 2020
From: v@r|n@@ch@ @end|ng |rom y@hoo@|r (varin sacha)
Date: Thu, 7 May 2020 20:17:15 +0000 (UTC)
Subject: [R] Replication : How to get only 1 value
References: <526588415.1508527.1588882635098.ref@mail.yahoo.com>
Message-ID: <526588415.1508527.1588882635098@mail.yahoo.com>

Dear R-experts,

My goal is to get only 1 value : the average/ the mean of the 100 MSE values. How can I finish my R code ?

###################################################################
my.experiment <- function()? {

n<-500
x<-runif(n, 0, 5)
z <- rnorm(n, 2, 3)
a <- runif(n, 0, 5) 

y_model<- 0.1*x^3 - 0.5 * z^2 - a + 10
y_obs <- y_model +c( rnorm(n*0.97, 0, 0.1), rnorm(n*0.03, 0, 0.5) )
fit1<- lm(y_obs~x^3+z^2+a)

MSE<-mean((fit1$fitted.values - y_model)^2)
return( c(MSE) )
}

my.data = t(replicate( 100, my.experiment() ))
summary(my.data)
################################################################


From g||ted|||e2014 @end|ng |rom gm@||@com  Thu May  7 23:00:29 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Thu, 7 May 2020 22:00:29 +0100
Subject: [R] Adding overlap legend to a histogram
Message-ID: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>

Dear Experts,
Greetings.

I am trying to display two datasets in a histogram. I have been able to
plot the graph and added the legend for the two colors. I am, however,
having difficulties adding a legend to represent the regions of overlap
(the third legend).  Below are my data and code.

Thank you very much for your kind response.
Best wishes
Ogbos


Part of the two data are:
96 11 28 -5.0439243156971
96 11 30 -6.47673663925309
96 12 03 -6.82839470197342
96 12 05 -6.21642465505491
96 12 07 -6.25580747537018
96 12 10 -5.77540853474434
96 12 14 -3.98857218877879
96 12 16 -4.25191042337454
96 12 20 -3.20551034549018
96 12 24 -4.2702754047348
96 12 28 -6.03085851418479
96 12 31 -6.51403505358144
97 01 02 -6.32673280540791
97 01 08 -5.8537576420137
97 01 13 -5.54092007919419
97 01 23 -4.03617528404112
97 01 28 -6.21414660666954
97 01 30 -5.82248535055029
97 02 02 -4.52703090771556
97 02 04 -4.24731404905759
97 02 06 -5.22553031464346
97 02 10 -3.31737825603324
97 02 13 -1.48147648915881
97 02 16 -1.80195032791485
97 02 19 -2.16701154625054
97 02 21 -2.06571846213066
97 02 24 -3.39623775344558
97 03 02 -4.70829707054833
97 03 07 -3.73377684116639
97 03 11 -2.76476446486583
97 03 19 -2.30766786606313
97 03 21 -2.36150976836853
97 03 24 -1.84664525535518
97 04 01 -4.30618772775492
97 04 12 -4.51646004530582
97 04 19 -4.12507873323636
97 04 23 -3.46413598467606
97 04 25 -2.95371560706153
97 05 02 -4.04170720343953
97 05 13 -5.60975985858423
97 05 15 -5.52471869367602
97 05 23 -3.85761006637094
97 05 26 -3.47054347969788
97 05 29 -3.46506967197854
97 06 04 -0.307928317047413
97 06 08 -2.87534017937205
97 06 11 -3.00325467983916
97 06 14 -3.15331224991201
97 06 26 -3.14375164434693
97 07 02 -1.28362211606

and the second is:
98 05 02 -4.09334391050803
98 06 07 -0.161822520097844
98 06 19 -0.274151702680992
98 06 21 -0.285384620939307
98 06 25 -0.554974659138863
98 08 23 -2.22867947962777
98 08 27 -5.44129410150581
98 09 25 -4.46403021303242
98 11 09 -0.723468433013585
98 12 14 -0.824564697338419
99 01 16 -0.880729288629993
99 01 19 -0.566207577397178
99 01 21 -0.566207577397178
99 01 24 -4.99197737117322
99 02 13 -1.15031932682955
99 02 18 -5.733349976222
99 02 23 -0.869496370371678
99 03 06 -0.319083375714252
99 03 19 -1.35251185547922
99 03 24 -1.33004601896259
99 04 11 -0.229220029647733
99 04 16 -0.521275904363918
99 04 22 -1.2626485094127
99 05 07 -0.453878394814029
99 05 10 -1.35251185547922
99 05 24 -1.97032235968653
99 05 30 -0.330316293972566
99 06 06 -1.24018267289607
99 06 23 -0.0157945827397513
99 06 27 -2.86895582035172
99 07 03 -1.04922306250472
99 07 08 -0.375247967005826
99 07 24 -0.139356683581214
99 07 28 -1.50977271109562
99 07 31 -0.465111313072344
99 08 20 -4.35170103044927
99 08 22 -4.86841527033175
99 08 25 -4.03717931921646
99 09 01 -2.2511453161444
99 09 05 -3.22840920461779
99 09 07 -2.98128500293486
99 09 09 -3.28457379590937
99 09 16 -4.08211099224972
99 09 18 -4.00348056444151
99 09 21 -4.17197433831624
99 09 25 -3.11608002203464
99 09 29 -3.85745262708342
99 10 03 -3.77882219927522

and my code is:
data <- read.table("AUTOFD2a", col.names = c("year", "month", "day","fd"))

new.century <- data$year < 50

data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)

data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x1 = data$date

 library(date)

c1a<-rgb(1,0,0,8/9)
hist(x1,breaks="years",freq=T,axes=F,col=c1a)


data <- read.table("MANFD2a", col.names = c("year", "month", "day","fd"))

new.century <- data$year < 50

data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)

data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x2 = data$date

c2a=rgb(0,0,1,8/9)
hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
 axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x1)), by="years"))
 axis(2)
legend("topright", c("AUTO", "MANUAL"), col=c("red", "blue"), lwd=10)

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Thu May  7 23:24:29 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 07:24:29 +1000
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
Message-ID: <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>

Hi Subhamitra,
For some reason, your data didn't make it through. Maybe you tried to
send an .xls or .xlsx file. If so, export it as CSV or if it's not too
big, just paste the text into your email.

Jim

On Thu, May 7, 2020 at 10:30 PM Subhamitra Patra
<subhamitra.patra at gmail.com> wrote:
>
> Dear R-users,
>
> I want to estimate ARCH test for multiple columns (i.e.,  from 2:21 COLUMNS
> ) in my data. For this purpose, I want to run a loop to calculate ARCH test
> results for each column in the data frame. I tried by using for loop and
> lapply function, but unable to write a loop for computing the ARCH test
> simultaneously for each column (i.e., from 2:21 columns) of my data frame.
>
> Below is my ARCH test code which I want to estimate for multiple columns of
> the data frame in a loop.
>
> library(tseries)
>
> library(FinTS)
>
> ArchTest (A, lags=1, demean = FALSE)
>
> Hence, A is a vector for which the ARCH test result is calculated. Here, I
> want to write a loop so that the ArchTest can be calculated simultaneously
> for each column of my data frame. From ARCH test result, I require only the
> calculated Chi-square value and its p-value for each column that stored in
> another matrix or object for each column as an output file.
>
> For your convenience, I attached my sample data below. Please find it.
>
> Please help me for which I shall be always grateful to you.
>
> Thank you.
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
> 05/07/20,
> 05:51:03 PM
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Thu May  7 23:27:10 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Thu, 7 May 2020 22:27:10 +0100
Subject: [R] Replication : How to get only 1 value
In-Reply-To: <526588415.1508527.1588882635098@mail.yahoo.com>
References: <526588415.1508527.1588882635098.ref@mail.yahoo.com>
 <526588415.1508527.1588882635098@mail.yahoo.com>
Message-ID: <b3f025c0-ba7d-9b61-045d-8e461ccc7c56@sapo.pt>

Hello,

That's just

mean(my.data)

Note that

summary(t(my.data))

also gives the mean. (You need to transpose because the way you ran 
replicate outputs a 1x100 matrix.)


Hope this helps,

Rui Barradas


?s 21:17 de 07/05/20, varin sacha via R-help escreveu:
> Dear R-experts,
> 
> My goal is to get only 1 value : the average/ the mean of the 100 MSE values. How can I finish my R code ?
> 
> ###################################################################
> my.experiment <- function()? {
> 
> n<-500
> x<-runif(n, 0, 5)
> z <- rnorm(n, 2, 3)
> a <- runif(n, 0, 5)
> 
> y_model<- 0.1*x^3 - 0.5 * z^2 - a + 10
> y_obs <- y_model +c( rnorm(n*0.97, 0, 0.1), rnorm(n*0.03, 0, 0.5) )
> fit1<- lm(y_obs~x^3+z^2+a)
> 
> MSE<-mean((fit1$fitted.values - y_model)^2)
> return( c(MSE) )
> }
> 
> my.data = t(replicate( 100, my.experiment() ))
> summary(my.data)
> ################################################################
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Thu May  7 23:41:38 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard McGarvey)
Date: Thu, 7 May 2020 17:41:38 -0400 (EDT)
Subject: [R] unstable results of nlxb fit
In-Reply-To: <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
 <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>
Message-ID: <755635422.1450418.1588887698767@connect.xfinity.com>

John/Petr, I think there is an issue between a global optimum and local optima. I added a multistart loop around the code to see if I could find different solutions. Here is the code I added (I am not a great coder so please excuse any inefficiencies in this code segment):

# Multistart approach
NT <- 100
Results <- matrix(data=NA, nrow = NT, ncol=5, dimnames=list(NULL,c("SS", "A", "B", "a", "b")))
A1 <- runif(NT,0,100)
B1 <- runif(NT,0,100)
a1 <- runif(NT,0.0,0.1)
b1 <- runif(NT,0.0,0.1)
for (I in 1:NT) {
  if (A1[I] > B1[I]) { # Ensure that the A'a are always the lower so that nlxb() always converge to the same values
    A0 <- A1[I]
    a0 <- a1[I]
    A1[I] <- B1[I]
    a1[I] <- b1[I]
    B1[I] <- A0
    b1[I] <- a0
  }
  fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
              start=list(A=A1[I], B=B1[I], a=a1[I], b=b1[I]))
  ccc <- coef(fit)
  Results[I,1] <- fit$ssquares
  Results[I,2] <- ccc[1]
  Results[I,3] <- ccc[2]
  Results[I,4] <- ccc[3]
  Results[I,5] <- ccc[4]
}
Results

What I found is that the minimum SS generated at each trial had two distinct values, 417.8 and 3359.2. The A,B,a, and b values when the SS was 417.8 were all the same but I got different values for the case where the minimal SS was 3359.2. This indicates that the SS=417.8 may be the global minimum solution whereas the others are local optima. Here are the iteration results for a 100 trial multistart:

Results
           SS           A           B           a           b
  [1,] 3359.2  8.3546e+03  6.8321e+00   -1.988226  2.6139e-02
  [2,] 3359.2  8.2865e+03  6.8321e+00   -5.201735  2.6139e-02
  [3,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
  [4,] 3359.2  6.8321e+00  7.7888e+02    0.026139 -7.2812e-01
  [5,] 3359.2 -3.9020e+01  4.5852e+01    0.026139  2.6139e-02
  [6,] 3359.2  6.8321e+00  2.6310e+02    0.026139 -1.8116e+00
  [7,] 3359.2 -2.1509e+01  2.8341e+01    0.026139  2.6139e-02
  [8,] 3359.2 -3.8075e+01  4.4908e+01    0.026139  2.6139e-02
  [9,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [10,] 3359.2  1.2466e+04  6.8321e+00   -4.196000  2.6139e-02
 [11,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [12,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [13,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [14,] 3359.2  3.8018e+02  6.8321e+00   -0.806414  2.6139e-02
 [15,] 3359.2 -3.1921e+00  1.0024e+01    0.026139  2.6139e-02
 [16,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [17,] 3359.2 -1.5938e+01  2.2770e+01    0.026139  2.6139e-02
 [18,] 3359.2 -3.1205e+01  3.8037e+01    0.026139  2.6139e-02
 [19,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [20,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [21,] 3359.2  8.6627e+03  6.8321e+00   -3.319778  2.6139e-02
 [22,] 3359.2  6.8321e+00  1.9318e+01    0.026139 -6.5773e-01
 [23,] 3359.2  6.2991e+01 -5.6159e+01    0.026139  2.6139e-02
 [24,] 3359.2  2.8865e-03  6.8321e+00   -1.576307  2.6139e-02
 [25,] 3359.2 -1.2496e+01  1.9328e+01    0.026139  2.6139e-02
 [26,] 3359.2 -5.9432e+00  1.2775e+01    0.026139  2.6139e-02
 [27,] 3359.2  1.6884e+02  6.8321e+00 -211.866423  2.6139e-02
 [28,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [29,] 3359.2  5.4972e+03  6.8321e+00   -3.432094  2.6139e-02
 [30,] 3359.2  6.8321e+00  1.4427e+03    0.026139 -4.2771e+02
 [31,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [32,] 3359.2  3.5760e+01 -2.8928e+01    0.026139  2.6139e-02
 [33,] 3359.2  6.8321e+00 -4.0737e+02    0.026139 -6.7152e-01
 [34,] 3359.2  6.8321e+00  1.2638e+04    0.026139 -2.8070e+00
 [35,] 3359.2  1.1813e+01 -4.9807e+00    0.026139  2.6139e-02
 [36,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [37,] 3359.2  6.8321e+00  1.2281e+03    0.026139 -3.0702e+02
 [38,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [39,] 3359.2 -2.6850e+01  3.3682e+01    0.026139  2.6139e-02
 [40,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [41,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [42,] 3359.2 -2.3279e+01  3.0111e+01    0.026139  2.6139e-02
 [43,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [44,] 3359.2  6.8321e+00  1.4550e+03    0.026139 -4.0303e+00
 [45,] 3359.2 -1.1386e+01  1.8218e+01    0.026139  2.6139e-02
 [46,] 3359.2  8.8026e+02  6.8321e+00  -65.430608  2.6139e-02
 [47,] 3359.2 -8.1985e+00  1.5031e+01    0.026139  2.6139e-02
 [48,] 3359.2 -6.7767e+00  1.3609e+01    0.026139  2.6139e-02
 [49,] 3359.2 -1.1436e+01  1.8268e+01    0.026139  2.6139e-02
 [50,] 3359.2  1.0710e+04  6.8321e+00   -2.349659  2.6139e-02
 [51,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [52,] 3359.2  6.8321e+00  7.1837e+02    0.026139 -7.4681e-01
 [53,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [54,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [55,] 3359.2 -4.8774e+00  6.8321e+00  -16.405584  2.6139e-02
 [56,] 3359.2  1.2687e+03  6.8321e+00   -3.775998  2.6139e-02
 [57,] 3359.2  1.5529e+01 -8.6967e+00    0.026139  2.6139e-02
 [58,] 3359.2 -1.0003e+01  1.6835e+01    0.026139  2.6139e-02
 [59,] 3359.2  6.8321e+00  3.9291e+02    0.026139 -4.1974e+02
 [60,] 3359.2 -2.1880e+01  2.8712e+01    0.026139  2.6139e-02
 [61,] 3359.2  4.1736e+03  6.8321e+00  -10.711457  2.6139e-02
 [62,] 3359.2 -3.3185e+01  4.0017e+01    0.026139  2.6139e-02
 [63,] 3359.2  7.6732e+02  6.8321e+00   -0.723977  2.6139e-02
 [64,] 3359.2  1.5334e+04  6.8321e+00  -52.573620  2.6139e-02
 [65,] 3359.2 -2.9556e+01  3.6388e+01    0.026139  2.6139e-02
 [66,] 3359.2 -1.0447e+00  7.8767e+00    0.026139  2.6139e-02
 [67,] 3359.2  6.8321e+00  2.1471e+02    0.026139 -7.0582e+01
 [68,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [69,] 3359.2 -2.2293e+01  2.9126e+01    0.026139  2.6139e-02
 [70,] 3359.2  6.2259e+02  6.8321e+00   -2.782527  2.6139e-02
 [71,] 3359.2 -1.4639e+01  2.1471e+01    0.026139  2.6139e-02
 [72,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [73,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [74,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [75,] 3359.2 -2.3449e+01  3.0281e+01    0.026139  2.6139e-02
 [76,] 3359.2 -2.5926e+01  6.8321e+00   -0.663656  2.6139e-02
 [77,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [78,] 3359.2  6.8321e+00  6.9426e+02    0.026139 -1.9442e+00
 [79,] 3359.2  2.8684e+02  6.8321e+00   -0.854394  2.6139e-02
 [80,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [81,] 3359.2 -4.5066e+01  5.1899e+01    0.026139  2.6139e-02
 [82,] 3359.2  4.4678e+03  6.8321e+00   -2.109446  2.6139e-02
 [83,] 3359.2  3.1376e+03  6.8321e+00   -1.104803  2.6139e-02
 [84,] 3359.2  6.8321e+00  1.1167e+02    0.026139 -1.0280e+00
 [85,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [86,] 3359.2  5.3864e+02  6.8321e+00   -0.657971  2.6139e-02
 [87,] 3359.2  4.8227e+01  6.8321e+00   -2.304024  2.6139e-02
 [88,] 3359.2 -2.2048e+01  2.8880e+01    0.026139  2.6139e-02
 [89,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
 [90,] 3359.2  6.8321e+00 -4.1689e+01    0.026139 -3.6049e+00
 [91,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
 [92,] 3359.2 -4.1265e+01  4.8097e+01    0.026139  2.6139e-02
 [93,] 3359.2 -1.1565e+01  1.8397e+01    0.026139  2.6139e-02
 [94,] 3359.2  2.3698e+01 -1.6866e+01    0.026139  2.6139e-02
 [95,] 3359.2  4.4700e+03  6.8321e+00  -12.836180  2.6139e-02
 [96,] 3359.2  4.6052e+04  6.8321e+00   -7.158584  2.6139e-02
 [97,] 3359.2  2.5464e+03  6.8321e+00   -1.811626  2.6139e-02
 [98,] 3359.2  6.8321e+00  1.0338e+03    0.026139 -1.5365e+01
 [99,] 3359.2  1.3783e+01 -6.9507e+00    0.026139  2.6139e-02
[100,] 3359.2  6.8321e+00  6.7153e+02    0.026139 -1.5975e+03


Hope this helps,

Bernard McGarvey


Director, Fort Myers Beach Lions Foundation, Inc.


Retired (Lilly Engineering Fellow).

> On May 7, 2020 at 9:33 AM J C Nash <profjcnash at gmail.com> wrote:
> 
> 
> The double exponential is well-known as a disaster to fit. Lanczos in his
> 1956 book Applied Analysis, p. 276 gives a good example which is worked through.
> I've included it with scripts using nlxb in my 2014 book on Nonlinear Parameter
> Optimization Using R Tools (Wiley). The scripts were on Wiley's site for the book,
> but I've had difficulty getting Wiley to fix things and not checked lately if it
> is still accessible. Ask off-list if you want the script and I'll dig into my
> archives.
> 
> nlxb (preferably from nlsr which you used rather than nlmrt which is now not
> maintained), will likely do as well as any general purpose code. There may be
> special approaches that do a bit better, but I suspect the reality is that
> the underlying problem is such that there are many sets of parameters with
> widely different values that will get quite similar sums of squares.
> 
> Best, JN
> 
> 
> On 2020-05-07 9:12 a.m., PIKAL Petr wrote:
> > Dear all
> > 
> > I started to use nlxb instead of nls to get rid of singular gradient error.
> > I try to fit double exponential function to my data, but results I obtain
> > are strongly dependent on starting values. 
> > 
> > tsmes ~ A*exp(a*plast) + B* exp(b*plast)
> > 
> > Changing b from 0.1 to 0.01 gives me completely different results. I usually
> > check result by a plot but could the result be inspected if it achieved good
> > result without plotting?
> > 
> > Or is there any way how to perform such task?
> > 
> > Cheers
> > Petr
> > 
> > Below is working example.
> > 
> >> dput(temp)
> > temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33, 
> > 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 
> > 44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 
> > 54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 
> > 68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 
> > 97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 
> > 141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 
> > 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 
> > 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 
> > 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90, 
> > 91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 
> > 102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 
> > 112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")
> > 
> > library(nlsr)
> > 
> > fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> > start=list(A=1, B=15, a=0.025, b=0.01))
> > coef(fit)
> >            A            B            a            b 
> > 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02 
> > 
> > plot(temp$plast, temp$tsmes, ylim=c(0,200))
> > lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
> > ccc <- coef(fit)
> > lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
> > lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
> > 
> > # wrong fit with slightly different b
> > fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> > start=list(A=1, B=15, a=0.025, b=0.1))
> > coef(fit)
> >            A            B            a            b 
> > 2911.6448377    6.8320597  -49.1373979    0.0261391 
> > lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
> > ccc <- coef(fit)
> > lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
> > lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
> > 
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Fri May  8 00:00:11 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Thu, 7 May 2020 18:00:11 -0400
Subject: [R] unstable results of nlxb fit
In-Reply-To: <755635422.1450418.1588887698767@connect.xfinity.com>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
 <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>
 <755635422.1450418.1588887698767@connect.xfinity.com>
Message-ID: <8cfa9396-7a22-ba80-b8be-48dd935fe408@gmail.com>

These results reflect my experience with this sort of problem.

A couple of comments:

1) optimx package has a multistart wrapper. I probably should have written one for
nlsr. Maybe Bernard and I should work on that. The issues are largely to make things
resistant to silly inputs, which even the smart users (you know, the ones looking
back from the mirror) introduce.

2) Sometimes using the bounds constraint capability in nlsr can be helpful, e.g.,
to ensure the exponent parameters are kept apart, can be useful.

3) Combining with Duncan's suggestion of solving for the linear parameters also
helps.

All of the above can be sensitive to particular data.

Best, JN

On 2020-05-07 5:41 p.m., Bernard McGarvey wrote:
> John/Petr, I think there is an issue between a global optimum and local optima. I added a multistart loop around the code to see if I could find different solutions. Here is the code I added (I am not a great coder so please excuse any inefficiencies in this code segment):
> 
> # Multistart approach
> NT <- 100
> Results <- matrix(data=NA, nrow = NT, ncol=5, dimnames=list(NULL,c("SS", "A", "B", "a", "b")))
> A1 <- runif(NT,0,100)
> B1 <- runif(NT,0,100)
> a1 <- runif(NT,0.0,0.1)
> b1 <- runif(NT,0.0,0.1)
> for (I in 1:NT) {
>   if (A1[I] > B1[I]) { # Ensure that the A'a are always the lower so that nlxb() always converge to the same values
>     A0 <- A1[I]
>     a0 <- a1[I]
>     A1[I] <- B1[I]
>     a1[I] <- b1[I]
>     B1[I] <- A0
>     b1[I] <- a0
>   }
>   fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
>               start=list(A=A1[I], B=B1[I], a=a1[I], b=b1[I]))
>   ccc <- coef(fit)
>   Results[I,1] <- fit$ssquares
>   Results[I,2] <- ccc[1]
>   Results[I,3] <- ccc[2]
>   Results[I,4] <- ccc[3]
>   Results[I,5] <- ccc[4]
> }
> Results
> 
> What I found is that the minimum SS generated at each trial had two distinct values, 417.8 and 3359.2. The A,B,a, and b values when the SS was 417.8 were all the same but I got different values for the case where the minimal SS was 3359.2. This indicates that the SS=417.8 may be the global minimum solution whereas the others are local optima. Here are the iteration results for a 100 trial multistart:
> 
> Results
>            SS           A           B           a           b
>   [1,] 3359.2  8.3546e+03  6.8321e+00   -1.988226  2.6139e-02
>   [2,] 3359.2  8.2865e+03  6.8321e+00   -5.201735  2.6139e-02
>   [3,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>   [4,] 3359.2  6.8321e+00  7.7888e+02    0.026139 -7.2812e-01
>   [5,] 3359.2 -3.9020e+01  4.5852e+01    0.026139  2.6139e-02
>   [6,] 3359.2  6.8321e+00  2.6310e+02    0.026139 -1.8116e+00
>   [7,] 3359.2 -2.1509e+01  2.8341e+01    0.026139  2.6139e-02
>   [8,] 3359.2 -3.8075e+01  4.4908e+01    0.026139  2.6139e-02
>   [9,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [10,] 3359.2  1.2466e+04  6.8321e+00   -4.196000  2.6139e-02
>  [11,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [12,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [13,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [14,] 3359.2  3.8018e+02  6.8321e+00   -0.806414  2.6139e-02
>  [15,] 3359.2 -3.1921e+00  1.0024e+01    0.026139  2.6139e-02
>  [16,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [17,] 3359.2 -1.5938e+01  2.2770e+01    0.026139  2.6139e-02
>  [18,] 3359.2 -3.1205e+01  3.8037e+01    0.026139  2.6139e-02
>  [19,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [20,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [21,] 3359.2  8.6627e+03  6.8321e+00   -3.319778  2.6139e-02
>  [22,] 3359.2  6.8321e+00  1.9318e+01    0.026139 -6.5773e-01
>  [23,] 3359.2  6.2991e+01 -5.6159e+01    0.026139  2.6139e-02
>  [24,] 3359.2  2.8865e-03  6.8321e+00   -1.576307  2.6139e-02
>  [25,] 3359.2 -1.2496e+01  1.9328e+01    0.026139  2.6139e-02
>  [26,] 3359.2 -5.9432e+00  1.2775e+01    0.026139  2.6139e-02
>  [27,] 3359.2  1.6884e+02  6.8321e+00 -211.866423  2.6139e-02
>  [28,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [29,] 3359.2  5.4972e+03  6.8321e+00   -3.432094  2.6139e-02
>  [30,] 3359.2  6.8321e+00  1.4427e+03    0.026139 -4.2771e+02
>  [31,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [32,] 3359.2  3.5760e+01 -2.8928e+01    0.026139  2.6139e-02
>  [33,] 3359.2  6.8321e+00 -4.0737e+02    0.026139 -6.7152e-01
>  [34,] 3359.2  6.8321e+00  1.2638e+04    0.026139 -2.8070e+00
>  [35,] 3359.2  1.1813e+01 -4.9807e+00    0.026139  2.6139e-02
>  [36,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [37,] 3359.2  6.8321e+00  1.2281e+03    0.026139 -3.0702e+02
>  [38,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [39,] 3359.2 -2.6850e+01  3.3682e+01    0.026139  2.6139e-02
>  [40,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [41,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [42,] 3359.2 -2.3279e+01  3.0111e+01    0.026139  2.6139e-02
>  [43,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [44,] 3359.2  6.8321e+00  1.4550e+03    0.026139 -4.0303e+00
>  [45,] 3359.2 -1.1386e+01  1.8218e+01    0.026139  2.6139e-02
>  [46,] 3359.2  8.8026e+02  6.8321e+00  -65.430608  2.6139e-02
>  [47,] 3359.2 -8.1985e+00  1.5031e+01    0.026139  2.6139e-02
>  [48,] 3359.2 -6.7767e+00  1.3609e+01    0.026139  2.6139e-02
>  [49,] 3359.2 -1.1436e+01  1.8268e+01    0.026139  2.6139e-02
>  [50,] 3359.2  1.0710e+04  6.8321e+00   -2.349659  2.6139e-02
>  [51,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [52,] 3359.2  6.8321e+00  7.1837e+02    0.026139 -7.4681e-01
>  [53,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [54,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [55,] 3359.2 -4.8774e+00  6.8321e+00  -16.405584  2.6139e-02
>  [56,] 3359.2  1.2687e+03  6.8321e+00   -3.775998  2.6139e-02
>  [57,] 3359.2  1.5529e+01 -8.6967e+00    0.026139  2.6139e-02
>  [58,] 3359.2 -1.0003e+01  1.6835e+01    0.026139  2.6139e-02
>  [59,] 3359.2  6.8321e+00  3.9291e+02    0.026139 -4.1974e+02
>  [60,] 3359.2 -2.1880e+01  2.8712e+01    0.026139  2.6139e-02
>  [61,] 3359.2  4.1736e+03  6.8321e+00  -10.711457  2.6139e-02
>  [62,] 3359.2 -3.3185e+01  4.0017e+01    0.026139  2.6139e-02
>  [63,] 3359.2  7.6732e+02  6.8321e+00   -0.723977  2.6139e-02
>  [64,] 3359.2  1.5334e+04  6.8321e+00  -52.573620  2.6139e-02
>  [65,] 3359.2 -2.9556e+01  3.6388e+01    0.026139  2.6139e-02
>  [66,] 3359.2 -1.0447e+00  7.8767e+00    0.026139  2.6139e-02
>  [67,] 3359.2  6.8321e+00  2.1471e+02    0.026139 -7.0582e+01
>  [68,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [69,] 3359.2 -2.2293e+01  2.9126e+01    0.026139  2.6139e-02
>  [70,] 3359.2  6.2259e+02  6.8321e+00   -2.782527  2.6139e-02
>  [71,] 3359.2 -1.4639e+01  2.1471e+01    0.026139  2.6139e-02
>  [72,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [73,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [74,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [75,] 3359.2 -2.3449e+01  3.0281e+01    0.026139  2.6139e-02
>  [76,] 3359.2 -2.5926e+01  6.8321e+00   -0.663656  2.6139e-02
>  [77,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [78,] 3359.2  6.8321e+00  6.9426e+02    0.026139 -1.9442e+00
>  [79,] 3359.2  2.8684e+02  6.8321e+00   -0.854394  2.6139e-02
>  [80,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [81,] 3359.2 -4.5066e+01  5.1899e+01    0.026139  2.6139e-02
>  [82,] 3359.2  4.4678e+03  6.8321e+00   -2.109446  2.6139e-02
>  [83,] 3359.2  3.1376e+03  6.8321e+00   -1.104803  2.6139e-02
>  [84,] 3359.2  6.8321e+00  1.1167e+02    0.026139 -1.0280e+00
>  [85,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [86,] 3359.2  5.3864e+02  6.8321e+00   -0.657971  2.6139e-02
>  [87,] 3359.2  4.8227e+01  6.8321e+00   -2.304024  2.6139e-02
>  [88,] 3359.2 -2.2048e+01  2.8880e+01    0.026139  2.6139e-02
>  [89,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
>  [90,] 3359.2  6.8321e+00 -4.1689e+01    0.026139 -3.6049e+00
>  [91,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
>  [92,] 3359.2 -4.1265e+01  4.8097e+01    0.026139  2.6139e-02
>  [93,] 3359.2 -1.1565e+01  1.8397e+01    0.026139  2.6139e-02
>  [94,] 3359.2  2.3698e+01 -1.6866e+01    0.026139  2.6139e-02
>  [95,] 3359.2  4.4700e+03  6.8321e+00  -12.836180  2.6139e-02
>  [96,] 3359.2  4.6052e+04  6.8321e+00   -7.158584  2.6139e-02
>  [97,] 3359.2  2.5464e+03  6.8321e+00   -1.811626  2.6139e-02
>  [98,] 3359.2  6.8321e+00  1.0338e+03    0.026139 -1.5365e+01
>  [99,] 3359.2  1.3783e+01 -6.9507e+00    0.026139  2.6139e-02
> [100,] 3359.2  6.8321e+00  6.7153e+02    0.026139 -1.5975e+03
> 
> 
> Hope this helps,
> 
> Bernard McGarvey
> 
> 
> Director, Fort Myers Beach Lions Foundation, Inc.
> 
> 
> Retired (Lilly Engineering Fellow).
> 
>> On May 7, 2020 at 9:33 AM J C Nash <profjcnash at gmail.com> wrote:
>>
>>
>> The double exponential is well-known as a disaster to fit. Lanczos in his
>> 1956 book Applied Analysis, p. 276 gives a good example which is worked through.
>> I've included it with scripts using nlxb in my 2014 book on Nonlinear Parameter
>> Optimization Using R Tools (Wiley). The scripts were on Wiley's site for the book,
>> but I've had difficulty getting Wiley to fix things and not checked lately if it
>> is still accessible. Ask off-list if you want the script and I'll dig into my
>> archives.
>>
>> nlxb (preferably from nlsr which you used rather than nlmrt which is now not
>> maintained), will likely do as well as any general purpose code. There may be
>> special approaches that do a bit better, but I suspect the reality is that
>> the underlying problem is such that there are many sets of parameters with
>> widely different values that will get quite similar sums of squares.
>>
>> Best, JN
>>
>>
>> On 2020-05-07 9:12 a.m., PIKAL Petr wrote:
>>> Dear all
>>>
>>> I started to use nlxb instead of nls to get rid of singular gradient error.
>>> I try to fit double exponential function to my data, but results I obtain
>>> are strongly dependent on starting values. 
>>>
>>> tsmes ~ A*exp(a*plast) + B* exp(b*plast)
>>>
>>> Changing b from 0.1 to 0.01 gives me completely different results. I usually
>>> check result by a plot but could the result be inspected if it achieved good
>>> result without plotting?
>>>
>>> Or is there any way how to perform such task?
>>>
>>> Cheers
>>> Petr
>>>
>>> Below is working example.
>>>
>>>> dput(temp)
>>> temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33, 
>>> 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 
>>> 44, 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 
>>> 54, 55, 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 
>>> 68, 70, 72, 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 
>>> 97, 99, 100, 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 
>>> 141, 153, 163, 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 
>>> 55, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 
>>> 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 
>>> 78, 79, 80, 81, 82, 83, 84, 85, 85, 86, 86, 87, 88, 88, 89, 90, 
>>> 91, 91, 93, 93, 94, 95, 96, 96, 97, 98, 98, 99, 100, 100, 101, 
>>> 102, 103, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 
>>> 112, 113, 113, 114, 115, 116)), row.names = 2411:2500, class = "data.frame")
>>>
>>> library(nlsr)
>>>
>>> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
>>> start=list(A=1, B=15, a=0.025, b=0.01))
>>> coef(fit)
>>>            A            B            a            b 
>>> 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02 
>>>
>>> plot(temp$plast, temp$tsmes, ylim=c(0,200))
>>> lines(temp$plast, predict(fit, newdata=temp), col="pink", lwd=3)
>>> ccc <- coef(fit)
>>> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
>>> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
>>>
>>> # wrong fit with slightly different b
>>> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
>>> start=list(A=1, B=15, a=0.025, b=0.1))
>>> coef(fit)
>>>            A            B            a            b 
>>> 2911.6448377    6.8320597  -49.1373979    0.0261391 
>>> lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3)
>>> ccc <- coef(fit)
>>> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
>>> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Fri May  8 02:12:56 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 10:12:56 +1000
Subject: [R] Adding overlap legend to a histogram
In-Reply-To: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
References: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
Message-ID: <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>

Hi Ogbos,
I don't think that your example allows us to work out what you are
trying to do. For one thing, "x1" and "x2" do not overlap. Also, you
are plotting the frequencies of dates of observations, which may not
be what you want.
The following code will correctly display your example:

hist(x1,breaks="years",freq=T,axes=F,xlim=c(9400,11100),col=c1a)
hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
 axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x2)), by="years"))
 axis(2)
legend("topleft", c("AUTO", "MANUAL"), fill=c("red", "blue"))

What it is displaying is the frequency of observations in your two
vectors by calendar year. If this is what you want, and you can
explain how you would like "overlap" to be displayed, we can probably
provide better help.

Jim


On Fri, May 8, 2020 at 7:01 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Experts,
> Greetings.
>
> I am trying to display two datasets in a histogram. I have been able to
> plot the graph and added the legend for the two colors. I am, however,
> having difficulties adding a legend to represent the regions of overlap
> (the third legend).  Below are my data and code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri May  8 04:17:56 2020
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 7 May 2020 21:17:56 -0500
Subject: [R] Error: Cannot use `+.gg()` with a single argument.
Message-ID: <CAF9-5jPRu09gA=oNHZvhUeyoY2pY=pdr55=Md_tB7FFatkiHiw@mail.gmail.com>

Hello,

I got this error:
Error: Cannot use `+.gg()` with a single argument. Did you
accidentally put + on a new line?

After running this:
data(murders)
library(ggplot2)
library(dplyr)
library(ggplot2)
ggplot(data=murders)

#define the slope of the line
r<-murders %>% summarize(rate=sum(total)/sum(population)*10^6) %>%.$rate
#mamke the plot
murders %>% ggplot(aes(population/10^6,total,label=abb))+
  +geom_abline(intercept = log10(r),lty=2,color="darkgrey")+
  +geom_point(aes(col=region), size=3)+
  +geom_text_repel()+
  +scale_x_log10()+
  +scale_y_log10()+
  +xlab("Populations in millions (log scale)")+
  +ylab("Total number of murders (log scale)")+
  +ggtitle("US Gun Murders in US 2010")+
  +scale_color_discrete(name="Region")+
  +theme_economist()

Is this an issue with my dplyr? Or how I can fix this code in order to work?

Thanks
Ana


From rmh @end|ng |rom temp|e@edu  Fri May  8 05:27:07 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Thu, 7 May 2020 23:27:07 -0400
Subject: [R] Error: Cannot use `+.gg()` with a single argument.
In-Reply-To: <CAF9-5jPRu09gA=oNHZvhUeyoY2pY=pdr55=Md_tB7FFatkiHiw@mail.gmail.com>
References: <CAF9-5jPRu09gA=oNHZvhUeyoY2pY=pdr55=Md_tB7FFatkiHiw@mail.gmail.com>
Message-ID: <CAGx1TMCJZxi2Veswfozu3PC5PmwgNCeM07k2HqCk+kiLQ9-T+A@mail.gmail.com>

It is just like the message suggested. You have a + at the end of each line
and
the beginning of the next.  The one at the end is required.  The ones at
the beginning are
causing the error message.

Please put spaces around your assignment arrows.
Difficult to read:  r<-murders
Easy to read:    r <- murders

On Thu, May 7, 2020 at 10:46 PM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> Hello,
>
> I got this error:
> Error: Cannot use `+.gg()` with a single argument. Did you
> accidentally put + on a new line?
>
> After running this:
> data(murders)
> library(ggplot2)
> library(dplyr)
> library(ggplot2)
> ggplot(data=murders)
>
> #define the slope of the line
> r<-murders %>% summarize(rate=sum(total)/sum(population)*10^6) %>%.$rate
> #mamke the plot
> murders %>% ggplot(aes(population/10^6,total,label=abb))+
>   +geom_abline(intercept = log10(r),lty=2,color="darkgrey")+
>   +geom_point(aes(col=region), size=3)+
>   +geom_text_repel()+
>   +scale_x_log10()+
>   +scale_y_log10()+
>   +xlab("Populations in millions (log scale)")+
>   +ylab("Total number of murders (log scale)")+
>   +ggtitle("US Gun Murders in US 2010")+
>   +scale_color_discrete(name="Region")+
>   +theme_economist()
>
> Is this an issue with my dplyr? Or how I can fix this code in order to
> work?
>
> Thanks
> Ana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri May  8 08:06:20 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 8 May 2020 07:06:20 +0100
Subject: [R] Adding overlap legend to a histogram
In-Reply-To: <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>
References: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
 <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>
Message-ID: <CAC8ss30gDSqkYtSTO-v_H+Z4XJq4Zn2oZ3uCzhnnYREC0fJPtg@mail.gmail.com>

Dear Jim,
Thank you for looking into this.
Sorry, there was actually no overlap in the small part of the data I
reported. My error of omission.

So when I run my full data with the adjustment you made, I got some thing
that was far from what I was expecting. That tell me that I need to send
the complete data to enable you correctly adjust the code, especially in
the light of the missing/present overlap.
I have used deput function to attach the two files. Please use any symbol
to depict the color/mark/legend of the overlap dates (just to enable the
reader visualize what is going on). I am actually trying to display event
frequency/occurrence per year.

Thank you and warmest regards
Ogbos

On Fri, May 8, 2020 at 1:13 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Ogbos,
> I don't think that your example allows us to work out what you are
> trying to do. For one thing, "x1" and "x2" do not overlap. Also, you
> are plotting the frequencies of dates of observations, which may not
> be what you want.
> The following code will correctly display your example:
>
> hist(x1,breaks="years",freq=T,axes=F,xlim=c(9400,11100),col=c1a)
> hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
>  axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x2)), by="years"))
>  axis(2)
> legend("topleft", c("AUTO", "MANUAL"), fill=c("red", "blue"))
>
> What it is displaying is the frequency of observations in your two
> vectors by calendar year. If this is what you want, and you can
> explain how you would like "overlap" to be displayed, we can probably
> provide better help.
>
> Jim
>
>
> On Fri, May 8, 2020 at 7:01 AM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >
> > Dear Experts,
> > Greetings.
> >
> > I am trying to display two datasets in a histogram. I have been able to
> > plot the graph and added the legend for the two colors. I am, however,
> > having difficulties adding a legend to represent the regions of overlap
> > (the third legend).  Below are my data and code.
>

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  8 08:59:08 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 May 2020 07:59:08 +0100
Subject: [R] Error: Cannot use `+.gg()` with a single argument.
In-Reply-To: <CAGx1TMCJZxi2Veswfozu3PC5PmwgNCeM07k2HqCk+kiLQ9-T+A@mail.gmail.com>
References: <CAF9-5jPRu09gA=oNHZvhUeyoY2pY=pdr55=Md_tB7FFatkiHiw@mail.gmail.com>
 <CAGx1TMCJZxi2Veswfozu3PC5PmwgNCeM07k2HqCk+kiLQ9-T+A@mail.gmail.com>
Message-ID: <75bec72b-6de9-7d92-7733-bb54939b00b8@sapo.pt>

Hello,

Richard's answer solves the problem, I'm writing about details in the 
OP's post.

Ana, your code example is missing some library() calls:

library(ggplot2)
library(ggrepel)
library(ggthemes)
library(dplyr)

# and where to find the data set
data(murders, package = "dslabs")


As for the dplyr pipe, you end it with .$rate to pull only that column's 
value, the following avoids that construct and is, I believe, more 
idiomatic:

#define the slope of the line
r <- murders %>%
   summarize(rate = sum(total)/sum(population)*10^6) %>%
   pull(rate)


Hope this helps,

Rui Barradas


?s 04:27 de 08/05/20, Richard M. Heiberger escreveu:
> It is just like the message suggested. You have a + at the end of each line
> and
> the beginning of the next.  The one at the end is required.  The ones at
> the beginning are
> causing the error message.
> 
> Please put spaces around your assignment arrows.
> Difficult to read:  r<-murders
> Easy to read:    r <- murders
> 
> On Thu, May 7, 2020 at 10:46 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> 
>> Hello,
>>
>> I got this error:
>> Error: Cannot use `+.gg()` with a single argument. Did you
>> accidentally put + on a new line?
>>
>> After running this:
>> data(murders)
>> library(ggplot2)
>> library(dplyr)
>> library(ggplot2)
>> ggplot(data=murders)
>>
>> #define the slope of the line
>> r<-murders %>% summarize(rate=sum(total)/sum(population)*10^6) %>%.$rate
>> #mamke the plot
>> murders %>% ggplot(aes(population/10^6,total,label=abb))+
>>    +geom_abline(intercept = log10(r),lty=2,color="darkgrey")+
>>    +geom_point(aes(col=region), size=3)+
>>    +geom_text_repel()+
>>    +scale_x_log10()+
>>    +scale_y_log10()+
>>    +xlab("Populations in millions (log scale)")+
>>    +ylab("Total number of murders (log scale)")+
>>    +ggtitle("US Gun Murders in US 2010")+
>>    +scale_color_discrete(name="Region")+
>>    +theme_economist()
>>
>> Is this an issue with my dplyr? Or how I can fix this code in order to
>> work?
>>
>> Thanks
>> Ana
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Fri May  8 09:24:40 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Fri, 8 May 2020 12:54:40 +0530
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
Message-ID: <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>

Dear Sir,

Herewith I am pasting a part of my sample data having 12 columns below, and
want to calculate ARCH test for the 12 columns by using a loop.

Please help me in this regard. Thank you very much for your help.

Year_Month A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 A11 A12
94-Jan 0.051197 7.05E-05 0.058806 -0.00818 0.538001 0.009766 0.025787
0.035478 0.056663 0.014665 0.23132 0.008644
94-Feb 0.06424 -0.01086 0.049823 -0.04989 0.557945 0.00974 0.027757 0.021494
0.016947 0.014584 0.229776 -0.02317
94-Mar 0.056168 -0.00626 0.061555 -0.03427 0.524705 0.009694 0.027632
-0.00656 0.008358 0.014499 0.190421 0.003026
94-Apr 0.129051 0.043813 0.060453 0.017469 0.545895 0.009615 0.01932 0.01171
0.016003 0.014412 0.140396 0.017556
94-May 0.142182 -0.03848 0.059938 0.015054 0.525178 0.009479 0.027741
0.000605 0.0185 0.014327 0.093228 -0.03989
94-Jun 0.152981 -0.03227 0.071485 -0.01025 0.363882 0.009323 0.030762
0.013005 0.03634 0.014239 0.035625 -0.01355
94-Jul 0.16216 0.046374 0.073669 0.020508 0.3405 0.00926 0.044822 -0.00954
0.042422 0.014154 0.037954 0.00097
94-Aug 0.124355 -0.06952 0.091429 0.015932 0.38519 0.009269 0.071701
0.000623 0.051954 0.01407 0.055852 0.007522
94-Sep 0.059405 0.057487 0.086265 -0.01169 0.401963 0.009171 0.086685
-0.01058 0.054404 0.013986 0.07285 0.002022
94-Oct 0.0594 0.021166 0.080765 0.006442 0.438915 0.009041 0.070351 0.006776
0.033622 0.013906 0.068344 -0.01532
94-Nov 0.072064 -0.03104 0.079567 -0.03295 0.521214 0.008986 0.066044
-0.01853 0.035202 0.013826 0.067093 -0.02278
94-Dec 0.068208 0.01024 0.069919 -0.01507 0.461059 0.008856 0.050985
0.009514 0.008638 0.013744 0.040348 0.00423
95-Jan 0.079074 -0.00153 0.070458 -0.04205 0.506227 0.00883 0.046561
-0.03907 0.015322 0.013662 0.034103 -0.00888
95-Feb 0.074231 -0.05728 0.062612 0.035992 0.487126 0.008815 0.052816
-0.01344 0.06728 0.013583 0.063281 -0.0054
95-Mar 0.065212 0.056084 0.095783 0.006825 0.476386 0.008774 0.047498
0.015178 0.040273 0.013499 0.060805 0.006099
95-Apr 0.081238 0.024283 0.098827 0.005791 0.432363 0.008748 0.06047
0.011613 0.013068 0.013417 0.058321 -0.01281
95-May 0.093726 0.008623 0.076698 0.027274 0.321103 0.008679 0.037962
0.00115 0.013647 0.013339 0.066724 -0.00271
95-Jun 0.113998 0.005484 0.073392 -0.00252 0.38195 0.008684 0.042794
-0.01133 0.054244 0.013261 0.055655 0.015941
95-Jul 0.097076 0.008842 0.090776 0.006378 0.622055 0.008728 0.036476
0.016159 0.055301 0.013188 0.057034 -0.0036
95-Aug 0.075751 0.002437 0.094687 -0.00398 0.637972 0.008839 0.052791
-0.00819 0.327487 0.013114 0.067734 0.00565
95-Sep 0.074714 0.001279 0.091216 0.013169 0.656225 0.008956 0.086582
-0.0013 0.690172 0.01304 0.059523 0.028675
95-Oct 0.048771 -0.01775 0.098525 0.003447 0.68386 0.009071 0.091073
-0.01597 0.640065 0.012967 0.030469 0.005139
95-Nov 0.069776 -0.00164 0.077763 0.00158 0.559675 0.008808 0.094129 0.01832
0.726821 0.012893 0.030908 -0.00955
95-Dec 0.135469 0.001886 0.074658 0.01263 0.563716 0.008888 0.113828
0.011372 0.737532 0.012822 0.224459 -0.00186
96-Jan 0.175166 0.00068 0.071721 0.030701 0.534648 0.009114 0.086481
0.016228 0.687297 0.013112 0.349764 0.000727
96-Feb 0.167327 0.013771 0.055352 -0.03142 0.556339 0.009119 0.080475
-0.00691 0.696365 0.013077 0.342758 -4.90E-05
96-Mar 0.158759 -0.02094 0.042232 -0.00331 0.532126 0.009041 0.077231
0.009009 0.579396 0.012271 0.342196 -0.002
96-Apr 0.116956 0.02624 0.051037 -0.01496 0.575416 0.009123 0.079496
0.017197 0.557262 0.012094 0.299566 0.022657
96-May 0.109049 -0.02648 0.059972 0.00658 0.616302 0.009086 0.095365
-0.01682 0.521757 0.011933 0.074309 0.021621
96-Jun 0.102001 2.71E-05 0.060901 -0.00372 0.593491 0.009213 0.095232
0.001363 0.523983 0.011757 0.070504 -0.00507
96-Jul 0.079941 -0.02107 0.046018 -0.00708 0.562537 0.009136 0.094451
-0.01132 0.534417 0.011413 0.073706 -0.00615
96-Aug 0.109775 0.005178 0.051713 0.007174 0.54939 0.009008 0.088945
-0.01136 0.445843 0.010925 0.066559 0.009937
96-Sep 0.089581 -0.0005 0.049835 0.016873 0.54664 0.008887 0.082659 0.011384
0.435423 0.010697 0.091269 0.00687
96-Oct 0.07429 -0.01499 0.063584 0.008829 0.485504 0.008965 0.072986
-0.01695 0.54066 0.010649 0.325364 0.012261
96-Nov 0.060441 0.021057 0.100844 0.018152 0.415023 0.009033 0.072366
-0.00222 0.646444 0.010653 0.323194 0.01409
96-Dec 0.061482 0.0218 0.142038 -6.42E-06 0.492536 0.008947 0.081333
-0.02433 0.661019 0.010555 0.367988 -0.00023
97-Jan 0.053437 0.025314 0.137257 -0.00659 0.578904 0.008841 0.074613
-0.0068 0.628154 0.010609 0.355763 0.00581
97-Feb 0.080489 -0.01411 0.123644 0.009692 0.571364 0.008794 0.07673
0.005832 0.549697 0.010781 0.21588 0.070824
97-Mar 0.097621 0.00073 0.115192 -0.04503 0.639719 0.008686 0.065906
-0.01063 0.543819 0.010442 0.129773 0.004692
97-Apr 0.112502 -0.00052 0.064499 0.007382 0.648139 0.008674 0.038621
0.006408 0.591661 0.010283 0.079461 0.009395
97-May 0.109789 0.028968 0.079382 0.032543 0.530901 0.008884 0.029301
0.039566 0.492504 0.01004 0.042617 -0.00151
97-Jun 0.087521 -0.03031 0.037389 0.001738 0.500643 0.00886 0.028772 0.0146
0.47233 0.009869 0.033241 0.001504
97-Jul 0.085527 0.01424 0.029224 0.018015 0.496859 0.008725 0.036717
-0.00956 0.443577 0.009995 0.026995 0.014942
97-Aug 0.077215 -0.00935 0.025766 -0.05705 0.424685 0.008555 0.034725
-0.02395 0.481891 0.010128 0.029444 0.021642
97-Sep 0.08778 0.012634 0.064433 0.012569 0.495125 0.008406 0.06119 0.002597
0.515191 0.010241 0.056915 0.002897
97-Oct 0.067236 -0.07837 0.100233 -0.08455 0.603348 0.008296 0.065799
-0.02425 0.545503 0.010191 0.071548 0.002394
97-Nov 0.074344 0.008919 0.169394 -0.00212 0.615421 0.008215 0.076836
-0.04056 0.552007 0.01035 0.131526 0.001346
97-Dec 0.066797 0.001889 0.180801 -0.0114 0.607294 0.008081 0.11593 -0.01879
0.573696 0.01082 0.283244 0.002348
98-Jan 0.050659 -0.01722 0.163617 0.074366 0.52707 0.007969 0.091612
0.027019 0.583443 0.011549 0.267086 -0.02785
98-Feb 0.049803 0.015915 0.04491 0.054682 0.38982 0.008092 0.092166 0.002436
0.61572 0.011637 0.275094 -0.0145
98-Mar 0.047308 0.017094 0.047185 0.000772 0.455737 0.008007 0.087419
-0.01693 0.594474 0.012426 0.286536 -0.01181
98-Apr 0.047385 0.003939 0.048202 -0.03154 0.567806 0.007983 0.060277
-0.00191 0.597842 0.013039 0.127336 0.107101
98-May 0.064783 0.025706 0.060724 -0.03807 0.684579 0.007976 0.048987
-0.01964 0.518052 0.013439 0.069782 0.006585
98-Jun 0.159381 -0.01328 0.052821 -0.01119 0.684776 0.007913 0.038652
0.002138 0.543559 0.012967 0.110726 -0.00548
98-Jul 0.151487 0.004139 0.051838 -0.00775 0.70082 0.007946 0.048077
-0.00496 0.585499 0.012684 0.075913 0.000926
98-Aug 0.148411 -0.04313 0.047802 -0.0209 0.66453 0.007953 0.046124 -0.02537
0.69816 0.012255 0.072707 -0.00505
98-Sep 0.16881 -0.03613 0.052287 0.018093 0.595134 0.008081 0.044761
-0.00243 0.677852 0.012169 0.075073 -0.0052
98-Oct 0.070201 -0.07938 0.055037 0.062009 0.549705 0.007892 0.051584
0.034295 0.691742 0.011869 0.052704 -0.0219
98-Nov 0.049853 0.028386 0.068794 0.005291 0.442349 0.007833 0.045537
0.030026 0.669312 0.0117 0.219442 -0.0081
98-Dec 0.069926 0.000449 0.101457 -0.00323 0.356563 0.007826 0.05571
0.000408 0.529556 0.0108 0.390611 -0.004
99-Jan 0.07435 -0.01122 0.096968 -0.01734 0.343742 0.007695 0.078749
0.012134 0.541906 0.010943 0.41004 0.008279
99-Feb 0.082127 0.021732 0.114963 -0.00191 0.345825 0.007578 0.083706
-0.00624 0.546338 0.011204 0.425059 -0.01535
99-Mar 0.081221 0.040004 0.111538 0.023871 0.358622 0.007456 0.079843
0.032283 0.566893 0.011074 0.401445 0.102554
99-Apr 0.086991 0.02926 0.065761 0.097698 0.301239 0.007323 0.097955 0.01104
0.529548 0.010871 0.076728 0.027323
99-May 0.077475 0.023956 0.074776 -0.02335 0.321715 0.007218 0.063023
0.011379 0.397527 0.01031 0.073261 -0.00508
99-Jun 0.076112 0.020359 0.068651 0.033434 0.4018 0.007095 0.06711 0.021294
0.408022 0.010055 0.06967 0.006254
99-Jul 0.080411 -0.01148 0.060564 -0.00474 0.39331 0.007002 0.066918 0.02464
0.478281 0.010005 0.066309 0.015189
99-Aug 0.068091 -0.01721 0.060539 0.004914 0.37717 0.006855 0.062521
0.007325 0.482744 0.010036 0.088398 0.007067
99-Sep 0.086042 0.008278 0.077601 -0.01802 0.403218 0.006718 0.067205
0.000943 0.628366 0.009918 0.147324 0.005286
99-Oct 0.075385 0.010131 0.061047 -0.00448 0.372075 0.00665 0.060542 0.0217
0.780534 0.009656 0.131252 0.005054
99-Nov 0.055049 0.020592 0.068442 0.035127 0.399578 0.006527 0.065934
0.025071 0.772524 0.009266 0.157148 0.027663
99-Dec 0.062593 0.046883 0.067727 0.04049 0.502933 0.006406 0.053723
0.014904 0.728022 0.008887 0.167636 0.018708
00-Jan 0.057252 0.004454 0.065601 -0.02153 0.558368 0.006257 0.049291
0.009493 0.605325 0.008994 0.09732 0.037374
00-Feb 0.065328 0.045896 0.075552 0.026155 0.555294 0.006163 0.057978
-0.00837 0.504593 0.009741 0.130217 0.00261
00-March 0.065141 -0.02182 0.068206 0.003206 0.596057 0.006076 0.054416
0.011653 0.529128 0.009573 0.090386 -0.01596
00-Apr 0.080721 0.023865 0.059405 -0.03907 0.5262 0.005846 0.057356 -0.02115
0.468138 0.009582 0.034706 0.004283
00-May 0.074753 0.016991 0.063155 -0.00071 0.575856 0.005665 0.066024
-0.01439 0.402228 0.009809 0.030408 0.002116
00-Jun 0.14341 0.004936 0.054784 0.026521 0.616701 0.005597 0.049632
0.013278 0.471112 0.009514 0.034816 0.006285
00-Jul 0.141626 -0.0135 0.053571 0.010033 0.600591 0.005475 0.055653
-0.02939 0.483099 0.009243 0.034094 0.00531
00-Aug 0.127419 0.0245 0.061189 0.003298 0.568755 0.005432 0.067319 0.014447
0.513747 0.009217 0.22144 0.015615
00-Sep 0.122004 -0.00474 0.050212 -0.02615 0.503157 0.005269 0.087926
-0.01138 0.581831 0.009141 0.367495 0.001285
00-Oct 0.078788 -0.10172 0.044231 -0.0075 0.432512 0.0051 0.089254 -0.01574
0.656075 0.009038 0.359533 0.000368
00-Nov 0.053822 0.010106 0.034679 -0.01469 0.513539 0.00498 0.100637
-0.01064 0.622279 0.008928 0.377782 -0.00477
00-Dec 0.069006 -0.0005 0.051647 0.025002 0.546061 0.004794 0.103877
-0.02231 0.610257 0.00881 0.333657 0.09513
1-Jan 0.071511 -0.00837 0.057658 0.017076 0.553587 0.004638 0.095702
-0.02487 0.56779 0.00868 0.250782 0.008235
1-Feb 0.076866 0.00833 0.060892 -0.02218 0.59127 0.0045 0.093535 -0.0101
0.533402 0.008675 0.240717 0.003711
1-Mar 0.07557 -0.0462 0.074634 -0.03465 0.595882 0.004429 0.068847 0.002673
0.479236 0.008611 0.091359 0.034367
1-Apr 0.070519 0.033694 0.060347 0.003217 0.480071 0.004227 0.04972 0.03196
0.561474 0.008402 0.052372 0.002317
1-May 0.064918 0.011544 0.031374 0.009111 0.551824 0.004201 0.023617
0.001112 0.560844 0.008168 0.043051 0.009939
1-Jun 0.104019 -0.00345 0.024989 -0.01172 0.568941 0.004169 0.020633 -0.0132
0.560457 0.008024 0.049529 0.007287
1-Jul 0.090489 0.017047 0.021945 -0.01905 0.557895 0.004073 0.022649
-0.02304 0.56767 0.007859 0.048501 0.004123
1-Aug 0.099604 -0.02219 0.028166 -0.02333 0.625414 0.004008 0.033092
-0.00533 0.604529 0.007885 0.054362 0.007967
1-Sep 0.107159 -0.07639 0.029653 -0.02712 0.622213 0.004059 0.060953
-0.02039 0.597122 0.007776 0.059832 -0.03705
1-Oct 0.092227 0.008594 0.063975 -0.01046 0.60189 0.004079 0.092008 -0.00481
0.559119 0.007765 0.073072 0.00552
1-Nov 0.133317 0.017435 0.092959 0.026051 0.502762 0.004133 0.103452
-0.00171 0.54558 0.007544 0.051221 0.001584
1-Dec 0.148023 0.019271 0.104346 0.012594 0.490295 0.003975 0.107077
-0.01876 0.533209 0.007311 0.056159 0.012704
2-Jan 0.150961 -0.01348 0.10693 -0.01523 0.426876 0.003902 0.100983 -0.00865
0.512898 0.007222 0.045519 0.006621
2-Feb 0.129901 -0.00175 0.078699 0.007048 0.352362 0.003811 0.086475
0.022773 0.559536 0.007018 0.051134 0.001665
2-Mar 0.122265 -0.06762 0.042505 0.008784 0.410423 0.003871 0.067548
0.005427 0.568946 0.006885 0.044913 0.009651
2-Apr 0.075202 -0.01114 0.04052 0.003142 0.46432 0.003887 0.06172 0.005093
0.525967 0.006694 0.049629 0.032768
2-May 0.072844 -0.00547 0.022389 -0.00371 0.449056 0.003853 0.040507
0.004051 0.557733 0.00658 0.036551 0.005127
2-Jun 0.066132 0.005908 0.058852 -0.0166 0.461077 0.00388 0.049413 -0.01205
0.552187 0.00646 0.022301 -0.00198
2-Jul 0.059293 0.007211 0.059812 -0.01172 0.501559 0.003894 0.060857
-0.01298 0.55335 0.006188 0.022222 -0.00236
2-Aug 0.088299 -0.01081 0.068442 -0.00538 0.526355 0.003923 0.082171
-0.00367 0.632755 0.006052 0.022559 -0.00173
2-Sep 0.085233 -0.00312 0.07763 -0.02517 0.519692 0.00395 0.115502 -0.01224
0.77597 0.006027 0.047021 0.00225
2-Oct 0.079523 0.010872 0.057461 0.020658 0.506263 0.003923 0.125037
-0.00671 0.776439 0.005841 0.241231 -0.00458
2-Nov 0.092114 0.019084 0.056722 0.015734 0.482294 0.003937 0.132646 0.01848
0.776393 0.005798 0.241103 -0.01662
2-Dec 0.100498 -0.01973 0.058859 -0.01878 0.452742 0.003915 0.12689 0.003786
0.676371 0.005501 0.264177 0.019179
3-Jan 0.129572 -0.00262 0.048539 0.00309 0.544448 0.003876 0.094442 0.027311
0.556401 0.004892 0.254185 0.004473
3-Feb 0.132895 -0.00044 0.058255 -0.00171 0.600125 0.003869 0.078469
0.005541 0.679645 0.004928 0.115786 0.110436
3-Mar 0.136859 0.027562 0.058904 -0.01381 0.556289 0.003903 0.070073
0.004383 0.700573 0.004891 0.032592 0.03212
3-Apr 0.136273 0.068605 0.064695 -0.00261 0.613031 0.003871 0.071458
0.005898 0.699115 0.0048 0.029105 0.009723
3-May 0.099559 0.071864 0.0722 0.019948 0.626308 0.003719 0.078507 0.016676
0.648559 0.00479 0.023664 0.026138
3-Jun 0.065194 -0.02172 0.063345 0.001001 0.57964 0.003541 0.074453 0.015788
0.495185 0.004489 0.020389 0.019138
3-Jul 0.064383 -0.0382 0.05803 0.011462 0.582394 0.00346 0.055675 0.002646
0.500264 0.004442 0.015041 0.016287
3-Aug 0.070518 -0.01549 0.063356 0.017911 0.465972 0.00349 0.066048 0.022058
0.581051 0.004493 0.012775 0.025527
3-Sep 0.059538 0.000566 0.064277 0.014528 0.441227 0.003367 0.076711
0.024424 0.605593 0.004563 0.034063 0.012428
3-Oct 0.071278 0.016929 0.07425 0.023864 0.438028 0.003189 0.066948 0.014291
0.546467 0.004617 0.081215 -0.00357
3-Nov 0.078345 0.014875 0.082316 0.002779 0.545115 0.003026 0.090011
-0.00223 0.519579 0.004587 0.137425 0.006393
3-Dec 0.086351 0.016485 0.083428 0.009423 0.550845 0.002851 0.088829 0.02042
0.492655 0.004664 0.093447 0.010691
4-Jan 0.081329 0.006615 0.070349 0.011773 0.503661 0.002664 0.0562 0.023223
0.513912 0.004819 0.04145 0.009952
4-Feb 0.06281 0.006856 0.06135 0.011275 0.295814 0.002551 0.048915 0.001594
0.523612 0.00499 0.023872 -0.00975
4-Mar 0.049264 -0.00044 0.05989 -0.02068 0.056343 0.002607 0.04823 0.027175
0.609287 0.005025 0.023252 0.01872
4-Apr 0.049299 0.009245 0.052224 -0.00065 0.158436 0.00264 0.044182 -0.0123
0.607031 0.004961 0.024989 0.023746
4-May 0.07551 0.005179 0.046009 0.009392 0.206344 0.002588 0.036726 -0.01127
0.576678 0.005017 0.030117 0.000588
4-Jun 0.106293 0.012995 0.051612 -0.00029 0.200992 0.002555 0.053382
0.009459 0.469148 0.005189 0.031416 0.009538
4-Jul 0.108154 -0.00959 0.047847 0.001517 0.227312 -0.00647 0.05747 -0.01319
0.476149 0.005079 0.314022 0.013775
4-Aug 0.081915 0.004152 0.053783 0.011214 0.198439 0.045954 0.062876
0.003317 0.44414 0.005068 0.303724 0.00769
4-Sep 0.081102 0.009467 0.062976 0.00507 0.093854 -0.00086 0.070439 -0.01041
0.439919 0.004963 0.286094 0.011138
4-Oct 0.060353 -0.0099 0.044424 0.006191 0.10147 0.012374 0.050592 0.002093
0.461951 0.004918 0.286624 0.014818
4-Nov 0.061972 0.016949 0.054118 0.017346 0.108899 -0.00125 0.048133
0.009199 0.556459 0.005029 0.174995 0.089482
4-Dec 0.067244 0.023686 0.070182 0.00137 0.089756 0.004985 0.055462 0.021193
0.672079 0.005 0.075406 -0.0024
5-Jan 0.07898 -0.00534 0.08631 -0.0093 0.091402 0.009871 0.073969 0.006254
0.66872 0.004962 0.048242 0.040205
5-Feb 0.076154 0.014852 0.079967 0.012873 0.079875 0.01805 0.080537 0.006993
0.677289 0.004799 0.031595 0.026168
5-Mar 0.077074 0.00191 0.090161 -0.01357 0.052445 -0.00301 0.071842 -0.00201
0.603639 0.00486 0.038924 0.027139
5-Apr 0.079171 0.016211 0.081519 0.008182 0.067475 0.017158 0.084696
-0.00711 0.482537 0.004869 0.032052 -0.00649
5-May 0.08762 0.001995 0.061132 -0.00617 0.108894 -0.00095 0.036985 0.011574
0.499607 0.004559 0.036882 0.02224
5-Jun 0.112654 -0.02902 0.062165 0.005573 0.150261 -0.00456 0.051033
0.000359 0.454388 0.004385 0.047875 0.031248
5-Jul 0.114144 0.014178 0.05508 0.009906 0.133826 -0.00185 0.049361 0.001181
0.535222 0.004236 0.051302 -0.0007
5-Aug 0.118755 0.009505 0.045739 0.000422 0.134694 0.002732 0.048689
0.013723 0.65319 0.004264 0.050513 0.025555
5-Sep 0.126867 0.01075 0.060408 0.013722 0.120036 0.006571 0.063265 0.020154
0.656104 0.004173 0.070757 0.002251
5-Oct 0.089795 -0.01696 0.055212 -0.02112 0.107882 0.011945 0.059214
0.006303 0.690117 0.004071 0.045845 0.020133
5-Nov 0.090791 0.008801 0.05865 0.008551 0.117452 0.003818 0.061104 0.005212
0.641982 0.004077 0.049325 0.010303
5-Dec 0.118981 0.016034 0.081244 -0.00309 0.124691 -0.00724 0.072101
0.020097 0.578578 0.004283 0.04876 0.013458
6-Jan 0.070068 -0.01575 0.054248 0.013942 0.102255 0.056449 0.080795
0.019344 0.586796 0.00406 0.047243 0.043854
6-Feb 0.073022 -0.00889 0.052663 0.002691 0.109722 -0.01689 0.087112
-0.00481 0.601005 0.003781 0.048973 0.007074
6-Mar 0.104843 0.00397 0.052227 -0.00155 0.081988 -0.0331 0.113525 0.005888
0.515759 0.003533 0.06839 0.013283
6-Apr 0.104731 0.032016 0.04815 0.026012 0.08035 0.003807 0.115429 0.005909
0.338814 0.003188 0.066514 -0.01532
6-May 0.09766 0.001961 0.035253 -0.01271 0.103763 -0.01503 0.081776 0.002416
0.330045 0.003131 0.069321 -0.03033
6-Jun 0.119052 -0.02364 0.039689 0.005487 0.114745 0.010777 0.081682
-0.00304 0.236855 0.003015 0.095066 0.097317
6-Jul 0.088549 0.008985 0.036248 0.010201 0.093526 0.009576 0.054863
-0.01742 0.102126 0.002985 0.134684 -0.1041
6-Aug 0.067978 0.019824 0.044921 0.005189 0.07627 0.030992 0.047808 0.003101
0.155821 0.002881 0.167384 0.033159
6-Sep 0.066656 0.007713 0.060278 0.001498 0.102424 0.004677 0.057565
-0.00311 0.205756 0.002718 0.161339 0.002583
6-Oct 0.068295 0.026603 0.053452 0.010615 0.099656 0.000584 0.058789
-0.00066 0.199022 -0.02302 0.155059 -0.06468
6-Nov 0.089642 0.001217 0.067761 0.007713 0.086734 -0.00508 0.054733
0.000595 0.196888 -0.01408 0.125395 -0.05451
6-Dec 0.099192 -0.00135 0.059676 0.022421 0.099908 0.027104 0.050893 0.0043
0.245125 0.0489 0.13109 -0.00058
7-Jan 0.105593 0.00331 0.039473 0.003456 0.047097 -0.00912 0.055139 0.019326
0.117142 0.028954 0.109013 -0.05588
7-Feb 0.118889 0.004966 0.040191 -0.00952 0.041858 0.009777 0.05421 0.012598
0.113921 4.07E-05 0.114541 0.075385
7-Mar 0.08916 0.015319 0.025478 0.001806 0.038149 0.007381 0.053036 -0.00099
0.123744 0.011148 0.118334 0.013907
7-Apr 0.080799 0.038111 0.0491 0.011014 0.049405 -0.00921 0.022374 -0.00828
0.128256 0.024402 0.109033 -0.00633
7-May 0.088313 0.00604 0.044642 0.000793 0.031703 0.020668 0.019772 0.011723
0.113827 0.041275 0.122305 -0.01308
7-Jun 0.07316 -0.01507 0.042463 0.014393 0.043263 0.009882 0.021969 -0.00014
0.117902 0.011965 0.214639 -0.00689
7-Jul 0.065668 -0.0048 0.046214 0.01654 0.037575 0.017381 0.024748 -0.00012
0.101637 0.019989 0.205776 0.05221
7-Aug 0.070372 -0.02158 0.043537 0.007309 0.042105 -0.00364 0.044089
-0.00605 0.095848 -0.00647 0.180665 0.001296
7-Sep 0.047904 0.03766 0.033164 0.031409 0.048308 0.001623 0.033734 -0.00284
0.127852 0.008018 0.160366 0.012009
7-Oct 0.05138 0.041531 0.059524 0.034967 0.04954 0.014953 0.029942 0.001174
0.123522 0.004083 0.141225 0.077784
7-Nov 0.037378 0.000217 0.054199 -0.01968 0.041879 0.009651 0.028309
-0.00059 0.107465 -0.01076 0.132328 0.045576
7-Dec 0.042732 -0.00199 0.064209 -0.00477 0.049397 0.014798 0.023225
-0.01046 0.094376 0.02261 0.149627 0.049116
8-Jan 0.035948 -0.01236 0.060851 -0.03168 0.040511 0.002253 0.026289
-0.00212 0.085185 0.017828 0.130155 0.01301
8-Feb 0.037398 -0.00124 0.054232 -0.03426 0.050906 0.014738 0.0282 0.002013
0.069707 0.021845 0.15015 -0.01344
8-Mar 0.042094 -0.0305 0.032707 -0.00876 0.05264 -0.01032 0.024265 -0.00324
0.05907 0.013726 0.152467 -0.05545
8-Apr 0.04852 0.01582 0.031455 0.026955 0.063245 0.011327 0.025647 0.015876
0.067814 -0.00104 0.124019 0.05047
8-May 0.059497 -0.00208 0.038082 -0.00581 0.037738 0.006043 0.028794
0.017551 0.071995 -0.00976 0.104521 -0.0247
8-Jun 0.096974 -0.05471 0.041072 -0.03588 0.085357 -0.00082 0.035479
-0.01578 0.119341 0.00171 0.099164 -0.01334
8-Jul 0.092663 -0.03785 0.045204 0.004457 0.118828 -0.00513 0.037903
-0.00115 0.145136 0.007081 0.093327 0.001238
8-Aug 0.110892 -0.00404 0.042687 -0.02514 0.192407 -0.00432 0.028538
-0.01015 0.13363 -0.00083 0.164095 -0.0194
8-Sep 0.101492 -0.04277 0.064079 -0.043 0.190126 -0.02691 0.03497 -0.03088
0.137926 -0.03217 0.176231 -0.02584
8-Oct 0.084709 -0.06177 0.050796 -0.06637 0.202988 -0.00546 0.017852
-0.01142 0.128956 -0.04807 0.194271 -0.17044
8-Nov 0.057129 -0.08266 0.068863 -0.00142 0.204409 -0.03338 0.031681
0.007806 0.117484 -0.02477 0.234215 -0.07913
8-Dec 0.079164 0.020009 0.106671 0.018427 0.146101 0.041635 0.051961
0.011684 0.104922 -0.05998 0.180431 0.174908
9-Jan 0.063432 -0.00669 0.093999 0.018161 0.105719 -0.03781 0.082073
-0.03367 0.106395 -0.05258 0.12232 -0.01428
9-Feb 0.05705 -0.01499 0.083275 -0.00843 0.101315 -0.01958 0.095826 -0.03223
0.087682 -0.09934 0.12882 0.012371
9-Mar 0.061702 0.030318 0.088468 0.012107 0.108591 0.001889 0.095932
0.011516 0.057128 0.036127 0.146176 0.037564
9-Apr 0.059506 0.082299 0.08067 0.061903 0.086351 -0.02017 0.095266 0.0144
0.050497 0.017112 0.124557 0.062549
9-May 0.049584 0.050675 0.04681 0.055353 0.059368 0.008474 0.038863 0.062884
0.162174 0.010617 0.249354 0.019946
9-Jun 0.050694 0.005618 0.044184 0.002126 0.078106 -0.01453 0.043025
0.005721 0.182842 0.01024 0.307549 -0.00516
9-Jul 0.066973 0.030587 0.038676 0.020141 0.084477 -0.02209 0.034024
0.015118 0.171025 0.000822 0.333595 -0.00769
9-Aug 0.062135 0.010437 0.053099 -0.00917 0.096394 0.010992 0.05032 0.007868
0.186191 0.012318 0.353648 -0.02295
9-Sep 0.050647 0.033668 0.059205 0.012482 0.111134 0.009082 0.067715
0.010795 0.138849 0.05987 0.292975 0.137462
9-Oct 0.055956 0.013449 0.055734 0.000706 0.086458 -0.00339 0.053182
-0.00297 0.079412 -0.02269 0.163511 -0.01146
9-Nov 0.058756 -0.01045 0.059039 0.001159 0.073077 -0.02239 0.057373
-0.01219 0.077946 -0.03549 0.165941 0.02084
9-Dec 0.054019 0.016576 0.10987 0.001472 0.045507 -0.00868 0.05764 0.001159
0.056954 0.003272 0.093262 -0.00356
10-Jan 0.065299 -0.0012 0.07959 -0.014 0.046484 0.007152 0.030854 0.01432
0.038635 -0.00133 0.124903 0.01488
10-Feb 0.080965 -0.00718 0.075793 0.0106 0.050117 -0.00315 0.025844 0.003503
0.028887 0.029483 0.138384 0.01319
10-Mar 0.087112 0.019504 0.08891 0.005904 0.056132 0.008442 0.036296 0.01051
0.038374 0.020292 0.138234 0.027963
10-Apr 0.088547 -0.02642 0.039839 0.026567 0.069717 0.006489 0.041708
-0.00507 0.041266 0.00206 0.147216 0.012707
10-May 0.101039 -0.05872 0.037815 -0.00875 0.083237 -0.04479 0.030418 -0.005
0.051597 -0.01986 0.128739 -0.02404
10-Jun 0.095199 -0.00342 0.057961 0.004086 0.123835 -0.0142 0.035956
-0.00352 0.091948 0.005044 0.135558 0.021432
10-Jul 0.075234 0.025279 0.063847 0.009611 0.116717 -0.0035 0.031698
0.002359 0.252143 0.011422 0.287141 0.000121
10-Aug 0.079156 -0.00047 0.051437 -0.00546 0.112914 0.004803 0.02562
-0.00573 0.261809 0.013716 0.309455 -0.01186
10-Sep 0.059055 0.042768 0.057365 0.019033 0.075016 0.0137 0.023117 0.008312
0.249929 0.029749 0.313657 0.01661
10-Oct 0.054609 0.009678 0.063531 0.005707 0.081634 0.000622 0.02408
-0.00129 0.189048 0.000912 0.25145 -0.00483
10-Nov 0.068635 -0.00222 0.060139 -0.0012 0.094356 0.011296 0.051482
0.006082 0.107408 -0.0728 0.172865 -0.05986
10-Dec 0.081359 0.024868 0.069255 -0.0048 0.088267 -0.00047 0.05896 0.011501
0.06167 0.003235 0.151708 0.013402
11-Jan 0.073495 -0.01786 0.067798 0.003442 0.099323 0.009847 0.067582
0.007326 0.06768 -0.00637 0.149 -0.01795
11-Feb 0.063406 -0.01603 0.069918 0.006861 0.088746 -0.01965 0.076099
0.01099 0.052349 -0.03146 0.12507 -0.02541
11-Mar 0.048697 0.019537 0.078758 0.001983 0.033649 -0.00549 0.0742 -0.00639
0.044473 0.007343 0.078477 0.055442
11-Apr 0.038096 -0.00979 0.072229 0.002315 0.031185 -0.00677 0.093811
0.007807 0.047309 0.018028 0.231757 0.017197
11-May 0.047878 -0.0032 0.066941 0.001751 0.089032 -0.01206 0.057944
0.002471 0.064242 -0.0069 0.325707 0.006877
11-Jun 0.039098 -0.00809 0.073197 -0.01205 0.11624 -0.00688 0.060207
0.004771 0.070361 -0.00677 0.333381 -0.01486
11-Jul 0.037719 0.001729 0.062112 0.006542 0.114612 -0.00613 0.054476
0.005025 0.094604 0.000269 0.373547 -0.00403
11-Aug 0.034513 -0.05368 0.05377 -0.01739 0.121313 -0.0161 0.068306 -0.01597
0.122455 -0.01018 0.363369 0.079906
11-Sep 0.02704 -0.01352 0.053927 -0.04383 0.103685 -0.02907 0.088478
-0.01848 0.131231 0.003976 0.330517 0.054048
11-Oct 0.038786 0.027062 0.029992 0.031106 0.079689 -0.00876 0.083739
0.006048 0.10505 0.006914 0.348418 0.006048
11-Nov 0.041044 -0.01657 0.03751 -0.02094 0.054133 0.019163 0.088931
-0.01241 0.072494 -0.01463 0.133484 0.074594
11-Dec 0.103128 -0.01263 0.046591 0.008993 0.048258 -0.0251 0.074848
-0.00018 0.07466 -0.00086 0.087007 0.028796
12-Jan 0.118227 0.026444 0.052006 0.041587 0.046611 0.001658 0.01574
0.035412 0.063587 0.002389 0.074204 0.016416
12-Feb 0.122598 -0.00084 0.069188 0.01369 0.050482 0.001776 0.023857
0.009134 0.06617 0.005087 0.093116 0.030493
12-Mar 0.104265 0.017232 0.076994 -0.01145 0.052082 0.003762 0.027545
-0.00288 0.076327 0.004981 0.099453 0.037708
12-Apr 0.098936 0.022583 0.06595 0.000605 0.057745 0.002054 0.037188
-0.01349 0.064687 0.000578 0.138139 -0.01452
12-May 0.089141 -0.01862 0.053403 -0.02149 0.040155 6.19E-05 0.043449
-0.01936 0.056818 -0.0068 0.163505 -0.04733
12-Jun 0.099204 0.0029 0.055061 0.009495 0.033332 0.005187 0.070085 0.011337
0.070332 -0.00277 0.225889 -0.01912
12-Jul 0.097595 0.01046 0.039244 0.011626 0.078823 -0.00086 0.059504 -0.0049
0.082291 -0.0082 0.250106 0.002272
12-Aug 0.102617 -0.00756 0.033855 -0.00345 0.084789 0.005599 0.063005
-0.00181 0.081742 0.005522 0.229006 0.040341
12-Sep 0.102183 0.039397 0.04452 0.015495 0.081538 0.00815 0.041798 9.62E-05
0.077269 0.01068 0.190494 -0.02643
12-Oct 0.063156 0.009069 0.075626 0.008326 0.094997 -0.00238 0.028839
-0.00493 0.091971 -0.00171 0.183817 0.026617
12-Nov 0.059478 0.014209 0.069961 0.00384 0.071881 0.000951 0.022743
0.007659 0.093547 0.006532 0.118919 0.007045
12-Dec 0.04359 -0.0137 0.076152 0.007972 0.061429 0.003239 0.028688 0.013573
0.055046 -0.00359 0.08988 0.012686
13-Jan 0.042402 0.003296 0.071592 0.013726 0.055347 0.000777 0.017266
0.017428 0.025965 0.010131 0.077671 0.022031
13-Feb 0.053014 -0.00078 0.068647 0.005744 0.066683 0.005203 0.028252
0.010783 0.030288 -0.00102 0.062858 0.003411
13-Mar 0.062571 0.010994 0.067563 -0.01074 0.049167 -0.00132 0.027015
0.015522 0.03963 -0.00144 0.057604 0.016195
13-Apr 0.087643 0.001583 0.0732 0.000713 0.048623 0.008947 0.030543 0.01957
0.039617 0.01346 0.107268 0.001796
13-May 0.155529 -3.78E-05 0.070439 -0.0007 0.047366 0.022614 0.024684
-0.00973 0.088442 0.004269 0.114926 0.011779
13-Jun 0.144978 -0.00465 0.057574 -0.02281 0.108893 -0.00518 0.035611
0.00317 0.279286 -0.00842 0.1939 -0.00194
13-Jul 0.130431 0.011634 0.072031 0.014305 0.130615 0.003684 0.045301
0.003532 0.304674 0.010773 0.227933 0.012467
13-Aug 0.120132 -0.00195 0.068647 -0.00052 0.122781 0.001305 0.057854
-0.00476 0.311714 -0.00642 0.203608 0.017791
13-Sep 0.06575 -0.00545 0.080394 0.016555 0.119251 0.000583 0.084581
0.014775 0.297759 0.010611 0.250571 0.00938
13-Oct 0.047298 0.005285 0.120425 0.004647 0.101134 0.022547 0.067475
0.001918 0.150446 0.043009 0.145828 0.041185
13-Nov 0.058826 0.017242 0.109948 0.006176 0.071817 0.008307 0.062375
0.00121 0.053042 -0.00604 0.05076 0.011435
13-Dec 0.047956 -0.00802 0.103437 -0.00066 0.084003 0.009607 0.066285
0.004847 0.050402 0.000148 0.054412 0.006267
14-Jan 0.06039 -0.0104 0.107847 -0.01456 0.088995 0.008308 0.049891 0.004923
0.060003 0.00252 0.04058 0.008332
14-Feb 0.048697 0.016089 0.064049 -0.00105 0.095994 0.015907 0.063598
0.002466 0.062014 -0.00279 0.040226 0.009207
14-Mar 0.048803 0.012027 0.066596 -0.00688 0.092639 0.000271 0.071203
-0.00798 0.063202 0.016329 0.063374 0.010131
14-Apr 0.054393 -2.77E-05 0.068392 0.001996 0.100696 0.019122 0.07642
-0.0071 0.094228 0.004257 0.061417 0.005074
14-May 0.082217 -0.00517 0.06419 0.002143 0.074215 0.008081 0.029942
-0.01012 0.111502 -0.00381 0.165315 0.008571
14-Jun 0.159199 0.001468 0.064119 0.005526 0.096729 -0.0062 0.030144 0.01206
0.140136 -0.01303 0.339514 -0.00723
14-Jul 0.161122 -0.00064 0.055495 0.016062 0.086379 0.003641 0.032367
0.001952 0.136925 -0.00857 0.329419 0.014535
14-Aug 0.149222 -0.01024 0.065349 -6.26E-05 0.091117 0.010869 0.024955
-0.00442 0.128269 0.003591 0.296628 0.017301
14-Sep 0.150027 0.005866 0.059474 -0.02036 0.103996 0.002301 0.033988
-0.00461 0.136176 -0.00337 0.291533 -0.00818
14-Oct 0.073368 -0.04392 0.067435 0.008965 0.080492 -0.01163 0.020897
-0.01229 0.113464 -0.01768 0.133705 -0.23033
14-Nov 0.089288 -0.00557 0.084815 -0.00035 0.082332 -0.01346 0.032396
0.008304 0.124252 -0.01689 0.092065 -0.03321
14-Dec 0.094367 0.000647 0.142574 0.006715 0.054609 0.000543 0.047399
-0.00105 0.113318 -0.01409 0.067618 0.011339
15-Jan 0.084719 -0.00414 0.138934 0.01119 0.035142 0.005869 0.050001
0.010183 0.105044 -0.00425 0.047547 0.020317
15-Feb 0.081396 0.020125 0.11201 0.004226 0.038827 0.018429 0.064911
0.014608 0.093223 -0.00763 0.044216 0.005959
15-Mar 0.070694 0.005148 0.090721 0.000676 0.04484 -0.00662 0.068016
0.002282 0.071088 -0.0235 0.050961 -0.00289
15-Apr 0.046477 0.009808 0.039194 0.042848 0.051099 -0.00654 0.058671
0.004364 0.049306 0.005851 0.036681 0.021859
15-May 0.114129 -0.00053 0.04534 -0.00301 0.067418 -0.00534 0.080592
0.000444 0.04979 -0.01078 0.03599 0.000393
15-Jun 0.155868 0.000911 0.055488 -0.00909 0.110349 0.003204 0.09499
-0.00218 0.045165 0.000869 0.042706 -0.01044
15-Jul 0.143998 0.01204 0.055155 -0.00927 0.105175 -0.01308 0.082481
0.005026 0.048215 -0.00283 0.043197 0.0185
15-Aug 0.139796 -0.011 0.053131 -0.02828 0.112001 -0.00753 0.066901 -0.01171
0.040166 -0.01927 0.044748 -0.01313
15-Sep 0.13442 -0.04179 0.049929 -0.01784 0.125934 -0.00807 0.047968
-0.04907 0.082883 -0.00433 0.088255 -0.00258
15-Oct 0.058975 0.019236 0.096952 0.021717 0.116633 -0.00389 0.044177
0.023988 0.110388 -0.01154 0.087908 -0.02361
15-Nov 0.059629 -0.00198 0.097191 -0.00652 0.111808 -0.00195 0.035322
-0.00198 0.103403 0.006467 0.097863 -7.92E-05
15-Dec 0.065516 0.000394 0.107856 -0.00221 0.109277 -0.00446 0.045038
-0.00139 0.105924 -0.00173 0.094134 -0.00311
16-Jan 0.069967 -0.01648 0.124405 -0.02534 0.080609 -0.00782 0.045002
-0.03628 0.067507 -0.02547 0.061998 -0.03317
16-Feb 0.059642 -0.01092 0.093536 -0.04365 0.065717 -0.00534 0.033701
-0.01313 0.052069 0.012975 0.07676 0.00127
16-Mar 0.085488 0.009066 0.097588 0.00784 0.066972 -0.01077 0.035102
0.009299 0.041685 0.00368 0.097081 0.003638
16-Apr 0.077174 -0.02227 0.076671 -0.00295 0.067276 -0.00647 0.021694
0.005411 0.049655 0.005316 0.088254 0.022902
16-May 0.069195 -0.00497 0.067416 -0.00932 0.082775 0.004632 0.033202
-0.00077 0.196307 -0.01142 0.119456 -0.00425
16-Jun 0.097455 -0.0136 0.061482 -0.00162 0.129515 0.012956 0.029188
-0.00652 0.176014 -0.00397 0.125573 0.006204
16-Jul 0.104777 0.014497 0.045435 0.016457 0.100695 0.014244 0.036942
0.018711 0.177081 0.000672 0.09933 -0.00171
16-Aug 0.099126 0.008854 0.050532 0.008233 0.101403 -0.00082 0.040107
-0.00017 0.180629 -0.00393 0.0904 -0.00772
16-Sep 0.095867 0.001927 0.058721 0.006162 0.092854 -0.00179 0.032254
0.007411 0.082157 -0.02736 0.083736 0.006736
16-Oct 0.071165 -0.02676 0.055384 -0.00487 0.07185 0.014646 0.020497 0.00806
0.058031 -0.00201 0.06654 0.033218
16-Nov 0.046141 0.002952 0.063315 -0.00122 0.073631 0.011232 0.027677
-0.01164 0.057347 0.010014 0.056544 0.033138
16-Dec 0.066547 0.000778 0.091166 -0.00091 0.086377 0.02276 0.036674
0.002547 0.06248 0.01014 0.073954 0.008925
17-Jan 0.062046 -0.00505 0.070734 0.01391 0.073518 0.014533 0.049839
0.007438 0.06157 0.022513 0.069436 0.002046
17-Feb 0.048757 0.017769 0.063187 0.003425 0.079388 0.012774 0.056143
0.00427 0.058395 -0.00668 0.164802 -0.00184
17-Mar 0.042455 -0.00482 0.062797 0.002898 0.071547 -0.00202 0.062613
0.001129 0.072438 0.0018 0.237231 0.010924

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/08/20,
12:53:17 PM

On Fri, May 8, 2020 at 2:54 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> For some reason, your data didn't make it through. Maybe you tried to
> send an .xls or .xlsx file. If so, export it as CSV or if it's not too
> big, just paste the text into your email.
>
> Jim
>
> On Thu, May 7, 2020 at 10:30 PM Subhamitra Patra
> <subhamitra.patra at gmail.com> wrote:
> >
> > Dear R-users,
> >
> > I want to estimate ARCH test for multiple columns (i.e.,  from 2:21
> COLUMNS
> > ) in my data. For this purpose, I want to run a loop to calculate ARCH
> test
> > results for each column in the data frame. I tried by using for loop and
> > lapply function, but unable to write a loop for computing the ARCH test
> > simultaneously for each column (i.e., from 2:21 columns) of my data
> frame.
> >
> > Below is my ARCH test code which I want to estimate for multiple columns
> of
> > the data frame in a loop.
> >
> > library(tseries)
> >
> > library(FinTS)
> >
> > ArchTest (A, lags=1, demean = FALSE)
> >
> > Hence, A is a vector for which the ARCH test result is calculated. Here,
> I
> > want to write a loop so that the ArchTest can be calculated
> simultaneously
> > for each column of my data frame. From ARCH test result, I require only
> the
> > calculated Chi-square value and its p-value for each column that stored
> in
> > another matrix or object for each column as an output file.
> >
> > For your convenience, I attached my sample data below. Please find it.
> >
> > Please help me for which I shall be always grateful to you.
> >
> > Thank you.
> >
> > --
> > *Best Regards,*
> > *Subhamitra Patra*
> > *Phd. Research Scholar*
> > *Department of Humanities and Social Sciences*
> > *Indian Institute of Technology, Kharagpur*
> > *INDIA*
> >
> > [image: Mailtrack]
> > <
> https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&
> >
> > Sender
> > notified by
> > Mailtrack
> > <
> https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&
> >
> > 05/07/20,
> > 05:51:03 PM
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>


-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri May  8 11:07:37 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 19:07:37 +1000
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
Message-ID: <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>

Hi Subhamitra,
This isn't too hard:

# read in the sample data that was
# saved in the file "sp_8_5.tab"
sp_8_5<-read.table("sp_8_5.tab",sep="\t",
 header=TRUE,stringsAsFactors=FALSE)
library(tseries)
library(FinTS)
# using "sapply", run the test on each column
spout<-sapply(sp_8_5[,2:12],ArchTest)

The list "spout" contains the test results. If you really want to use a
loop:

spout<-list()
for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])

Jim


On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Dear Sir,
>
> Herewith I am pasting a part of my sample data having 12 columns below,
> and want to calculate ARCH test for the 12 columns by using a loop.
>
>

	[[alternative HTML version deleted]]


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Fri May  8 12:23:46 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Fri, 8 May 2020 15:53:46 +0530
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
Message-ID: <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>

Dear Sir,

Thank you very much for such an excellent solution to my problem. I was
trying sapply function since last days, but was really unable to write
properly. Now, I understood my mistake in using sapply function in the
code. Therefore, I have two queries regarding this which I want to discuss
here just for my learning purpose.

1. While using sapply function for estimating one method across the columns
of a data frame, one needs to define the list of the output table after
using sapply so that the test results for each column will be consistently
stored in an output object, right?

2. In the spout<- list() command, what spout[[i-1]]  indicates?

Sir, one more possibility which I would like to ask related to my above
problem just to learn for further R programming language.

After running your suggested code, all the results for each column are
being stored in the spout object. From this, I need only the statistics and
P-value for each column. So, my queries are:

1. Is there any way to extract only two values (i.e., statistics and
p-value) for each column that stored in spout object and save these two
values in another R data frame for each column?
 or
2. Is there any possibility that the statistics and p-value calculated for
each column can directly export to a word file in a table format (having 4
columns and 3 rows). In particular, is it possible to extract both
statistic and p-value results for each column to an MS word file with the
format of A1, A2, A3, A4 column results in 1st row, A5, A6, A7, A8 column
results in 2nd row, and A9, A10, A11, A12 column results in the 3rd row of
the table?


Like before, your suggestion will definitely help me to learn the advanced
R language.

Thank you very much for your help.

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/08/20,
03:47:26 PM

On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> This isn't too hard:
>
> # read in the sample data that was
> # saved in the file "sp_8_5.tab"
> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>  header=TRUE,stringsAsFactors=FALSE)
> library(tseries)
> library(FinTS)
> # using "sapply", run the test on each column
> spout<-sapply(sp_8_5[,2:12],ArchTest)
>
> The list "spout" contains the test results. If you really want to use a
> loop:
>
> spout<-list()
> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>
> Jim
>
>
> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Dear Sir,
>>
>> Herewith I am pasting a part of my sample data having 12 columns below,
>> and want to calculate ARCH test for the 12 columns by using a loop.
>>
>>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri May  8 12:32:11 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 20:32:11 +1000
Subject: [R] Adding overlap legend to a histogram
In-Reply-To: <CAC8ss30gDSqkYtSTO-v_H+Z4XJq4Zn2oZ3uCzhnnYREC0fJPtg@mail.gmail.com>
References: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
 <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>
 <CAC8ss30gDSqkYtSTO-v_H+Z4XJq4Zn2oZ3uCzhnnYREC0fJPtg@mail.gmail.com>
Message-ID: <CA+8X3fUL9UP=r-pzQwHRVrQMgtyZrSUVXrPhVpRT=O=S2Wqn8A@mail.gmail.com>

Hi Ogbos,
While this solution is not entirely correct, I think it is a start.
First I took your data files and made them "sourceable" by adding
"FD[1|2]<-" at the top and renaming them "FD1.R" and "FD2.R". Running
the following code produces something that is at least close to what
you want. The code is fairly well commented so it should be easy to
see what I have done:

# read in the FD1 data
source("FD1.R")
# read in the FD2 data
source("FD2.R")
# convert year-month-day columns to dates
FD1$data.year<-FD1$data.year+ifelse(FD1$data.year < 50,2000,1900)
FD1$date<-as.Date(paste(FD1$data.year,FD1$data.month,FD1$data.day,sep="-"),
 format="%Y-%m-%d")
FD2$data.year<-FD2$data.year+ifelse(FD2$data.year < 50,2000,1900)
FD2$date<-as.Date(paste(FD2$data.year,FD2$data.month,FD2$data.day,sep="-"),
 format="%Y-%m-%d")
# check the ranges for overlap
range(FD1$date)
range(FD2$date)
# get the overall range of the plot
xlim<-range(c(FD1$date,FD2$date))
# FD1 spans the date range so xlim is not really needed here
# now get the counts for each data set
FD1counts<-as.vector(table(cut(FD1$date,breaks="years")))
# FD2 is missing 1996, 1997 and 2016 so add zeros at the beginning and end
FD2counts<-c(0,0,as.vector(table(cut(FD2$date,breaks="years"))),0)
# set up the bar colors
barcol<-matrix(c(rep("red",2),rep("blue",18),"red",
 rep("red",2),rep("green",18),"red"),nrow=2,byrow=TRUE)
# use barp as barplot can't do the colors
library(plotrix)
barp(rbind(FD1counts,FD2counts),names.arg=1996:2016,
 main="Observation counts for FD1 and FD2",
 xlab="Year",ylab="Observations",col=barcol)
legend(12,80,c("FD1 only","FD1 & FD2","FD2 & FD1"),
 fill=c("red","blue","green"))

This shows the overlap in blue and green. You can make the overlap
colors whatever you like. It doesn't account for the fact that FD2
only overlaps for part of a year on both ends. You may not be worried
about this.

Jim

On Fri, May 8, 2020 at 4:07 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>
> Dear Jim,
> Thank you for looking into this.
> Sorry, there was actually no overlap in the small part of the data I reported. My error of omission.
>
> So when I run my full data with the adjustment you made, I got some thing that was far from what I was expecting. That tell me that I need to send the complete data to enable you correctly adjust the code, especially in the light of the missing/present overlap.
> I have used deput function to attach the two files. Please use any symbol to depict the color/mark/legend of the overlap dates (just to enable the reader visualize what is going on). I am actually trying to display event frequency/occurrence per year.
>
> Thank you and warmest regards
> Ogbos
>
> On Fri, May 8, 2020 at 1:13 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Ogbos,
>> I don't think that your example allows us to work out what you are
>> trying to do. For one thing, "x1" and "x2" do not overlap. Also, you
>> are plotting the frequencies of dates of observations, which may not
>> be what you want.
>> The following code will correctly display your example:
>>
>> hist(x1,breaks="years",freq=T,axes=F,xlim=c(9400,11100),col=c1a)
>> hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
>>  axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x2)), by="years"))
>>  axis(2)
>> legend("topleft", c("AUTO", "MANUAL"), fill=c("red", "blue"))
>>
>> What it is displaying is the frequency of observations in your two
>> vectors by calendar year. If this is what you want, and you can
>> explain how you would like "overlap" to be displayed, we can probably
>> provide better help.
>>
>> Jim
>>
>>
>> On Fri, May 8, 2020 at 7:01 AM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
>> >
>> > Dear Experts,
>> > Greetings.
>> >
>> > I am trying to display two datasets in a histogram. I have been able to
>> > plot the graph and added the legend for the two colors. I am, however,
>> > having difficulties adding a legend to represent the regions of overlap
>> > (the third legend).  Below are my data and code.


From drj|m|emon @end|ng |rom gm@||@com  Fri May  8 13:17:06 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 May 2020 21:17:06 +1000
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
 <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
Message-ID: <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>

1) In general, *apply functions return a list with the number of elements
equal to the number of columns or other elements of the input data. You can
assign that list as I have to "spout" in the first example.

2) spout<-list() assigns the name "spout" to an empty list. As we are
processing columns 2 to 12 of the input data, spout[[i-1]] assigns the
results to elements 1 to 11 of the list "spout". Just a low trick.

1a) Yes, you can create a "wrapper" function that will return only the
statistic and p.value.

# create a function that returns only the
# statistic and p.value as a string
archStatP<-function(x) {
 archout<-ArchTest(x)
 return(sprintf("ChiSq = %f, p = %f",archout$statistic,archout$p.value))
}
# using "lapply", run the test on each column
spout<-lapply(sp_8_5[,2:12],archStatP)

Note that I should have used "lapply". I didn't check the output carefully
enough.

2a) Now you only have to separate the strings in "spout" with TAB
characters and import the result into Excel. I have to wash the dishes, so
you're on your own.

Jim

On Fri, May 8, 2020 at 8:26 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Dear Sir,
>
> Thank you very much for such an excellent solution to my problem. I was
> trying sapply function since last days, but was really unable to write
> properly. Now, I understood my mistake in using sapply function in the
> code. Therefore, I have two queries regarding this which I want to discuss
> here just for my learning purpose.
>
> 1. While using sapply function for estimating one method across the
> columns of a data frame, one needs to define the list of the output table
> after using sapply so that the test results for each column will be
> consistently stored in an output object, right?
>
> 2. In the spout<- list() command, what spout[[i-1]]  indicates?
>
> Sir, one more possibility which I would like to ask related to my above
> problem just to learn for further R programming language.
>
> After running your suggested code, all the results for each column are
> being stored in the spout object. From this, I need only the statistics and
> P-value for each column. So, my queries are:
>
> 1. Is there any way to extract only two values (i.e., statistics and
> p-value) for each column that stored in spout object and save these two
> values in another R data frame for each column?
>  or
> 2. Is there any possibility that the statistics and p-value calculated for
> each column can directly export to a word file in a table format (having 4
> columns and 3 rows). In particular, is it possible to extract both
> statistic and p-value results for each column to an MS word file with the
> format of A1, A2, A3, A4 column results in 1st row, A5, A6, A7, A8 column
> results in 2nd row, and A9, A10, A11, A12 column results in the 3rd row of
> the table?
>
>
> Like before, your suggestion will definitely help me to learn the advanced
> R language.
>
> Thank you very much for your help.
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
> 03:47:26 PM
>
> On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi Subhamitra,
>> This isn't too hard:
>>
>> # read in the sample data that was
>> # saved in the file "sp_8_5.tab"
>> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>>  header=TRUE,stringsAsFactors=FALSE)
>> library(tseries)
>> library(FinTS)
>> # using "sapply", run the test on each column
>> spout<-sapply(sp_8_5[,2:12],ArchTest)
>>
>> The list "spout" contains the test results. If you really want to use a
>> loop:
>>
>> spout<-list()
>> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>>
>> Jim
>>
>>
>> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
>> subhamitra.patra at gmail.com> wrote:
>>
>>> Dear Sir,
>>>
>>> Herewith I am pasting a part of my sample data having 12 columns below,
>>> and want to calculate ARCH test for the 12 columns by using a loop.
>>>
>>>
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  8 14:07:08 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 May 2020 13:07:08 +0100
Subject: [R] Warning in install.packages : converting NULL pointer to R NULL
Message-ID: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>

Hello,

R 4.0.0 on Ubuntu 20.04, sessionInfo() below.

Since I updated to R 4.0 that every time I try to install a package with 
install.packages() the warning in the title shows up at the end, be the 
installation successful or not. If it is successful, the package loads 
with no problems, so I'm not very worried but it isn't normal (expected) 
behavior, is it?

Here is a run of install.packages().


install.packages('cowplot')
Installing package into ?/usr/local/lib/R/site-library?
(as ?lib? is unspecified)
trying URL 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
==================================================
downloaded 1.2 MB

* installing *source* package ?cowplot? ...
** package ?cowplot? successfully unpacked and MD5 sums checked
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation 
path
* DONE (cowplot)

The downloaded source packages are in
	?/tmp/Rtmp9NXQkt/downloaded_packages?
Warning in install.packages :
   converting NULL pointer to R NULL


Also, I'm running this on RStudio and haven't changed the R library 
directory.


sessionInfo()
R version 4.0.0 (2020-04-24)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 20.04 LTS

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0

locale:
  [1] LC_CTYPE=pt_PT.UTF-8       LC_NUMERIC=C 
LC_TIME=pt_PT.UTF-8
  [4] LC_COLLATE=pt_PT.UTF-8     LC_MONETARY=pt_PT.UTF-8 
LC_MESSAGES=pt_PT.UTF-8
  [7] LC_PAPER=pt_PT.UTF-8       LC_NAME=C                  LC_ADDRESS=C 

[10] LC_TELEPHONE=C             LC_MEASUREMENT=pt_PT.UTF-8 
LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] MASS_7.3-51.6  ggthemes_4.2.0 ggrepel_0.8.2  dplyr_0.8.5 
ggplot2_3.3.0

loaded via a namespace (and not attached):
  [1] zoo_1.8-8          tidyselect_1.0.0   purrr_0.3.4 
reshape2_1.4.4     haven_2.2.0
  [6] lattice_0.20-41    sodium_1.1         carData_3.0-3 
colorspace_1.4-1   vctrs_0.2.4
[11] yaml_2.2.1         rlang_0.4.6        pillar_1.4.3 
withr_2.2.0        foreign_0.8-79
[16] glue_1.4.0         readxl_1.3.1       lifecycle_0.2.0    plyr_1.8.6 
         stringr_1.4.0
[21] MatrixModels_0.4-1 munsell_0.5.0      gtable_0.3.0 
cellranger_1.1.0   zip_2.0.4
[26] rio_0.5.16         forcats_0.5.0      SparseM_1.78 
quantreg_5.55      curl_4.3
[31] tis_1.38           Rcpp_1.0.4.6       readr_1.3.1 
scales_1.1.0       abind_1.4-5
[36] farver_2.0.3       sos_2.0-0          brew_1.0-6 
digest_0.6.25      hms_0.5.3
[41] png_0.1-7          stringi_1.4.6      openxlsx_4.1.4     grid_4.0.0 
         tools_4.0.0
[46] magrittr_1.5       tibble_3.0.1       pacman_0.5.1 
crayon_1.3.4       car_3.0-7
[51] pkgconfig_2.0.3    ellipsis_0.3.0     Matrix_1.2-18 
data.table_1.12.8  assertthat_0.2.1
[56] httr_1.4.1         rstudioapi_0.11    R6_2.4.1 
compiler_4.0.0


Thanks in advance,

Rui Barradas


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Fri May  8 14:26:14 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Fri, 8 May 2020 16:56:14 +0430
Subject: [R] Question about topic modelling
Message-ID: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>

Hi,
I hope you are doing well!
I have a question about topic modeling. Please consider summarized steps
for making a LDA (Latent Direchlet Allocation) model:
1-importing data
2-making a corpus.
3-pre-processing and cleaning data
4-making term document matrix
5-Apply LDA in topicmodel package.
During mentioned steps, should i convert my data to Tidy format using
unnest_tokens or not?

Many thanks!
With best regards,

-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Fri May  8 14:31:54 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Fri, 8 May 2020 17:01:54 +0430
Subject: [R] Question about package "SentimentAnalysis"
In-Reply-To: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>
References: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>
Message-ID: <CAGN=ytNHHmuP3Jn_6UAaaN9c3HQCDuEMG6JYRc4OeKjs+4_8Uw@mail.gmail.com>

Hi,
I hope you are doing well!
I read a vignette (
https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html)
about interested package, "SentimentAnalysis". But i faced with a question.
In mentioned  vignette, the sentiment has been applied on a sentence or
multiple sentences separately. Can this package calculate sentiment
direction/score for a long texts?
for example:

# Create a vector of strings
documents <- "Wow, I really like the new light sabers!That book was
excellent.R is a fantastic language.The service in this restaurant was
miserable.This is neither positive or negative."

# Analyze sentiment
sentiment <- analyzeSentiment(documents)

Many thanks!
With best regards,

-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Fri May  8 14:33:12 2020
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Fri, 8 May 2020 13:33:12 +0100
Subject: [R] Adding overlap legend to a histogram: FIXED!!!!!!!!!!!!
In-Reply-To: <CA+8X3fUL9UP=r-pzQwHRVrQMgtyZrSUVXrPhVpRT=O=S2Wqn8A@mail.gmail.com>
References: <CAC8ss30Av_LoM4eYh9TsPhEnpUWG4nDxGduoDiREiuQDmpL+sQ@mail.gmail.com>
 <CA+8X3fUZOcmHjJ=E5yLG5j+266US+vFgaYJS1CPR0ofJDZ8fpw@mail.gmail.com>
 <CAC8ss30gDSqkYtSTO-v_H+Z4XJq4Zn2oZ3uCzhnnYREC0fJPtg@mail.gmail.com>
 <CA+8X3fUL9UP=r-pzQwHRVrQMgtyZrSUVXrPhVpRT=O=S2Wqn8A@mail.gmail.com>
Message-ID: <CAC8ss31GFH4BU4PNpvTedE1ow=vLy4A6hHyVb=P_LdohSTGWuA@mail.gmail.com>

Dear Jim,
This is too great!!! I nearly got lost as I struggle to compare my data
with the graph.  I have to use a coincident algorithm to compare the two
datasets with the histogram before I begin to understand what is going on.

Thank you for giving me more than I requested/expected!!! This is the kind
of confusing/complicating diagrams that best suit some reviewers.

I am yet at the verge of understanding the entire results and will surely
return with some more queries as I progress.

Warmest regards
Ogbos

On Fri, May 8, 2020 at 11:32 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Ogbos,
> While this solution is not entirely correct, I think it is a start.
> First I took your data files and made them "sourceable" by adding
> "FD[1|2]<-" at the top and renaming them "FD1.R" and "FD2.R". Running
> the following code produces something that is at least close to what
> you want. The code is fairly well commented so it should be easy to
> see what I have done:
>
> # read in the FD1 data
> source("FD1.R")
> # read in the FD2 data
> source("FD2.R")
> # convert year-month-day columns to dates
> FD1$data.year<-FD1$data.year+ifelse(FD1$data.year < 50,2000,1900)
> FD1$date<-as.Date(paste(FD1$data.year,FD1$data.month,FD1$data.day,sep="-"),
>  format="%Y-%m-%d")
> FD2$data.year<-FD2$data.year+ifelse(FD2$data.year < 50,2000,1900)
> FD2$date<-as.Date(paste(FD2$data.year,FD2$data.month,FD2$data.day,sep="-"),
>  format="%Y-%m-%d")
> # check the ranges for overlap
> range(FD1$date)
> range(FD2$date)
> # get the overall range of the plot
> xlim<-range(c(FD1$date,FD2$date))
> # FD1 spans the date range so xlim is not really needed here
> # now get the counts for each data set
> FD1counts<-as.vector(table(cut(FD1$date,breaks="years")))
> # FD2 is missing 1996, 1997 and 2016 so add zeros at the beginning and end
> FD2counts<-c(0,0,as.vector(table(cut(FD2$date,breaks="years"))),0)
> # set up the bar colors
> barcol<-matrix(c(rep("red",2),rep("blue",18),"red",
>  rep("red",2),rep("green",18),"red"),nrow=2,byrow=TRUE)
> # use barp as barplot can't do the colors
> library(plotrix)
> barp(rbind(FD1counts,FD2counts),names.arg=1996:2016,
>  main="Observation counts for FD1 and FD2",
>  xlab="Year",ylab="Observations",col=barcol)
> legend(12,80,c("FD1 only","FD1 & FD2","FD2 & FD1"),
>  fill=c("red","blue","green"))
>
> This shows the overlap in blue and green. You can make the overlap
> colors whatever you like. It doesn't account for the fact that FD2
> only overlaps for part of a year on both ends. You may not be worried
> about this.
>
> Jim
>
> On Fri, May 8, 2020 at 4:07 PM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >
> > Dear Jim,
> > Thank you for looking into this.
> > Sorry, there was actually no overlap in the small part of the data I
> reported. My error of omission.
> >
> > So when I run my full data with the adjustment you made, I got some
> thing that was far from what I was expecting. That tell me that I need to
> send the complete data to enable you correctly adjust the code, especially
> in the light of the missing/present overlap.
> > I have used deput function to attach the two files. Please use any
> symbol to depict the color/mark/legend of the overlap dates (just to enable
> the reader visualize what is going on). I am actually trying to display
> event frequency/occurrence per year.
> >
> > Thank you and warmest regards
> > Ogbos
> >
> > On Fri, May 8, 2020 at 1:13 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >>
> >> Hi Ogbos,
> >> I don't think that your example allows us to work out what you are
> >> trying to do. For one thing, "x1" and "x2" do not overlap. Also, you
> >> are plotting the frequencies of dates of observations, which may not
> >> be what you want.
> >> The following code will correctly display your example:
> >>
> >> hist(x1,breaks="years",freq=T,axes=F,xlim=c(9400,11100),col=c1a)
> >> hist(x2,breaks="years",freq=T,axes=F,add=T,col=c2a)
> >>  axis.Date(1, at=seq(as.Date(min(x1)), as.Date(max(x2)), by="years"))
> >>  axis(2)
> >> legend("topleft", c("AUTO", "MANUAL"), fill=c("red", "blue"))
> >>
> >> What it is displaying is the frequency of observations in your two
> >> vectors by calendar year. If this is what you want, and you can
> >> explain how you would like "overlap" to be displayed, we can probably
> >> provide better help.
> >>
> >> Jim
> >>
> >>
> >> On Fri, May 8, 2020 at 7:01 AM Ogbos Okike <giftedlife2014 at gmail.com>
> wrote:
> >> >
> >> > Dear Experts,
> >> > Greetings.
> >> >
> >> > I am trying to display two datasets in a histogram. I have been able
> to
> >> > plot the graph and added the legend for the two colors. I am, however,
> >> > having difficulties adding a legend to represent the regions of
> overlap
> >> > (the third legend).  Below are my data and code.
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri May  8 14:56:46 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 8 May 2020 08:56:46 -0400
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
Message-ID: <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>

That looks like an RStudio message.  Do you get it if you run 
install.packages() in command line R?

Duncan Murdoch

On 08/05/2020 8:07 a.m., Rui Barradas wrote:
> Hello,
> 
> R 4.0.0 on Ubuntu 20.04, sessionInfo() below.
> 
> Since I updated to R 4.0 that every time I try to install a package with
> install.packages() the warning in the title shows up at the end, be the
> installation successful or not. If it is successful, the package loads
> with no problems, so I'm not very worried but it isn't normal (expected)
> behavior, is it?
> 
> Here is a run of install.packages().
> 
> 
> install.packages('cowplot')
> Installing package into ?/usr/local/lib/R/site-library?
> (as ?lib? is unspecified)
> trying URL 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
> Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
> ==================================================
> downloaded 1.2 MB
> 
> * installing *source* package ?cowplot? ...
> ** package ?cowplot? successfully unpacked and MD5 sums checked
> ** using staged installation
> ** R
> ** inst
> ** byte-compile and prepare package for lazy loading
> ** help
> *** installing help indices
> *** copying figures
> ** building package indices
> ** installing vignettes
> ** testing if installed package can be loaded from temporary location
> ** testing if installed package can be loaded from final location
> ** testing if installed package keeps a record of temporary installation
> path
> * DONE (cowplot)
> 
> The downloaded source packages are in
> 	?/tmp/Rtmp9NXQkt/downloaded_packages?
> Warning in install.packages :
>     converting NULL pointer to R NULL
> 
> 
> Also, I'm running this on RStudio and haven't changed the R library
> directory.
> 
> 
> sessionInfo()
> R version 4.0.0 (2020-04-24)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 20.04 LTS
> 
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
> 
> locale:
>    [1] LC_CTYPE=pt_PT.UTF-8       LC_NUMERIC=C
> LC_TIME=pt_PT.UTF-8
>    [4] LC_COLLATE=pt_PT.UTF-8     LC_MONETARY=pt_PT.UTF-8
> LC_MESSAGES=pt_PT.UTF-8
>    [7] LC_PAPER=pt_PT.UTF-8       LC_NAME=C                  LC_ADDRESS=C
> 
> [10] LC_TELEPHONE=C             LC_MEASUREMENT=pt_PT.UTF-8
> LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
> [1] MASS_7.3-51.6  ggthemes_4.2.0 ggrepel_0.8.2  dplyr_0.8.5
> ggplot2_3.3.0
> 
> loaded via a namespace (and not attached):
>    [1] zoo_1.8-8          tidyselect_1.0.0   purrr_0.3.4
> reshape2_1.4.4     haven_2.2.0
>    [6] lattice_0.20-41    sodium_1.1         carData_3.0-3
> colorspace_1.4-1   vctrs_0.2.4
> [11] yaml_2.2.1         rlang_0.4.6        pillar_1.4.3
> withr_2.2.0        foreign_0.8-79
> [16] glue_1.4.0         readxl_1.3.1       lifecycle_0.2.0    plyr_1.8.6
>           stringr_1.4.0
> [21] MatrixModels_0.4-1 munsell_0.5.0      gtable_0.3.0
> cellranger_1.1.0   zip_2.0.4
> [26] rio_0.5.16         forcats_0.5.0      SparseM_1.78
> quantreg_5.55      curl_4.3
> [31] tis_1.38           Rcpp_1.0.4.6       readr_1.3.1
> scales_1.1.0       abind_1.4-5
> [36] farver_2.0.3       sos_2.0-0          brew_1.0-6
> digest_0.6.25      hms_0.5.3
> [41] png_0.1-7          stringi_1.4.6      openxlsx_4.1.4     grid_4.0.0
>           tools_4.0.0
> [46] magrittr_1.5       tibble_3.0.1       pacman_0.5.1
> crayon_1.3.4       car_3.0-7
> [51] pkgconfig_2.0.3    ellipsis_0.3.0     Matrix_1.2-18
> data.table_1.12.8  assertthat_0.2.1
> [56] httr_1.4.1         rstudioapi_0.11    R6_2.4.1
> compiler_4.0.0
> 
> 
> Thanks in advance,
> 
> Rui Barradas
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Fri May  8 15:25:20 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Fri, 8 May 2020 18:55:20 +0530
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
 <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
 <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>
Message-ID: <CAOFE=kO6Pz=vqOR6+mBvjkMMBMhv9bX9s9nUCRnBoF9dGkdiVA@mail.gmail.com>

Dear Sir,

Thank you very much for your wonderful suggestion for my problem. Your
suggested code has excellently worked and successfully extracted the
statistics and p-value in another R object.

Concerning your last suggestion, I attempted to separate the strings with
TAB character in the "spout" object by using different alternative packages
like dplyr, tidyr, qdap, ans also by using split,strsplit function so that
can export the statistics and p-values for each column to excel, and later
to the MSword file, but got the below error.

By using the  split function, I wrote the code as,
*string[] split = s.Split(spout, '\t')*
where I got the following errors.
Error: unexpected symbol in "string[] split"
Error: unexpected symbol in "string[[]]split"
Error in strsplit(row, "\t") : non-character argument

Then I tried with  strsplit function by the below code
*strsplit(spout, split)*
But, got the below error as
Error in as.character(split) :
  cannot coerce type 'closure' to vector of type 'character'.

Then used dplyr and tidyr package and the wrote the below code
library(dplyr)
library(tidyr)
*separate(spout,value,into=c(?ChiSq?,?p?),sep=?,?)*
*separate(spout,List of length 12,into=c(?ChiSq?,?p?),sep="\t")*
But, got the errors as,
Error: unexpected input in "separate(spout,value,into=c(?"
Error: unexpected symbol in "separate(spout,List of"

Then used qdap package with the code below

*colsplit2df(spout,, c("ChiSq", "p"), ",")*
*colsplit2df(spout,, c("ChiSq", "p"), sep = "\t")*
But got the following errors
Error in dataframe[, splitcol] : incorrect number of dimensions
In addition: Warning message:
In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
  dataframe object is not of the class data.frame
Error in dataframe[, splitcol] : incorrect number of dimensions
In addition: Warning message:
In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
  dataframe object is not of the class data.frame

Sir, please suggest me where I am going wrong in the above to separate
string in the "spout" object.

Thank you very much for your help.

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/08/20,
06:51:46 PM

On Fri, May 8, 2020 at 4:47 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> 1) In general, *apply functions return a list with the number of elements
> equal to the number of columns or other elements of the input data. You can
> assign that list as I have to "spout" in the first example.
>
> 2) spout<-list() assigns the name "spout" to an empty list. As we are
> processing columns 2 to 12 of the input data, spout[[i-1]] assigns the
> results to elements 1 to 11 of the list "spout". Just a low trick.
>
> 1a) Yes, you can create a "wrapper" function that will return only the
> statistic and p.value.
>
> # create a function that returns only the
> # statistic and p.value as a string
> archStatP<-function(x) {
>  archout<-ArchTest(x)
>  return(sprintf("ChiSq = %f, p = %f",archout$statistic,archout$p.value))
> }
> # using "lapply", run the test on each column
> spout<-lapply(sp_8_5[,2:12],archStatP)
>
> Note that I should have used "lapply". I didn't check the output carefully
> enough.
>
> 2a) Now you only have to separate the strings in "spout" with TAB
> characters and import the result into Excel. I have to wash the dishes, so
> you're on your own.
>
> Jim
>
> On Fri, May 8, 2020 at 8:26 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Dear Sir,
>>
>> Thank you very much for such an excellent solution to my problem. I was
>> trying sapply function since last days, but was really unable to write
>> properly. Now, I understood my mistake in using sapply function in the
>> code. Therefore, I have two queries regarding this which I want to discuss
>> here just for my learning purpose.
>>
>> 1. While using sapply function for estimating one method across the
>> columns of a data frame, one needs to define the list of the output table
>> after using sapply so that the test results for each column will be
>> consistently stored in an output object, right?
>>
>> 2. In the spout<- list() command, what spout[[i-1]]  indicates?
>>
>> Sir, one more possibility which I would like to ask related to my above
>> problem just to learn for further R programming language.
>>
>> After running your suggested code, all the results for each column are
>> being stored in the spout object. From this, I need only the statistics and
>> P-value for each column. So, my queries are:
>>
>> 1. Is there any way to extract only two values (i.e., statistics and
>> p-value) for each column that stored in spout object and save these two
>> values in another R data frame for each column?
>>  or
>> 2. Is there any possibility that the statistics and p-value
>> calculated for each column can directly export to a word file in a table
>> format (having 4 columns and 3 rows). In particular, is it possible to
>> extract both statistic and p-value results for each column to an MS word
>> file with the format of A1, A2, A3, A4 column results in 1st row, A5, A6,
>> A7, A8 column results in 2nd row, and A9, A10, A11, A12 column results in
>> the 3rd row of the table?
>>
>>
>> Like before, your suggestion will definitely help me to learn the
>> advanced R language.
>>
>> Thank you very much for your help.
>>
>> [image: Mailtrack]
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>> notified by
>> Mailtrack
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
>> 03:47:26 PM
>>
>> On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>>> Hi Subhamitra,
>>> This isn't too hard:
>>>
>>> # read in the sample data that was
>>> # saved in the file "sp_8_5.tab"
>>> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>>>  header=TRUE,stringsAsFactors=FALSE)
>>> library(tseries)
>>> library(FinTS)
>>> # using "sapply", run the test on each column
>>> spout<-sapply(sp_8_5[,2:12],ArchTest)
>>>
>>> The list "spout" contains the test results. If you really want to use a
>>> loop:
>>>
>>> spout<-list()
>>> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>>>
>>> Jim
>>>
>>>
>>> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
>>> subhamitra.patra at gmail.com> wrote:
>>>
>>>> Dear Sir,
>>>>
>>>> Herewith I am pasting a part of my sample data having 12 columns below,
>>>> and want to calculate ARCH test for the 12 columns by using a loop.
>>>>
>>>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  8 15:46:45 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 May 2020 14:46:45 +0100
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
 <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>
Message-ID: <d05f1e8b-c0d3-9106-22ec-c0bce0187b06@sapo.pt>

Hello,

You are right,

Rscript -e 'install.packages("car")'

doesn't give that message, I will ask RStudio support.
And sorry to spam the list with something I should have checked, I'm so 
used to working in GUI 's that I forgot about the command line.

Thanks,

Rui Barradas

?s 13:56 de 08/05/20, Duncan Murdoch escreveu:
> That looks like an RStudio message.? Do you get it if you run 
> install.packages() in command line R?
> 
> Duncan Murdoch
> 
> On 08/05/2020 8:07 a.m., Rui Barradas wrote:
>> Hello,
>>
>> R 4.0.0 on Ubuntu 20.04, sessionInfo() below.
>>
>> Since I updated to R 4.0 that every time I try to install a package with
>> install.packages() the warning in the title shows up at the end, be the
>> installation successful or not. If it is successful, the package loads
>> with no problems, so I'm not very worried but it isn't normal (expected)
>> behavior, is it?
>>
>> Here is a run of install.packages().
>>
>>
>> install.packages('cowplot')
>> Installing package into ?/usr/local/lib/R/site-library?
>> (as ?lib? is unspecified)
>> trying URL 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
>> Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
>> ==================================================
>> downloaded 1.2 MB
>>
>> * installing *source* package ?cowplot? ...
>> ** package ?cowplot? successfully unpacked and MD5 sums checked
>> ** using staged installation
>> ** R
>> ** inst
>> ** byte-compile and prepare package for lazy loading
>> ** help
>> *** installing help indices
>> *** copying figures
>> ** building package indices
>> ** installing vignettes
>> ** testing if installed package can be loaded from temporary location
>> ** testing if installed package can be loaded from final location
>> ** testing if installed package keeps a record of temporary installation
>> path
>> * DONE (cowplot)
>>
>> The downloaded source packages are in
>> ?????/tmp/Rtmp9NXQkt/downloaded_packages?
>> Warning in install.packages :
>> ??? converting NULL pointer to R NULL
>>
>>
>> Also, I'm running this on RStudio and haven't changed the R library
>> directory.
>>
>>
>> sessionInfo()
>> R version 4.0.0 (2020-04-24)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>> Running under: Ubuntu 20.04 LTS
>>
>> Matrix products: default
>> BLAS:?? /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
>> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
>>
>> locale:
>> ?? [1] LC_CTYPE=pt_PT.UTF-8?????? LC_NUMERIC=C
>> LC_TIME=pt_PT.UTF-8
>> ?? [4] LC_COLLATE=pt_PT.UTF-8???? LC_MONETARY=pt_PT.UTF-8
>> LC_MESSAGES=pt_PT.UTF-8
>> ?? [7] LC_PAPER=pt_PT.UTF-8?????? LC_NAME=C????????????????? LC_ADDRESS=C
>>
>> [10] LC_TELEPHONE=C???????????? LC_MEASUREMENT=pt_PT.UTF-8
>> LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
>>
>> other attached packages:
>> [1] MASS_7.3-51.6? ggthemes_4.2.0 ggrepel_0.8.2? dplyr_0.8.5
>> ggplot2_3.3.0
>>
>> loaded via a namespace (and not attached):
>> ?? [1] zoo_1.8-8????????? tidyselect_1.0.0?? purrr_0.3.4
>> reshape2_1.4.4???? haven_2.2.0
>> ?? [6] lattice_0.20-41??? sodium_1.1???????? carData_3.0-3
>> colorspace_1.4-1?? vctrs_0.2.4
>> [11] yaml_2.2.1???????? rlang_0.4.6??????? pillar_1.4.3
>> withr_2.2.0??????? foreign_0.8-79
>> [16] glue_1.4.0???????? readxl_1.3.1?????? lifecycle_0.2.0??? plyr_1.8.6
>> ????????? stringr_1.4.0
>> [21] MatrixModels_0.4-1 munsell_0.5.0????? gtable_0.3.0
>> cellranger_1.1.0?? zip_2.0.4
>> [26] rio_0.5.16???????? forcats_0.5.0????? SparseM_1.78
>> quantreg_5.55????? curl_4.3
>> [31] tis_1.38?????????? Rcpp_1.0.4.6?????? readr_1.3.1
>> scales_1.1.0?????? abind_1.4-5
>> [36] farver_2.0.3?????? sos_2.0-0????????? brew_1.0-6
>> digest_0.6.25????? hms_0.5.3
>> [41] png_0.1-7????????? stringi_1.4.6????? openxlsx_4.1.4???? grid_4.0.0
>> ????????? tools_4.0.0
>> [46] magrittr_1.5?????? tibble_3.0.1?????? pacman_0.5.1
>> crayon_1.3.4?????? car_3.0-7
>> [51] pkgconfig_2.0.3??? ellipsis_0.3.0???? Matrix_1.2-18
>> data.table_1.12.8? assertthat_0.2.1
>> [56] httr_1.4.1???????? rstudioapi_0.11??? R6_2.4.1
>> compiler_4.0.0
>>
>>
>> Thanks in advance,
>>
>> Rui Barradas
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri May  8 16:30:46 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 8 May 2020 16:30:46 +0200
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <d05f1e8b-c0d3-9106-22ec-c0bce0187b06@sapo.pt>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
 <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>
 <d05f1e8b-c0d3-9106-22ec-c0bce0187b06@sapo.pt>
Message-ID: <24245.27926.352000.88429@stat.math.ethz.ch>

>>>>> Rui Barradas 
>>>>>     on Fri, 8 May 2020 14:46:45 +0100 writes:

    > Hello, You are right,

    > Rscript -e 'install.packages("car")'

    > doesn't give that message, I will ask RStudio support.
    > And sorry to spam the list with something I should have
    > checked, I'm so used to working in GUI 's that I forgot
    > about the command line.

Well, you forgot that Rstudio wraps quite a bit around R.
If you use install.packages() inside Rstudio you get their own
version instead of R's ... :

> install.packages
function (...) 
.rs.callAs(name, hook, original, ...)
<environment: 0x55a548da49f0>
>

And they have really tweaked R in a way that it behaves
illogically, and even I don't see how they kept their version of
install.packages hidden from the conflicts() and find()
functions :

> find("install.packages")
[1] "package:utils"

But of course

> identical(install.packages, utils::install.packages)
[1] FALSE

(Now closing Rstudio again .. and revert to use ESS)

Martin

    > Thanks,

    > Rui Barradas

    > ?s 13:56 de 08/05/20, Duncan Murdoch escreveu:
    >> That looks like an RStudio message.? Do you get it if you
    >> run install.packages() in command line R?
    >> 
    >> Duncan Murdoch
    >> 
    >> On 08/05/2020 8:07 a.m., Rui Barradas wrote:
    >>> Hello,
    >>> 
    >>> R 4.0.0 on Ubuntu 20.04, sessionInfo() below.
    >>> 
    >>> Since I updated to R 4.0 that every time I try to
    >>> install a package with install.packages() the warning in
    >>> the title shows up at the end, be the installation
    >>> successful or not. If it is successful, the package
    >>> loads with no problems, so I'm not very worried but it
    >>> isn't normal (expected) behavior, is it?
    >>> 
    >>> Here is a run of install.packages().
    >>> 
    >>> 
    >>> install.packages('cowplot') Installing package into
    >>> ?/usr/local/lib/R/site-library? (as ?lib? is
    >>> unspecified) trying URL
    >>> 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
    >>> Content type 'application/x-gzip' length 1275585 bytes
    >>> (1.2 MB)
    >>> ==================================================
    >>> downloaded 1.2 MB
    >>> 
    >>> * installing *source* package ?cowplot? ...  ** package
    >>> ?cowplot? successfully unpacked and MD5 sums checked **
    >>> using staged installation ** R ** inst ** byte-compile
    >>> and prepare package for lazy loading ** help ***
    >>> installing help indices *** copying figures ** building
    >>> package indices ** installing vignettes ** testing if
    >>> installed package can be loaded from temporary location
    >>> ** testing if installed package can be loaded from final
    >>> location ** testing if installed package keeps a record
    >>> of temporary installation path * DONE (cowplot)
    >>> 
    >>> The downloaded source packages are in
    >>> ?????/tmp/Rtmp9NXQkt/downloaded_packages? Warning in
    >>> install.packages : ??? converting NULL pointer to R NULL
    >>> 
    >>> 
    >>> Also, I'm running this on RStudio and haven't changed
    >>> the R library directory.
    >>> 
    >>> 
    >>> sessionInfo() R version 4.0.0 (2020-04-24) Platform:
    >>> x86_64-pc-linux-gnu (64-bit) Running under: Ubuntu 20.04
    >>> LTS
    >>> 
    >>> Matrix products: default BLAS:??
    >>> /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 LAPACK:
    >>> /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
    >>> 
    >>> locale: ?? [1] LC_CTYPE=pt_PT.UTF-8?????? LC_NUMERIC=C
    >>> LC_TIME=pt_PT.UTF-8 ?? [4] LC_COLLATE=pt_PT.UTF-8????
    >>> LC_MONETARY=pt_PT.UTF-8 LC_MESSAGES=pt_PT.UTF-8 ?? [7]
    >>> LC_PAPER=pt_PT.UTF-8?????? LC_NAME=C?????????????????
    >>> LC_ADDRESS=C
    >>> 
    >>> [10] LC_TELEPHONE=C????????????
    >>> LC_MEASUREMENT=pt_PT.UTF-8 LC_IDENTIFICATION=C
    >>> 
    >>> attached base packages: [1] stats???? graphics?
    >>> grDevices utils???? datasets? methods?? base
    >>> 
    >>> other attached packages: [1] MASS_7.3-51.6?
    >>> ggthemes_4.2.0 ggrepel_0.8.2? dplyr_0.8.5 ggplot2_3.3.0
    >>> 
    >>> loaded via a namespace (and not attached): ?? [1]
    >>> zoo_1.8-8????????? tidyselect_1.0.0?? purrr_0.3.4
    >>> reshape2_1.4.4???? haven_2.2.0 ?? [6] lattice_0.20-41???
    >>> sodium_1.1???????? carData_3.0-3 colorspace_1.4-1??
    >>> vctrs_0.2.4 [11] yaml_2.2.1???????? rlang_0.4.6???????
    >>> pillar_1.4.3 withr_2.2.0??????? foreign_0.8-79 [16]
    >>> glue_1.4.0???????? readxl_1.3.1?????? lifecycle_0.2.0???
    >>> plyr_1.8.6 ????????? stringr_1.4.0 [21]
    >>> MatrixModels_0.4-1 munsell_0.5.0????? gtable_0.3.0
    >>> cellranger_1.1.0?? zip_2.0.4 [26] rio_0.5.16????????
    >>> forcats_0.5.0????? SparseM_1.78 quantreg_5.55?????
    >>> curl_4.3 [31] tis_1.38?????????? Rcpp_1.0.4.6??????
    >>> readr_1.3.1 scales_1.1.0?????? abind_1.4-5 [36]
    >>> farver_2.0.3?????? sos_2.0-0????????? brew_1.0-6
    >>> digest_0.6.25????? hms_0.5.3 [41] png_0.1-7?????????
    >>> stringi_1.4.6????? openxlsx_4.1.4???? grid_4.0.0
    >>> ????????? tools_4.0.0 [46] magrittr_1.5??????
    >>> tibble_3.0.1?????? pacman_0.5.1 crayon_1.3.4??????
    >>> car_3.0-7 [51] pkgconfig_2.0.3??? ellipsis_0.3.0????
    >>> Matrix_1.2-18 data.table_1.12.8? assertthat_0.2.1 [56]
    >>> httr_1.4.1???????? rstudioapi_0.11??? R6_2.4.1
    >>> compiler_4.0.0
    >>> 
    >>> 
    >>> Thanks in advance,
    >>> 
    >>> Rui Barradas
    >>> 
    >>> ______________________________________________
    >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >>> PLEASE do read the posting guide
    >>> http://www.R-project.org/posting-guide.html and provide
    >>> commented, minimal, self-contained, reproducible code.
    >>> 
    >> 

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From g@v@n@mcgr@th @end|ng |rom dbc@@w@@gov@@u  Wed May  6 05:50:12 2020
From: g@v@n@mcgr@th @end|ng |rom dbc@@w@@gov@@u (Gavan McGrath)
Date: Wed, 6 May 2020 03:50:12 +0000
Subject: [R] Rtools virus
Message-ID: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>

Hi,
My IT department instructed me to uninstall Windows 64-bit: rtools40-x86_64.exe as it contained a virus which they identified at

https://www.virustotal.com/gui/file/5c10d60e73dd0186e8f886ef0b9388bb7dbdfdc17366c14c16183edb08fdb58a/detection

Kind Regards,

Dr Gavan McGrath, PhD, B.E.

Research Scientist
Biodiversity and Conservation Science
Department of Biodiversity, Conservation and Attractions
Street Address: 17 Dick Perry Avenue, Kensington, WA 6151, Australia
Postal Address Locked Bag 104, Bentley Delivery Centre, WA 6983, Australia
Phone: +618 9219 9447 Mobile: +61 458 559 765
Email: gavan.mcgrath at dbca.wa.gov.au

Adjunct Research Fellow
School of Agriculture and Environment
The University of Western Australia
Perth, Western Australia
Email: gavan.mcgrath at uwa.edu.au

________________________________
 This message is confidential and is intended for the recipient named above. If you are not the intended recipient, you must not disclose, use or copy the message or any part of it. If you received this message in error, please notify the sender immediately by replying to this message, then delete it from your system.


From @@w||t @end|ng |rom unom@h@@edu  Wed May  6 21:54:43 2020
From: @@w||t @end|ng |rom unom@h@@edu (Andrew Swift)
Date: Wed, 6 May 2020 19:54:43 +0000
Subject: [R] Function Hints in Mac Dark Mode
Message-ID: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>

Sorry, wasn?t sure exactly where to post this but I noticed that with R 4.0.0 when running a Mac in Dark Mode that the Function Hints at the bottom of the Console and Editor windows become invisible.  
Thanks. 

From rom@no|@on @end|ng |rom yon@e|@@c@kr  Thu May  7 09:46:05 2020
From: rom@no|@on @end|ng |rom yon@e|@@c@kr (Roman Olson)
Date: Thu, 7 May 2020 16:46:05 +0900
Subject: [R] Bug in function arguments autocomplete and ellipsis?
Message-ID: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>

Dear All,

I am wondering whether function arguments autocomplete causes a bug when additional ellipsis arguments are used.

Example:
a = function(robot) {
    cat(robot, "\n")
 }
a(r=5) prints 5, meaning that r is autocompleted to ?robot?. Not sure if this is normal behavior, but this does not cause problems.

This would be fine, but somehow causes problems when there is an additional ellipsis argument that can be used to autocomplete an already existing argument. In the example below, when we are calling sens.analysis.all, everything starting with q is an additional argument (?). Now, k is missed completely. And that is because there is another actual argument that starts with k ? key.legend.axes (it is assigned to 4 instead). If ?k=4? is changed to ?kk=4? the problem disappears.  

sens.analysis.all <- function(func, outgrid, parvec, parmin, parmax,
                              length.pgrid, outvname, zlim, plabs,
                              gridlab, mylog, outlog=FALSE, ytick=NULL,
                              xline=NULL, yline=NULL, mypal=topo.colors,
                              key.legend.axes=NULL, plot.guidance=FALSE, ... ) {

   cat(..1, "\n")
   cat(..2, "\n")
   cat(..3, "\n")
 
   out=1
   out
} 

out = sens.analysis.all(numer.wait.times.wrps, NA,
     c(5.4, 0.008, 1.5), c(4.9, 0.0079, 2.0), c(5.5, 0.0090, 3.5),
       length.pgrid=10, outvname="cvs", zlim=c(0.3, 2),
       plabs=c("mu", "lambda", "b"), gridlab="Soil Moisture [mm]",
       mylog=FALSE, outlog=FALSE, yline=2, plot.guidance=FALSE, q=3.1, k=4, y.c=670,
       realgrid=seq(630, 670, by=4), myseed=0,
       nt=1000000, burnin=300000, bin.cutoff=500,
       bdW.prelim=prec.struct.Mal)

From ||9212001 @end|ng |rom y@hoo@com  Thu May  7 15:23:09 2020
From: ||9212001 @end|ng |rom y@hoo@com (aiguo li)
Date: Thu, 7 May 2020 13:23:09 +0000 (UTC)
Subject: [R] Need a suggestion on a package to make a figure
References: <216384015.2752852.1588857789484.ref@mail.yahoo.com>
Message-ID: <216384015.2752852.1588857789484@mail.yahoo.com>

Hello all,
I need to make a table with a value imaged by greater than certain value as attached.? Could you give me a suggestions on which R package will be good for this type of table?
Thanks and stay safe!
Aiguo
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample_table.pdf
Type: application/pdf
Size: 26796 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200507/3e183657/attachment.pdf>

From k||6891 @end|ng |rom gm@||@com  Thu May  7 21:07:50 2020
From: k||6891 @end|ng |rom gm@||@com (Kevin Li)
Date: Thu, 7 May 2020 15:07:50 -0400
Subject: [R] Why doesn't rpart split further
Message-ID: <CAMTq2VorZsAztWQd6OHBg4fP9X06RGsV9WMUA_HuFHOOkmY_bw@mail.gmail.com>

Hi,

I am using the rpart package to construct regression trees and for the
purposes of simulation, would like to the tree completely split: each
leaf should contain exactly one observation.

However, I have observed that even by setting minsplit = 2, i.e.,

```
control <- rpart.control(
    minsplit = 2,
    cp = -1,
    xval = 0,
    maxcompete = 0,
    usesurrogate=0,
    maxdepth=30
)

model <- rpart(...., control = control)
```

the model will still have leaf nodes with more than one observation.
In fact, when I choose a subset of my dataset which fall into the same
terminal leaf, and run rpart on that subset, further split will occur.
Any advice on why this is occuring? Thanks!


Best regards,
Kevin


P.S. A snippet to showcase the behavior above:

--

library(rpart)
library(data.table)

mu <- function(x, y, z) sin(10 * pi * x + 2 * y) - cos(10 * pi * y) + exp(z)

control <- rpart.control(
    minsplit = 2,
    cp = -1,
    xval = 0,
    maxcompete = 0,
    usesurrogate=0,
    maxdepth=30
)

gen.data <- function(n, sd = 0.5) {
    X <- matrix(runif(3 * n), ncol=3)
    colnames(X) <- c('x', 'y', 'z')
    e <- rnorm(n, sd = sd)

    X <- data.table(X)
    X[, mu := mu(x, y, z)]
    X[, A := mu + e]
    return(X[])
}

# Run rpart on the simulated dataset ...
set.seed(12321)
X <- gen.data(30000, sd = 0.1)
X[, i := .I]
mod <- rpart(A ~ x + y + z, X, control = control)

frame <- as.data.table(mod$frame, keep.rownames=TRUE)
frame[, rn := as.integer(rn)][, i := .I]
setnames(frame, "rn", "id")

splits <- as.data.table(mod$splits, keep.rownames=TRUE)
setnames(splits, "rn", "var")
splits[, var := factor(var)]

where <- data.table(i = seq(1, X[,.N]), where=mod$where)


# m = 7191 is the row of the leaf that contains the most observations,
# in this case 11.
m <- frame[var == "<leaf>"][order(-n)][1, i]
obs <- where[where == m, i]

# Collect those 11 observations another dataframe
X2 <- X[i %in% obs]

# observe that rpart will split on that subset again, why?
mod2 <- rpart(A ~ x + y + z, X2, control=control)


From @me|@ne1 @end|ng |rom jhm|@edu  Thu May  7 21:32:36 2020
From: @me|@ne1 @end|ng |rom jhm|@edu (Allison Meisner)
Date: Thu, 7 May 2020 19:32:36 +0000
Subject: [R] Error in summary.warnings?
Message-ID: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>

I believe there is an error in the summary.warnings function (typically called via 'summary(warnings())'). Below is a minimal working example:

#########

testfunction <- function(x){
if(x > 30){
warning("A big problem (should be 20 of these)")
}else{
warning("Bigger problem (should be 30 of these)")
}
}

for(i in 1:50){
testfunction(i)
}

summary(warnings())

#########

I checked the code for summary.warnings:

function (object, ...)
{
    msgs <- names(object)
    calls <- as.character(object)
    ss <- ": "
    c.m. <- paste(calls, msgs, sep = ss)
    if (length(i.no.call <- which(calls == "NULL")))
        c.m.[i.no.call] <- substr(c.m.[i.no.call], nchar(paste0("NULL",
            ss)) + 1L, 100000L)
    tm <- table(c.m., deparse.level = 0L)
    structure(unique(object), counts = as.vector(tm), class = "summary.warnings")
}

The problem appears to be in the last line: unique preserves the order of the input, but counts reflects the counts in the table tm, which is a problem because table names are in alphabetical order.

Am I missing something?

Allison



----------

Allison Meisner, PhD

Postdoctoral Fellow

Department of Biostatistics

Johns Hopkins Bloomberg School of Public Health

615 N. Wolfe Street

Baltimore, MD 21205



	[[alternative HTML version deleted]]


From gerh|g4 @end|ng |rom gm@||@com  Fri May  8 05:23:14 2020
From: gerh|g4 @end|ng |rom gm@||@com (Sam Rizzuto)
Date: Thu, 7 May 2020 23:23:14 -0400
Subject: [R] Obtain One Row From ggcorr() Matrix
Message-ID: <D309ECD9-D91D-43C6-89F9-22D13C3A895F@gmail.com>

Hi all,

I am looking to obtain one column/row of a correlation matrix from my data using the function ggcorr() from library(GGally). 

As an example, using the mtcars dataset, I have the following code that can reproduce one row/column:

df <- cor(x = mtcars$mpg, y = mtcars[2:11], use = ?everything?)
library(corrplot)
corrplot(df, tl.srt = 45, method = ?color?, addCoef.col = ?black?, cl.cex = 0.56)

This shows all the correlations between mpg. However using ggcorr() and plotting it:

ggcorr(mtcars, method = c(?everything?), label = TRUE, label_size = 2, label_round = 4)

It shows a much nice and prettier looking correlation plot but does it for all variables. 

I have tried putting mtcars$mpg and mtcars[,1] to only return the one row, but neither seem to work. 

Any ideas on how to do it using ggcorr()?

Thanks,
Sam


From @@mue|| @end|ng |rom ||||no|@@edu  Thu May  7 17:32:14 2020
From: @@mue|| @end|ng |rom ||||no|@@edu (Bonfim Fernandes, Samuel)
Date: Thu, 7 May 2020 15:32:14 +0000
Subject: [R] [R-pkgs] simplePHENOTYPES: SIMulation of Pleiotropic,
 Linked and Epistatic PHENOTYPES
Message-ID: <C478E421-C30E-4D57-B0EB-6A45B83E03D3@illinois.edu>

Hi All,

I hope this message finds you well.
I have developed the simplePHENOTYPES package to simulate single and multiple (correlated) traits in a wide range of scenarios, including additive, dominance, and epistatic (AxA) models.
The newest version, simplePHENOTYPES v1.2.4, which has just been released on CRAN, has substantial improvements compared to the previous one.

Some of the new features include:
- Options for using VCF, plink bed/ped files, GDS, HapMap, and Numeric as input files.
- Implementation of two types of spurious pleiotropy simulation (?direct" and "indirect").
- Simulation of residual correlation among traits.

Please check it our here:
https://cran.r-project.org/package=simplePHENOTYPES
Vignettes for the most common scenarios one would want to simulate may be found here:
https://cran.r-project.org/web/packages/simplePHENOTYPES/vignettes/simplePHENOTYPES.html

Best regards,
Samuel Fernandes

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May  8 16:58:34 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 08 May 2020 07:58:34 -0700
Subject: [R] Rtools virus
In-Reply-To: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>
References: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>
Message-ID: <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>

Sorry to hear that. It is most likely a false positive (antivirus software has little incentive to minimise false positives), but no one here can follow up on your report because you did not say precisely which website you downloaded it from.

On May 5, 2020 8:50:12 PM PDT, Gavan McGrath <gavan.mcgrath at dbca.wa.gov.au> wrote:
>Hi,
>My IT department instructed me to uninstall Windows 64-bit:
>rtools40-x86_64.exe as it contained a virus which they identified at
>
>https://www.virustotal.com/gui/file/5c10d60e73dd0186e8f886ef0b9388bb7dbdfdc17366c14c16183edb08fdb58a/detection
>
>Kind Regards,
>
>Dr Gavan McGrath, PhD, B.E.
>
>Research Scientist
>Biodiversity and Conservation Science
>Department of Biodiversity, Conservation and Attractions
>Street Address: 17 Dick Perry Avenue, Kensington, WA 6151, Australia
>Postal Address Locked Bag 104, Bentley Delivery Centre, WA 6983,
>Australia
>Phone: +618 9219 9447 Mobile: +61 458 559 765
>Email: gavan.mcgrath at dbca.wa.gov.au
>
>Adjunct Research Fellow
>School of Agriculture and Environment
>The University of Western Australia
>Perth, Western Australia
>Email: gavan.mcgrath at uwa.edu.au
>
>________________________________
>This message is confidential and is intended for the recipient named
>above. If you are not the intended recipient, you must not disclose,
>use or copy the message or any part of it. If you received this message
>in error, please notify the sender immediately by replying to this
>message, then delete it from your system.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri May  8 17:02:42 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 08 May 2020 08:02:42 -0700
Subject: [R] Function Hints in Mac Dark Mode
In-Reply-To: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>
References: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>
Message-ID: <32244604-925F-4F40-A200-3F1AA6D95D36@dcn.davis.ca.us>

You seem to be confusing R and RStudio... so yeah, wrong mailing list. I don't know exactly where you should post either. Perhaps the GitHub issues page for RStudio?

On May 6, 2020 12:54:43 PM PDT, Andrew Swift via R-help <r-help at r-project.org> wrote:
>Sorry, wasn?t sure exactly where to post this but I noticed that with R
>4.0.0 when running a Mac in Dark Mode that the Function Hints at the
>bottom of the Console and Editor windows become invisible.  
>Thanks. 
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @@w||t @end|ng |rom unom@h@@edu  Fri May  8 17:12:37 2020
From: @@w||t @end|ng |rom unom@h@@edu (Andrew Swift)
Date: Fri, 8 May 2020 15:12:37 +0000
Subject: [R] Function Hints in Mac Dark Mode
In-Reply-To: <769FC136-30D0-4151-98AB-E29C22A4D6F7@me.com>
References: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>
 <32244604-925F-4F40-A200-3F1AA6D95D36@dcn.davis.ca.us>
 <769FC136-30D0-4151-98AB-E29C22A4D6F7@me.com>
Message-ID: <AB763CC0-4825-464D-B06D-9A2875A8E54A@unomaha.edu>

Marc,

Yes, that is exactly the issue.  I?ll post to r-sig-mac.

On May 8, 2020, at 10:11 AM, Marc Schwartz <marc_schwartz at me.com<mailto:marc_schwartz at me.com>> wrote:



On May 8, 2020, at 11:02 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:

You seem to be confusing R and RStudio... so yeah, wrong mailing list. I don't know exactly where you should post either. Perhaps the GitHub issues page for RStudio?

On May 6, 2020 12:54:43 PM PDT, Andrew Swift via R-help <r-help at r-project.org<mailto:r-help at r-project.org>> wrote:
Sorry, wasn?t sure exactly where to post this but I noticed that with R
4.0.0 when running a Mac in Dark Mode that the Function Hints at the
bottom of the Console and Editor windows become invisible.
Thanks.


Hi,

Actually, I am not sure that is the case.

See the attached screen capture of the default R.app (which I do not use) when the desktop is set to dark mode, which I also do not use.

When in light mode, those hints in the lower left hand corner are black, and they switch to white in dark mode, making them almost impossible to see.

For Andrew, if this is correct, and you are not referring to RStudio per Jeff, you should post this to r-sig-mac:

  https://stat.ethz.ch/mailman/listinfo/r-sig-mac<https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dsig-2Dmac&d=DwMFaQ&c=Cu5g146wZdoqVuKpTNsYHeFX_rg6kWhlkLF8Eft-wwo&r=W-hEH5pfgw6X6us1FQf4wr3R0QD-0iVKleJgEEY57pQ&m=eFzkYDSccZeaT6c3QZuFD8BxeoZmisZ1KA4oac3myN4&s=B_Lk9KI3pylJo27TNuCNwTyCM-i5Va2K-gKrWvIbjNg&e=>

Otherwise, if it is RStudio, they have their own support here:

   https://support.rstudio.com/hc/en-us<https://urldefense.proofpoint.com/v2/url?u=https-3A__support.rstudio.com_hc_en-2Dus&d=DwMFaQ&c=Cu5g146wZdoqVuKpTNsYHeFX_rg6kWhlkLF8Eft-wwo&r=W-hEH5pfgw6X6us1FQf4wr3R0QD-0iVKleJgEEY57pQ&m=eFzkYDSccZeaT6c3QZuFD8BxeoZmisZ1KA4oac3myN4&s=Dj-fBrHh61yxxSaZL_kUbp0dHadpIXholuPiNVejemA&e=>

Regards,

Marc Schwartz


<Screen Shot 2020-05-08 at 11.05.02 AM.png>



	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri May  8 17:37:29 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 8 May 2020 17:37:29 +0200
Subject: [R] Error in summary.warnings?
In-Reply-To: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>
References: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>
Message-ID: <24245.31929.569398.561576@stat.math.ethz.ch>

>>>>> Allison Meisner 
>>>>>     on Thu, 7 May 2020 19:32:36 +0000 writes:

    > I believe there is an error in the summary.warnings function (typically called via 'summary(warnings())'). Below is a minimal working example:
    > #########

    > testfunction <- function(x){
    >  if(x > 30){
    >      warning("A big problem (should be 20 of these)")
    >  }else{
    >      warning("Bigger problem (should be 30 of these)")
    >  }
    > }

    > for(i in 1:50){
    >     testfunction(i)
    > }

    > summary(warnings())

    > #########

    > I checked the code for summary.warnings:

    > function (object, ...)
    > {
    > msgs <- names(object)
    > calls <- as.character(object)
    > ss <- ": "
    > c.m. <- paste(calls, msgs, sep = ss)
    > if (length(i.no.call <- which(calls == "NULL")))
    > c.m.[i.no.call] <- substr(c.m.[i.no.call], nchar(paste0("NULL",
    > ss)) + 1L, 100000L)
    > tm <- table(c.m., deparse.level = 0L)
    > structure(unique(object), counts = as.vector(tm), class = "summary.warnings")
    > }

    > The problem appears to be in the last line: unique preserves the order of the input, but counts reflects the counts in the table tm, which is a problem because table names are in alphabetical order.

    > Am I missing something?

No -- I think you are perfect and I was very imperfect ;-)  when
I created and tested the function ..

This will be fixed in the next versions of R.

Thank you very much for the report  and the nice concise
reproducible example!

Best regards,
Martin

    > Allison
    > ----------
    > Allison Meisner, PhD
    > Postdoctoral Fellow
    > Department of Biostatistics
    > Johns Hopkins Bloomberg School of Public Health
    > 615 N. Wolfe Street
    > Baltimore, MD 21205

Martin Maechler
ETH Zurich  and   R Core team


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Fri May  8 17:51:32 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 May 2020 16:51:32 +0100
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <24245.27926.352000.88429@stat.math.ethz.ch>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
 <a835fb6e-b229-1460-0ce5-107594ba3f5b@gmail.com>
 <d05f1e8b-c0d3-9106-22ec-c0bce0187b06@sapo.pt>
 <24245.27926.352000.88429@stat.math.ethz.ch>
Message-ID: <f42464df-cc07-4fd7-3f8c-b18010eb11cb@sapo.pt>

Hello,

My main error was that this is the first time it happens and I didn't do 
any real thinking, I just assumed that it was an upgrade both R and 
Ubuntu issue.

Thanks, the reminder of the differences between R and RStudio was very 
helpful.

Rui Barradas

?s 15:30 de 08/05/20, Martin Maechler escreveu:
>>>>>> Rui Barradas
>>>>>>      on Fri, 8 May 2020 14:46:45 +0100 writes:
> 
>      > Hello, You are right,
> 
>      > Rscript -e 'install.packages("car")'
> 
>      > doesn't give that message, I will ask RStudio support.
>      > And sorry to spam the list with something I should have
>      > checked, I'm so used to working in GUI 's that I forgot
>      > about the command line.
> 
> Well, you forgot that Rstudio wraps quite a bit around R.
> If you use install.packages() inside Rstudio you get their own
> version instead of R's ... :
> 
>> install.packages
> function (...)
> .rs.callAs(name, hook, original, ...)
> <environment: 0x55a548da49f0>
>>
> 
> And they have really tweaked R in a way that it behaves
> illogically, and even I don't see how they kept their version of
> install.packages hidden from the conflicts() and find()
> functions :
> 
>> find("install.packages")
> [1] "package:utils"
> 
> But of course
> 
>> identical(install.packages, utils::install.packages)
> [1] FALSE
> 
> (Now closing Rstudio again .. and revert to use ESS)
> 
> Martin
> 
>      > Thanks,
> 
>      > Rui Barradas
> 
>      > ?s 13:56 de 08/05/20, Duncan Murdoch escreveu:
>      >> That looks like an RStudio message.? Do you get it if you
>      >> run install.packages() in command line R?
>      >>
>      >> Duncan Murdoch
>      >>
>      >> On 08/05/2020 8:07 a.m., Rui Barradas wrote:
>      >>> Hello,
>      >>>
>      >>> R 4.0.0 on Ubuntu 20.04, sessionInfo() below.
>      >>>
>      >>> Since I updated to R 4.0 that every time I try to
>      >>> install a package with install.packages() the warning in
>      >>> the title shows up at the end, be the installation
>      >>> successful or not. If it is successful, the package
>      >>> loads with no problems, so I'm not very worried but it
>      >>> isn't normal (expected) behavior, is it?
>      >>>
>      >>> Here is a run of install.packages().
>      >>>
>      >>>
>      >>> install.packages('cowplot') Installing package into
>      >>> ?/usr/local/lib/R/site-library? (as ?lib? is
>      >>> unspecified) trying URL
>      >>> 'https://cloud.r-project.org/src/contrib/cowplot_1.0.0.tar.gz'
>      >>> Content type 'application/x-gzip' length 1275585 bytes
>      >>> (1.2 MB)
>      >>> ==================================================
>      >>> downloaded 1.2 MB
>      >>>
>      >>> * installing *source* package ?cowplot? ...  ** package
>      >>> ?cowplot? successfully unpacked and MD5 sums checked **
>      >>> using staged installation ** R ** inst ** byte-compile
>      >>> and prepare package for lazy loading ** help ***
>      >>> installing help indices *** copying figures ** building
>      >>> package indices ** installing vignettes ** testing if
>      >>> installed package can be loaded from temporary location
>      >>> ** testing if installed package can be loaded from final
>      >>> location ** testing if installed package keeps a record
>      >>> of temporary installation path * DONE (cowplot)
>      >>>
>      >>> The downloaded source packages are in
>      >>> ?????/tmp/Rtmp9NXQkt/downloaded_packages? Warning in
>      >>> install.packages : ??? converting NULL pointer to R NULL
>      >>>
>      >>>
>      >>> Also, I'm running this on RStudio and haven't changed
>      >>> the R library directory.
>      >>>
>      >>>
>      >>> sessionInfo() R version 4.0.0 (2020-04-24) Platform:
>      >>> x86_64-pc-linux-gnu (64-bit) Running under: Ubuntu 20.04
>      >>> LTS
>      >>>
>      >>> Matrix products: default BLAS:
>      >>> /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 LAPACK:
>      >>> /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
>      >>>
>      >>> locale: ?? [1] LC_CTYPE=pt_PT.UTF-8?????? LC_NUMERIC=C
>      >>> LC_TIME=pt_PT.UTF-8 ?? [4] LC_COLLATE=pt_PT.UTF-8
>      >>> LC_MONETARY=pt_PT.UTF-8 LC_MESSAGES=pt_PT.UTF-8 ?? [7]
>      >>> LC_PAPER=pt_PT.UTF-8?????? LC_NAME=C
>      >>> LC_ADDRESS=C
>      >>>
>      >>> [10] LC_TELEPHONE=C
>      >>> LC_MEASUREMENT=pt_PT.UTF-8 LC_IDENTIFICATION=C
>      >>>
>      >>> attached base packages: [1] stats???? graphics
>      >>> grDevices utils???? datasets? methods?? base
>      >>>
>      >>> other attached packages: [1] MASS_7.3-51.6
>      >>> ggthemes_4.2.0 ggrepel_0.8.2? dplyr_0.8.5 ggplot2_3.3.0
>      >>>
>      >>> loaded via a namespace (and not attached): ?? [1]
>      >>> zoo_1.8-8????????? tidyselect_1.0.0?? purrr_0.3.4
>      >>> reshape2_1.4.4???? haven_2.2.0 ?? [6] lattice_0.20-41
>      >>> sodium_1.1???????? carData_3.0-3 colorspace_1.4-1
>      >>> vctrs_0.2.4 [11] yaml_2.2.1???????? rlang_0.4.6
>      >>> pillar_1.4.3 withr_2.2.0??????? foreign_0.8-79 [16]
>      >>> glue_1.4.0???????? readxl_1.3.1?????? lifecycle_0.2.0
>      >>> plyr_1.8.6 ????????? stringr_1.4.0 [21]
>      >>> MatrixModels_0.4-1 munsell_0.5.0????? gtable_0.3.0
>      >>> cellranger_1.1.0?? zip_2.0.4 [26] rio_0.5.16
>      >>> forcats_0.5.0????? SparseM_1.78 quantreg_5.55
>      >>> curl_4.3 [31] tis_1.38?????????? Rcpp_1.0.4.6
>      >>> readr_1.3.1 scales_1.1.0?????? abind_1.4-5 [36]
>      >>> farver_2.0.3?????? sos_2.0-0????????? brew_1.0-6
>      >>> digest_0.6.25????? hms_0.5.3 [41] png_0.1-7
>      >>> stringi_1.4.6????? openxlsx_4.1.4???? grid_4.0.0
>      >>> ????????? tools_4.0.0 [46] magrittr_1.5
>      >>> tibble_3.0.1?????? pacman_0.5.1 crayon_1.3.4
>      >>> car_3.0-7 [51] pkgconfig_2.0.3??? ellipsis_0.3.0
>      >>> Matrix_1.2-18 data.table_1.12.8? assertthat_0.2.1 [56]
>      >>> httr_1.4.1???????? rstudioapi_0.11??? R6_2.4.1
>      >>> compiler_4.0.0
>      >>>
>      >>>
>      >>> Thanks in advance,
>      >>>
>      >>> Rui Barradas
>      >>>
>      >>> ______________________________________________
>      >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>      >>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
>      >>> PLEASE do read the posting guide
>      >>> http://www.R-project.org/posting-guide.html and provide
>      >>> commented, minimal, self-contained, reproducible code.
>      >>>
>      >>
> 
>      > ______________________________________________
>      > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>      > more, see https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>      > http://www.R-project.org/posting-guide.html and provide
>      > commented, minimal, self-contained, reproducible code.
>


From kry|ov@r00t @end|ng |rom gm@||@com  Fri May  8 18:08:09 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 8 May 2020 19:08:09 +0300
Subject: [R] Rtools virus
In-Reply-To: <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>
References: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>
 <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>
Message-ID: <20200508190809.463228d0@trisector>

On Fri, 08 May 2020 07:58:34 -0700
Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:

>no one here can follow up on your report because you did not say
>precisely which website you downloaded it from

To be fair, the SHA-256 sum in the VirusTotal report matches the one of
rtools40-x86_64.exe:

wget -qO- \
 https://cran.r-project.org/bin/windows/Rtools/rtools40-x86_64.exe | \
 sha256sum -
# 5c10d60e73dd0186e8f886ef0b9388bb7dbdfdc17366c14c16183edb08fdb58a  -

But I do agree that this is most likely a false positive: the AV engine
that detected it seems to be one of the less widely used and the virus
description [*] is as generic as it gets (if one is to believe Google
Translate).

-- 
Best regards,
Ivan

[*]
http://virusinfo.jiangmin.com/queryInfo.asp?virword=Trojan.Pincav.aml


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri May  8 18:22:24 2020
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 8 May 2020 12:22:24 -0400
Subject: [R] Bug in function arguments autocomplete and ellipsis?
In-Reply-To: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
References: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
Message-ID: <60784bd5-81e7-8a24-da9d-d43f3864d987@gmail.com>

On 07/05/2020 3:46 a.m., Roman Olson wrote:
> Dear All,
> 
> I am wondering whether function arguments autocomplete causes a bug when additional ellipsis arguments are used.
> 
> Example:
> a = function(robot) {
>      cat(robot, "\n")
>   }
> a(r=5) prints 5, meaning that r is autocompleted to ?robot?. Not sure if this is normal behavior, but this does not cause problems.
> 
> This would be fine, but somehow causes problems when there is an additional ellipsis argument that can be used to autocomplete an already existing argument. In the example below, when we are calling sens.analysis.all, everything starting with q is an additional argument (?). Now, k is missed completely. And that is because there is another actual argument that starts with k ? key.legend.axes (it is assigned to 4 instead). If ?k=4? is changed to ?kk=4? the problem disappears.
> 
> sens.analysis.all <- function(func, outgrid, parvec, parmin, parmax,
>                                length.pgrid, outvname, zlim, plabs,
>                                gridlab, mylog, outlog=FALSE, ytick=NULL,
>                                xline=NULL, yline=NULL, mypal=topo.colors,
>                                key.legend.axes=NULL, plot.guidance=FALSE, ... ) {
> 
>     cat(..1, "\n")
>     cat(..2, "\n")
>     cat(..3, "\n")
>   
>     out=1
>     out
> }
> 
> out = sens.analysis.all(numer.wait.times.wrps, NA,
>       c(5.4, 0.008, 1.5), c(4.9, 0.0079, 2.0), c(5.5, 0.0090, 3.5),
>         length.pgrid=10, outvname="cvs", zlim=c(0.3, 2),
>         plabs=c("mu", "lambda", "b"), gridlab="Soil Moisture [mm]",
>         mylog=FALSE, outlog=FALSE, yline=2, plot.guidance=FALSE, q=3.1, k=4, y.c=670,
>         realgrid=seq(630, 670, by=4), myseed=0,
>         nt=1000000, burnin=300000, bin.cutoff=500,
>         bdW.prelim=prec.struct.Mal)

I'm afraid I don't understand what you find surprising here.  In your 
first example, the "r" gets attached to "robot" because of partial 
matching.  In the second example, the "k" gets attached to 
"key.legend.axes" for the same reason.  This is expected behaviour.  If 
you don't want to allow  partial matching to arguments, they need to be 
placed *after* the ellipsis.  Those arguments will need to be spelled 
out in full.

Duncan Murdoch


From bgunter@4567 @end|ng |rom gm@||@com  Fri May  8 18:24:46 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 8 May 2020 09:24:46 -0700
Subject: [R] Bug in function arguments autocomplete and ellipsis?
In-Reply-To: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
References: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
Message-ID: <CAGxFJbT3vbOoe-OHWwDGAC9fcuyE8C0tQVqKp9kg6uv+Nd0vOg@mail.gmail.com>

It would help if you consulted the docs, in this case, **The R
Language Definition** and, in particular, 4.3.2 on argument matching.
I won't repeat what it is there, but I believe it will suffice to
dispel your confusion.


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Fri, May 8, 2020 at 7:49 AM Roman Olson <romanolson at yonsei.ac.kr> wrote:
>
> Dear All,
>
> I am wondering whether function arguments autocomplete causes a bug when additional ellipsis arguments are used.
>
> Example:
> a = function(robot) {
>     cat(robot, "\n")
>  }
> a(r=5) prints 5, meaning that r is autocompleted to ?robot?. Not sure if this is normal behavior, but this does not cause problems.
>
> This would be fine, but somehow causes problems when there is an additional ellipsis argument that can be used to autocomplete an already existing argument. In the example below, when we are calling sens.analysis.all, everything starting with q is an additional argument (?). Now, k is missed completely. And that is because there is another actual argument that starts with k ? key.legend.axes (it is assigned to 4 instead). If ?k=4? is changed to ?kk=4? the problem disappears.
>
> sens.analysis.all <- function(func, outgrid, parvec, parmin, parmax,
>                               length.pgrid, outvname, zlim, plabs,
>                               gridlab, mylog, outlog=FALSE, ytick=NULL,
>                               xline=NULL, yline=NULL, mypal=topo.colors,
>                               key.legend.axes=NULL, plot.guidance=FALSE, ... ) {
>
>    cat(..1, "\n")
>    cat(..2, "\n")
>    cat(..3, "\n")
>
>    out=1
>    out
> }
>
> out = sens.analysis.all(numer.wait.times.wrps, NA,
>      c(5.4, 0.008, 1.5), c(4.9, 0.0079, 2.0), c(5.5, 0.0090, 3.5),
>        length.pgrid=10, outvname="cvs", zlim=c(0.3, 2),
>        plabs=c("mu", "lambda", "b"), gridlab="Soil Moisture [mm]",
>        mylog=FALSE, outlog=FALSE, yline=2, plot.guidance=FALSE, q=3.1, k=4, y.c=670,
>        realgrid=seq(630, 670, by=4), myseed=0,
>        nt=1000000, burnin=300000, bin.cutoff=500,
>        bdW.prelim=prec.struct.Mal)
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Fri May  8 19:34:13 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Fri, 8 May 2020 13:34:13 -0400
Subject: [R] Question about package "SentimentAnalysis"
In-Reply-To: <CAGN=ytNHHmuP3Jn_6UAaaN9c3HQCDuEMG6JYRc4OeKjs+4_8Uw@mail.gmail.com>
References: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>
 <CAGN=ytNHHmuP3Jn_6UAaaN9c3HQCDuEMG6JYRc4OeKjs+4_8Uw@mail.gmail.com>
Message-ID: <CAKZQJMCo8vZM8E514_zmXdZ++sR7dXWK7oC+Nd2OWb1Z2iuQCw@mail.gmail.com>

I think your best bet is to ask the author/maintainer, Stefan Feuerriegel
,about this. The reference manual
https://cran.r-project.org/web/packages/SentimentAnalysis/SentimentAnalysis.pdf
gives his email address as <sentiment at sfeuerriegel.com>

On Fri, 8 May 2020 at 08:32, Mehdi Dadkhah <mehdidadkhah91 at gmail.com> wrote:

> Hi,
> I hope you are doing well!
> I read a vignette (
>
> https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html
> )
> about interested package, "SentimentAnalysis". But i faced with a question.
> In mentioned  vignette, the sentiment has been applied on a sentence or
> multiple sentences separately. Can this package calculate sentiment
> direction/score for a long texts?
> for example:
>
> # Create a vector of strings
> documents <- "Wow, I really like the new light sabers!That book was
> excellent.R is a fantastic language.The service in this restaurant was
> miserable.This is neither positive or negative."
>
> # Analyze sentiment
> sentiment <- analyzeSentiment(documents)
>
> Many thanks!
> With best regards,
>
> --
> *Mehdi Dadkhah*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From g@v@n@mcgr@th @end|ng |rom dbc@@w@@gov@@u  Fri May  8 17:08:05 2020
From: g@v@n@mcgr@th @end|ng |rom dbc@@w@@gov@@u (Gavan McGrath)
Date: Fri, 8 May 2020 15:08:05 +0000
Subject: [R] Rtools virus
In-Reply-To: <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>
References: <ME2PR01MB3714417631C9A5EC7B3C434C9BA40@ME2PR01MB3714.ausprd01.prod.outlook.com>,
 <CF2FB038-0258-4698-B10B-0435C592BF5A@dcn.davis.ca.us>
Message-ID: <ME2PR01MB371426B0F3130404AE66052D9BA20@ME2PR01MB3714.ausprd01.prod.outlook.com>

Thanks Jeff,
It was downloaded from https://cran.r-project.org/bin/windows/Rtools/

Kind Regards,
Gavan

Dr Gavan McGrath, PhD, B.E.

Research Scientist
Biodiversity and Conservation Science
Department of Biodiversity, Conservation and Attractions
Street Address: 17 Dick Perry Avenue, Kensington,
Postal Address Locked Bag 104, Bentley Delivery Centre, WA 6983
Phone: +618 9219 9447 Mobile: +61 458 559 765
Email: gavan.mcgrath at dbca.wa.gov.au

________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Friday, May 8, 2020 10:58:34 PM
To: r-help at r-project.org <r-help at r-project.org>; Gavan McGrath <gavan.mcgrath at dbca.wa.gov.au>; r-help at R-project.org <r-help at R-project.org>
Subject: Re: [R] Rtools virus

[External Email] This email was sent from outside the department ? be cautious, particularly with links and attachments.

Sorry to hear that. It is most likely a false positive (antivirus software has little incentive to minimise false positives), but no one here can follow up on your report because you did not say precisely which website you downloaded it from.

On May 5, 2020 8:50:12 PM PDT, Gavan McGrath <gavan.mcgrath at dbca.wa.gov.au> wrote:
>Hi,
>My IT department instructed me to uninstall Windows 64-bit:
>rtools40-x86_64.exe as it contained a virus which they identified at
>
>https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.virustotal.com%2Fgui%2Ffile%2F5c10d60e73dd0186e8f886ef0b9388bb7dbdfdc17366c14c16183edb08fdb58a%2Fdetection&amp;data=02%7C01%7Cgavan.mcgrath%40dbca.wa.gov.au%7Cd11b1d11755f4072370108d7f3604a58%7C7b934664cdcf4e28a3ee1a5bcca0a1b6%7C1%7C0%7C637245467257277714&amp;sdata=KQOX0z8gqcwhZaR9CUNpDMN1WCMTOFeqzafAoiQEYIw%3D&amp;reserved=0
>
>Kind Regards,
>
>Dr Gavan McGrath, PhD, B.E.
>
>Research Scientist
>Biodiversity and Conservation Science
>Department of Biodiversity, Conservation and Attractions
>Street Address: 17 Dick Perry Avenue, Kensington, WA 6151, Australia
>Postal Address Locked Bag 104, Bentley Delivery Centre, WA 6983,
>Australia
>Phone: +618 9219 9447 Mobile: +61 458 559 765
>Email: gavan.mcgrath at dbca.wa.gov.au
>
>Adjunct Research Fellow
>School of Agriculture and Environment
>The University of Western Australia
>Perth, Western Australia
>Email: gavan.mcgrath at uwa.edu.au
>
>________________________________
>This message is confidential and is intended for the recipient named
>above. If you are not the intended recipient, you must not disclose,
>use or copy the message or any part of it. If you received this message
>in error, please notify the sender immediately by replying to this
>message, then delete it from your system.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://aus01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7Cgavan.mcgrath%40dbca.wa.gov.au%7Cd11b1d11755f4072370108d7f3604a58%7C7b934664cdcf4e28a3ee1a5bcca0a1b6%7C1%7C0%7C637245467257287711&amp;sdata=GDLcw1UMHpiVcfAWoqdNaloY1phY%2FI0mMPaiX8TaJr8%3D&amp;reserved=0
>PLEASE do read the posting guide
>https://aus01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7Cgavan.mcgrath%40dbca.wa.gov.au%7Cd11b1d11755f4072370108d7f3604a58%7C7b934664cdcf4e28a3ee1a5bcca0a1b6%7C1%7C0%7C637245467257287711&amp;sdata=isvQYq8ImiPnxS0OoMae5eGCzx6M9EXs85iVpEQ52uk%3D&amp;reserved=0
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.
________________________________
This message is confidential and is intended for the recipient named above. If you are not the intended recipient, you must not disclose, use or copy the message or any part of it. If you received this message in error, please notify the sender immediately by replying to this message, then delete it from your system.

	[[alternative HTML version deleted]]


From m@rc_@chw@rtz @end|ng |rom me@com  Fri May  8 17:11:15 2020
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Fri, 8 May 2020 11:11:15 -0400
Subject: [R] Function Hints in Mac Dark Mode
In-Reply-To: <32244604-925F-4F40-A200-3F1AA6D95D36@dcn.davis.ca.us>
References: <1E9E4AA6-D87A-46C2-A585-459645F6EF6D@unomaha.edu>
 <32244604-925F-4F40-A200-3F1AA6D95D36@dcn.davis.ca.us>
Message-ID: <769FC136-30D0-4151-98AB-E29C22A4D6F7@me.com>



> On May 8, 2020, at 11:02 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> You seem to be confusing R and RStudio... so yeah, wrong mailing list. I don't know exactly where you should post either. Perhaps the GitHub issues page for RStudio?
> 
> On May 6, 2020 12:54:43 PM PDT, Andrew Swift via R-help <r-help at r-project.org> wrote:
>> Sorry, wasn?t sure exactly where to post this but I noticed that with R
>> 4.0.0 when running a Mac in Dark Mode that the Function Hints at the
>> bottom of the Console and Editor windows become invisible.  
>> Thanks. 


Hi,

Actually, I am not sure that is the case. 

See the attached screen capture of the default R.app (which I do not use) when the desktop is set to dark mode, which I also do not use.

When in light mode, those hints in the lower left hand corner are black, and they switch to white in dark mode, making them almost impossible to see.

For Andrew, if this is correct, and you are not referring to RStudio per Jeff, you should post this to r-sig-mac:
  
  https://stat.ethz.ch/mailman/listinfo/r-sig-mac

Otherwise, if it is RStudio, they have their own support here:

   https://support.rstudio.com/hc/en-us

Regards,

Marc Schwartz





From r@turner @end|ng |rom @uck|@nd@@c@nz  Sat May  9 00:11:42 2020
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sat, 9 May 2020 10:11:42 +1200
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
Message-ID: <3e3568fa-3b32-0d33-0418-12026bce7628@auckland.ac.nz>


Hi Rui.  Doesn't happen to me under Ubuntu 18.04:

> install.packages('cowplot',lib=.Rlib) 
> trying URL 'https://cloud.r-project.org/src/contrib
>/cowplot_1.0.0.tar.gz'
> Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
> ==================================================
> downloaded 1.2 MB
> 
> * installing *source* package ?cowplot? ...
> ** package ?cowplot? successfully unpacked and MD5 sums checked
> ** using staged installation
> ** R
> ** inst
> ** byte-compile and prepare package for lazy loading
> ** help
> *** installing help indices
> *** copying figures
> ** building package indices
> ** installing vignettes
> ** testing if installed package can be loaded from temporary location
> ** testing if installed package can be loaded from final location
> ** testing if installed package keeps a record of temporary installation path
> * DONE (cowplot)
> 
> The downloaded source packages are in
> 	?/tmp/RtmpG8wb97/downloaded_packages?

My session info is as follows:

> sessionInfo()
> R version 4.0.0 (2020-04-24)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 18.04.4 LTS
> 
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
> LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3
> 
> Random number generation:
>  RNG:     Mersenne-Twister 
>  Normal:  Inversion 
>  Sample:  Rounding 
>  
> locale:
>  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
>  [3] LC_TIME=en_NZ.UTF-8        LC_COLLATE=en_GB.UTF-8    
>  [5] LC_MONETARY=en_NZ.UTF-8    LC_MESSAGES=en_GB.UTF-8   
>  [7] LC_PAPER=en_NZ.UTF-8       LC_NAME=C                 
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
> [11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C       
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base     
> 
> other attached packages:
> [1] brev_0.0-4
> 
> loaded via a namespace (and not attached):
> [1] compiler_4.0.0 tools_4.0.0  

No idea what to suggest.  Sorry.

cheers,

Rolf

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jrkr|de@u @end|ng |rom gm@||@com  Fri May  8 19:35:57 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Fri, 8 May 2020 13:35:57 -0400
Subject: [R] Need a suggestion on a package to make a figure
In-Reply-To: <216384015.2752852.1588857789484@mail.yahoo.com>
References: <216384015.2752852.1588857789484.ref@mail.yahoo.com>
 <216384015.2752852.1588857789484@mail.yahoo.com>
Message-ID: <CAKZQJMA0AKKWx4NEDcik66EmM9LzSZVKWdzoeVugdPFsVD7YWw-4623@mail.gmail.com>

What are you doing?

 http://adv-r.had.co.nz/Reproducibility.html

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

On Fri, 8 May 2020 at 10:50, aiguo li via R-help <r-help at r-project.org>
wrote:

> Hello all,
> I need to make a table with a value imaged by greater than certain value
> as attached.  Could you give me a suggestions on which R package will be
> good for this type of table?
> Thanks and stay safe!
> Aiguo______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


From rom@no|@on @end|ng |rom yon@e|@@c@kr  Sat May  9 04:38:17 2020
From: rom@no|@on @end|ng |rom yon@e|@@c@kr (Roman Olson)
Date: Sat, 9 May 2020 11:38:17 +0900
Subject: [R] Bug in function arguments autocomplete and ellipsis?
In-Reply-To: <CAGxFJbT3vbOoe-OHWwDGAC9fcuyE8C0tQVqKp9kg6uv+Nd0vOg@mail.gmail.com>
References: <3E8A851C-9211-4CDE-B9B6-35DAF25828C9@yonsei.ac.kr>
 <CAGxFJbT3vbOoe-OHWwDGAC9fcuyE8C0tQVqKp9kg6uv+Nd0vOg@mail.gmail.com>
Message-ID: <9095577E-093D-489C-9C8D-56F5D5588260@yonsei.ac.kr>

Dear All,

Thanks for clarifying this. Now I know that this is expected behavior, and will try to place ? before the rest of the arguments.

Stay home and stay safe!
-Roman

> 2020. 5. 9. ?? 1:24, Bert Gunter <bgunter.4567 at gmail.com> ??:
> 
> The R
> Language Definition


	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sat May  9 05:28:13 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 9 May 2020 13:28:13 +1000
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CAOFE=kO6Pz=vqOR6+mBvjkMMBMhv9bX9s9nUCRnBoF9dGkdiVA@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
 <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
 <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>
 <CAOFE=kO6Pz=vqOR6+mBvjkMMBMhv9bX9s9nUCRnBoF9dGkdiVA@mail.gmail.com>
Message-ID: <CA+8X3fVgYkZ00gqqVhdaE7Nn=VvQ_x=eoBdjqBKsmALsTGfB5A@mail.gmail.com>

Hi Subhamitra,
I have washed the dishes and had a night's sleep, so I can now deal with
your text munging problem. First, I'll reiterate the solution I sent:

sp_8_5<-read.table("sp_8_5.tab",sep="\t",
 header=TRUE,stringsAsFactors=FALSE)
library(tseries)
library(FinTS)
# create a function that returns only the
# statistic and p.value as a string
archStatP<-function(x) {
 archout<-ArchTest(x)
 # I have truncated the values here
 return(sprintf("ChiSq = %.1f, p = %.3f",archout$statistic,archout$p.value))
}
# using "lapply", run the test on each column
spout<-lapply(sp_8_5[,2:13],archStatP)

If you look at "spout" you will see that it is a list of 12 character
strings. I arranged this as you seem to want the contents of a 3x4 table in
a document. This is one way to do it, there are others.

First, create a text table of the desired dimensions. I'll do it with loops
as you seem to be familiar with them:

# open a text file
sink("sp_8_5.txt")
for(row in 0:2) {
 for(column in 1:4)
  cat(spout[[column+row*4]],ifelse(column < 4,"\t","\n"))
}
sink()

If you open this file in a text editor (e.g. Notepad) you will see that it
contains 3 lines (rows), each with four TAB separated strings. Now to
import this into a word processing document. I don't have MS Word, so I'll
do it with Libre Office Writer and hope that the procedure is similar.

Move to where you want the table in your document
Select Insert|Text from file from the top menu
Select (highlight) the text you have imported
Select Convert|Text to table from the top menu

The highlighted area should become a table. I had to reduce the font size
from 12 to 10 to get the strings to fit into the cells.

There are probably a few more changes that you will want, so let me know if
you strike trouble.

Jim


On Fri, May 8, 2020 at 11:28 PM Subhamitra Patra <subhamitra.patra at gmail.com>
wrote:

> Dear Sir,
>
> Thank you very much for your wonderful suggestion for my problem. Your
> suggested code has excellently worked and successfully extracted the
> statistics and p-value in another R object.
>
> Concerning your last suggestion, I attempted to separate the strings with
> TAB character in the "spout" object by using different alternative packages
> like dplyr, tidyr, qdap, ans also by using split,strsplit function so that
> can export the statistics and p-values for each column to excel, and later
> to the MSword file, but got the below error.
>
> By using the  split function, I wrote the code as,
> *string[] split = s.Split(spout, '\t')*
> where I got the following errors.
> Error: unexpected symbol in "string[] split"
> Error: unexpected symbol in "string[[]]split"
> Error in strsplit(row, "\t") : non-character argument
>
> Then I tried with  strsplit function by the below code
> *strsplit(spout, split)*
> But, got the below error as
> Error in as.character(split) :
>   cannot coerce type 'closure' to vector of type 'character'.
>
> Then used dplyr and tidyr package and the wrote the below code
> library(dplyr)
> library(tidyr)
> *separate(spout,value,into=c(?ChiSq?,?p?),sep=?,?)*
> *separate(spout,List of length 12,into=c(?ChiSq?,?p?),sep="\t")*
> But, got the errors as,
> Error: unexpected input in "separate(spout,value,into=c(?"
> Error: unexpected symbol in "separate(spout,List of"
>
> Then used qdap package with the code below
>
> *colsplit2df(spout,, c("ChiSq", "p"), ",")*
> *colsplit2df(spout,, c("ChiSq", "p"), sep = "\t")*
> But got the following errors
> Error in dataframe[, splitcol] : incorrect number of dimensions
> In addition: Warning message:
> In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
>   dataframe object is not of the class data.frame
> Error in dataframe[, splitcol] : incorrect number of dimensions
> In addition: Warning message:
> In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
>   dataframe object is not of the class data.frame
>
> Sir, please suggest me where I am going wrong in the above to separate
> string in the "spout" object.
>
> Thank you very much for your help.
>
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
> 06:51:46 PM
>
> On Fri, May 8, 2020 at 4:47 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> 1) In general, *apply functions return a list with the number of elements
>> equal to the number of columns or other elements of the input data. You can
>> assign that list as I have to "spout" in the first example.
>>
>> 2) spout<-list() assigns the name "spout" to an empty list. As we are
>> processing columns 2 to 12 of the input data, spout[[i-1]] assigns the
>> results to elements 1 to 11 of the list "spout". Just a low trick.
>>
>> 1a) Yes, you can create a "wrapper" function that will return only the
>> statistic and p.value.
>>
>> # create a function that returns only the
>> # statistic and p.value as a string
>> archStatP<-function(x) {
>>  archout<-ArchTest(x)
>>  return(sprintf("ChiSq = %f, p = %f",archout$statistic,archout$p.value))
>> }
>> # using "lapply", run the test on each column
>> spout<-lapply(sp_8_5[,2:12],archStatP)
>>
>> Note that I should have used "lapply". I didn't check the output
>> carefully enough.
>>
>> 2a) Now you only have to separate the strings in "spout" with TAB
>> characters and import the result into Excel. I have to wash the dishes, so
>> you're on your own.
>>
>> Jim
>>
>> On Fri, May 8, 2020 at 8:26 PM Subhamitra Patra <
>> subhamitra.patra at gmail.com> wrote:
>>
>>> Dear Sir,
>>>
>>> Thank you very much for such an excellent solution to my problem. I was
>>> trying sapply function since last days, but was really unable to write
>>> properly. Now, I understood my mistake in using sapply function in the
>>> code. Therefore, I have two queries regarding this which I want to discuss
>>> here just for my learning purpose.
>>>
>>> 1. While using sapply function for estimating one method across the
>>> columns of a data frame, one needs to define the list of the output table
>>> after using sapply so that the test results for each column will be
>>> consistently stored in an output object, right?
>>>
>>> 2. In the spout<- list() command, what spout[[i-1]]  indicates?
>>>
>>> Sir, one more possibility which I would like to ask related to my above
>>> problem just to learn for further R programming language.
>>>
>>> After running your suggested code, all the results for each column are
>>> being stored in the spout object. From this, I need only the statistics and
>>> P-value for each column. So, my queries are:
>>>
>>> 1. Is there any way to extract only two values (i.e., statistics and
>>> p-value) for each column that stored in spout object and save these two
>>> values in another R data frame for each column?
>>>  or
>>> 2. Is there any possibility that the statistics and p-value
>>> calculated for each column can directly export to a word file in a table
>>> format (having 4 columns and 3 rows). In particular, is it possible to
>>> extract both statistic and p-value results for each column to an MS word
>>> file with the format of A1, A2, A3, A4 column results in 1st row, A5, A6,
>>> A7, A8 column results in 2nd row, and A9, A10, A11, A12 column results in
>>> the 3rd row of the table?
>>>
>>>
>>> Like before, your suggestion will definitely help me to learn the
>>> advanced R language.
>>>
>>> Thank you very much for your help.
>>>
>>> [image: Mailtrack]
>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>> notified by
>>> Mailtrack
>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
>>> 03:47:26 PM
>>>
>>> On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>>
>>>> Hi Subhamitra,
>>>> This isn't too hard:
>>>>
>>>> # read in the sample data that was
>>>> # saved in the file "sp_8_5.tab"
>>>> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>>>>  header=TRUE,stringsAsFactors=FALSE)
>>>> library(tseries)
>>>> library(FinTS)
>>>> # using "sapply", run the test on each column
>>>> spout<-sapply(sp_8_5[,2:12],ArchTest)
>>>>
>>>> The list "spout" contains the test results. If you really want to use a
>>>> loop:
>>>>
>>>> spout<-list()
>>>> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>>>>
>>>> Jim
>>>>
>>>>
>>>> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
>>>> subhamitra.patra at gmail.com> wrote:
>>>>
>>>>> Dear Sir,
>>>>>
>>>>> Herewith I am pasting a part of my sample data having 12 columns
>>>>> below, and want to calculate ARCH test for the 12 columns by using a loop.
>>>>>
>>>>>
>>>
>>> --
>>> *Best Regards,*
>>> *Subhamitra Patra*
>>> *Phd. Research Scholar*
>>> *Department of Humanities and Social Sciences*
>>> *Indian Institute of Technology, Kharagpur*
>>> *INDIA*
>>>
>>
>
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
>

	[[alternative HTML version deleted]]


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Sat May  9 05:33:09 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Sat, 9 May 2020 08:03:09 +0430
Subject: [R] Question about package "SentimentAnalysis"
In-Reply-To: <CAKZQJMCo8vZM8E514_zmXdZ++sR7dXWK7oC+Nd2OWb1Z2iuQCw@mail.gmail.com>
References: <CAGN=ytPU_WAtzC6XDO_MMePRrtEn1TL+N-kmA+E_76KkNza1kA@mail.gmail.com>
 <CAGN=ytNHHmuP3Jn_6UAaaN9c3HQCDuEMG6JYRc4OeKjs+4_8Uw@mail.gmail.com>
 <CAKZQJMCo8vZM8E514_zmXdZ++sR7dXWK7oC+Nd2OWb1Z2iuQCw@mail.gmail.com>
Message-ID: <CAGN=ytOWE5S1joNu1yOpjep=EU7C1rmhpL1uQSwPKRW4gugo4g@mail.gmail.com>

Thank you!

On Fri, May 8, 2020 at 10:04 PM John Kane <jrkrideau at gmail.com> wrote:

> I think your best bet is to ask the author/maintainer, Stefan Feuerriegel
> ,about this. The reference manual
> https://cran.r-project.org/web/packages/SentimentAnalysis/SentimentAnalysis.pdf
> gives his email address as <sentiment at sfeuerriegel.com>
>
> On Fri, 8 May 2020 at 08:32, Mehdi Dadkhah <mehdidadkhah91 at gmail.com>
> wrote:
>
>> Hi,
>> I hope you are doing well!
>> I read a vignette (
>>
>> https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html
>> )
>> about interested package, "SentimentAnalysis". But i faced with a
>> question.
>> In mentioned  vignette, the sentiment has been applied on a sentence or
>> multiple sentences separately. Can this package calculate sentiment
>> direction/score for a long texts?
>> for example:
>>
>> # Create a vector of strings
>> documents <- "Wow, I really like the new light sabers!That book was
>> excellent.R is a fantastic language.The service in this restaurant was
>> miserable.This is neither positive or negative."
>>
>> # Analyze sentiment
>> sentiment <- analyzeSentiment(documents)
>>
>> Many thanks!
>> With best regards,
>>
>> --
>> *Mehdi Dadkhah*
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> --
> John Kane
> Kingston ON Canada
>


-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Sat May  9 09:20:32 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Sat, 9 May 2020 11:50:32 +0430
Subject: [R] Question about package "SentimentAnalysis"
In-Reply-To: <5d06afeb-9a0f-4536-81c0-8dc1ee6b8c11@email.android.com>
References: <CAGN=ytOWE5S1joNu1yOpjep=EU7C1rmhpL1uQSwPKRW4gugo4g@mail.gmail.com>
 <5d06afeb-9a0f-4536-81c0-8dc1ee6b8c11@email.android.com>
Message-ID: <CAGN=ytOw_BoVX9PH=3niT7n+BmkX7MUFY01kJz8C+r-2MUDo9w@mail.gmail.com>

Thank you!

On Sat, May 9, 2020 at 11:44 AM <cpolwart at chemo.org.uk> wrote:

> Or, to split the paragraph into sentences, analyse each sentence and
> decide how to agregate the result...
>
>
>
> On 9 May 2020 04:33, Mehdi Dadkhah <mehdidadkhah91 at gmail.com> wrote:
>
> Thank you!
>
> On Fri, May 8, 2020 at 10:04 PM John Kane <jrkrideau at gmail.com> wrote:
>
> > I think your best bet is to ask the author/maintainer, Stefan
> Feuerriegel
> > ,about this. The reference manual
> >
> https://cran.r-project.org/web/packages/SentimentAnalysis/SentimentAnalysis.pdf
> > gives his email address as <sentiment at sfeuerriegel.com>
> >
> > On Fri, 8 May 2020 at 08:32, Mehdi Dadkhah <mehdidadkhah91 at gmail.com>
> > wrote:
> >
> >> Hi,
> >> I hope you are doing well!
> >> I read a vignette (
> >>
> >>
> https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html
> >> )
> >> about interested package, "SentimentAnalysis". But i faced with a
> >> question.
> >> In mentioned  vignette, the sentiment has been applied on a sentence or
> >> multiple sentences separately. Can this package calculate sentiment
> >> direction/score for a long texts?
> >> for example:
> >>
> >> # Create a vector of strings
> >> documents <- "Wow, I really like the new light sabers!That book was
> >> excellent.R is a fantastic language.The service in this restaurant was
> >> miserable.This is neither positive or negative."
> >>
> >> # Analyze sentiment
> >> sentiment <- analyzeSentiment(documents)
> >>
> >> Many thanks!
> >> With best regards,
> >>
> >> --
> >> *Mehdi Dadkhah*
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
> > --
> > John Kane
> > Kingston ON Canada
> >
>
>
> --
> *Mehdi Dadkhah*
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>

-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sat May  9 05:08:10 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 8 May 2020 20:08:10 -0700
Subject: [R] [R-pkgs] addScales package available on CRAN
Message-ID: <CAGxFJbSMF=+Xk6L0jBFRxEELJ1cLqUhmJJJJxM_sy5XnwxbcBg@mail.gmail.com>

addScales is a small package that modifies trellis objects created
using lattice graphics by adding horizontal and/or vertical reference
lines that provide visual scaling information. This is mostly useful
for multi-panel plots that use the relation = 'free' option in their
'scales' argument list. Examples show when and how this might aid data
visualization.

Please feel free to contact me with suggestions for improvement.

Bert Gunter

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n|  Fri May  8 20:30:12 2020
From: wo||g@ng@v|echtb@uer @end|ng |rom m@@@tr|chtun|ver@|ty@n| (Viechtbauer, Wolfgang (SP))
Date: Fri, 8 May 2020 18:30:12 +0000
Subject: [R] [R-pkgs] mathjaxr: Using 'Mathjax' in Rd Files
Message-ID: <986895facff141379e5d379a666e5bcb@UM-MAIL3214.unimaas.nl>

Dear All,

I would like to announce the release of the 'mathjaxr' package: 

https://cran.r-project.org/package=mathjaxr

The package provides MathJax and a handful of macros to enable its use within Rd files for rendering equations in the HTML help files.

Package authors wanting to make use of the package and its functionality need to:

1. install the mathjaxr package,
2. add mathjaxr to 'Suggests' or 'Imports' in the DESCRIPTION file of their package,
3. add mathjaxr to 'RdMacros' in the DESCRIPTION file of their package (or add 'RdMacros: mathjaxr' if the DESCRIPTION file does not yet contain a 'RdMacros' entry),

One can then enable the use of MathJax by calling the \loadmathjax macro within the \description{} section of an .Rd file.

An inline equation can then be added with the \mjeqn{latex}{ascii} macro, with the LaTeX commands for the equation given between the first set of curly brackets (which will be rendered in the HTML and PDF help pages) and the plain-text version of the equation given between the second set of curly brackets (which will be shown in the plain text help). With the \mjdeqn{latex}{ascii} macro, one can add 'displayed equations' (as in LaTeX's displaymath environment). Single argument versions of these macros, namely \mjseqn{latexascii} and \mjsdeqn{latexascii}, are also available.

I hope package authors will find this useful for providing fully rendered equations not only in the PDF manual, but also in the HTML help files (which the vast majority of users are probably looking at).

Feedback, comments, suggestions more than welcome. For bug reports, please go to:

https://github.com/wviechtb/mathjaxr/issues

Best,
Wolfgang

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Sat May  9 11:32:25 2020
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Sat, 9 May 2020 09:32:25 +0000
Subject: [R] Change the colours of some of the labels of the x-axis
In-Reply-To: <dff646d6-dc28-5258-b300-275c82a9dfca@sapo.pt>
References: <8B435C9568170B469AE31E8891E8CC4F809B8064@ESINO.regionemarche.intra>,
 <dff646d6-dc28-5258-b300-275c82a9dfca@sapo.pt>
Message-ID: <8B435C9568170B469AE31E8891E8CC4F809B83CF@ESINO.regionemarche.intra>

Thank you very much.

Stefano

         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

________________________________________
Da: Rui Barradas [ruipbarradas at sapo.pt]
Inviato: gioved? 7 maggio 2020 17.41
A: Stefano Sofia; r-help at r-project.org
Oggetto: Re: [R] Change the colours of some of the labels of the x-axis

Hello,

You cannot pass a vector of colors to col.axis, you need to plot the
axis twice, once the normal axis, like in your code, then overplot just
those last 2 labels.


xlabels <- seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"),
"days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"),
by="12 hours")

plot(df_plot$data_POSIX, df_plot$hs1, type="b",
      xlim = c(min(xlabels), max(xlabels)), ylim=c(0, 10), col="blue",
xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
axis.POSIXct(1, at = xlabels, format="h%H-%d-%m-%y", pos=0, las = 2)
axis.POSIXct(1, at = tail(xlabels, 2),
              format="h%H-%d-%m-%y", pos=0, las = 2, col.axis = "red")

axis(side=2, at=seq(0, 15, 5))



Hope this helps,

Rui Barradas

?s 15:09 de 07/05/20, Stefano Sofia escreveu:
> Dear R users,
> in a plot I need to change the colours of some of the labels of the x-axis.
> In the example below reported, I would like to have the labels of the last and second-last date in red.
> I tried to find the solution searching the web, but I could not understand the hints and I was not sure I could adapt them to my example.
> There might be an easy way to do that?
>
> Thank you for your attention and your help
> Stefano
>
> first_day <- "2005-01-23-09-00"
> last_day <- "2005-01-27-09-00"
> first_day_POSIX <- as.POSIXct(first_day, format="%Y-%m-%d-%H-%M")
> last_day_POSIX <- as.POSIXct(last_day, format="%Y-%m-%d-%H-%M")
> df_plot <- data.frame(data_POSIX=seq(first_day_POSIX, last_day_POSIX, by="30 mins"))
> df_plot$hs1 <- 5
> df_plot$hs2 <- 7
>
> plot(df_plot$data_POSIX, df_plot$hs1, type="b", ylim=c(0, 10), col="blue", xaxt="n", axes=F, main="", xlab="", ylab="Snow height")
> lines(df_plot$data_POSIX, df_plot$hs2, ylim=c(0, 10), col="red", type="b")
> axis.POSIXct(1, at=seq(trunc(as.POSIXct(first_day, format="%Y-%m-%d-%H-%M"), "days"), trunc(as.POSIXct(last_day, format="%Y-%m-%d-%H-%M"), "days"), by="12 hours"), format="h%H-%d-%m-%y", pos=0)
> axis(side=2, at=seq(0, 15, 5))
>
>
>           (oo)
> --oOO--( )--OOo----------------
> Stefano Sofia PhD
> Civil Protection - Marche Region
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona
> Uff: 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
>
> --
> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> This message was scanned by Libra ESVA and is believed to be clean.
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

--

Questo messaggio  stato analizzato con Libra ESVA ed  risultato non infetto.


________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Sat May  9 12:06:30 2020
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Sat, 9 May 2020 14:36:30 +0430
Subject: [R] genericSummary in LSAfun
In-Reply-To: <CAGN=ytM4+Cn_egGauCt+hMZ4jX0OVQNacaLtgvHzmpfPy5WFbw@mail.gmail.com>
References: <CAGN=ytM4+Cn_egGauCt+hMZ4jX0OVQNacaLtgvHzmpfPy5WFbw@mail.gmail.com>
Message-ID: <CAGN=ytM6=ntjuRX_q7pbaYFzb5HbiV8VwR5i6JoZB_FRiX9_7w@mail.gmail.com>

Hi,
I hope you are doing well!
I have a data frame with a column. it contains about 140 posts. In each
row, there is a blog post. I named this data frame as "posttext". When i
use  genericSummary() function, it returns paragraph instead sentences.
What is problem?


Command which i use:

summaryp<-genericSummary(posttext,k=1,split=c(".","!","?"),min=5,breakdown=FALSE)



Many thanks!
With best regards,

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Sat May  9 13:24:40 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 9 May 2020 21:24:40 +1000
Subject: [R] Need a suggestion on a package to make a figure
In-Reply-To: <216384015.2752852.1588857789484@mail.yahoo.com>
References: <216384015.2752852.1588857789484.ref@mail.yahoo.com>
 <216384015.2752852.1588857789484@mail.yahoo.com>
Message-ID: <CA+8X3fUBHezzeg4Rict5EA-WD6Vr+=64rH6W2DAvNzzKjdjL6Q@mail.gmail.com>

Hi Aiguo
Just for fun, I tried to work out what you wanted. I think you are
looking for a function that creates a horizontal stacked bar image of
a logical vector:

TFbar<-function(x,file="TFbar",grDev="png",width=100,height=25,
 col=c("green","red")) {

 do.call(grDev,
  list(file=paste(file,grDev,sep="."),width=width,height=height))
 par(xaxs="i",mar=c(0,0,0,0))
 plot(0,xlim=c(1,width),ylim=c(1,height),type="n",axes=FALSE)
 Twidth<-width*sum(x)/length(x)
 Fwidth<-width-Twidth
 rect(1,1,Twidth,height,col=col[1])
 rect(Twidth,1,width,height,col=col[2])
 dev.off()
 return(Twidth/width)
}

To instantiate this with your example:

TFnames<-paste0("Pathway",1:4)
for(i in 1:4) {
 assign("x",sample(c(TRUE,FALSE),100,TRUE))
 TFbar(x,TFnames[i])
}
TFtags<-paste0("<img src=",TFnames,".png>")
TFdf<-data.frame(GO_Terms=TFnames,S1=TFtags,stringsAsFactors=FALSE)
library(prettyR)
delim.table(TFdf,filename="TFtable.htm",leading.delim=FALSE,
 label="",html=TRUE,show.rownames=FALSE)

Sadly, I found a bug in delim.table that will be corrected in the next
version. The attached HTML file shows the table you get with the
corrected version.

Jim

On Sat, May 9, 2020 at 12:50 AM aiguo li via R-help
<r-help at r-project.org> wrote:
>
> Hello all,
> I need to make a table with a value imaged by greater than certain value as attached.  Could you give me a suggestions on which R package will be good for this type of table?
> Thanks and stay safe!
> Aiguo______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From drj|m|emon @end|ng |rom gm@||@com  Sat May  9 13:26:28 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 9 May 2020 21:26:28 +1000
Subject: [R] Need a suggestion on a package to make a figure
In-Reply-To: <CA+8X3fUBHezzeg4Rict5EA-WD6Vr+=64rH6W2DAvNzzKjdjL6Q@mail.gmail.com>
References: <216384015.2752852.1588857789484.ref@mail.yahoo.com>
 <216384015.2752852.1588857789484@mail.yahoo.com>
 <CA+8X3fUBHezzeg4Rict5EA-WD6Vr+=64rH6W2DAvNzzKjdjL6Q@mail.gmail.com>
Message-ID: <CA+8X3fXTJFwUMMvC_sxbwAmv_n+_=3zKUag-LcHeCoZipNgswg@mail.gmail.com>

Hi again,
Oops, you'll need the images as well. Save the HTML file and images to
the same directory.

Jim

On Sat, May 9, 2020 at 9:24 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Aiguo
> Just for fun, I tried to work out what you wanted. I think you are
> looking for a function that creates a horizontal stacked bar image of
> a logical vector:
>
> TFbar<-function(x,file="TFbar",grDev="png",width=100,height=25,
>  col=c("green","red")) {
>
>  do.call(grDev,
>   list(file=paste(file,grDev,sep="."),width=width,height=height))
>  par(xaxs="i",mar=c(0,0,0,0))
>  plot(0,xlim=c(1,width),ylim=c(1,height),type="n",axes=FALSE)
>  Twidth<-width*sum(x)/length(x)
>  Fwidth<-width-Twidth
>  rect(1,1,Twidth,height,col=col[1])
>  rect(Twidth,1,width,height,col=col[2])
>  dev.off()
>  return(Twidth/width)
> }
>
> To instantiate this with your example:
>
> TFnames<-paste0("Pathway",1:4)
> for(i in 1:4) {
>  assign("x",sample(c(TRUE,FALSE),100,TRUE))
>  TFbar(x,TFnames[i])
> }
> TFtags<-paste0("<img src=",TFnames,".png>")
> TFdf<-data.frame(GO_Terms=TFnames,S1=TFtags,stringsAsFactors=FALSE)
> library(prettyR)
> delim.table(TFdf,filename="TFtable.htm",leading.delim=FALSE,
>  label="",html=TRUE,show.rownames=FALSE)
>
> Sadly, I found a bug in delim.table that will be corrected in the next
> version. The attached HTML file shows the table you get with the
> corrected version.
>
> Jim
>
> On Sat, May 9, 2020 at 12:50 AM aiguo li via R-help
> <r-help at r-project.org> wrote:
> >
> > Hello all,
> > I need to make a table with a value imaged by greater than certain value as attached.  Could you give me a suggestions on which R package will be good for this type of table?
> > Thanks and stay safe!
> > Aiguo______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Pathway1.png
Type: image/png
Size: 169 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/3837b0d1/attachment.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Pathway2.png
Type: image/png
Size: 167 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/3837b0d1/attachment-0001.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Pathway3.png
Type: image/png
Size: 168 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/3837b0d1/attachment-0002.png>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Pathway4.png
Type: image/png
Size: 170 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/3837b0d1/attachment-0003.png>

From |8|5h9 @end|ng |rom gm@||@com  Sat May  9 12:06:30 2020
From: |8|5h9 @end|ng |rom gm@||@com (=?utf-8?Q?Fernando_A_L=C3=B3pez_Hern=C3=A1ndez?=)
Date: Sat, 9 May 2020 12:06:30 +0200
Subject: [R] [R-pkgs] New version 'spsur' for the estimation of Spatial
 Seemingly Unrelated Regression
Message-ID: <10919E94-320E-43E7-9C78-1032E95915EA@gmail.com>

Dear all
I am pleased to announce that the R package ?spsur? for the estimation of Spatial Seemingly Unrelated Regression upload a new version in the CRAN. 

https://cran.r-project.org/web/packages/spsur/index.html <https://cran.r-project.org/web/packages/spsur/index.html>
This version of ?spsur? has been adapted to the standard arguments used in similar packages (spatialreg, spdep, splm, sphet) and include new functionalities.

A new user guide is available on: https://cran.r-project.org/web/packages/spsur/vignettes/vig1.html <https://cran.r-project.org/web/packages/spsur/vignettes/vig1.html>
I hope you find interest this package for you research or for your PhD students. 
Fernando A. L?pez Hern?ndez
fernando.lopez at upct.es <mailto:fernando.lopez at upct.es>
http://metodos.upct.es/falopez/ <http://metodos.upct.es/falopez/>
	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From kobyeongm|n @end|ng |rom kore@@@c@kr  Sat May  9 11:45:34 2020
From: kobyeongm|n @end|ng |rom kore@@@c@kr (=?UTF-8?B?4oCN6rOg67OR66+8WyDtlZnrtoDsnqztlZkgLyDqsr3soJztlZnqs7wgXQ==?=)
Date: Sat, 9 May 2020 11:45:34 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
Message-ID: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>

Dear list,

there is a bug with the *solve() *function that I cannot find a solution
for a month. So I ask for your help.

*Whenever I try to invert a matrix using the said function, the console
hangs*.
Below I explain more about this situation.

Consider the code

D = matrix(
data = c(1, 2, 3, 4),
nrow = 2,
ncol = 2,
byrow = TRUE)
solve(D)

1. *If I launch the code in R called from a terminal, say, Konsole, the
session will freeze.*
  * I know that the exact timing of the system freeze is when I execute the
solve( ) function.
  * According to htop, one of my CPU core is used by 100% when this happens.

2. *If I launch the same code within RStudio, the code works as expected.*
However, if I call it using the terminal inside RStudio, the session hangs.
  * If the solve() function is used within RMarkdown document, the session
will freeze and the document will not be generated.

3. Launching R with --vanilla does not resolve the issue.

4. Rebooting the PC, using my external graphic card, reinstalling the
r-base-core package in apt, and trying with different terminal emulators do
not help.

5. From the documentation of the solve( ) function in R, it can be seen
that solve(A, B) actually takes two arguments: A is a matrix, and B a
vector or a matrix. If B is a vector, it solves the linear system Ax = B.
If B is a matrix, it solves AX = B and returns X. If nothing is given in
the second argument, it automatically assumes identity matrix of
appropriate size as B. **The first function of solving linear system
works.** If I specify matrices as the second argument, however, the same
problem happens.

6. Using QR decomposition with qr.solve(A) still works well.

Here are my questions:

*1. Has anyone had the same problem as me?*
*2. I also seek recommendations on how to fix this issue.*

For your information, I am using R version 3.6.3 installed from the default
apt repository. Here is the output of which R and R -- version:

    kobyeongmin at odie:~$ which R
    /usr/bin/R

    kobyeongmin at odie:~$ R --version
    R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
    Copyright (C) 2020 The R Foundation for Statistical Computing
    Platform: x86_64-pc-linux-gnu (64-bit)

    R is free software and comes with ABSOLUTELY NO WARRANTY.
    You are welcome to redistribute it under the terms of the
    GNU General Public License versions 2 or 3.
    For more information about these matters see
    https://www.gnu.org/licenses/.

Thank you for reading this, and stay safe!

Best regards

Ko Byeongmin

	[[alternative HTML version deleted]]


From jo@o||gue|r@@ @end|ng |rom gm@||@com  Sat May  9 13:29:32 2020
From: jo@o||gue|r@@ @end|ng |rom gm@||@com (=?UTF-8?Q?Jo=c3=a3o_Marreiros?=)
Date: Sat, 9 May 2020 13:29:32 +0200
Subject: [R] ggplot2 axis
Message-ID: <61f15ea8-0794-0a01-90e6-937ceae15f45@gmail.com>

Dear users,
Does anyone had a problem with ggplot concerning the axis not being 
shown? (see attachment)

ggplot(db, aes (x = lenght, y = width, color = support)) +
 ? geom_point(size=2) +
 ? stat_ellipse() +
 ? labs(x="Lenght (mm)", y="width (mm)", title="Boxplot", color = "Support")

I'm using the following version: ggplot2 3.3.0.

Thank you for your help.

Joao M.



-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplot.png
Type: image/png
Size: 58980 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200509/37ac2fc2/attachment.png>

From edd @end|ng |rom deb|@n@org  Sat May  9 17:30:17 2020
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Sat, 9 May 2020 10:30:17 -0500
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
Message-ID: <24246.52361.934223.41992@rob.eddelbuettel.com>


We can see that you use Linux.

Are you by chance

 - on a Debian or Ubuntu system, and 
 - have the libopenblas package installed ?

If so then it is a known bug with the libopenblas0-pthread package.

Installing libopen0-openmp (and also removing libopenblas0-pthread) should
fix it.

This is likely CPU dependent. But we need more info. Can you maybe come to the
r-sig-debian list (subscription needed to reduce spam) and we continue there?

Dirk


-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Sat May  9 17:33:01 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 9 May 2020 17:33:01 +0200
Subject: [R] A stopifnot() nastiness, even if not a bug
In-Reply-To: <24212.52203.211594.608671@stat.math.ethz.ch>
References: <CAGxFJbSAA1t7GF6=2Sv+43syUQtjq1pY-DBppsqE+OE5Rnvp5Q@mail.gmail.com>
 <24212.11722.632757.269523@stat.math.ethz.ch>
 <AA9D2F47-252B-4D37-9F75-027818AE810E@gmail.com>
 <24212.23386.542411.93591@stat.math.ethz.ch>
 <93676b28-7b79-b70e-f2e4-1f1527fb80e6@fredhutch.org>
 <CAF8bMcZD12+L606rwH7+KRUCqzpEFTrnkbm09u+MWGxhSrh5FQ@mail.gmail.com>
 <24212.52203.211594.608671@stat.math.ethz.ch>
Message-ID: <24246.52525.875294.14177@stat.math.ethz.ch>

>>>>> Martin Maechler 
>>>>>     on Mon, 13 Apr 2020 22:30:35 +0200 writes:

>>>>> William Dunlap 
>>>>>     on Mon, 13 Apr 2020 09:57:11 -0700 writes:

    >> You can avoid the problem in Martin's example by only giving scalars to
    >> stopifnot().  E.g., using stopifnot(all(x>0)) or stopifnot(length(x)==1,
    x> 0) instead of stopifnot(x>0).  I think having stopifnot call
    >> all(predicate) if length(predicate)!=1 was probably a mistake.

> well, maybe.

> As I brought up the 0-length example:  One could think of making
> an exception for  logical(0)  and treat that as non-TRUE.

> (for R-devel only, [......])

> Martin

I have been a bit sad that nobody (not even Herv?) reacted to my
proposal, 4 weeks ago.

As I agree that it is safer for stopifnot() to be less lenient
here, and not allow the usual behavior of logical(0) to be
treated as TRUE, namely  as in   all(logical(0))  |-->  TRUE ,
I had actually implemented the above proposal in my own version of R-devel,
(but not committed!), nicely introducing a new optional argument
'allow.logical0'  where

- allow.logical0 = FALSE  is the new default

- allow.logical0 = TRUE   is back compatible

What I found is that this (not back compatible) change lead to a
few test breakages also in R & recommended packages, and IIRC in
a few of my own packages.  Still probably only in about 1 in 1000 
of the stopifnot cases, but in practically all cases, the breakage was
"wrong" in the sense that {conceptual example}

      stopifnot( f1(x) == f2(x) )

should test (almost, say apart from names(.)) identical behavior
of f1() and f2()  and that would naturally also extend to the
case of 0-extent 'x'.

So I had to change the above (half a dozen, say) cases to

    stopifnot( f1(x) == f2(x) , allow.logical0 = TRUE)

to keep the test working as it was intended to.
The nice thing about the change is that it is also working in
current versions of R  where  allow.logical is not a special
argument and just treated as part of '...' and is it TRUE, does
not change the semantic of stopifnot() in current (possibly,
then, "previous") versions of R.

Overall I think it may be a good idea to consider this
not-back-compatible change to  stopifnot()
  (if only to get Herv? into continue using it ! ;-) ;-))

BUT  I assume quite a few other people may have to get used to
see the following error in their stopifnot() code and will have
to add  occasional   'allow.logical0 = TRUE'  to those cases the
old behavior was really the intended one.

(I will have to finally get Matrix 1.3-0 released to CRAN
 before committing the change to R-devel,  and I may also ask
 help of someone to check all CRAN/Bioc against that change so I
 can alert package authors who need to adapt).

Please let us know your thoughts on this.

Martin



    >> On Mon, Apr 13, 2020 at 9:28 AM Herv? Pag?s <hpages at fredhutch.org> wrote:

    >>> 
    >>> 
    >>> On 4/13/20 05:30, Martin Maechler wrote:
    >>> >>>>>> peter dalgaard
    >>> >>>>>>      on Mon, 13 Apr 2020 12:00:38 +0200 writes:
    >>> >
    >>> >      > Inline...
    >>> >      >> On 13 Apr 2020, at 11:15 , Martin Maechler <
    >>> maechler at stat.math.ethz.ch> wrote:
    >>> >      >>
    >>> >      >>>>>>> Bert Gunter
    >>> >      >>>>>>> on Sun, 12 Apr 2020 16:30:09 -0700 writes:
    >>> >      >>
    >>> >      >>> Don't know if this has come up before, but ...
    >>> >      >>>> x <- c(0,0)
    >>> >      >>>> length(x)
    >>> >      >>> [1] 2
    >>> >      >>> ## but
    >>> >      >>>> stopifnot(length(x))
    >>> >      >>> Error: length(x) is not TRUE
    >>> >      >>> Called from: top level
    >>> >      >>> ## but
    >>> >      >>>> stopifnot(length(x) > 0)  ## not an error;  nor is
    >>> >      >>>> stopifnot(as.logical(length(x)))
    >>> >      >>> ## Ouch!
    >>> >      >>
    >>> >      >>> Maybe the man page should say something about not assuming
    >>> automatic
    >>> >      >>> coercion to logical, which is the usual expectation. Or fix
    >>> this.
    >>> >      >>
    >>> >      >>> Bert Gunter
    >>> >      >>
    >>> >      >> Well, what about the top most paragraph of the help page is not
    >>> clear here ?
    >>> >      >>
    >>> >      >>> Description:
    >>> >      >>
    >>> >      >>> If any of the expressions (in '...' or 'exprs') are not 'all'
    >>> >      >>> 'TRUE', 'stop' is called, producing an error message indicating
    >>> >      >>> the _first_ expression which was not ('all') true.
    >>> >      >>
    >>> >
    >>> >      > This, however, is somewhat less clear:
    >>> >
    >>> >      > ..., exprs: any number of (typically but not necessarily
    >>> ?logical?) R
    >>> >      > expressions, which should each evaluate to (a logical vector
    >>> >      > of all) ?TRUE?.  Use _either_ ?...? _or_ ?exprs?, the latter
    >>> >
    >>> >      > What does it mean, "typically but not necessarily ?logical?"?
    >>> >
    >>> > That's a good question: The '(....)' must have been put there a while
    >>> ago.
    >>> > I agree that it's not at all helpful. Strictly, we are really
    >>> > dealing with unevaluated expressions anyway ("promises"), but
    >>> > definitely all of them must evaluate to logical (vector or
    >>> > array..) of all TRUE values.  In the very beginning of
    >>> > stopifnot(), I had thought that it should also work in other
    >>> > cases, e.g.,  for    Matrix(TRUE, 4,5)  {from the Matrix package} etc,
    >>> > but several use cases had convinced us / me that stopifnot
    >>> > should be stricter...
    >>> >
    >>> >      > The code actually tests explicitly with is.logical, as far as I
    >>> can tell.
    >>> >
    >>> >      > This creates a discrepancy between if(!...)stop(...) and
    >>> stopifnot(),
    >>> >
    >>> > yes indeed, on purpose now, for a very long time ...
    >>> >
    >>> > There's another discrepancy, more dangerous I think,
    >>> > as shown in the following
    >>> > {Note this discrepancy has been noted for a long time .. also on
    >>> >   this R-devel list} :
    >>> >
    >>> >    m <- matrix(1:12, 3,4)
    >>> >    i <- (1:4) %% 2 == 1  & (0:3) %% 5 == 0
    >>> >
    >>> >    stopifnot(dim(m[,i]) == c(3,1))       # seems fine
    >>> >
    >>> >    if(dim(m[,i]) != c(3,1)) stop("wrong dim") # gives an error (but not
    >>> ..)
    >>> 
    >>> mmh... that is not good. I was under the impression that we could at
    >>> least expect 'stopifnot(x)' to be equivalent to 'if (!isTRUE(x))
    >>> stop(...)'. I'll have to revisit my use of stopifnot() in many many
    >>> places... again :-/ Or may be just stop using it and use 'if
    >>> (!isTRUE(...))' instead.
    >>> 
    >>> H.
    >>> 
    >>> >
    >>> >
    >>> > Martin
    >>> >
    >>> >      >> as in
    >>> >      >> f <- function (x) if (!x) stop(paste(deparse(substitute(x)), "is
    >>> not TRUE"))
    >>> >      >> f(0)
    >>> >      > Error in f(0) : 0 is not TRUE
    >>> >      >> f(1)
    >>> >      >> stopifnot(0)
    >>> >      > Error: 0 is not TRUE
    >>> >      >> stopifnot(1)
    >>> >      > Error: 1 is not TRUE
    >>> >
    >>> >      > -pd
    >>> >
    >>> >
    >>> >      >> If useR's expectations alone would guide the behavior of a
    >>> >      >> computer language, the language would have to behave
    >>> >      >> "personalized" and give different results depending on the user,
    >>> >      >> which may be desirable in medicine or psychotherapy but not with
    >>> R.

    >>> >      >> Martin


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Sat May  9 17:37:36 2020
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 9 May 2020 17:37:36 +0200
Subject: [R] Error in summary.warnings?
In-Reply-To: <24245.31929.569398.561576@stat.math.ethz.ch>
References: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>
 <24245.31929.569398.561576@stat.math.ethz.ch>
Message-ID: <24246.52800.589144.210912@stat.math.ethz.ch>

>>>>> Martin Maechler 
>>>>>     on Fri, 8 May 2020 17:37:29 +0200 writes:

>>>>> Allison Meisner 
>>>>>     on Thu, 7 May 2020 19:32:36 +0000 writes:

    > I believe there is an error in the summary.warnings function (typically called via 'summary(warnings())'). Below is a minimal working example:

    > #########

    > testfunction <- function(x){
    >  if(x > 30){
    >      warning("A big problem (should be 20 of these)")
    >  }else{
    >      warning("Bigger problem (should be 30 of these)")
    >  }
    > }

    > for(i in 1:50){
    >     testfunction(i)
    > }

    > summary(warnings())

    > #########

    > I checked the code for summary.warnings:

    > function (object, ...)
    > {
    >  msgs <- names(object)
    >  calls <- as.character(object)
    >  ss <- ": "
    >  c.m. <- paste(calls, msgs, sep = ss)
    >  if(length(i.no.call <- which(calls == "NULL")))
    >     c.m.[i.no.call] <- substr(c.m.[i.no.call],
    > 				  nchar(paste0("NULL", ss))+1L, 100000L)
    >  tm <- table(c.m., deparse.level = 0L)
    >  structure(unique(object), counts = as.vector(tm), class = "summary.warnings")
    > }


    >> The problem appears to be in the last line: unique preserves the order of the input, but counts reflects the counts in the table tm, which is a problem because table names are in alphabetical order.

I've committed the fix.  If you are interested,
I've replaced the last 2 lines with

    i.uniq <- which(!duplicated(object, incomparables=FALSE))
    tm <- table(factor(c.m., levels=c.m.[i.uniq]), deparse.level=0L)
    structure(object[i.uniq], counts = as.vector(tm), class = "summary.warnings")


which (at least conceptually) should even be faster the previous code.

Thank you again,
Martin

    >> Am I missing something?

    > No -- I think you are perfect and I was very imperfect ;-)  when
    > I created and tested the function ..

    > This will be fixed in the next versions of R.

    > Thank you very much for the report  and the nice concise
    > reproducible example!

    > Best regards,
    > Martin

    >> Allison
    >> ----------
    >> Allison Meisner, PhD
    >> Postdoctoral Fellow
    >> Department of Biostatistics
    >> Johns Hopkins Bloomberg School of Public Health
    >> 615 N. Wolfe Street
    >> Baltimore, MD 21205

    > Martin Maechler
    > ETH Zurich  and   R Core team

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Sat May  9 17:40:32 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Sat, 9 May 2020 11:40:32 -0400
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
Message-ID: <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>

I get the output at the bottom, which seems OK.

Can you include sessionInfo() output?
Possibly this is a quirk of the particular distro or machine, BLAS or LAPACK,
or something in your workspace. However, if we have full information, someone may be
able to run the same setup in a VM (if I have the .iso and can find
a way to set up R 3.6.3 easily, I'll be willing). It may be that you should
reinstall libblas or liblapack as one thing to try.

JN

Here's my output:

> D = matrix(
+ data = c(1, 2, 3, 4),
+ nrow = 2,
+ ncol = 2,
+ byrow = TRUE)
> solve(D)
     [,1] [,2]
[1,] -2.0  1.0
[2,]  1.5 -0.5
> D %*% solve(D)
     [,1]         [,2]
[1,]    1 1.110223e-16
[2,]    0 1.000000e+00
> sessionInfo()
R version 4.0.0 (2020-04-24)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 19.3

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

locale:
 [1] LC_CTYPE=en_CA.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_CA.UTF-8        LC_COLLATE=en_CA.UTF-8
 [5] LC_MONETARY=en_CA.UTF-8    LC_MESSAGES=en_CA.UTF-8
 [7] LC_PAPER=en_CA.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_4.0.0
>

On 2020-05-09 5:45 a.m., ????[ ???? / ???? ] wrote:
> Dear list,
> 
> there is a bug with the *solve() *function that I cannot find a solution
> for a month. So I ask for your help.
> 
> *Whenever I try to invert a matrix using the said function, the console
> hangs*.
> Below I explain more about this situation.
> 
> Consider the code
> 
> D = matrix(
> data = c(1, 2, 3, 4),
> nrow = 2,
> ncol = 2,
> byrow = TRUE)
> solve(D)
> 
> 1. *If I launch the code in R called from a terminal, say, Konsole, the
> session will freeze.*
>   * I know that the exact timing of the system freeze is when I execute the
> solve( ) function.
>   * According to htop, one of my CPU core is used by 100% when this happens.
> 
> 2. *If I launch the same code within RStudio, the code works as expected.*
> However, if I call it using the terminal inside RStudio, the session hangs.
>   * If the solve() function is used within RMarkdown document, the session
> will freeze and the document will not be generated.
> 
> 3. Launching R with --vanilla does not resolve the issue.
> 
> 4. Rebooting the PC, using my external graphic card, reinstalling the
> r-base-core package in apt, and trying with different terminal emulators do
> not help.
> 
> 5. From the documentation of the solve( ) function in R, it can be seen
> that solve(A, B) actually takes two arguments: A is a matrix, and B a
> vector or a matrix. If B is a vector, it solves the linear system Ax = B.
> If B is a matrix, it solves AX = B and returns X. If nothing is given in
> the second argument, it automatically assumes identity matrix of
> appropriate size as B. **The first function of solving linear system
> works.** If I specify matrices as the second argument, however, the same
> problem happens.
> 
> 6. Using QR decomposition with qr.solve(A) still works well.
> 
> Here are my questions:
> 
> *1. Has anyone had the same problem as me?*
> *2. I also seek recommendations on how to fix this issue.*
> 
> For your information, I am using R version 3.6.3 installed from the default
> apt repository. Here is the output of which R and R -- version:
> 
>     kobyeongmin at odie:~$ which R
>     /usr/bin/R
> 
>     kobyeongmin at odie:~$ R --version
>     R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
>     Copyright (C) 2020 The R Foundation for Statistical Computing
>     Platform: x86_64-pc-linux-gnu (64-bit)
> 
>     R is free software and comes with ABSOLUTELY NO WARRANTY.
>     You are welcome to redistribute it under the terms of the
>     GNU General Public License versions 2 or 3.
>     For more information about these matters see
>     https://www.gnu.org/licenses/.
> 
> Thank you for reading this, and stay safe!
> 
> Best regards
> 
> Ko Byeongmin
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From edd @end|ng |rom deb|@n@org  Sat May  9 18:35:12 2020
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Sat, 9 May 2020 11:35:12 -0500
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <24246.52361.934223.41992@rob.eddelbuettel.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <24246.52361.934223.41992@rob.eddelbuettel.com>
Message-ID: <24246.56256.474662.24178@rob.eddelbuettel.com>


On 9 May 2020 at 10:30, Dirk Eddelbuettel wrote:
| 
| We can see that you use Linux.
| 
| Are you by chance
| 
|  - on a Debian or Ubuntu system, and 
|  - have the libopenblas package installed ?
| 
| If so then it is a known bug with the libopenblas0-pthread package.
| 
| Installing libopen0-openmp (and also removing libopenblas0-pthread) should
| fix it.

That was imprecise. It should read "installing libopenblas-openmp-dev" and
removing both "libopenblas-pthread-dev libopenblas0-pthread".

I cannot reproduce the bug on the hardware I have access to (i5, i7, xeon)
though I was able to a few weeks ago. Maybe something changed already...

There was also a brief thread on the debian-science list (within Debian)
starting with https://lists.debian.org/debian-science/2020/04/msg00081.html
and leading to https://lists.debian.org/debian-science/2020/05/msg00003.html
(and this cross from April to May)

We would need some more information on hardware to chase this.

Dirk
 
| This is likely CPU dependent. But we need more info. Can you maybe come to the
| r-sig-debian list (subscription needed to reduce spam) and we continue there?
| 
| Dirk
| 
| 
| -- 
| http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
| 
| ______________________________________________
| R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
| https://stat.ethz.ch/mailman/listinfo/r-help
| PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
| and provide commented, minimal, self-contained, reproducible code.

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat May  9 19:00:43 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 9 May 2020 18:00:43 +0100
Subject: [R] ggplot2 axis
In-Reply-To: <61f15ea8-0794-0a01-90e6-937ceae15f45@gmail.com>
References: <61f15ea8-0794-0a01-90e6-937ceae15f45@gmail.com>
Message-ID: <81617bee-5179-3de1-e4b1-aa6827ffc9f8@sapo.pt>

Hello,

I cannot reproduce this. If I do

db <- iris[3:5]
names(db) <- c("length", "width", "support")

and change 'lenght' to 'length' in the ggplot call both axis are plotted.
But the code is not reproducible, db is missing. Can you post the output of


dput(head(db, 30))


in a next mail?

Hope this helps,

Rui Barradas

?s 12:29 de 09/05/20, Jo?o Marreiros escreveu:
> Dear users,
> Does anyone had a problem with ggplot concerning the axis not being 
> shown? (see attachment)
> 
> ggplot(db, aes (x = lenght, y = width, color = support)) +
>  ? geom_point(size=2) +
>  ? stat_ellipse() +
>  ? labs(x="Lenght (mm)", y="width (mm)", title="Boxplot", color = 
> "Support")
> 
> I'm using the following version: ggplot2 3.3.0.
> 
> Thank you for your help.
> 
> Joao M.
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From iuke-tier@ey m@iii@g oii uiow@@edu  Sat May  9 20:13:17 2020
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Sat, 9 May 2020 13:13:17 -0500 (CDT)
Subject: [R] [External] Re:  A stopifnot() nastiness, even if not a bug
In-Reply-To: <24246.52525.875294.14177@stat.math.ethz.ch>
References: <CAGxFJbSAA1t7GF6=2Sv+43syUQtjq1pY-DBppsqE+OE5Rnvp5Q@mail.gmail.com>
 <24212.11722.632757.269523@stat.math.ethz.ch>
 <AA9D2F47-252B-4D37-9F75-027818AE810E@gmail.com>
 <24212.23386.542411.93591@stat.math.ethz.ch>
 <93676b28-7b79-b70e-f2e4-1f1527fb80e6@fredhutch.org>
 <CAF8bMcZD12+L606rwH7+KRUCqzpEFTrnkbm09u+MWGxhSrh5FQ@mail.gmail.com>
 <24212.52203.211594.608671@stat.math.ethz.ch>
 <24246.52525.875294.14177@stat.math.ethz.ch>
Message-ID: <alpine.DEB.2.21.2005091311510.8504@luke-Latitude-7480>

Since you asked ...

There are two different use cases for stopifnot: in tests and
in implementation code.

stopifnot seems to be optimized for the test code case where speed and
clarity may not be that important.

For expresing assertions in implemantation code clarity and simplicity
are important. What I would want is something equivalent to

stpifnt0 <- function(e)
     if (! isTRUE(e))
         stop(deparse(exprs[i]), "is not true")

(maybe with some additional gettetxt/deparse/call fiddling magic, but
only _after_ the fail, not on the hot path for a passed test.)

I would not want to have to worry about fine print about whether there
is an implicit 'all' for an expression returning a vector, or whether
that is really an 'all' or a variant that treate the empty set
differently.

I might consider a multiargument verison along the lines of

stpfnt <- function(...) {
     exprs <- as.list(substitute(list(...)))[-1]
     for (i in seq_along(exprs))
         if (! isTRUE(...elt(i)))
             stop(deparse(exprs[i]), "is not true")
}

But I think in most cases I rather write two separate assertions.

The advantage of stpfnt0 is code is that it is more concise than
writing out the test and the code to produce an error message,
especially a nice one. But it this comes at a big performance cost
then using it would be harder to jsutify.

With simple semantics like the stpfnt0 definition here the compiler
could be taught to optimize it's use to cost little more than the test
in the passing case. That gets harder the more complexity is added to
stopifnot.

We could add a simpler stopifnot0, but I'm not sure I want to go
there.

Best,

luke

On Sat, 9 May 2020, Martin Maechler wrote:

>>>>>> Martin Maechler
>>>>>>     on Mon, 13 Apr 2020 22:30:35 +0200 writes:
>
>>>>>> William Dunlap
>>>>>>     on Mon, 13 Apr 2020 09:57:11 -0700 writes:
>
>    >> You can avoid the problem in Martin's example by only giving scalars to
>    >> stopifnot().  E.g., using stopifnot(all(x>0)) or stopifnot(length(x)==1,
>    x> 0) instead of stopifnot(x>0).  I think having stopifnot call
>    >> all(predicate) if length(predicate)!=1 was probably a mistake.
>
>> well, maybe.
>
>> As I brought up the 0-length example:  One could think of making
>> an exception for  logical(0)  and treat that as non-TRUE.
>
>> (for R-devel only, [......])
>
>> Martin
>
> I have been a bit sad that nobody (not even Herv?) reacted to my
> proposal, 4 weeks ago.
>
> As I agree that it is safer for stopifnot() to be less lenient
> here, and not allow the usual behavior of logical(0) to be
> treated as TRUE, namely  as in   all(logical(0))  |-->  TRUE ,
> I had actually implemented the above proposal in my own version of R-devel,
> (but not committed!), nicely introducing a new optional argument
> 'allow.logical0'  where
>
> - allow.logical0 = FALSE  is the new default
>
> - allow.logical0 = TRUE   is back compatible
>
> What I found is that this (not back compatible) change lead to a
> few test breakages also in R & recommended packages, and IIRC in
> a few of my own packages.  Still probably only in about 1 in 1000
> of the stopifnot cases, but in practically all cases, the breakage was
> "wrong" in the sense that {conceptual example}
>
>      stopifnot( f1(x) == f2(x) )
>
> should test (almost, say apart from names(.)) identical behavior
> of f1() and f2()  and that would naturally also extend to the
> case of 0-extent 'x'.
>
> So I had to change the above (half a dozen, say) cases to
>
>    stopifnot( f1(x) == f2(x) , allow.logical0 = TRUE)
>
> to keep the test working as it was intended to.
> The nice thing about the change is that it is also working in
> current versions of R  where  allow.logical is not a special
> argument and just treated as part of '...' and is it TRUE, does
> not change the semantic of stopifnot() in current (possibly,
> then, "previous") versions of R.
>
> Overall I think it may be a good idea to consider this
> not-back-compatible change to  stopifnot()
>  (if only to get Herv? into continue using it ! ;-) ;-))
>
> BUT  I assume quite a few other people may have to get used to
> see the following error in their stopifnot() code and will have
> to add  occasional   'allow.logical0 = TRUE'  to those cases the
> old behavior was really the intended one.
>
> (I will have to finally get Matrix 1.3-0 released to CRAN
> before committing the change to R-devel,  and I may also ask
> help of someone to check all CRAN/Bioc against that change so I
> can alert package authors who need to adapt).
>
> Please let us know your thoughts on this.
>
> Martin
>
>
>
>    >> On Mon, Apr 13, 2020 at 9:28 AM Herv? Pag?s <hpages at fredhutch.org> wrote:
>
>    >>>
>    >>>
>    >>> On 4/13/20 05:30, Martin Maechler wrote:
>    >>> >>>>>> peter dalgaard
>    >>> >>>>>>      on Mon, 13 Apr 2020 12:00:38 +0200 writes:
>    >>> >
>    >>> >      > Inline...
>    >>> >      >> On 13 Apr 2020, at 11:15 , Martin Maechler <
>    >>> maechler at stat.math.ethz.ch> wrote:
>    >>> >      >>
>    >>> >      >>>>>>> Bert Gunter
>    >>> >      >>>>>>> on Sun, 12 Apr 2020 16:30:09 -0700 writes:
>    >>> >      >>
>    >>> >      >>> Don't know if this has come up before, but ...
>    >>> >      >>>> x <- c(0,0)
>    >>> >      >>>> length(x)
>    >>> >      >>> [1] 2
>    >>> >      >>> ## but
>    >>> >      >>>> stopifnot(length(x))
>    >>> >      >>> Error: length(x) is not TRUE
>    >>> >      >>> Called from: top level
>    >>> >      >>> ## but
>    >>> >      >>>> stopifnot(length(x) > 0)  ## not an error;  nor is
>    >>> >      >>>> stopifnot(as.logical(length(x)))
>    >>> >      >>> ## Ouch!
>    >>> >      >>
>    >>> >      >>> Maybe the man page should say something about not assuming
>    >>> automatic
>    >>> >      >>> coercion to logical, which is the usual expectation. Or fix
>    >>> this.
>    >>> >      >>
>    >>> >      >>> Bert Gunter
>    >>> >      >>
>    >>> >      >> Well, what about the top most paragraph of the help page is not
>    >>> clear here ?
>    >>> >      >>
>    >>> >      >>> Description:
>    >>> >      >>
>    >>> >      >>> If any of the expressions (in '...' or 'exprs') are not 'all'
>    >>> >      >>> 'TRUE', 'stop' is called, producing an error message indicating
>    >>> >      >>> the _first_ expression which was not ('all') true.
>    >>> >      >>
>    >>> >
>    >>> >      > This, however, is somewhat less clear:
>    >>> >
>    >>> >      > ..., exprs: any number of (typically but not necessarily
>    >>> ?logical?) R
>    >>> >      > expressions, which should each evaluate to (a logical vector
>    >>> >      > of all) ?TRUE?.  Use _either_ ?...? _or_ ?exprs?, the latter
>    >>> >
>    >>> >      > What does it mean, "typically but not necessarily ?logical?"?
>    >>> >
>    >>> > That's a good question: The '(....)' must have been put there a while
>    >>> ago.
>    >>> > I agree that it's not at all helpful. Strictly, we are really
>    >>> > dealing with unevaluated expressions anyway ("promises"), but
>    >>> > definitely all of them must evaluate to logical (vector or
>    >>> > array..) of all TRUE values.  In the very beginning of
>    >>> > stopifnot(), I had thought that it should also work in other
>    >>> > cases, e.g.,  for    Matrix(TRUE, 4,5)  {from the Matrix package} etc,
>    >>> > but several use cases had convinced us / me that stopifnot
>    >>> > should be stricter...
>    >>> >
>    >>> >      > The code actually tests explicitly with is.logical, as far as I
>    >>> can tell.
>    >>> >
>    >>> >      > This creates a discrepancy between if(!...)stop(...) and
>    >>> stopifnot(),
>    >>> >
>    >>> > yes indeed, on purpose now, for a very long time ...
>    >>> >
>    >>> > There's another discrepancy, more dangerous I think,
>    >>> > as shown in the following
>    >>> > {Note this discrepancy has been noted for a long time .. also on
>    >>> >   this R-devel list} :
>    >>> >
>    >>> >    m <- matrix(1:12, 3,4)
>    >>> >    i <- (1:4) %% 2 == 1  & (0:3) %% 5 == 0
>    >>> >
>    >>> >    stopifnot(dim(m[,i]) == c(3,1))       # seems fine
>    >>> >
>    >>> >    if(dim(m[,i]) != c(3,1)) stop("wrong dim") # gives an error (but not
>    >>> ..)
>    >>>
>    >>> mmh... that is not good. I was under the impression that we could at
>    >>> least expect 'stopifnot(x)' to be equivalent to 'if (!isTRUE(x))
>    >>> stop(...)'. I'll have to revisit my use of stopifnot() in many many
>    >>> places... again :-/ Or may be just stop using it and use 'if
>    >>> (!isTRUE(...))' instead.
>    >>>
>    >>> H.
>    >>>
>    >>> >
>    >>> >
>    >>> > Martin
>    >>> >
>    >>> >      >> as in
>    >>> >      >> f <- function (x) if (!x) stop(paste(deparse(substitute(x)), "is
>    >>> not TRUE"))
>    >>> >      >> f(0)
>    >>> >      > Error in f(0) : 0 is not TRUE
>    >>> >      >> f(1)
>    >>> >      >> stopifnot(0)
>    >>> >      > Error: 0 is not TRUE
>    >>> >      >> stopifnot(1)
>    >>> >      > Error: 1 is not TRUE
>    >>> >
>    >>> >      > -pd
>    >>> >
>    >>> >
>    >>> >      >> If useR's expectations alone would guide the behavior of a
>    >>> >      >> computer language, the language would have to behave
>    >>> >      >> "personalized" and give different results depending on the user,
>    >>> >      >> which may be desirable in medicine or psychotherapy but not with
>    >>> R.
>
>    >>> >      >> Martin
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From @me|@ne1 @end|ng |rom jhm|@edu  Sat May  9 18:39:07 2020
From: @me|@ne1 @end|ng |rom jhm|@edu (Allison Meisner)
Date: Sat, 9 May 2020 16:39:07 +0000
Subject: [R] Error in summary.warnings?
In-Reply-To: <24246.52800.589144.210912@stat.math.ethz.ch>
References: <BL0PR01MB43069FB2C1BC342FFDC19142EFA50@BL0PR01MB4306.prod.exchangelabs.com>
 <24245.31929.569398.561576@stat.math.ethz.ch>,
 <24246.52800.589144.210912@stat.math.ethz.ch>
Message-ID: <BL0PR01MB4306CE1175CECA82E17CC48CEFA30@BL0PR01MB4306.prod.exchangelabs.com>

Great! Thanks for sharing your fix.

Allison
________________________________
From: Martin Maechler <maechler at stat.math.ethz.ch>
Sent: Saturday, May 9, 2020 11:37 AM
To: Allison Meisner <ameisne1 at jhmi.edu>; r-help at r-project.org <r-help at r-project.org>
Cc: Martin Maechler <maechler at stat.math.ethz.ch>
Subject: Re: [R] Error in summary.warnings?


>>>>> Martin Maechler
>>>>>     on Fri, 8 May 2020 17:37:29 +0200 writes:

>>>>> Allison Meisner
>>>>>     on Thu, 7 May 2020 19:32:36 +0000 writes:

    > I believe there is an error in the summary.warnings function (typically called via 'summary(warnings())'). Below is a minimal working example:

    > #########

    > testfunction <- function(x){
    >  if(x > 30){
    >      warning("A big problem (should be 20 of these)")
    >  }else{
    >      warning("Bigger problem (should be 30 of these)")
    >  }
    > }

    > for(i in 1:50){
    >     testfunction(i)
    > }

    > summary(warnings())

    > #########

    > I checked the code for summary.warnings:

    > function (object, ...)
    > {
    >  msgs <- names(object)
    >  calls <- as.character(object)
    >  ss <- ": "
    >  c.m. <- paste(calls, msgs, sep = ss)
    >  if(length(i.no.call <- which(calls == "NULL")))
    >     c.m.[i.no.call] <- substr(c.m.[i.no.call],
    >                              nchar(paste0("NULL", ss))+1L, 100000L)
    >  tm <- table(c.m., deparse.level = 0L)
    >  structure(unique(object), counts = as.vector(tm), class = "summary.warnings")
    > }


    >> The problem appears to be in the last line: unique preserves the order of the input, but counts reflects the counts in the table tm, which is a problem because table names are in alphabetical order.

I've committed the fix.  If you are interested,
I've replaced the last 2 lines with

    i.uniq <- which(!duplicated(object, incomparables=FALSE))
    tm <- table(factor(c.m., levels=c.m.[i.uniq]), deparse.level=0L)
    structure(object[i.uniq], counts = as.vector(tm), class = "summary.warnings")


which (at least conceptually) should even be faster the previous code.

Thank you again,
Martin

    >> Am I missing something?

    > No -- I think you are perfect and I was very imperfect ;-)  when
    > I created and tested the function ..

    > This will be fixed in the next versions of R.

    > Thank you very much for the report  and the nice concise
    > reproducible example!

    > Best regards,
    > Martin

    >> Allison
    >> ----------
    >> Allison Meisner, PhD
    >> Postdoctoral Fellow
    >> Department of Biostatistics
    >> Johns Hopkins Bloomberg School of Public Health
    >> 615 N. Wolfe Street
    >> Baltimore, MD 21205

    > Martin Maechler
    > ETH Zurich  and   R Core team

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    > https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    > and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From @xe|@urb|z @end|ng |rom gm@||@com  Sun May 10 01:45:40 2020
From: @xe|@urb|z @end|ng |rom gm@||@com (Axel Urbiz)
Date: Sat, 9 May 2020 19:45:40 -0400
Subject: [R] Loop inside dplyr::mutate
Message-ID: <9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>

Hello, 

Is there a less verbose approach to obtaining the PC_i variables inside the mutate?

library(tidyverse)
sim_data <- data.frame(borrower_id = sort(rep(1:10, 20)),
                       quarter = rep(1:20, 10),
                       pd = runif(length(rep(1:20, 10)))) # conditional probs
 
sim_data_wide <- tidyr::spread(sim_data, quarter, pd)  
colnames(sim_data_wide)[-1] <- paste0("P_", colnames(sim_data_wide)[-1])
      
# Compute cumulative probs
sim_data_wide <- sim_data_wide %>%
                  mutate(PC_1 = P_1,
                         PC_2 = 1-(1-P_1)*(1-P_2),
                         PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
                         PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
                         PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
                         PC_6 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
                         PC_7 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
                         PC_8 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
                         PC_9 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
                         PC_10 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
                        )


Thanks,
Axel.
	[[alternative HTML version deleted]]


From m@|||P@dpo@t @end|ng |rom gm@||@com  Sun May 10 02:30:46 2020
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Sun, 10 May 2020 03:30:46 +0300
Subject: [R] Date format
Message-ID: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>

I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
and applied to MY DATA, but got:
"Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
P.S. I can not understand ?as.Date()

SAMPLE CODE
library(ggplot2)
library(dplyr)
library(hrbrthemes)
data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
header=T)

str(data)
'data.frame': 1822 obs. of  2 variables:
 $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01" ...
 $ value: num  136 147 147 140 126 ...

data$date <- as.Date(data$date)

# Plot
data %>%
  tail(10) %>%
  ggplot( aes(x=date, y=value)) +
    geom_line( color="grey") +
    geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
    theme_ipsum() +
    ggtitle("Evolution of bitcoin price")


MY DATA
mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")

str(mydata)
'data.frame': 7 obs. of  2 variables:
 $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000" ...
 $ value: int  11 12 13 14 15 16 17

mydata$date <- as.Date(mydata$date)
Error in as.Date.numeric(mydata$date) : 'origin' must be supplied


From md@umner @end|ng |rom gm@||@com  Sun May 10 02:42:43 2020
From: md@umner @end|ng |rom gm@||@com (Michael Sumner)
Date: Sun, 10 May 2020 10:42:43 +1000
Subject: [R] the volcano orientation
Message-ID: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>

Does anyone know why 'volcano' is oriented as it is?

image(volcano)  ## filled.contour is the same

I know it's all arbitrary, but north-up is a pretty solid convention. Is
there any reason why the classic 'image()' example data set would not
default to this orientation?

A Google map of the site (in Web Mercator):

https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667


For image(), the north-up orientation is 't(volcano[,ncol(volcano):1])'.

If you are interested in a roughly georeferenced version I have code here:

https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8

(Also see fortunes::fortune("conventions") )

Best, Mike


-- 
Michael Sumner
Software and Database Engineer
Australian Antarctic Division
Hobart, Australia
e-mail: mdsumner at gmail.com

	[[alternative HTML version deleted]]


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Sun May 10 02:40:42 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Sat, 9 May 2020 18:40:42 -0600
Subject: [R] predicting waste per capita - is a gaussian model correct?
Message-ID: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>

Dear list,

I am new to this list and I hope it is ok to post here even though I
already posted this question on Cross Validated.

I am trying to predict the daily amount of waste per person produced in the
fishery sector. We surveyed fishing boats at the end of their fishing trip
and the variables I have are duration of trip (days), number of fishers,
waste category and waste weight (g), boat ID.

For each fishing trip I calculated grams of waste per person per day, i.e.
daily waste per capita. To predict daily waste per capita, I am using a
gaussian mixed effect model with log(waste per capita) as response variable
(I transformed it cause it was not normally distributed - and I'm not sure
it's correct to do so). Explanatory variable is waste category and boat ID
is a random effect. I use the predict function to estimate daily waste per
capita for each category and then back transformed it with exp(...).

My question is: is it correct to transform daily weight per capita to fit a
gaussian model?

Thanks so much for your advice!

Alessandra

	[[alternative HTML version deleted]]


From |e@gr@|n@ @end|ng |rom gm@||@com  Sun May 10 02:13:34 2020
From: |e@gr@|n@ @end|ng |rom gm@||@com (Adrien FABRE)
Date: Sun, 10 May 2020 02:13:34 +0200
Subject: [R] Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
Message-ID: <CAKPvFjCEmUKBYj6=m0_kt7D8ziw8KhSgdX3_9ncb1SC0+xo1_A@mail.gmail.com>

I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
days ago, and Ubuntu from 18.04 to 20.04 yesterday.

Since then, R sometimes never terminates when executing certain commands:
ivreg (from package AER), summary (of a logit regression) and logitmfx
(from package mfx). Sometimes these commands run fine, but most of the time
I have to kill the process because R won't terminate the execution, even
when pressing the red Stop button in RStudio.

When I tried example('AER'), it worked fine. Then I re-installed the
package AER. It threw 10 warnings of type In readLines(file, skipNul =
TRUE) :  cannot open compressed file
'/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
file or directory' where [package] is abind, colorspace, dichromat... (but
not AER).

Since then example('AER') throws a warning: no help found for ?AER?.

I've removed and reinstalled R 4.0: it didn't help. Besides, the apt purge
r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
removing r-base-core, directory '/usr/lib/R/site-library' not empty so not
removed. Also, there was a bunch of Package [package] is not installed, so
not removed, including for [package] equal to r-cran-abind and the other
listed above (this purge also returned a bunch of Note, selecting [package]
for glob 'r-cran-*').

I have the same bug when using R from the terminal. For the record, I was
probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
can't recall if this issue started after I upgraded R and RStudio (which
would be my best guess) or after I upgraded Ubuntu (a day or two later).

I hope someone can help.

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Sun May 10 02:46:08 2020
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Sun, 10 May 2020 00:46:08 +0000
Subject: [R] Loop inside dplyr::mutate
In-Reply-To: <13331_1589067958_049NjvZv006105_9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
References: <13331_1589067958_049NjvZv006105_9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
Message-ID: <F72DF2D9-A41E-441E-9F1C-F3EEC890BCE0@mcmaster.ca>

Dear Axel,

Assuming that you're not wedded to using mutate():

> D1 <- 1 - as.matrix(sim_data_wide[, 2:11])
> D2 <- matrix(0, 10, 10)
> colnames(D2) <- paste0("PC_", 1:10)
> for (i in 1:10) D2[, i] <- 1 - apply(D1[, 1:i, drop=FALSE], 1, prod)
> all.equal(D2, as.matrix(sim_data_wide[, 22:31]))
[1] TRUE 

I hope this helps,
 John

> On May 9, 2020, at 7:45 PM, Axel Urbiz <axel.urbiz at gmail.com> wrote:
> 
> Hello, 
> 
> Is there a less verbose approach to obtaining the PC_i variables inside the mutate?
> 
> library(tidyverse)
> sim_data <- data.frame(borrower_id = sort(rep(1:10, 20)),
>                       quarter = rep(1:20, 10),
>                       pd = runif(length(rep(1:20, 10)))) # conditional probs
> 
> sim_data_wide <- tidyr::spread(sim_data, quarter, pd)  
> colnames(sim_data_wide)[-1] <- paste0("P_", colnames(sim_data_wide)[-1])
> 
> # Compute cumulative probs
> sim_data_wide <- sim_data_wide %>%
>                  mutate(PC_1 = P_1,
>                         PC_2 = 1-(1-P_1)*(1-P_2),
>                         PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
>                         PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
>                         PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
>                         PC_6 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
>                         PC_7 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
>                         PC_8 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
>                         PC_9 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
>                         PC_10 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
>                        )
> 
> 
> Thanks,
> Axel.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May 10 03:17:24 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 May 2020 18:17:24 -0700
Subject: [R] Loop inside dplyr::mutate
In-Reply-To: <9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
References: <9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
Message-ID: <D673A040-56B1-47C0-8AB6-4677019D9C22@dcn.davis.ca.us>

Does this help?

sim_wide2 <- (
    sim_data
%>% arrange( borrower_id, quarter )
%>% group_by( borrower_id )
%>% mutate( cumpd = 1 - cumprod( 1 - pd ) )
%>% ungroup()
%>% mutate( qlbl = paste0( "PC_", quarter ) )
%>% select( borrower_id, qlbl, cumpd )
%>% spread( qlbl, cumpd )
)

On May 9, 2020 4:45:40 PM PDT, Axel Urbiz <axel.urbiz at gmail.com> wrote:
>Hello, 
>
>Is there a less verbose approach to obtaining the PC_i variables inside
>the mutate?
>
>library(tidyverse)
>sim_data <- data.frame(borrower_id = sort(rep(1:10, 20)),
>                       quarter = rep(1:20, 10),
>                 pd = runif(length(rep(1:20, 10)))) # conditional probs
> 
>sim_data_wide <- tidyr::spread(sim_data, quarter, pd)  
>colnames(sim_data_wide)[-1] <- paste0("P_",
>colnames(sim_data_wide)[-1])
>      
># Compute cumulative probs
>sim_data_wide <- sim_data_wide %>%
>                  mutate(PC_1 = P_1,
>                         PC_2 = 1-(1-P_1)*(1-P_2),
>                         PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
>                         PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
>                      PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
>              PC_6 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
>      PC_7 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
>PC_8 =
>1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
>PC_9 =
>1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
>PC_10 =
>1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
>                        )
>
>
>Thanks,
>Axel.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May 10 03:23:19 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 May 2020 18:23:19 -0700
Subject: [R] predicting waste per capita - is a gaussian model correct?
In-Reply-To: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
References: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
Message-ID: <5E75B7DA-819D-41AC-902B-C01F76A03A3C@dcn.davis.ca.us>

It could possibly be alright, except that:

a) you included no reference to your other post
b) you posted here using HTML format, which can severely corrupt what we see on this plain text only mailing list
c) your question is off topic, as your question is about statistics (theory) rather than R (a syntax and semantics for implementing theory).

So, no, not ok this time.

On May 9, 2020 5:40:42 PM PDT, Alessandra Bielli <bielli.alessandra at gmail.com> wrote:
>Dear list,
>
>I am new to this list and I hope it is ok to post here even though I
>already posted this question on Cross Validated.
>
>I am trying to predict the daily amount of waste per person produced in
>the
>fishery sector. We surveyed fishing boats at the end of their fishing
>trip
>and the variables I have are duration of trip (days), number of
>fishers,
>waste category and waste weight (g), boat ID.
>
>For each fishing trip I calculated grams of waste per person per day,
>i.e.
>daily waste per capita. To predict daily waste per capita, I am using a
>gaussian mixed effect model with log(waste per capita) as response
>variable
>(I transformed it cause it was not normally distributed - and I'm not
>sure
>it's correct to do so). Explanatory variable is waste category and boat
>ID
>is a random effect. I use the predict function to estimate daily waste
>per
>capita for each category and then back transformed it with exp(...).
>
>My question is: is it correct to transform daily weight per capita to
>fit a
>gaussian model?
>
>Thanks so much for your advice!
>
>Alessandra
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From rmh @end|ng |rom temp|e@edu  Sun May 10 03:40:28 2020
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Sat, 9 May 2020 21:40:28 -0400
Subject: [R] Loop inside dplyr::mutate
In-Reply-To: <D673A040-56B1-47C0-8AB6-4677019D9C22@dcn.davis.ca.us>
References: <9B339F38-5E93-41A3-A57D-B10C32C8F7CD@gmail.com>
 <D673A040-56B1-47C0-8AB6-4677019D9C22@dcn.davis.ca.us>
Message-ID: <CAGx1TMABq+tGEoJZq8ksG5-TqB=LWqZooD3OgTrWMA2t2BhSwA@mail.gmail.com>

## I start with sim_data_wide

sim_data_wide <- tidyr::spread(sim_data, quarter, pd)

## and calculate wide
wide1 <- with(sim_data_wide, cbind(PC_1 = P_1,
                       PC_2 = 1-(1-P_1)*(1-P_2),
                       PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
                       PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
                       PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
                       PC_6 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
                       PC_7 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
                       PC_8 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
                       PC_9 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
                       PC_10 =
1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
                      )
)

## this simpler sequence gets the same value

A <- 1-sim_data_wide[,2:11]
B <- t(apply(A, 1, cumprod)[-1,])
wide2 <- cbind(sim_data_wide[,2], 1-B)
dimnames(wide2)[[2]] <- paste0("PC_", 1:10)

all.equal(wide1, wide2)


On Sat, May 9, 2020 at 9:28 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Does this help?
>
> sim_wide2 <- (
>     sim_data
> %>% arrange( borrower_id, quarter )
> %>% group_by( borrower_id )
> %>% mutate( cumpd = 1 - cumprod( 1 - pd ) )
> %>% ungroup()
> %>% mutate( qlbl = paste0( "PC_", quarter ) )
> %>% select( borrower_id, qlbl, cumpd )
> %>% spread( qlbl, cumpd )
> )
>
> On May 9, 2020 4:45:40 PM PDT, Axel Urbiz <axel.urbiz at gmail.com> wrote:
> >Hello,
> >
> >Is there a less verbose approach to obtaining the PC_i variables inside
> >the mutate?
> >
> >library(tidyverse)
> >sim_data <- data.frame(borrower_id = sort(rep(1:10, 20)),
> >                       quarter = rep(1:20, 10),
> >                 pd = runif(length(rep(1:20, 10)))) # conditional probs
> >
> >sim_data_wide <- tidyr::spread(sim_data, quarter, pd)
> >colnames(sim_data_wide)[-1] <- paste0("P_",
> >colnames(sim_data_wide)[-1])
> >
> ># Compute cumulative probs
> >sim_data_wide <- sim_data_wide %>%
> >                  mutate(PC_1 = P_1,
> >                         PC_2 = 1-(1-P_1)*(1-P_2),
> >                         PC_3 = 1-(1-P_1)*(1-P_2)*(1-P_3),
> >                         PC_4 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4),
> >                      PC_5 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5),
> >              PC_6 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6),
> >      PC_7 = 1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7),
> >PC_8 =
> >1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8),
> >PC_9 =
> >1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9),
> >PC_10 =
>
> >1-(1-P_1)*(1-P_2)*(1-P_3)*(1-P_4)*(1-P_5)*(1-P_6)*(1-P_7)*(1-P_8)*(1-P_9)*(1-P_10)
> >                        )
> >
> >
> >Thanks,
> >Axel.
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun May 10 04:17:16 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 9 May 2020 19:17:16 -0700
Subject: [R] Date format
In-Reply-To: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
Message-ID: <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>

$date is a factor, which is coded as numeric values internally, which
as.date sees as numeric, and therefore:
"as.Date will accept numeric data (the number of days since an epoch),
but only if origin is supplied." (from ?as.Date)

You need to supply a format argument to as.Date to get it to handle
the factor properly; e.g.
"%d.%m.%Y"  should work. See ?strptime for formatting details.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, May 9, 2020 at 5:31 PM Medic <mailiPadpost at gmail.com> wrote:
>
> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
> and applied to MY DATA, but got:
> "Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
> P.S. I can not understand ?as.Date()
>
> SAMPLE CODE
> library(ggplot2)
> library(dplyr)
> library(hrbrthemes)
> data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
> header=T)
>
> str(data)
> 'data.frame': 1822 obs. of  2 variables:
>  $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01" ...
>  $ value: num  136 147 147 140 126 ...
>
> data$date <- as.Date(data$date)
>
> # Plot
> data %>%
>   tail(10) %>%
>   ggplot( aes(x=date, y=value)) +
>     geom_line( color="grey") +
>     geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>     theme_ipsum() +
>     ggtitle("Evolution of bitcoin price")
>
>
> MY DATA
> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
>
> str(mydata)
> 'data.frame': 7 obs. of  2 variables:
>  $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000" ...
>  $ value: int  11 12 13 14 15 16 17
>
> mydata$date <- as.Date(mydata$date)
> Error in as.Date.numeric(mydata$date) : 'origin' must be supplied
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sun May 10 05:03:33 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 May 2020 20:03:33 -0700
Subject: [R] Date format
In-Reply-To: <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
 <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
Message-ID: <F83AF5EA-41B0-45D1-9D75-6596C0C4FD4C@dcn.davis.ca.us>

... but str says it is character. This must be 4.0...

On May 9, 2020 7:17:16 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>$date is a factor, which is coded as numeric values internally, which
>as.date sees as numeric, and therefore:
>"as.Date will accept numeric data (the number of days since an epoch),
>but only if origin is supplied." (from ?as.Date)
>
>You need to supply a format argument to as.Date to get it to handle
>the factor properly; e.g.
>"%d.%m.%Y"  should work. See ?strptime for formatting details.
>
>Bert Gunter
>
>"The trouble with having an open mind is that people keep coming along
>and sticking things into it."
>-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>On Sat, May 9, 2020 at 5:31 PM Medic <mailiPadpost at gmail.com> wrote:
>>
>> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
>> and applied to MY DATA, but got:
>> "Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
>> P.S. I can not understand ?as.Date()
>>
>> SAMPLE CODE
>> library(ggplot2)
>> library(dplyr)
>> library(hrbrthemes)
>> data <-
>read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
>> header=T)
>>
>> str(data)
>> 'data.frame': 1822 obs. of  2 variables:
>>  $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01"
>...
>>  $ value: num  136 147 147 140 126 ...
>>
>> data$date <- as.Date(data$date)
>>
>> # Plot
>> data %>%
>>   tail(10) %>%
>>   ggplot( aes(x=date, y=value)) +
>>     geom_line( color="grey") +
>>     geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>>     theme_ipsum() +
>>     ggtitle("Evolution of bitcoin price")
>>
>>
>> MY DATA
>> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
>>
>> str(mydata)
>> 'data.frame': 7 obs. of  2 variables:
>>  $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000"
>...
>>  $ value: int  11 12 13 14 15 16 17
>>
>> mydata$date <- as.Date(mydata$date)
>> Error in as.Date.numeric(mydata$date) : 'origin' must be supplied
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From bgunter@4567 @end|ng |rom gm@||@com  Sun May 10 06:02:46 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 9 May 2020 21:02:46 -0700
Subject: [R] Date format
In-Reply-To: <F83AF5EA-41B0-45D1-9D75-6596C0C4FD4C@dcn.davis.ca.us>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
 <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
 <F83AF5EA-41B0-45D1-9D75-6596C0C4FD4C@dcn.davis.ca.us>
Message-ID: <CAGxFJbQsOJQEogrGudbpEJ18Cb+srbM6zfxyFqaLkZP8vGVSgA@mail.gmail.com>

True. Whence the error message then?

Still, in my attempt to reproduce, the format statement worked.

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Sat, May 9, 2020 at 8:03 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> ... but str says it is character. This must be 4.0...
>
> On May 9, 2020 7:17:16 PM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> >$date is a factor, which is coded as numeric values internally, which
> >as.date sees as numeric, and therefore:
> >"as.Date will accept numeric data (the number of days since an epoch),
> >but only if origin is supplied." (from ?as.Date)
> >
> >You need to supply a format argument to as.Date to get it to handle
> >the factor properly; e.g.
> >"%d.%m.%Y"  should work. See ?strptime for formatting details.
> >
> >Bert Gunter
> >
> >"The trouble with having an open mind is that people keep coming along
> >and sticking things into it."
> >-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >On Sat, May 9, 2020 at 5:31 PM Medic <mailiPadpost at gmail.com> wrote:
> >>
> >> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
> >> and applied to MY DATA, but got:
> >> "Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
> >> P.S. I can not understand ?as.Date()
> >>
> >> SAMPLE CODE
> >> library(ggplot2)
> >> library(dplyr)
> >> library(hrbrthemes)
> >> data <-
> >read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
> >> header=T)
> >>
> >> str(data)
> >> 'data.frame': 1822 obs. of  2 variables:
> >>  $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01"
> >...
> >>  $ value: num  136 147 147 140 126 ...
> >>
> >> data$date <- as.Date(data$date)
> >>
> >> # Plot
> >> data %>%
> >>   tail(10) %>%
> >>   ggplot( aes(x=date, y=value)) +
> >>     geom_line( color="grey") +
> >>     geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
> >>     theme_ipsum() +
> >>     ggtitle("Evolution of bitcoin price")
> >>
> >>
> >> MY DATA
> >> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
> >>
> >> str(mydata)
> >> 'data.frame': 7 obs. of  2 variables:
> >>  $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000"
> >...
> >>  $ value: int  11 12 13 14 15 16 17
> >>
> >> mydata$date <- as.Date(mydata$date)
> >> Error in as.Date.numeric(mydata$date) : 'origin' must be supplied
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.


From rhur||n @end|ng |rom gwdg@de  Sun May 10 08:22:06 2020
From: rhur||n @end|ng |rom gwdg@de (Rainer Hurling)
Date: Sun, 10 May 2020 08:22:06 +0200
Subject: [R] Date format
In-Reply-To: <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
 <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
Message-ID: <c06795d3-e636-0963-32e3-69418cbc7a2b@gwdg.de>

Am 10.05.20 um 04:17 schrieb Bert Gunter:
> $date is a factor, which is coded as numeric values internally, which
> as.date sees as numeric, and therefore:
> "as.Date will accept numeric data (the number of days since an epoch),
> but only if origin is supplied." (from ?as.Date)

as.Date is also able to read ISO date formatted strings by default (w/o
format).

as.Date("2013-04-28") works, as.Date("28.04.2013") not. The second
example needs as.Date("28.04.2013", format = "%d.%m.%Y").

> 
> You need to supply a format argument to as.Date to get it to handle
> the factor properly; e.g.
> "%d.%m.%Y"  should work. See ?strptime for formatting details.
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> On Sat, May 9, 2020 at 5:31 PM Medic <mailiPadpost at gmail.com> wrote:
>>
>> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
>> and applied to MY DATA, but got:
>> "Error in as.Date.numeric(mydata$date) : 'origin' must be supplied".
>> P.S. I can not understand ?as.Date()
>>
>> SAMPLE CODE
>> library(ggplot2)
>> library(dplyr)
>> library(hrbrthemes)
>> data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv",
>> header=T)
>>
>> str(data)
>> 'data.frame': 1822 obs. of  2 variables:
>>  $ date : chr  "2013-04-28" "2013-04-29" "2013-04-30" "2013-05-01" ...
>>  $ value: num  136 147 147 140 126 ...
>>
>> data$date <- as.Date(data$date)
>>
>> # Plot
>> data %>%
>>   tail(10) %>%
>>   ggplot( aes(x=date, y=value)) +
>>     geom_line( color="grey") +
>>     geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>>     theme_ipsum() +
>>     ggtitle("Evolution of bitcoin price")
>>
>>
>> MY DATA
>> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
>>
>> str(mydata)
>> 'data.frame': 7 obs. of  2 variables:
>>  $ date : chr  "01.01.2000" "02.01.2000" "03.01.2000" "04.01.2000" ...
>>  $ value: int  11 12 13 14 15 16 17
>>
>> mydata$date <- as.Date(mydata$date)
>> Error in as.Date.numeric(mydata$date) : 'origin' must be supplied
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From e@ @end|ng |rom enr|co@chum@nn@net  Sun May 10 08:47:04 2020
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Sun, 10 May 2020 08:47:04 +0200
Subject: [R] 
 Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
In-Reply-To: <CAKPvFjCEmUKBYj6=m0_kt7D8ziw8KhSgdX3_9ncb1SC0+xo1_A@mail.gmail.com>
 (Adrien FABRE's message of "Sun, 10 May 2020 02:13:34 +0200")
References: <CAKPvFjCEmUKBYj6=m0_kt7D8ziw8KhSgdX3_9ncb1SC0+xo1_A@mail.gmail.com>
Message-ID: <87k11kl21j.fsf@enricoschumann.net>

>>>>> "Adrien" == Adrien FABRE <lesgrains at gmail.com> writes:

  Adrien> I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
  Adrien> days ago, and Ubuntu from 18.04 to 20.04 yesterday.

  Adrien> Since then, R sometimes never terminates when executing certain commands:
  Adrien> ivreg (from package AER), summary (of a logit regression) and logitmfx
  Adrien> (from package mfx). Sometimes these commands run fine, but most of the time
  Adrien> I have to kill the process because R won't terminate the execution, even
  Adrien> when pressing the red Stop button in RStudio.

  Adrien> When I tried example('AER'), it worked fine. Then I re-installed the
  Adrien> package AER. It threw 10 warnings of type In readLines(file, skipNul =
  Adrien> TRUE) :  cannot open compressed file
  Adrien> '/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
  Adrien> file or directory' where [package] is abind, colorspace, dichromat... (but
  Adrien> not AER).

  Adrien> Since then example('AER') throws a warning: no help found for ?AER?.

  Adrien> I've removed and reinstalled R 4.0: it didn't help. Besides, the apt purge
  Adrien> r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
  Adrien> removing r-base-core, directory '/usr/lib/R/site-library' not empty so not
  Adrien> removed. Also, there was a bunch of Package [package] is not installed, so
  Adrien> not removed, including for [package] equal to r-cran-abind and the other
  Adrien> listed above (this purge also returned a bunch of Note, selecting [package]
  Adrien> for glob 'r-cran-*').

  Adrien> I have the same bug when using R from the terminal. For the record, I was
  Adrien> probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
  Adrien> can't recall if this issue started after I upgraded R and RStudio (which
  Adrien> would be my best guess) or after I upgraded Ubuntu (a day or two later).

  Adrien> I hope someone can help.

There has been a discussion on R-SIG-Debian recently,
and /perhaps/ it is related to your troubles.

See https://stat.ethz.ch/pipermail/r-sig-debian/2020-April/003159.html
and in particular
https://stat.ethz.ch/pipermail/r-sig-debian/2020-April/003166.html
.


-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From m@|||P@dpo@t @end|ng |rom gm@||@com  Sun May 10 09:15:51 2020
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Sun, 10 May 2020 10:15:51 +0300
Subject: [R] Date format
Message-ID: <CAH6117JToaaoFQssgiuMbiR7d_SEBWK2HWuRqxWFKugRP-Q5KQ@mail.gmail.com>

I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
and applied to MY DATA, but got:
"Don't know how to automatically pick scale for object ..."
P.S. 1) R ver. 4.0 (Yes, Jeff);  2) Attached: mydata_dput (1 ??)

SAMPLE CODE
library(ggplot2)
library(dplyr)
library(hrbrthemes)
data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv
", header=T)

data$date <- as.Date(data$date)

# Plot
data %>%
  tail(10) %>%
  ggplot( aes(x=date, y=value)) +
    geom_line( color="grey") +
    geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
    theme_ipsum() +
    ggtitle("Evolution of bitcoin price")

======
MY DATA
mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")

str(mydata)
'data.frame': 7 obs. of  2 variables:
 $ date : chr  "01.01.2000" "02.01.2000" ...
 $ value: int  11 12 ...

mydata$date <- as.Date(mydata$date, "%d.%m.%Y")

str(mydata$date)
Date[1:7], format: "2000-01-01"

# Bert, thanks for the explanation!
# Rainer, thanks for the specific code!

# And then the problem:
mydata %>%
    tail(10) %>%
    ggplot( aes(x=mydata, y=value)) +
    geom_line( color="grey") +
    geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
    theme_ipsum() +
    ggtitle("Evolution")

"Don't know how to automatically pick scale for object of type
data.frame. Defaulting to continuous.
Error: Aesthetics must be either length 1 or the same as the data (7): x"

From m@rong|u@|u|g| @end|ng |rom gm@||@com  Sun May 10 10:17:47 2020
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Sun, 10 May 2020 10:17:47 +0200
Subject: [R] How to determine whether a value belong to a cumulative
 distribution?
Message-ID: <CAMk+s2RNWxxmMeOX6ZQo_Cko-=R0YpPUOnAar7d=isQp+JatYg@mail.gmail.com>

Hello,
I am trying to translate a mathematical formula into R. The formula (or
rather a  set of formulas) is meant to determine the first outlier in a
sequence of measurements. To do this, a parameter r is calculated; this is
essentially the ratio between the variance of the value x and the sum of
the variances of the x-1 elements of the series. x follows a certain
distribution (namely, sigmoid), whereas r follows a cumulative empirical
one.
The text says:
"Each r is distributed as t under the model. Therefore, we can test the
hypothesis whether a single observation deviates from the model by
comparing r with the t distribution, where F(?) is the cumulative
distribution function of the t distribution:
                                P-value = 2 * [1 ? F(1 ? |r|)]
"
I generated a cumulative function with
```
cum_fun = ecdf(abs(x[1:n])
```
which gives me:
```
> n=3
> Empirical CDF
Call: ecdf(abs(x{1:n])
 x[1:3] = 5.5568, 6.5737, 7.2471
```
But now how can I determine if x belongs to the distribution?
If I do, as in the formula:
```
> p = 2 * (1-cum_fun)
Error in 1 - cum_fun : non-numeric argument to binary operator
```
Can I get a p-value associated with this association?
Thank you

-- 
Best regards,
Luigi

	[[alternative HTML version deleted]]


From kobyeongm|n @end|ng |rom kore@@@c@kr  Sun May 10 10:24:51 2020
From: kobyeongm|n @end|ng |rom kore@@@c@kr (Ko Byeongmin)
Date: Sun, 10 May 2020 10:24:51 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3 - SOLVED
In-Reply-To: <24246.56256.474662.24178@rob.eddelbuettel.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <24246.52361.934223.41992@rob.eddelbuettel.com>
 <24246.56256.474662.24178@rob.eddelbuettel.com>
Message-ID: <1346d8eb-57d4-d7e9-f4a0-4a1cb4e33290@korea.ac.kr>

Dear list,

Dirk and Professor J.C. Nash gave me an invaluable help! Prof. Nash
mentioned:

> Possibly this is a quirk of the particular distro or machine, BLAS or LAPACK[.]
This was indeed the case, and Dirk's suggestion to

> [install] libopenblas-openmp-dev" and
> [remove] both "libopenblas-pthread-dev libopenblas0-pthread"
immediately solved the problem!

Before applying the fix, I took note of my /sessionInfo() /output:

> > sessionInfo()
> R version 3.6.3 (2020-02-29)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 20.04 LTS
>
> Matrix products: default
> BLAS:?? /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3
>
> locale:
> ?[1] LC_CTYPE=en_US.UTF-8?????? LC_NUMERIC=C
> ?[3] LC_TIME=de_DE.UTF-8??????? LC_COLLATE=en_US.UTF-8
> ?[5] LC_MONETARY=de_DE.UTF-8??? LC_MESSAGES=en_US.UTF-8
> ?[7] LC_PAPER=de_DE.UTF-8?????? LC_NAME=C
> ?[9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C
> [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats???? graphics? grDevices utils???? datasets? methods base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.6.3
*Is there anything that I can do, so that others**
**do not experience the same issue?* Writing
a bug report comes into my mind...

In this last paragraph I append some information on
my hardware, as Dirk suggested in his previous mail,
as well as a summary of the software I'm using.

> Operating System: Kubuntu 20.04
> KDE Plasma Version: 5.18.4
> KDE Frameworks Version: 5.68.0
> Qt Version: 5.12.8
> Kernel Version: 5.4.0-28-generic
> OS Type: 64-bit
> Processors: 8 ? Intel? Core? i5-8300H CPU @ 2.30GHz
> Memory: 7,4 GiB of RAM
Again, thanks for all the help! I really appreciate it. :)

Byeongmin

On 09.05.20 18:35, Dirk Eddelbuettel wrote:
> On 9 May 2020 at 10:30, Dirk Eddelbuettel wrote:
> |
> | We can see that you use Linux.
> |
> | Are you by chance
> |
> |  - on a Debian or Ubuntu system, and
> |  - have the libopenblas package installed ?
> |
> | If so then it is a known bug with the libopenblas0-pthread package.
> |
> | Installing libopen0-openmp (and also removing libopenblas0-pthread) should
> | fix it.
>
> That was imprecise. It should read "installing libopenblas-openmp-dev" and
> removing both "libopenblas-pthread-dev libopenblas0-pthread".
>
> I cannot reproduce the bug on the hardware I have access to (i5, i7, xeon)
> though I was able to a few weeks ago. Maybe something changed already...
>
> There was also a brief thread on the debian-science list (within Debian)
> starting with https://lists.debian.org/debian-science/2020/04/msg00081.html
> and leading to https://lists.debian.org/debian-science/2020/05/msg00003.html
> (and this cross from April to May)
>
> We would need some more information on hardware to chase this.
>
> Dirk
>   
> | This is likely CPU dependent. But we need more info. Can you maybe come to the
> | r-sig-debian list (subscription needed to reduce spam) and we continue there?
> |
> | Dirk
> |
> |
> | --
> | http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
> |
> | ______________________________________________
> | R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> | https://stat.ethz.ch/mailman/listinfo/r-help
> | PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> | and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@|||P@dpo@t @end|ng |rom gm@||@com  Sun May 10 10:27:51 2020
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Sun, 10 May 2020 11:27:51 +0300
Subject: [R] Date format
Message-ID: <CAH6117JohtWqCOs-EAXXyU0rWrkFNUdsxcCj-UyoAFo=doH_Sw@mail.gmail.com>

Many Thanks!!!
> cpolwart at chemo.org.uk:
> Your X axis is plotting mydata not date?
> Use aes(x=date


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun May 10 11:54:01 2020
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 10 May 2020 10:54:01 +0100
Subject: [R] 
 Warning in install.packages : converting NULL pointer to R NULL
In-Reply-To: <3e3568fa-3b32-0d33-0418-12026bce7628@auckland.ac.nz>
References: <bbd798c8-eaa7-1b8d-cb39-7e8926600520@sapo.pt>
 <3e3568fa-3b32-0d33-0418-12026bce7628@auckland.ac.nz>
Message-ID: <53adb45b-5c8d-6d37-434a-7625493bf2ad@sapo.pt>

Hello,

Thanks for the info.
It was an update issue, I updated to Ubuntu 20.04 and R to 4.0.0 but not 
RStudio. Now that I have all three most recent versions it works as 
before, with no strange warning message.
So I guess this thread is closed.

Thanks to all,

Rui Barradas

?s 23:11 de 08/05/20, Rolf Turner escreveu:
> 
> Hi Rui.? Doesn't happen to me under Ubuntu 18.04:
> 
>> install.packages('cowplot',lib=.Rlib) trying URL 
>> 'https://cloud.r-project.org/src/contrib
>> /cowplot_1.0.0.tar.gz'
>> Content type 'application/x-gzip' length 1275585 bytes (1.2 MB)
>> ==================================================
>> downloaded 1.2 MB
>>
>> * installing *source* package ?cowplot? ...
>> ** package ?cowplot? successfully unpacked and MD5 sums checked
>> ** using staged installation
>> ** R
>> ** inst
>> ** byte-compile and prepare package for lazy loading
>> ** help
>> *** installing help indices
>> *** copying figures
>> ** building package indices
>> ** installing vignettes
>> ** testing if installed package can be loaded from temporary location
>> ** testing if installed package can be loaded from final location
>> ** testing if installed package keeps a record of temporary 
>> installation path
>> * DONE (cowplot)
>>
>> The downloaded source packages are in
>> ?????/tmp/RtmpG8wb97/downloaded_packages?
> 
> My session info is as follows:
> 
>> sessionInfo()
>> R version 4.0.0 (2020-04-24)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>> Running under: Ubuntu 18.04.4 LTS
>>
>> Matrix products: default
>> BLAS:?? /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
>> LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3
>>
>> Random number generation:
>> ?RNG:???? Mersenne-Twister ?Normal:? Inversion ?Sample:? Rounding
>> locale:
>> ?[1] LC_CTYPE=en_GB.UTF-8?????? LC_NUMERIC=C ?[3] 
>> LC_TIME=en_NZ.UTF-8??????? LC_COLLATE=en_GB.UTF-8 ?[5] 
>> LC_MONETARY=en_NZ.UTF-8??? LC_MESSAGES=en_GB.UTF-8 ?[7] 
>> LC_PAPER=en_NZ.UTF-8?????? LC_NAME=C ?[9] LC_ADDRESS=C               
>> LC_TELEPHONE=C [11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C
>> attached base packages:
>> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
>> other attached packages:
>> [1] brev_0.0-4
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_4.0.0 tools_4.0.0 
> 
> No idea what to suggest.? Sorry.
> 
> cheers,
> 
> Rolf
>


From kry|ov@r00t @end|ng |rom gm@||@com  Sun May 10 14:02:43 2020
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sun, 10 May 2020 15:02:43 +0300
Subject: [R] How to determine whether a value belong to a cumulative
 distribution?
In-Reply-To: <CAMk+s2RNWxxmMeOX6ZQo_Cko-=R0YpPUOnAar7d=isQp+JatYg@mail.gmail.com>
References: <CAMk+s2RNWxxmMeOX6ZQo_Cko-=R0YpPUOnAar7d=isQp+JatYg@mail.gmail.com>
Message-ID: <20200510150243.0ce1b940@trisector>

On Sun, 10 May 2020 10:17:47 +0200
Luigi Marongiu <marongiu.luigi at gmail.com> wrote:

>If I do, as in the formula:
>```
>> p = 2 * (1-cum_fun)  
>Error in 1 - cum_fun : non-numeric argument to binary operator
>```

The ecdf function returns another function that calculates the ECDF
value for an arbitrary input. For example,

e <- ecdf(1:10)
e
# Empirical CDF
# Call: ecdf(1:10)
#  x[1:10] =      1,      2,      3,  ...,      9,     10
e(c(-1, 5, 100)) # call the returned value as a function
# [1] 0.0 0.5 1.0

If you want to see the empirical distribution function values for the
points of the dataset itself, call the function returned by ecdf with
the same data again:

x <- 1:10
ecdf(x)(x)
# [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

If you want to calculate the CDF for a given value of 1 ? |r|, pass
this value as an argument to the function returned by ecdf:

cum_fun <- ecdf(abs(x[1:n])
p <- 2 * (1 - cum_fun(1 - abs(r)))

On the other hand, given the quotes from the text, I think than you
might need to use the theoretical t distribution function (available as
`dt` in R) in the formula instead of ECDF:

df <- ... # degrees of freedom for Student t distribution
p <- 2 * (1 - dt(1 - abs(r), df))

I am not sure about that, though.

-- 
Best regards,
Ivan


From btupper @end|ng |rom b|ge|ow@org  Sun May 10 16:05:00 2020
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Sun, 10 May 2020 10:05:00 -0400
Subject: [R] the volcano orientation
In-Reply-To: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
Message-ID: <CALrbzg3AGw7fY10MDb4RY39=htcijhQVppJzf1815Cs_4ubtjQ@mail.gmail.com>

Hi,

Hmmm.  The only place I have ever seen a georeferenced version of 'volcano'
is here...

https://waterdata.usgs.gov/blog/inlmiscmaps/

It was on the internet so I assumed it was true. Now, I suspect that, since
the original survey by Ross Ihaka, continental drift is happening waaaay
faster than anyone guessed. Could be a decent grant proposal somewhere in
all this.

Seriously, though, I haven't any idea why 'volcano' is the way it is shown,
nor was I awake enough to actually look at a map as you have done. I would
love-love-love to see a georeferenced version be part of the stars package
as example data. It's small enough to be lightweight but has enough
information in it to be handy for meaningful demonstrations. Maybe along
the lines of ...

https://gist.github.com/btupper/8e8eb8c0ebf4402a3f87b5638eca954a

... but with the correct spatial info.

Cheers,
Ben

On Sat, May 9, 2020 at 8:44 PM Michael Sumner <mdsumner at gmail.com> wrote:

> Does anyone know why 'volcano' is oriented as it is?
>
> image(volcano)  ## filled.contour is the same
>
> I know it's all arbitrary, but north-up is a pretty solid convention. Is
> there any reason why the classic 'image()' example data set would not
> default to this orientation?
>
> A Google map of the site (in Web Mercator):
>
>
> https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
>
>
> For image(), the north-up orientation is 't(volcano[,ncol(volcano):1])'.
>
> If you are interested in a roughly georeferenced version I have code here:
>
> https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
>
> (Also see fortunes::fortune("conventions") )
>
> Best, Mike
>
>
> --
> Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> Hobart, Australia
> e-mail: mdsumner at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
East Boothbay, Maine
http://www.bigelow.org/
https://eco.bigelow.org

	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Sun May 10 23:03:23 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Mon, 11 May 2020 09:03:23 +1200
Subject: [R] the volcano orientation
In-Reply-To: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
Message-ID: <CAB8pepxEtn9bsMkrRjMN=Rm=LvgzfP3_7Sz=1RjeGcCTyDRPwQ@mail.gmail.com>

> Does anyone know why 'volcano' is oriented as it is?
> image(volcano)  ## filled.contour is the same

Great question!

graphics::image produces a "plot".
It follows the same x y conventions as other plots in the graphics package.
It's *defaults* are not designed to display photos, etc.

However, the format of the volcano data is not consistent with either
the defaults of graphics::image or what I (personally) would expect in
a photographic data, with top-left value at top-left of matrix and
bottom-right point at bottom-right of matrix, but that's debatable...

According, the documentation of the volcano data:

    A matrix with 87 rows and 61 columns,
    rows corresponding to grid lines running east to west
    and columns to grid lines running south to north.

Perhaps that could be improved slightly...?

And one more thing that caught me out.
My initial expectation (using a simple interpretation) was the data
would need to be transposed and then either the ylim reversed or the
rows reversed.
But when I tried to plot the volcano data using my own function (which
does just that), I got the wrong result.

But in the documentation for graphics::image we have:

    "Need to transpose and flip"
    image(t(volcano)[ncol(volcano):1,])

Which produces the right result.

I had to think about this for a while...

The example for graphics::image above is actually transposing the
matrix *twice*.
First in the input to the function, and then again (implicitly), where
rows (going down the data) are interpreted as x (going right across
the plot).


From @purd|e@@ @end|ng |rom gm@||@com  Sun May 10 23:21:10 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Mon, 11 May 2020 09:21:10 +1200
Subject: [R] the volcano orientation
In-Reply-To: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
Message-ID: <CAB8pepzqxtCyDhhcMdmmJjOXRXhxxvrc2vcAYbfUrxhUU8Xv3w@mail.gmail.com>

Sorry, one more thing.
My response didn't really answer your question.
But I would say that the formats of most datasets used in statistics
are reflective of the preferences of the people that collected or
published them, at the time...

Also, I've found the older publications quite often have considerable merit...

On Sun, May 10, 2020 at 12:44 PM Michael Sumner <mdsumner at gmail.com> wrote:
>
> Does anyone know why 'volcano' is oriented as it is?
>
> image(volcano)  ## filled.contour is the same
>
> I know it's all arbitrary, but north-up is a pretty solid convention. Is
> there any reason why the classic 'image()' example data set would not
> default to this orientation?
>
> A Google map of the site (in Web Mercator):
>
> https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
>
>
> For image(), the north-up orientation is 't(volcano[,ncol(volcano):1])'.
>
> If you are interested in a roughly georeferenced version I have code here:
>
> https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
>
> (Also see fortunes::fortune("conventions") )
>
> Best, Mike
>
>
> --
> Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> Hobart, Australia
> e-mail: mdsumner at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |r@|nj @end|ng |rom gm@||@com  Sun May 10 23:44:17 2020
From: |r@|nj @end|ng |rom gm@||@com (John C Frain)
Date: Sun, 10 May 2020 22:44:17 +0100
Subject: [R] predicting waste per capita - is a gaussian model correct?
In-Reply-To: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
References: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
Message-ID: <CAHrK516nu-TmMJq=Rzp1ddYr=RAnX41k8=aPfhgE4ZJNFbf88A@mail.gmail.com>

On Sun, 10 May 2020 at 02:00, Alessandra Bielli <bielli.alessandra at gmail.com>
wrote:

> Dear list,
>
> I am new to this list and I hope it is ok to post here even though I
> already posted this question on Cross Validated.
>
> I am trying to predict the daily amount of waste per person produced in the
> fishery sector. We surveyed fishing boats at the end of their fishing trip
> and the variables I have are duration of trip (days), number of fishers,
> waste category and waste weight (g), boat ID.
>
> For each fishing trip I calculated grams of waste per person per day, i.e.
> daily waste per capita. To predict daily waste per capita, I am using a
> gaussian mixed effect model with log(waste per capita) as response variable
> (I transformed it cause it was not normally distributed - and I'm not sure
> it's correct to do so). Explanatory variable is waste category and boat ID
> is a random effect. I use the predict function to estimate daily waste per
> capita for each category and then back transformed it with exp(...).
>
> My question is: is it correct to transform daily weight per capita to fit a
> gaussian model?
>
> Thanks so much for your advice!
>
> Alessandra
>
There is no requirement that the dependent variable in a "regression" type
estimation follows a gaussian distribution.  You need a model of the
process and then use an estimation technique to estimate your model.  If
effects in your model are additive do not use a log transformation. If
effects are multiplicative then use a log transformation.  The choice
should be determined by the mechanics of the problem and not by the
statistics.  If you do use a log transformation the trying to reverse the
process using an exponential transformation will be biased.  The extent of
that bias depends on your problem and it would not be possible to estimate
the significance of the bias without a much greater knowledge of the
process and data.  I would suggest that you consult a competent
statistician.

John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

	[[alternative HTML version deleted]]


From h@r@h|t@khedk@r @end|ng |rom gm@||@com  Sun May 10 08:27:53 2020
From: h@r@h|t@khedk@r @end|ng |rom gm@||@com (Harshita Khedkar)
Date: Sun, 10 May 2020 14:27:53 +0800
Subject: [R] Unable to install Sequin R
Message-ID: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>

Hello,

This is Harshita here,

I am trying to install Sequin R and could not able to get it to work.
Please find the attached error message, and let me know how o go ahead
about it.

Thank you and best regards,
Harshita Khedkar.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 2020-05-10 (1).png
Type: image/png
Size: 190215 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200510/6653f569/attachment.png>

From drj|m|emon @end|ng |rom gm@||@com  Mon May 11 00:40:32 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 11 May 2020 08:40:32 +1000
Subject: [R] Unable to install Sequin R
In-Reply-To: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>
References: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>
Message-ID: <CA+8X3fU9gVBOvsvB7CVQ=DMLG1Tf-drOtOxrCotqR8U8Qptbng@mail.gmail.com>

Hi Harshita,
I think you are trying to install the package "seqinr" (Biological
Sequences Retrieval and Analysis).

1) When you see something like <packagename> in the help pages, it
means "Insert the name of the package here". So you really want:

install.packages("seqinr")

2) Both spelling and capitalization matter in R. From your screenshot
you seem to be using Windows, where capitalization often doesn't
matter. So you have to spell words correctly and use the correct case.

Jim

On Mon, May 11, 2020 at 8:31 AM Harshita Khedkar
<harshitakhedkar at gmail.com> wrote:
>
> Hello,
>
> This is Harshita here,
>
> I am trying to install Sequin R and could not able to get it to work.
> Please find the attached error message, and let me know how o go ahead
> about it.
>
> Thank you and best regards,
> Harshita Khedkar.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Mon May 11 00:56:29 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Mon, 11 May 2020 10:56:29 +1200
Subject: [R] predicting waste per capita - is a gaussian model correct?
In-Reply-To: <CAHrK516nu-TmMJq=Rzp1ddYr=RAnX41k8=aPfhgE4ZJNFbf88A@mail.gmail.com>
References: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
 <CAHrK516nu-TmMJq=Rzp1ddYr=RAnX41k8=aPfhgE4ZJNFbf88A@mail.gmail.com>
Message-ID: <CAB8pepwn-cP8abZ=1Jw7yKnd8z+5Er1MNSUpR7ZD+SSNkaPCOA@mail.gmail.com>

Well, this is 100% off-topic...
And I wasn't planning to answer the OP's question.

However, I disagree with your answer.

> There is no requirement that the dependent variable in a "regression" type
> estimation follows a gaussian distribution.

False.
It's depends on what type of '"regression" type estimation' one uses,
among other things.

> You need a model of the
> process and then use an estimation technique to estimate your model.  If
> effects in your model are additive do not use a log transformation. If
> effects are multiplicative then use a log transformation.

The main question is, does the model satisfy the *assumptions*.

> The choice
> should be determined by the mechanics of the problem and not by the
> statistics.

While a mechanistic understanding is definitely valuable...
If the criteria for a good model vs a bad model, was whether the model
was consistent with mechanistic theory/understanding, then nearly
every statistical model I've seen would be a bad model.
I would say, a good model is one that is useful...

> If you do use a log transformation the trying to reverse the
> process using an exponential transformation will be biased.
> The extent of
> that bias depends on your problem and it would not be possible to estimate
> the significance of the bias without a much greater knowledge of the
> process and data.

Never heard of this before...
But I do note back-transformation is not trivial, and I'm not an
expert on back-transformations.

> I would suggest that you consult a competent
> statistician.

I agree on that part...


From twoo|m@n @end|ng |rom ont@rgettek@com  Mon May 11 01:48:04 2020
From: twoo|m@n @end|ng |rom ont@rgettek@com (Tom Woolman)
Date: Sun, 10 May 2020 19:48:04 -0400
Subject: [R] random forest significance testing tools
In-Reply-To: <c06795d3-e636-0963-32e3-69418cbc7a2b@gwdg.de>
References: <CAH6117+6Yw3kcUcdtNKJA2xE0=LC11QYXYB1Yax6ThH9gqTaDA@mail.gmail.com>
 <CAGxFJbSaLaF4uak1rA+qFpkCuuusidhWHLrtJVYoK300nSiHUA@mail.gmail.com>
 <c06795d3-e636-0963-32e3-69418cbc7a2b@gwdg.de>
Message-ID: <20200510194804.Horde.jcEhxIUy-BDAU7hGYgio9KH@www.ontargettek.com>

Hi everyone. I'm using a random forest in R to successfully perform a  
classification on a dichotomous DV in a dataset that has 29 IVs of  
type double and approximately 285,000 records. I ran my model on a  
70/30 train/test split of the original dataset.

I'm trying to use the rfUtilities package for rf model selection and  
performance evaluation, in order to generate a p-value and other  
quantitative performance statistics for use in hypothesis testing,  
similar to what I would do with a logistic regression glm model.

The initial random forest model results and OOB error estimates were  
as follows:

randomForest(formula = Class ~ ., data = train)
                Type of random forest: classification
                      Number of trees: 500
No. of variables tried at each split: 5

         OOB estimate of  error rate: 0.04%
Confusion matrix:
        0   1  class.error
0 199004  16 8.039393e-05
1     73 271 2.122093e-01


I'm running this model on my laptop (Win10, 8 GB RAM) as I don't have  
access to my server during the pandemic. The rfUtilities function call  
works (or at least it doesn't give me an error message or crash), but  
it's been running for over a day in RStudio on the original rf model  
and the training dataset without providing any results.

For anyone who has used the rfUtilities package before, is this just  
too large of a dataframe for a Win10 laptop to process effectively or  
should I be doing something different? This is my first time using the  
rfUtilities package and I understand that it is relatively new.

The function call for the rfUtilities function rf.significance is as  
follows (rf is my original random forest data model from the  
randomForest function):

rf.perm <- rf.significance(rf, train[,1:29], nperm=99, ntree=500)


Thanks in advance.

Tom Woolman
PhD student, Indiana State University


From r@oknz @end|ng |rom gm@||@com  Mon May 11 03:56:49 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Mon, 11 May 2020 13:56:49 +1200
Subject: [R] the volcano orientation
In-Reply-To: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
Message-ID: <CABcYAdK4U7K_FwM=w04k5dK8rbbT5uKo-WyHBMmcBUQuaNwBYA@mail.gmail.com>

Hey, I know that volcano!  It's walking distance from the Intermediate
school I attended.
To you it's a plot; to me it's a place.
So I offer you four scenarios.

1. You think of it as a place you know and have been.
    In that case the "right" orientation is the one that best matches
what you are used to seeing.
    For me, that would put the peak on the right of the plot.

2. You think of it as a patch in a map.
    In that case the "right:" orientation is the one that matches the map.
    That would put the peak at the bottom of the plot.

3. You think of it as a product of geological processes, and are
perhaps interested in
    whether there is any connection between the orientation of the
volcano and the
    direction the Auckland hot-spot (currently at White Island) was moving.
    In that case you'd choose south-west -> north-east as the primary axis.
    (I think.  Not really sure.)

4. You think of it as a picture, an illustration in a textbook.  It
might need to be cropped
    vertically so you can fit another illustration on the same page.
For that and
    perceptual reasons you want the major linear axis of the image to
be  horizontal.
    In that case, what we have now is a perfectly reasonable choice.

"Quality is fitness for use."

On Sun, 10 May 2020 at 12:44, Michael Sumner <mdsumner at gmail.com> wrote:
>
> Does anyone know why 'volcano' is oriented as it is?
>
> image(volcano)  ## filled.contour is the same
>
> I know it's all arbitrary, but north-up is a pretty solid convention. Is
> there any reason why the classic 'image()' example data set would not
> default to this orientation?
>
> A Google map of the site (in Web Mercator):
>
> https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
>
>
> For image(), the north-up orientation is 't(volcano[,ncol(volcano):1])'.
>
> If you are interested in a roughly georeferenced version I have code here:
>
> https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
>
> (Also see fortunes::fortune("conventions") )
>
> Best, Mike
>
>
> --
> Michael Sumner
> Software and Database Engineer
> Australian Antarctic Division
> Hobart, Australia
> e-mail: mdsumner at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Mon May 11 08:09:43 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Mon, 11 May 2020 16:09:43 +1000
Subject: [R] Unable to install Sequin R
In-Reply-To: <CAFV24eHNdc_cm97D8iaGVW-R2TMRBwoDfkQmmaWKt+6x6_AMpg@mail.gmail.com>
References: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>
 <CA+8X3fU9gVBOvsvB7CVQ=DMLG1Tf-drOtOxrCotqR8U8Qptbng@mail.gmail.com>
 <CAFV24eHNdc_cm97D8iaGVW-R2TMRBwoDfkQmmaWKt+6x6_AMpg@mail.gmail.com>
Message-ID: <CA+8X3fWFcS8it5iQjcwXj_MquKE5OHF6JEkaeH1j6wfVfyVk-A@mail.gmail.com>

Hi Harshita,
This usually means that R-4.0.0 is sufficiently different from R-3.6.3
that seqinr no longer works in R-4.0.0. You may want to go back to
R-3.6.3 if you really need it. You may also want to contact Simon
Penel, the maintainer (see the seqinr package page on CRAN) for
information on when an updated version will appear.

Jim

On Mon, May 11, 2020 at 3:31 PM Harshita Khedkar
<harshitakhedkar at gmail.com> wrote:
>
> Dear Jim,
>
> Thank you for your email, I tried the steps given by you and received the message as sequinr is not available for R version 4.0.
>
> Please find the attached screenshot.
> Please let me know how to proceed further.
>
>
> Thanks & Regards
>
>
> On Mon, May 11, 2020 at 6:40 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Harshita,
>> I think you are trying to install the package "seqinr" (Biological
>> Sequences Retrieval and Analysis).
>>
>> 1) When you see something like <packagename> in the help pages, it
>> means "Insert the name of the package here". So you really want:
>>
>> install.packages("seqinr")
>>
>> 2) Both spelling and capitalization matter in R. From your screenshot
>> you seem to be using Windows, where capitalization often doesn't
>> matter. So you have to spell words correctly and use the correct case.
>>
>> Jim
>>
>> On Mon, May 11, 2020 at 8:31 AM Harshita Khedkar
>> <harshitakhedkar at gmail.com> wrote:
>> >
>> > Hello,
>> >
>> > This is Harshita here,
>> >
>> > I am trying to install Sequin R and could not able to get it to work.
>> > Please find the attached error message, and let me know how o go ahead
>> > about it.
>> >
>> > Thank you and best regards,
>> > Harshita Khedkar.
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Mon May 11 09:13:01 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 11 May 2020 07:13:01 +0000
Subject: [R] unstable results of nlxb fit
In-Reply-To: <8cfa9396-7a22-ba80-b8be-48dd935fe408@gmail.com>
References: <745db2c879ba43149ff7e1f009219cca@SRVEXCHCM1302.precheza.cz>
 <16e4ad6a-12dd-ccfe-0b54-403be54d8b6e@gmail.com>
 <755635422.1450418.1588887698767@connect.xfinity.com>
 <8cfa9396-7a22-ba80-b8be-48dd935fe408@gmail.com>
Message-ID: <86f8ef8eb8ee48ec987cadea8ece3e0a@SRVEXCHCM1302.precheza.cz>

Dear all.

Thank you for your answers. I will try Duncan's approach (if I could manage 
it).

The issue is that first part of my data (actually temperature) up to certain 
time approximately follow one exponential. After that, another process 
prevails and the temperature increase starts to be "explosive". That is why I 
used these two exponentials. As I have many experiments I wanted to perform 
the fit programmatically.

Which leads me to the approach that in each cycle I perform a plot which I 
visually inspect. If I consider the fit satisfactory I keep results. If not, I 
perform the fit with different starting values until it is OK. I am aware that 
it is not optimal but should be easiest.

Thank you again.

Best regards
Petr

> -----Original Message-----
> From: J C Nash <profjcnash at gmail.com>
> Sent: Friday, May 8, 2020 12:00 AM
> To: Bernard McGarvey <mcgarvey.bernard at comcast.net>; PIKAL Petr
> <petr.pikal at precheza.cz>; r-help <r-help at r-project.org>
> Subject: Re: [R] unstable results of nlxb fit
>
> These results reflect my experience with this sort of problem.
>
> A couple of comments:
>
> 1) optimx package has a multistart wrapper. I probably should have written
> one for nlsr. Maybe Bernard and I should work on that. The issues are 
> largely
> to make things resistant to silly inputs, which even the smart users (you 
> know,
> the ones looking back from the mirror) introduce.
>
> 2) Sometimes using the bounds constraint capability in nlsr can be helpful,
> e.g., to ensure the exponent parameters are kept apart, can be useful.
>
> 3) Combining with Duncan's suggestion of solving for the linear parameters
> also helps.
>
> All of the above can be sensitive to particular data.
>
> Best, JN
>
> On 2020-05-07 5:41 p.m., Bernard McGarvey wrote:
> > John/Petr, I think there is an issue between a global optimum and local
> optima. I added a multistart loop around the code to see if I could find
> different solutions. Here is the code I added (I am not a great coder so 
> please
> excuse any inefficiencies in this code segment):
> >
> > # Multistart approach
> > NT <- 100
> > Results <- matrix(data=NA, nrow = NT, ncol=5,
> > dimnames=list(NULL,c("SS", "A", "B", "a", "b")))
> > A1 <- runif(NT,0,100)
> > B1 <- runif(NT,0,100)
> > a1 <- runif(NT,0.0,0.1)
> > b1 <- runif(NT,0.0,0.1)
> > for (I in 1:NT) {
> >   if (A1[I] > B1[I]) { # Ensure that the A'a are always the lower so that 
> > nlxb()
> always converge to the same values
> >     A0 <- A1[I]
> >     a0 <- a1[I]
> >     A1[I] <- B1[I]
> >     a1[I] <- b1[I]
> >     B1[I] <- A0
> >     b1[I] <- a0
> >   }
> >   fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> >               start=list(A=A1[I], B=B1[I], a=a1[I], b=b1[I]))
> >   ccc <- coef(fit)
> >   Results[I,1] <- fit$ssquares
> >   Results[I,2] <- ccc[1]
> >   Results[I,3] <- ccc[2]
> >   Results[I,4] <- ccc[3]
> >   Results[I,5] <- ccc[4]
> > }
> > Results
> >
> > What I found is that the minimum SS generated at each trial had two
> distinct values, 417.8 and 3359.2. The A,B,a, and b values when the SS was
> 417.8 were all the same but I got different values for the case where the
> minimal SS was 3359.2. This indicates that the SS=417.8 may be the global
> minimum solution whereas the others are local optima. Here are the iteration
> results for a 100 trial multistart:
> >
> > Results
> >            SS           A           B           a           b
> >   [1,] 3359.2  8.3546e+03  6.8321e+00   -1.988226  2.6139e-02
> >   [2,] 3359.2  8.2865e+03  6.8321e+00   -5.201735  2.6139e-02
> >   [3,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >   [4,] 3359.2  6.8321e+00  7.7888e+02    0.026139 -7.2812e-01
> >   [5,] 3359.2 -3.9020e+01  4.5852e+01    0.026139  2.6139e-02
> >   [6,] 3359.2  6.8321e+00  2.6310e+02    0.026139 -1.8116e+00
> >   [7,] 3359.2 -2.1509e+01  2.8341e+01    0.026139  2.6139e-02
> >   [8,] 3359.2 -3.8075e+01  4.4908e+01    0.026139  2.6139e-02
> >   [9,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [10,] 3359.2  1.2466e+04  6.8321e+00   -4.196000  2.6139e-02
> >  [11,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [12,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [13,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [14,] 3359.2  3.8018e+02  6.8321e+00   -0.806414  2.6139e-02
> >  [15,] 3359.2 -3.1921e+00  1.0024e+01    0.026139  2.6139e-02
> >  [16,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [17,] 3359.2 -1.5938e+01  2.2770e+01    0.026139  2.6139e-02
> >  [18,] 3359.2 -3.1205e+01  3.8037e+01    0.026139  2.6139e-02
> >  [19,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [20,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [21,] 3359.2  8.6627e+03  6.8321e+00   -3.319778  2.6139e-02
> >  [22,] 3359.2  6.8321e+00  1.9318e+01    0.026139 -6.5773e-01
> >  [23,] 3359.2  6.2991e+01 -5.6159e+01    0.026139  2.6139e-02
> >  [24,] 3359.2  2.8865e-03  6.8321e+00   -1.576307  2.6139e-02
> >  [25,] 3359.2 -1.2496e+01  1.9328e+01    0.026139  2.6139e-02
> >  [26,] 3359.2 -5.9432e+00  1.2775e+01    0.026139  2.6139e-02
> >  [27,] 3359.2  1.6884e+02  6.8321e+00 -211.866423  2.6139e-02
> >  [28,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [29,] 3359.2  5.4972e+03  6.8321e+00   -3.432094  2.6139e-02
> >  [30,] 3359.2  6.8321e+00  1.4427e+03    0.026139 -4.2771e+02
> >  [31,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [32,] 3359.2  3.5760e+01 -2.8928e+01    0.026139  2.6139e-02
> >  [33,] 3359.2  6.8321e+00 -4.0737e+02    0.026139 -6.7152e-01
> >  [34,] 3359.2  6.8321e+00  1.2638e+04    0.026139 -2.8070e+00
> >  [35,] 3359.2  1.1813e+01 -4.9807e+00    0.026139  2.6139e-02
> >  [36,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [37,] 3359.2  6.8321e+00  1.2281e+03    0.026139 -3.0702e+02
> >  [38,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [39,] 3359.2 -2.6850e+01  3.3682e+01    0.026139  2.6139e-02
> >  [40,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [41,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [42,] 3359.2 -2.3279e+01  3.0111e+01    0.026139  2.6139e-02
> >  [43,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [44,] 3359.2  6.8321e+00  1.4550e+03    0.026139 -4.0303e+00
> >  [45,] 3359.2 -1.1386e+01  1.8218e+01    0.026139  2.6139e-02
> >  [46,] 3359.2  8.8026e+02  6.8321e+00  -65.430608  2.6139e-02
> >  [47,] 3359.2 -8.1985e+00  1.5031e+01    0.026139  2.6139e-02
> >  [48,] 3359.2 -6.7767e+00  1.3609e+01    0.026139  2.6139e-02
> >  [49,] 3359.2 -1.1436e+01  1.8268e+01    0.026139  2.6139e-02
> >  [50,] 3359.2  1.0710e+04  6.8321e+00   -2.349659  2.6139e-02
> >  [51,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [52,] 3359.2  6.8321e+00  7.1837e+02    0.026139 -7.4681e-01
> >  [53,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [54,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [55,] 3359.2 -4.8774e+00  6.8321e+00  -16.405584  2.6139e-02
> >  [56,] 3359.2  1.2687e+03  6.8321e+00   -3.775998  2.6139e-02
> >  [57,] 3359.2  1.5529e+01 -8.6967e+00    0.026139  2.6139e-02
> >  [58,] 3359.2 -1.0003e+01  1.6835e+01    0.026139  2.6139e-02
> >  [59,] 3359.2  6.8321e+00  3.9291e+02    0.026139 -4.1974e+02
> >  [60,] 3359.2 -2.1880e+01  2.8712e+01    0.026139  2.6139e-02
> >  [61,] 3359.2  4.1736e+03  6.8321e+00  -10.711457  2.6139e-02
> >  [62,] 3359.2 -3.3185e+01  4.0017e+01    0.026139  2.6139e-02
> >  [63,] 3359.2  7.6732e+02  6.8321e+00   -0.723977  2.6139e-02
> >  [64,] 3359.2  1.5334e+04  6.8321e+00  -52.573620  2.6139e-02
> >  [65,] 3359.2 -2.9556e+01  3.6388e+01    0.026139  2.6139e-02
> >  [66,] 3359.2 -1.0447e+00  7.8767e+00    0.026139  2.6139e-02
> >  [67,] 3359.2  6.8321e+00  2.1471e+02    0.026139 -7.0582e+01
> >  [68,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [69,] 3359.2 -2.2293e+01  2.9126e+01    0.026139  2.6139e-02
> >  [70,] 3359.2  6.2259e+02  6.8321e+00   -2.782527  2.6139e-02
> >  [71,] 3359.2 -1.4639e+01  2.1471e+01    0.026139  2.6139e-02
> >  [72,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [73,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [74,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [75,] 3359.2 -2.3449e+01  3.0281e+01    0.026139  2.6139e-02
> >  [76,] 3359.2 -2.5926e+01  6.8321e+00   -0.663656  2.6139e-02
> >  [77,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [78,] 3359.2  6.8321e+00  6.9426e+02    0.026139 -1.9442e+00
> >  [79,] 3359.2  2.8684e+02  6.8321e+00   -0.854394  2.6139e-02
> >  [80,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [81,] 3359.2 -4.5066e+01  5.1899e+01    0.026139  2.6139e-02
> >  [82,] 3359.2  4.4678e+03  6.8321e+00   -2.109446  2.6139e-02
> >  [83,] 3359.2  3.1376e+03  6.8321e+00   -1.104803  2.6139e-02
> >  [84,] 3359.2  6.8321e+00  1.1167e+02    0.026139 -1.0280e+00
> >  [85,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [86,] 3359.2  5.3864e+02  6.8321e+00   -0.657971  2.6139e-02
> >  [87,] 3359.2  4.8227e+01  6.8321e+00   -2.304024  2.6139e-02
> >  [88,] 3359.2 -2.2048e+01  2.8880e+01    0.026139  2.6139e-02
> >  [89,]  417.8  3.9452e-13  9.7727e+00    0.280227  2.1798e-02
> >  [90,] 3359.2  6.8321e+00 -4.1689e+01    0.026139 -3.6049e+00
> >  [91,]  417.8  9.7727e+00  3.9452e-13    0.021798  2.8023e-01
> >  [92,] 3359.2 -4.1265e+01  4.8097e+01    0.026139  2.6139e-02
> >  [93,] 3359.2 -1.1565e+01  1.8397e+01    0.026139  2.6139e-02
> >  [94,] 3359.2  2.3698e+01 -1.6866e+01    0.026139  2.6139e-02
> >  [95,] 3359.2  4.4700e+03  6.8321e+00  -12.836180  2.6139e-02
> >  [96,] 3359.2  4.6052e+04  6.8321e+00   -7.158584  2.6139e-02
> >  [97,] 3359.2  2.5464e+03  6.8321e+00   -1.811626  2.6139e-02
> >  [98,] 3359.2  6.8321e+00  1.0338e+03    0.026139 -1.5365e+01
> >  [99,] 3359.2  1.3783e+01 -6.9507e+00    0.026139  2.6139e-02
> > [100,] 3359.2  6.8321e+00  6.7153e+02    0.026139 -1.5975e+03
> >
> >
> > Hope this helps,
> >
> > Bernard McGarvey
> >
> >
> > Director, Fort Myers Beach Lions Foundation, Inc.
> >
> >
> > Retired (Lilly Engineering Fellow).
> >
> >> On May 7, 2020 at 9:33 AM J C Nash <profjcnash at gmail.com> wrote:
> >>
> >>
> >> The double exponential is well-known as a disaster to fit. Lanczos in
> >> his
> >> 1956 book Applied Analysis, p. 276 gives a good example which is worked
> through.
> >> I've included it with scripts using nlxb in my 2014 book on Nonlinear
> >> Parameter Optimization Using R Tools (Wiley). The scripts were on
> >> Wiley's site for the book, but I've had difficulty getting Wiley to
> >> fix things and not checked lately if it is still accessible. Ask
> >> off-list if you want the script and I'll dig into my archives.
> >>
> >> nlxb (preferably from nlsr which you used rather than nlmrt which is
> >> now not maintained), will likely do as well as any general purpose
> >> code. There may be special approaches that do a bit better, but I
> >> suspect the reality is that the underlying problem is such that there
> >> are many sets of parameters with widely different values that will get 
> >> quite
> similar sums of squares.
> >>
> >> Best, JN
> >>
> >>
> >> On 2020-05-07 9:12 a.m., PIKAL Petr wrote:
> >>> Dear all
> >>>
> >>> I started to use nlxb instead of nls to get rid of singular gradient 
> >>> error.
> >>> I try to fit double exponential function to my data, but results I
> >>> obtain are strongly dependent on starting values.
> >>>
> >>> tsmes ~ A*exp(a*plast) + B* exp(b*plast)
> >>>
> >>> Changing b from 0.1 to 0.01 gives me completely different results. I
> >>> usually check result by a plot but could the result be inspected if
> >>> it achieved good result without plotting?
> >>>
> >>> Or is there any way how to perform such task?
> >>>
> >>> Cheers
> >>> Petr
> >>>
> >>> Below is working example.
> >>>
> >>>> dput(temp)
> >>> temp <- structure(list(tsmes = c(31, 32, 32, 32, 32, 32, 32, 32, 33,
> >>> 34, 35, 35, 36, 36, 36, 37, 38, 39, 40, 40, 40, 40, 40, 41, 43, 44,
> >>> 44, 44, 46, 47, 47, 47, 47, 48, 49, 51, 51, 51, 52, 53, 54, 54, 55,
> >>> 57, 57, 57, 59, 59, 60, 62, 63, 64, 65, 66, 66, 67, 67, 68, 70, 72,
> >>> 74, 76, 78, 81, 84, 85, 86, 88, 90, 91, 92, 94, 96, 97, 99, 100,
> >>> 102, 104, 106, 109, 112, 115, 119, 123, 127, 133, 141, 153, 163,
> >>> 171), plast = c(50, 51, 52, 52, 53, 53, 53, 54, 55, 55, 56, 57, 58,
> >>> 59, 60, 61, 62, 63, 64, 64, 64, 65, 65, 66, 66, 67, 68, 69, 70, 71,
> >>> 72, 73, 74, 75, 75, 76, 76, 77, 77, 78, 78, 79, 80, 81, 82, 83, 84,
> >>> 85, 85, 86, 86, 87, 88, 88, 89, 90, 91, 91, 93, 93, 94, 95, 96, 96,
> >>> 97, 98, 98, 99, 100, 100, 101, 102, 103, 103, 104, 105, 106, 107,
> >>> 107, 108, 109, 110, 111, 112, 112, 113, 113, 114, 115, 116)),
> >>> row.names = 2411:2500, class = "data.frame")
> >>>
> >>> library(nlsr)
> >>>
> >>> fit <- nlxb(tsmes ~ A*exp(a*plast) + B* exp(b*plast), data=temp,
> >>> start=list(A=1, B=15, a=0.025, b=0.01))
> >>> coef(fit)
> >>>            A            B            a            b
> >>> 3.945167e-13 9.772749e+00 2.802274e-01 2.179781e-02
> >>>
> >>> plot(temp$plast, temp$tsmes, ylim=c(0,200)) lines(temp$plast,
> >>> predict(fit, newdata=temp), col="pink", lwd=3) ccc <- coef(fit)
> >>> lines(0:120,ccc[1]*exp(ccc[3]*(0:120)))
> >>> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2)
> >>>
> >>> # wrong fit with slightly different b fit <- nlxb(tsmes ~
> >>> A*exp(a*plast) + B* exp(b*plast), data=temp, start=list(A=1, B=15,
> >>> a=0.025, b=0.1))
> >>> coef(fit)
> >>>            A            B            a            b
> >>> 2911.6448377    6.8320597  -49.1373979    0.0261391
> >>> lines(temp$plast, predict(fit, newdata=temp), col="red", lwd=3) ccc
> >>> <- coef(fit) lines(0:120,ccc[1]*exp(ccc[3]*(0:120)), col="red")
> >>> lines(0:120,ccc[2]*exp(ccc[4]*(0:120)), lty=3, lwd=2, col="red")
> >>>
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.

From k@r|@@ch||||ng @end|ng |rom un|-bonn@de  Mon May 11 09:48:13 2020
From: k@r|@@ch||||ng @end|ng |rom un|-bonn@de (Karl Schilling)
Date: Mon, 11 May 2020 09:48:13 +0200
Subject: [R] Unable to install Sequin R
In-Reply-To: <CA+8X3fWFcS8it5iQjcwXj_MquKE5OHF6JEkaeH1j6wfVfyVk-A@mail.gmail.com>
References: <CAFV24eGLc6bkPCKYzF=HbsT9653MTcW+EqP60X2WhJyVp7Q-sg@mail.gmail.com>
 <CA+8X3fU9gVBOvsvB7CVQ=DMLG1Tf-drOtOxrCotqR8U8Qptbng@mail.gmail.com>
 <CAFV24eHNdc_cm97D8iaGVW-R2TMRBwoDfkQmmaWKt+6x6_AMpg@mail.gmail.com>
 <CA+8X3fWFcS8it5iQjcwXj_MquKE5OHF6JEkaeH1j6wfVfyVk-A@mail.gmail.com>
Message-ID: <41cf8570-ea43-4229-207f-174e2ed5eb11@uni-bonn.de>

Dear Harshita,

I just tried to install seqinr on my win10 machine (R4.0) and had no 
problem.

In your mail, you still had a typo: you wrote seqinr with an "u" - maybe 
that is the problem

Best
-- 
Karl Schilling


From b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com  Mon May 11 03:59:14 2020
From: b|e|||@@|e@@@ndr@ @end|ng |rom gm@||@com (Alessandra Bielli)
Date: Sun, 10 May 2020 19:59:14 -0600
Subject: [R] predicting waste per capita - is a gaussian model correct?
In-Reply-To: <CAB8pepwn-cP8abZ=1Jw7yKnd8z+5Er1MNSUpR7ZD+SSNkaPCOA@mail.gmail.com>
References: <CA+6N3yXDvFdjUeCKgoKi=3ZAnKLTv1jn0hodN1QN8=Bs79840g@mail.gmail.com>
 <CAHrK516nu-TmMJq=Rzp1ddYr=RAnX41k8=aPfhgE4ZJNFbf88A@mail.gmail.com>
 <CAB8pepwn-cP8abZ=1Jw7yKnd8z+5Er1MNSUpR7ZD+SSNkaPCOA@mail.gmail.com>
Message-ID: <CA+6N3yV7-_ZJ5TG8BQxWYwDQoenSuBNqvGt6EaLzcz63qWfe4A@mail.gmail.com>

Dear all

First of all apologies for the off-topic question and for not respecting
the other points.
Second, thanks for your advice and opinion I will definitely consult a
statistician.

Regards,

Alessandra

On Sun, May 10, 2020 at 4:57 PM Abby Spurdle <spurdle.a at gmail.com> wrote:

> Well, this is 100% off-topic...
> And I wasn't planning to answer the OP's question.
>
> However, I disagree with your answer.
>
> > There is no requirement that the dependent variable in a "regression"
> type
> > estimation follows a gaussian distribution.
>
> False.
> It's depends on what type of '"regression" type estimation' one uses,
> among other things.
>
> > You need a model of the
> > process and then use an estimation technique to estimate your model.  If
> > effects in your model are additive do not use a log transformation. If
> > effects are multiplicative then use a log transformation.
>
> The main question is, does the model satisfy the *assumptions*.
>
> > The choice
> > should be determined by the mechanics of the problem and not by the
> > statistics.
>
> While a mechanistic understanding is definitely valuable...
> If the criteria for a good model vs a bad model, was whether the model
> was consistent with mechanistic theory/understanding, then nearly
> every statistical model I've seen would be a bad model.
> I would say, a good model is one that is useful...
>
> > If you do use a log transformation the trying to reverse the
> > process using an exponential transformation will be biased.
> > The extent of
> > that bias depends on your problem and it would not be possible to
> estimate
> > the significance of the bias without a much greater knowledge of the
> > process and data.
>
> Never heard of this before...
> But I do note back-transformation is not trivial, and I'm not an
> expert on back-transformations.
>
> > I would suggest that you consult a competent
> > statistician.
>
> I agree on that part...
>

	[[alternative HTML version deleted]]


From @denener @end|ng |rom hotm@||@com  Mon May 11 16:28:59 2020
From: @denener @end|ng |rom hotm@||@com (karl adenener)
Date: Mon, 11 May 2020 14:28:59 +0000
Subject: [R] My dream ...
Message-ID: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>

It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.

Where could problems arise?
Does anyone know of a suitable R-Package or software?
Does anyone have the time and inclination to create a flexibly customizable package?

greetings
Adenener

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Mon May 11 17:19:51 2020
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Mon, 11 May 2020 11:19:51 -0400
Subject: [R] My dream ...
In-Reply-To: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
Message-ID: <CAJc=yOFdYqEn58bcgcQ0hmQTUn8ixp_=BKoUebXr7rKQYHiQXw@mail.gmail.com>

This isn't an R code question and you posted in HTML, but briefly:

Simulate data that could arise from your study, including missing and
outliers, then write code that runs th analyses. Put the code in an
open-science archive.

Then run it as is when you actually have the data.

There will probably be some hiccups depending on how good your
simulation is, but that's the in-principle solution.

On Mon, May 11, 2020 at 11:10 AM karl adenener <adenener at hotmail.com> wrote:
>
> It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.
>
> Where could problems arise?
> Does anyone know of a suitable R-Package or software?
> Does anyone have the time and inclination to create a flexibly customizable package?
>
> greetings
> Adenener
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Patrick S. Malone, Ph.D., Malone Quantitative
NEW Service Models: http://malonequantitative.com

He/Him/His


From jwd @end|ng |rom @urewe@t@net  Mon May 11 18:58:09 2020
From: jwd @end|ng |rom @urewe@t@net (John)
Date: Mon, 11 May 2020 09:58:09 -0700
Subject: [R] the volcano orientation
In-Reply-To: <CABcYAdK4U7K_FwM=w04k5dK8rbbT5uKo-WyHBMmcBUQuaNwBYA@mail.gmail.com>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
 <CABcYAdK4U7K_FwM=w04k5dK8rbbT5uKo-WyHBMmcBUQuaNwBYA@mail.gmail.com>
Message-ID: <20200511095809.51439268@Draco>

Out of curiosity, and considering the bewildering array of projections
and grids in use for various mapping purposes, you seem to be saying in your
example 2 that the grid coordinates number south to north and east to
west.  Given scale of the coordinate numbers, would that be a national
grid system employed in New Zealnd?

J. W. Dougherty

On Mon, 11 May 2020 13:56:49 +1200
"Richard O'Keefe" <raoknz at gmail.com> wrote:

> Hey, I know that volcano!  It's walking distance from the Intermediate
> school I attended.
> To you it's a plot; to me it's a place.
> So I offer you four scenarios.
> 
> 1. You think of it as a place you know and have been.
>     In that case the "right" orientation is the one that best matches
> what you are used to seeing.
>     For me, that would put the peak on the right of the plot.
> 
> 2. You think of it as a patch in a map.
>     In that case the "right:" orientation is the one that matches the
> map. That would put the peak at the bottom of the plot.
> 
> 3. You think of it as a product of geological processes, and are
> perhaps interested in
>     whether there is any connection between the orientation of the
> volcano and the
>     direction the Auckland hot-spot (currently at White Island) was
> moving. In that case you'd choose south-west -> north-east as the
> primary axis. (I think.  Not really sure.)
> 
> 4. You think of it as a picture, an illustration in a textbook.  It
> might need to be cropped
>     vertically so you can fit another illustration on the same page.
> For that and
>     perceptual reasons you want the major linear axis of the image to
> be  horizontal.
>     In that case, what we have now is a perfectly reasonable choice.
> 
> "Quality is fitness for use."
> 
> On Sun, 10 May 2020 at 12:44, Michael Sumner <mdsumner at gmail.com>
> wrote:
> >
> > Does anyone know why 'volcano' is oriented as it is?
> >
> > image(volcano)  ## filled.contour is the same
> >
> > I know it's all arbitrary, but north-up is a pretty solid
> > convention. Is there any reason why the classic 'image()' example
> > data set would not default to this orientation?
> >
> > A Google map of the site (in Web Mercator):
> >
> > https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
> >
> >
> > For image(), the north-up orientation is
> > 't(volcano[,ncol(volcano):1])'.
> >
> > If you are interested in a roughly georeferenced version I have
> > code here:
> >
> > https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
> >
> > (Also see fortunes::fortune("conventions") )
> >
> > Best, Mike
> >
> >
> > --
> > Michael Sumner
> > Software and Database Engineer
> > Australian Antarctic Division
> > Hobart, Australia
> > e-mail: mdsumner at gmail.com
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.  
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html and provide commented,
> minimal, self-contained, reproducible code.


From 538280 @end|ng |rom gm@||@com  Mon May 11 20:11:03 2020
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Mon, 11 May 2020 12:11:03 -0600
Subject: [R] My dream ...
In-Reply-To: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
Message-ID: <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>

It is a nice dream, but it is really abdicating ethical responsibility
to the computer instead of the researcher.  And I personally don't
trust computers over people for this.

What could go wrong?

First, how do you guarantee that the statistical plan was locked in
place before the data was collected?  With your proposed system some
people will still run a plan on their data, make changes, run again,
etc. until they get what they want, then claim that the statistical
plan came before the data that was used to tune it (I consider this
unethical, but see no way for R or computers in general to prevent
this without human oversight).

Second, what if the data shows something that you did not anticipate?
This methodology would prevent you doing Exploratory Data Analysis and
adapting accordingly.  If people start using this as a black box that
is "blessed" by the package as purely "objective", then many will not
even do any EDA and take the results as "True" when they are not even
appropriate.

Better would be a regular system for submitting your code to a
clinical trial registry or some other pre-study registry so that other
can compare what you did to what you claimed you were going to do.  If
you don't want to submit the entire code, you could submit an md5 hash
to the registry, then others could check to see if the code ran (put
into an online supplement) differs from the registered version.  If
the data show something unanticipated then you can show the original
code and the modified code along with your reasoning and let the
consumer decide if the changes were justified.

This could still be worked around if someone was really motivated, but
it is probably the best we will see.

In my opinion the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience (most AI that I have seen
is really doing a bunch of what I would consider to be boring, really
fast so people don't have to).  Leave the Intelligence to the people.

On Mon, May 11, 2020 at 9:10 AM karl adenener <adenener at hotmail.com> wrote:
>
> It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.
>
> Where could problems arise?
> Does anyone know of a suitable R-Package or software?
> Does anyone have the time and inclination to create a flexibly customizable package?
>
> greetings
> Adenener
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From rkoenker @end|ng |rom ||||no|@@edu  Mon May 11 21:22:27 2020
From: rkoenker @end|ng |rom ||||no|@@edu (Koenker, Roger W)
Date: Mon, 11 May 2020 19:22:27 +0000
Subject: [R] My dream ...
In-Reply-To: <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
Message-ID: <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>

Definitely a fortune:

"the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience"

Greg Snow in response to a question about automated R-analysis.

Roger Koenker
r.koenker at ucl.ac.uk<mailto:r.koenker at ucl.ac.uk>
Honorary Professor of Economics
Department of Economics, UCL
Emeritus Professor of Economics
and Statistics, UIUC


On May 11, 2020, at 7:11 PM, Greg Snow <538280 at gmail.com<mailto:538280 at gmail.com>> wrote:

It is a nice dream, but it is really abdicating ethical responsibility
to the computer instead of the researcher.  And I personally don't
trust computers over people for this.

What could go wrong?

First, how do you guarantee that the statistical plan was locked in
place before the data was collected?  With your proposed system some
people will still run a plan on their data, make changes, run again,
etc. until they get what they want, then claim that the statistical
plan came before the data that was used to tune it (I consider this
unethical, but see no way for R or computers in general to prevent
this without human oversight).

Second, what if the data shows something that you did not anticipate?
This methodology would prevent you doing Exploratory Data Analysis and
adapting accordingly.  If people start using this as a black box that
is "blessed" by the package as purely "objective", then many will not
even do any EDA and take the results as "True" when they are not even
appropriate.

Better would be a regular system for submitting your code to a
clinical trial registry or some other pre-study registry so that other
can compare what you did to what you claimed you were going to do.  If
you don't want to submit the entire code, you could submit an md5 hash
to the registry, then others could check to see if the code ran (put
into an online supplement) differs from the registered version.  If
the data show something unanticipated then you can show the original
code and the modified code along with your reasoning and let the
consumer decide if the changes were justified.

This could still be worked around if someone was really motivated, but
it is probably the best we will see.

In my opinion the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience (most AI that I have seen
is really doing a bunch of what I would consider to be boring, really
fast so people don't have to).  Leave the Intelligence to the people.

On Mon, May 11, 2020 at 9:10 AM karl adenener <adenener at hotmail.com<mailto:adenener at hotmail.com>> wrote:

It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.

Where could problems arise?
Does anyone know of a suitable R-Package or software?
Does anyone have the time and inclination to create a flexibly customizable package?

greetings
Adenener

       [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



--
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com<mailto:538280 at gmail.com>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon May 11 21:26:44 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 11 May 2020 12:26:44 -0700
Subject: [R] My dream ...
In-Reply-To: <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
Message-ID: <27988C9E-6D16-4D34-9613-761B864210D4@dcn.davis.ca.us>

Seconded!

On May 11, 2020 12:22:27 PM PDT, "Koenker, Roger W" <rkoenker at illinois.edu> wrote:
>Definitely a fortune:
>
>"the advantage of computers is not Artificial
>Intelligence, but rather Artificial Patience"
>
>Greg Snow in response to a question about automated R-analysis.
>
>Roger Koenker
>r.koenker at ucl.ac.uk<mailto:r.koenker at ucl.ac.uk>
>Honorary Professor of Economics
>Department of Economics, UCL
>Emeritus Professor of Economics
>and Statistics, UIUC
>
>
>On May 11, 2020, at 7:11 PM, Greg Snow
><538280 at gmail.com<mailto:538280 at gmail.com>> wrote:
>
>It is a nice dream, but it is really abdicating ethical responsibility
>to the computer instead of the researcher.  And I personally don't
>trust computers over people for this.
>
>What could go wrong?
>
>First, how do you guarantee that the statistical plan was locked in
>place before the data was collected?  With your proposed system some
>people will still run a plan on their data, make changes, run again,
>etc. until they get what they want, then claim that the statistical
>plan came before the data that was used to tune it (I consider this
>unethical, but see no way for R or computers in general to prevent
>this without human oversight).
>
>Second, what if the data shows something that you did not anticipate?
>This methodology would prevent you doing Exploratory Data Analysis and
>adapting accordingly.  If people start using this as a black box that
>is "blessed" by the package as purely "objective", then many will not
>even do any EDA and take the results as "True" when they are not even
>appropriate.
>
>Better would be a regular system for submitting your code to a
>clinical trial registry or some other pre-study registry so that other
>can compare what you did to what you claimed you were going to do.  If
>you don't want to submit the entire code, you could submit an md5 hash
>to the registry, then others could check to see if the code ran (put
>into an online supplement) differs from the registered version.  If
>the data show something unanticipated then you can show the original
>code and the modified code along with your reasoning and let the
>consumer decide if the changes were justified.
>
>This could still be worked around if someone was really motivated, but
>it is probably the best we will see.
>
>In my opinion the advantage of computers is not Artificial
>Intelligence, but rather Artificial Patience (most AI that I have seen
>is really doing a bunch of what I would consider to be boring, really
>fast so people don't have to).  Leave the Intelligence to the people.
>
>On Mon, May 11, 2020 at 9:10 AM karl adenener
><adenener at hotmail.com<mailto:adenener at hotmail.com>> wrote:
>
>It would be a dream, there would be a R-based software, which I
>configure according to my study (type of data, limits for meaningful
>measurements, handling of outliers and missing measurements, test
>method etc.), which then reads my original measurement data and after
>some computing time the software provides me with the statistical
>analysis. All steps of the evaluation have to be defined before the
>start of the study and cannot be changed after the start of the study.
>
>Where could problems arise?
>Does anyone know of a suitable R-Package or software?
>Does anyone have the time and inclination to create a flexibly
>customizable package?
>
>greetings
>Adenener
>
>       [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
>UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>
>--
>Gregory (Greg) L. Snow Ph.D.
>538280 at gmail.com<mailto:538280 at gmail.com>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From j@ork|n @end|ng |rom @om@um@ry|@nd@edu  Mon May 11 21:50:03 2020
From: j@ork|n @end|ng |rom @om@um@ry|@nd@edu (Sorkin, John)
Date: Mon, 11 May 2020 19:50:03 +0000
Subject: [R] R package for discrete-time competing-risk anlayses..
In-Reply-To: <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>,
 <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
Message-ID: <MN2PR03MB5167C91EF77FF078D430AA31E2A10@MN2PR03MB5167.namprd03.prod.outlook.com>

Can someone direct me to an R package that can run discrete-time competing risk analyses?

Thank you,

John



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)


________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of Koenker, Roger W <rkoenker at illinois.edu>
Sent: Monday, May 11, 2020 3:22 PM
To: Greg Snow <538280 at gmail.com>
Cc: r-help at r-project.org <r-help at r-project.org>; karl adenener <adenener at hotmail.com>
Subject: Re: [R] My dream ...

Definitely a fortune:

"the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience"

Greg Snow in response to a question about automated R-analysis.

Roger Koenker
r.koenker at ucl.ac.uk<mailto:r.koenker at ucl.ac.uk>
Honorary Professor of Economics
Department of Economics, UCL
Emeritus Professor of Economics
and Statistics, UIUC


On May 11, 2020, at 7:11 PM, Greg Snow <538280 at gmail.com<mailto:538280 at gmail.com>> wrote:

It is a nice dream, but it is really abdicating ethical responsibility
to the computer instead of the researcher.  And I personally don't
trust computers over people for this.

What could go wrong?

First, how do you guarantee that the statistical plan was locked in
place before the data was collected?  With your proposed system some
people will still run a plan on their data, make changes, run again,
etc. until they get what they want, then claim that the statistical
plan came before the data that was used to tune it (I consider this
unethical, but see no way for R or computers in general to prevent
this without human oversight).

Second, what if the data shows something that you did not anticipate?
This methodology would prevent you doing Exploratory Data Analysis and
adapting accordingly.  If people start using this as a black box that
is "blessed" by the package as purely "objective", then many will not
even do any EDA and take the results as "True" when they are not even
appropriate.

Better would be a regular system for submitting your code to a
clinical trial registry or some other pre-study registry so that other
can compare what you did to what you claimed you were going to do.  If
you don't want to submit the entire code, you could submit an md5 hash
to the registry, then others could check to see if the code ran (put
into an online supplement) differs from the registered version.  If
the data show something unanticipated then you can show the original
code and the modified code along with your reasoning and let the
consumer decide if the changes were justified.

This could still be worked around if someone was really motivated, but
it is probably the best we will see.

In my opinion the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience (most AI that I have seen
is really doing a bunch of what I would consider to be boring, really
fast so people don't have to).  Leave the Intelligence to the people.

On Mon, May 11, 2020 at 9:10 AM karl adenener <adenener at hotmail.com<mailto:adenener at hotmail.com>> wrote:

It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.

Where could problems arise?
Does anyone know of a suitable R-Package or software?
Does anyone have the time and inclination to create a flexibly customizable package?

greetings
Adenener

       [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=0uX2IMimnT3HFKDY%2Fubv0JjEMYcSEbOtgzqEuzDPFow%3D&amp;reserved=0
PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=UwzCHC4JGmC0YbjliQ8XhUdMSGju7EeyWfYVRFFVcjE%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.



--
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com<mailto:538280 at gmail.com>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=0uX2IMimnT3HFKDY%2Fubv0JjEMYcSEbOtgzqEuzDPFow%3D&amp;reserved=0
PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=PXbXmhRfsosD1leVo19l7o95QrQ2OBdGlSAiUNWIgNs%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=JE5ixJ4K6LIvA5DyhterWla8jryKhDLCnblbQFNd93o%3D&amp;reserved=0
PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=PXbXmhRfsosD1leVo19l7o95QrQ2OBdGlSAiUNWIgNs%3D&amp;reserved=0
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Mon May 11 22:50:28 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 11 May 2020 13:50:28 -0700
Subject: [R] R package for discrete-time competing-risk anlayses..
In-Reply-To: <MN2PR03MB5167C91EF77FF078D430AA31E2A10@MN2PR03MB5167.namprd03.prod.outlook.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <B6991FF7-A8F0-4B8C-A8F6-B419F397FCED@illinois.edu>
 <MN2PR03MB5167C91EF77FF078D430AA31E2A10@MN2PR03MB5167.namprd03.prod.outlook.com>
Message-ID: <CAGxFJbSCmGXMPyYmU0ROZP+r=hAwiO6HagdzF5VAA7aTx2HHbw@mail.gmail.com>

Search!
"discrete time competing risk analysis" on rseek.org

The survival task view on CRAN:
https://CRAN.R-project.org/view=Survival

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Mon, May 11, 2020 at 12:50 PM Sorkin, John <jsorkin at som.umaryland.edu> wrote:
>
> Can someone direct me to an R package that can run discrete-time competing risk analyses?
>
> Thank you,
>
> John
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> ________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of Koenker, Roger W <rkoenker at illinois.edu>
> Sent: Monday, May 11, 2020 3:22 PM
> To: Greg Snow <538280 at gmail.com>
> Cc: r-help at r-project.org <r-help at r-project.org>; karl adenener <adenener at hotmail.com>
> Subject: Re: [R] My dream ...
>
> Definitely a fortune:
>
> "the advantage of computers is not Artificial
> Intelligence, but rather Artificial Patience"
>
> Greg Snow in response to a question about automated R-analysis.
>
> Roger Koenker
> r.koenker at ucl.ac.uk<mailto:r.koenker at ucl.ac.uk>
> Honorary Professor of Economics
> Department of Economics, UCL
> Emeritus Professor of Economics
> and Statistics, UIUC
>
>
> On May 11, 2020, at 7:11 PM, Greg Snow <538280 at gmail.com<mailto:538280 at gmail.com>> wrote:
>
> It is a nice dream, but it is really abdicating ethical responsibility
> to the computer instead of the researcher.  And I personally don't
> trust computers over people for this.
>
> What could go wrong?
>
> First, how do you guarantee that the statistical plan was locked in
> place before the data was collected?  With your proposed system some
> people will still run a plan on their data, make changes, run again,
> etc. until they get what they want, then claim that the statistical
> plan came before the data that was used to tune it (I consider this
> unethical, but see no way for R or computers in general to prevent
> this without human oversight).
>
> Second, what if the data shows something that you did not anticipate?
> This methodology would prevent you doing Exploratory Data Analysis and
> adapting accordingly.  If people start using this as a black box that
> is "blessed" by the package as purely "objective", then many will not
> even do any EDA and take the results as "True" when they are not even
> appropriate.
>
> Better would be a regular system for submitting your code to a
> clinical trial registry or some other pre-study registry so that other
> can compare what you did to what you claimed you were going to do.  If
> you don't want to submit the entire code, you could submit an md5 hash
> to the registry, then others could check to see if the code ran (put
> into an online supplement) differs from the registered version.  If
> the data show something unanticipated then you can show the original
> code and the modified code along with your reasoning and let the
> consumer decide if the changes were justified.
>
> This could still be worked around if someone was really motivated, but
> it is probably the best we will see.
>
> In my opinion the advantage of computers is not Artificial
> Intelligence, but rather Artificial Patience (most AI that I have seen
> is really doing a bunch of what I would consider to be boring, really
> fast so people don't have to).  Leave the Intelligence to the people.
>
> On Mon, May 11, 2020 at 9:10 AM karl adenener <adenener at hotmail.com<mailto:adenener at hotmail.com>> wrote:
>
> It would be a dream, there would be a R-based software, which I configure according to my study (type of data, limits for meaningful measurements, handling of outliers and missing measurements, test method etc.), which then reads my original measurement data and after some computing time the software provides me with the statistical analysis. All steps of the evaluation have to be defined before the start of the study and cannot be changed after the start of the study.
>
> Where could problems arise?
> Does anyone know of a suitable R-Package or software?
> Does anyone have the time and inclination to create a flexibly customizable package?
>
> greetings
> Adenener
>
>        [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=0uX2IMimnT3HFKDY%2Fubv0JjEMYcSEbOtgzqEuzDPFow%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=UwzCHC4JGmC0YbjliQ8XhUdMSGju7EeyWfYVRFFVcjE%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com<mailto:538280 at gmail.com>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761516112&amp;sdata=0uX2IMimnT3HFKDY%2Fubv0JjEMYcSEbOtgzqEuzDPFow%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=PXbXmhRfsosD1leVo19l7o95QrQ2OBdGlSAiUNWIgNs%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-help&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=JE5ixJ4K6LIvA5DyhterWla8jryKhDLCnblbQFNd93o%3D&amp;reserved=0
> PLEASE do read the posting guide https://nam03.safelinks.protection.outlook.com/?url=http%3A%2F%2Fwww.r-project.org%2Fposting-guide.html&amp;data=02%7C01%7C%7C45b2872863f24e3f1ba608d7f5e0b3c3%7C717009a620de461a88940312a395cac9%7C0%7C0%7C637248217761526108&amp;sdata=PXbXmhRfsosD1leVo19l7o95QrQ2OBdGlSAiUNWIgNs%3D&amp;reserved=0
> and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@oknz @end|ng |rom gm@||@com  Mon May 11 23:10:33 2020
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Tue, 12 May 2020 09:10:33 +1200
Subject: [R] the volcano orientation
In-Reply-To: <20200511095809.51439268@Draco>
References: <CAAcGz9-d1Hq7zBO0vaaTWnrETAMX64=zESvM=QKon4au2tgvAw@mail.gmail.com>
 <CABcYAdK4U7K_FwM=w04k5dK8rbbT5uKo-WyHBMmcBUQuaNwBYA@mail.gmail.com>
 <20200511095809.51439268@Draco>
Message-ID: <CABcYAdJrQgp21Rd2WkG3gfymRp0abXAZcOMhVCyafOvr0jnqpg@mail.gmail.com>

Like other countries, New Zealand revises its maps and its coordinate
system from time to time.  The one in use at the time that image was
digitised is probably the one described here:
https://www.linz.govt.nz/data/geodetic-system/datums-projections-heights/projections/new-zealand-map-grid-nzmg

On Tue, 12 May 2020 at 04:58, John via R-help <r-help at r-project.org> wrote:
>
> Out of curiosity, and considering the bewildering array of projections
> and grids in use for various mapping purposes, you seem to be saying in your
> example 2 that the grid coordinates number south to north and east to
> west.  Given scale of the coordinate numbers, would that be a national
> grid system employed in New Zealnd?
>
> J. W. Dougherty
>
> On Mon, 11 May 2020 13:56:49 +1200
> "Richard O'Keefe" <raoknz at gmail.com> wrote:
>
> > Hey, I know that volcano!  It's walking distance from the Intermediate
> > school I attended.
> > To you it's a plot; to me it's a place.
> > So I offer you four scenarios.
> >
> > 1. You think of it as a place you know and have been.
> >     In that case the "right" orientation is the one that best matches
> > what you are used to seeing.
> >     For me, that would put the peak on the right of the plot.
> >
> > 2. You think of it as a patch in a map.
> >     In that case the "right:" orientation is the one that matches the
> > map. That would put the peak at the bottom of the plot.
> >
> > 3. You think of it as a product of geological processes, and are
> > perhaps interested in
> >     whether there is any connection between the orientation of the
> > volcano and the
> >     direction the Auckland hot-spot (currently at White Island) was
> > moving. In that case you'd choose south-west -> north-east as the
> > primary axis. (I think.  Not really sure.)
> >
> > 4. You think of it as a picture, an illustration in a textbook.  It
> > might need to be cropped
> >     vertically so you can fit another illustration on the same page.
> > For that and
> >     perceptual reasons you want the major linear axis of the image to
> > be  horizontal.
> >     In that case, what we have now is a perfectly reasonable choice.
> >
> > "Quality is fitness for use."
> >
> > On Sun, 10 May 2020 at 12:44, Michael Sumner <mdsumner at gmail.com>
> > wrote:
> > >
> > > Does anyone know why 'volcano' is oriented as it is?
> > >
> > > image(volcano)  ## filled.contour is the same
> > >
> > > I know it's all arbitrary, but north-up is a pretty solid
> > > convention. Is there any reason why the classic 'image()' example
> > > data set would not default to this orientation?
> > >
> > > A Google map of the site (in Web Mercator):
> > >
> > > https://www.google.com/maps/place/Maungawhau+%2F+Mount+Eden/@-36.8763271,174.7619561,856m/data=!3m1!1e3!4m8!1m2!2m1!1smaungawhau!3m4!1s0x6d0d47db8d7bd1ff:0x8bcffe2a5c7360d2!8m2!3d-36.8666667!4d174.7666667
> > >
> > >
> > > For image(), the north-up orientation is
> > > 't(volcano[,ncol(volcano):1])'.
> > >
> > > If you are interested in a roughly georeferenced version I have
> > > code here:
> > >
> > > https://gist.github.com/mdsumner/20fe3ffa04421bf8e0517c19085e5fd8
> > >
> > > (Also see fortunes::fortune("conventions") )
> > >
> > > Best, Mike
> > >
> > >
> > > --
> > > Michael Sumner
> > > Software and Database Engineer
> > > Australian Antarctic Division
> > > Hobart, Australia
> > > e-mail: mdsumner at gmail.com
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html and provide commented,
> > > minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html and provide commented,
> > minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Tue May 12 04:34:47 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 12 May 2020 14:34:47 +1200
Subject: [R] genericSummary in LSAfun
In-Reply-To: <CAGN=ytM6=ntjuRX_q7pbaYFzb5HbiV8VwR5i6JoZB_FRiX9_7w@mail.gmail.com>
References: <CAGN=ytM4+Cn_egGauCt+hMZ4jX0OVQNacaLtgvHzmpfPy5WFbw@mail.gmail.com>
 <CAGN=ytM6=ntjuRX_q7pbaYFzb5HbiV8VwR5i6JoZB_FRiX9_7w@mail.gmail.com>
Message-ID: <CAB8pepzTZ3HuE5tj0XK1JXRkpLDz_MKrn9byP3u2co2eViWLZw@mail.gmail.com>

Does increasing the value of k (the second argument) help?

Also, if I understand the documentation correctly, the first argument
should a single string, not a data.frame.
I note that the paste function (with collapse="") can be used to turn
a character vector into a single string.


On Sat, May 9, 2020 at 10:07 PM Mehdi Dadkhah <mehdidadkhah91 at gmail.com> wrote:
>
> Hi,
> I hope you are doing well!
> I have a data frame with a column. it contains about 140 posts. In each
> row, there is a blog post. I named this data frame as "posttext". When i
> use  genericSummary() function, it returns paragraph instead sentences.
> What is problem?
>
>
> Command which i use:
>
> summaryp<-genericSummary(posttext,k=1,split=c(".","!","?"),min=5,breakdown=FALSE)
>
>
>
> Many thanks!
> With best regards,
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Tue May 12 04:53:27 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 12 May 2020 14:53:27 +1200
Subject: [R] stats:: spline's method could not be monoH.FC
In-Reply-To: <24240.9662.85376.166974@stat.math.ethz.ch>
References: <ema4758abf-559f-4e73-90ff-10c2692ea0ce@bioinfo5>
 <CAB8pepxq6j+x+WVbnLM+jw6BXh196pjDLEG4gJgzU5tGCoJs+A@mail.gmail.com>
 <24237.41759.777920.646961@stat.math.ethz.ch>
 <CAB8pepy14iNNEE4vMdrFeNpMCJ85RV26MdafWyun_jCG0nTYNA@mail.gmail.com>
 <24240.9662.85376.166974@stat.math.ethz.ch>
Message-ID: <CAB8pepyYOa18nEb6No-HTNQCp+42Bv6KYPp4W3k=+eNwWiZYkw@mail.gmail.com>

Hi Martin,
(In regards to your last ***two*** posts).

(Excerpt, post one)
> Well, as I know spent enough time reading and thinking, I'd
> really like to add   method = "clamped" to splinefun() and also
> the other one where fix the 2nd derivatives (to arbitrary values
> instead of zero).

(Excerpt, post two)
> So as a matter of fact I would ask for patches there (both in C
> and in R calling C ... maybe too much for 30 minutes ;-)

I'm happy to have a look a the spline code.
However, there's likely to be a substantial delay.

First, I need to:
(1) Change operating systems. Hopefully soon...
(2) Learn to build and debug R packages with non-trivial C code.
(3) Update my kubik package.
Will be looking at second derivatives, but more importantly rewriting
the root finding algorithm.
(4) Familiarize myself with spline theory (other than bezier/hermite).

My guess is I will get there in about eight to ten months, from now.
If you need it done sooner, and want someone else to do it, that's fine...

Also, it may be a good idea for you to work out what you would like
the top-level function signatures to look like...


From |@rchuby @end|ng |rom gm@||@com  Mon May 11 23:09:40 2020
From: |@rchuby @end|ng |rom gm@||@com (Fernando Archuby)
Date: Mon, 11 May 2020 18:09:40 -0300
Subject: [R] Convex hulls after nMDS or PCoA
Message-ID: <CAE09ejdPAk0BgadOdiU01Tp9LXoW84e-H1Go8U2tAKS2OdhDKg@mail.gmail.com>

Dear all. I need to build convex hulls around groups of points of an
ordination (nMDS or PCoA). The groups were independently established with
cluster analysis. I have learnt to plot convex hulls but would need to
create every of the 21 groups independently. Do you know how could I just
add the hulls to the ordination plot in an easier way?
Thank you very much,
Fernando.

-- 

*Dr. Fernando M. Archuby*
CONICET-UNLP
Tel?fono Personal: +54-221-15-6129667.
farchuby at gmail.com
paleobiologia at gmail.com

	[[alternative HTML version deleted]]


From rhur||n @end|ng |rom gwdg@de  Tue May 12 09:50:44 2020
From: rhur||n @end|ng |rom gwdg@de (Rainer Hurling)
Date: Tue, 12 May 2020 09:50:44 +0200
Subject: [R] Date format
In-Reply-To: <CAH6117JToaaoFQssgiuMbiR7d_SEBWK2HWuRqxWFKugRP-Q5KQ@mail.gmail.com>
References: <CAH6117JToaaoFQssgiuMbiR7d_SEBWK2HWuRqxWFKugRP-Q5KQ@mail.gmail.com>
Message-ID: <86880d13-252f-a936-4151-3047c3d4149f@gwdg.de>

Hi Medic,

Am 10.05.20 um 09:15 schrieb Medic:
> I took a SAMPLE CODE (for Connected scatterplot) from the R gallery
> and applied to MY DATA, but got:
> "Don't know how to automatically pick scale for object ..."
> P.S. 1) R ver. 4.0 (Yes, Jeff);  2) Attached: mydata_dput (1 ??)
> 
> SAMPLE CODE
> library(ggplot2)
> library(dplyr)
> library(hrbrthemes)
> data <- read.table("https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv
> ", header=T)
> 
> data$date <- as.Date(data$date)
> 
> # Plot
> data %>%
>    tail(10) %>%
>    ggplot( aes(x=date, y=value)) +
>      geom_line( color="grey") +
>      geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>      theme_ipsum() +
>      ggtitle("Evolution of bitcoin price")
> 
> ======
> MY DATA
> mydata <- read.table("E:/mydata.csv", header=TRUE, sep=";", dec=",")
> 
> str(mydata)
> 'data.frame': 7 obs. of  2 variables:
>   $ date : chr  "01.01.2000" "02.01.2000" ...
>   $ value: int  11 12 ...
> 
> mydata$date <- as.Date(mydata$date, "%d.%m.%Y")
> 
> str(mydata$date)
> Date[1:7], format: "2000-01-01"
> 
> # Bert, thanks for the explanation!
> # Rainer, thanks for the specific code!
> 
> # And then the problem:
> mydata %>%
>      tail(10) %>%
>      ggplot( aes(x=mydata, y=value)) +
>      geom_line( color="grey") +
>      geom_point(shape=21, color="black", fill="#69b3a2", size=6) +
>      theme_ipsum() +
>      ggtitle("Evolution")
> 
> "Don't know how to automatically pick scale for object of type
> data.frame. Defaulting to continuous.
> Error: Aesthetics must be either length 1 or the same as the data (7): x"

Perhaps only a little typo? Pls try

ggplot( aes(x=date, y=value))
               ^^^^


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Tue May 12 10:11:41 2020
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Tue, 12 May 2020 08:11:41 +0000
Subject: [R] Classification of wind events
Message-ID: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>

Dear R list users,
I am aware that this question is not strictly related, at the present moment, to R code and it is more general. Please forgive me, but I need to share my thoughts with you.

Foehn conditions on the southern slope of Alps happen with strong northerly flows that impact perpendicularly over the Apls. This situation triggers strong northerly leeward winds.
Given a single automatic weather station, I would like to identify these periods starting from wind direction and wind intensity data. Frequency of data is quarter of hour.
I would really find difficult to detect the moving windows of these events:
- I can't analyse data day by day;
- at the beginning and at the end of each event, when the process is not at full speed yet, the rotation is not always perfectly identifiable;
- I cannot claim in principle that the direction of each consecutive observation is costantly and strictly from the chosen direction.

Does anybody have a clue on how to start to build this process in the right way?

Thank you for your attention and your help
Stefano

         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


	[[alternative HTML version deleted]]


From @purd|e@@ @end|ng |rom gm@||@com  Tue May 12 10:38:31 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 12 May 2020 20:38:31 +1200
Subject: [R] My dream ...
In-Reply-To: <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
Message-ID: <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>

> In my opinion the advantage of computers is not Artificial
> Intelligence, but rather Artificial Patience (most AI that I have seen
> is really doing a bunch of what I would consider to be boring, really
> fast so people don't have to).  Leave the Intelligence to the people.

Hmmm...
https://en.wikipedia.org/wiki/Artificial_intelligence_in_video_games

Also, I found the following while searching for battle chess:
https://youtu.be/hBNG7444lOw

(Warning: Contains aggressive chess tactics).

Also, correct me if I'm wrong, but doesn't Emacs have historical
connections to AI research...?


From tuech|er @end|ng |rom gmx@@t  Tue May 12 11:10:17 2020
From: tuech|er @end|ng |rom gmx@@t (Heinz Tuechler)
Date: Tue, 12 May 2020 11:10:17 +0200
Subject: [R] My dream ...
In-Reply-To: <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
Message-ID: <b46d3d76-e772-3054-bb73-16a84c64a96a@gmx.at>

Abby Spurdle wrote/hat geschrieben on/am 12.05.2020 10:38:
>> In my opinion the advantage of computers is not Artificial
>> Intelligence, but rather Artificial Patience (most AI that I have seen
>> is really doing a bunch of what I would consider to be boring, really
>> fast so people don't have to).  Leave the Intelligence to the people.
>
> Hmmm...
> https://en.wikipedia.org/wiki/Artificial_intelligence_in_video_games
>
> Also, I found the following while searching for battle chess:
> https://youtu.be/hBNG7444lOw
>
> (Warning: Contains aggressive chess tactics).
>
> Also, correct me if I'm wrong, but doesn't Emacs have historical
> connections to AI research...?
>
Maybe a matter of definition, but admittedly I have to use a lot of my
intelligence for doing boring work.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From Po||ngW @end|ng |rom @etn@@com  Mon May 11 17:58:38 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Mon, 11 May 2020 15:58:38 +0000
Subject: [R] Help with Kmeans output and using broom to tidy etc..
Message-ID: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>

#RStudio Version Version 1.2.1335 need this one--> 1.2.5019
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello:

I have data that I am trying to manipulate for Kmeans clustering.

Original data looks like this

str(geo1) 
# 'data.frame':	2352 obs. of  5 variables:
# $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 86 726 1702 ...
# $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 31 12 12 ...
# $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 727 1127 1304 ...
# $ latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
# $ longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...

I created a subset adding column prop_of_total 
str(trnd1_tbl)
tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
 $ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ state        : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36 10 11 26 38 ...
 $ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
 $ Basecount2   : num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
 $ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425 0.000425 ...


Then I spread it

trnd2_tbl <- trnd1_tbl %>% 
    dplyr::select(city, state, prop_of_total) %>% 
    spread(key = city, value = prop_of_total, fill = 0) #remove the NA's with fill

str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)

Then I run a Kmeans

kmeans_obj1 <- trnd2_tbl  %>% 
  dplyr::select(- state) %>% 
  kmeans(centers = 20, nstart = 100)

str(kmeans_obj1)
List of 9
 $ cluster     : int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
 $ centers     : num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:20] "1" "2" "3" "4" ...
  .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
 $ totss       : num 0.00158
 $ withinss    : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
 $ tot.withinss: num 0.0000848
 $ betweenss   : num 0.0015
 $ size        : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
 $ iter        : int 3
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"

Then I go and try to tidy:

#Tidy, glance, augment
#Just makes it easier to use or view the obj's in the obj list
  
  broom::tidy(kmeans_obj1) %>% glimpse()

	broom::glance(kmeans_obj1)
##A tibble: 1 x 4
# totss tot.withinss betweenss  iter
# <dbl>        <dbl>     <dbl> <int>
#   1 0.00158    0.0000848   0.00150     3

However, when I run this piece I get an error:

broom::augment(kmeans_obj1, trnd2_tbl) %>% 
  dplyr::select(city, .cluster)             

#Error: Must subset columns with a valid subscript vector.
# The subscript has the wrong type `data.frame<
 # u: double
#  x: double
>`.
i It must be numeric or character.

Here is the back trace:

rlang::last_error()

# Backtrace:
#   1. broom::augment(kmeans_obj1, trnd2_tbl)
# 9. dplyr::select(., city, .cluster)
# 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
# 12. tidyselect:::eval_select_impl(...)
# 20. tidyselect:::vars_select_eval(...)
# 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
# 22. tidyselect:::eval_c(expr, data_mask, context_mask)
# 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
# 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
# 25. tidyselect:::as_indices_sel_impl(...)
# 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
# 27. vctrs::vec_as_subscript(x, logical = "error")

I am not sure what I am supposed to fix?

Maybe someone has had similar error and can advise me please?

Thank you.

WHP







Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From Po||ngW @end|ng |rom @etn@@com  Tue May 12 12:15:14 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 12 May 2020 10:15:14 +0000
Subject: [R] Help with R-Markdown please
Message-ID: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>

#UPDATED 05/05/2020
#RStudio Version Version 1.2.1335 need this one--> 1.2.5019
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Good morning.

This is the first time I have tried RMarkdown on new laptop with new employer.
I am not sure how to go about fixing this problem below?

Would someone please advise course of action.

Thank you.

WHP 



processing file: Member-Geo-Location-Test-V1.Rmd
  |..........                                                            |  14%
  ordinary text without R code

  |....................                                                  |  29%
label: unnamed-chunk-1 (with options) 
List of 1
 $ echo: logi FALSE

  |..............................                                        |  43%
  ordinary text without R code

  |........................................                              |  57%
label: setup (with options) 
List of 2
 $ include: logi FALSE
 $ warning: logi FALSE

  |..................................................                    |  71%
  ordinary text without R code

  |............................................................          |  86%
label: Table1 (with options) 
List of 2
 $ echo   : logi FALSE
 $ results: chr "asis"

  |......................................................................| 100%
   inline R code fragments


output file: Member-Geo-Location-Test-V1.knit.md

"C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member-Geo-Location-Test-V1.utf8.md --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash+smart --output Member-Geo-Location-Test-V1.html --email-obfuscation none --self-contained --standalone --section-divs --template "\\winp-oaf-113\FldrRedir_1$\A436798\Data\R\R-4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight --variable highlightjs=1 --variable "theme:bootstrap" --include-in-header "C:\Users\A436798\AppData\Local\Temp\RtmpSajZla\rmarkdown-str2ae846253313.html" --mathjax --variable "mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
Could not fetch http://?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html
HttpExceptionRequest Request {
  host                 = ""
  port                 = 80
  secure               = False
  requestHeaders       = []
  path                 = "/"
  queryString          = "?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html"
  method               = "GET"
  proxy                = Nothing
  rawBody              = False
  redirectCount        = 10
  responseTimeout      = ResponseTimeoutDefault
  requestVersion       = HTTP/1.1
}
 (InvalidDestinationHost "")
Error: pandoc document conversion failed with error 61
Execution halted

WHP


Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From rub@k @end|ng |rom m@th@@@u@dk  Tue May 12 14:42:48 2020
From: rub@k @end|ng |rom m@th@@@u@dk (Ege Rubak)
Date: Tue, 12 May 2020 12:42:48 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>

Looks like your files are on a Windows network drive (UNC path). I have
experienced many problems with this on a Windows laptop from work. If
at all possible you should avoid this. As an absolute minimum make sure
your R library (collection of installed packages) is on the local file
system and not a UNC path. If you are lucky this could be enough to
make things work.

You can check your library location(s) with the command .libPaths() in
an R session.

Good luck.

Ege

On Tue, 2020-05-12 at 10:15 +0000, Poling, William via R-help wrote:
> #UPDATED 05/05/2020
> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo() 
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
> 
> Good morning.
> 
> This is the first time I have tried RMarkdown on new laptop with new
> employer.
> I am not sure how to go about fixing this problem below?
> 
> Would someone please advise course of action.
> 
> Thank you.
> 
> WHP 
> 
> 
> 
> processing file: Member-Geo-Location-Test-V1.Rmd
>  
> |..........                                                          
>   |  14%
>   ordinary text without R code
> 
>  
> |....................                                                
>   |  29%
> label: unnamed-chunk-1 (with options) 
> List of 1
>  $ echo: logi FALSE
> 
>  
> |..............................                                      
>   |  43%
>   ordinary text without R code
> 
>  
> |........................................                            
>   |  57%
> label: setup (with options) 
> List of 2
>  $ include: logi FALSE
>  $ warning: logi FALSE
> 
>  
> |..................................................                  
>   |  71%
>   ordinary text without R code
> 
>  
> |............................................................        
>   |  86%
> label: Table1 (with options) 
> List of 2
>  $ echo   : logi FALSE
>  $ results: chr "asis"
> 
>  
> |....................................................................
> ..| 100%
>    inline R code fragments
> 
> 
> output file: Member-Geo-Location-Test-V1.knit.md
> 
> "C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member-
> Geo-Location-Test-V1.utf8.md --to html4 --from
> markdown+autolink_bare_uris+tex_math_single_backslash+smart --output
> Member-Geo-Location-Test-V1.html --email-obfuscation none --self-
> contained --standalone --section-divs --template "\\winp-oaf-
> 113\FldrRedir_1$\A436798\Data\R\R-
> 4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight --
> variable highlightjs=1 --variable "theme:bootstrap" --include-in-
> header "C:\Users\A436798\AppData\Local\Temp\RtmpSajZla\rmarkdown-
> str2ae846253313.html" --mathjax --variable "mathjax-url:
> https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML
> " --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter
> "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
> Could not fetch http://?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html
> HttpExceptionRequest Request {
>   host                 = ""
>   port                 = 80
>   secure               = False
>   requestHeaders       = []
>   path                 = "/"
>   queryString          = "?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html"
>   method               = "GET"
>   proxy                = Nothing
>   rawBody              = False
>   redirectCount        = 10
>   responseTimeout      = ResponseTimeoutDefault
>   requestVersion       = HTTP/1.1
> }
>  (InvalidDestinationHost "")
> Error: pandoc document conversion failed with error 61
> Execution halted
> 
> WHP
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may
> con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-- 
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University
Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

From Po||ngW @end|ng |rom @etn@@com  Tue May 12 14:47:11 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 12 May 2020 12:47:11 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
Message-ID: <BYAPR06MB5383BD2BE3FDFE4728FDFCB1AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>

Wow, ok, yep "\\\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library"

Thank you Ege. I did not have this issue with previous employer, I will see what our IT group says.

Thank you 

WHP


William H. Poling Ph.D., MPH? |?Senior Data Scientist, Medicare Stars, CVS Health 
p?813-777-5030


Proprietary

-----Original Message-----
From: Ege Rubak <rubak at math.aau.dk> 
Sent: Tuesday, May 12, 2020 7:43 AM
To: Poling, William <PolingW at aetna.com>; r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with R-Markdown please

**** External Email - Use Caution ****

Looks like your files are on a Windows network drive (UNC path). I have experienced many problems with this on a Windows laptop from work. If at all possible you should avoid this. As an absolute minimum make sure your R library (collection of installed packages) is on the local file system and not a UNC path. If you are lucky this could be enough to make things work.

You can check your library location(s) with the command .libPaths() in an R session.

Good luck.

Ege

On Tue, 2020-05-12 at 10:15 +0000, Poling, William via R-help wrote:
> #UPDATED 05/05/2020
> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
> 
> Good morning.
> 
> This is the first time I have tried RMarkdown on new laptop with new 
> employer.
> I am not sure how to go about fixing this problem below?
> 
> Would someone please advise course of action.
> 
> Thank you.
> 
> WHP
> 
> 
> 
> processing file: Member-Geo-Location-Test-V1.Rmd
>  
> |..........                                                          
>   |  14%
>   ordinary text without R code
> 
>  
> |....................                                                
>   |  29%
> label: unnamed-chunk-1 (with options) List of 1  $ echo: logi FALSE
> 
>  
> |..............................                                      
>   |  43%
>   ordinary text without R code
> 
>  
> |........................................                            
>   |  57%
> label: setup (with options)
> List of 2
>  $ include: logi FALSE
>  $ warning: logi FALSE
> 
>  
> |..................................................                  
>   |  71%
>   ordinary text without R code
> 
>  
> |............................................................        
>   |  86%
> label: Table1 (with options)
> List of 2
>  $ echo   : logi FALSE
>  $ results: chr "asis"
> 
>  
> |....................................................................
> ..| 100%
>    inline R code fragments
> 
> 
> output file: Member-Geo-Location-Test-V1.knit.md
> 
> "C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member- 
> Geo-Location-Test-V1.utf8.md --to html4 --from
> markdown+autolink_bare_uris+tex_math_single_backslash+smart --output
> Member-Geo-Location-Test-V1.html --email-obfuscation none --self- 
> contained --standalone --section-divs --template "\\winp-oaf-
> 113\FldrRedir_1$\A436798\Data\R\R-
> 4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight -- 
> variable highlightjs=1 --variable "theme:bootstrap" --include-in- 
> header "C:\Users\A436798\AppData\Local\Temp\RtmpSajZla\rmarkdown-
> str2ae846253313.html" --mathjax --variable "mathjax-url:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__mathjax.rstudio.c
> om_latest_MathJax.js-3Fconfig-3DTeX-2DAMS-2DMML-5FHTMLorMML&d=DwIGaQ&c
> =wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTC
> tKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLXO8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=S
> MNU_B--j9FvBKh-0u1NG5RHd7AtLkvyVsi9ybyUwYU&e=
> " --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter
> "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
> Could not fetch http://?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html
> HttpExceptionRequest Request {
>   host                 = ""
>   port                 = 80
>   secure               = False
>   requestHeaders       = []
>   path                 = "/"
>   queryString          = "?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html"
>   method               = "GET"
>   proxy                = Nothing
>   rawBody              = False
>   redirectCount        = 10
>   responseTimeout      = ResponseTimeoutDefault
>   requestVersion       = HTTP/1.1
> }
>  (InvalidDestinationHost "")
> Error: pandoc document conversion failed with error 61 Execution 
> halted
> 
> WHP
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLXO
> 8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=W5iLmdkyVAbq4AQuAsTYsadWBk1jduptKOZLR25
> jPWo&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLX
> O8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=5V2OtH149c6MxDhlirX-Yd-v8uuEuVYih9DRq-
> ZmBF8&e= and provide commented, minimal, self-contained, reproducible 
> code.
--
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From er|cjberger @end|ng |rom gm@||@com  Tue May 12 15:39:24 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 12 May 2020 16:39:24 +0300
Subject: [R] Help with Kmeans output and using broom to tidy etc..
In-Reply-To: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>

Can you create a reproducible example?
Your question involves objects that are unknown to us. (geo1, trnd1_tbl)

On Tue, May 12, 2020 at 2:41 PM Poling, William via R-help <
r-help at r-project.org> wrote:

> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
>
> Hello:
>
> I have data that I am trying to manipulate for Kmeans clustering.
>
> Original data looks like this
>
> str(geo1)
> # 'data.frame': 2352 obs. of  5 variables:
> # $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690
> 1336 86 726 1702 ...
> # $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9
> 32 13 31 12 12 ...
> # $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230
> 698 965 1330 515 727 1127 1304 ...
> # $ latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
> # $ longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
>
> I created a subset adding column prop_of_total
> str(trnd1_tbl)
> tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
>  $ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8
> 9 10 ...
>  $ state        : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36
> 10 11 26 38 ...
>  $ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
>  $ Basecount2   : num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
>  $ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425
> 0.000425 ...
>
>
> Then I spread it
>
> trnd2_tbl <- trnd1_tbl %>%
>     dplyr::select(city, state, prop_of_total) %>%
>     spread(key = city, value = prop_of_total, fill = 0) #remove the NA's
> with fill
>
> str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)
>
> Then I run a Kmeans
>
> kmeans_obj1 <- trnd2_tbl  %>%
>   dplyr::select(- state) %>%
>   kmeans(centers = 20, nstart = 100)
>
> str(kmeans_obj1)
> List of 9
>  $ cluster     : int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
>  $ centers     : num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:20] "1" "2" "3" "4" ...
>   .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
>  $ totss       : num 0.00158
>  $ withinss    : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
>  $ tot.withinss: num 0.0000848
>  $ betweenss   : num 0.0015
>  $ size        : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
>  $ iter        : int 3
>  $ ifault      : int 0
>  - attr(*, "class")= chr "kmeans"
>
> Then I go and try to tidy:
>
> #Tidy, glance, augment
> #Just makes it easier to use or view the obj's in the obj list
>
>   broom::tidy(kmeans_obj1) %>% glimpse()
>
>         broom::glance(kmeans_obj1)
> ##A tibble: 1 x 4
> # totss tot.withinss betweenss  iter
> # <dbl>        <dbl>     <dbl> <int>
> #   1 0.00158    0.0000848   0.00150     3
>
> However, when I run this piece I get an error:
>
> broom::augment(kmeans_obj1, trnd2_tbl) %>%
>   dplyr::select(city, .cluster)
>
> #Error: Must subset columns with a valid subscript vector.
> # The subscript has the wrong type `data.frame<
>  # u: double
> #  x: double
> >`.
> i It must be numeric or character.
>
> Here is the back trace:
>
> rlang::last_error()
>
> # Backtrace:
> #   1. broom::augment(kmeans_obj1, trnd2_tbl)
> # 9. dplyr::select(., city, .cluster)
> # 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
> # 12. tidyselect:::eval_select_impl(...)
> # 20. tidyselect:::vars_select_eval(...)
> # 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
> # 22. tidyselect:::eval_c(expr, data_mask, context_mask)
> # 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
> # 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
> # 25. tidyselect:::as_indices_sel_impl(...)
> # 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
> # 27. vctrs::vec_as_subscript(x, logical = "error")
>
> I am not sure what I am supposed to fix?
>
> Maybe someone has had similar error and can advise me please?
>
> Thank you.
>
> WHP
>
>
>
>
>
>
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From 538280 @end|ng |rom gm@||@com  Tue May 12 16:01:05 2020
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Tue, 12 May 2020 08:01:05 -0600
Subject: [R] My dream ...
In-Reply-To: <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
Message-ID: <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>

Here is one of my favorites:
https://medium.com/@ODSC/how-300-matchboxes-learned-to-play-tic-tac-toe-using-menace-35e0e4c29fc


On Tue, May 12, 2020 at 2:39 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> > In my opinion the advantage of computers is not Artificial
> > Intelligence, but rather Artificial Patience (most AI that I have seen
> > is really doing a bunch of what I would consider to be boring, really
> > fast so people don't have to).  Leave the Intelligence to the people.
>
> Hmmm...
> https://en.wikipedia.org/wiki/Artificial_intelligence_in_video_games
>
> Also, I found the following while searching for battle chess:
> https://youtu.be/hBNG7444lOw
>
> (Warning: Contains aggressive chess tactics).
>
> Also, correct me if I'm wrong, but doesn't Emacs have historical
> connections to AI research...?



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue May 12 16:16:50 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 12 May 2020 07:16:50 -0700
Subject: [R] Classification of wind events
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
Message-ID: <1E86B627-DA3B-4336-8589-1FC76D12B86E@dcn.davis.ca.us>

Please make a reproducible R example of input and output.

On May 12, 2020 1:11:41 AM PDT, Stefano Sofia <stefano.sofia at regione.marche.it> wrote:
>Dear R list users,
>I am aware that this question is not strictly related, at the present
>moment, to R code and it is more general. Please forgive me, but I need
>to share my thoughts with you.
>
>Foehn conditions on the southern slope of Alps happen with strong
>northerly flows that impact perpendicularly over the Apls. This
>situation triggers strong northerly leeward winds.
>Given a single automatic weather station, I would like to identify
>these periods starting from wind direction and wind intensity data.
>Frequency of data is quarter of hour.
>I would really find difficult to detect the moving windows of these
>events:
>- I can't analyse data day by day;
>- at the beginning and at the end of each event, when the process is
>not at full speed yet, the rotation is not always perfectly
>identifiable;
>- I cannot claim in principle that the direction of each consecutive
>observation is costantly and strictly from the chosen direction.
>
>Does anybody have a clue on how to start to build this process in the
>right way?
>
>Thank you for your attention and your help
>Stefano
>
>         (oo)
>--oOO--( )--OOo----------------
>Stefano Sofia PhD
>Civil Protection - Marche Region
>Meteo Section
>Snow Section
>Via del Colle Ameno 5
>60126 Torrette di Ancona, Ancona
>Uff: 071 806 7743
>E-mail: stefano.sofia at regione.marche.it
>---Oo---------oO----------------
>
>________________________________
>
>AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
>informazioni confidenziali, pertanto ? destinato solo a persone
>autorizzate alla ricezione. I messaggi di posta elettronica per i
>client di Regione Marche possono contenere informazioni confidenziali e
>con privilegi legali. Se non si ? il destinatario specificato, non
>leggere, copiare, inoltrare o archiviare questo messaggio. Se si ?
>ricevuto questo messaggio per errore, inoltrarlo al mittente ed
>eliminarlo completamente dal sistema del proprio computer. Ai sensi
>dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit?
>ed urgenza, la risposta al presente messaggio di posta elettronica pu?
>essere visionata da persone estranee al destinatario.
>IMPORTANT NOTICE: This e-mail message is intended to be received only
>by persons entitled to receive the confidential information it may
>contain. E-mail messages to clients of Regione Marche may contain
>information that is confidential and legally privileged. Please do not
>read, copy, forward, or store this message unless you are an intended
>recipient of it. If you have received this message in error, please
>forward it to the sender and delete it completely from your computer
>system.
>
>--
>Questo messaggio  stato analizzato da Libra ESVA ed  risultato non
>infetto.
>This message was scanned by Libra ESVA and is believed to be clean.
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @@r@h@go@|ee @end|ng |rom gm@||@com  Tue May 12 16:53:58 2020
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Tue, 12 May 2020 10:53:58 -0400
Subject: [R] Convex hulls after nMDS or PCoA
In-Reply-To: <CAE09ejdPAk0BgadOdiU01Tp9LXoW84e-H1Go8U2tAKS2OdhDKg@mail.gmail.com>
References: <CAE09ejdPAk0BgadOdiU01Tp9LXoW84e-H1Go8U2tAKS2OdhDKg@mail.gmail.com>
Message-ID: <CAM_vju=1FXhkGTYY98==Mda=qpMnqU9WN4=ocr8a2K+79QKDhw@mail.gmail.com>

Hi,

I use a modified version of dbscan::hullplot() but https://rseek.org
turns up a bunch of possibilities.

Sarah

On Tue, May 12, 2020 at 2:11 AM Fernando Archuby <farchuby at gmail.com> wrote:
>
> Dear all. I need to build convex hulls around groups of points of an
> ordination (nMDS or PCoA). The groups were independently established with
> cluster analysis. I have learnt to plot convex hulls but would need to
> create every of the 21 groups independently. Do you know how could I just
> add the hulls to the ordination plot in an easier way?
> Thank you very much,
> Fernando.
>
> --


-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From Po||ngW @end|ng |rom @etn@@com  Tue May 12 18:10:55 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 12 May 2020 16:10:55 +0000
Subject: [R] Help with Kmeans output and using broom to tidy etc..
In-Reply-To: <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>
References: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>
Message-ID: <BYAPR06MB53832C0F432D88624E84AC25AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello Eric, thank you so much for your consideration.

Here are snippets of data that I hope will be helpful

WHP 

geo1a <- geo1[, c(2:5)] <-- eliminating ID which is not useful for my purposes anyway

#This is for R-Help use
geo1a <- geo1a %>% top_n(25)

state           city latitude longitude
1     ME      FAIRFIELD 44.64485 -69.65948
2     ME      JONESPORT 44.57935 -67.56743
3     ME        CASWELL 46.97529 -67.83023
4     ME      ELLSWORTH 44.52916 -68.38717
5     ME     VASSALBORO 44.45095 -69.60629
6     ME          UNION 44.20059 -69.26123
7     ME        PALERMO 44.45142 -69.41115
8     ME          ORONO 44.87426 -68.68327
9     ME    SANGERVILLE 45.10138 -69.33580
10    ME      ISLESBORO 44.29015 -68.90812
11    ME        TOPSHAM 43.93600 -69.96565
12    ME       FREEPORT 43.84089 -70.11160
13    ME      SKOWHEGAN 44.76687 -69.71644
14    ME    MILLINOCKET 45.65501 -68.70261
15    ME      ORRINGTON 44.72417 -68.74026
16    ME     ST. GEORGE 43.96726 -69.20827
17    ME FORT FAIRFIELD 46.80911 -67.88079
18    ME      MARS HILL 46.56580 -67.89006
19    ME       FREEPORT 43.85302 -70.03726
20    ME         EASTON 46.64143 -67.91203
21    ME     WATERVILLE 44.53621 -69.65913
22    ME      BRUNSWICK 43.87771 -69.96297
23    ME      BRUNSWICK 43.91719 -69.89905
24    ME      BUCKSPORT 44.60665 -68.81892
25    ME        FAYETTE 44.46380 -70.12047


trnd1_tbla <- trnd1_tbl %>% top_n(25)
print(trnd1_tbla)
head(trnd1_tbla,n=25)

A tibble: 25 x 5
   city      state Basecountsum Basecount2 prop_of_total
   <fct>     <fct>        <dbl>      <dbl>         <dbl>
 1 ATLANTA   GA            2352         12       0.00510
 2 BRADENTON FL            2352          8       0.00340
 3 BROOKLYN  NY            2352         30       0.0128 
 4 CHARLOTTE NC            2352          8       0.00340
 5 CHICAGO   IL            2352         17       0.00723
 6 COLUMBUS  OH            2352         11       0.00468
 7 CUMMING   GA            2352          8       0.00340
 8 DALLAS    TX            2352          8       0.00340
 9 ERIE      PA            2352         12       0.00510
10 HOUSTON   TX            2352         12       0.00510
# ... with 15 more rows

WHP

From: Eric Berger <ericjberger at gmail.com> 
Sent: Tuesday, May 12, 2020 8:39 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Kmeans output and using broom to tidy etc..

**** External Email - Use Caution ****
Can you create a reproducible example??
Your question involves objects that are unknown to us. (geo1, trnd1_tbl)

On Tue, May 12, 2020 at 2:41 PM Poling, William via R-help <mailto:r-help at r-project.org> wrote:
#RStudio Version Version 1.2.1335 need this one--> 1.2.5019
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello:

I have data that I am trying to manipulate for Kmeans clustering.

Original data looks like this

str(geo1) 
# 'data.frame': 2352 obs. of? 5 variables:
# $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 86 726 1702 ...
# $ state? ? ? ? ? ?: Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 31 12 12 ...
# $ city? ? ? ? ? ? : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 727 1127 1304 ...
# $ latitude? ? ? ? : num? 40.4 31.2 40.8 42.1 26.8 ...
# $ longitude? ? ? ?: num? -79.9 -81.5 -74 -91.6 -82.1 ...

I created a subset adding column prop_of_total 
str(trnd1_tbl)
tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
?$ city? ? ? ? ?: Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8 9 10 ...
?$ state? ? ? ? : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36 10 11 26 38 ...
?$ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
?$ Basecount2? ?: num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
?$ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425 0.000425 ...


Then I spread it

trnd2_tbl <- trnd1_tbl %>% 
? ? dplyr::select(city, state, prop_of_total) %>% 
? ? spread(key = city, value = prop_of_total, fill = 0) #remove the NA's with fill

str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)

Then I run a Kmeans

kmeans_obj1 <- trnd2_tbl? %>% 
? dplyr::select(- state) %>% 
? kmeans(centers = 20, nstart = 100)

str(kmeans_obj1)
List of 9
?$ cluster? ? ?: int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
?$ centers? ? ?: num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
? ..- attr(*, "dimnames")=List of 2
? .. ..$ : chr [1:20] "1" "2" "3" "4" ...
? .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
?$ totss? ? ? ?: num 0.00158
?$ withinss? ? : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
?$ tot.withinss: num 0.0000848
?$ betweenss? ?: num 0.0015
?$ size? ? ? ? : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
?$ iter? ? ? ? : int 3
?$ ifault? ? ? : int 0
?- attr(*, "class")= chr "kmeans"

Then I go and try to tidy:

#Tidy, glance, augment
#Just makes it easier to use or view the obj's in the obj list

? broom::tidy(kmeans_obj1) %>% glimpse()

? ? ? ? broom::glance(kmeans_obj1)
##A tibble: 1 x 4
# totss tot.withinss betweenss? iter
# <dbl>? ? ? ? <dbl>? ? ?<dbl> <int>
#? ?1 0.00158? ? 0.0000848? ?0.00150? ? ?3

However, when I run this piece I get an error:

broom::augment(kmeans_obj1, trnd2_tbl) %>% 
? dplyr::select(city, .cluster)? ? ? ? ? ? ?

#Error: Must subset columns with a valid subscript vector.
# The subscript has the wrong type `data.frame<
?# u: double
#? x: double
>`.
i It must be numeric or character.

Here is the back trace:

rlang::last_error()

# Backtrace:
#? ?1. broom::augment(kmeans_obj1, trnd2_tbl)
# 9. dplyr::select(., city, .cluster)
# 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
# 12. tidyselect:::eval_select_impl(...)
# 20. tidyselect:::vars_select_eval(...)
# 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
# 22. tidyselect:::eval_c(expr, data_mask, context_mask)
# 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
# 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
# 25. tidyselect:::as_indices_sel_impl(...)
# 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
# 27. vctrs::vec_as_subscript(x, logical = "error")

I am not sure what I am supposed to fix?

Maybe someone has had similar error and can advise me please?

Thank you.

WHP







Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}

______________________________________________
mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=eSV6ISkAsnmonaRvNdtmx4Lr9vumgXwMYF87DoRP86s&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=8wmXM73ofNcrn1i9gF-qxOzj7zRJZSPcaA5qg0vggG4&e=
and provide commented, minimal, self-contained, reproducible code.

Proprietary

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From er|cjberger @end|ng |rom gm@||@com  Tue May 12 21:07:10 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 12 May 2020 22:07:10 +0300
Subject: [R] Help with Kmeans output and using broom to tidy etc..
In-Reply-To: <BYAPR06MB53832C0F432D88624E84AC25AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>
 <BYAPR06MB53832C0F432D88624E84AC25AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CAGgJW74Zt+8dK=9kbssX_vK2yb_pxkyNiA+Shr-JAAsoi1PJeg@mail.gmail.com>

Please use dput()



On Tue, May 12, 2020 at 7:11 PM Poling, William <PolingW at aetna.com> wrote:

> Hello Eric, thank you so much for your consideration.
>
> Here are snippets of data that I hope will be helpful
>
> WHP
>
> geo1a <- geo1[, c(2:5)] <-- eliminating ID which is not useful for my
> purposes anyway
>
> #This is for R-Help use
> geo1a <- geo1a %>% top_n(25)
>
> state           city latitude longitude
> 1     ME      FAIRFIELD 44.64485 -69.65948
> 2     ME      JONESPORT 44.57935 -67.56743
> 3     ME        CASWELL 46.97529 -67.83023
> 4     ME      ELLSWORTH 44.52916 -68.38717
> 5     ME     VASSALBORO 44.45095 -69.60629
> 6     ME          UNION 44.20059 -69.26123
> 7     ME        PALERMO 44.45142 -69.41115
> 8     ME          ORONO 44.87426 -68.68327
> 9     ME    SANGERVILLE 45.10138 -69.33580
> 10    ME      ISLESBORO 44.29015 -68.90812
> 11    ME        TOPSHAM 43.93600 -69.96565
> 12    ME       FREEPORT 43.84089 -70.11160
> 13    ME      SKOWHEGAN 44.76687 -69.71644
> 14    ME    MILLINOCKET 45.65501 -68.70261
> 15    ME      ORRINGTON 44.72417 -68.74026
> 16    ME     ST. GEORGE 43.96726 -69.20827
> 17    ME FORT FAIRFIELD 46.80911 -67.88079
> 18    ME      MARS HILL 46.56580 -67.89006
> 19    ME       FREEPORT 43.85302 -70.03726
> 20    ME         EASTON 46.64143 -67.91203
> 21    ME     WATERVILLE 44.53621 -69.65913
> 22    ME      BRUNSWICK 43.87771 -69.96297
> 23    ME      BRUNSWICK 43.91719 -69.89905
> 24    ME      BUCKSPORT 44.60665 -68.81892
> 25    ME        FAYETTE 44.46380 -70.12047
>
>
> trnd1_tbla <- trnd1_tbl %>% top_n(25)
> print(trnd1_tbla)
> head(trnd1_tbla,n=25)
>
> A tibble: 25 x 5
>    city      state Basecountsum Basecount2 prop_of_total
>    <fct>     <fct>        <dbl>      <dbl>         <dbl>
>  1 ATLANTA   GA            2352         12       0.00510
>  2 BRADENTON FL            2352          8       0.00340
>  3 BROOKLYN  NY            2352         30       0.0128
>  4 CHARLOTTE NC            2352          8       0.00340
>  5 CHICAGO   IL            2352         17       0.00723
>  6 COLUMBUS  OH            2352         11       0.00468
>  7 CUMMING   GA            2352          8       0.00340
>  8 DALLAS    TX            2352          8       0.00340
>  9 ERIE      PA            2352         12       0.00510
> 10 HOUSTON   TX            2352         12       0.00510
> # ... with 15 more rows
>
> WHP
>
> From: Eric Berger <ericjberger at gmail.com>
> Sent: Tuesday, May 12, 2020 8:39 AM
> To: Poling, William <PolingW at aetna.com>
> Cc: r-help at r-project.org
> Subject: [EXTERNAL] Re: [R] Help with Kmeans output and using broom to
> tidy etc..
>
> **** External Email - Use Caution ****
> Can you create a reproducible example?
> Your question involves objects that are unknown to us. (geo1, trnd1_tbl)
>
> On Tue, May 12, 2020 at 2:41 PM Poling, William via R-help <mailto:
> r-help at r-project.org> wrote:
> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
>
> Hello:
>
> I have data that I am trying to manipulate for Kmeans clustering.
>
> Original data looks like this
>
> str(geo1)
> # 'data.frame': 2352 obs. of  5 variables:
> # $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690
> 1336 86 726 1702 ...
> # $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9
> 32 13 31 12 12 ...
> # $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230
> 698 965 1330 515 727 1127 1304 ...
> # $ latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
> # $ longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...
>
> I created a subset adding column prop_of_total
> str(trnd1_tbl)
> tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
>  $ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8
> 9 10 ...
>  $ state        : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36
> 10 11 26 38 ...
>  $ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
>  $ Basecount2   : num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
>  $ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425
> 0.000425 ...
>
>
> Then I spread it
>
> trnd2_tbl <- trnd1_tbl %>%
>     dplyr::select(city, state, prop_of_total) %>%
>     spread(key = city, value = prop_of_total, fill = 0) #remove the NA's
> with fill
>
> str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)
>
> Then I run a Kmeans
>
> kmeans_obj1 <- trnd2_tbl  %>%
>   dplyr::select(- state) %>%
>   kmeans(centers = 20, nstart = 100)
>
> str(kmeans_obj1)
> List of 9
>  $ cluster     : int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
>  $ centers     : num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:20] "1" "2" "3" "4" ...
>   .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
>  $ totss       : num 0.00158
>  $ withinss    : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
>  $ tot.withinss: num 0.0000848
>  $ betweenss   : num 0.0015
>  $ size        : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
>  $ iter        : int 3
>  $ ifault      : int 0
>  - attr(*, "class")= chr "kmeans"
>
> Then I go and try to tidy:
>
> #Tidy, glance, augment
> #Just makes it easier to use or view the obj's in the obj list
>
>   broom::tidy(kmeans_obj1) %>% glimpse()
>
>         broom::glance(kmeans_obj1)
> ##A tibble: 1 x 4
> # totss tot.withinss betweenss  iter
> # <dbl>        <dbl>     <dbl> <int>
> #   1 0.00158    0.0000848   0.00150     3
>
> However, when I run this piece I get an error:
>
> broom::augment(kmeans_obj1, trnd2_tbl) %>%
>   dplyr::select(city, .cluster)
>
> #Error: Must subset columns with a valid subscript vector.
> # The subscript has the wrong type `data.frame<
>  # u: double
> #  x: double
> >`.
> i It must be numeric or character.
>
> Here is the back trace:
>
> rlang::last_error()
>
> # Backtrace:
> #   1. broom::augment(kmeans_obj1, trnd2_tbl)
> # 9. dplyr::select(., city, .cluster)
> # 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
> # 12. tidyselect:::eval_select_impl(...)
> # 20. tidyselect:::vars_select_eval(...)
> # 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
> # 22. tidyselect:::eval_c(expr, data_mask, context_mask)
> # 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
> # 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
> # 25. tidyselect:::as_indices_sel_impl(...)
> # 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
> # 27. vctrs::vec_as_subscript(x, logical = "error")
>
> I am not sure what I am supposed to fix?
>
> Maybe someone has had similar error and can advise me please?
>
> Thank you.
>
> WHP
>
>
>
>
>
>
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}
>
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=eSV6ISkAsnmonaRvNdtmx4Lr9vumgXwMYF87DoRP86s&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=8wmXM73ofNcrn1i9gF-qxOzj7zRJZSPcaA5qg0vggG4&e=
> and provide commented, minimal, self-contained, reproducible code.
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:
> This e-mail may contain confidential or privileged information. If you
> think you have received this e-mail in error, please advise the sender by
> reply e-mail and then delete this e-mail immediately.
> This e-mail may also contain protected health information (PHI) with
> information about sensitive medical conditions, including, but not limited
> to, treatment for substance use disorders, behavioral health, HIV/AIDS, or
> pregnancy. This type of information may be protected by various federal
> and/or state laws which prohibit any further disclosure without the express
> written consent of the person to whom it pertains or as otherwise permitted
> by law. Any unauthorized further disclosure may be considered a violation
> of federal and/or state law. A general authorization for the release of
> medical or other information may NOT be sufficient consent for release of
> this type of information.
> Thank you. Aetna
>

	[[alternative HTML version deleted]]


From Po||ngW @end|ng |rom @etn@@com  Tue May 12 21:19:58 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Tue, 12 May 2020 19:19:58 +0000
Subject: [R] Help with Kmeans output and using broom to tidy etc..
In-Reply-To: <CAGgJW74Zt+8dK=9kbssX_vK2yb_pxkyNiA+Shr-JAAsoi1PJeg@mail.gmail.com>
References: <BYAPR06MB5383EA994C10F43076877EE4AEA10@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGgJW74OPs+mB8OKLc0BCpaCiNMO08=-Ex+8mW5qtk38+Lnh8A@mail.gmail.com>
 <BYAPR06MB53832C0F432D88624E84AC25AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGgJW74Zt+8dK=9kbssX_vK2yb_pxkyNiA+Shr-JAAsoi1PJeg@mail.gmail.com>
Message-ID: <BYAPR06MB5383571CED3926DF39CA0D96AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>

Yes, of course. Thank you for your patience.

str(geo1a)
'data.frame':     25 obs. of  4 variables:
$ state    : Factor w/ 41 levels "AL","AR","AZ",..: 19 19 19 19 19 19 19 19 19 19 ...
$ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 366 574 193 343 1223 1207 885 871 1054 556 ...
$ latitude : num  44.6 44.6 47 44.5 44.5 ...
$ longitude: num  -69.7 -67.6 -67.8 -68.4 -69.6 ...

dput(geo1a)
structure(list(state = structure(c(19L, 19L, 19L, 19L, 19L, 19L,
19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L,
19L, 19L, 19L, 19L, 19L, 19L), .Label = c("AL", "AR", "AZ", "CA",
"CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL", "IN", "KS", "KY",
"LA", "MA", "MD", "ME", "MI", "MN", "MO", "NC", "NE", "NJ", "NM",
"NV", "NY", "OH", "OK", "OR", "PA", "SC", "SD", "TN", "TX", "UT",
"VA", "WA", "WI", "WV"), class = "factor"), city = structure(c(366L,
574L, 193L, 343L, 1223L, 1207L, 885L, 871L, 1054L, 556L, 1192L,
414L, 1097L, 749L, 872L, 1134L, 397L, 700L, 414L, 332L, 1256L,
156L, 156L, 158L, 375L), .Label = c("ABBOTTSTOWN", "ABILENE",
"ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE",
"ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK",
"ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO",
"AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON",
"ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA",
"APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE",
"ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO",
"ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS",
"ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL",
"AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY",
"BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT",
"BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG",
"BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE",
"BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY",
"BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR",
"BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET",
"BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY",
"BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF",
"BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN",
"BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD",
"BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON",
"BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS",
"BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN",
"BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE",
"BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD",
"BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH",
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD",
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE",
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
    latitude = c(44.644849, 44.579346, 46.975293, 44.529164,
    44.450953, 44.200589, 44.45142, 44.874264, 45.101378, 44.29015,
    43.935997, 43.840888, 44.766867, 45.655006, 44.724172, 43.967257,
    46.80911, 46.565795, 43.853017, 46.64143, 44.53621, 43.877711,
    43.91719, 44.606652, 44.4638), longitude = c(-69.659477,
    -67.567428, -67.83023, -68.387175, -69.606293, -69.261228,
    -69.411145, -68.683265, -69.335795, -68.90812, -69.965652,
    -70.111596, -69.716441, -68.702608, -68.740265, -69.208272,
    -67.880793, -67.890059, -70.037259, -67.91203, -69.65913,
    -69.962967, -69.899049, -68.818915, -70.12047)), row.names = c(NA,
-25L), class = "data.frame")

str(trnd1_tbla)
tibble [27 x 5] (S3: tbl_df/tbl/data.frame)
$ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 50 130 152 209 217 242 269 272 358 530 ...
$ state        : Factor w/ 41 levels "AL","AR","AZ",..: 10 9 28 23 12 29 10 36 32 36 ...
$ Basecountsum : num [1:27] 2352 2352 2352 2352 2352 ...
$ Basecount2   : num [1:27] 12 8 30 8 17 11 8 8 12 12 ...
$ prop_of_total: num [1:27] 0.0051 0.0034 0.01276 0.0034 0.00723 ...

dput(trnd1_tbla)
structure(list(city = structure(c(50L, 130L, 152L, 209L, 217L,
242L, 269L, 272L, 358L, 530L, 544L, 563L, 580L, 613L, 618L, 879L,
919L, 932L, 970L, 1002L, 1033L, 1045L, 1056L, 1127L, 1140L, 1296L,
1330L), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS",
"ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA",
"ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN",
"ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", "AMBLER",
"AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER",
"ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", "APEX",
"APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY",
"ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND",
"ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH",
"AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA",
"AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE",
"BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT",
"BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON ROUGE",
"BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN",
"BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR",
"BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE",
"BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON",
"BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL",
"BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE",
"BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG",
"BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL",
"BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH",
"BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE",
"BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", "BOYERTOWN",
"BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", "BRANDON",
"BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", "BRENTON",
"BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON",
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW",
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS",
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT",
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK",
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK",
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL",
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON",
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON",
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE",
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO",
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON",
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT",
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON",
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE",
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE",
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH",
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES",
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS",
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION",
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE",
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY",
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA",
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY",
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN",
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER",
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS",
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT",
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR",
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH",
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY",
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER",
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER",
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH",
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE",
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO",
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND",
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT",
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON",
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE",
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY",
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY",
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW",
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE",
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD",
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS",
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE",
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE",
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT",
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM",
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD",
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE",
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH",
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM",
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON",
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE",
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA",
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE",
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW",
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM",
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS",
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS",
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER",
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE",
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD",
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON",
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE",
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON",
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL",
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE",
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH",
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE",
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS",
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE",
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD",
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS",
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON",
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON",
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE",
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL",
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY",
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP",
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS",
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE",
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN",
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE",
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON",
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN",
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN",
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF",
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS",
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH",
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE",
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS",
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE",
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD",
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG",
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES",
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN",
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA",
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL",
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH",
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG",
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF",
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON",
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE",
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP",
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON",
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS",
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL",
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE",
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN",
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND",
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN",
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG",
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS",
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA",
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN",
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK",
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE",
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION",
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY",
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE",
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON",
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE",
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO",
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT",
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT",
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS",
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES",
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG",
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK",
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON",
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE",
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK",
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT",
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA",
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK",
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON",
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS",
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE",
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON",
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE",
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION",
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO",
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD",
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR",
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE",
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY",
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN",
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE",
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS",
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI",
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN",
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY",
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY",
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE",
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE",
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON",
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE",
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE",
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT",
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY",
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN",
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK",
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL",
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON",
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE",
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH",
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS",
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE",
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT",
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN",
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO",
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR",
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS",
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY",
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO",
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN",
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA",
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE",
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE",
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN",
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG",
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON",
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE",
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY",
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON",
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO",
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND",
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD",
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE",
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH",
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY",
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL",
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG",
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY",
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY",
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD",
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD",
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT",
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY",
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE",
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE",
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE",
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK",
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA",
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST",
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER",
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY",
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA",
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO",
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO",
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA",
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH",
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN",
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG",
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA",
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE",
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER",
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO",
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN",
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH",
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD",
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER",
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD",
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE",
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD",
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK",
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS",
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON",
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE",
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS",
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH",
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS",
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
    state = structure(c(10L, 9L, 28L, 23L, 12L, 29L, 10L, 36L,
    32L, 36L, 22L, 9L, 22L, 32L, 27L, 14L, 32L, 32L, 23L, 12L,
    22L, 36L, 9L, 22L, 28L, 14L, 32L), .Label = c("AL", "AR",
    "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "IA", "IL",
    "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO",
    "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA",
    "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"), class = "factor"),
    Basecountsum = c(2352, 2352, 2352, 2352, 2352, 2352, 2352,
    2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352,
    2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352, 2352
    ), Basecount2 = c(12, 8, 30, 8, 17, 11, 8, 8, 12, 12, 8,
    9, 11, 9, 14, 8, 8, 23, 10, 12, 14, 20, 8, 8, 21, 9, 11),
    prop_of_total = c(0.00510204081632653, 0.00340136054421769,
    0.0127551020408163, 0.00340136054421769, 0.00722789115646259,
    0.00467687074829932, 0.00340136054421769, 0.00340136054421769,
    0.00510204081632653, 0.00510204081632653, 0.00340136054421769,
    0.0038265306122449, 0.00467687074829932, 0.0038265306122449,
    0.00595238095238095, 0.00340136054421769, 0.00340136054421769,
    0.00977891156462585, 0.00425170068027211, 0.00510204081632653,
    0.00595238095238095, 0.00850340136054422, 0.00340136054421769,
    0.00340136054421769, 0.00892857142857143, 0.0038265306122449,
    0.00467687074829932)), class = c("tbl_df", "tbl", "data.frame"
), row.names = c(NA, -27L))



William H. Poling Ph.D., MPH  | Senior Data Scientist, Medicare Stars, CVS Health
p 813-777-5030

From: Eric Berger <ericjberger at gmail.com>
Sent: Tuesday, May 12, 2020 2:07 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: Re: [R] Help with Kmeans output and using broom to tidy etc..

**** External Email - Use Caution ****
Please use dput()



On Tue, May 12, 2020 at 7:11 PM Poling, William <PolingW at aetna.com<mailto:PolingW at aetna.com>> wrote:
Hello Eric, thank you so much for your consideration.

Here are snippets of data that I hope will be helpful

WHP

geo1a <- geo1[, c(2:5)] <-- eliminating ID which is not useful for my purposes anyway

#This is for R-Help use
geo1a <- geo1a %>% top_n(25)

state           city latitude longitude
1     ME      FAIRFIELD 44.64485 -69.65948
2     ME      JONESPORT 44.57935 -67.56743
3     ME        CASWELL 46.97529 -67.83023
4     ME      ELLSWORTH 44.52916 -68.38717
5     ME     VASSALBORO 44.45095 -69.60629
6     ME          UNION 44.20059 -69.26123
7     ME        PALERMO 44.45142 -69.41115
8     ME          ORONO 44.87426 -68.68327
9     ME    SANGERVILLE 45.10138 -69.33580
10    ME      ISLESBORO 44.29015 -68.90812
11    ME        TOPSHAM 43.93600 -69.96565
12    ME       FREEPORT 43.84089 -70.11160
13    ME      SKOWHEGAN 44.76687 -69.71644
14    ME    MILLINOCKET 45.65501 -68.70261
15    ME      ORRINGTON 44.72417 -68.74026
16    ME     ST. GEORGE 43.96726 -69.20827
17    ME FORT FAIRFIELD 46.80911 -67.88079
18    ME      MARS HILL 46.56580 -67.89006
19    ME       FREEPORT 43.85302 -70.03726
20    ME         EASTON 46.64143 -67.91203
21    ME     WATERVILLE 44.53621 -69.65913
22    ME      BRUNSWICK 43.87771 -69.96297
23    ME      BRUNSWICK 43.91719 -69.89905
24    ME      BUCKSPORT 44.60665 -68.81892
25    ME        FAYETTE 44.46380 -70.12047


trnd1_tbla <- trnd1_tbl %>% top_n(25)
print(trnd1_tbla)
head(trnd1_tbla,n=25)

A tibble: 25 x 5
   city      state Basecountsum Basecount2 prop_of_total
   <fct>     <fct>        <dbl>      <dbl>         <dbl>
 1 ATLANTA   GA            2352         12       0.00510
 2 BRADENTON FL            2352          8       0.00340
 3 BROOKLYN  NY            2352         30       0.0128
 4 CHARLOTTE NC            2352          8       0.00340
 5 CHICAGO   IL            2352         17       0.00723
 6 COLUMBUS  OH            2352         11       0.00468
 7 CUMMING   GA            2352          8       0.00340
 8 DALLAS    TX            2352          8       0.00340
 9 ERIE      PA            2352         12       0.00510
10 HOUSTON   TX            2352         12       0.00510
# ... with 15 more rows

WHP

From: Eric Berger <ericjberger at gmail.com<mailto:ericjberger at gmail.com>>
Sent: Tuesday, May 12, 2020 8:39 AM
To: Poling, William <PolingW at aetna.com<mailto:PolingW at aetna.com>>
Cc: r-help at r-project.org<mailto:r-help at r-project.org>
Subject: [EXTERNAL] Re: [R] Help with Kmeans output and using broom to tidy etc..

**** External Email - Use Caution ****
Can you create a reproducible example?
Your question involves objects that are unknown to us. (geo1, trnd1_tbl)

On Tue, May 12, 2020 at 2:41 PM Poling, William via R-help <mailto:r-help at r-project.org<mailto:r-help at r-project.org>> wrote:
#RStudio Version Version 1.2.1335 need this one--> 1.2.5019
sessionInfo()
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello:

I have data that I am trying to manipulate for Kmeans clustering.

Original data looks like this

str(geo1)
# 'data.frame': 2352 obs. of  5 variables:
# $ ID: Factor w/ 2352 levels "101040199600",..: 590 908 976 509 1674 690 1336 86 726 1702 ...
# $ state           : Factor w/ 41 levels "AL","AR","AZ",..: 32 10 25 11 9 32 13 31 12 12 ...
# $ city            : Factor w/ 1337 levels "ABBOTTSTOWN",..: 932 156 230 698 965 1330 515 727 1127 1304 ...
# $ latitude        : num  40.4 31.2 40.8 42.1 26.8 ...
# $ longitude       : num  -79.9 -81.5 -74 -91.6 -82.1 ...

I created a subset adding column prop_of_total
str(trnd1_tbl)
tibble [1,457 x 5] (S3: tbl_df/tbl/data.frame)
 $ city         : Factor w/ 1337 levels "ABBOTTSTOWN",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ state        : Factor w/ 41 levels "AL","AR","AZ",..: 32 36 10 28 12 36 10 11 26 38 ...
 $ Basecountsum : num [1:1457] 2352 2352 2352 2352 2352 ...
 $ Basecount2   : num [1:1457] 1 1 1 1 1 2 1 1 2 1 ...
 $ prop_of_total: num [1:1457] 0.000425 0.000425 0.000425 0.000425 0.000425 ...


Then I spread it

trnd2_tbl <- trnd1_tbl %>%
    dplyr::select(city, state, prop_of_total) %>%
    spread(key = city, value = prop_of_total, fill = 0) #remove the NA's with fill

str(trnd2_tbl)#tibble [41 x 1,338] (S3: tbl_df/tbl/data.frame)

Then I run a Kmeans

kmeans_obj1 <- trnd2_tbl  %>%
  dplyr::select(- state) %>%
  kmeans(centers = 20, nstart = 100)

str(kmeans_obj1)
List of 9
 $ cluster     : int [1:41] 11 11 9 11 11 4 11 11 16 2 ...
 $ centers     : num [1:20, 1:1337] 0 0 0 0 0 0 0 0 0 0 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:20] "1" "2" "3" "4" ...
  .. ..$ : chr [1:1337] "ABBOTTSTOWN" "ABILENE" "ACWORTH" "ADAMS" ...
 $ totss       : num 0.00158
 $ withinss    : num [1:20] 0 0 0 0 0 0 0 0 0 0 ...
 $ tot.withinss: num 0.0000848
 $ betweenss   : num 0.0015
 $ size        : int [1:20] 1 1 1 1 1 1 1 1 1 1 ...
 $ iter        : int 3
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"

Then I go and try to tidy:

#Tidy, glance, augment
#Just makes it easier to use or view the obj's in the obj list

  broom::tidy(kmeans_obj1) %>% glimpse()

        broom::glance(kmeans_obj1)
##A tibble: 1 x 4
# totss tot.withinss betweenss  iter
# <dbl>        <dbl>     <dbl> <int>
#   1 0.00158    0.0000848   0.00150     3

However, when I run this piece I get an error:

broom::augment(kmeans_obj1, trnd2_tbl) %>%
  dplyr::select(city, .cluster)

#Error: Must subset columns with a valid subscript vector.
# The subscript has the wrong type `data.frame<
 # u: double
#  x: double
>`.
i It must be numeric or character.

Here is the back trace:

rlang::last_error()

# Backtrace:
#   1. broom::augment(kmeans_obj1, trnd2_tbl)
# 9. dplyr::select(., city, .cluster)
# 11. tidyselect::vars_select(tbl_vars(.data), !!!enquos(...))
# 12. tidyselect:::eval_select_impl(...)
# 20. tidyselect:::vars_select_eval(...)
# 21. tidyselect:::walk_data_tree(expr, data_mask, context_mask)
# 22. tidyselect:::eval_c(expr, data_mask, context_mask)
# 23. tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
# 24. tidyselect:::walk_data_tree(new, data_mask, context_mask)
# 25. tidyselect:::as_indices_sel_impl(...)
# 26. tidyselect:::as_indices_impl(x, vars, strict = strict)
# 27. vctrs::vec_as_subscript(x, logical = "error")

I am not sure what I am supposed to fix?

Maybe someone has had similar error and can advise me please?

Thank you.

WHP







Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}

______________________________________________
mailto:R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Dhelp&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=eSV6ISkAsnmonaRvNdtmx4Lr9vumgXwMYF87DoRP86s&e=
PLEASE do read the posting guide https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.org_posting-2Dguide.html&d=DwMFaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=sMhCVDVDKajwJ9te2qVsWXQ2aq4kAe7150EICM51Pw4&s=8wmXM73ofNcrn1i9gF-qxOzj7zRJZSPcaA5qg0vggG4&e=
and provide commented, minimal, self-contained, reproducible code.

Proprietary

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna


Proprietary

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 00:09:56 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 08:09:56 +1000
Subject: [R] My dream ...
In-Reply-To: <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
Message-ID: <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>

Abby Spurdle:
In my opinion the advantage of computers is not Artificial
Intelligence, but rather Artificial Patience (most AI that I have seen
is really doing a bunch of what I would consider to be boring, really
fast so people don't have to).  Leave the Intelligence to the people.

Abby's response contains a complaint that is often directed at
technical advances. So what if we can devise a way to perform some
boring task rapidly? I answer that it allows us to delegate the boring
task to the machine and proceed with the integration of the results.
We run the risk of Douglas Adams' delightful result that we cannot
understand, but nearly all of the "big" scientific endeavors stand
upon the shoulders of machines doing boring tasks whose duration at
human speed would see us all out. My idea of AI is a sort of teamwork
between the error-prone synthesis of man and the precise analysis of
machine, not a struggle for dominance  of one or the other.

Jim


From you@r|@|@nou@ @end|ng |rom gm@||@com  Wed May 13 01:37:32 2020
From: you@r|@|@nou@ @end|ng |rom gm@||@com (Yousri Fanous)
Date: Tue, 12 May 2020 19:37:32 -0400
Subject: [R] How to write the segmented equation from MARS coefficients
Message-ID: <CADsEwScSk0vqd90rMsk4MaUq2TrN6jRwo9z=xSL_dzThAHx=GQ@mail.gmail.com>

Hello

I created a MARS model from earth package.for one predictor EngDispl vs FE.
These are the resulting coefficients:

 summary(marsFit)$coefficients
                        y
(Intercept)     18.049001
h(EngDispl-4.3) -4.378082
h(4.3-EngDispl) 10.925240
h(EngDispl-2.3)  5.942387
h(EngDispl-3.5) -2.552332

I am at a loss at writing the equation linking the dependent and
independent variables for such an output.
What is h? Is it a function / operator?

Thank you for your help

Yousri fanous
IBM Canada
Software developer


From @purd|e@@ @end|ng |rom gm@||@com  Wed May 13 02:13:09 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Wed, 13 May 2020 12:13:09 +1200
Subject: [R] My dream ...
In-Reply-To: <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
 <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
Message-ID: <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>

Hi Jim,

I think you've mis-quoted me.
I didn't say that.


On Wed, May 13, 2020 at 10:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Abby Spurdle:
> In my opinion the advantage of computers is not Artificial
> Intelligence, but rather Artificial Patience (most AI that I have seen
> is really doing a bunch of what I would consider to be boring, really
> fast so people don't have to).  Leave the Intelligence to the people.
>
> Abby's response contains a complaint that is often directed at
> technical advances. So what if we can devise a way to perform some
> boring task rapidly? I answer that it allows us to delegate the boring
> task to the machine and proceed with the integration of the results.
> We run the risk of Douglas Adams' delightful result that we cannot
> understand, but nearly all of the "big" scientific endeavors stand
> upon the shoulders of machines doing boring tasks whose duration at
> human speed would see us all out. My idea of AI is a sort of teamwork
> between the error-prone synthesis of man and the precise analysis of
> machine, not a struggle for dominance  of one or the other.
>
> Jim
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 02:23:49 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 10:23:49 +1000
Subject: [R] My dream ...
In-Reply-To: <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
 <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
 <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>
Message-ID: <CA+8X3fUGA+7YbJsUij2eUb_hPLOQXAezJ5apMwwmgdNUzR8AHg@mail.gmail.com>

Sorry, it was listed in Hans' email as a reply from you. Far be it
from me to speak for someone else.

Jim

On Wed, May 13, 2020 at 10:13 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> Hi Jim,
>
> I think you've mis-quoted me.
> I didn't say that.
>
>
> On Wed, May 13, 2020 at 10:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Abby Spurdle:
> > In my opinion the advantage of computers is not Artificial
> > Intelligence, but rather Artificial Patience (most AI that I have seen
> > is really doing a bunch of what I would consider to be boring, really
> > fast so people don't have to).  Leave the Intelligence to the people.
> >
> > Abby's response contains a complaint that is often directed at
> > technical advances. So what if we can devise a way to perform some
> > boring task rapidly? I answer that it allows us to delegate the boring
> > task to the machine and proceed with the integration of the results.
> > We run the risk of Douglas Adams' delightful result that we cannot
> > understand, but nearly all of the "big" scientific endeavors stand
> > upon the shoulders of machines doing boring tasks whose duration at
> > human speed would see us all out. My idea of AI is a sort of teamwork
> > between the error-prone synthesis of man and the precise analysis of
> > machine, not a struggle for dominance  of one or the other.
> >
> > Jim
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 13 02:25:49 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 12 May 2020 17:25:49 -0700
Subject: [R] How to write the segmented equation from MARS coefficients
In-Reply-To: <CADsEwScSk0vqd90rMsk4MaUq2TrN6jRwo9z=xSL_dzThAHx=GQ@mail.gmail.com>
References: <CADsEwScSk0vqd90rMsk4MaUq2TrN6jRwo9z=xSL_dzThAHx=GQ@mail.gmail.com>
Message-ID: <8892166D-3257-4577-8412-C8866887BB74@dcn.davis.ca.us>

Carefully read the description of the package... it uses a particular variety of splines. You should go read the paper they reference.

On May 12, 2020 4:37:32 PM PDT, Yousri Fanous <yousri.fanous at gmail.com> wrote:
>Hello
>
>I created a MARS model from earth package.for one predictor EngDispl vs
>FE.
>These are the resulting coefficients:
>
> summary(marsFit)$coefficients
>                        y
>(Intercept)     18.049001
>h(EngDispl-4.3) -4.378082
>h(4.3-EngDispl) 10.925240
>h(EngDispl-2.3)  5.942387
>h(EngDispl-3.5) -2.552332
>
>I am at a loss at writing the equation linking the dependent and
>independent variables for such an output.
>What is h? Is it a function / operator?
>
>Thank you for your help
>
>Yousri fanous
>IBM Canada
>Software developer
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 02:56:45 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 00:56:45 +0000
Subject: [R] Help with Radius problem
Message-ID: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

#RStudio Version Version 1.2.1335 
sessionInfo() 
# R version 4.0.0 Patched (2020-05-03 r78349)
#Platform: x86_64-w64-mingw32/x64 (64-bit)
#Running under: Windows 10 x64 (build 17763)

Hello I am trying to figure out how to create concentration of ID's in the following data, based on determining the highest concentration in the smallest radius area based on Longitude and Latitude (geo location)

I have reviewed many websites looking for a function or tutorial, (if you are aware of either please let me know)

I have worked through the following to see if they are close to what I need, but they are not.

#Possible use https://stackoverflow.com/questions/21977720/r-finding-closest-neighboring-point-and-number-of-neighbors-within-a-given-rad

#Possible math ref http://janmatuschek.de/LatitudeLongitudeBoundingCoordinates

#Possible use https://stackoverflow.com/questions/39008850/find-number-of-points-within-a-radius-in-r-using-lon-and-lat-coordinates

# https://gis.stackexchange.com/questions/229453/create-a-circle-of-defined-radius-around-a-point-and-then-find-the-overlapping-a

# https://stackoverflow.com/questions/23071026/drawing-a-circle-with-a-radius-of-a-defined-distance-in-a-map/23072079#23072079

My actual data is 2353 records with 41 states and 1337 cities represented. Here is a sample below.

Some might call this a hotspot question I suppose: "What is the maximum number of ID's I can gather in the smallest radius size given the data at hand?

For example it would be useful to have a function that I could use to test different combinations of radius size and concentration of ID #'s by geo location.

Any insight would be very much appreciated.

Thank you.
WHP

str(sample)
'data.frame':	35 obs. of  5 variables:
 $ state    : Factor w/ 41 levels "AL","AR","AZ",..: 19 29 9 9 10 30 33 35 41 12 ...
 $ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 281 1280 129 223 989 721 550 731 1325 688 ...
 $ latitude : num  43.5 40.1 26.5 27.9 31.9 ...
 $ longitude: num  -70.6 -82.9 -80.1 -82.7 -81.4 ...
 $ ID       : int  2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 ...

dput(sample)
structure(list(state = structure(c(19L, 29L, 9L, 9L, 10L, 30L, 
33L, 35L, 41L, 12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 
28L, 12L, 28L, 28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 
10L, 22L, 10L), .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", 
"DC", "DE", "FL", "GA", "IA", "IL", "IN", "KS", "KY", "LA", "MA", 
"MD", "ME", "MI", "MN", "MO", "NC", "NE", "NJ", "NM", "NV", "NY", 
"OH", "OK", "OR", "PA", "SC", "SD", "TN", "TX", "UT", "VA", "WA", 
"WI", "WV"), class = "factor"), city = structure(c(281L, 1280L, 
129L, 223L, 989L, 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 
759L, 1171L, 1305L, 587L, 272L, 581L, 263L, 152L, 217L, 152L, 
390L, 5L, 571L, 1006L, 1162L, 939L, 170L, 1033L, 1002L, 586L, 
586L, 192L, 586L), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", 
"ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", 
"ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", 
"ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", 
"AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", 
"ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", 
"APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", 
"ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", 
"ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", 
"ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", 
"AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", 
"BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", 
"BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", 
"BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", 
"BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", 
"BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", 
"BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", 
"BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", 
"BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", 
"BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", 
"BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", 
"BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", 
"BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", 
"BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", 
"BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", 
"BRANDON", "BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", 
"BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON", 
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW", 
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS", 
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT", 
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK", 
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK", 
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL", 
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON", 
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON", 
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE", 
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO", 
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON", 
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT", 
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON", 
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE", 
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE", 
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH", 
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES", 
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS", 
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION", 
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", 
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", 
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", 
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", 
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", 
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", 
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", 
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", 
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", 
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", 
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", 
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", 
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO", 
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND", 
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT", 
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON", 
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE", 
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", 
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY", 
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW", 
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE", 
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD", 
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS", 
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE", 
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE", 
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT", 
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM", 
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD", 
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE", 
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH", 
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM", 
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON", 
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", 
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", 
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", 
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", 
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", 
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", 
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", 
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", 
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", 
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", 
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", 
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", 
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", 
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", 
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", 
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", 
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", 
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", 
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", 
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", 
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", 
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", 
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON", 
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN", 
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN", 
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF", 
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS", 
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH", 
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE", 
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS", 
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE", 
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD", 
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG", 
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES", 
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN", 
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA", 
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL", 
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH", 
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG", 
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF", 
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON", 
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", 
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP", 
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON", 
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS", 
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL", 
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", 
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", 
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", 
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", 
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", 
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", 
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", 
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", 
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", 
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", 
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", 
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", 
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", 
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", 
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", 
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", 
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", 
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", 
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", 
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", 
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", 
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS", 
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", 
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", 
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", 
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", 
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", 
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", 
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", 
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", 
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", 
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", 
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", 
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", 
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", 
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", 
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE", 
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE", 
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON", 
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE", 
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE", 
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT", 
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY", 
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN", 
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK", 
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL", 
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON", 
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE", 
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH", 
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS", 
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", 
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", 
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA", 
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE", 
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE", 
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN", 
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG", 
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON", 
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE", 
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY", 
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON", 
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO", 
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND", 
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD", 
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE", 
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH", 
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY", 
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL", 
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG", 
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY", 
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY", 
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD", 
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD", 
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT", 
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY", 
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE", 
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE", 
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE", 
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK", 
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA", 
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST", 
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", 
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", 
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", 
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", 
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", 
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", 
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", 
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", 
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE", 
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", 
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", 
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"), 
    latitude = c(43.53462, 40.142492, 26.538149, 27.9062, 31.90104, 
    34.833083, 35.118836, 35.136047, 39.425712, 40.519207, 40.101126, 
    30.044457, 33.62287, 27.910276, 39.752254, 47.403778, 32.961929, 
    29.820199, 38.408649, 40.668828, 41.983948, 40.58116, 40.7253, 
    41.921667, 41.669921, 36.442552, 38.564663, 37.455315, 40.867774, 
    38.783554, 42.244381, 34.024332, 34.001644, 36.622704, 34.017832
    ), longitude = c(-70.58809, -82.888789, -80.113071, -82.6661, 
    -81.372992, -95.840964, -82.016013, -89.92625, -80.276676, 
    -89.778385, -87.641063, -94.888036, -117.663119, -82.530568, 
    -75.574437, -122.209047, -96.799309, -95.746255, -80.553953, 
    -73.923314, -87.713482, -73.9731, -73.8187, -87.987023, -93.759478, 
    -94.110903, -90.00597, -93.257109, -79.932483, -90.201858, 
    -89.100233, -84.625923, -84.567614, -93.912921, -84.612564
    ), ID = 2318:2352), row.names = c(NA, -35L), class = "data.frame")


Proprietary

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From |e@gr@|n@ @end|ng |rom gm@||@com  Wed May 13 03:01:59 2020
From: |e@gr@|n@ @end|ng |rom gm@||@com (Adrien FABRE)
Date: Wed, 13 May 2020 03:01:59 +0200
Subject: [R] Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
Message-ID: <CAKPvFjA6Nfv=H5cFmamCOgXKmrY8ZHniECwvKUqr78xGmSUY_w@mail.gmail.com>

I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
days ago, and Ubuntu from 18.04 to 20.04 yesterday.

Since then, R sometimes never terminates when executing certain commands:
ivreg (from package AER), summary (of a logit regression) and logitmfx
(from package mfx). Sometimes these commands run fine, but most of the time
I have to kill the process because R won't terminate the execution, even
when pressing the red Stop button in RStudio.

When I tried example('AER'), it worked fine. Then I re-installed the
package AER. It threw 10 warnings of type In readLines(file, skipNul =
TRUE) :  cannot open compressed file
'/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
file or directory' where [package] is abind, colorspace, dichromat... (but
not AER).

Since then example('AER') throws a warning: no help found for ?AER?.

I've removed and reinstalled R 4.0: it didn't help. Besides, the apt purge
r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
removing r-base-core, directory '/usr/lib/R/site-library' not empty so not
removed. Also, there was a bunch of Package [package] is not installed, so
not removed, including for [package] equal to r-cran-abind and the other
listed above (this purge also returned a bunch of Note, selecting [package]
for glob 'r-cran-*').

I have the same bug when using R from the terminal. For the record, I was
probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
can't recall if this issue started after I upgraded R and RStudio (which
would be my best guess) or after I upgraded Ubuntu (a day or two later).

I hope someone can help.

-- 
Adrien Fabre

?cole d'?conomie de Paris/Paris School of Economics ? Universit? Paris 1
(R4-47)

Page personnelle/Home page <http://sites.google.com/view/adrien-fabre>
(+33/0)6.10.37.90.51

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed May 13 03:37:26 2020
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 12 May 2020 18:37:26 -0700
Subject: [R] 
 Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
In-Reply-To: <CAKPvFjA6Nfv=H5cFmamCOgXKmrY8ZHniECwvKUqr78xGmSUY_w@mail.gmail.com>
References: <CAKPvFjA6Nfv=H5cFmamCOgXKmrY8ZHniECwvKUqr78xGmSUY_w@mail.gmail.com>
Message-ID: <CAGxFJbQpGHTPXkt1O1ti9vmO4mamJBoHdWFW1oxZvs0NKRuVJA@mail.gmail.com>

Please, please, please ... RStudio is a wholly separate product from
R. Post on their website if you think Rstudio has problems. And ubuntu
concerns should be posted on r-sig-debian, not here! This list is
about R programming issues (mostly).

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, May 12, 2020 at 6:07 PM Adrien FABRE <lesgrains at gmail.com> wrote:
>
> I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
> days ago, and Ubuntu from 18.04 to 20.04 yesterday.
>
> Since then, R sometimes never terminates when executing certain commands:
> ivreg (from package AER), summary (of a logit regression) and logitmfx
> (from package mfx). Sometimes these commands run fine, but most of the time
> I have to kill the process because R won't terminate the execution, even
> when pressing the red Stop button in RStudio.
>
> When I tried example('AER'), it worked fine. Then I re-installed the
> package AER. It threw 10 warnings of type In readLines(file, skipNul =
> TRUE) :  cannot open compressed file
> '/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
> file or directory' where [package] is abind, colorspace, dichromat... (but
> not AER).
>
> Since then example('AER') throws a warning: no help found for ?AER?.
>
> I've removed and reinstalled R 4.0: it didn't help. Besides, the apt purge
> r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
> removing r-base-core, directory '/usr/lib/R/site-library' not empty so not
> removed. Also, there was a bunch of Package [package] is not installed, so
> not removed, including for [package] equal to r-cran-abind and the other
> listed above (this purge also returned a bunch of Note, selecting [package]
> for glob 'r-cran-*').
>
> I have the same bug when using R from the terminal. For the record, I was
> probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
> can't recall if this issue started after I upgraded R and RStudio (which
> would be my best guess) or after I upgraded Ubuntu (a day or two later).
>
> I hope someone can help.
>
> --
> Adrien Fabre
>
> ?cole d'?conomie de Paris/Paris School of Economics ? Universit? Paris 1
> (R4-47)
>
> Page personnelle/Home page <http://sites.google.com/view/adrien-fabre>
> (+33/0)6.10.37.90.51
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From er|cjberger @end|ng |rom gm@||@com  Wed May 13 07:17:21 2020
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Wed, 13 May 2020 08:17:21 +0300
Subject: [R] 
 Sometimes commands do not terminate after upgrading to R 4.0
 and Ubuntu 20.04
In-Reply-To: <CAGxFJbQpGHTPXkt1O1ti9vmO4mamJBoHdWFW1oxZvs0NKRuVJA@mail.gmail.com>
References: <CAKPvFjA6Nfv=H5cFmamCOgXKmrY8ZHniECwvKUqr78xGmSUY_w@mail.gmail.com>
 <CAGxFJbQpGHTPXkt1O1ti9vmO4mamJBoHdWFW1oxZvs0NKRuVJA@mail.gmail.com>
Message-ID: <CAGgJW75yA+Kpb29a303bV4ZiOHPGXT8RynNBzcfJspqCr_7rcQ@mail.gmail.com>

Adrien,
you posted this same item 3 days ago to this list. And someone responded to
it.
Why are you posting the identical thing again to this list?
As Bert writes, you should post to the r-sig-debian list.

Eric


On Wed, May 13, 2020 at 4:38 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Please, please, please ... RStudio is a wholly separate product from
> R. Post on their website if you think Rstudio has problems. And ubuntu
> concerns should be posted on r-sig-debian, not here! This list is
> about R programming issues (mostly).
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
> On Tue, May 12, 2020 at 6:07 PM Adrien FABRE <lesgrains at gmail.com> wrote:
> >
> > I have upgraded R (from 3.6 to 4.0) and RStudio (from 1.1 to 1.2.5) a few
> > days ago, and Ubuntu from 18.04 to 20.04 yesterday.
> >
> > Since then, R sometimes never terminates when executing certain commands:
> > ivreg (from package AER), summary (of a logit regression) and logitmfx
> > (from package mfx). Sometimes these commands run fine, but most of the
> time
> > I have to kill the process because R won't terminate the execution, even
> > when pressing the red Stop button in RStudio.
> >
> > When I tried example('AER'), it worked fine. Then I re-installed the
> > package AER. It threw 10 warnings of type In readLines(file, skipNul =
> > TRUE) :  cannot open compressed file
> > '/usr/lib/R/site-library/[package]/DESCRIPTION', probable reason 'No such
> > file or directory' where [package] is abind, colorspace, dichromat...
> (but
> > not AER).
> >
> > Since then example('AER') throws a warning: no help found for ?AER?.
> >
> > I've removed and reinstalled R 4.0: it didn't help. Besides, the apt
> purge
> > r-base* r-recommended r-cran-* threw a warning: dpkg: warning: while
> > removing r-base-core, directory '/usr/lib/R/site-library' not empty so
> not
> > removed. Also, there was a bunch of Package [package] is not installed,
> so
> > not removed, including for [package] equal to r-cran-abind and the other
> > listed above (this purge also returned a bunch of Note, selecting
> [package]
> > for glob 'r-cran-*').
> >
> > I have the same bug when using R from the terminal. For the record, I was
> > probably working on RStudio during the upgrade to Ubuntu 20.04. Also, I
> > can't recall if this issue started after I upgraded R and RStudio (which
> > would be my best guess) or after I upgraded Ubuntu (a day or two later).
> >
> > I hope someone can help.
> >
> > --
> > Adrien Fabre
> >
> > ?cole d'?conomie de Paris/Paris School of Economics ? Universit? Paris 1
> > (R4-47)
> >
> > Page personnelle/Home page <http://sites.google.com/view/adrien-fabre>
> > (+33/0)6.10.37.90.51
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 08:38:47 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 16:38:47 +1000
Subject: [R] Help with Radius problem
In-Reply-To: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CA+8X3fW6kLo-f3E32uUZMO5=93GMtjNixSpuiX4qo50_+E2_pw@mail.gmail.com>

Hi Bill,
A while ago I devised a couple of functions to accumulate millions of
geographic locations of events and then display the resulting matrix
of values on an existing plot. This may be of use to you, at least in
the visualization of the density of the locations. As your example
data only included a few points, the resulting plot looks pretty
chunky as I had to use a large symbol to make the cells with non-zero
counts obvious.

# read in your sample data
source("wp_data.R")
library(plotrix)
geomat<-makeDensityMatrix(geodat[,c("latitude","longitude")],
xlim=range(geodat$longitude),ylim=range(geodat$latitude))
# Range of density (>0) - 1.119816 3.922018
latlim<-range(geodat$latitude)
lonlim<-range(geodat$longitude)
library(maps)
map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
 red=c(0.5,1),green=0,blue=0,pch=15)

Jim

On Wed, May 13, 2020 at 10:57 AM Poling, William via R-help
<r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit)
> #Running under: Windows 10 x64 (build 17763)
>
> Hello I am trying to figure out how to create concentration of ID's in the following data, based on determining the highest concentration in the smallest radius area based on Longitude and Latitude (geo location)
>
> I have reviewed many websites looking for a function or tutorial, (if you are aware of either please let me know)
>


From pd@|gd @end|ng |rom gm@||@com  Wed May 13 08:39:09 2020
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Wed, 13 May 2020 08:39:09 +0200
Subject: [R] My dream ...
In-Reply-To: <CA+8X3fUGA+7YbJsUij2eUb_hPLOQXAezJ5apMwwmgdNUzR8AHg@mail.gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
 <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
 <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>
 <CA+8X3fUGA+7YbJsUij2eUb_hPLOQXAezJ5apMwwmgdNUzR8AHg@mail.gmail.com>
Message-ID: <7B8F6D09-4866-4CD8-B9D8-DC3DCB7A8031@gmail.com>

Hans? Try Heinz ;-)

Actually listed as a quote _in_ Abby's, originally by Greg Snow, but w/o attribution...

-pd  



> On 13 May 2020, at 02:23 , Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Sorry, it was listed in Hans' email as a reply from you. Far be it
> from me to speak for someone else.
> 
> Jim
> 
> On Wed, May 13, 2020 at 10:13 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
>> 
>> Hi Jim,
>> 
>> I think you've mis-quoted me.
>> I didn't say that.
>> 
>> 
>> On Wed, May 13, 2020 at 10:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>>> 
>>> Abby Spurdle:
>>> In my opinion the advantage of computers is not Artificial
>>> Intelligence, but rather Artificial Patience (most AI that I have seen
>>> is really doing a bunch of what I would consider to be boring, really
>>> fast so people don't have to).  Leave the Intelligence to the people.
>>> 
>>> Abby's response contains a complaint that is often directed at
>>> technical advances. So what if we can devise a way to perform some
>>> boring task rapidly? I answer that it allows us to delegate the boring
>>> task to the machine and proceed with the integration of the results.
>>> We run the risk of Douglas Adams' delightful result that we cannot
>>> understand, but nearly all of the "big" scientific endeavors stand
>>> upon the shoulders of machines doing boring tasks whose duration at
>>> human speed would see us all out. My idea of AI is a sort of teamwork
>>> between the error-prone synthesis of man and the precise analysis of
>>> machine, not a struggle for dominance  of one or the other.
>>> 
>>> Jim
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 08:40:49 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 16:40:49 +1000
Subject: [R] My dream ...
In-Reply-To: <7B8F6D09-4866-4CD8-B9D8-DC3DCB7A8031@gmail.com>
References: <AM0PR05MB44362CE7DCB868F3A2938E60DDA10@AM0PR05MB4436.eurprd05.prod.outlook.com>
 <CAFEqCdwRtZ3wWhmUG9kU=PRWPemufTq2r7cYKmg0X5vo074F0w@mail.gmail.com>
 <CAB8pepx-x1yoYWjmJdH727z66smn-eh43P_E4o8R5gqj2aH9Ug@mail.gmail.com>
 <CAFEqCdwbXpD9DpgSEsHdsv0jBqqx0hLpu10ZN4SyejV06do4Jg@mail.gmail.com>
 <CA+8X3fVgo0TursqcgWCqmVD1vDF3mAWm0_Ku_CVrMBqiqMp7Xw@mail.gmail.com>
 <CAB8pepytv-Atm4YVokWS_NQzERqDyvJTxE7fHgvJRisVD9rdog@mail.gmail.com>
 <CA+8X3fUGA+7YbJsUij2eUb_hPLOQXAezJ5apMwwmgdNUzR8AHg@mail.gmail.com>
 <7B8F6D09-4866-4CD8-B9D8-DC3DCB7A8031@gmail.com>
Message-ID: <CA+8X3fW99FDpYgwhs=AobDyXjbnhaPWeY5S69TZMa2qHfEwjTw@mail.gmail.com>

Well, let's hope that was my big screw up for today...

On Wed, May 13, 2020 at 4:39 PM peter dalgaard <pdalgd at gmail.com> wrote:
>
> Hans? Try Heinz ;-)
>
> Actually listed as a quote _in_ Abby's, originally by Greg Snow, but w/o attribution...
>
> -pd
>
>
>
> > On 13 May 2020, at 02:23 , Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Sorry, it was listed in Hans' email as a reply from you. Far be it
> > from me to speak for someone else.
> >
> > Jim
> >
> > On Wed, May 13, 2020 at 10:13 AM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >>
> >> Hi Jim,
> >>
> >> I think you've mis-quoted me.
> >> I didn't say that.
> >>
> >>
> >> On Wed, May 13, 2020 at 10:10 AM Jim Lemon <drjimlemon at gmail.com> wrote:
> >>>
> >>> Abby Spurdle:
> >>> In my opinion the advantage of computers is not Artificial
> >>> Intelligence, but rather Artificial Patience (most AI that I have seen
> >>> is really doing a bunch of what I would consider to be boring, really
> >>> fast so people don't have to).  Leave the Intelligence to the people.
> >>>
> >>> Abby's response contains a complaint that is often directed at
> >>> technical advances. So what if we can devise a way to perform some
> >>> boring task rapidly? I answer that it allows us to delegate the boring
> >>> task to the machine and proceed with the integration of the results.
> >>> We run the risk of Douglas Adams' delightful result that we cannot
> >>> understand, but nearly all of the "big" scientific endeavors stand
> >>> upon the shoulders of machines doing boring tasks whose duration at
> >>> human speed would see us all out. My idea of AI is a sort of teamwork
> >>> between the error-prone synthesis of man and the precise analysis of
> >>> machine, not a struggle for dominance  of one or the other.
> >>>
> >>> Jim
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>


From drj|m|emon @end|ng |rom gm@||@com  Wed May 13 11:01:18 2020
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 May 2020 19:01:18 +1000
Subject: [R] Classification of wind events
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F809B8DEA@ESINO.regionemarche.intra>
Message-ID: <CA+8X3fUsXA7reESuWpGC2jBGRRryF6RcYa5jZxMWV7oktF1kFg@mail.gmail.com>

Hi Stefano,
Given only one observation point you will find it difficult. If your
automatic weather station is in the low area where the foehn wind is
felt, it can only be distinguished from a dry katabatic wind if the
upwind conditions are known. There is a similar but milder version of
this in eastern Australia, but it is usually of the latter sort. There
may be a way to measure turbulence above the peak of the high ground
with radar or something, but I'm not familiar with that.

Jim

On Tue, May 12, 2020 at 6:13 PM Stefano Sofia
<stefano.sofia at regione.marche.it> wrote:
>
> Dear R list users,
> I am aware that this question is not strictly related, at the present moment, to R code and it is more general. Please forgive me, but I need to share my thoughts with you.
>
> Foehn conditions on the southern slope of Alps happen with strong northerly flows that impact perpendicularly over the Apls. This situation triggers strong northerly leeward winds.
> Given a single automatic weather station, I would like to identify these periods starting from wind direction and wind intensity data. Frequency of data is quarter of hour.
> I would really find difficult to detect the moving windows of these events:
> - I can't analyse data day by day;
> - at the beginning and at the end of each event, when the process is not at full speed yet, the rotation is not always perfectly identifiable;
> - I cannot claim in principle that the direction of each consecutive observation is costantly and strictly from the chosen direction.
>
> Does anybody have a clue on how to start to build this process in the right way?
>
> Thank you for your attention and your help
> Stefano
>
>          (oo)
> --oOO--( )--OOo----------------
> Stefano Sofia PhD
> Civil Protection - Marche Region
> Meteo Section
> Snow Section
> Via del Colle Ameno 5
> 60126 Torrette di Ancona, Ancona
> Uff: 071 806 7743
> E-mail: stefano.sofia at regione.marche.it
> ---Oo---------oO----------------
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
>
> --
> Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
> This message was scanned by Libra ESVA and is believed to be clean.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bog@@o@chr|@to|er @end|ng |rom gm@||@com  Wed May 13 11:26:24 2020
From: bog@@o@chr|@to|er @end|ng |rom gm@||@com (Christofer Bogaso)
Date: Wed, 13 May 2020 14:56:24 +0530
Subject: [R] Fitting Richards' curve
Message-ID: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>

Hi,

Is there any R package to fit Richards' curve in the form of
https://en.wikipedia.org/wiki/Generalised_logistic_function

I found there is one package grofit, but currently defunct.

Any pointer appreciated.


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 11:30:57 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 09:30:57 +0000
Subject: [R] Help with Radius problem
In-Reply-To: <CAGxFJbSv3jV5eszHqMcf5JYZSvy2OBHHpxq2W5FPPcju8jp+qA@mail.gmail.com>
References: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAGxFJbSv3jV5eszHqMcf5JYZSvy2OBHHpxq2W5FPPcju8jp+qA@mail.gmail.com>
Message-ID: <BYAPR06MB53839852EC2BA4299B8C4465AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Good morning Bert. I will sign up for r-sig-geo and review your suggested link as well, thank you very much for your response.

WHP




Proprietary

-----Original Message-----
From: Bert Gunter <bgunter.4567 at gmail.com> 
Sent: Tuesday, May 12, 2020 8:30 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Radius problem

**** External Email - Use Caution ****

No insight But have you consulted:
https://urldefense.proofpoint.com/v2/url?u=https-3A__CRAN.R-2Dproject.org_view-3DSpatial&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=aT7qePkClGUoKBh0QatR5mvySgsYeUp9gU0WLReStTU&e= 

Also, I believe this would be better posted on r-sig-geo, not here.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )

On Tue, May 12, 2020 at 5:57 PM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
>
> Hello I am trying to figure out how to create concentration of ID's in 
> the following data, based on determining the highest concentration in 
> the smallest radius area based on Longitude and Latitude (geo 
> location)
>
> I have reviewed many websites looking for a function or tutorial, (if 
> you are aware of either please let me know)
>
> I have worked through the following to see if they are close to what I need, but they are not.
>
> #Possible use 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_21977720_r-2Dfinding-2Dclosest-2Dneighboring-2Dpoint-2Dand-
> 2Dnumber-2Dof-2Dneighbors-2Dwithin-2Da-2Dgiven-2Drad&d=DwIBaQ&c=wluqKI
> iwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2
> ExlYvnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=t6qPeVA_
> C9VbEMtn5earDo2F53CkazDyPAQrL81PifM&e=
>
> #Possible math ref 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__janmatuschek.de_La
> titudeLongitudeBoundingCoordinates&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn
> 0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=gb
> t_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=o5R4BF22ow2gJd8RgJOS0pVRn6
> 6a4uCBcXXgK59Z1SM&e=
>
> #Possible use 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_39008850_find-2Dnumber-2Dof-2Dpoints-2Dwithin-2Da-2Dradius-
> 2Din-2Dr-2Dusing-2Dlon-2Dand-2Dlat-2Dcoordinates&d=DwIBaQ&c=wluqKIiwff
> OpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlY
> vnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=cr5fY67h9cmL
> ZlNVPGcJ5gysN_qVQC-xZetElqwYVhk&e=
>
> # 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__gis.stackexchange
> .com_questions_229453_create-2Da-2Dcircle-2Dof-2Ddefined-2Dradius-2Dar
> ound-2Da-2Dpoint-2Dand-2Dthen-2Dfind-2Dthe-2Doverlapping-2Da&d=DwIBaQ&
> c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmT
> CtKvneM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=
> xK2iPfFjKfUevj39Uk8ux2OZYYxE3N4kOQU3ecP0W4M&e=
>
> # 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stackoverflow.com
> _questions_23071026_drawing-2Da-2Dcircle-2Dwith-2Da-2Dradius-2Dof-2Da-
> 2Ddefined-2Ddistance-2Din-2Da-2Dmap_23072079-2323072079&d=DwIBaQ&c=wlu
> qKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvn
> eM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0-nqZs2cDd5cTAtzTGE6OOluljG2U0&s=susH1
> Qr4Cz3oohO3OF6Bs6PWuAzo5hc7WMQccLvuX08&e=
>
> My actual data is 2353 records with 41 states and 1337 cities represented. Here is a sample below.
>
> Some might call this a hotspot question I suppose: "What is the maximum number of ID's I can gather in the smallest radius size given the data at hand?
>
> For example it would be useful to have a function that I could use to test different combinations of radius size and concentration of ID #'s by geo location.
>
> Any insight would be very much appreciated.
>
> Thank you.
> WHP
>
> str(sample)
> 'data.frame':   35 obs. of  5 variables:
>  $ state    : Factor w/ 41 levels "AL","AR","AZ",..: 19 29 9 9 10 30 33 35 41 12 ...
>  $ city     : Factor w/ 1337 levels "ABBOTTSTOWN",..: 281 1280 129 223 989 721 550 731 1325 688 ...
>  $ latitude : num  43.5 40.1 26.5 27.9 31.9 ...
>  $ longitude: num  -70.6 -82.9 -80.1 -82.7 -81.4 ...
>  $ ID       : int  2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 ...
>
> dput(sample)
> structure(list(state = structure(c(19L, 29L, 9L, 9L, 10L, 30L, 33L, 
> 35L, 41L, 12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 
> 28L, 28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L), 
> .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", 
> "IA", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", 
> "MO", "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", 
> "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"), class = 
> "factor"), city = structure(c(281L, 1280L, 129L, 223L, 989L, 721L, 
> 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 1305L, 587L, 272L, 
> 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 1006L, 1162L, 939L, 
> 170L, 1033L, 1002L, 586L, 586L, 192L, 586L), .Label = c("ABBOTTSTOWN", 
> "ABILENE", "ACWORTH", "ADAMS", "ADDISON", "ADKINS", "ALBANY", "ALBIA", 
> "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", "ALIQUIPPA", "ALISO VIEJO", 
> "ALLEN PARK", "ALLENTOWN", "ALPHA", "ALPHARETTA", "ALPINE", "ALTOONA", 
> "AMARILLO", "AMBLER", "AMBRIDGE", "AMHERST", "AMITYVILLE", "ANAHIEM", 
> "ANDERSON", "ANDOVER", "ANGIER", "ANGLETON", "ANKENY", "ANN ARBOR", 
> "ANZA", "APEX", "APOLLO", "ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", 
> "ARKANSAS CITY", "ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", 
> "ASHEVILLE", "ASHLAND", "ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", 
> "ATLANTA", "ATLANTIC BCH", "AUBORN", "AUGUSTA", "AURORA", "AUSTELL", 
> "AUSTIN", "AVENTURA", "AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN 
> CITY", "BALDWIN PLACE", "BALLWIN", "BANGOR", "BARBOURSVILLE", 
> "BARNEGAT", "BARTLETT", "BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", 
> "BATESBURG", "BATON ROUGE", "BAXTER SPRINGS", "BAY CITY", "BAYONNE", 
> "BAYSIDE", "BAYTOWN", "BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER 
> CITY", "BELFAIR", "BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", 
> "BELLMORE", "BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", 
> "BENTON", "BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", 
> "BETHEL", "BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", 
> "BIGLERVILLE", "BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", 
> "BLACKSBURG", "BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", 
> "BLUE BELL", "BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", 
> "BODFISH", "BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", 
> "BONNEY LAKE", "BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING 
> GREEN", "BOYERTOWN", "BOYNTON BEACH", "BRADENTON", "BRADENVILLE", 
> "BRANDAMORE", "BRANDON", "BRANFORD", "BRANSON", "BRASELTON", 
> "BREEZEWOOD", "BRENTON", "BRENTWOOD", "BREWSTER", "BRICK", 
> "BRIDGEPORT", "BRIDGETON", "BRIGHAM CITY", "BROADDUS", "BROCKPORT", 
> "BROGUE", "BROKEN ARROW", "BRONX", "BROOKFIELD", "BROOKHAVEN", 
> "BROOKLYN", "BROWNS MILLS", "BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", 
> "BRYAN", "BUCKSPORT", "BUDD LAKE", "BUFFALO", "BULVERDE", 
> "BUNKERVILLE", "BURBANK", "BURFORD", "BURIEN", "BURLINGTON", "BURR 
> RIDGE", "BURTON", "BUSKIRK", "BUTLER", "BUXTON", "BYRON", "CALERA", 
> "CAMBRIDGE", "CAMP HILL", "CAMPBELL", "CANAAN", "CANAL WINCHESTER", 
> "CANASTOTA", "CANTON", "CARDIFF BY THE SEA", "CARL JUNCTION", 
> "CARLISLE", "CARLTON", "CAROL STREAM", "CARROLLTON", "CARSON CITY", 
> "CARTERSVILLE", "CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", 
> "CASWELL", "CATO", "CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER 
> POINT", "CENTERTON", "CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN 
> FALLS", "CHALFONT", "CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", 
> "CHARLESTON", "CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", 
> "CHESHIRE", "CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", 
> "CHILLICOTHE", "CHOCTAW", "CICERO", "CINCINNATI", "CLAY", 
> "CLEARWATER", "CLEARWATER BEACH", "CLEMMONS", "CLENDENIN", 
> "CLEVELAND", "CLEVELAND HTS", "CLEVES", "CLIFFSIDE PARK", "CLIFTON", 
> "CLIFTON PARK", "CLIFTON SPGS", "CLINTON", "COACHELLA", "COAL 
> TOWNSHIP", "COATESVILLE", "COLLEGE STATION", "COLLINSVILLE", 
> "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", "CONCORD", "CONCORD 
> TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", "CONYERS", 
> "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", "CORNELIUS", 
> "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", "COTTAGE HILLS", 
> "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", "CRANBERRY 
> TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", "CUMBERLAND 
> FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", "DALLAS", 
> "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
> "DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
> "DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
> "DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", "DES 
> MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", "DIXON", 
> "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", "DOYLESTOWN", 
> "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", "DUNCAN", "DUNCANNON", 
> "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", "DURHAM", "DUTTON", "EAGLE 
> GROVE", "EAST AURORA", "EAST CHICAGO", "EAST EARL", "EAST HANOVER", 
> "EAST HARTFORD", "EAST HARTLAND", "EAST LEROY", "EAST LIVERPOOL", 
> "EAST MEADOW", "EAST NORTHPORT", "EAST ORANGE", "EAST PEORIA", "EAST 
> SAINT LOUIS", "EASTON", "EDGERTON", "EDISON", "EDMOND", "EGG HBR TWP", 
> "EL DORADO", "EL PASO", "ELDRIDGE", "ELGIN", "ELIZABETHVILLE", 
> "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", "ELMHURST", "ELMWOOD PARK", 
> "ELOY", "ELY", "EMERSON", "EMORY", "ENDICOTT", "ENGLEWOOD", "ENID", 
> "ENNICE", "ENOLA", "ENUMCLAW", "EPHRATA", "ERIE", "ESTERO", "EUREKA 
> SPRINGS", "EVANSVILLE", "EVERETT", "EXCELSIOR SPRINGS", "EXTON", 
> "FAIRFAX", "FAIRFIELD", "FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", 
> "FALMOUTH", "FAR HILLS", "FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", 
> "FAYETTE", "FAYETTEVILLE", "FELTON", "FENTON", "FERGUSON", "FIELDALE", 
> "FINDLAY", "FINGERVILLE", "FLEMINGTON", "FLINT", "FLORENCE", 
> "FLORESVILLE", "FLORISSANT", "FLOVILLA", "FLOWER MOUND", "FLUSHING", 
> "FOGELSVILLE", "FOLSOM", "FONTANELLE", "FOREST HILLS", "FOREST PARK", 
> "FORT DODGE", "FORT FAIRFIELD", "FORT GAY", "FORT LAUDERDALE", "FORT 
> LEE", "FORT MILL", "FORT MOHAVE", "FORT PAYNE", "FORT PIERCE", "FORT 
> SMITH", "FORT WAYNE", "FORT WORTH", "FOSTORIA", "FOUNTAIN INN", 
> "FRANKFORT", "FRANKLIN", "FREEDOM", "FREEHOLD", "FREEPORT", "FREMONT", 
> "FULLERTON", "FULSHEAR", "FULTON", "GADSDEN", "GAFFNEY", 
> "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", "GARDNERS", "GARNETT 
> VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", "GETTYSBURG", 
> "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", "GLEN CARBON", "GLEN 
> COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", "GLENWOOD", "GOLDEN 
> CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", "GOSHEN", 
> "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", "GRANDVIEW", 
> "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", "GRASS VALLEY", 
> "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", "GREENLAWN", 
> "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", "GREER", 
> "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", "HADLEY", 
> "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", "HAMBURG", 
> "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
> "HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
> "HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
> "HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
> "HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
> "HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", "HIGH 
> HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
> "HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
> "HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
> "HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", "HOT 
> SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", "HUFFMAN", 
> "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", "HUNTINGTON 
> BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", "HURST", 
> "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
> "INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", "IRMO", 
> "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", "ISSAQUAH", 
> "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", "JACKSONVILLE", 
> "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", "JEFFERSON", "JEFFERSON 
> CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", "JONESBORO", "JONESPORT", 
> "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", "KANNAPOLIS", "KANSAS 
> CITY", "KATY", "KEARNEY", "KELLERTON", "KEMAH", "KENMORE", "KENNESAW", 
> "KENT", "KINGMAN", "KINGS MOUNTAIN", "KINGSLAND", "KINGSTON", 
> "KINGWOOD", "KIRKLAND", "KUTZTOWN", "LA QUINTA", "LA RUE", "LA VERNE", 
> "LAFAYETTE", "LAKE BLUFF", "LAKE IN THE HILL", "LAKE ISABELLA", "LAKE 
> PLACID", "LAKE SAINT LOUIS", "LAKE TAPPS", "LAKE VILLAGE", "LAKE 
> WAUKOMIS", "LAKE WORTH", "LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD 
> RCH", "LAMBERTVILLE", "LANCASTER", "LANDISVILLE", "LANSDALE", 
> "LAREDO", "LARGO", "LAS VEGAS", "LATROBE", "LAUDERHILL", "LAWNDALE", 
> "LAWRENCE", "LAWRENCEVILLE", "LE MARS", "LE ROY", "LEAGUE CITY", 
> "LEAVENWORTH", "LEAWOOD", "LEBANON", "LECOMPTON", "LEECHBURG", "LEES 
> SUMMIT", "LEESBURG", "LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", 
> "LEVITTOWN", "LEWES", "LEWISTOWN", "LEXINGTON", "LIBERTY TWP", 
> "LILLINGTON", "LINCOLN", "LINDENHURST", "LINTON", "LISBON", 
> "LITCHFIELD PK", "LITHIA", "LITITZ", "LITTLE EGG HARBOR TO", "LK 
> FOREST PK", "LK PEEKSKILL", "LOCKHART", "LOCKWOOD", "LOGAN", "LOMA 
> LINDA", "LONE JACK", "LONG BEACH", "LONG LANE", "LONGVIEW", "LORAIN", 
> "LORTON", "LOS ANGELES", "LOUISBURG", "LOVES PARK", "LOWER BURRELL", 
> "LOWRY CITY", "LUBBOCK", "LUGOFF", "LUMBERTON", "LYNBROOK", 
> "LYNNWOOD", "MACHESNEY PARK", "MACON", "MAGNOLIA", "MAHOPAC", 
> "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", "MANASSAS", "MANCHACA", 
> "MANCHESTER", "MANCHESTER TOWNSHIP", "MANCHESTER TW", "MANHEIM", 
> "MANITO", "MANLIUS", "MANNINGTON", "MANSFIELD", "MANSURA", "MAPLE 
> PARK", "MARBLE FALLS", "MARCELLUS", "MARICOPA", "MARIETTA", "MARION", 
> "MARKLEYSBURG", "MARS HILL", "MARSHALLTOWN", "MARSHFIELD", 
> "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", "MASON CITY", "MASSEY", 
> "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", "MATTAPOISETT", 
> "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", "MC DONALD", "MC 
> KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", "MCALESTER", "MCDONOUGH", 
> "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", "MECHANICSVILLE", 
> "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", "MERIDEN", 
> "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", "MESQUITE", 
> "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", "MIDLAND", 
> "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", "MILLEDGEVILLE", 
> "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", "MINEOLA", "MINERAL 
> SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", "MISSION HILLS", 
> "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", "MOLINE", 
> "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", "MONTCLAIR", 
> "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", "MORGANTOWN", 
> "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", "MOUNT JOY", 
> "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", "MOUNT UNION", 
> "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", "MULVANE", 
> "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
> "MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
> "NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
> "NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", "NEW 
> BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", "NEW 
> CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", "NEW 
> LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", "NEW 
> ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
> "NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
> "NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
> "NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
> "NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
> "NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND 
> HILLS", "NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
> "NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", "OAK 
> RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", "OMAHA", 
> "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", "ORONO", 
> "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", "OTTAWA", 
> "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", "OYSTER BAY", 
> "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", "PALMDALE", "PALMYRA", 
> "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", "PARLIN", "PARMA", 
> "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", "PAXTON", "PEA 
> RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", "PELION", 
> "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", "PENNSBURG", 
> "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", "PERRYSBURG", 
> "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", "PHOENIX", 
> "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", "PINE BLUFF", 
> "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", "PITMAN", 
> "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
> "PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO 
> LAKE", "POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", 
> "PORT CHARLOTTE", "PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", 
> "PORT WASHINGTON", "PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", 
> "POTTSVILLE", "POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE 
> VILLAGE", "PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT 
> PLEASANT", "PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", 
> "QUINCY", "RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", 
> "READLYN", "RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO 
> PARK", "REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", 
> "RICH HILL", "RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", 
> "RINCON", "RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", 
> "RIVERSIDE", "ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", 
> "ROCKAWAY BCH", "ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", 
> "ROGERS", "ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
> "ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
> "ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
> "RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
> "SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", "SAINT 
> CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", "SAINT 
> PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
> "SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
> "SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
> "SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", 
> "SARASOTA", "SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", 
> "SAYVILLE", "SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", 
> "SCOTT", "SCOTTSDALE", "SEAFORD", "SEALE", "SEATTLE", "SEDONA", 
> "SEGUIN", "SELDEN", "SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", 
> "SHARON", "SHARPSBURG", "SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", 
> "SHELLSBURG", "SHELTON", "SHENANDOAH", "SHORELINE", "SHOREWOOD", 
> "SHREVEPORT", "SICKLERVILLE", "SILER CITY", "SILOAM SPRINGS", "SIMON", 
> "SIMPSONVILLE", "SIMSBURY", "SIOUX CITY", "SIOUX FALLS", "SKOKIE", 
> "SKOWHEGAN", "SLATINGTON", "SLIDELL", "SMITHFIELD", "SN BERNRDNO", 
> "SNELLVILLE", "SOCORRO", "SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH 
> BELOIT", "SOUTH BEND", "SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH 
> PARK", "SOUTH PLAINFIELD", "SOUTH SIOUX CITY", "SOUTHAMPTON", 
> "SOUTHBURY", "SOUTHGATE", "SPARKS", "SPARROW BUSH", "SPENCER", 
> "SPRING", "SPRING BRANCH", "SPRING GROVE", "SPRING HILL", "SPRING 
> HOPE", "SPRING VALLEY", "SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD 
> GARDENS", "SPRINGHILL", "SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", 
> "ST PETERSBURG", "ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", 
> "STANLEY", "STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", 
> "STILLMAN VALLEY", "STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", 
> "STRAFFORD", "STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", 
> "SUFFIELD", "SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", 
> "SUMMIT", "SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", 
> "SWEENY", "SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", 
> "TAFTVILLE", "TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", 
> "TAYLORVILLE", "TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", 
> "THOMASVILLE", "TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY 
> PARK", "TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", 
> "TOPEKA", "TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS 
> RST", "TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
> "TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
> "UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
> "URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", "VAN 
> BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
> "VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
> "VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", "W 
> TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
> "WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
> "WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
> "WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
> "WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
> "WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", "WEST 
> CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", "WEST 
> HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", "WEST 
> POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", "WEST VALLEY 
> CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", "WESTMORELAND 
> CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", "WHARTON", "WHEELING", 
> "WHITE PLAINS", "WHITEHALL", "WHITESTONE", "WHITESTOWN", "WHITING", 
> "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", "WILLARD", 
> "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
> "WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
> "WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
> "WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
> "WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
> "WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
> "YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", "YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"),
>     latitude = c(43.53462, 40.142492, 26.538149, 27.9062, 31.90104,
>     34.833083, 35.118836, 35.136047, 39.425712, 40.519207, 40.101126,
>     30.044457, 33.62287, 27.910276, 39.752254, 47.403778, 32.961929,
>     29.820199, 38.408649, 40.668828, 41.983948, 40.58116, 40.7253,
>     41.921667, 41.669921, 36.442552, 38.564663, 37.455315, 40.867774,
>     38.783554, 42.244381, 34.024332, 34.001644, 36.622704, 34.017832
>     ), longitude = c(-70.58809, -82.888789, -80.113071, -82.6661,
>     -81.372992, -95.840964, -82.016013, -89.92625, -80.276676,
>     -89.778385, -87.641063, -94.888036, -117.663119, -82.530568,
>     -75.574437, -122.209047, -96.799309, -95.746255, -80.553953,
>     -73.923314, -87.713482, -73.9731, -73.8187, -87.987023, -93.759478,
>     -94.110903, -90.00597, -93.257109, -79.932483, -90.201858,
>     -89.100233, -84.625923, -84.567614, -93.912921, -84.612564
>     ), ID = 2318:2352), row.names = c(NA, -35L), class = "data.frame")
>
>
> Proprietary
>
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0-
> nqZs2cDd5cTAtzTGE6OOluljG2U0&s=QL9JHIC7CR7kgMybKwS9d3Q3PSpRgaxHKx9QG1o
> Fv_M&e= PLEASE do read the posting guide 
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIBaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=gbt_psl_UbXJa0
> -nqZs2cDd5cTAtzTGE6OOluljG2U0&s=L9Wz0HxE7TZHsDtBQPJw8BkJwzig5LFyl9_kpe
> pkPVc&e= and provide commented, minimal, self-contained, reproducible 
> code.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From petr@p|k@| @end|ng |rom prechez@@cz  Wed May 13 11:38:47 2020
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 13 May 2020 09:38:47 +0000
Subject: [R] Fitting Richards' curve
In-Reply-To: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
Message-ID: <ab1c0c0f77b64626b42c64629d12c972@SRVEXCHCM1302.precheza.cz>

Hi Christofer

Try FlexParamCurve or maybe drc package.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Christofer Bogaso
> Sent: Wednesday, May 13, 2020 11:26 AM
> To: r-help <r-help at r-project.org>
> Subject: [R] Fitting Richards' curve
> 
> Hi,
> 
> Is there any R package to fit Richards' curve in the form of
> https://en.wikipedia.org/wiki/Generalised_logistic_function
> 
> I found there is one package grofit, but currently defunct.
> 
> Any pointer appreciated.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From pd@|gd @end|ng |rom gm@||@com  Wed May 13 11:41:59 2020
From: pd@|gd @end|ng |rom gm@||@com (Peter Dalgaard)
Date: Wed, 13 May 2020 11:41:59 +0200
Subject: [R] Fitting Richards' curve
In-Reply-To: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
Message-ID: <A9518FD1-4E6D-4BF2-95D2-9F165CAD38FA@gmail.com>

Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)

-pd 

> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
> 
> Hi,
> 
> Is there any R package to fit Richards' curve in the form of
> https://en.wikipedia.org/wiki/Generalised_logistic_function
> 
> I found there is one package grofit, but currently defunct.
> 
> Any pointer appreciated.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 11:53:12 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 09:53:12 +0000
Subject: [R] Help with Radius problem
In-Reply-To: <CA+8X3fW6kLo-f3E32uUZMO5=93GMtjNixSpuiX4qo50_+E2_pw@mail.gmail.com>
References: <BYAPR06MB5383B486628866FD2F36917CAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CA+8X3fW6kLo-f3E32uUZMO5=93GMtjNixSpuiX4qo50_+E2_pw@mail.gmail.com>
Message-ID: <BYAPR06MB5383FA2A6F643D6C96D630DEAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Good morning Jim.

This is awesome start, visualization is splendid, thank you very much.

I have signed on to r-sig-geo and submitted my question there. I have no idea of the volume of traffic on that list
However, hopefully, I will gain additional insight into how to determine max number of member locations in the most minimum radius area.

Have a great day!

WHP



Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com> 
Sent: Wednesday, May 13, 2020 1:39 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Radius problem

**** External Email - Use Caution ****

Hi Bill,
A while ago I devised a couple of functions to accumulate millions of geographic locations of events and then display the resulting matrix of values on an existing plot. This may be of use to you, at least in the visualization of the density of the locations. As your example data only included a few points, the resulting plot looks pretty chunky as I had to use a large symbol to make the cells with non-zero counts obvious.

# read in your sample data
source("wp_data.R")
library(plotrix)
geomat<-makeDensityMatrix(geodat[,c("latitude","longitude")],
xlim=range(geodat$longitude),ylim=range(geodat$latitude))
# Range of density (>0) - 1.119816 3.922018
latlim<-range(geodat$latitude)
lonlim<-range(geodat$longitude)
library(maps)
map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
 red=c(0.5,1),green=0,blue=0,pch=15)

Jim

On Wed, May 13, 2020 at 10:57 AM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
>
> Hello I am trying to figure out how to create concentration of ID's in 
> the following data, based on determining the highest concentration in 
> the smallest radius area based on Longitude and Latitude (geo 
> location)
>
> I have reviewed many websites looking for a function or tutorial, (if 
> you are aware of either please let me know)
>

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Wed May 13 13:28:26 2020
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Wed, 13 May 2020 16:58:26 +0530
Subject: [R] 
 [R ] Writing loop to estimate ARCH test for a multiple columns
 of a data frame?
In-Reply-To: <CA+8X3fVgYkZ00gqqVhdaE7Nn=VvQ_x=eoBdjqBKsmALsTGfB5A@mail.gmail.com>
References: <CAOFE=kMaSgqy7_dMsB3WtTgN+BDa4b5m28GO0uxm0rUsDmqc9g@mail.gmail.com>
 <CA+8X3fWOMFQ8Ptz75j8DjhJuaEKi8nR1p+82LeNqTcB2NfSjKA@mail.gmail.com>
 <CAOFE=kOtwzrr3cFXhvvS+db+iw=p4eg1Fm6JCGtCzwessQ90FA@mail.gmail.com>
 <CA+8X3fUB8Mp9Pn8RO7PB3knoEgLHEdT6LuwxyxPbNMvQNoVG5A@mail.gmail.com>
 <CAOFE=kNNFOy=GCSWidG+Fds4qi5r7w9M9SafZ+oqcQw-xeKmkg@mail.gmail.com>
 <CA+8X3fXcztWSqhUiKAS+uc+fH57RCV7jN5ihLeRbz5rUgkp28g@mail.gmail.com>
 <CAOFE=kO6Pz=vqOR6+mBvjkMMBMhv9bX9s9nUCRnBoF9dGkdiVA@mail.gmail.com>
 <CA+8X3fVgYkZ00gqqVhdaE7Nn=VvQ_x=eoBdjqBKsmALsTGfB5A@mail.gmail.com>
Message-ID: <CAOFE=kPqS0VyLeU-VDCd=2a9B4d98Gv3ov1oxUriW0adCEJexw@mail.gmail.com>

Dear Sir,

I am so sorry that due to certain inconveniences, I became late to try your
suggested code and to reply to your email.

Thank you very much for your wonderful solution and suggestion for my
problem. Like before,  Your suggested code has worked awesome. Even, I
successfully imported the required output to the word following your
suggested similar path for the Libre office editor.

But, I have certain queries on your suggested code mentioned below which I
would like to discuss with you for my further learning.

1. Is there any difference between reading the tab and text file in R
because when I used  sp_8_5<-read.table("sp_8_5.tab",sep="\t",


header=TRUE,stringsAsFactors=FALSE)
it had thrown some error. But, when I changed the sp_8_5.tab into
sp_8_5.text, it worked. So, here my query, "does R read tab and text file
differently, however, both the files are similar"?

2. In the code, "return(sprintf("ChiSq = %.1f, p =
%.3f",archout$statistic,archout$p.value))", sprintf stands for printing the
particular results (i.e., statistics and p-value), right? Further, "ChiSq =
%.1f, p = %.3f" indicate the calling the values up to 1 and 3 decimal
points respectively, right? kindly correct me if I am worng in my
interpretation.

3. While opening a text file, sink("sp_8_5.txt")
                                         for(row in 0:2) {
                                         for(column in 1:4)

cat(spout[[column+row*4]],ifelse(column
< 4,"\t","\n"))
                                         }
                                                   sink()
3.1. what sink indicates, I think here sink calls for the arranging of the
statistics and p-values in the required 3*4 dimension in the generated text
file, right? Please educate me.
3.2 Hence, the results are arranged in 3 rows and 4 columns in the text
file. I understand the code for arranging loop for columns [i.e.,
for(column in 1:4) ], but i didn't understand the loop for row [i.e., for(row
in 0:2)]. In particular, what is the logic behind the setting of 2 rather
than 3 for 3 rows in "for(row in 0:2)"?
3.3. In the code, "cat(spout[[column+row*4]],ifelse(column <
4,"\t","\n"))", what cat indicates? what is the logic behind [column+row*4]
 and ifelse(column < 4,"\t","\n") ? This is my major query in the entire
code. Please help me to understand this line.


Along with the above queries in your suggested code, I have one more query that
is it possible to rename each row and column? Actually, why I am asking
this because I have data from 80 countries, and each country has 5 columns
of data arranging in 5 columns. In other words, the total number of columns
in my study is 400. While doing the ARCH test for each column, there may be
a mistake to arrange the results in the text file. Thus, I want to arrange
the resulted statistics for 5 columns (for instance A1, A2, A3, A4, A5) for
each country in the following way which I think will definitely avoid any
kind of typo-mistake in arranging output in the text file. In other words,
Each row will have results for each country arranged in 5 columns for the
particular 5 variables which help to identify the particular result for the
particular columns of the particular countries in an easy manner.


Country           A1        A2       A3     A4     A5
India              0.65      0.33   0.32   0.12  0.34
Israel              0.35      0.05   0.10    0.15   0.23
Australia          0.43      0.25    0.45    0.55    0.56

and so on.


Thank you very much, Sir, for educating a R learner for which I shall be
always grateful to you.


[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
05/13/20,
04:56:34 PM

On Sat, May 9, 2020 at 8:58 AM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Subhamitra,
> I have washed the dishes and had a night's sleep, so I can now deal with
> your text munging problem. First, I'll reiterate the solution I sent:
>
> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>  header=TRUE,stringsAsFactors=FALSE)
> library(tseries)
> library(FinTS)
> # create a function that returns only the
> # statistic and p.value as a string
> archStatP<-function(x) {
>  archout<-ArchTest(x)
>  # I have truncated the values here
>  return(sprintf("ChiSq = %.1f, p =
> %.3f",archout$statistic,archout$p.value))
> }
> # using "lapply", run the test on each column
> spout<-lapply(sp_8_5[,2:13],archStatP)
>
> If you look at "spout" you will see that it is a list of 12 character
> strings. I arranged this as you seem to want the contents of a 3x4 table in
> a document. This is one way to do it, there are others.
>
> First, create a text table of the desired dimensions. I'll do it with
> loops as you seem to be familiar with them:
>
> # open a text file
> sink("sp_8_5.txt")
> for(row in 0:2) {
>  for(column in 1:4)
>   cat(spout[[column+row*4]],ifelse(column < 4,"\t","\n"))
> }
> sink()
>
> If you open this file in a text editor (e.g. Notepad) you will see that it
> contains 3 lines (rows), each with four TAB separated strings. Now to
> import this into a word processing document. I don't have MS Word, so I'll
> do it with Libre Office Writer and hope that the procedure is similar.
>
> Move to where you want the table in your document
> Select Insert|Text from file from the top menu
> Select (highlight) the text you have imported
> Select Convert|Text to table from the top menu
>
> The highlighted area should become a table. I had to reduce the font size
> from 12 to 10 to get the strings to fit into the cells.
>
> There are probably a few more changes that you will want, so let me know
> if you strike trouble.
>
> Jim
>
>
> On Fri, May 8, 2020 at 11:28 PM Subhamitra Patra <
> subhamitra.patra at gmail.com> wrote:
>
>> Dear Sir,
>>
>> Thank you very much for your wonderful suggestion for my problem. Your
>> suggested code has excellently worked and successfully extracted the
>> statistics and p-value in another R object.
>>
>> Concerning your last suggestion, I attempted to separate the strings with
>> TAB character in the "spout" object by using different alternative packages
>> like dplyr, tidyr, qdap, ans also by using split,strsplit function so that
>> can export the statistics and p-values for each column to excel, and later
>> to the MSword file, but got the below error.
>>
>> By using the  split function, I wrote the code as,
>> *string[] split = s.Split(spout, '\t')*
>> where I got the following errors.
>> Error: unexpected symbol in "string[] split"
>> Error: unexpected symbol in "string[[]]split"
>> Error in strsplit(row, "\t") : non-character argument
>>
>> Then I tried with  strsplit function by the below code
>> *strsplit(spout, split)*
>> But, got the below error as
>> Error in as.character(split) :
>>   cannot coerce type 'closure' to vector of type 'character'.
>>
>> Then used dplyr and tidyr package and the wrote the below code
>> library(dplyr)
>> library(tidyr)
>> *separate(spout,value,into=c(?ChiSq?,?p?),sep=?,?)*
>> *separate(spout,List of length 12,into=c(?ChiSq?,?p?),sep="\t")*
>> But, got the errors as,
>> Error: unexpected input in "separate(spout,value,into=c(?"
>> Error: unexpected symbol in "separate(spout,List of"
>>
>> Then used qdap package with the code below
>>
>> *colsplit2df(spout,, c("ChiSq", "p"), ",")*
>> *colsplit2df(spout,, c("ChiSq", "p"), sep = "\t")*
>> But got the following errors
>> Error in dataframe[, splitcol] : incorrect number of dimensions
>> In addition: Warning message:
>> In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
>>   dataframe object is not of the class data.frame
>> Error in dataframe[, splitcol] : incorrect number of dimensions
>> In addition: Warning message:
>> In colsplit2df_helper(dataframe = dataframe, splitcol = splitcols[i],  :
>>   dataframe object is not of the class data.frame
>>
>> Sir, please suggest me where I am going wrong in the above to separate
>> string in the "spout" object.
>>
>> Thank you very much for your help.
>>
>> [image: Mailtrack]
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>> notified by
>> Mailtrack
>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
>> 06:51:46 PM
>>
>> On Fri, May 8, 2020 at 4:47 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>>> 1) In general, *apply functions return a list with the number of
>>> elements equal to the number of columns or other elements of the input
>>> data. You can assign that list as I have to "spout" in the first example.
>>>
>>> 2) spout<-list() assigns the name "spout" to an empty list. As we are
>>> processing columns 2 to 12 of the input data, spout[[i-1]] assigns the
>>> results to elements 1 to 11 of the list "spout". Just a low trick.
>>>
>>> 1a) Yes, you can create a "wrapper" function that will return only the
>>> statistic and p.value.
>>>
>>> # create a function that returns only the
>>> # statistic and p.value as a string
>>> archStatP<-function(x) {
>>>  archout<-ArchTest(x)
>>>  return(sprintf("ChiSq = %f, p = %f",archout$statistic,archout$p.value))
>>> }
>>> # using "lapply", run the test on each column
>>> spout<-lapply(sp_8_5[,2:12],archStatP)
>>>
>>> Note that I should have used "lapply". I didn't check the output
>>> carefully enough.
>>>
>>> 2a) Now you only have to separate the strings in "spout" with TAB
>>> characters and import the result into Excel. I have to wash the dishes, so
>>> you're on your own.
>>>
>>> Jim
>>>
>>> On Fri, May 8, 2020 at 8:26 PM Subhamitra Patra <
>>> subhamitra.patra at gmail.com> wrote:
>>>
>>>> Dear Sir,
>>>>
>>>> Thank you very much for such an excellent solution to my problem. I was
>>>> trying sapply function since last days, but was really unable to write
>>>> properly. Now, I understood my mistake in using sapply function in the
>>>> code. Therefore, I have two queries regarding this which I want to discuss
>>>> here just for my learning purpose.
>>>>
>>>> 1. While using sapply function for estimating one method across the
>>>> columns of a data frame, one needs to define the list of the output table
>>>> after using sapply so that the test results for each column will be
>>>> consistently stored in an output object, right?
>>>>
>>>> 2. In the spout<- list() command, what spout[[i-1]]  indicates?
>>>>
>>>> Sir, one more possibility which I would like to ask related to my above
>>>> problem just to learn for further R programming language.
>>>>
>>>> After running your suggested code, all the results for each column are
>>>> being stored in the spout object. From this, I need only the statistics and
>>>> P-value for each column. So, my queries are:
>>>>
>>>> 1. Is there any way to extract only two values (i.e., statistics and
>>>> p-value) for each column that stored in spout object and save these two
>>>> values in another R data frame for each column?
>>>>  or
>>>> 2. Is there any possibility that the statistics and p-value
>>>> calculated for each column can directly export to a word file in a table
>>>> format (having 4 columns and 3 rows). In particular, is it possible to
>>>> extract both statistic and p-value results for each column to an MS word
>>>> file with the format of A1, A2, A3, A4 column results in 1st row, A5, A6,
>>>> A7, A8 column results in 2nd row, and A9, A10, A11, A12 column results in
>>>> the 3rd row of the table?
>>>>
>>>>
>>>> Like before, your suggestion will definitely help me to learn the
>>>> advanced R language.
>>>>
>>>> Thank you very much for your help.
>>>>
>>>> [image: Mailtrack]
>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> Sender
>>>> notified by
>>>> Mailtrack
>>>> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&> 05/08/20,
>>>> 03:47:26 PM
>>>>
>>>> On Fri, May 8, 2020 at 2:37 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>>>
>>>>> Hi Subhamitra,
>>>>> This isn't too hard:
>>>>>
>>>>> # read in the sample data that was
>>>>> # saved in the file "sp_8_5.tab"
>>>>> sp_8_5<-read.table("sp_8_5.tab",sep="\t",
>>>>>  header=TRUE,stringsAsFactors=FALSE)
>>>>> library(tseries)
>>>>> library(FinTS)
>>>>> # using "sapply", run the test on each column
>>>>> spout<-sapply(sp_8_5[,2:12],ArchTest)
>>>>>
>>>>> The list "spout" contains the test results. If you really want to use
>>>>> a loop:
>>>>>
>>>>> spout<-list()
>>>>> for(i in 2:12) spout[[i-1]]<-ArchTest(sp_8_5[,i])
>>>>>
>>>>> Jim
>>>>>
>>>>>
>>>>> On Fri, May 8, 2020 at 5:27 PM Subhamitra Patra <
>>>>> subhamitra.patra at gmail.com> wrote:
>>>>>
>>>>>> Dear Sir,
>>>>>>
>>>>>> Herewith I am pasting a part of my sample data having 12 columns
>>>>>> below, and want to calculate ARCH test for the 12 columns by using a loop.
>>>>>>
>>>>>>
>>>>
>>>> --
>>>> *Best Regards,*
>>>> *Subhamitra Patra*
>>>> *Phd. Research Scholar*
>>>> *Department of Humanities and Social Sciences*
>>>> *Indian Institute of Technology, Kharagpur*
>>>> *INDIA*
>>>>
>>>
>>
>> --
>> *Best Regards,*
>> *Subhamitra Patra*
>> *Phd. Research Scholar*
>> *Department of Humanities and Social Sciences*
>> *Indian Institute of Technology, Kharagpur*
>> *INDIA*
>>
>

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From pro|jcn@@h @end|ng |rom gm@||@com  Wed May 13 14:42:23 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 13 May 2020 08:42:23 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <A9518FD1-4E6D-4BF2-95D2-9F165CAD38FA@gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
 <A9518FD1-4E6D-4BF2-95D2-9F165CAD38FA@gmail.com>
Message-ID: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>

The Richards' curve is analytic, so nlsr::nlxb() should work better than nls() for getting derivatives --
the dreaded "singular gradient" error will likely stop nls(). Also likely, since even a 3-parameter
logistic can suffer from it (my long-standing Hobbs weed infestation problem below), is
that the Jacobian will be near-singular. And badly scaled. Nonlinear fitting problems essentially
have different scale in different portions of the parameter space.

You may also want to "fix" or mask one or more parameters to reduce the dimensionality of the problem,
and nlsr::nlxb() can do that.

The Hobbs problem has the following 12 data values for time points 1:12

# Data for Hobbs problem
ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
tdat  <-  seq_along(ydat) # for testing

An unscaled model is

eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))

This problem looks simple, but has given lots of software grief over nearly 5 decades. In 1974 an
extensive search had all commonly available software failing, which led to the code that evolved
into nlsr, though there are plenty of cases where really awful code will luckily find a good
solution. The issue is getting a solution and knowing it is reasonable. I suspect a Richards'
model will be more difficult unless the OP has a lot of data and maybe some external information
to fix or constrain some parameters.

JN


On 2020-05-13 5:41 a.m., Peter Dalgaard wrote:
> Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)
> 
> -pd 
> 
>> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>
>> Hi,
>>
>> Is there any R package to fit Richards' curve in the form of
>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>>
>> I found there is one package grofit, but currently defunct.
>>
>> Any pointer appreciated.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From m@n|@hmukherjee @end|ng |rom hotm@||@com  Wed May 13 15:33:03 2020
From: m@n|@hmukherjee @end|ng |rom hotm@||@com (Manish Mukherjee)
Date: Wed, 13 May 2020 13:33:03 +0000
Subject: [R] Extracting the first currency value from PDF files
Message-ID: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>

Hi All,

Need some help with the following code , i have a number of pdf files , and the first page of those files gives a currency value $xxx,xxx,xxx . How to extract this value from a number of PDF files and put it in a data frame . I am able to do it for a single file
with the code where opinions is the text data and 1 is the first currency value
```
d=str_nth_currency(opinions, 1)
df = subset(d, select = c(amount) )
df

I want this to loop over multiple pdf files

I have tried somesthing like this but not working
for (i in 1:length(files)){
  print(i)
  pdf_text(paste("filepath ", files[i],sep = ""))
  str_nth_currency(files[i], 1)
}


Please help.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 13 15:44:14 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 13 May 2020 06:44:14 -0700
Subject: [R] Extracting the first currency value from PDF files
In-Reply-To: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
References: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
Message-ID: <7555D8CD-0D2F-4DC5-8B39-17AA2EF21BA4@dcn.davis.ca.us>

PDF files are actually "programs" that place graphic symbols on pages, and the order in which those symbols are placed (the order in which most pdf-to-text conversions return characters) may have nothing to do with how they appear visually. There is not even a guarantee that those symbols are represented as characters in the file... they could be part of embedded bitmaps.

In summary, you need to review what your "pdf_text" function is able to extract from your files without filtering... it may or may not be consistent enough to allow you to do what you want... and we certainly have no idea what it is able to extract from your files.

On May 13, 2020 6:33:03 AM PDT, Manish Mukherjee <manishmukherjee at hotmail.com> wrote:
>Hi All,
>
>Need some help with the following code , i have a number of pdf files ,
>and the first page of those files gives a currency value $xxx,xxx,xxx .
>How to extract this value from a number of PDF files and put it in a
>data frame . I am able to do it for a single file
>with the code where opinions is the text data and 1 is the first
>currency value
>```
>d=str_nth_currency(opinions, 1)
>df = subset(d, select = c(amount) )
>df
>
>I want this to loop over multiple pdf files
>
>I have tried somesthing like this but not working
>for (i in 1:length(files)){
>  print(i)
>  pdf_text(paste("filepath ", files[i],sep = ""))
>  str_nth_currency(files[i], 1)
>}
>
>
>Please help.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jr@| @end|ng |rom po@teo@no  Wed May 13 16:17:06 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 16:17:06 +0200
Subject: [R] Extracting the first currency value from PDF files
In-Reply-To: <7555D8CD-0D2F-4DC5-8B39-17AA2EF21BA4@dcn.davis.ca.us>
References: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
 <7555D8CD-0D2F-4DC5-8B39-17AA2EF21BA4@dcn.davis.ca.us>
Message-ID: <20200513141706.GA1705207@posteo.no>

On 2020-05-13 06:44 -0700, Jeff Newmiller wrote:
> On May 13, 2020 6:33:03 AM PDT, Manish Mukherjee wrote:
> > 
> > How to extract this value from a number 
> > of PDF files and put it in a data frame. 
> 
> they could be part of embedded bitmaps.

Dear Manish and Jeff,

I recently found the programs pdftoppm [1] 
and Google tesseract [2] to be really useful 
when reading text from pdfs formatted as "a 
single column of text of variable sizes", 
e.g. a receipt from a grocery store :)

folder <- "path/to/pdfs"
pdfs <- list.files(folder, ".pdf$")
pdf <- pdfs[1]
cmd <-
  paste0("pdftoppm -png -r 500 ",
         folder, pdf, " /tmp/out && ",
         "tesseract /tmp/out-1.png - ",
         "-l nor --psm 4")
lines <- system(cmd, intern=TRUE)
# x <- lapply(x, system, intern=TRUE)
# names(x) <- pdfs
# saveRDS(x, "texts.rds")

In any other case with a sensibly formatted 
pdf, I would have used pdftotext [3] ...

Best,
Rasmus

[1] https://manpages.debian.org/buster/poppler-utils/pdftoppm.1.en.html
[2] https://manpages.debian.org/buster/tesseract-ocr/tesseract.1.en.html
[3] https://manpages.debian.org/buster/poppler-utils/pdftotext.1.en.html


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed May 13 17:05:59 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 13 May 2020 11:05:59 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
References: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
Message-ID: <C80AB811-BABB-4FD0-B47F-D3BAC2578317@comcast.net>

John, have you ever looked at interval optimization as an alternative since it can lead to provably global minima?

Bernard
Sent from my iPhone so please excuse the spelling!"

> On May 13, 2020, at 8:42 AM, J C Nash <profjcnash at gmail.com> wrote:
> 
> ?The Richards' curve is analytic, so nlsr::nlxb() should work better than nls() for getting derivatives --
> the dreaded "singular gradient" error will likely stop nls(). Also likely, since even a 3-parameter
> logistic can suffer from it (my long-standing Hobbs weed infestation problem below), is
> that the Jacobian will be near-singular. And badly scaled. Nonlinear fitting problems essentially
> have different scale in different portions of the parameter space.
> 
> You may also want to "fix" or mask one or more parameters to reduce the dimensionality of the problem,
> and nlsr::nlxb() can do that.
> 
> The Hobbs problem has the following 12 data values for time points 1:12
> 
> # Data for Hobbs problem
> ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
>          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
> tdat  <-  seq_along(ydat) # for testing
> 
> An unscaled model is
> 
> eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
> 
> This problem looks simple, but has given lots of software grief over nearly 5 decades. In 1974 an
> extensive search had all commonly available software failing, which led to the code that evolved
> into nlsr, though there are plenty of cases where really awful code will luckily find a good
> solution. The issue is getting a solution and knowing it is reasonable. I suspect a Richards'
> model will be more difficult unless the OP has a lot of data and maybe some external information
> to fix or constrain some parameters.
> 
> JN
> 
> 
>> On 2020-05-13 5:41 a.m., Peter Dalgaard wrote:
>> Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)
>> 
>> -pd 
>> 
>>>> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>> 
>>> Hi,
>>> 
>>> Is there any R package to fit Richards' curve in the form of
>>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>>> 
>>> I found there is one package grofit, but currently defunct.
>>> 
>>> Any pointer appreciated.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Wed May 13 17:11:48 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 13 May 2020 11:11:48 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <C80AB811-BABB-4FD0-B47F-D3BAC2578317@comcast.net>
References: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
 <C80AB811-BABB-4FD0-B47F-D3BAC2578317@comcast.net>
Message-ID: <cb1776b5-5716-5379-ab84-7248881bf165@gmail.com>

Many moons ago (I think early 80s) I looked at some of the global optimizers,
including several based on intervals. For problems of this size, your suggestion
makes a lot of sense, though it has been so long since I looked at those techniques
that I will avoid detailed comment.

I've not looked to see if there are any such solvers for R, but would be happy
to learn (probably best off-list). Also I'm willing to work at a modest pace on
developing one. A starting point might be nls2 package.

Best, JN

On 2020-05-13 11:05 a.m., Bernard Comcast wrote:
> John, have you ever looked at interval optimization as an alternative since it can lead to provably global minima?
> 
> Bernard
> Sent from my iPhone so please excuse the spelling!"
> 
>> On May 13, 2020, at 8:42 AM, J C Nash <profjcnash at gmail.com> wrote:
>>
>> ?The Richards' curve is analytic, so nlsr::nlxb() should work better than nls() for getting derivatives --
>> the dreaded "singular gradient" error will likely stop nls(). Also likely, since even a 3-parameter
>> logistic can suffer from it (my long-standing Hobbs weed infestation problem below), is
>> that the Jacobian will be near-singular. And badly scaled. Nonlinear fitting problems essentially
>> have different scale in different portions of the parameter space.
>>
>> You may also want to "fix" or mask one or more parameters to reduce the dimensionality of the problem,
>> and nlsr::nlxb() can do that.
>>
>> The Hobbs problem has the following 12 data values for time points 1:12
>>
>> # Data for Hobbs problem
>> ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
>>          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
>> tdat  <-  seq_along(ydat) # for testing
>>
>> An unscaled model is
>>
>> eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
>>
>> This problem looks simple, but has given lots of software grief over nearly 5 decades. In 1974 an
>> extensive search had all commonly available software failing, which led to the code that evolved
>> into nlsr, though there are plenty of cases where really awful code will luckily find a good
>> solution. The issue is getting a solution and knowing it is reasonable. I suspect a Richards'
>> model will be more difficult unless the OP has a lot of data and maybe some external information
>> to fix or constrain some parameters.
>>
>> JN
>>
>>
>>> On 2020-05-13 5:41 a.m., Peter Dalgaard wrote:
>>> Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)
>>>
>>> -pd 
>>>
>>>>> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>>>
>>>> Hi,
>>>>
>>>> Is there any R package to fit Richards' curve in the form of
>>>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>>>>
>>>> I found there is one package grofit, but currently defunct.
>>>>
>>>> Any pointer appreciated.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From mcg@rvey@bern@rd @end|ng |rom comc@@t@net  Wed May 13 17:15:50 2020
From: mcg@rvey@bern@rd @end|ng |rom comc@@t@net (Bernard Comcast)
Date: Wed, 13 May 2020 11:15:50 -0400
Subject: [R] Fitting Richards' curve
In-Reply-To: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
References: <05c6b23a-277d-e3ee-e9bc-384f0f886189@gmail.com>
Message-ID: <4F3563DB-CBEB-4E0B-83D0-DB99EB3A5006@comcast.net>

Also, in the full curve referenced on Wikpedia, the parameters Q And M are confounded - you only need one or the other But not both. If you are using both and trying to estimate them both you will have problems.

I have fitted these curves quite easily using the Solver in Excel.

Bernard
Sent from my iPhone so please excuse the spelling!"

> On May 13, 2020, at 8:42 AM, J C Nash <profjcnash at gmail.com> wrote:
> 
> ?The Richards' curve is analytic, so nlsr::nlxb() should work better than nls() for getting derivatives --
> the dreaded "singular gradient" error will likely stop nls(). Also likely, since even a 3-parameter
> logistic can suffer from it (my long-standing Hobbs weed infestation problem below), is
> that the Jacobian will be near-singular. And badly scaled. Nonlinear fitting problems essentially
> have different scale in different portions of the parameter space.
> 
> You may also want to "fix" or mask one or more parameters to reduce the dimensionality of the problem,
> and nlsr::nlxb() can do that.
> 
> The Hobbs problem has the following 12 data values for time points 1:12
> 
> # Data for Hobbs problem
> ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443,
>          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
> tdat  <-  seq_along(ydat) # for testing
> 
> An unscaled model is
> 
> eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
> 
> This problem looks simple, but has given lots of software grief over nearly 5 decades. In 1974 an
> extensive search had all commonly available software failing, which led to the code that evolved
> into nlsr, though there are plenty of cases where really awful code will luckily find a good
> solution. The issue is getting a solution and knowing it is reasonable. I suspect a Richards'
> model will be more difficult unless the OP has a lot of data and maybe some external information
> to fix or constrain some parameters.
> 
> JN
> 
> 
>> On 2020-05-13 5:41 a.m., Peter Dalgaard wrote:
>> Shouldn't be hard to set up with nls(). (I kind of suspect that the Richards curve has more flexibility than data can resolve, especially the subset (Q,B,nu) seems highly related, but hey, it's your data...)
>> 
>> -pd 
>> 
>>>> On 13 May 2020, at 11:26 , Christofer Bogaso <bogaso.christofer at gmail.com> wrote:
>>> 
>>> Hi,
>>> 
>>> Is there any R package to fit Richards' curve in the form of
>>> https://en.wikipedia.org/wiki/Generalised_logistic_function
>>> 
>>> I found there is one package grofit, but currently defunct.
>>> 
>>> Any pointer appreciated.
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jr@| @end|ng |rom po@teo@no  Wed May 13 17:28:39 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 17:28:39 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
Message-ID: <20200513152839.GA1776058@posteo.no>

On 2020-05-09 11:40 -0400, J C Nash wrote:
> 
> > solve(D)
>      [,1] [,2]
> [1,] -2.0  1.0
> [2,]  1.5 -0.5
> > D %*% solve(D)
>      [,1]         [,2]
> [1,]    1 1.110223e-16
> [2,]    0 1.000000e+00
> >

Dear list,

I get another solution on my Linux i7-7500U 
laptop, but the same solution on my FreeBSD 
E3-1240Lv5 machine with a really old R 
version (without BLAS) ...

How is this possible?

> D %*% solve(D)
             [,1] [,2]
[1,] 1.000000e+00    0
[2,] 8.881784e-16    1
> sessionInfo()
R version 3.6.3 (2020-02-29)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Arch Linux

Matrix products: default
BLAS:   /usr/lib/libopenblasp-r0.3.9.so
LAPACK: /usr/lib/liblapack.so.3.9.0

locale:
 [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_DK.UTF-8        LC_COLLATE=en_GB.UTF-8    
 [5] LC_MONETARY=nb_NO.UTF-8    LC_MESSAGES=en_GB.UTF-8   
 [7] LC_PAPER=nb_NO.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=nb_NO.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] compiler_3.6.3
> 

>From the machine running FreeBSD:

> D %*% solve(D)
     [,1]         [,2]
[1,]    1 1.110223e-16
[2,]    0 1.000000e+00
> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: amd64-portbld-freebsd12.0 (64-bit)
Running under: FreeBSD hmm 12.0-RELEASE FreeBSD 12.0-RELEASE r341666 GENERIC  amd64

Matrix products: default
LAPACK: /usr/local/lib/R/lib/libRlapack.so

locale:
[1] C/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.5.2
>

Best,
Rasmus


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 18:13:13 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 16:13:13 +0000
Subject: [R] Help with Radius problem --update
Message-ID: <BYAPR06MB53831A2287FAA73544C21AEFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello all. Thank you in advance for any additional suggestions.

I have with, Jim's help, found some traction in my pursuit of this problem. "determine the largest concentration of members in the smallest radius"
However, I need guidance in efficiencies as I will explain below.

1. I have used Jim's routine to map ALL Member density -- Very helpful visualization

geomat<-makeDensityMatrix(geo1[,c("latitude","longitude")],
                          xlim=range(geo1$longitude),ylim=range(geo1$latitude))
# Range of density (>0) - 1.119816 3.922018
latlim<-range(geo1$latitude)
lonlim<-range(geo1$longitude)

#library(maps) Now in pkgs 


map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
            red=c(0.5,1),green=0,blue=0,pch=15)
            title("All Member Geo Density Plot")

2. Then using this reference -->  https://stackoverflow.com/questions/60141556/how-to-calculate-distance-between-points-from-a-reference-point-using-r and the sf pkg functions
3. And identifying and using a single point in my data located in the most dense city "Brooklyn" I have created a distance column in miles with Brooklyn as reference point

str(geo1)

individual_dets_sf <- st_as_sf(geo1, coords = c("longitude", "latitude"),
                               crs = 4326) %>% 
  ungroup()
brooklyn <- data.frame("longitude" = -73.973516, "latitude" = 40.57672)
brooklyn_sf <- st_as_sf(brooklyn, 
                       coords = c("longitude", "latitude"),
                       crs = 4326,
                       remove = FALSE) 

individual_dets_sf_2 <- individual_dets_sf %>% 
  mutate(distances = st_distance(., brooklyn_sf, by_element = TRUE))

individual_dets_sf_2$distances

View(individual_dets_sf_2)

individual_dets_sf_3 <- as.data.frame(individual_dets_sf_2)

4. Then I did the following;

I arbitrarily created 8 Radius routines 100 Mi through 2000k Mi
Example:
#Radius1----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi, MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 2000) %>%
  tally()


Then I used the tibble info from the console to manually calculate the percent of members covered and missed
# A tibble: 2226 x 3 
#2226/2352 = 0.9464286-1 =-0.0535714 missed

 My optimal Radius by the way is 1200k Mi. which I mapped like I did for All members above

My new question is:

Is there a more elegant way to output something like this that I created manually and entered into excel

Radius Distance	                                 Members Covered	Percent Covered	Percent Missed		
R1              2000Mi	                            2226	                       0.9464286	               0.0535714		                                              Missed Diff From Radius2
R2              1500Mi  	                            2126	                       0.9039116	               0.0960884		                                              
R3              1200Mi	                            1990	                       0.8460884	               0.1539116        Optimal	                                                                           0.0578232
R4              1000Mi	                            1644	                       0.6989796	               0.3010204		                                                                           0.204932
R5                900Mi	                            1573	                       0.6687925	               0.3312075		
R6                700Mi	                            1198	                       0.5093537	               0.4906463	Break Even point 	
R7                500Mi	                              972	                       0.4132653	               0.4132653		
R8                100Mi	                              356	                       0.1513605	               0.1513605		
 

Isn't there a better way to do all this below rather than manually? See sample data below please?
Here is where I would like to create some efficiencies please.

#Radius1----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 2000) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 2226 x 3 2226/2352 = 0.9464286-1 =-0.0535714 missed


#Radius2----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 1500) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 2126 x 3 2126/2352 = 0.9039116-1 =-0.0960884 missed

#Radius3----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 1200) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 1990 x 3 1990/2352 = 0.8460884-1 =-0.1539116 missed


#Radius4----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 1000) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 1644 x 3 1644/2352 = 0.6989796-1 =--0.3010204 missed

#Radius5----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 900) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 1573 x 3 1573/2352 = 0.6687925-1 =--0.3312075 missed

#Radius6----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 700) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 1198 x 3 1198/2352 = 0.5093537-1 =-0.4906463 missed ~ 50/50 at 700 mile radius

#Radius7----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 500) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 972 x 3 972/2352 = 0.4132653-1 =-0.5867347 missed

#Radius8----

individual_dets_sf_3X %>% 
  
  dplyr::select(MBR_SUBSCRIBERID,distances_Mi) %>% 
  group_by(distances_Mi,MBR_SUBSCRIBERID) %>% 
  filter(distances_Mi <= 100) %>%
  tally()
#summarize(mean_size = mean(Prov_ActiveMonths))
## A tibble: 356 x 2 356/2352 = 0.1513605-1 =-0.8486395 missed
	


dput(sample)
structure(list(state = structure(c(2L, 22L, 10L, 12L, 6L, 22L, 
11L, 22L, 22L, 32L, 19L, 29L, 9L, 9L, 10L, 30L, 33L, 35L, 41L, 
12L, 12L, 36L, 4L, 9L, 8L, 39L, 36L, 36L, 41L, 28L, 12L, 28L, 
28L, 12L, 11L, 2L, 12L, 22L, 32L, 22L, 12L, 10L, 10L, 22L, 10L
), .Label = c("AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", 
"FL", "GA", "IA", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", 
"MI", "MN", "MO", "NC", "NE", "NJ", "NM", "NV", "NY", "OH", "OK", 
"OR", "PA", "SC", "SD", "TN", "TX", "UT", "VA", "WA", "WI", "WV"
), class = "factor"), city = structure(c(838L, 1030L, 262L, 218L, 
166L, 973L, 339L, 451L, 660L, 358L, 281L, 1280L, 129L, 223L, 
989L, 721L, 550L, 731L, 1325L, 688L, 1184L, 281L, 759L, 1171L, 
1305L, 587L, 272L, 581L, 263L, 152L, 217L, 152L, 390L, 5L, 571L, 
1006L, 1162L, 939L, 170L, 1033L, 1002L, 586L, 586L, 192L, 586L
), .Label = c("ABBOTTSTOWN", "ABILENE", "ACWORTH", "ADAMS", "ADDISON", 
"ADKINS", "ALBANY", "ALBIA", "ALBUQUERQUE", "ALEXANDRIA", "ALFRED", 
"ALIQUIPPA", "ALISO VIEJO", "ALLEN PARK", "ALLENTOWN", "ALPHA", 
"ALPHARETTA", "ALPINE", "ALTOONA", "AMARILLO", "AMBLER", "AMBRIDGE", 
"AMHERST", "AMITYVILLE", "ANAHIEM", "ANDERSON", "ANDOVER", "ANGIER", 
"ANGLETON", "ANKENY", "ANN ARBOR", "ANZA", "APEX", "APOLLO", 
"ARCADIA", "ARCHDALE", "ARDMORE", "ARGYLE", "ARKANSAS CITY", 
"ARLINGTON", "ARNOLD", "ARTHUR", "ASHEBORO", "ASHEVILLE", "ASHLAND", 
"ASHLAND CITY", "ASHTON", "ASTON", "ATHENS", "ATLANTA", "ATLANTIC BCH", 
"AUBORN", "AUGUSTA", "AURORA", "AUSTELL", "AUSTIN", "AVENTURA", 
"AVONDALE ESTATES", "BAKERSFIELD", "BALDWIN CITY", "BALDWIN PLACE", 
"BALLWIN", "BANGOR", "BARBOURSVILLE", "BARNEGAT", "BARTLETT", 
"BARTON", "BARTONVILLE", "BASSETT", "BATAVIA", "BATESBURG", "BATON ROUGE", 
"BAXTER SPRINGS", "BAY CITY", "BAYONNE", "BAYSIDE", "BAYTOWN", 
"BEAR", "BEAUMONT", "BEECH CREEK", "BEECHER CITY", "BELFAIR", 
"BELLE VERNON", "BELLEAIR", "BELLEVUE", "BELLMAWR", "BELLMORE", 
"BELLWOOD", "BELMAR", "BELMONT", "BELVIDERE", "BENNET", "BENTON", 
"BENTONVILLE", "BERLIN", "BESSEMER CITY", "BETHANY", "BETHEL", 
"BETHEL PARK", "BETHESDA", "BETHLEHEM", "BETTENDORF", "BIGLERVILLE", 
"BINGHAMTON", "BIRDSBORO", "BIWABIK", "BLACK MOUNTAIN", "BLACKSBURG", 
"BLAINE", "BLAIRSVILLE", "BLAKESLEE", "BLOOMFIELD", "BLUE BELL", 
"BLUE GRASS", "BLUE POINT", "BLUE SPRINGS", "BOCA RATON", "BODFISH", 
"BOILING SPRINGS", "BOLIVAR", "BON AQUA", "BONNER SPRINGS", "BONNEY LAKE", 
"BOONEVILLE", "BOSSIER CITY", "BOUNTIFUL", "BOWLING GREEN", "BOYERTOWN", 
"BOYNTON BEACH", "BRADENTON", "BRADENVILLE", "BRANDAMORE", "BRANDON", 
"BRANFORD", "BRANSON", "BRASELTON", "BREEZEWOOD", "BRENTON", 
"BRENTWOOD", "BREWSTER", "BRICK", "BRIDGEPORT", "BRIDGETON", 
"BRIGHAM CITY", "BROADDUS", "BROCKPORT", "BROGUE", "BROKEN ARROW", 
"BRONX", "BROOKFIELD", "BROOKHAVEN", "BROOKLYN", "BROWNS MILLS", 
"BROWNSBURG", "BROWNSVILLE", "BRUNSWICK", "BRYAN", "BUCKSPORT", 
"BUDD LAKE", "BUFFALO", "BULVERDE", "BUNKERVILLE", "BURBANK", 
"BURFORD", "BURIEN", "BURLINGTON", "BURR RIDGE", "BURTON", "BUSKIRK", 
"BUTLER", "BUXTON", "BYRON", "CALERA", "CAMBRIDGE", "CAMP HILL", 
"CAMPBELL", "CANAAN", "CANAL WINCHESTER", "CANASTOTA", "CANTON", 
"CARDIFF BY THE SEA", "CARL JUNCTION", "CARLISLE", "CARLTON", 
"CAROL STREAM", "CARROLLTON", "CARSON CITY", "CARTERSVILLE", 
"CARTHAGE", "CARY", "CASEYVILLE", "CASSVILLE", "CASWELL", "CATO", 
"CEDAR RAPIDS", "CEDARBURG", "CEDARVILLE", "CENTER POINT", "CENTERTON", 
"CENTRAL ISLIP", "CENTRAL POINT", "CHAGRIN FALLS", "CHALFONT", 
"CHAPEL HILL", "CHAPIN", "CHARDON", "CHARITON", "CHARLESTON", 
"CHARLOTTE", "CHEROKEE", "CHERRY HILL", "CHERRY VALLEY", "CHESHIRE", 
"CHEST SPRINGS", "CHESTER", "CHESTERTON", "CHICAGO", "CHILLICOTHE", 
"CHOCTAW", "CICERO", "CINCINNATI", "CLAY", "CLEARWATER", "CLEARWATER BEACH", 
"CLEMMONS", "CLENDENIN", "CLEVELAND", "CLEVELAND HTS", "CLEVES", 
"CLIFFSIDE PARK", "CLIFTON", "CLIFTON PARK", "CLIFTON SPGS", 
"CLINTON", "COACHELLA", "COAL TOWNSHIP", "COATESVILLE", "COLLEGE STATION", 
"COLLINSVILLE", "COLONIA", "COLUMBIA", "COLUMBUS", "COMMERCE", 
"CONCORD", "CONCORD TOWNSHIP", "CONNELLSVILLE", "CONROE", "CONWAY", 
"CONYERS", "COOKEVILLE", "COON RAPIDS", "CORAOPOLIS", "CORDOVA", 
"CORNELIUS", "CORNWALL", "CORONADO", "CORPUS CHRISTI", "CORRY", 
"COTTAGE HILLS", "COTTLEVILLE", "COTTONWOOD", "COVINGTON", "COWEN", 
"CRANBERRY TOWNSHIP", "CROWN POINT", "CUBA", "CUMBERLAND CENTER", 
"CUMBERLAND FORESIDE", "CUMMING", "CUYAHOGA FLS", "CYPRESS", 
"DALLAS", "DALLASTOWN", "DANBURY", "DANIA BCH", "DANVILLE", "DAVENPORT", 
"DAVIE", "DAVIS JUNCTION", "DAWSON", "DAYTON", "DE SOTO", "DECATUR", 
"DEFIANCE", "DEFOREST", "DELAVAN", "DELMAR", "DELRAY BEACH", 
"DEMOTTE", "DENISON", "DENVER", "DEPTFORD", "DERBY", "DERRY", 
"DES MOINES", "DETROIT", "DICKINSON", "DILLSBURG", "DITTMER", 
"DIXON", "DONNA", "DOTHAN", "DOUGLAS", "DOUGLASVILLE", "DOVER", 
"DOYLESTOWN", "DREXEL HILL", "DUBLIN", "DUCHESNE", "DULUTH", 
"DUNCAN", "DUNCANNON", "DUNCANSVILLE", "DUNEDIN", "DUNNSVILLE", 
"DURHAM", "DUTTON", "EAGLE GROVE", "EAST AURORA", "EAST CHICAGO", 
"EAST EARL", "EAST HANOVER", "EAST HARTFORD", "EAST HARTLAND", 
"EAST LEROY", "EAST LIVERPOOL", "EAST MEADOW", "EAST NORTHPORT", 
"EAST ORANGE", "EAST PEORIA", "EAST SAINT LOUIS", "EASTON", "EDGERTON", 
"EDISON", "EDMOND", "EGG HBR TWP", "EL DORADO", "EL PASO", "ELDRIDGE", 
"ELGIN", "ELIZABETHVILLE", "ELLISVILLE", "ELLSWORTH", "ELLWOOD CITY", 
"ELMHURST", "ELMWOOD PARK", "ELOY", "ELY", "EMERSON", "EMORY", 
"ENDICOTT", "ENGLEWOOD", "ENID", "ENNICE", "ENOLA", "ENUMCLAW", 
"EPHRATA", "ERIE", "ESTERO", "EUREKA SPRINGS", "EVANSVILLE", 
"EVERETT", "EXCELSIOR SPRINGS", "EXTON", "FAIRFAX", "FAIRFIELD", 
"FAIRHAVEN", "FAIRVIEW", "FAIRVIEW PARK", "FALMOUTH", "FAR HILLS", 
"FAR ROCKAWAY", "FARMINGDALE", "FARMINGTON", "FAYETTE", "FAYETTEVILLE", 
"FELTON", "FENTON", "FERGUSON", "FIELDALE", "FINDLAY", "FINGERVILLE", 
"FLEMINGTON", "FLINT", "FLORENCE", "FLORESVILLE", "FLORISSANT", 
"FLOVILLA", "FLOWER MOUND", "FLUSHING", "FOGELSVILLE", "FOLSOM", 
"FONTANELLE", "FOREST HILLS", "FOREST PARK", "FORT DODGE", "FORT FAIRFIELD", 
"FORT GAY", "FORT LAUDERDALE", "FORT LEE", "FORT MILL", "FORT MOHAVE", 
"FORT PAYNE", "FORT PIERCE", "FORT SMITH", "FORT WAYNE", "FORT WORTH", 
"FOSTORIA", "FOUNTAIN INN", "FRANKFORT", "FRANKLIN", "FREEDOM", 
"FREEHOLD", "FREEPORT", "FREMONT", "FULLERTON", "FULSHEAR", "FULTON", 
"GADSDEN", "GAFFNEY", "GAINESVILLE", "GALLOWAY", "GARDEN PRAIRIE", 
"GARDNERS", "GARNETT VALLEY", "GARY", "GASTON", "GASTONIA", "GENEVA", 
"GETTYSBURG", "GIBSONIA", "GILBERT", "GIRARD", "GLEN BURNIE", 
"GLEN CARBON", "GLEN COVE", "GLEN MILLS", "GLENSHAW", "GLENVIEW", 
"GLENWOOD", "GOLDEN CITY", "GOLIAD", "GOODSON", "GOODYEAR", "GORHAM", 
"GOSHEN", "GRANBURY", "GRANBY", "GRAND PRAIRIE", "GRAND RAPIDS", 
"GRANDVIEW", "GRANITE CITY", "GRANITE FALLS", "GRANTS PASS", 
"GRASS VALLEY", "GRAVETTE", "GRAYSVILLE", "GREEN LANE", "GREENBRIER", 
"GREENLAWN", "GREENSBORO", "GREENSBURG", "GREENUP", "GREENVILLE", 
"GREER", "GRIFFIN", "GRIFFITH", "GROTON", "GUADALUPE", "GUILFORD", 
"HADLEY", "HAGAN", "HALF WAY", "HALLANDALE BEACH", "HAMBLETON", 
"HAMBURG", "HAMDEN", "HAMILTON", "HAMMOND", "HAMMONTON", "HAMPSHIRE", 
"HAPEVILLE", "HARBORCREEK", "HARLAN", "HARRISBURG", "HARRISON", 
"HARRISONVILLE", "HARTWELL", "HASTINGS", "HATFIELD", "HAVERHILL", 
"HAWK POINT", "HAYESVILLE", "HELEN", "HENDERSON", "HENDERSONVILLE", 
"HENRICO", "HENRIETTA", "HERMITAGE", "HERNDON", "HERSHEY", "HIALEAH", 
"HIAWATHA", "HIBBING", "HICKORY", "HICKORY GROVE", "HIDDENITE", 
"HIGH HILL", "HIGH POINT", "HIGH RIDGE", "HIGHLAND", "HIGHLAND MILLS", 
"HINCKLEY", "HIRAM", "HOBART", "HOLBROOK", "HOLDEN", "HOLDENVILLE", 
"HOLLADAY", "HOLLIS", "HOLLY SPRINGS", "HOLLYWOOD", "HOLTWOOD", 
"HOMER", "HOMESTEAD", "HONEYVILLE", "HOPE MILLS", "HOT SPRINGS", 
"HOT SPRINGS NATIONAL PARK", "HOUSTON", "HUBBARD", "HUDSON", 
"HUFFMAN", "HULL", "HUMANSVILLE", "HUNTERSVILLE", "HUNTINGDON", 
"HUNTINGTON BEACH", "HUNTS POINT", "HURDLE MILLS", "HURRICANE", 
"HURST", "IMPERIAL", "INDEPENDENCE", "INDIAN SHORES", "INDIAN TRAIL", 
"INDIANA", "INDIANAPOLIS", "INDIANOLA", "INMAN", "IOWA CITY", 
"IRMO", "IRVINGTON", "IRWIN", "ISANTI", "ISLESBORO", "ISLIP", 
"ISSAQUAH", "ITHACA", "JACKSBORO", "JACKSON", "JACKSON HTS", 
"JACKSONVILLE", "JAMAICA", "JAMESTOWN", "JASPER", "JEANNETTE", 
"JEFFERSON", "JEFFERSON CITY", "JEWELL", "JOHNSTON", "JOHNSTOWN", 
"JONESBORO", "JONESPORT", "JONESTOWN", "JOPLIN", "JUPITER", "KANKAKEE", 
"KANNAPOLIS", "KANSAS CITY", "KATY", "KEARNEY", "KELLERTON", 
"KEMAH", "KENMORE", "KENNESAW", "KENT", "KINGMAN", "KINGS MOUNTAIN", 
"KINGSLAND", "KINGSTON", "KINGWOOD", "KIRKLAND", "KUTZTOWN", 
"LA QUINTA", "LA RUE", "LA VERNE", "LAFAYETTE", "LAKE BLUFF", 
"LAKE IN THE HILL", "LAKE ISABELLA", "LAKE PLACID", "LAKE SAINT LOUIS", 
"LAKE TAPPS", "LAKE VILLAGE", "LAKE WAUKOMIS", "LAKE WORTH", 
"LAKEBAY", "LAKELAND", "LAKEWOOD", "LAKEWOOD RCH", "LAMBERTVILLE", 
"LANCASTER", "LANDISVILLE", "LANSDALE", "LAREDO", "LARGO", "LAS VEGAS", 
"LATROBE", "LAUDERHILL", "LAWNDALE", "LAWRENCE", "LAWRENCEVILLE", 
"LE MARS", "LE ROY", "LEAGUE CITY", "LEAVENWORTH", "LEAWOOD", 
"LEBANON", "LECOMPTON", "LEECHBURG", "LEES SUMMIT", "LEESBURG", 
"LEESVILLE", "LEETONIA", "LENEXA", "LENOIR", "LEVITTOWN", "LEWES", 
"LEWISTOWN", "LEXINGTON", "LIBERTY TWP", "LILLINGTON", "LINCOLN", 
"LINDENHURST", "LINTON", "LISBON", "LITCHFIELD PK", "LITHIA", 
"LITITZ", "LITTLE EGG HARBOR TO", "LK FOREST PK", "LK PEEKSKILL", 
"LOCKHART", "LOCKWOOD", "LOGAN", "LOMA LINDA", "LONE JACK", "LONG BEACH", 
"LONG LANE", "LONGVIEW", "LORAIN", "LORTON", "LOS ANGELES", "LOUISBURG", 
"LOVES PARK", "LOWER BURRELL", "LOWRY CITY", "LUBBOCK", "LUGOFF", 
"LUMBERTON", "LYNBROOK", "LYNNWOOD", "MACHESNEY PARK", "MACON", 
"MAGNOLIA", "MAHOPAC", "MAHWAH", "MAINEVILLE", "MALVERN", "MALVERNE", 
"MANASSAS", "MANCHACA", "MANCHESTER", "MANCHESTER TOWNSHIP", 
"MANCHESTER TW", "MANHEIM", "MANITO", "MANLIUS", "MANNINGTON", 
"MANSFIELD", "MANSURA", "MAPLE PARK", "MARBLE FALLS", "MARCELLUS", 
"MARICOPA", "MARIETTA", "MARION", "MARKLEYSBURG", "MARS HILL", 
"MARSHALLTOWN", "MARSHFIELD", "MARSHVILLE", "MARTINSVILLE", "MARYSVILLE", 
"MASON CITY", "MASSEY", "MASSILLON", "MASURY", "MATAWAN", "MATEWAN", 
"MATTAPOISETT", "MATTHEWS", "MAYFIELD HTS", "MAYSEL", "MC CLELLAND", 
"MC DONALD", "MC KEES ROCKS", "MC SHERRYSTOWN", "MC VEYTOWN", 
"MCALESTER", "MCDONOUGH", "MCKEESPORT", "MCKINNEY", "MECHANICSBURG", 
"MECHANICSVILLE", "MEDFORD", "MEDIA", "MEDWAY", "MELVILLE", "MEMPHIS", 
"MERIDEN", "MERRIAM", "MERRICK", "MERRILL", "MERRILLVILLE", "MESA", 
"MESQUITE", "MIAMI", "MIAMI GARDENS", "MICHIGAN CITY", "MIDDLETOWN", 
"MIDLAND", "MIFFLINTOWN", "MILFORD", "MILL CREEK", "MILLBROOK", 
"MILLEDGEVILLE", "MILLINOCKET", "MILNER", "MILTON", "MILWAUKEE", 
"MINEOLA", "MINERAL SPGS", "MINERAL WELLS", "MINNEAPOLIS", "MISSION", 
"MISSION HILLS", "MISSION VIEJO", "MISSOURI CITY", "MISSOURI VALLEY", 
"MOLINE", "MONESSEN", "MONROE", "MONROE TOWNSHIP", "MONROEVILLE", 
"MONTCLAIR", "MONTEREY", "MONTICELLO", "MOORESVILLE", "MORGANTON", 
"MORGANTOWN", "MORRIS PLAINS", "MOUND", "MOUND CITY", "MOUNT HOPE", 
"MOUNT JOY", "MOUNT LAUREL", "MOUNT PLEASANT", "MOUNT POCONO", 
"MOUNT UNION", "MOUNT WOLF", "MOUNTVILLE", "MT HOLLY", "MT PLEASANT", 
"MULVANE", "MUNCIE", "MUNSTER", "MURFREESBORO", "MURRAY", "MURRELLS INLT", 
"MUSTANG", "MYERSTOWN", "MYSTIC", "N BRANFORD", "N FT MYERS", 
"NAPERVILLE", "NAPLES", "NAPOLEON", "NASHVILLE", "NATCHITOCHES", 
"NAUGATUCK", "NEOSHO", "NEPTUNE BEACH", "NESCONSET", "NETCONG", 
"NEW BLOOMFIELD", "NEW BRAUNFELS", "NEW BRITAIN", "NEW BRUNSWICK", 
"NEW CASTLE", "NEW CUMBERLAND", "NEW HAVEN", "NEW KENSINGTON", 
"NEW LONDON", "NEW ORLEANS", "NEW PRESTON", "NEW PROVIDENCE", 
"NEW ROCHELLE", "NEW TRIPOLI", "NEW ULM", "NEW WINDSOR", "NEW YORK", 
"NEWARK", "NEWBURGH HEIGHTS", "NEWINGTON", "NEWNAN", "NEWPORT", 
"NEWPORT BEACH", "NEWTON", "NEWTONVILLE", "NIAGARA FALLS", "NIXA", 
"NOANK", "NOKOMIS", "NORCROSS", "NORFOLK", "NORMAN", "NORMANDY PARK", 
"NORTH BABYLON", "NORTH HAVEN", "NORTH HUNTINGDON", "NORTH JACKSON", 
"NORTH LITTLE ROCK", "NORTH MIAMI", "NORTH PORT", "NORTH RICHLAND HILLS", 
"NORTH WALES", "NORTH WATERBORO", "NORTHFIELD", "NORTHRIDGE", 
"NORTON", "NORWALK", "NOTTINGHAM", "NUTLEY", "NYACK", "O FALLON", 
"OAK RIDGE", "OAKDALE", "OCALA", "OCEANSIDE", "OCILLA", "OLATHE", 
"OMAHA", "OMRO", "ONA", "ONALASKA", "OPA LOCKA", "ORADLL", "ORION", 
"ORONO", "ORRINGTON", "OSCEOLA", "OSKALOOSA", "OSSINING", "OSWEGO", 
"OTTAWA", "OVALO", "OVERLAND PARK", "OWASSO", "OWEGO", "OXFORD", 
"OYSTER BAY", "OZARK", "PALERMO", "PALM CITY", "PALM HARBOR", 
"PALMDALE", "PALMYRA", "PALOS HEIGHTS", "PANA", "PAOLA", "PARK RIDGE", 
"PARLIN", "PARMA", "PARRISH", "PARROTT", "PATASKALA", "PAULS VALLEY", 
"PAXTON", "PEA RIDGE", "PEARLAND", "PECULIAR", "PEEKSKILL", "PEKIN", 
"PELION", "PEMBROKE", "PEMBROKE PINES", "PENN YAN", "PENNELLVILLE", 
"PENNSBURG", "PENNSVILLE", "PEORIA", "PERRY", "PERRYOPOLIS", 
"PERRYSBURG", "PERRYVILLE", "PHARR", "PHILADELPHIA", "PHILIPPI", 
"PHOENIX", "PICKENS", "PICKERINGTON", "PIERSON", "PILOT MOUNTAIN", 
"PINE BLUFF", "PINE HILL", "PINEVILLE", "PINNELLAS PARK", "PISCATAWAY", 
"PITMAN", "PITTSBURGH", "PLACIDA", "PLAINFIELD", "PLANO", "PLANT CITY", 
"PLATTEKILL", "PLEASANT HILL", "PLEASANT HOPE", "PLYMOUTH", "POCONO LAKE", 
"POMFRET CENTER", "POMPANO BEACH", "POOLER", "PORT CARBON", "PORT CHARLOTTE", 
"PORT NORRIS", "PORT ORANGE", "PORT SAINT LUCIE", "PORT WASHINGTON", 
"PORTAGE", "PORTER", "PORTERSVILLE", "PORTLAND", "POTTSVILLE", 
"POUGHKEEPSIE", "POWDER SPRINGS", "POWELL", "PRAIRIE VILLAGE", 
"PRESCOTT", "PRESCOTT VALLEY", "PRINCETON", "PROSPECT", "PT PLEASANT", 
"PUNTA GORDA", "PURCHASE", "QUAKERTOWN", "QUEENS VLG", "QUINCY", 
"RALEIGH", "RANDLEMAN", "RANDOLPH", "RAYTOWN", "READING", "READLYN", 
"RED BANK", "RED HOUSE", "RED LION", "REEDERS", "REGO PARK", 
"REHOBOTH BCH", "REIDSVILLE", "RENO", "RENTON", "REPUBLIC", "RICH HILL", 
"RICHARDS", "RICHMOND", "RICHMOND HILL", "RIDGEWOOD", "RINCON", 
"RIO GRANDE CITY", "RIO RANCHO", "RIVERDALE", "RIVERHEAD", "RIVERSIDE", 
"ROANOKE", "ROCHELLE", "ROCHESTER", "ROCK HILL", "ROCKAWAY BCH", 
"ROCKFORD", "ROCKMART", "ROCKY HILL", "ROCKY MOUNT", "ROGERS", 
"ROGERSVILLE", "ROMULUS", "ROOSEVELT", "ROSAMOND", "ROSCOE", 
"ROSENBERG", "ROSENDALE", "ROSWELL", "ROTONDA WEST", "ROUGEMONT", 
"ROXBORO", "ROY", "RUNNELLS", "RURAL HALL", "RUSH VALLEY", "RUSKIN", 
"RUSTBURG", "RUTHER GLEN", "RUTHERFORDTON", "S DEERFIELD", "SACO", 
"SAGINAW", "SAINT AUGUSTINE", "SAINT CHARLES", "SAINT CLAIR", 
"SAINT CLAIRSVILLE", "SAINT LOUIS", "SAINT MARYS", "SAINT MATTHEWS", 
"SAINT PAULS", "SAINT PETERS", "SAINT PETERSBURG", "SALEM", "SALISBURY", 
"SALT LAKE CITY", "SALTSBURG", "SALUNGA", "SAMMAMISH", "SAN ANTONIO", 
"SAN BENITO", "SAN CLEMENTE", "SAN DIEGO", "SAN ELIZARIO", "SAN JUAN", 
"SANDUSKY", "SANDY", "SANFORD", "SANGERVILLE", "SANTA ANA", "SARASOTA", 
"SARATOGA SPRINGS", "SARCOXIE", "SAUGUS", "SAVANNAH", "SAYVILLE", 
"SCALY MOUNTAIN", "SCHAEFFERSTOWN", "SCHULENBURG", "SCOTT", "SCOTTSDALE", 
"SEAFORD", "SEALE", "SEATTLE", "SEDONA", "SEGUIN", "SELDEN", 
"SENECA", "SENOIA", "SEVEN VALLEYS", "SEYMOUR", "SHARON", "SHARPSBURG", 
"SHARPSVILLE", "SHAVANO PARK", "SHAWNEE", "SHELLSBURG", "SHELTON", 
"SHENANDOAH", "SHORELINE", "SHOREWOOD", "SHREVEPORT", "SICKLERVILLE", 
"SILER CITY", "SILOAM SPRINGS", "SIMON", "SIMPSONVILLE", "SIMSBURY", 
"SIOUX CITY", "SIOUX FALLS", "SKOKIE", "SKOWHEGAN", "SLATINGTON", 
"SLIDELL", "SMITHFIELD", "SN BERNRDNO", "SNELLVILLE", "SOCORRO", 
"SOMERS", "SOPHIA", "SOUTH AMBOY", "SOUTH BELOIT", "SOUTH BEND", 
"SOUTH DARTMOUTH", "SOUTH HOUSTON", "SOUTH PARK", "SOUTH PLAINFIELD", 
"SOUTH SIOUX CITY", "SOUTHAMPTON", "SOUTHBURY", "SOUTHGATE", 
"SPARKS", "SPARROW BUSH", "SPENCER", "SPRING", "SPRING BRANCH", 
"SPRING GROVE", "SPRING HILL", "SPRING HOPE", "SPRING VALLEY", 
"SPRINGDALE", "SPRINGFIELD", "SPRINGFIELD GARDENS", "SPRINGHILL", 
"SPRINGTOWN", "SPRNGFLD GDNS", "SPRUCE PINE", "ST PETERSBURG", 
"ST. GEORGE", "STAFFORD", "STAMFORD", "STANDISH", "STANLEY", 
"STANTON", "STATEN ISLAND", "STATESVILLE", "STERLING", "STILLMAN VALLEY", 
"STILWELL", "STOCKBRIDGE", "STOCKTON", "STONE MTN", "STRAFFORD", 
"STRATFORD", "STRONGSVILLE", "STROUDSBURG", "STRUTHERS", "SUFFIELD", 
"SUGAR HILL", "SUGAR LAND", "SULPHUR", "SUMMERVILLE", "SUMMIT", 
"SUN CITY", "SUNBURY", "SUWANEE", "SWANSEA", "SWANTON", "SWEENY", 
"SWISSVALE", "SYCAMORE", "SYLVANIA", "SYRACUSE", "TACOMA", "TAFTVILLE", 
"TAMPA", "TAPPAHANNOCK", "TAR HEEL", "TAUNTON", "TAYLORVILLE", 
"TECUMSEH", "THE WOODLANDS", "THEODOSIA", "THOMASTON", "THOMASVILLE", 
"TIERRA VERDE", "TIFTON", "TILLSON", "TILTON", "TINLEY PARK", 
"TOCCOA", "TOLEDO", "TOLLAND", "TOMBALL", "TOMS RIVER", "TOPEKA", 
"TOPSHAM", "TORRINGTON", "TOVEY", "TOWNSEND", "TRAVELERS RST", 
"TRENT", "TRINITY", "TROUTMAN", "TROY", "TRUMBULL", "TUCKER", 
"TULALIP", "TURNER", "TUSCUMBIA", "TYRONE", "UNION", "UNION CITY", 
"UNIVERSAL CTY", "UNIVERSITY HTS", "UPPR CHICHSTR", "URBANA", 
"URBANDALE", "UTICA", "VAIL", "VALE", "VALENCIA", "VALPARAISO", 
"VAN BUREN", "VAN METER", "VAN WERT", "VANDERGRIFT", "VASSALBORO", 
"VAUGHN", "VENICE", "VERNAL", "VERNON", "VERONA", "VICTOR", "VIDALIA", 
"VIENNA", "VILLA GROVE", "VILLA RIDGE", "VOLANT", "W HAMPTON BCH", 
"W TERRE HAUTE", "WADSWORTH", "WAHOO", "WAKE FOREST", "WALDEN", 
"WALLINGFORD", "WALLINGTON", "WALNUT COVE", "WANAQUE", "WARFORDSBURG", 
"WARMINSTER", "WARREN", "WARSAW", "WARTHEN", "WASHINGTON", "WASOLA", 
"WATAUGA", "WATERBURY", "WATERFORD", "WATERLOO", "WATERVILLE", 
"WATKINS GLEN", "WAXAHACHIE", "WAYCROSS", "WAYNESBORO", "WEBSTER", 
"WEIRTON", "WELLSBORO", "WELLSVILLE", "WENTZVILLE", "WESLACO", 
"WEST CHESTER", "WEST DEPTFORD", "WEST DES MOINES", "WEST HAVEN", 
"WEST HICKORY", "WEST LIBERTY", "WEST MIFFLIN", "WEST PALM BEACH", 
"WEST POINT", "WEST READING", "WEST SENECA", "WEST SUFFIELD", 
"WEST VALLEY CITY", "WESTERVILLE", "WESTHAMPTON", "WESTMINSTER", 
"WESTMORELAND CITY", "WESTPORT", "WESTVILLE", "WETHERSFIELD", 
"WHARTON", "WHEELING", "WHITE PLAINS", "WHITEHALL", "WHITESTONE", 
"WHITESTOWN", "WHITING", "WHITMAN", "WHITTIER", "WICHITA", "WILDWOOD", 
"WILLARD", "WILLIAMSBURG", "WILLIAMSPORT", "WILLOW GROVE", "WILLOWBROOK", 
"WILLOWICK", "WILMETTE", "WILMINGTON", "WILSON", "WILTON MANORS", 
"WIMBERLEY", "WINDCREST", "WINDER", "WINNEBAGO", "WINSIDE", "WINSTON", 
"WINSTON SALEM", "WINTERSET", "WINTHROP", "WOLCOTT", "WOODBRIDGE", 
"WOODBURY HEIGHTS", "WOODLAND PARK", "WOODLAWN", "WOODS CROSS", 
"WOODSIDE", "WOODSTOCK", "WORTHINGTON", "WYOMISSING", "YARMOUTH", 
"YOE", "YONKERS", "YORK", "YORKTOWN", "YORKTOWN HEIGHTS", "YOUNG HARRIS", 
"YOUNGSTOWN", "YPSILANTI", "ZELIENOPLE", "ZEPHYRHILLS"), class = "factor"), 
    clusters = c(6L, 10L, 3L, 10L, 5L, 6L, 10L, 6L, 6L, 7L, 5L, 
    7L, 8L, 8L, 3L, 6L, 3L, 10L, 7L, 10L, 10L, 4L, 2L, 8L, 9L, 
    1L, 4L, 4L, 7L, 5L, 10L, 5L, 5L, 10L, 6L, 6L, 10L, 6L, 7L, 
    10L, 10L, 3L, 3L, 6L, 3L), Longitude = c(-93.883554, -90.582058, 
    -83.868923, -89.544083, -72.902467, -94.458904, -90.588533, 
    -94.527843, -92.93769, -80.029456, -70.58809, -82.888789, 
    -80.113071, -82.6661, -81.372992, -95.840964, -82.016013, 
    -89.92625, -80.276676, -89.778385, -87.641063, -94.888036, 
    -117.663119, -82.530568, -75.574437, -122.209047, -96.799309, 
    -95.746255, -80.553953, -73.923314, -87.713482, -73.9731, 
    -73.8187, -87.987023, -93.759478, -94.110903, -90.00597, 
    -93.257109, -79.932483, -90.201858, -89.100233, -84.625923, 
    -84.567614, -93.912921, -84.612564), Latitude = c(34.503045, 
    38.752515, 33.406527, 40.893588, 41.733088, 38.999032, 41.657674, 
    38.890929, 37.639766, 42.099693, 43.53462, 40.142492, 26.538149, 
    27.9062, 31.90104, 34.833083, 35.118836, 35.136047, 39.425712, 
    40.519207, 40.101126, 30.044457, 33.62287, 27.910276, 39.752254, 
    47.403778, 32.961929, 29.820199, 38.408649, 40.668828, 41.983948, 
    40.58116, 40.7253, 41.921667, 41.669921, 36.442552, 38.564663, 
    37.455315, 40.867774, 38.783554, 42.244381, 34.024332, 34.001644, 
    36.622704, 34.017832), distances_Me = c(1879089.079, 1437296.523, 
    1185638.742, 1313997.97, 156759.9125, 1759443.351, 1398398.997, 
    1767861.917, 1668917.322, 534234.2576, 431749.2965, 758508.4701, 
    1657031.938, 1615575.668, 1169085.702, 2024642.021, 930944.4681, 
    1525322.746, 553069.7532, 1337002.822, 1161367.37, 2223496.376, 
    3918520.212, 1609088.602, 164252.5146, 3878581.001, 2197910.688, 
    2304526.789, 614908.3151, 11075.24597, 1160375.454, 494.2997496, 
    21063.72622, 1182681.434, 1662351.873, 1810881.911, 1393581.436, 
    1701982.013, 504395.772, 1404348.991, 1276505.454, 1190457.718, 
    1188029.899, 1787261.336, 1189994.055), distances_Mi = c(1167.607468, 
    893.0913246, 736.719012, 816.477441, 97.40573055, 1093.263337, 
    868.9216123, 1098.494372, 1037.01328, 331.9565399, 268.2755749, 
    471.3135552, 1029.628072, 1003.868436, 726.4334682, 1258.049536, 
    578.4599174, 947.7880794, 343.6603307, 830.7719404, 721.6375366, 
    1381.611443, 2434.846498, 999.8375755, 102.0614003, 2410.029515, 
    1365.713293, 1431.96122, 382.0848884, 6.881813138, 721.0211909, 
    0.30714248, 13.08834388, 734.8814328, 1032.933715, 1125.225657, 
    865.9281298, 1057.55865, 313.4158337, 872.6187534, 793.1807589, 
    739.7133739, 738.2048023, 1110.548567, 739.4252678), ID = 2308:2352), row.names = c(NA, 
-45L), class = "data.frame")























Proprietary

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com> 
Sent: Wednesday, May 13, 2020 1:39 AM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Radius problem

**** External Email - Use Caution ****

Hi Bill,
A while ago I devised a couple of functions to accumulate millions of geographic locations of events and then display the resulting matrix of values on an existing plot. This may be of use to you, at least in the visualization of the density of the locations. As your example data only included a few points, the resulting plot looks pretty chunky as I had to use a large symbol to make the cells with non-zero counts obvious.

# read in your sample data
source("wp_data.R")
library(plotrix)
geomat<-makeDensityMatrix(geodat[,c("latitude","longitude")],
xlim=range(geodat$longitude),ylim=range(geodat$latitude))
# Range of density (>0) - 1.119816 3.922018
latlim<-range(geodat$latitude)
lonlim<-range(geodat$longitude)
library(maps)
map("world",xlim=lonlim,ylim=latlim)
axis(1)
axis(2)
densityGrid(geomat,range.cex=c(1,5),xlim=lonlim,ylim=latlim,
 red=c(0.5,1),green=0,blue=0,pch=15)

Jim

On Wed, May 13, 2020 at 10:57 AM Poling, William via R-help <r-help at r-project.org> wrote:
>
> #RStudio Version Version 1.2.1335
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
>
> Hello I am trying to figure out how to create concentration of ID's in 
> the following data, based on determining the highest concentration in 
> the smallest radius area based on Longitude and Latitude (geo 
> location)
>
> I have reviewed many websites looking for a function or tutorial, (if 
> you are aware of either please let me know)
>

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From pro|jcn@@h @end|ng |rom gm@||@com  Wed May 13 19:04:36 2020
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 13 May 2020 13:04:36 -0400
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <20200513152839.GA1776058@posteo.no>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
Message-ID: <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>

Note that my sessionInfo() gave

R version 4.0.0 (2020-04-24)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Linux Mint 19.3

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

So you have an older R but newer libraries, and the libblas is
a different one.

Given the output is very similar, and within the rounding
margins of the double arithmetic, this looks like the
libraries are very slightly different. I suppose I should
be more inquisitive and try to seek out the changelog or other
description of the differences, but ...

JN


On 2020-05-13 11:28 a.m., Rasmus Liland wrote:
> On 2020-05-09 11:40 -0400, J C Nash wrote:
>>
>>> solve(D)
>>      [,1] [,2]
>> [1,] -2.0  1.0
>> [2,]  1.5 -0.5
>>> D %*% solve(D)
>>      [,1]         [,2]
>> [1,]    1 1.110223e-16
>> [2,]    0 1.000000e+00
>>>
> 
> Dear list,
> 
> I get another solution on my Linux i7-7500U 
> laptop, but the same solution on my FreeBSD 
> E3-1240Lv5 machine with a really old R 
> version (without BLAS) ...
> 
> How is this possible?
> 
>> D %*% solve(D)
>              [,1] [,2]
> [1,] 1.000000e+00    0
> [2,] 8.881784e-16    1
>> sessionInfo()
> R version 3.6.3 (2020-02-29)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Arch Linux
> 
> Matrix products: default
> BLAS:   /usr/lib/libopenblasp-r0.3.9.so
> LAPACK: /usr/lib/liblapack.so.3.9.0
> 
> locale:
>  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
>  [3] LC_TIME=en_DK.UTF-8        LC_COLLATE=en_GB.UTF-8    
>  [5] LC_MONETARY=nb_NO.UTF-8    LC_MESSAGES=en_GB.UTF-8   
>  [7] LC_PAPER=nb_NO.UTF-8       LC_NAME=C                 
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
> [11] LC_MEASUREMENT=nb_NO.UTF-8 LC_IDENTIFICATION=C       
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base     
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.6.3
>>
> 
> From the machine running FreeBSD:
> 
>> D %*% solve(D)
>      [,1]         [,2]
> [1,]    1 1.110223e-16
> [2,]    0 1.000000e+00
>> sessionInfo()
> R version 3.5.2 (2018-12-20)
> Platform: amd64-portbld-freebsd12.0 (64-bit)
> Running under: FreeBSD hmm 12.0-RELEASE FreeBSD 12.0-RELEASE r341666 GENERIC  amd64
> 
> Matrix products: default
> LAPACK: /usr/local/lib/R/lib/libRlapack.so
> 
> locale:
> [1] C/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.5.2
>>
> 
> Best,
> Rasmus
>


From jr@| @end|ng |rom po@teo@no  Wed May 13 20:29:33 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 20:29:33 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
Message-ID: <20200513182933.GG1776058@posteo.no>

On 2020-05-13 13:04 -0400, J C Nash wrote:
> On 2020-05-13 11:28 a.m., Rasmus Liland wrote:
> > 
> > I get another solution on my Linux i7-7500U 
> > 
> > > D %*% solve(D)
> >              [,1] [,2]
> > [1,] 1.000000e+00    0
> > [2,] 8.881784e-16    1
> > > sessionInfo()
> > BLAS:   /usr/lib/libopenblasp-r0.3.9.so
> > LAPACK: /usr/lib/liblapack.so.3.9.0
> 
> Note that my sessionInfo() gave
> 
> R version 4.0.0 (2020-04-24)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Linux Mint 19.3
> 
> Matrix products: default
> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1
> 
> So you have an older R but newer libraries, and the libblas is
> a different one.
> 
> Given the output is very similar, and within the rounding
> margins of the double arithmetic, this looks like the
> libraries are very slightly different. I suppose I should
> be more inquisitive and try to seek out the changelog or other
> description of the differences, but ...
> 
> JN

Dear JN,

I was thinking BLAS could be changed to 
OpenBLAS, apparently not:

If I switch from OpenBLAS back to regular 
BLAS, the output is as expected ... I thought 
OpenBLAS should be a real alternative to BLAS 
in many cases, but not in this example?

> D %*% solve(D)
     [,1]         [,2]
[1,]    1 1.110223e-16
[2,]    0 1.000000e+00
> sessionInfo()
R version 3.6.3 (2020-02-29)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Arch Linux

Matrix products: default
BLAS:   /usr/lib/libblas.so.3.9.0
LAPACK: /usr/lib/liblapack.so.3.9.0

Best,
Rasmus


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 13 20:44:54 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 13 May 2020 11:44:54 -0700
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <20200513182933.GG1776058@posteo.no>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
 <20200513182933.GG1776058@posteo.no>
Message-ID: <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>

Depending on reproducibility in the least significant bits of floating point calculations is a bad practice. Just because you decide based on this one example that one implementation of BLAS is better than another does not mean that will be true for all specific examples. IMO you are drawing conclusions on data that is effectively random and should change your definition of "sufficient to the task".

On May 13, 2020 11:29:33 AM PDT, Rasmus Liland <jral at posteo.no> wrote:
>On 2020-05-13 13:04 -0400, J C Nash wrote:
>> On 2020-05-13 11:28 a.m., Rasmus Liland wrote:
>> > 
>> > I get another solution on my Linux i7-7500U 
>> > 
>> > > D %*% solve(D)
>> >              [,1] [,2]
>> > [1,] 1.000000e+00    0
>> > [2,] 8.881784e-16    1
>> > > sessionInfo()
>> > BLAS:   /usr/lib/libopenblasp-r0.3.9.so
>> > LAPACK: /usr/lib/liblapack.so.3.9.0
>> 
>> Note that my sessionInfo() gave
>> 
>> R version 4.0.0 (2020-04-24)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>> Running under: Linux Mint 19.3
>> 
>> Matrix products: default
>> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
>> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1
>> 
>> So you have an older R but newer libraries, and the libblas is
>> a different one.
>> 
>> Given the output is very similar, and within the rounding
>> margins of the double arithmetic, this looks like the
>> libraries are very slightly different. I suppose I should
>> be more inquisitive and try to seek out the changelog or other
>> description of the differences, but ...
>> 
>> JN
>
>Dear JN,
>
>I was thinking BLAS could be changed to 
>OpenBLAS, apparently not:
>
>If I switch from OpenBLAS back to regular 
>BLAS, the output is as expected ... I thought 
>OpenBLAS should be a real alternative to BLAS 
>in many cases, but not in this example?
>
>> D %*% solve(D)
>     [,1]         [,2]
>[1,]    1 1.110223e-16
>[2,]    0 1.000000e+00
>> sessionInfo()
>R version 3.6.3 (2020-02-29)
>Platform: x86_64-pc-linux-gnu (64-bit)
>Running under: Arch Linux
>
>Matrix products: default
>BLAS:   /usr/lib/libblas.so.3.9.0
>LAPACK: /usr/lib/liblapack.so.3.9.0
>
>Best,
>Rasmus
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 20:46:30 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 18:46:30 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
Message-ID: <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello all.

I am still struggling with this issue.

It appears that new installations are going to local drive.


# #Test 05/13/2020
# install.packages("abjutils")
# package ?abjutils? successfully unpacked and MD5 sums checked
# 
# The downloaded binary packages are in
# C:\Users\A436798\AppData\Local\Temp\RtmpCuXNJn\downloaded_packages

However, when I run .libPaths() it indicates our UNC path

[1] "\\\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library"


Is there a way for me to change this myself in my instance of R?

Thank you.

WHP 



Proprietary

-----Original Message-----
From: Ege Rubak <rubak at math.aau.dk> 
Sent: Tuesday, May 12, 2020 7:43 AM
To: Poling, William <PolingW at aetna.com>; r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with R-Markdown please

**** External Email - Use Caution ****

Looks like your files are on a Windows network drive (UNC path). I have experienced many problems with this on a Windows laptop from work. If at all possible you should avoid this. As an absolute minimum make sure your R library (collection of installed packages) is on the local file system and not a UNC path. If you are lucky this could be enough to make things work.

You can check your library location(s) with the command .libPaths() in an R session.

Good luck.

Ege

On Tue, 2020-05-12 at 10:15 +0000, Poling, William via R-help wrote:
> #UPDATED 05/05/2020
> #RStudio Version Version 1.2.1335 need this one--> 1.2.5019
> sessionInfo()
> # R version 4.0.0 Patched (2020-05-03 r78349)
> #Platform: x86_64-w64-mingw32/x64 (64-bit) #Running under: Windows 10 
> x64 (build 17763)
> 
> Good morning.
> 
> This is the first time I have tried RMarkdown on new laptop with new 
> employer.
> I am not sure how to go about fixing this problem below?
> 
> Would someone please advise course of action.
> 
> Thank you.
> 
> WHP
> 
> 
> 
> processing file: Member-Geo-Location-Test-V1.Rmd
>  
> |..........                                                          
>   |  14%
>   ordinary text without R code
> 
>  
> |....................                                                
>   |  29%
> label: unnamed-chunk-1 (with options) List of 1  $ echo: logi FALSE
> 
>  
> |..............................                                      
>   |  43%
>   ordinary text without R code
> 
>  
> |........................................                            
>   |  57%
> label: setup (with options)
> List of 2
>  $ include: logi FALSE
>  $ warning: logi FALSE
> 
>  
> |..................................................                  
>   |  71%
>   ordinary text without R code
> 
>  
> |............................................................        
>   |  86%
> label: Table1 (with options)
> List of 2
>  $ echo   : logi FALSE
>  $ results: chr "asis"
> 
>  
> |....................................................................
> ..| 100%
>    inline R code fragments
> 
> 
> output file: Member-Geo-Location-Test-V1.knit.md
> 
> "C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member- 
> Geo-Location-Test-V1.utf8.md --to html4 --from
> markdown+autolink_bare_uris+tex_math_single_backslash+smart --output
> Member-Geo-Location-Test-V1.html --email-obfuscation none --self- 
> contained --standalone --section-divs --template "\\winp-oaf-
> 113\FldrRedir_1$\A436798\Data\R\R-
> 4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight -- 
> variable highlightjs=1 --variable "theme:bootstrap" --include-in- 
> header "C:\Users\A436798\AppData\Local\Temp\RtmpSajZla\rmarkdown-
> str2ae846253313.html" --mathjax --variable "mathjax-url:
> https://urldefense.proofpoint.com/v2/url?u=https-3A__mathjax.rstudio.c
> om_latest_MathJax.js-3Fconfig-3DTeX-2DAMS-2DMML-5FHTMLorMML&d=DwIGaQ&c
> =wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTC
> tKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLXO8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=S
> MNU_B--j9FvBKh-0u1NG5RHd7AtLkvyVsi9ybyUwYU&e=
> " --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter
> "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
> Could not fetch http://?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html
> HttpExceptionRequest Request {
>   host                 = ""
>   port                 = 80
>   secure               = False
>   requestHeaders       = []
>   path                 = "/"
>   queryString          = "?/UNC/winp-oaf-
> 113/FldrRedir_1$/A436798/Data/R/R-
> 4.0.0patched/library/rmarkdown/rmd/h/default.html"
>   method               = "GET"
>   proxy                = Nothing
>   rawBody              = False
>   redirectCount        = 10
>   responseTimeout      = ResponseTimeoutDefault
>   requestVersion       = HTTP/1.1
> }
>  (InvalidDestinationHost "")
> Error: pandoc document conversion failed with error 61 Execution 
> halted
> 
> WHP
> 
> 
> Proprietary
> 
> NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may 
> con...{{dropped:16}}
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mail
> man_listinfo_r-2Dhelp&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmv
> OXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLXO
> 8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=W5iLmdkyVAbq4AQuAsTYsadWBk1jduptKOZLR25
> jPWo&e=
> PLEASE do read the posting guide
> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.R-2Dproject.or
> g_posting-2Dguide.html&d=DwIGaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmm
> vOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=uBce0mQRO8wdLX
> O8BDTtjk4R92KVzpjtkQ-NqYiAHCo&s=5V2OtH149c6MxDhlirX-Yd-v8uuEuVYih9DRq-
> ZmBF8&e= and provide commented, minimal, self-contained, reproducible 
> code.
--
Ege Rubak, Associate Professor,
Department of Mathematical Sciences, Aalborg University Skjernvej 4A, 9220 Aalborg East, Denmark
Phone: (+45)99408861
Mobile: (+45)30230252
Email: rubak at math.aau.dk

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From jr@| @end|ng |rom po@teo@no  Wed May 13 20:57:09 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 20:57:09 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
 <20200513182933.GG1776058@posteo.no>
 <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>
Message-ID: <20200513185709.GH1776058@posteo.no>

On 2020-05-13 11:44 -0700, Jeff Newmiller wrote:
> Depending on reproducibility in the least 
> significant bits of floating point 
> calculations is a bad practice. Just 
> because you decide based on this one 
> example that one implementation of BLAS is 
> better than another does not mean that will 
> be true for all specific examples. IMO you 
> are drawing conclusions on data that is 
> effectively random and should change your 
> definition of "sufficient to the task".

Dear Jeff,

Right, so I really would have wanted OpenBLAS 
to be as reproducible as regular BLAS in this 
one random example, but my hands remains tied 
on this since I do not know anything about 
BLAS ... 

More interestingly, could you dream up any 
idea as to what might cause this difference?

Best,
Rasmus


From jr@| @end|ng |rom po@teo@no  Wed May 13 21:09:51 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 21:09:51 +0200
Subject: [R] Help with R-Markdown please
In-Reply-To: <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <20200513190951.GI1776058@posteo.no>

On 2020-05-13 18:46 +0000, Poling, William via R-help wrote:
> Hello all.
> 
> I am still struggling with this issue.
> 
> It appears that new installations are going 
> to local drive.
> 
> # #Test 05/13/2020
> # install.packages("abjutils")
> # package ?abjutils? successfully unpacked and MD5 sums checked
> # 
> # The downloaded binary packages are in
> # C:\Users\A436798\AppData\Local\Temp\RtmpCuXNJn\downloaded_packages
> 
> However, when I run .libPaths() it 
> indicates our UNC path
> 
> [1] "\\\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library"
> 
> Is there a way for me to change this myself 
> in my instance of R?

Dear William,

Perhaps you could try to set the variable 
R_LIBS_USER to some local folder.  I do not 
know how to do this on Windows, but there 
seems to be some guys in this rstudio 
discussion thread that came to some sort of a 
conclusion: 
https://community.rstudio.com/t/help-regarding-package-installation-renviron-rprofile-r-libs-r-libs-site-and-r-libs-user-oh-my/13888/8 

Since long ago, I have had this line in my 
~/.zshrc.local  :)

[ -e /usr/bin/Rscript ] && R_LIBS_USER=`Rscript -e 'cat(Sys.getenv("R_LIBS_USER"))'`

Best,
Rasmus


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 21:27:02 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 19:27:02 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <20200513190951.GI1776058@posteo.no>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513190951.GI1776058@posteo.no>
Message-ID: <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hi Rasmus, thank you I will see if this is something I can do without IT admin access.

In the mean time I have reloaded rmarkdown. To local

package ?rmarkdown? successfully unpacked and MD5 sums checked

The downloaded binary packages are in
	C:\Users\A436798\AppData\Local\Temp\RtmpYjSy7G\downloaded_packages

library(rmarkdown)

But R is still looking for it in : " Could not fetch http://?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html"


More recently I have installed other pkgs for the first time use and they have gone to local like above for rmarkdown and I have used functions from them so they are being searched for correctly I guess?

Very confusing.

Thank you for your response Sir!

WHP

"C:/Program Files/RStudio/bin/pandoc/pandoc" +RTS -K512m -RTS Member-Geo-Location-Test-V1.utf8.md --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash+smart --output Member-Geo-Location-Test-V1.html --email-obfuscation none --self-contained --standalone --section-divs --template "\\winp-oaf-113\FldrRedir_1$\A436798\Data\R\R-4.0.0patched\library\rmarkdown\rmd\h\default.html" --no-highlight --variable highlightjs=1 --variable "theme:bootstrap" --include-in-header "C:\Users\A436798\AppData\Local\Temp\RtmpglP9GJ\rmarkdown-str314c55414a00.html" --mathjax --variable "mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/lua/pagebreak.lua" --lua-filter "\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/lua/latex-div.lua" 
output file: Member-Geo-Location-Test-V1.knit.md

Could not fetch http://?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html
HttpExceptionRequest Request {
  host                 = ""
  port                 = 80
  secure               = False
  requestHeaders       = []
  path                 = "/"
  queryString          = "?/UNC/winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library/rmarkdown/rmd/h/default.html"
  method               = "GET"
  proxy                = Nothing
  rawBody              = False
  redirectCount        = 10
  responseTimeout      = ResponseTimeoutDefault
  requestVersion       = HTTP/1.1
}
 (InvalidDestinationHost "")
Error: pandoc document conversion failed with error 61
Execution halted


WHP


Proprietary

-----Original Message-----
From: Rasmus Liland <jral at posteo.no> 
Sent: Wednesday, May 13, 2020 2:10 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with R-Markdown please

**** External Email - Use Caution ****

On 2020-05-13 18:46 +0000, Poling, William via R-help wrote:
> Hello all.
> 
> I am still struggling with this issue.
> 
> It appears that new installations are going to local drive.
> 
> # #Test 05/13/2020
> # install.packages("abjutils")
> # package ?abjutils? successfully unpacked and MD5 sums checked # # 
> The downloaded binary packages are in # 
> C:\Users\A436798\AppData\Local\Temp\RtmpCuXNJn\downloaded_packages
> 
> However, when I run .libPaths() it
> indicates our UNC path
> 
> [1] "\\\\winp-oaf-113/FldrRedir_1$/A436798/Data/R/R-4.0.0patched/library"
> 
> Is there a way for me to change this myself in my instance of R?

Dear William,

Perhaps you could try to set the variable R_LIBS_USER to some local folder.  I do not know how to do this on Windows, but there seems to be some guys in this rstudio discussion thread that came to some sort of a
conclusion: 
https://urldefense.proofpoint.com/v2/url?u=https-3A__community.rstudio.com_t_help-2Dregarding-2Dpackage-2Dinstallation-2Drenviron-2Drprofile-2Dr-2Dlibs-2Dr-2Dlibs-2Dsite-2Dand-2Dr-2Dlibs-2Duser-2Doh-2Dmy_13888_8&d=DwIDaQ&c=wluqKIiwffOpZ6k5sqMWMBOn0vyYnlulRJmmvOXCFpM&r=j7MrcIQm2xjHa8v-2mTpmTCtKvneM2ExlYvnUWbsByY&m=q_5fam9luSaeNgs-KOUZS-w-YAVaPzQdZgIlejBCDFA&s=NWDnltFH8cjC-pj6K08lWXC1FjfFcyY1mueSZnor9Ng&e=  

Since long ago, I have had this line in my ~/.zshrc.local  :)

[ -e /usr/bin/Rscript ] && R_LIBS_USER=`Rscript -e 'cat(Sys.getenv("R_LIBS_USER"))'`

Best,
Rasmus

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From jr@| @end|ng |rom po@teo@no  Wed May 13 21:49:30 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 21:49:30 +0200
Subject: [R] Help with R-Markdown please
In-Reply-To: <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513190951.GI1776058@posteo.no>
 <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <20200513194930.GJ1776058@posteo.no>

On 2020-05-13 19:27 +0000, Poling, William wrote:
> if this is something I can do without IT admin access.

Hi!  O.T. on laptops: Also, perhaps it is 
easier to find another laptop.  There seems 
to be some great ThinkPads readily available 
anywhere around the U.S., like an X61 or X220 
or something, at least that is my impression 
from various Facebook groups ... I mean if it 
is easier for you to complete your work on 
another machine where you have admin access 
... just for making a rmarkdown presentation 
or something, not for working with the 
sensitive insurance data ... 

Best,
Rasmus


From Po||ngW @end|ng |rom @etn@@com  Wed May 13 21:54:04 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 19:54:04 +0000
Subject: [R] Help with R-Markdown please
In-Reply-To: <20200513194930.GJ1776058@posteo.no>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513190951.GI1776058@posteo.no>
 <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513194930.GJ1776058@posteo.no>
Message-ID: <BYAPR06MB538371A33E8A8E86F0AF5216AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

I have R on personal laptop for consultative purposes from time to time, however, I cannot move data, confidentiality constraints, as you can imagine.

I have initiated another IT ticket with organization, I think I will get to the bottom of this at some point.

Thank you.

WHP


Proprietary

-----Original Message-----
From: Rasmus Liland <jral at posteo.no> 
Sent: Wednesday, May 13, 2020 2:50 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: Re: [R] Help with R-Markdown please

**** External Email - Use Caution ****

On 2020-05-13 19:27 +0000, Poling, William wrote:
> if this is something I can do without IT admin access.

Hi!  O.T. on laptops: Also, perhaps it is easier to find another laptop.  There seems to be some great ThinkPads readily available anywhere around the U.S., like an X61 or X220 or something, at least that is my impression from various Facebook groups ... I mean if it is easier for you to complete your work on another machine where you have admin access ... just for making a rmarkdown presentation or something, not for working with the sensitive insurance data ... 

Best,
Rasmus

NOTICE TO RECIPIENT OF INFORMATION:\ This e-mail may con...{{dropped:16}}


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed May 13 22:13:00 2020
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 13 May 2020 13:13:00 -0700
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <20200513185709.GH1776058@posteo.no>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
 <20200513182933.GG1776058@posteo.no>
 <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>
 <20200513185709.GH1776058@posteo.no>
Message-ID: <2DAD00EF-5DB9-40DC-9BEB-B2A598492810@dcn.davis.ca.us>

In general, any time you deal with floating point numbers having different magnitudes, you risk pushing some low precision bits out of the result. Simply changing the sequence of calculations such as a literal polynomial evaluation versus Horner's method can obtain different results. Take a course in Numerical Analysis to learn more.

[1] https://en.m.wikipedia.org/wiki/Horner%27s_method
[2] https://en.m.wikipedia.org/wiki/Numerical_analysis

On May 13, 2020 11:57:09 AM PDT, Rasmus Liland <jral at posteo.no> wrote:
>On 2020-05-13 11:44 -0700, Jeff Newmiller wrote:
>> Depending on reproducibility in the least 
>> significant bits of floating point 
>> calculations is a bad practice. Just 
>> because you decide based on this one 
>> example that one implementation of BLAS is 
>> better than another does not mean that will 
>> be true for all specific examples. IMO you 
>> are drawing conclusions on data that is 
>> effectively random and should change your 
>> definition of "sufficient to the task".
>
>Dear Jeff,
>
>Right, so I really would have wanted OpenBLAS 
>to be as reproducible as regular BLAS in this 
>one random example, but my hands remains tied 
>on this since I do not know anything about 
>BLAS ... 
>
>More interestingly, could you dream up any 
>idea as to what might cause this difference?
>
>Best,
>Rasmus

-- 
Sent from my phone. Please excuse my brevity.


From @purd|e@@ @end|ng |rom gm@||@com  Wed May 13 22:17:16 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 14 May 2020 08:17:16 +1200
Subject: [R] Help with Radius problem --update
In-Reply-To: <BYAPR06MB53831A2287FAA73544C21AEFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53831A2287FAA73544C21AEFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <CAB8pepxirFcetLgnK1pyHqdkr7Q1aNeERrfkxoLTrQkerQxEKg@mail.gmail.com>

> "determine the largest concentration of members in the smallest radius"

I haven't read the whole thread, and I'm not familiar with this topic.
However, looking at it from an intuitive perspective, isn't the
smallest radius zero.
If the concentration means the number of "members" divided by the area...
...then would the largest concentration be undefined, merely because
of division by zero.

Perhaps your question is a valid one.
But if you're wanting a large pool of people to consider helping, I'd
recommend a more (mathematically) precise definition of the problem
you're trying to solve.


From jr@| @end|ng |rom po@teo@no  Wed May 13 23:03:58 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 23:03:58 +0200
Subject: [R] solve() function freezes CLI in GNU R 3.6.3
In-Reply-To: <2DAD00EF-5DB9-40DC-9BEB-B2A598492810@dcn.davis.ca.us>
References: <CAL26gRRVrOxcs_KwDY0XOc9g8PbyYkYp7gz4Qsj-Fj_fdfWmvw@mail.gmail.com>
 <7754f2dc-33a5-ff7b-4867-ff0a3fe0e613@gmail.com>
 <20200513152839.GA1776058@posteo.no>
 <1c5e1a18-2e87-9507-5156-f5140692e4b8@gmail.com>
 <20200513182933.GG1776058@posteo.no>
 <CF387B40-317A-4A0D-8B3C-FB322A5E448B@dcn.davis.ca.us>
 <20200513185709.GH1776058@posteo.no>
 <2DAD00EF-5DB9-40DC-9BEB-B2A598492810@dcn.davis.ca.us>
Message-ID: <20200513210358.GK1776058@posteo.no>

On 2020-05-13 13:13 -0700, Jeff Newmiller wrote:
> In general, any time you deal with floating 
> point numbers having different magnitudes, 
> you risk pushing some low precision bits 
> out of the result. Simply changing the 
> sequence of calculations such as a literal 
> polynomial evaluation versus Horner's 
> method can obtain different results. Take a 
> course in Numerical Analysis to learn 
> more.
> 
> [1] https://en.m.wikipedia.org/wiki/Horner%27s_method
> [2] https://en.m.wikipedia.org/wiki/Numerical_analysis

Right, it seems fairly interesting.  I'll 
look into it at some point.

/JR

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20200513/8fa8a0f6/attachment.sig>

From Po||ngW @end|ng |rom @etn@@com  Wed May 13 23:05:09 2020
From: Po||ngW @end|ng |rom @etn@@com (Poling, William)
Date: Wed, 13 May 2020 21:05:09 +0000
Subject: [R] Help with Radius problem --update
In-Reply-To: <CAB8pepxirFcetLgnK1pyHqdkr7Q1aNeERrfkxoLTrQkerQxEKg@mail.gmail.com>
References: <BYAPR06MB53831A2287FAA73544C21AEFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <CAB8pepxirFcetLgnK1pyHqdkr7Q1aNeERrfkxoLTrQkerQxEKg@mail.gmail.com>
Message-ID: <BYAPR06MB538339AFB669DEDF219F24AFAEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>

Hello Abby and thank you for your response.

Your surly correct.

I have not worked a problem like this previously, however, I am learning fast.

I did not think I would need to apply mathematical formula-calculations for this task, math in general not my primary area of expertise, but always very curious.

However your point is provocative and now I am interested to learn more in that space as well.

By researching the literature on Radii, and disk partial coverage problem, and anything in stack overflow remotely relevant to my question I am slowly piecing a solution together.

What I described in the string is my best thinking in terms of delivering something, anything, at the moment that I can build on, refine, etc.

1. I have data set with 2352 geo locations in 41 states from Maine to California
2. I have determined, using density plot (thank to Jim L), that the most reasonable reference point in terms of realizing population density using distance among my 2353 is Brooklyn, NY.
3. Calculating distances from that point to the other 2351 based on a script referenced in stack over flow I simply plug in, manually-iteratively, distances (8 at the moment) and determine how many members are in that radius
4. Then determine the % decay from Max to Min of these 8 distances and settle on the one that has the highest concentration with the least decay
4. This piece (Item 3) I would like to make more efficient and was the point of my last submission to the R-Help list on this topic

For example: Radius2 = 1500 miles, Radius3 = 1200 miles, Radius4 = 1000 miles
The difference b/w Radius2 in terms of percent missing coverage and Radius3 is 0.057 while the percent missing coverage b/w Radius2 and Radius4 is 0.204

So my logic is that despite being a larger radius ,Radius3 vs Radius4, the trade off is more inclusive of the membership and therefore optimal among the 8 iterations. For the moment

I appreciate your response and any direction or advice would be most appreciated.

WHP

 



Proprietary

-----Original Message-----
From: Abby Spurdle <spurdle.a at gmail.com> 
Sent: Wednesday, May 13, 2020 3:17 PM
To: Poling, William <PolingW at aetna.com>
Cc: r-help at r-project.org
Subject: [EXTERNAL] Re: [R] Help with Radius problem --update

**** External Email - Use Caution ****

> "determine the largest concentration of members in the smallest radius"

I haven't read the whole thread, and I'm not familiar with this topic.
However, looking at it from an intuitive perspective, isn't the smallest radius zero.
If the concentration means the number of "members" divided by the area...
...then would the largest concentration be undefined, merely because of division by zero.

Perhaps your question is a valid one.
But if you're wanting a large pool of people to consider helping, I'd recommend a more (mathematically) precise definition of the problem you're trying to solve.

NOTICE TO RECIPIENT OF INFORMATION:
This e-mail may contain confidential or privileged information. If you think you have received this e-mail in error, please advise the sender by reply e-mail and then delete this e-mail immediately.  
This e-mail may also contain protected health information (PHI) with information about sensitive medical conditions, including, but not limited to, treatment for substance use disorders, behavioral health, HIV/AIDS, or pregnancy. This type of information may be protected by various federal and/or state laws which prohibit any further disclosure without the express written consent of the person to whom it pertains or as otherwise permitted by law. Any unauthorized further disclosure may be considered a violation of federal and/or state law. A general authorization for the release of medical or other information may NOT be sufficient consent for release of this type of information.
Thank you. Aetna

From jr@| @end|ng |rom po@teo@no  Wed May 13 23:13:40 2020
From: jr@| @end|ng |rom po@teo@no (Rasmus Liland)
Date: Wed, 13 May 2020 23:13:40 +0200
Subject: [R] Help with R-Markdown please
In-Reply-To: <BYAPR06MB538371A33E8A8E86F0AF5216AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
References: <BYAPR06MB53837B82850FAC0378779843AEBE0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <e1212a34f6b1dbabeddb85c4ad3cf015eb4c8c16.camel@math.aau.dk>
 <BYAPR06MB5383D57CB539FC1E76498221AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513190951.GI1776058@posteo.no>
 <BYAPR06MB5383E715E004A64C543041C4AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
 <20200513194930.GJ1776058@posteo.no>
 <BYAPR06MB538371A33E8A8E86F0AF5216AEBF0@BYAPR06MB5383.namprd06.prod.outlook.com>
Message-ID: <20200513211340.GL1776058@posteo.no>

On 2020-05-13 19:54 +0000, Poling, William wrote:
> I have R on personal laptop for 
> consultative purposes from time to time, 
> however, I cannot move data, 
> confidentiality constraints, as you can 
> imagine.
> 
> I have initiated another IT ticket with 
> organization, I think I will get to the 
> bottom of this at some point.

Great, I hope you get to the bottom of this! 
??????


From @purd|e@@ @end|ng |rom gm@||@com  Wed May 13 23:15:54 2020
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Thu, 14 May 2020 09:15:54 +1200
Subject: [R] Fitting Richards' curve
In-Reply-To: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
References: <CA+dpOJmuzENh8zusk3hhb3Kpa6kpkcpQTRZ1H1S3Z0rp-ZSztw@mail.gmail.com>
Message-ID: <CAB8pepwt2hzL0kyfP7QPwf9tFA5QtAxR_1AfzGpP-_j8TJNNug@mail.gmail.com>

Hi Christofer,

This doesn't really answer your question.
But if the goal is to fit an S-shaped curve to data, with increased
flexibility...
(I'm assuming that's the goal).

...then I'd like to note the option of splines (or smoothing), subject
to shape constraints...

My guess, is it's probably easier to model the inverse of a growth
curve this way, than to model the growth curve directly.
In which case, a 4-piece to 10-piece spline should give considerably flexibly.

It's possible that Martin's package, cobs, can do this, but not sure,
I haven't tried it.
And there may be other R packages for fitting splines/smoothers to
data, subject to shape constraints.

If not, I'm guessing it wouldn't be too difficult to implement, via
extensions to the quadprog package, for quadratic programming.


On Wed, May 13, 2020 at 9:26 PM Christofer Bogaso
<bogaso.christofer at gmail.com> wrote:
>
> Hi,
>
> Is there any R package to fit Richards' curve in the form of
> https://en.wikipedia.org/wiki/Generalised_logistic_function
>
> I found there is one package grofit, but currently defunct.
>
> Any pointer appreciated.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkr|de@u @end|ng |rom gm@||@com  Wed May 13 16:04:11 2020
From: jrkr|de@u @end|ng |rom gm@||@com (John Kane)
Date: Wed, 13 May 2020 10:04:11 -0400
Subject: [R] Extracting the first currency value from PDF files
In-Reply-To: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
References: <MA1PR01MB3513376769EBEFABDAB09D05B9BF0@MA1PR01MB3513.INDPRD01.PROD.OUTLOOK.COM>
Message-ID: <CAKZQJMBEaa93pygciMsa=aeS2Qj4X-EL-GybTZzD+Tc8WcDaDQ-17@mail.gmail.com>

It looks like you are using the str_nth_currency() function from the strex
package but we have no idea of what the pdf files are or how you are
importing them is to R.
We need a lot more information on what you are doing "before" you use the
function.

Have a look at
http://adv-r.had.co.nz/Reproducibility.html
or
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example



On Wed, 13 May 2020 at 09:33, Manish Mukherjee <manishmukherjee at hotmail.com>
wrote:

> Hi All,
>
> Need some help with the following code , i have a number of pdf files ,
> and the first page of those files gives a currency value $xxx,xxx,xxx . How
> to extract this value from a number of PDF files and put it in a data frame
> . I am able to do it for a single file
> with the code where opinions is the text data and 1 is the first currency
> value
> ```
> d=str_nth_currency(opinions, 1)
> df = subset(d, select = c(amount) )
> df
>
> I want this to loop over multiple pdf files
>
> I have tried somesthing like this but not working
> for (i in 1:length(files)){
>   print(i)
>   pdf_text(paste("filepath ", files[i],sep = ""))
>   str_nth_currency(files[i], 1)
> }
>
>
> Please help.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
John Kane
Kingston ON Canada

	[[alternative HTML version deleted]]


